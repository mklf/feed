{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-08T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"NUS-IDS at FinCausal 2021: Dependency Tree in Graph Neural Network for Better Cause-Effect Span Detection. (arXiv:2110.02991v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02991","description":"<p>Automatic identification of cause-effect spans in financial documents is\nimportant for causality modelling and understanding reasons that lead to\nfinancial events. To exploit the observation that words are more connected to\nother words with the same cause-effect type in a dependency tree, we construct\nuseful graph embeddings by incorporating dependency relation features through a\ngraph neural network. Our model builds on a baseline BERT token classifier with\nViterbi decoding, and outperforms this baseline in cross-validation and during\nthe competition. In the official run of FinCausal 2021, we obtained Precision,\nRecall, and F1 scores of 95.56%, 95.56% and 95.57% that all ranked 1st place,\nand an Exact Match score of 86.05% which ranked 3rd place.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1\">Fiona Anting Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1\">See-Kiong Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Multimodal Language Representations using Convolutional Autoencoders. (arXiv:2110.03007v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03007","description":"<p>Multimodal Language Analysis is a demanding area of research, since it is\nassociated with two requirements: combining different modalities and capturing\ntemporal information. During the last years, several works have been proposed\nin the area, mostly centered around supervised learning in downstream tasks. In\nthis paper we propose extracting unsupervised Multimodal Language\nrepresentations that are universal and can be applied to different tasks.\nTowards this end, we map the word-level aligned multimodal sequences to 2-D\nmatrices and then use Convolutional Autoencoders to learn embeddings by\ncombining multiple datasets. Extensive experimentation on Sentiment Analysis\n(MOSEI) and Emotion Recognition (IEMOCAP) indicate that the learned\nrepresentations can achieve near-state-of-the-art performance with just the use\nof a Logistic Regression algorithm for downstream classification. It is also\nshown that our method is extremely lightweight and can be easily generalized to\nother tasks and unseen data with small performance drop and almost the same\nnumber of parameters. The proposed multimodal representation models are\nopen-sourced and will help grow the applicability of Multimodal Language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koromilas_P/0/1/0/all/0/1\">Panagiotis Koromilas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannakopoulos_T/0/1/0/all/0/1\">Theodoros Giannakopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emphasis control for parallel neural TTS. (arXiv:2110.03012v1 [eess.AS])","link":"http://arxiv.org/abs/2110.03012","description":"<p>The semantic information conveyed by a speech signal is strongly influenced\nby local variations in prosody. Recent parallel neural text-to-speech (TTS)\nsynthesis methods are able to generate speech with high fidelity while\nmaintaining high performance. However, these systems often lack simple control\nover the output prosody, thus restricting the semantic information conveyable\nfor a given text. This paper proposes a hierarchical parallel neural TTS system\nfor prosodic emphasis control by learning a latent space that directly\ncorresponds to a change in emphasis. Three candidate features for the latent\nspace are compared: 1) Variance of pitch and duration within words in a\nsentence, 2) a wavelet based feature computed from pitch, energy, and duration\nand 3) a learned combination of the above features. Objective measures reveal\nthat the proposed methods are able to achieve a wide range of emphasis\nmodification, and subjective evaluations on the degree of emphasis and the\noverall quality indicate that they show promise for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Seshadri_S/0/1/0/all/0/1\">Shreyas Seshadri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raitio_T/0/1/0/all/0/1\">Tuomo Raitio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Castellani_D/0/1/0/all/0/1\">Dan Castellani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiangchuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation. (arXiv:2110.03036v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03036","description":"<p>A \"bigger is better\" explosion in the number of parameters in deep neural\nnetworks has made it increasingly challenging to make state-of-the-art networks\naccessible in compute-restricted environments. Compression techniques have\ntaken on renewed importance as a way to bridge the gap. However, evaluation of\nthe trade-offs incurred by popular compression techniques has been centered on\nhigh-resource datasets. In this work, we instead consider the impact of\ncompression in a data-limited regime. We introduce the term low-resource double\nbind to refer to the co-occurrence of data limitations and compute resource\nconstraints. This is a common setting for NLP for low-resource languages, yet\nthe trade-offs in performance are poorly studied. Our work offers surprising\ninsights into the relationship between capacity and generalization in\ndata-limited regimes for the task of machine translation. Our experiments on\nmagnitude pruning for translations from English into Yoruba, Hausa, Igbo and\nGerman show that in low-resource regimes, sparsity preserves performance on\nfrequent sentences but has a disparate impact on infrequent ones. However, it\nimproves robustness to out-of-distribution shifts, especially for datasets that\nare very distinct from the training distribution. Our findings suggest that\nsparsity can play a beneficial role at curbing memorization of low frequency\nattributes, and therefore offers a promising solution to the low-resource\ndouble bind.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahia_O/0/1/0/all/0/1\">Orevaoghene Ahia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1\">Julia Kreutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Categorical Features in End-to-End ASR. (arXiv:2110.03047v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03047","description":"<p>All-neural, end-to-end ASR systems gained rapid interest from the speech\nrecognition community. Such systems convert speech input to text units using a\nsingle trainable neural network model. E2E models require large amounts of\npaired speech text data that is expensive to obtain. The amount of data\navailable varies across different languages and dialects. It is critical to\nmake use of all these data so that both low resource languages and high\nresource languages can be improved. When we want to deploy an ASR system for a\nnew application domain, the amount of domain specific training data is very\nlimited. To be able to leverage data from existing domains is important for ASR\naccuracy in the new domain. In this paper, we treat all these aspects as\ncategorical information in an ASR system, and propose a simple yet effective\nway to integrate categorical features into E2E model. We perform detailed\nanalysis on various training strategies, and find that building a joint model\nthat includes categorical features can be more accurate than multiple\nindependently trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rongqing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Neurons Invariant to Sentence Structural Changes in Neural Machine Translation. (arXiv:2110.03067v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03067","description":"<p>To gain insight into the role neurons play, we study the activation patterns\ncorresponding to meaning-preserving paraphrases (e.g., active-passive). We\ncompile a dataset of controlled syntactic paraphrases in English with their\nreference German translations and demonstrate our model-agnostic approach with\nthe Transformer translation model. First, we identify neurons that correlate\nacross paraphrases and dissect the observed correlation into possible\nconfounds. Although lower-level components are found as the cause of similar\nactivations, no sentence-level semantics or syntax are detected locally. Later,\nwe manipulate neuron activations to influence translation towards a particular\nsyntactic form. We find that a simple value shift is effective, and more so\nwhen many neurons are modified. These suggest that complex syntactic\nconstructions are indeed encoded in the model. We conclude by discussing how to\nbetter manipulate it using the correlations we first obtained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_G/0/1/0/all/0/1\">Gal Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRAFT-What you always wanted to know but could not find about block-based environments. (arXiv:2110.03073v1 [cs.SE])","link":"http://arxiv.org/abs/2110.03073","description":"<p>Block-based environments are visual programming environments, which are\nbecoming more and more popular because of their ease of use. The ease of use\ncomes thanks to their intuitive graphical representation and structural\nmetaphors (jigsaw-like puzzles) to display valid combinations of language\nconstructs to the users. Part of the current popularity of block-based\nenvironments is thanks to Scratch. As a result they are often associated with\ntools for children or young learners. However, it is unclear how these types of\nprogramming environments are developed and used in general. So we conducted a\nsystematic literature review on block-based environments by studying 152 papers\npublished between 2014 and 2020, and a non-systematic tool review of 32\nblock-based environments. In particular, we provide a helpful inventory of\nblock-based editors for end-users on different topics and domains. Likewise, we\nfocused on identifying the main components of block-based environments, how\nthey are engineered, and how they are used. This survey should be equally\nhelpful for language engineering researchers and language engineers alike.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merino_M/0/1/0/all/0/1\">Mauricio Verano Merino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinju_J/0/1/0/all/0/1\">Jurgen Vinju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brand_M/0/1/0/all/0/1\">Mark van den Brand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTC Variations Through New WFST Topologies. (arXiv:2110.03098v1 [eess.AS])","link":"http://arxiv.org/abs/2110.03098","description":"<p>This paper presents novel Weighted Finite-State Transducer (WFST) topologies\nto implement Connectionist Temporal Classification (CTC)-like algorithms for\nautomatic speech recognition. Three new CTC variants are proposed: (1) the\n\"compact-CTC\", in which direct transitions between units are replaced with\n&lt;epsilon&gt; back-off transitions; (2) the \"minimal-CTC\", that only adds &lt;blank&gt;\nself-loops when used in WFST-composition; and (3) \"selfless-CTC\", that\ndisallows self-loop for non-blank units. The new CTC variants have several\nbenefits, such as reducing decoding graph size and GPU memory required for\ntraining while keeping model accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Laptev_A/0/1/0/all/0/1\">Aleksandr Laptev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Majumdar_S/0/1/0/all/0/1\">Somshubra Majumdar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cut the CARP: Fishing for zero-shot story evaluation. (arXiv:2110.03111v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03111","description":"<p>Recent advances in large-scale language models (Raffel et al., 2019; Brown et\nal., 2020) have brought significant qualitative and quantitative improvements\nin machine-driven text generation. Despite this, generation and evaluation of\nmachine-generated narrative text remains a challenging problem. Objective\nevaluation of computationally-generated stories may be prohibitively expensive,\nrequire meticulously annotated datasets, or may not adequately measure the\nlogical coherence of a generated story's narratological structure.\n</p>\n<p>Informed by recent advances in contrastive learning (Radford et al., 2021),\nwe present Contrastive Authoring and Reviewing Pairing (CARP): a scalable,\nefficient method for performing qualitatively superior, zero-shot evaluation of\nstories. We show a strong correlation between human evaluation of stories and\nthose of CARP. Model outputs more significantly correlate with corresponding\nhuman input than those language-model based methods which utilize finetuning or\nprompt engineering approaches. We also present and analyze the Story-Critique\nDataset, a new corpora composed of 1.3 million aligned story-critique pairs\nderived from over 80,000 stories. We expect this corpus to be of interest to\nNLP researchers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matiana_S/0/1/0/all/0/1\">Shahbuland Matiana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">JR Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teehan_R/0/1/0/all/0/1\">Ryan Teehan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castricato_L/0/1/0/all/0/1\">Louis Castricato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Leo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frazier_S/0/1/0/all/0/1\">Spencer Frazier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Transformer-Based Language Models on Extractive Question Answering. (arXiv:2110.03142v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03142","description":"<p>Question Answering (QA) is a task in natural language processing that has\nseen considerable growth after the advent of transformers. There has been a\nsurge in QA datasets that have been proposed to challenge natural language\nprocessing models to improve human and existing model performance. Many\npre-trained language models have proven to be incredibly effective at the task\nof extractive question answering. However, generalizability remains as a\nchallenge for the majority of these models. That is, some datasets require\nmodels to reason more than others. In this paper, we train various pre-trained\nlanguage models and fine-tune them on multiple question answering datasets of\nvarying levels of difficulty to determine which of the models are capable of\ngeneralizing the most comprehensively across different datasets. Further, we\npropose a new architecture, BERT-BiLSTM, and compare it with other language\nmodels to determine if adding more bidirectionality can improve model\nperformance. Using the F1-score as our metric, we find that the RoBERTa and\nBART pre-trained models perform the best across all datasets and that our\nBERT-BiLSTM model outperforms the baseline BERT model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pearce_K/0/1/0/all/0/1\">Kate Pearce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_T/0/1/0/all/0/1\">Tiffany Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komanduri_A/0/1/0/all/0/1\">Aneesh Komanduri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1\">Justin Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transcribe-to-Diarize: Neural Speaker Diarization for Unlimited Number of Speakers using End-to-End Speaker-Attributed ASR. (arXiv:2110.03151v1 [eess.AS])","link":"http://arxiv.org/abs/2110.03151","description":"<p>This paper presents Transcribe-to-Diarize, a new approach for neural speaker\ndiarization that uses an end-to-end (E2E) speaker-attributed automatic speech\nrecognition (SA-ASR). The E2E SA-ASR is a joint model that was recently\nproposed for speaker counting, multi-talker speech recognition, and speaker\nidentification from monaural audio that contains overlapping speech. Although\nthe E2E SA-ASR model originally does not estimate any time-related information,\nwe show that the start and end times of each word can be estimated with\nsufficient accuracy from the internal state of the E2E SA-ASR by adding a small\nnumber of learnable parameters. Similar to the target-speaker voice activity\ndetection (TS-VAD)-based diarization method, the E2E SA-ASR model is applied to\nestimate speech activity of each speaker while it has the advantages of (i)\nhandling unlimited number of speakers, (ii) leveraging linguistic information\nfor speaker diarization, and (iii) simultaneously generating speaker-attributed\ntranscriptions. Experimental results on the LibriCSS and AMI corpora show that\nthe proposed method achieves significantly better diarization error rate than\nvarious existing speaker diarization methods when the number of speakers is\nunknown, and achieves a comparable performance to TS-VAD when the number of\nspeakers is given in advance. The proposed method simultaneously generates\nspeaker-attributed transcription with state-of-the-art accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transliteration of Foreign Words in Burmese. (arXiv:2110.03163v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03163","description":"<p>This manuscript provides general descriptions on transliteration of foreign\nwords in the Burmese language. Phenomena caused by phonetic and orthographic\nissues are discussed. Based on this work, we expect to gradually establish\nprescriptive guidelines to normalize the transliteration in Burmese in future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Chenchen Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HowSumm: A Multi-Document Summarization Dataset Derived from WikiHow Articles. (arXiv:2110.03179v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03179","description":"<p>We present \\textsc{HowSumm}, a novel large-scale dataset for the task of\nquery-focused multi-document summarization (qMDS), which targets the use-case\nof generating actionable instructions from a set of sources. This use-case is\ndifferent from the use-cases covered in existing multi-document summarization\n(MDS) datasets and is applicable to educational and industrial scenarios. We\nemployed automatic methods, and leveraged statistics from existing\nhuman-crafted qMDS datasets, to create \\textsc{HowSumm} from wikiHow website\narticles and the sources they cite. We describe the creation of the dataset and\ndiscuss the unique features that distinguish it from other summarization\ncorpora. Automatic and human evaluations of both extractive and abstractive\nsummarization models on the dataset reveal that there is room for improvement.\n% in existing summarization models We propose that \\textsc{HowSumm} can be\nleveraged to advance summarization research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boni_O/0/1/0/all/0/1\">Odellia Boni</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Feigenblat_G/0/1/0/all/0/1\">Guy Feigenblat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lev_G/0/1/0/all/0/1\">Guy Lev</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Shmueli_Scheuer_M/0/1/0/all/0/1\">Michal Shmueli-Scheuer</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sznajder_B/0/1/0/all/0/1\">Benjamin Sznajder</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Konopnicki_D/0/1/0/all/0/1\">David Konopnicki</a> ((1) IBM Research - AI)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GNN is a Counter? Revisiting GNN for Question Answering. (arXiv:2110.03192v1 [cs.AI])","link":"http://arxiv.org/abs/2110.03192","description":"<p>Question Answering (QA) has been a long-standing research topic in AI and NLP\nfields, and a wealth of studies have been conducted to attempt to equip QA\nsystems with human-level reasoning capability. To approximate the complicated\nhuman reasoning process, state-of-the-art QA systems commonly use pre-trained\nlanguage models (LMs) to access knowledge encoded in LMs together with\nelaborately designed modules based on Graph Neural Networks (GNNs) to perform\nreasoning over knowledge graphs (KGs). However, many problems remain open\nregarding the reasoning functionality of these GNN-based modules. Can these\nGNN-based modules really perform a complex reasoning process? Are they under-\nor over-complicated for QA? To open the black box of GNN and investigate these\nproblems, we dissect state-of-the-art GNN modules for QA and analyze their\nreasoning capability. We discover that even a very simple graph neural counter\ncan outperform all the existing GNN modules on CommonsenseQA and OpenBookQA,\ntwo popular QA benchmark datasets which heavily rely on knowledge-aware\nreasoning. Our work reveals that existing knowledge-aware GNN modules may only\ncarry out some simple reasoning such as counting. It remains a challenging open\nproblem to build comprehensive reasoning modules for knowledge-powered QA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Le Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Influence Tuning: Demoting Spurious Correlations via Instance Attribution and Instance-Driven Updates. (arXiv:2110.03212v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03212","description":"<p>Among the most critical limitations of deep learning NLP models are their\nlack of interpretability, and their reliance on spurious correlations. Prior\nwork proposed various approaches to interpreting the black-box models to unveil\nthe spurious correlations, but the research was primarily used in\nhuman-computer interaction scenarios. It still remains underexplored whether or\nhow such model interpretations can be used to automatically \"unlearn\"\nconfounding features. In this work, we propose influence tuning--a procedure\nthat leverages model interpretations to update the model parameters towards a\nplausible interpretation (rather than an interpretation that relies on spurious\npatterns in the data) in addition to learning to predict the task labels. We\nshow that in a controlled setup, influence tuning can help deconfounding the\nmodel from spurious patterns in data, significantly outperforming baseline\nmethods that use adversarial training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaochuang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Continual Knowledge Learning of Language Models. (arXiv:2110.03215v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03215","description":"<p>Large Language Models (LMs) are known to encode world knowledge in their\nparameters as they pretrain on a vast amount of web corpus, which is often\nutilized for performing knowledge-dependent downstream tasks such as question\nanswering, fact-checking, and open dialogue. In real-world scenarios, the world\nknowledge stored in the LMs can quickly become outdated as the world changes,\nbut it is non-trivial to avoid catastrophic forgetting and reliably acquire new\nknowledge while preserving invariant knowledge. To push the community towards\nbetter maintenance of ever-changing LMs, we formulate a new continual learning\n(CL) problem called Continual Knowledge Learning (CKL). We construct a new\nbenchmark and metric to quantify the retention of time-invariant world\nknowledge, the update of outdated knowledge, and the acquisition of new\nknowledge. We adopt applicable recent methods from literature to create several\nstrong baselines. Through extensive experiments, we find that CKL exhibits\nunique challenges that are not addressed in previous CL setups, where parameter\nexpansion is necessary to reliably retain and learn knowledge simultaneously.\nBy highlighting the critical causes of knowledge forgetting, we show that CKL\nis a challenging and important problem that helps us better understand and\ntrain ever-changing LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joel Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Seonghyeon Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sohee Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Joongbo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Janghoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyeonghun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Stanley Jungkyu Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layer-wise Pruning of Transformer Attention Heads for Efficient Language Modeling. (arXiv:2110.03252v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03252","description":"<p>While Transformer-based models have shown impressive language modeling\nperformance, the large computation cost is often prohibitive for practical use.\nAttention head pruning, which removes unnecessary attention heads in the\nmultihead attention, is a promising technique to solve this problem. However,\nit does not evenly reduce the overall load because the heavy feedforward module\nis not affected by head pruning. In this paper, we apply layer-wise attention\nhead pruning on All-attention Transformer so that the entire computation and\nthe number of parameters can be reduced proportionally to the number of pruned\nheads. While the architecture has the potential to fully utilize head pruning,\nwe propose three training methods that are especially helpful to minimize\nperformance degradation and stabilize the pruning process. Our pruned model\nshows consistently lower perplexity within a comparable parameter size than\nTransformer-XL on WikiText-103 language modeling benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shim_K/0/1/0/all/0/1\">Kyuhong Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_I/0/1/0/all/0/1\">Iksoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_W/0/1/0/all/0/1\">Wonyong Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jungwook Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Situated Dialogue Learning through Procedural Environment Generation. (arXiv:2110.03262v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03262","description":"<p>We teach goal-driven agents to interactively act and speak in situated\nenvironments by training on generated curriculums. Our agents operate in LIGHT\n(Urbanek et al. 2019) -- a large-scale crowd-sourced fantasy text adventure\ngame wherein an agent perceives and interacts with the world through textual\nnatural language. Goals in this environment take the form of character-based\nquests, consisting of personas and motivations. We augment LIGHT by learning to\nprocedurally generate additional novel textual worlds and quests to create a\ncurriculum of steadily increasing difficulty for training agents to achieve\nsuch goals. In particular, we measure curriculum difficulty in terms of the\nrarity of the quest in the original training distribution -- an easier\nenvironment is one that is more likely to have been found in the unaugmented\ndataset. An ablation study shows that this method of learning from the tail of\na distribution results in significantly higher generalization abilities as\nmeasured by zero-shot performance on never-before-seen quests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Renee Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark O. Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-tasking Dialogue Comprehension with Discourse Parsing. (arXiv:2110.03269v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03269","description":"<p>Multi-party dialogue machine reading comprehension (MRC) raises an even more\nchallenging understanding goal on dialogue with more than two involved\nspeakers, compared with the traditional plain passage style MRC. To accurately\nperform the question-answering (QA) task according to such multi-party\ndialogue, models have to handle fundamentally different discourse relationships\nfrom common non-dialogue plain text, where discourse relations are supposed to\nconnect two far apart utterances in a linguistics-motivated way.To further\nexplore the role of such unusual discourse structure on the correlated QA task\nin terms of MRC, we propose the first multi-task model for jointly performing\nQA and discourse parsing (DP) on the multi-party dialogue MRC task. Our\nproposed model is evaluated on the latest benchmark Molweni, whose results\nindicate that training with complementary tasks indeed benefits not only QA\ntask, but also DP task itself. We further find that the joint model is\ndistinctly stronger when handling longer dialogues which again verifies the\nnecessity of DP in the related MRC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuchen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Autism Spectrum Disorders with Machine Learning Models Using Speech Transcripts. (arXiv:2110.03281v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03281","description":"<p>Autism spectrum disorder (ASD) can be defined as a neurodevelopmental\ndisorder that affects how children interact, communicate and socialize with\nothers. This disorder can occur in a broad spectrum of symptoms, with varying\neffects and severity. While there is no permanent cure for ASD, early detection\nand proactive treatment can substantially improve the lives of many children.\nCurrent methods to accurately diagnose ASD are invasive, time-consuming, and\ntedious. They can also be subjective perspectives of a number of clinicians\ninvolved, including pediatricians, speech pathologists, psychologists, and\npsychiatrists. New technologies are rapidly emerging that include machine\nlearning models using speech, computer vision from facial, retinal, and brain\nMRI images of patients to accurately and timely detect this disorder. Our\nresearch focuses on computational linguistics and machine learning using speech\ndata from TalkBank, the world's largest spoken language database. We used data\nof both ASD and Typical Development (TD) in children from TalkBank to develop\nmachine learning models to accurately predict ASD. More than 50 features were\nused from specifically two datasets in TalkBank to run our experiments using\nfive different classifiers. Logistic Regression and Random Forest models were\nfound to be the most effective for each of these two main datasets, with an\naccuracy of 0.75. These experiments confirm that while significant\nopportunities exist for improving the accuracy, machine learning models can\nreliably predict ASD status in children for effective diagnosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_V/0/1/0/all/0/1\">Vikram Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assaf_R/0/1/0/all/0/1\">Rida Assaf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Supermask Pruning: Learning to Prune Image Captioning Models. (arXiv:2110.03298v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03298","description":"<p>With the advancement of deep models, research work on image captioning has\nled to a remarkable gain in raw performance over the last decade, along with\nincreasing model complexity and computational cost. However, surprisingly works\non compression of deep networks for image captioning task has received little\nto no attention. For the first time in image captioning research, we provide an\nextensive comparison of various unstructured weight pruning methods on three\ndifferent popular image captioning architectures, namely Soft-Attention,\nUp-Down and Object Relation Transformer. Following this, we propose a novel\nend-to-end weight pruning method that performs gradual sparsification based on\nweight sensitivity to the training loss. The pruning schemes are then extended\nwith encoder pruning, where we show that conducting both decoder pruning and\ntraining simultaneously prior to the encoder pruning provides good overall\nperformance. Empirically, we show that an 80% to 95% sparse network (up to 75%\nreduction in model size) can either match or outperform its dense counterpart.\nThe code and pre-trained models for Up-Down and Object Relation Transformer\nthat are capable of achieving CIDEr scores &gt;120 on the MS-COCO dataset but with\nonly 8.7 MB and 14.5 MB in model size (size reduction of 96% and 94%\nrespectively against dense versions) are publicly available at\nhttps://github.com/jiahuei/sparse-image-captioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jia Huei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Chee Seng Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuah_J/0/1/0/all/0/1\">Joon Huang Chuah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Latent Holes of VAEs for Text Generation. (arXiv:2110.03318v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03318","description":"<p>In this paper, we provide the first focused study on the discontinuities\n(aka. holes) in the latent space of Variational Auto-Encoders (VAEs), a\nphenomenon which has been shown to have a detrimental effect on model capacity.\nWhen investigating latent holes, existing works are exclusively centred around\nthe encoder network and they merely explore the existence of holes. We tackle\nthese limitations by proposing a highly efficient Tree-based Decoder-Centric\n(TDC) algorithm for latent hole identification, with a focal point on the text\ndomain. In contrast to past studies, our approach pays attention to the decoder\nnetwork, as a decoder has a direct impact on the model's output quality.\nFurthermore, we provide, for the first time, in-depth empirical analysis of the\nlatent hole phenomenon, investigating several important aspects such as how the\nholes impact VAE algorithms' performance on text generation, and how the holes\nare distributed in the latent space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruizhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xutan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Logic-Based Framework for Natural Language Inference in Dutch. (arXiv:2110.03323v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03323","description":"<p>At its core, the system is powered by two ${\\lambda}$-calculi, used as\nsyntactic and semantic theories, respectively. Sentences are first converted to\nsyntactic proofs and terms of the linear ${\\lambda}$-calculus using a choice of\ntwo parsers: an Alpino-based pipeline, and Neural Proof Nets. The syntactic\nterms are then converted to semantic terms of the simply typed\n${\\lambda}$-calculus, via a set of hand designed type- and term-level\ntransformations. Pairs of semantic terms are then fed to an automated theorem\nprover for natural logic which reasons with them while using lexical relations\nfound in the Open Dutch WordNet. We evaluate the reasoning pipeline on the\nrecently created Dutch natural language inference dataset, and achieve\npromising results, remaining only within a $1.1-3.2{\\%}$ performance margin to\nstrong neural baselines. To the best of our knowledge, the reasoning pipeline\nis the first logic-based system for Dutch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abzianidze_L/0/1/0/all/0/1\">Lasha Abzianidze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kogkalidis_K/0/1/0/all/0/1\">Konstantinos Kogkalidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back from the future: bidirectional CTC decoding using future information in speech recognition. (arXiv:2110.03326v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03326","description":"<p>In this paper, we propose a simple but effective method to decode the output\nof Connectionist Temporal Classifier (CTC) model using a bi-directional neural\nlanguage model. The bidirectional language model uses the future as well as the\npast information in order to predict the next output in the sequence. The\nproposed method based on bi-directional beam search takes advantage of the CTC\ngreedy decoding output to represent the noisy future information. Experiments\non the Librispeechdataset demonstrate the superiority of our proposed method\ncompared to baselines using unidirectional decoding. In particular, the boost\ninaccuracy is most apparent at the start of a sequence which is the most\nerroneous part for existing systems based on unidirectional decoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_N/0/1/0/all/0/1\">Namkyu Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Geonmin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Han-Gyu Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Language Learning for Entity Matching. (arXiv:2110.03338v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03338","description":"<p>Transformer-based matching methods have significantly moved the\nstate-of-the-art for less-structured matching tasks involving textual entity\ndescriptions. In order to excel on these tasks, Transformer-based matching\nmethods require a decent amount of training pairs. Providing enough training\ndata can be challenging, especially if a matcher for non-English entity\ndescriptions should be learned. This paper explores along the use case of\nmatching product offers from different e-shops to which extent it is possible\nto improve the performance of Transformer-based entity matchers by\ncomplementing a small set of training pairs in the target language, German in\nour case, with a larger set of English-language training pairs. Our experiments\nusing different Transformers show that extending the German set with English\npairs is always beneficial. The impact of adding the English pairs is\nespecially high in low-resource settings in which only a rather small number of\nnon-English pairs is available. As it is often possible to automatically gather\nEnglish training pairs from the Web by using schema.org annotations, our\nresults could proof relevant for many product matching scenarios targeting\nlow-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peeters_R/0/1/0/all/0/1\">Ralph Peeters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bizer_C/0/1/0/all/0/1\">Christian Bizer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisualTTS: TTS with Accurate Lip-Speech Synchronization for Automatic Voice Over. (arXiv:2110.03342v1 [eess.AS])","link":"http://arxiv.org/abs/2110.03342","description":"<p>In this paper, we formulate a novel task to synthesize speech in sync with a\nsilent pre-recorded video, denoted as automatic voice over (AVO). Unlike\ntraditional speech synthesis, AVO seeks to generate not only human-sounding\nspeech, but also perfect lip-speech synchronization. A natural solution to AVO\nis to condition the speech rendering on the temporal progression of lip\nsequence in the video. We propose a novel text-to-speech model that is\nconditioned on visual input, named VisualTTS, for accurate lip-speech\nsynchronization. The proposed VisualTTS adopts two novel mechanisms that are 1)\ntextual-visual attention, and 2) visual fusion strategy during acoustic\ndecoding, which both contribute to forming accurate alignment between the input\ntext content and lip motion in input lip sequence. Experimental results show\nthat VisualTTS achieves accurate lip-speech synchronization and outperforms all\nbaseline systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_J/0/1/0/all/0/1\">Junchen Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sisman_B/0/1/0/all/0/1\">Berrak Sisman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noisy Text Data: Achilles' Heel of popular transformer based NLP models. (arXiv:2110.03353v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03353","description":"<p>In the last few years, the ML community has created a number of new NLP\nmodels based on transformer architecture. These models have shown great\nperformance for various NLP tasks on benchmark datasets, often surpassing SOTA\nresults. Buoyed with this success, one often finds industry practitioners\nactively experimenting with fine-tuning these models to build NLP applications\nfor industry use cases. However, for most datasets that are used by\npractitioners to build industrial NLP applications, it is hard to guarantee the\npresence of any noise in the data. While most transformer based NLP models have\nperformed exceedingly well in transferring the learnings from one dataset to\nanother, it remains unclear how these models perform when fine-tuned on noisy\ntext. We address the open question by Kumar et al. (2020) to explore the\nsensitivity of popular transformer based NLP models to noise in the text data.\nWe continue working with the noise as defined by them -- spelling mistakes &amp;\ntypos (which are the most commonly occurring noise). We show (via experimental\nresults) that these models perform badly on most common NLP tasks namely text\nclassification, textual similarity, NER, question answering, text summarization\non benchmark datasets. We further show that as the noise in data increases, the\nperformance degrades. Our findings suggest that one must be vary of the\npresence of noise in their datasets while fine-tuning popular transformer based\nNLP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bagla_K/0/1/0/all/0/1\">Kartikay Bagla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ankit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shivam Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anuj Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition. (arXiv:2110.03370v1 [cs.SD])","link":"http://arxiv.org/abs/2110.03370","description":"<p>In this paper, we present WenetSpeech, a multi-domain Mandarin corpus\nconsisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly\nlabeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in\ntotal. We collect the data from YouTube and Podcast, which covers a variety of\nspeaking styles, scenarios, domains, topics, and noisy conditions. An optical\ncharacter recognition (OCR) based method is introduced to generate the\naudio/text segmentation candidates for the YouTube data on its corresponding\nvideo captions, while a high-quality ASR transcription system is used to\ngenerate audio/text pair candidates for the Podcast data. Then we propose a\nnovel end-to-end label error detection approach to further validate and filter\nthe candidates. We also provide three manually labelled high-quality test sets\nalong with WenetSpeech for evaluation -- Dev for cross-validation purpose in\ntraining, Test_Net, collected from Internet for matched test, and\nTest\\_Meeting, recorded from real meetings for more challenging mismatched\ntest. Baseline systems trained with WenetSpeech are provided for three popular\nspeech recognition toolkits, namely Kaldi, ESPnet, and WeNet, and recognition\nresults on the three test sets are also provided as benchmarks. To the best of\nour knowledge, WenetSpeech is the current largest open-sourced Mandarin speech\ncorpus with transcriptions, which benefits research on production-level speech\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1\">Hang Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Q/0/1/0/all/0/1\">Qijie Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_H/0/1/0/all/0/1\">Hui Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chenchen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhendong Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled dimensionality reduction for noise-robust speaker diarisation. (arXiv:2110.03380v1 [cs.SD])","link":"http://arxiv.org/abs/2110.03380","description":"<p>The objective of this work is to train noise-robust speaker embeddings for\nspeaker diarisation. Speaker embeddings play a crucial role in the performance\nof diarisation systems, but they often capture spurious information such as\nnoise and reverberation, adversely affecting performance. Our previous work\nhave proposed an auto-encoder-based dimensionality reduction module to help\nremove the spurious information. However, they do not explicitly separate such\ninformation and have also been found to be sensitive to hyperparameter values.\nTo this end, we propose two contributions to overcome these issues: (i) a novel\ndimensionality reduction framework that can disentangle spurious information\nfrom the speaker embeddings; (ii) the use of a speech/non-speech indicator to\nprevent the speaker code from learning from the background noise. Through a\nrange of experiments conducted on four different datasets, our approach\nconsistently demonstrates the state-of-the-art performance among models that do\nnot adopt ensembles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">You Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_H/0/1/0/all/0/1\">Hee-Soo Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Jee-weon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Youngki Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bong-Jin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Joon Son Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beam Search with Bidirectional Strategies for Neural Response Generation. (arXiv:2110.03389v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03389","description":"<p>Sequence-to-sequence neural networks have been widely used in language-based\napplications as they have flexible capabilities to learn various language\nmodels. However, when seeking for the optimal language response through trained\nneural networks, current existing approaches such as beam-search decoder\nstrategies are still not able reaching to promising performances. Instead of\ndeveloping various decoder strategies based on a \"regular sentence order\"\nneural network (a trained model by outputting sentences from left-to-right\norder), we leveraged \"reverse\" order as additional language model (a trained\nmodel by outputting sentences from right-to-left order) which can provide\ndifferent perspectives for the path finding problems. In this paper, we propose\nbidirectional strategies in searching paths by combining two networks\n(left-to-right and right-to-left language models) making a bidirectional beam\nsearch possible. Besides, our solution allows us using any similarity measure\nin our sentence selection criterion. Our approaches demonstrate better\nperformance compared to the unidirectional beam search strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chouchang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varni_G/0/1/0/all/0/1\">Giovanna Varni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chlo&#xe9; Clavel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Attention always needed? A Case Study on Language Identification from Speech. (arXiv:2110.03427v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03427","description":"<p>Language Identification (LID), a recommended initial step to Automatic Speech\nRecognition (ASR), is used to detect a spoken language from audio specimens. In\nstate-of-the-art systems capable of multilingual speech processing, however,\nusers have to explicitly set one or more languages before using them. LID,\ntherefore, plays a very important role in situations where ASR based systems\ncannot parse the uttered language in multilingual contexts causing failure in\nspeech recognition. We propose an attention based convolutional recurrent\nneural network (CRNN with Attention) that works on Mel-frequency Cepstral\nCoefficient (MFCC) features of audio specimens. Additionally, we reproduce some\nstate-of-the-art approaches, namely Convolutional Neural Network (CNN) and\nConvolutional Recurrent Neural Network (CRNN), and compare them to our proposed\nmethod. We performed extensive evaluation on thirteen different Indian\nlanguages and our model achieves classification accuracy over 98%. Our LID\nmodel is robust to noise and provides 91.2% accuracy in a noisy scenario. The\nproposed model is easily extensible to new languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mandal_A/0/1/0/all/0/1\">Atanu Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Santanu Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_I/0/1/0/all/0/1\">Indranil Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_M/0/1/0/all/0/1\">Mahidas Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naskar_S/0/1/0/all/0/1\">Sudip Kumar Naskar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretrained Language Models are Symbolic Mathematics Solvers too!. (arXiv:2110.03501v1 [stat.ML])","link":"http://arxiv.org/abs/2110.03501","description":"<p>Solving symbolic mathematics has always been of in the arena of human\ningenuity that needs compositional reasoning and recurrence. However, recent\nstudies have shown that large-scale language models such as transformers are\nuniversal and surprisingly can be trained as a sequence-to-sequence task to\nsolve complex mathematical equations. These large transformer models need\nhumongous amounts of training data to generalize to unseen symbolic mathematics\nproblems. In this paper, we present a sample efficient way of solving the\nsymbolic tasks by first pretraining the transformer model with language\ntranslation and then fine-tuning the pretrained transformer model to solve the\ndownstream task of symbolic mathematics. We achieve comparable accuracy on the\nintegration task with our pretrained model while using around $1.5$ orders of\nmagnitude less number of training samples with respect to the state-of-the-art\ndeep learning for symbolic mathematics. The test accuracy on differential\nequation tasks is considerably lower comparing with integration as they need\nhigher order recursions that are not present in language translations. We\npretrain our model with different pairs of language translations. Our results\nshow language bias in solving symbolic mathematics tasks. Finally, we study the\nrobustness of the fine-tuned model on symbolic math tasks against distribution\nshift, and our approach generalizes better in distribution shift scenarios for\nthe function integration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Noorbakhsh_K/0/1/0/all/0/1\">Kimia Noorbakhsh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sulaiman_M/0/1/0/all/0/1\">Modar Sulaiman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sharifi_M/0/1/0/all/0/1\">Mahdi Sharifi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Roy_K/0/1/0/all/0/1\">Kallol Roy</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jamshidi_P/0/1/0/all/0/1\">Pooyan Jamshidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mandarin-English Code-switching Speech Recognition with Self-supervised Speech Representation Models. (arXiv:2110.03504v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03504","description":"<p>Code-switching (CS) is common in daily conversations where more than one\nlanguage is used within a sentence. The difficulties of CS speech recognition\nlie in alternating languages and the lack of transcribed data. Therefore, this\npaper uses the recently successful self-supervised learning (SSL) methods to\nleverage many unlabeled speech data without CS. We show that hidden\nrepresentations of SSL models offer frame-level language identity even if the\nmodels are trained with English speech only. Jointly training CTC and language\nidentification modules with self-supervised speech representations improves CS\nspeech recognition performance. Furthermore, using multilingual speech data for\npre-training obtains the best CS speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tseng_L/0/1/0/all/0/1\">Liang-Hsuan Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yu-Kuan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Single-Trial Representational Similarity Analysis with EEG to track semantic similarity in emotional word processing. (arXiv:2110.03529v1 [q-bio.NC])","link":"http://arxiv.org/abs/2110.03529","description":"<p>Electroencephalography (EEG) is a powerful non-invasive brain imaging\ntechnique with a high temporal resolution that has seen extensive use across\nmultiple areas of cognitive science research. This thesis adapts\nrepresentational similarity analysis (RSA) to single-trial EEG datasets and\nintroduces its principles to EEG researchers unfamiliar with multivariate\nanalyses. We have two separate aims: 1. we want to explore the effectiveness of\nsingle-trial RSA on EEG datasets; 2. we want to utilize single-trial RSA and\ncomputational semantic models to investigate the role of semantic meaning in\nemotional word processing. We report two primary findings: 1. single-trial RSA\non EEG datasets can produce meaningful and interpretable results given a high\nnumber of trials and subjects; 2. single-trial RSA reveals that emotional\nprocessing in the 500-800ms time window is associated with additional semantic\nanalysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Cheng_F/0/1/0/all/0/1\">Feng Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mRAT-SQL+GAP:A Portuguese Text-to-SQL Transformer. (arXiv:2110.03546v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03546","description":"<p>The translation of natural language questions to SQL queries has attracted\ngrowing attention, in particular in connection with transformers and similar\nlanguage models. A large number of techniques are geared towards the English\nlanguage; in this work, we thus investigated translation to SQL when input\nquestions are given in the Portuguese language. To do so, we properly adapted\nstate-of-the-art tools and resources. We changed the RAT-SQL+GAP system by\nrelying on a multilingual BART model (we report tests with other language\nmodels), and we produced a translated version of the Spider dataset. Our\nexperiments expose interesting phenomena that arise when non-English languages\nare targeted; in particular, it is better to train with original and translated\ntraining datasets together, even if a single target language is desired. This\nmultilingual BART model fine-tuned with a double-size training dataset (English\nand Portuguese) achieved 83% of the baseline, making inferences for the\nPortuguese test dataset. This investigation can help other researchers to\nproduce results in Machine Learning in a language different from English. Our\nmultilingual ready version of RAT-SQL+GAP and the data are available,\nopen-sourced as mRAT-SQL+GAP at: https://github.com/C4AI/gap-text2sql\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jose_M/0/1/0/all/0/1\">Marcelo Archanjo Jos&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cozman_F/0/1/0/all/0/1\">Fabio Gagliardi Cozman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Magic dust for cross-lingual adaptation of monolingual wav2vec-2.0. (arXiv:2110.03560v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03560","description":"<p>We propose a simple and effective cross-lingual transfer learning method to\nadapt monolingual wav2vec-2.0 models for Automatic Speech Recognition (ASR) in\nresource-scarce languages. We show that a monolingual wav2vec-2.0 is a good\nfew-shot ASR learner in several languages. We improve its performance further\nvia several iterations of Dropout Uncertainty-Driven Self-Training (DUST) by\nusing a moderate-sized unlabeled speech dataset in the target language. A key\nfinding of this work is that the adapted monolingual wav2vec-2.0 achieves\nsimilar performance as the topline multilingual XLSR model, which is trained on\nfifty-three languages, on the target language ASR task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khurana_S/0/1/0/all/0/1\">Sameer Khurana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurent_A/0/1/0/all/0/1\">Antoine Laurent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GeSERA: General-domain Summary Evaluation by Relevance Analysis. (arXiv:2110.03567v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03567","description":"<p>We present GeSERA, an open-source improved version of SERA for evaluating\nautomatic extractive and abstractive summaries from the general domain. SERA is\nbased on a search engine that compares candidate and reference summaries\n(called queries) against an information retrieval document base (called index).\nSERA was originally designed for the biomedical domain only, where it showed a\nbetter correlation with manual methods than the widely used lexical-based ROUGE\nmethod. In this paper, we take out SERA from the biomedical domain to the\ngeneral one by adapting its content-based method to successfully evaluate\nsummaries from the general domain. First, we improve the query reformulation\nstrategy with POS Tags analysis of general-domain corpora. Second, we replace\nthe biomedical index used in SERA with two article collections from AQUAINT-2\nand Wikipedia. We conduct experiments with TAC2008, TAC2009, and CNNDM\ndatasets. Results show that, in most cases, GeSERA achieves higher correlations\nwith manual evaluation methods than SERA, while it reduces its gap with ROUGE\nfor general-domain summary evaluation. GeSERA even surpasses ROUGE in two cases\nof TAC2009. Finally, we conduct extensive experiments and provide a\ncomprehensive study of the impact of human annotators and the index size on\nsummary evaluation with SERA and GeSERA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Espejel_J/0/1/0/all/0/1\">Jessica L&#xf3;pez Espejel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalendar_G/0/1/0/all/0/1\">Ga&#xeb;l de Chalendar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flores_J/0/1/0/all/0/1\">Jorge Garcia Flores</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charnois_T/0/1/0/all/0/1\">Thierry Charnois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_I/0/1/0/all/0/1\">Ivan Vladimir Meza Ruiz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridge to Target Domain by Prototypical Contrastive Learning and Label Confusion: Re-explore Zero-Shot Learning for Slot Filling. (arXiv:2110.03572v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03572","description":"<p>Zero-shot cross-domain slot filling alleviates the data dependence in the\ncase of data scarcity in the target domain, which has aroused extensive\nresearch. However, as most of the existing methods do not achieve effective\nknowledge transfer to the target domain, they just fit the distribution of the\nseen slot and show poor performance on unseen slot in the target domain. To\nsolve this, we propose a novel approach based on prototypical contrastive\nlearning with a dynamic label confusion strategy for zero-shot slot filling.\nThe prototypical contrastive learning aims to reconstruct the semantic\nconstraints of labels, and we introduce the label confusion strategy to\nestablish the label dependence between the source domains and the target domain\non-the-fly. Experimental results show that our model achieves significant\nimprovement on the unseen slots, while also set new state-of-the-arts on slot\nfilling task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Keqing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yuanmeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiran Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applying Phonological Features in Multilingual Text-To-Speech. (arXiv:2110.03609v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03609","description":"<p>This study investigates whether phonological features can be applied in\ntext-to-speech systems to generate native and non-native speech. We present a\nmapping between ARPABET/pinyin-&gt;SAMPA/SAMPA-SC-&gt;phonological features in this\npaper, and tested whether native, non-native, and code-switched speech could be\nsuccessfully generated using this mapping. We ran two experiments, one with a\nsmall dataset and one with a larger dataset. The results proved that\nphonological features can be a feasible input system, although it needs further\ninvestigation to improve model performance. The accented output generated by\nthe TTS models also helps with understanding human second language acquisition\nprocesses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Huinan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiewen Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Retriever-Ranker for dense text retrieval. (arXiv:2110.03611v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03611","description":"<p>Current dense text retrieval models face two typical challenges. First, it\nadopts a siamese dual-encoder architecture to encode query and document\nindependently for fast indexing and searching, whereas neglecting the\nfiner-grained term-wise interactions. This results in a sub-optimal recall\nperformance. Second, it highly relies on a negative sampling technique to build\nup the negative documents in its contrastive loss. To address these challenges,\nwe present Adversarial Retriever-Ranker (AR2), which consists of a dual-encoder\nretriever plus a cross-encoder ranker. The two models are jointly optimized\naccording to a minimax adversarial objective: the retriever learns to retrieve\nnegative documents to cheat the ranker, while the ranker learns to rank a\ncollection of candidates including both the ground-truth and the retrieved\nones, as well as providing progressive direct feedback to the dual-encoder\nretriever. Through this adversarial game, the retriever gradually produces\nharder negative documents to train a better ranker, whereas the cross-encoder\nranker provides progressive feedback to improve retriever. We evaluate AR2 on\nthree benchmarks. Experimental results show that AR2 consistently and\nsignificantly outperforms existing dense retriever methods and achieves new\nstate-of-the-art results on all of them. This includes the improvements on\nNatural Questions R@5 to 77.9%(+2.1%), TriviaQA R@5 to 78.2%(+1.4), and\nMS-MARCO MRR@10 to 39.5%(+1.3%). We will make our code, models, and data\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jiancheng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Direction of Data Collection Matters: Implications of Causal and Anticausal Learning in NLP. (arXiv:2110.03618v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03618","description":"<p>The principle of independent causal mechanisms (ICM) states that generative\nprocesses of real world data consist of independent modules which do not\ninfluence or inform each other. While this idea has led to fruitful\ndevelopments in the field of causal inference, it is not widely-known in the\nNLP community. In this work, we argue that the causal direction of the data\ncollection process bears nontrivial implications that can explain a number of\npublished NLP findings, such as differences in semi-supervised learning (SSL)\nand domain adaptation (DA) performance across different settings. We categorize\ncommon NLP tasks according to their causal direction and empirically assay the\nvalidity of the ICM principle for text data using minimum description length.\nWe conduct an extensive meta-analysis of over 100 published SSL and 30 DA\nstudies, and find that the results are consistent with our expectations based\non causal insights. This work presents the first attempt to analyze the ICM\nprinciple in NLP, and provides constructive suggestions for future modeling\nchoices. Code available at https://github.com/zhijing-jin/icm4nlp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingwei Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaidhya_T/0/1/0/all/0/1\">Tejas Vaidhya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaushal_A/0/1/0/all/0/1\">Ayush Kaushal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoelkopf_B/0/1/0/all/0/1\">Bernhard Schoelkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying the Suicidal Tendency on Social Media: A Survey. (arXiv:2110.03663v1 [cs.SI])","link":"http://arxiv.org/abs/2110.03663","description":"<p>Amid lockdown period more people express their feelings over social media\nplatforms due to closed third-place and academic researchers have witnessed\nstrong associations between the mental healthcare and social media posts. The\nstress for a brief period may lead to clinical depressions and the long-lasting\ntraits of prevailing depressions can be life threatening with suicidal ideation\nas the possible outcome. The increasing concern towards the rise in number of\nsuicide cases is because it is one of the leading cause of premature but\npreventable death. Recent studies have shown that mining social media data has\nhelped in quantifying the suicidal tendency of users at risk. This potential\nmanuscript elucidates the taxonomy of mental healthcare and highlights some\nrecent attempts in examining the potential of quantifying suicidal tendency on\nsocial media data. This manuscript presents the classification of heterogeneous\nfeatures from social media data and handling feature vector representation.\nAiming to identify the new research directions and advances in the development\nof Machine Learning (ML) and Deep Learning (DL) based models, a quantitative\nsynthesis and a qualitative review was carried out with corpus of over 77\npotential research articles related to stress, depression and suicide risk from\n2013 to 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1\">Muskan Garg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TBCOV: Two Billion Multilingual COVID-19 Tweets with Sentiment, Entity, Geo, and Gender Labels. (arXiv:2110.03664v1 [cs.SI])","link":"http://arxiv.org/abs/2110.03664","description":"<p>The widespread usage of social networks during mass convergence events, such\nas health emergencies and disease outbreaks, provides instant access to\ncitizen-generated data that carry rich information about public opinions,\nsentiments, urgent needs, and situational reports. Such information can help\nauthorities understand the emergent situation and react accordingly. Moreover,\nsocial media plays a vital role in tackling misinformation and disinformation.\nThis work presents TBCOV, a large-scale Twitter dataset comprising more than\ntwo billion multilingual tweets related to the COVID-19 pandemic collected\nworldwide over a continuous period of more than one year. More importantly,\nseveral state-of-the-art deep learning models are used to enrich the data with\nimportant attributes, including sentiment labels, named-entities (e.g.,\nmentions of persons, organizations, locations), user types, and gender\ninformation. Last but not least, a geotagging method is proposed to assign\ncountry, state, county, and city information to tweets, enabling a myriad of\ndata analysis tasks to understand real-world issues. Our sentiment and trend\nanalyses reveal interesting insights and confirm TBCOV's broad coverage of\nimportant topics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1\">Muhammad Imran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qazi_U/0/1/0/all/0/1\">Umair Qazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1\">Ferda Ofli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention. (arXiv:2006.03654v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2006.03654","description":"<p>Recent progress in pre-trained neural language models has significantly\nimproved the performance of many natural language processing (NLP) tasks. In\nthis paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT\nwith disentangled attention) that improves the BERT and RoBERTa models using\ntwo novel techniques. The first is the disentangled attention mechanism, where\neach word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed\nusing disentangled matrices on their contents and relative positions,\nrespectively. Second, an enhanced mask decoder is used to incorporate absolute\npositions in the decoding layer to predict the masked tokens in model\npre-training. In addition, a new virtual adversarial training method is used\nfor fine-tuning to improve models' generalization. We show that these\ntechniques significantly improve the efficiency of model pre-training and the\nperformance of both natural language understanding (NLU) and natural langauge\ngeneration (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model\ntrained on half of the training data performs consistently better on a wide\nrange of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%),\non SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).\nNotably, we scale up DeBERTa by training a larger version that consists of 48\nTransform layers with 1.5 billion parameters. The significant performance boost\nmakes the single DeBERTa model surpass the human performance on the SuperGLUE\nbenchmark (Wang et al., 2019a) for the first time in terms of macro-average\nscore (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the\nSuperGLUE leaderboard as of January 6, 2021, out performing the human baseline\nby a decent margin (90.3 versus 89.8).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arabic aspect based sentiment analysis using bidirectional GRU based models. (arXiv:2101.10539v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.10539","description":"<p>Aspect-based Sentiment analysis (ABSA) accomplishes a fine-grained analysis\nthat defines the aspects of a given document or sentence and the sentiments\nconveyed regarding each aspect. This level of analysis is the most detailed\nversion that is capable of exploring the nuanced viewpoints of the reviews. The\nbulk of study in ABSA focuses on English with very little work available in\nArabic. Most previous work in Arabic has been based on regular methods of\nmachine learning that mainly depends on a group of rare resources and tools for\nanalyzing and processing Arabic content such as lexicons, but the lack of those\nresources presents another challenge. In order to address these challenges,\nDeep Learning (DL)-based methods are proposed using two models based on Gated\nRecurrent Units (GRU) neural networks for ABSA. The first is a DL model that\ntakes advantage of word and character representations by combining\nbidirectional GRU, Convolutional Neural Network (CNN), and Conditional Random\nField (CRF) making up the (BGRU-CNN-CRF) model to extract the main opinionated\naspects (OTE). The second is an interactive attention network based on\nbidirectional GRU (IAN-BGRU) to identify sentiment polarity toward extracted\naspects. We evaluated our models using the benchmarked Arabic hotel reviews\ndataset. The results indicate that the proposed methods are better than\nbaseline research on both tasks having 39.7% enhancement in F1-score for\nopinion target extraction (T2) and 7.58% in accuracy for aspect-based sentiment\npolarity classification (T3). Achieving F1 score of 70.67% for T2, and accuracy\nof 83.98% for T3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelgwad_M/0/1/0/all/0/1\">Mohammed M.Abdelgwad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soliman_T/0/1/0/all/0/1\">Taysir Hassan A Soliman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taloba_A/0/1/0/all/0/1\">Ahmed I.Taloba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farghaly_M/0/1/0/all/0/1\">Mohamed Fawzy Farghaly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation. (arXiv:2104.07412v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07412","description":"<p>Machine learning has brought striking advances in multilingual natural\nlanguage processing capabilities over the past year. For example, the latest\ntechniques have improved the state-of-the-art performance on the XTREME\nmultilingual benchmark by more than 13 points. While a sizeable gap to\nhuman-level performance remains, improvements have been easier to achieve in\nsome tasks than in others. This paper analyzes the current state of\ncross-lingual transfer learning and summarizes some lessons learned. In order\nto catalyze meaningful progress, we extend XTREME to XTREME-R, which consists\nof an improved set of ten natural language understanding tasks, including\nchallenging language-agnostic retrieval tasks, and covers 50 typologically\ndiverse languages. In addition, we provide a massively multilingual diagnostic\nsuite (MultiCheckList) and fine-grained multi-dataset evaluation capabilities\nthrough an interactive public leaderboard to gain a better understanding of\nsuch models. The leaderboard and code for XTREME-R will be made available at\nhttps://sites.research.google/xtreme and\nhttps://github.com/google-research/xtreme respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botha_J/0/1/0/all/0/1\">Jan Botha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddhant_A/0/1/0/all/0/1\">Aditya Siddhant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrette_D/0/1/0/all/0/1\">Dan Garrette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Melvin Johnson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Adaptive Document-Level Neural Machine Translation. (arXiv:2104.08259v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08259","description":"<p>Most existing document-level neural machine translation (NMT) models leverage\na fixed number of the previous or all global source sentences to handle the\ncontext-independent problem in standard NMT. However, the translating of each\nsource sentence benefits from various sizes of context, and inappropriate\ncontext may harm the translation performance. In this work, we introduce a\ndata-adaptive method that enables the model to adopt the necessary and useful\ncontext. Specifically, we introduce a light predictor into two document-level\ntranslation models to select the explicit context. Experiments demonstrate the\nproposed approach can significantly improve the performance over the previous\nmethods with a gain up to 1.99 BLEU points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linlin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enriching a Model's Notion of Belief using a Persistent Memory. (arXiv:2104.08401v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08401","description":"<p>Although pretrained language models (PTLMs) have been shown to contain\nsignificant amounts of world knowledge, they can still produce inconsistent\nanswers to questions when probed, even after using specialized training\ntechniques to reduce inconsistency. As a result, it can be hard to identify\nwhat the model actually \"believes\" about the world. Our goal is to reduce this\nproblem, so systems are more globally consistent and accurate in their answers.\nOur approach is to add a memory component -- a BeliefBank -- that records a\nmodel's answers, and two mechanisms that use it to improve consistency among\nbeliefs. First, a reasoning component -- a weighted SAT solver -- improves\nconsistency by flipping answers that significantly clash with others. Second, a\nfeedback component re-queries the model but using known beliefs as context. We\nshow that, in a controlled experimental setting, these two mechanisms improve\nboth accuracy and consistency. This is significant as it is a first step\ntowards endowing models with an evolving memory, allowing them to construct a\nmore coherent picture of the world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kassner_N/0/1/0/all/0/1\">Nora Kassner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1\">Oyvind Tafjord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Schutze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Speech Recognition. (arXiv:2105.11084v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.11084","description":"<p>Despite rapid progress in the recent past, current speech recognition systems\nstill require labeled training data which limits this technology to a small\nfraction of the languages spoken around the globe. This paper describes\nwav2vec-U, short for wav2vec Unsupervised, a method to train speech recognition\nmodels without any labeled data. We leverage self-supervised speech\nrepresentations to segment unlabeled audio and learn a mapping from these\nrepresentations to phonemes via adversarial training. The right representations\nare key to the success of our method. Compared to the best previous\nunsupervised work, wav2vec-U reduces the phoneme error rate on the TIMIT\nbenchmark from 26.1 to 11.3. On the larger English Librispeech benchmark,\nwav2vec-U achieves a word error rate of 5.9 on test-other, rivaling some of the\nbest published systems trained on 960 hours of labeled data from only two years\nago. We also experiment on nine other languages, including low-resource\nlanguages such as Kyrgyz, Swahili and Tatar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1\">Alexis Conneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Language Model for Efficient Linguistic Steganalysis. (arXiv:2107.12168v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.12168","description":"<p>Recent advances in linguistic steganalysis have successively applied CNN,\nRNN, GNN and other efficient deep models for detecting secret information in\ngenerative texts. These methods tend to seek stronger feature extractors to\nachieve higher steganalysis effects. However, we have found through experiments\nthat there actually exists significant difference between automatically\ngenerated stego texts and carrier texts in terms of the conditional probability\ndistribution of individual words. Such kind of difference can be naturally\ncaptured by the language model used for generating stego texts. Through further\nexperiments, we conclude that this ability can be transplanted to a text\nclassifier by pre-training and fine-tuning to improve the detection\nperformance. Motivated by this insight, we propose two methods for efficient\nlinguistic steganalysis. One is to pre-train a language model based on RNN, and\nthe other is to pre-train a sequence autoencoder. The results indicate that the\ntwo methods have different degrees of performance gain compared to the randomly\ninitialized RNN, and the convergence speed is significantly accelerated.\nMoreover, our methods have achieved the state-of-the-art detection results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_B/0/1/0/all/0/1\">Biao Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanzhou Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1\">Guorui Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Similar Language Translation With Transfer Learning. (arXiv:2108.03533v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2108.03533","description":"<p>We investigate transfer learning based on pre-trained neural machine\ntranslation models to translate between (low-resource) similar languages. This\nwork is part of our contribution to the WMT 2021 Similar Languages Translation\nShared Task where we submitted models for different language pairs, including\nFrench-Bambara, Spanish-Catalan, and Spanish-Portuguese in both directions. Our\nmodels for Catalan-Spanish ($82.79$ BLEU) and Portuguese-Spanish ($87.11$ BLEU)\nrank top 1 in the official shared task evaluation, and we are the only team to\nsubmit models for the French-Bambara pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adebara_I/0/1/0/all/0/1\">Ife Adebara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Exploration in Quality Filtering of Text Data. (arXiv:2109.00698v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00698","description":"<p>While conventional wisdom suggests that more aggressively filtering data from\nlow-quality sources like Common Crawl always monotonically improves the quality\nof training data, we find that aggressive filtering can in fact lead to a\ndecrease in model quality on a wide array of downstream tasks for a GPT-like\nlanguage model. We speculate that this is because optimizing sufficiently\nstrongly for a proxy metric harms performance on the true objective, suggesting\na need for more robust filtering objectives when attempting to filter more\naggressively. We hope this work leads to detailed analysis of the effects of\ndataset filtering design choices on downstream model performance in future\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Leo Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factorized Neural Transducer for Efficient Language Model Adaptation. (arXiv:2110.01500v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01500","description":"<p>In recent years, end-to-end (E2E) based automatic speech recognition (ASR)\nsystems have achieved great success due to their simplicity and promising\nperformance. Neural Transducer based models are increasingly popular in\nstreaming E2E based ASR systems and have been reported to outperform the\ntraditional hybrid system in some scenarios. However, the joint optimization of\nacoustic model, lexicon and language model in neural Transducer also brings\nabout challenges to utilize pure text for language model adaptation. This\ndrawback might prevent their potential applications in practice. In order to\naddress this issue, in this paper, we propose a novel model, factorized neural\nTransducer, by factorizing the blank and vocabulary prediction, and adopting a\nstandalone language model for the vocabulary prediction. It is expected that\nthis factorization can transfer the improvement of the standalone language\nmodel to the Transducer for speech recognition, which allows various language\nmodel adaptation techniques to be applied. We demonstrate that the proposed\nfactorized neural Transducer yields 15% to 20% WER improvements when\nout-of-domain text data is used for language model adaptation, at the cost of a\nminor degradation in WER on a general test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1\">Sarangarajan Parthasarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rerunning OCR: A Machine Learning Approach to Quality Assessment and Enhancement Prediction. (arXiv:2110.01661v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01661","description":"<p>Iterating with new and improved OCR solutions enforces decisions to be taken\nwhen it comes to targeting the right reprocessing candidates. This especially\napplies when the underlying data collection is of considerable size and rather\ndiverse in terms of fonts, languages, periods of publication and consequently\nOCR quality. This article captures the efforts of the National Library of\nLuxembourg to support those exact decisions. They are crucial in order to\nguarantee low computational overhead and reduced quality degradation risks,\ncombined with a more quantifiable OCR improvement. In particular, this work\nexplains the methodology of the library with respect to text block level\nquality assessment. As an extension of this technique, another contribution\ncomes in the form of a regression model that takes the enhancement potential of\na new OCR engine into account. They both mark promising approaches, especially\nfor cultural institutions dealing with historic data of lower quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1\">Pit Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactively Generating Explanations for Transformer Language Models. (arXiv:2110.02058v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02058","description":"<p>Transformer language models are state-of-the-art in a multitude of NLP tasks.\nDespite these successes, their opaqueness remains problematic. Recent methods\naiming to provide interpretability and explainability to black-box models\nprimarily focus on post-hoc explanations of (sometimes spurious) input-output\ncorrelations. Instead, we emphasize using prototype networks directly\nincorporated into the model architecture and hence explain the reasoning\nprocess behind the network's decisions. Moreover, while our architecture\nperforms on par with several language models, it enables one to learn from user\ninteractions. This not only offers a better understanding of language models\nbut uses human capabilities to incorporate knowledge outside of the rigid range\nof purely data-driven approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1\">Felix Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tauchmann_C/0/1/0/all/0/1\">Christopher Tauchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPAD: An Optimized Policy-based Active Learning Framework for Document Content Analysis. (arXiv:2110.02069v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2110.02069","description":"<p>Documents are central to many business systems, and include forms, reports,\ncontracts, invoices or purchase orders. The information in documents is\ntypically in natural language, but can be organized in various layouts and\nformats. There have been recent spurt of interest in understanding document\ncontent with novel deep learning architectures. However, document understanding\ntasks need dense information annotations, which are costly to scale and\ngeneralize. Several active learning techniques have been proposed to reduce the\noverall budget of annotation while maintaining the performance of the\nunderlying deep learning model. However, most of these techniques work only for\nclassification problems. But content detection is a more complex task, and has\nbeen scarcely explored in active learning literature. In this paper, we propose\n\\textit{OPAD}, a novel framework using reinforcement policy for active learning\nin content detection tasks for documents. The proposed framework learns the\nacquisition function to decide the samples to be selected while optimizing\nperformance metrics that the tasks typically have. Furthermore, we extend to\nweak labelling scenarios to further reduce the cost of annotation\nsignificantly. We propose novel rewards to account for class imbalance and user\nfeedback in the annotation interface, to improve the active learning method. We\nshow superior performance of the proposed \\textit{OPAD} framework for active\nlearning for various tasks related to document understanding like layout\nparsing, object detection and named entity recognition. Ablation studies for\nhuman feedback and class imbalance rewards are presented, along with a\ncomparison of annotation times for different approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1\">Sumit Shekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guda_B/0/1/0/all/0/1\">Bhanu Prakash Reddy Guda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaubey_A/0/1/0/all/0/1\">Ashutosh Chaubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jindal_I/0/1/0/all/0/1\">Ishan Jindal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Avneet Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Contextual Adaptation with Neural Associative Memory for On-Device Personalized Speech Recognition. (arXiv:2110.02220v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.02220","description":"<p>Fast contextual adaptation has shown to be effective in improving Automatic\nSpeech Recognition (ASR) of rare words and when combined with an on-device\npersonalized training, it can yield an even better recognition result. However,\nthe traditional re-scoring approaches based on an external language model is\nprone to diverge during the personalized training. In this work, we introduce a\nmodel-based end-to-end contextual adaptation approach that is decoder-agnostic\nand amenable to on-device personalization. Our on-device simulation experiments\ndemonstrate that the proposed approach outperforms the traditional re-scoring\ntechnique by 12% relative WER and 15.7% entity mention specific F1-score in a\ncontinues personalization scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Munkhdalai_T/0/1/0/all/0/1\">Tsendsuren Munkhdalai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chandorkar_A/0/1/0/all/0/1\">Angad Chandorkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1\">Fan Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chua_M/0/1/0/all/0/1\">Mason Chua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoNet: Pooling Network for Efficient Token Mixing in Long Sequences. (arXiv:2110.02442v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02442","description":"<p>Transformer-based models have achieved great success in various NLP, vision,\nand speech tasks. However, the core of Transformer, the self-attention\nmechanism, has a quadratic time and memory complexity with respect to the\nsequence length, which hinders applications of Transformer-based models to long\nsequences. Many approaches have been proposed to mitigate this problem, such as\nsparse attention mechanisms, low-rank matrix approximations and scalable\nkernels, and token mixing alternatives to self-attention. We propose a novel\nPooling Network (PoNet) for token mixing in long sequences with linear\ncomplexity. We design multi-granularity pooling and pooling fusion to capture\ndifferent levels of contextual information and combine their interactions with\ntokens. On the Long Range Arena benchmark, PoNet significantly outperforms\nTransformer and achieves competitive accuracy, while being only slightly slower\nthan the fastest model, FNet, across all sequence lengths measured on GPUs. We\nalso conduct systematic studies on the transfer learning capability of PoNet\nand observe that PoNet achieves 96.0% of the accuracy of BERT on the GLUE\nbenchmark, outperforming FNet by 4.5% relative. Comprehensive ablation analysis\ndemonstrates effectiveness of the designed multi-granularity pooling and\npooling fusion for token mixing in long sequences and efficacy of the designed\npre-training tasks for PoNet to learn transferable contextualized language\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chao-Hong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siqi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSG@HASOC-Dravidian CodeMixFIRE2021: Pretrained Transformers for Offensive Language Identification in Tanglish. (arXiv:2110.02852v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02852","description":"<p>This paper describes the system submitted to Dravidian-Codemix-HASOC2021:\nHate Speech and Offensive Language Identification in Dravidian Languages\n(Tamil-English and Malayalam-English). This task aims to identify offensive\ncontent in code-mixed comments/posts in Dravidian Languages collected from\nsocial media. Our approach utilizes pooling the last layers of pretrained\ntransformer multilingual BERT for this task which helped us achieve rank nine\non the leaderboard with a weighted average score of 0.61 for the Tamil-English\ndataset in subtask B. After the task deadline, we sampled the dataset uniformly\nand used the MuRIL pretrained model, which helped us achieve a weighted average\nscore of 0.67, the top score in the leaderboard. Furthermore, our approach to\nutilizing the pretrained models helps reuse our models for the same task with a\ndifferent dataset. Our code and models are available in\nhttps://github.com/seanbenhur/tanglish-offensive-language-identification\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benhur_S/0/1/0/all/0/1\">Sean Benhur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivanraju_K/0/1/0/all/0/1\">Kanchana Sivanraju</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-to-Sequence Lexical Normalization with Multilingual Transformers. (arXiv:2110.02869v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02869","description":"<p>Current benchmark tasks for natural language processing contain text that is\nqualitatively different from the text used in informal day to day digital\ncommunication. This discrepancy has led to severe performance degradation of\nstate-of-the-art NLP models when fine-tuned on real-world data. One way to\nresolve this issue is through lexical normalization, which is the process of\ntransforming non-standard text, usually from social media, into a more\nstandardized form. In this work, we propose a sentence-level\nsequence-to-sequence model based on mBART, which frames the problem as a\nmachine translation problem. As the noisy text is a pervasive problem across\nlanguages, not just English, we leverage the multi-lingual pre-training of\nmBART to fine-tune it to our data. While current approaches mainly operate at\nthe word or subword level, we argue that this approach is straightforward from\na technical standpoint and builds upon existing pre-trained transformer\nnetworks. Our results show that while word-level, intrinsic, performance\nevaluation is behind other methods, our model improves performance on\nextrinsic, downstream tasks through normalization compared to models operating\non raw, unprocessed, social media text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1\">Liviu P. Dinu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Learning Canonical Embedding for Non-rigid Shape Matching. (arXiv:2110.02994v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02994","description":"<p>This paper provides a novel framework that learns canonical embeddings for\nnon-rigid shape matching. In contrast to prior work in this direction, our\nframework is trained end-to-end and thus avoids instabilities and constraints\nassociated with the commonly-used Laplace-Beltrami basis or sequential\noptimization schemes. On multiple datasets, we demonstrate that learning self\nsymmetry maps with a deep functional map projects 3D shapes into a low\ndimensional canonical embedding that facilitates non-rigid shape correspondence\nvia a simple nearest neighbor search. Our framework outperforms multiple recent\nlearning based methods on FAUST and SHREC benchmarks while being\ncomputationally cheaper, data-efficient, and robust.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abhishek Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1\">Maks Ovsjanikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Scale Convolutional Neural Network for Automated AMD Classification using Retinal OCT Images. (arXiv:2110.03002v1 [eess.IV])","link":"http://arxiv.org/abs/2110.03002","description":"<p>Age-related macular degeneration (AMD) is the most common cause of blindness\nin developed countries, especially in people over 60 years of age. The workload\nof specialists and the healthcare system in this field has increased in recent\nyears mainly dues to three reasons: 1) increased use of retinal optical\ncoherence tomography (OCT) imaging technique, 2) prevalence of population aging\nworldwide, and 3) chronic nature of AMD. Recent developments in deep learning\nhave provided a unique opportunity for the development of fully automated\ndiagnosis frameworks. Considering the presence of AMD-related retinal\npathologies in varying sizes in OCT images, our objective was to propose a\nmulti-scale convolutional neural network (CNN) capable of distinguishing\npathologies using receptive fields with various sizes. The multi-scale CNN was\ndesigned based on the feature pyramid network (FPN) structure and was used to\ndiagnose normal and two common clinical characteristics of dry and wet AMD,\nnamely drusen and choroidal neovascularization (CNV). The proposed method was\nevaluated on a national dataset gathered at Noor Eye Hospital (NEH), consisting\nof 12649 retinal OCT images from 441 patients, and a UCSD public dataset,\nconsisting of 108312 OCT images. The results show that the multi-scale\nFPN-based structure was able to improve the base model's overall accuracy by\n0.4% to 3.3% for different backbone models. In addition, gradual learning\nimproved the performance in two phases from 87.2%+-2.5% to 93.4%+-1.4% by\npre-training the base model on ImageNet weights in the first phase and\nfine-tuning the resulting model on a dataset of OCT images in the second phase.\nThe promising quantitative and qualitative results of the proposed architecture\nprove the suitability of the proposed method to be used as a screening tool in\nhealthcare centers assisting ophthalmologists in making better diagnostic\ndecisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sotoudeh_Paima_S/0/1/0/all/0/1\">Saman Sotoudeh-Paima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jodeiri_A/0/1/0/all/0/1\">Ata Jodeiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hajizadeh_F/0/1/0/all/0/1\">Fedra Hajizadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soltanian_Zadeh_H/0/1/0/all/0/1\">Hamid Soltanian-Zadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Centric Semi-Supervised Learning. (arXiv:2110.03006v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03006","description":"<p>We study unsupervised data selection for semi-supervised learning (SSL),\nwhere a large-scale unlabeled data is available and a small subset of data is\nbudgeted for label acquisition. Existing SSL methods focus on learning a model\nthat effectively integrates information from given small labeled data and large\nunlabeled data, whereas we focus on selecting the right data for SSL without\nany label or task information, in an also stark contrast to supervised data\nselection for active learning. Intuitively, instances to be labeled shall\ncollectively have maximum diversity and coverage for downstream tasks, and\nindividually have maximum information propagation utility for SSL. We formalize\nthese concepts in a three-step data-centric SSL method that improves FixMatch\nin stability and accuracy by 8% on CIFAR-10 (0.08% labeled) and 14% on\nImageNet-1K (0.2% labeled). Our work demonstrates that a small compute spent on\ncareful labeled data selection brings big annotation efficiency and model\nperformance gain without changing the learning pipeline. Our completely\nunsupervised data selection can be easily extended to other weakly supervised\nlearning settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_L/0/1/0/all/0/1\">Long Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepBBS: Deep Best Buddies for Point Cloud Registration. (arXiv:2110.03016v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03016","description":"<p>Recently, several deep learning approaches have been proposed for point cloud\nregistration. These methods train a network to generate a representation that\nhelps finding matching points in two 3D point clouds. Finding good matches\nallows them to calculate the transformation between the point clouds\naccurately. Two challenges of these techniques are dealing with occlusions and\ngeneralizing to objects of classes unseen during training. This work proposes\nDeepBBS, a novel method for learning a representation that takes into account\nthe best buddy distance between points during training. Best Buddies (i.e.,\nmutual nearest neighbors) are pairs of points nearest to each other. The Best\nBuddies criterion is a strong indication for correct matches that, in turn,\nleads to accurate registration. Our experiments show improved performance\ncompared to previous methods. In particular, our learned representation leads\nto an accurate registration for partial shapes and in unseen categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hezroni_I/0/1/0/all/0/1\">Itan Hezroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drory_A/0/1/0/all/0/1\">Amnon Drory</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avidan_S/0/1/0/all/0/1\">Shai Avidan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamically Decoding Source Domain Knowledge For Unseen Domain Generalization. (arXiv:2110.03027v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03027","description":"<p>Domain generalization is an important problem which has gain much attention\nrecently. While most existing studies focus on learning domain-invariant\nfeature representations, some researchers try ensemble learning of multi\nexperts and demonstrate promising performance. However, in existing\nmulti-expert learning frameworks, the source domain knowledge has not yet been\nmuch explored, resulting in sub-optimal performance. In this paper, we propose\nto adapt Transformers for the purpose of dynamically decoding source domain\nknowledge for domain generalization. Specifically, we build one domain-specific\nlocal expert per source domain, and one domain-agnostic feature branch as\nquery. Then, all local-domain features will be encoded by Transformer encoders,\nas source domain knowledge in memory. While in the Transformer decoders, the\ndomain-agnostic query will interact with the memory in the cross-attention\nmodule, where similar domains with the input will contribute more in the\nattention output. This way, the source domain knowledge will be dynamically\ndecoded for the inference of the current input from unseen domain. Therefore,\nthis mechanism makes the proposed method well generalizable to unseen domains.\nThe proposed method is evaluated on three benchmarks in the domain\ngeneralization field. The comparison with the state-of-the-art methods shows\nthat the proposed method achieves the best performance, outperforming the\nothers with a clear gap.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_C/0/1/0/all/0/1\">Cuicui Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandakumar_K/0/1/0/all/0/1\">Karthik Nandakumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FOD-A: A Dataset for Foreign Object Debris in Airports. (arXiv:2110.03072v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03072","description":"<p>Foreign Object Debris (FOD) detection has attracted increased attention in\nthe area of machine learning and computer vision. However, a robust and\npublicly available image dataset for FOD has not been initialized. To this end,\nthis paper introduces an image dataset of FOD, named FOD in Airports (FOD-A).\nFOD-A object categories have been selected based on guidance from prior\ndocumentation and related research by the Federal Aviation Administration\n(FAA). In addition to the primary annotations of bounding boxes for object\ndetection, FOD-A provides labeled environmental conditions. As such, each\nannotation instance is further categorized into three light level categories\n(bright, dim, and dark) and two weather categories (dry and wet). Currently,\nFOD-A has released 31 object categories and over 30,000 annotation instances.\nThis paper presents the creation methodology, discusses the publicly available\ndataset extension process, and demonstrates the practicality of FOD-A with\nwidely used machine learning models for object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Munyer_T/0/1/0/all/0/1\">Travis Munyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Pei-Chi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1\">Xin Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-Scale Topological Radar Localization Using Learned Descriptors. (arXiv:2110.03081v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03081","description":"<p>In this work, we propose a method for large-scale topological localization\nbased on radar scan images using learned descriptors. We present a simple yet\nefficient deep network architecture to compute a rotationally invariant\ndiscriminative global descriptor from a radar scan image. The performance and\ngeneralization ability of the proposed method is experimentally evaluated on\ntwo large scale driving datasets: MulRan and Oxford Radar RobotCar.\nAdditionally, we present a comparative evaluation of radar-based and\nLiDAR-based localization using learned global descriptors. Our code and trained\nmodels are publicly available on the project website.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Komorowski_J/0/1/0/all/0/1\">Jacek Komorowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wysoczanska_M/0/1/0/all/0/1\">Monika Wysoczanska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzcinski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-based Excavator Activity Analysis and Safety Monitoring System. (arXiv:2110.03083v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03083","description":"<p>In this paper, we propose an excavator activity analysis and safety\nmonitoring system, leveraging recent advancements in deep learning and computer\nvision. Our proposed system detects the surrounding environment and the\nexcavators while estimating the poses and actions of the excavators. Compared\nto previous systems, our method achieves higher accuracy in object detection,\npose estimation, and action recognition tasks. In addition, we build an\nexcavator dataset using the Autonomous Excavator System (AES) on the waste\ndisposal recycle scene to demonstrate the effectiveness of our system. We also\nevaluate our method on a benchmark construction dataset. The experimental\nresults show that the proposed action recognition approach outperforms the\nstate-of-the-art approaches on top-1 accuracy by about 5.18%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sibo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Player Tracking and Identification in Ice Hockey. (arXiv:2110.03090v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03090","description":"<p>Tracking and identifying players is a fundamental step in computer\nvision-based ice hockey analytics. The data generated by tracking is used in\nmany other downstream tasks, such as game event detection and game strategy\nanalysis. Player tracking and identification is a challenging problem since the\nmotion of players in hockey is fast-paced and non-linear when compared to\npedestrians. There is also significant camera panning and zooming in hockey\nbroadcast video. Identifying players in ice hockey is challenging since the\nplayers of the same team look almost identical, with the jersey number the only\ndiscriminating factor between players. In this paper, an automated system to\ntrack and identify players in broadcast NHL hockey videos is introduced. The\nsystem is composed of three components (1) Player tracking, (2) Team\nidentification and (3) Player identification. Due to the absence of publicly\navailable datasets, the datasets used to train the three components are\nannotated manually. Player tracking is performed with the help of a state of\nthe art tracking algorithm obtaining a Multi-Object Tracking Accuracy (MOTA)\nscore of 94.5%. For team identification, the away-team jerseys are grouped into\na single class and home-team jerseys are grouped in classes according to their\njersey color. A convolutional neural network is then trained on the team\nidentification dataset. The team identification network gets an accuracy of 97%\non the test set. A novel player identification model is introduced that\nutilizes a temporal one-dimensional convolutional network to identify players\nfrom player bounding box sequences. The player identification model further\ntakes advantage of the available NHL game roster data to obtain a player\nidentification accuracy of 83%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vats_K/0/1/0/all/0/1\">Kanav Vats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walters_P/0/1/0/all/0/1\">Pascale Walters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fani_M/0/1/0/all/0/1\">Mehrnaz Fani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clausi_D/0/1/0/all/0/1\">David A. Clausi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelek_J/0/1/0/all/0/1\">John Zelek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Fractal Pre-training. (arXiv:2110.03091v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03091","description":"<p>The deep neural networks used in modern computer vision systems require\nenormous image datasets to train them. These carefully-curated datasets\ntypically have a million or more images, across a thousand or more distinct\ncategories. The process of creating and curating such a dataset is a monumental\nundertaking, demanding extensive effort and labelling expense and necessitating\ncareful navigation of technical and social issues such as label accuracy,\ncopyright ownership, and content bias.\n</p>\n<p>What if we had a way to harness the power of large image datasets but with\nfew or none of the major issues and concerns currently faced? This paper\nextends the recent work of Kataoka et. al. (2020), proposing an improved\npre-training dataset based on dynamically-generated fractal images. Challenging\nissues with large-scale image datasets become points of elegance for fractal\npre-training: perfect label accuracy at zero cost; no need to store/transmit\nlarge image archives; no privacy/demographic bias/concerns of inappropriate\ncontent, as no humans are pictured; limitless supply and diversity of images;\nand the images are free/open-source. Perhaps surprisingly, avoiding these\ndifficulties imposes only a small penalty in performance. Leveraging a\nnewly-proposed pre-training task -- multi-instance prediction -- our\nexperiments demonstrate that fine-tuning a network pre-trained using fractals\nattains 92.7-98.1\\% of the accuracy of an ImageNet pre-trained network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anderson_C/0/1/0/all/0/1\">Connor Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farrell_R/0/1/0/all/0/1\">Ryan Farrell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Pneumonia Localization via Cross-Attention on Medical Images and Reports. (arXiv:2110.03094v1 [eess.IV])","link":"http://arxiv.org/abs/2110.03094","description":"<p>Localization and characterization of diseases like pneumonia are primary\nsteps in a clinical pipeline, facilitating detailed clinical diagnosis and\nsubsequent treatment planning. Additionally, such location annotated datasets\ncan provide a pathway for deep learning models to be used for downstream tasks.\nHowever, acquiring quality annotations is expensive on human resources and\nusually requires domain expertise. On the other hand, medical reports contain a\nplethora of information both about pneumonia characteristics and its location.\nIn this paper, we propose a novel weakly-supervised attention-driven deep\nlearning model that leverages encoded information in medical reports during\ntraining to facilitate better localization. Our model also performs\nclassification of attributes that are associated to pneumonia and extracted\nfrom medical reports for supervision. Both the classification and localization\nare trained in conjunction and once trained, the model can be utilized for both\nthe localization and characterization of pneumonia using only the input image.\nIn this paper, we explore and analyze the model using chest X-ray datasets and\ndemonstrate qualitatively and quantitatively that the introduction of textual\ninformation improves pneumonia localization. We showcase quantitative results\non two datasets, MIMIC-CXR and Chest X-ray-8, and we also showcase severity\ncharacterization on the COVID-19 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bhalodia_R/0/1/0/all/0/1\">Riddhish Bhalodia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hatamizadeh_A/0/1/0/all/0/1\">Ali Hatamizadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tam_L/0/1/0/all/0/1\">Leo Tam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyue Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaosong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turkbey_E/0/1/0/all/0/1\">Evrim Turkbey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Which Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective. (arXiv:2110.03095v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03095","description":"<p>Deep neural networks (DNNs) often rely on easy-to-learn discriminatory\nfeatures, or cues, that are not necessarily essential to the problem at hand.\nFor example, ducks in an image may be recognized based on their typical\nbackground scenery, such as lakes or streams. This phenomenon, also known as\nshortcut learning, is emerging as a key limitation of the current generation of\nmachine learning models. In this work, we introduce a set of experiments to\ndeepen our understanding of shortcut learning and its implications. We design a\ntraining setup with several shortcut cues, named WCST-ML, where each cue is\nequally conducive to the visual recognition problem at hand. Even under equal\nopportunities, we observe that (1) certain cues are preferred to others, (2)\nsolutions biased to the easy-to-learn cues tend to converge to relatively flat\nminima on the loss surface, and (3) the solutions focusing on those preferred\ncues are far more abundant in the parameter space. We explain the abundance of\ncertain cues via their Kolmogorov (descriptional) complexity: solutions\ncorresponding to Kolmogorov-simple cues are abundant in the parameter space and\nare thus preferred by DNNs. Our studies are based on the synthetic dataset\nDSprites and the face dataset UTKFace. In our WCST-ML, we observe that the\ninborn bias of models leans toward simple cues, such as color and ethnicity.\nOur findings emphasize the importance of active human intervention to remove\nthe inborn model biases that may cause negative societal impacts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scimeca_L/0/1/0/all/0/1\">Luca Scimeca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seong Joon Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sanghyuk Chun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poli_M/0/1/0/all/0/1\">Michael Poli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sangdoo Yun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPEED+: Next Generation Dataset for Spacecraft Pose Estimation across Domain Gap. (arXiv:2110.03101v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03101","description":"<p>Autonomous vision-based spaceborne navigation is an enabling technology for\nfuture on-orbit servicing and space logistics missions. While computer vision\nin general has benefited from Machine Learning (ML), training and validating\nspaceborne ML models are extremely challenging due to the impracticality of\nacquiring a large-scale labeled dataset of images of the intended target in the\nspace environment. Existing datasets, such as Spacecraft PosE Estimation\nDataset (SPEED), have so far mostly relied on synthetic images for both\ntraining and validation, which are easy to mass-produce but fail to resemble\nthe visual features and illumination variability inherent to the target\nspaceborne images. In order to bridge the gap between the current practices and\nthe intended applications in future space missions, this paper introduces\nSPEED+: the next generation spacecraft pose estimation dataset with specific\nemphasis on domain gap. In addition to 60,000 synthetic images for training,\nSPEED+ includes 9,531 simulated images of a spacecraft mockup model captured\nfrom the Testbed for Rendezvous and Optical Navigation (TRON) facility. TRON is\na first-of-a-kind robotic testbed capable of capturing an arbitrary number of\ntarget images with accurate and maximally diverse pose labels and high-fidelity\nspaceborne illumination conditions. SPEED+ will be used in the upcoming\ninternational Satellite Pose Estimation Challenge co-hosted with the Advanced\nConcepts Team of the European Space Agency to evaluate and compare the\nrobustness of spaceborne ML models trained on synthetic images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1\">Tae Ha Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martens_M/0/1/0/all/0/1\">Marcus M&#xe4;rtens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lecuyer_G/0/1/0/all/0/1\">Gurvan Lecuyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izzo_D/0/1/0/all/0/1\">Dario Izzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAmico_S/0/1/0/all/0/1\">Simone D&#x27;Amico</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning a Metacognition for Object Detection. (arXiv:2110.03105v1 [cs.AI])","link":"http://arxiv.org/abs/2110.03105","description":"<p>In contrast to object recognition models, humans do not blindly trust their\nperception when building representations of the world, instead recruiting\nmetacognition to detect percepts that are unreliable or false, such as when we\nrealize that we mistook one object for another. We propose METAGEN, an\nunsupervised model that enhances object recognition models through a\nmetacognition. Given noisy output from an object-detection model, METAGEN\nlearns a meta-representation of how its perceptual system works and uses it to\ninfer the objects in the world responsible for the detections. METAGEN achieves\nthis by conditioning its inference on basic principles of objects that even\nhuman infants understand (known as Spelke principles: object permanence,\ncohesion, and spatiotemporal continuity). We test METAGEN on a variety of\nstate-of-the-art object detection neural networks. We find that METAGEN quickly\nlearns an accurate metacognitive representation of the neural network, and that\nthis improves detection accuracy by filling in objects that the detection model\nmissed and removing hallucinated objects. This approach enables generalization\nto out-of-sample data and outperforms comparison models that lack a\nmetacognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berke_M/0/1/0/all/0/1\">Marlene Berke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belledonne_M/0/1/0/all/0/1\">Mario Belledonne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azerbayez_Z/0/1/0/all/0/1\">Zhangir Azerbayez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jara_Ettinger_J/0/1/0/all/0/1\">Julian Jara-Ettinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generic tool for numerical simulation of transformation-diffusion processes in complex volume geometric shapes: application to microbial decomposition of organic matter. (arXiv:2110.03130v1 [eess.IV])","link":"http://arxiv.org/abs/2110.03130","description":"<p>This paper presents a generic framework for the numerical simulation of\ntransformation-diffusion processes in complex volume geometric shapes. This\nwork follows a previous one devoted to the simulation of microbial degradation\nof organic matter in porous system at microscopic scale. We generalized and\nimproved the MOSAIC method significantly and thus yielding a much more generic\nand efficient numerical simulation scheme. In particular, regarding the\nsimulation of diffusion processes from the graph, in this study we proposed a\ncompletely explicit and semi-implicit numerical scheme that can significantly\nreduce the computational complexity. We validated our method by comparing the\nresults to the one provided by classical Lattice Boltzmann Method (LBM) within\nthe context of microbial decomposition simulation. For the same datasets, we\nobtained similar results in a significantly shorter computing time (i.e., 10-15\nminutes) than the prior work (several hours). Besides the classical LBM method\ntakes around 3 weeks computing time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Monga_O/0/1/0/all/0/1\">Olivier Monga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hecht_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Hecht</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moto_S/0/1/0/all/0/1\">Serge Moto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mbe_B/0/1/0/all/0/1\">Bruno Mbe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garnier_P/0/1/0/all/0/1\">Patricia Garnier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pot_V/0/1/0/all/0/1\">Val&#xe9;rie Pot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Sharpness-aware Minimization for Improved Training of Neural Networks. (arXiv:2110.03141v1 [cs.AI])","link":"http://arxiv.org/abs/2110.03141","description":"<p>Overparametrized Deep Neural Networks (DNNs) often achieve astounding\nperformances, but may potentially result in severe generalization error.\nRecently, the relation between the sharpness of the loss landscape and the\ngeneralization error has been established by Foret et al. (2020), in which the\nSharpness Aware Minimizer (SAM) was proposed to mitigate the degradation of the\ngeneralization. Unfortunately, SAM s computational cost is roughly double that\nof base optimizers, such as Stochastic Gradient Descent (SGD). This paper thus\nproposes Efficient Sharpness Aware Minimizer (ESAM), which boosts SAM s\nefficiency at no cost to its generalization performance. ESAM includes two\nnovel and efficient training strategies-StochasticWeight Perturbation and\nSharpness-Sensitive Data Selection. In the former, the sharpness measure is\napproximated by perturbing a stochastically chosen set of weights in each\niteration; in the latter, the SAM loss is optimized using only a judiciously\nselected subset of data that is sensitive to the sharpness. We provide\ntheoretical explanations as to why these strategies perform well. We also show,\nvia extensive experiments on the CIFAR and ImageNet datasets, that ESAM\nenhances the efficiency over SAM from requiring 100% extra computations to 40%\nvis-a-vis base optimizers, while test accuracies are preserved or even\nimproved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jiawei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hanshu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_L/0/1/0/all/0/1\">Liangli Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1\">Rick Siow Mong Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-UDA: Unsupervised Domain Adaptive Thermal Object Detection using Meta-Learning. (arXiv:2110.03143v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03143","description":"<p>Object detectors trained on large-scale RGB datasets are being extensively\nemployed in real-world applications. However, these RGB-trained models suffer a\nperformance drop under adverse illumination and lighting conditions. Infrared\n(IR) cameras are robust under such conditions and can be helpful in real-world\napplications. Though thermal cameras are widely used for military applications\nand increasingly for commercial applications, there is a lack of robust\nalgorithms to robustly exploit the thermal imagery due to the limited\navailability of labeled thermal data. In this work, we aim to enhance the\nobject detection performance in the thermal domain by leveraging the labeled\nvisible domain data in an Unsupervised Domain Adaptation (UDA) setting. We\npropose an algorithm agnostic meta-learning framework to improve existing UDA\nmethods instead of proposing a new UDA strategy. We achieve this by\nmeta-learning the initial condition of the detector, which facilitates the\nadaptation process with fine updates without overfitting or getting stuck at\nlocal optima. However, meta-learning the initial condition for the detection\nscenario is computationally heavy due to long and intractable computation\ngraphs. Therefore, we propose an online meta-learning paradigm which performs\nonline updates resulting in a short and tractable computation graph. To this\nend, we demonstrate the superiority of our method over many baselines in the\nUDA setting, producing a state-of-the-art thermal detector for the KAIST and\nDSIAC datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+VS_V/0/1/0/all/0/1\">Vibashan VS</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poster_D/0/1/0/all/0/1\">Domenick Poster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Suya You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shuowen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DoubleStar: Long-Range Attack Towards Depth Estimation based Obstacle Avoidance in Autonomous Systems. (arXiv:2110.03154v1 [cs.CR])","link":"http://arxiv.org/abs/2110.03154","description":"<p>Depth estimation-based obstacle avoidance has been widely adopted by\nautonomous systems (drones and vehicles) for safety purpose. It normally relies\non a stereo camera to automatically detect obstacles and make flying/driving\ndecisions, e.g., stopping several meters ahead of the obstacle in the path or\nmoving away from the detected obstacle. In this paper, we explore new security\nrisks associated with the stereo vision-based depth estimation algorithms used\nfor obstacle avoidance. By exploiting the weaknesses of the stereo matching in\ndepth estimation algorithms and the lens flare effect in optical imaging, we\npropose DoubleStar, a long-range attack that injects fake obstacle depth by\nprojecting pure light from two complementary light sources.\n</p>\n<p>DoubleStar includes two distinctive attack formats: beams attack and orbs\nattack, which leverage projected light beams and lens flare orbs respectively\nto cause false depth perception. We successfully attack two commercial stereo\ncameras designed for autonomous systems (ZED and Intel RealSense). The\nvisualization of fake depth perceived by the stereo cameras illustrates the\nfalse stereo matching induced by DoubleStar. We further use Ardupilot to\nsimulate the attack and demonstrate its impact on drones. To validate the\nattack on real systems, we perform a real-world attack towards a commercial\ndrone equipped with state-of-the-art obstacle avoidance algorithms. Our attack\ncan continuously bring a flying drone to a sudden stop or drift it away across\na long distance under various lighting conditions, even bypassing sensor fusion\nmechanisms. Specifically, our experimental results show that DoubleStar creates\nfake depth up to 15 meters in distance at night and up to 8 meters during the\ndaytime. To mitigate this newly discovered threat, we provide discussions on\npotential countermeasures to defend against DoubleStar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Ce Zhou</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiben Yan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yan Shi</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a> (2) ((1) Michigan State University, (2) Lehigh University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TreeGCN-ED: Encoding Point Cloud using a Tree-Structured Graph Network. (arXiv:2110.03170v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03170","description":"<p>Point cloud is an efficient way of representing and storing 3D geometric\ndata. Deep learning algorithms on point clouds are time and memory efficient.\nSeveral methods such as PointNet and FoldingNet have been proposed for\nprocessing point clouds. This work proposes an autoencoder based framework to\ngenerate robust embeddings for point clouds by utilizing hierarchical\ninformation using graph convolution. We perform multiple experiments to assess\nthe quality of embeddings generated by the proposed encoder architecture and\nvisualize the t-SNE map to highlight its ability to distinguish between\ndifferent object classes. We further demonstrate the applicability of the\nproposed framework in applications like: 3D point cloud completion and Single\nimage based 3D reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Prajwal Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadekar_K/0/1/0/all/0/1\">Kaustubh Sadekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1\">Shanmuganathan Raman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tile Embedding: A General Representation for Procedural Level Generation via Machine Learning. (arXiv:2110.03181v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03181","description":"<p>In recent years, Procedural Level Generation via Machine Learning (PLGML)\ntechniques have been applied to generate game levels with machine learning.\nThese approaches rely on human-annotated representations of game levels.\nCreating annotated datasets for games requires domain knowledge and is\ntime-consuming. Hence, though a large number of video games exist, annotated\ndatasets are curated only for a small handful. Thus current PLGML techniques\nhave been explored in limited domains, with Super Mario Bros. as the most\ncommon example. To address this problem, we present tile embeddings, a unified,\naffordance-rich representation for tile-based 2D games. To learn this\nembedding, we employ autoencoders trained on the visual and semantic\ninformation of tiles from a set of existing, human-annotated games. We evaluate\nthis representation on its ability to predict affordances for unseen tiles, and\nto serve as a PLGML representation for annotated and unannotated games.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jadhav_M/0/1/0/all/0/1\">Mrunal Jadhav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1\">Matthew Guzdial</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint optimization of system design and reconstruction in MIMO radar imaging. (arXiv:2110.03218v1 [eess.SP])","link":"http://arxiv.org/abs/2110.03218","description":"<p>Multiple-input multiple-output (MIMO) radar is one of the leading depth\nsensing modalities. However, the usage of multiple receive channels lead to\nrelative high costs and prevent the penetration of MIMOs in many areas such as\nthe automotive industry. Over the last years, few studies concentrated on\ndesigning reduced measurement schemes and image reconstruction schemes for MIMO\nradars, however these problems have been so far addressed separately. On the\nother hand, recent works in optical computational imaging have demonstrated\ngrowing success of simultaneous learning-based design of the acquisition and\nreconstruction schemes, manifesting significant improvement in the\nreconstruction quality. Inspired by these successes, in this work, we propose\nto learn MIMO acquisition parameters in the form of receive (Rx) antenna\nelements locations jointly with an image neural-network based reconstruction.\nTo this end, we propose an algorithm for training the combined\nacquisition-reconstruction pipeline end-to-end in a differentiable way. We\ndemonstrate the significance of using our learned acquisition parameters with\nand without the neural-network reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Weiss_T/0/1/0/all/0/1\">Tomer Weiss</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peretz_N/0/1/0/all/0/1\">Nissim Peretz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vedula_S/0/1/0/all/0/1\">Sanketh Vedula</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feuer_A/0/1/0/all/0/1\">Arie Feuer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bronstein_A/0/1/0/all/0/1\">Alex Bronstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient Step Denoiser for convergent Plug-and-Play. (arXiv:2110.03220v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03220","description":"<p>Plug-and-Play methods constitute a class of iterative algorithms for imaging\nproblems where regularization is performed by an off-the-shelf denoiser.\nAlthough Plug-and-Play methods can lead to tremendous visual performance for\nvarious image problems, the few existing convergence guarantees are based on\nunrealistic (or suboptimal) hypotheses on the denoiser, or limited to strongly\nconvex data terms. In this work, we propose a new type of Plug-and-Play\nmethods, based on half-quadratic splitting, for which the denoiser is realized\nas a gradient descent step on a functional parameterized by a deep neural\nnetwork. Exploiting convergence results for proximal gradient descent\nalgorithms in the non-convex setting, we show that the proposed Plug-and-Play\nalgorithm is a convergent iterative scheme that targets stationary points of an\nexplicit global functional. Besides, experiments show that it is possible to\nlearn such a deep denoiser while not compromising the performance in comparison\nto other state-of-the-art deep denoisers used in Plug-and-Play schemes. We\napply our proximal gradient algorithm to various ill-posed inverse problems,\ne.g. deblurring, super-resolution and inpainting. For all these applications,\nnumerical results empirically confirm the convergence results. Experiments also\nshow that this new algorithm reaches state-of-the-art performance, both\nquantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hurault_S/0/1/0/all/0/1\">Samuel Hurault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leclaire_A/0/1/0/all/0/1\">Arthur Leclaire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadakis_N/0/1/0/all/0/1\">Nicolas Papadakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Design of an Intelligent Vision Algorithm for Recognition and Classification of Apples in an Orchard Scene. (arXiv:2110.03232v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03232","description":"<p>Apple is one of the remarkable fresh fruit that contains a high degree of\nnutritious and medicinal value. Hand harvesting of apples by seasonal\nfarmworkers increases physical damages on the surface of these fruits, which\ncauses a great loss in marketing quality. The main objective of this study is\nfocused on designing a robust vision algorithm for robotic apple harvesters.\nThe proposed algorithm is able to recognize and classify 4-classes of objects\nfound in an orchard scene including apples, leaves, trunk and branches, and sky\ninto two apples and non-apples classes. 100 digital images of Red Delicious\napples and 100 digital images of Golden Delicious apples were selected among\n1000 captured images of apples from 18 apple gardens in West Azerbaijan, Iran.\nAn image processing algorithm is proposed for segmentation and extraction of\nthe image classes based on the color characteristics of mentioned classes.\nInvariant-Momentums were chosen as the extracted features from the segmented\nclasses, e.g. apples. Multilayer Feedforward Neural Networks, MFNNs, were used\nas an artificial intelligence tool for the recognition and classification of\nimage classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balanji_H/0/1/0/all/0/1\">Hamid Majidi Balanji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Didar_A/0/1/0/all/0/1\">Alaeedin Rahmani Didar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derafshi_M/0/1/0/all/0/1\">Mohamadali Hadad Derafshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Depth Completion for Active Stereo. (arXiv:2110.03234v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03234","description":"<p>Active stereo systems are widely used in the robotics industry due to their\nlow cost and high quality depth maps. These depth sensors, however, suffer from\nstereo artefacts and do not provide dense depth estimates. In this work, we\npresent the first self-supervised depth completion method for active stereo\nsystems that predicts accurate dense depth maps. Our system leverages a\nfeature-based visual inertial SLAM system to produce motion estimates and\naccurate (but sparse) 3D landmarks. The 3D landmarks are used both as model\ninput and as supervision during training. The motion estimates are used in our\nnovel reconstruction loss that relies on a combination of passive and active\nstereo frames, resulting in significant improvements in textureless areas that\nare common in indoor environments. Due to the non-existence of publicly\navailable active stereo datasets, we release a real dataset together with\nadditional information for a publicly available synthetic dataset needed for\nactive depth completion and prediction. Through rigorous evaluations we show\nthat our method outperforms state of the art on both datasets. Additionally we\nshow how our method obtains more complete, and therefore safer, 3D maps when\nused in a robotic platform\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Warburg_F/0/1/0/all/0/1\">Frederik Warburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Juarez_D/0/1/0/all/0/1\">Daniel Hernandez-Juarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarrio_J/0/1/0/all/0/1\">Juan Tarrio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakhitov_A/0/1/0/all/0/1\">Alexander Vakhitov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonde_U/0/1/0/all/0/1\">Ujwal Bonde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alcantarilla_P/0/1/0/all/0/1\">Pablo Alcantarilla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Colored Point Cloud to Image Alignment. (arXiv:2110.03249v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03249","description":"<p>Recognition and segmentation of objects in images enjoy the wealth of large\nvolume of well annotated data. At the other end, when dealing with the\nreconstruction of geometric structures of objects from images, there is a\nlimited amount of accurate data available for supervised learning. One type of\nsuch geometric data with insufficient amount required for deep learning is real\nworld accurate RGB-D images. The lack of accurate RGB-D datasets is one of the\nobstacles in the evolution of geometric scene reconstructions from images. One\nsolution to creating such a dataset is to capture RGB images while\nsimultaneously using an accurate depth scanning device that assigns a depth\nvalue to each pixel. A major challenge in acquiring such ground truth data is\nthe accurate alignment between the RGB images and the measured depth and color\nprofiles. We introduce a differential optimization method that aligns a colored\npoint cloud to a given color image via iterative geometric and color matching.\nThe proposed method enables the construction of RGB-D datasets for specific\ncamera systems. In the suggested framework, the optimization minimizes the\ndifference between the colors of the image pixels and the corresponding colors\nof the projected points to the camera plane. We assume that the colors produced\nby the geometric scanner camera and the color camera sensor are different and\nthus are characterized by different chromatic acquisition properties. We align\nthe different color spaces while compensating for their corresponding color\nappearance. Under this setup, we find the transformation between the camera\nimage and the point cloud colors by iterating between matching the relative\nlocation of the point cloud and matching colors. The successful alignments\nproduced by the proposed method are demonstrated on both synthetic data with\nquantitative evaluation and real world scenes with qualitative results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rotstein_N/0/1/0/all/0/1\">Noam Rotstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bracha_A/0/1/0/all/0/1\">Amit Bracha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimmel_R/0/1/0/all/0/1\">Ron Kimmel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving MC-Dropout Uncertainty Estimates with Calibration Error-based Optimization. (arXiv:2110.03260v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03260","description":"<p>Uncertainty quantification of machine learning and deep learning methods\nplays an important role in enhancing trust to the obtained result. In recent\nyears, a numerous number of uncertainty quantification methods have been\nintroduced. Monte Carlo dropout (MC-Dropout) is one of the most well-known\ntechniques to quantify uncertainty in deep learning methods. In this study, we\npropose two new loss functions by combining cross entropy with Expected\nCalibration Error (ECE) and Predictive Entropy (PE). The obtained results\nclearly show that the new proposed loss functions lead to having a calibrated\nMC-Dropout method. Our results confirmed the great impact of the new hybrid\nloss functions for minimising the overlap between the distributions of\nuncertainty estimates for correct and incorrect predictions without sacrificing\nthe model's overall performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shamsi_A/0/1/0/all/0/1\">Afshar Shamsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgharnezhad_H/0/1/0/all/0/1\">Hamzeh Asgharnezhad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdar_M/0/1/0/all/0/1\">Moloud Abdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajally_A/0/1/0/all/0/1\">AmirReza Tajally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leung_H/0/1/0/all/0/1\">Henry Leung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Propagating State Uncertainty Through Trajectory Forecasting. (arXiv:2110.03267v1 [cs.RO])","link":"http://arxiv.org/abs/2110.03267","description":"<p>Uncertainty pervades through the modern robotic autonomy stack, with nearly\nevery component (e.g., sensors, detection, classification, tracking, behavior\nprediction) producing continuous or discrete probabilistic distributions.\nTrajectory forecasting, in particular, is surrounded by uncertainty as its\ninputs are produced by (noisy) upstream perception and its outputs are\npredictions that are often probabilistic for use in downstream planning.\nHowever, most trajectory forecasting methods do not account for upstream\nuncertainty, instead taking only the most-likely values. As a result,\nperceptual uncertainties are not propagated through forecasting and predictions\nare frequently overconfident. To address this, we present a novel method for\nincorporating perceptual state uncertainty in trajectory forecasting, a key\ncomponent of which is a new statistical distance-based loss function which\nencourages predicting uncertainties that better match upstream perception. We\nevaluate our approach both in illustrative simulations and on large-scale,\nreal-world data, demonstrating its efficacy in propagating perceptual state\nuncertainty through prediction and producing more calibrated predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivanovic_B/0/1/0/all/0/1\">Boris Ivanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yifeng/0/1/0/all/0/1\">Yifeng</a> (Richard)Lin, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_S/0/1/0/all/0/1\">Shubham Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarty_P/0/1/0/all/0/1\">Punarjay Chakravarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Injecting Planning-Awareness into Prediction and Detection Evaluation. (arXiv:2110.03270v1 [cs.RO])","link":"http://arxiv.org/abs/2110.03270","description":"<p>Detecting other agents and forecasting their behavior is an integral part of\nthe modern robotic autonomy stack, especially in safety-critical scenarios\nentailing human-robot interaction such as autonomous driving. Due to the\nimportance of these components, there has been a significant amount of interest\nand research in perception and trajectory forecasting, resulting in a wide\nvariety of approaches. Common to most works, however, is the use of the same\nfew accuracy-based evaluation metrics, e.g., intersection-over-union,\ndisplacement error, log-likelihood, etc. While these metrics are informative,\nthey are task-agnostic and outputs that are evaluated as equal can lead to\nvastly different outcomes in downstream planning and decision making. In this\nwork, we take a step back and critically assess current evaluation metrics,\nproposing task-aware metrics as a better measure of performance in systems\nwhere they are deployed. Experiments on an illustrative simulation as well as\nreal-world autonomous driving data validate that our proposed task-aware\nmetrics are able to account for outcome asymmetry and provide a better estimate\nof a model's closed-loop performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivanovic_B/0/1/0/all/0/1\">Boris Ivanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Virtual Multi-Modality Self-Supervised Foreground Matting for Human-Object Interaction. (arXiv:2110.03278v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03278","description":"<p>Most existing human matting algorithms tried to separate pure human-only\nforeground from the background. In this paper, we propose a Virtual\nMulti-modality Foreground Matting (VMFM) method to learn human-object\ninteractive foreground (human and objects interacted with him or her) from a\nraw RGB image. The VMFM method requires no additional inputs, e.g. trimap or\nknown background. We reformulate foreground matting as a self-supervised\nmulti-modality problem: factor each input image into estimated depth map,\nsegmentation mask, and interaction heatmap using three auto-encoders. In order\nto fully utilize the characteristics of each modality, we first train a dual\nencoder-to-decoder network to estimate the same alpha matte. Then we introduce\na self-supervised method: Complementary Learning(CL) to predict deviation\nprobability map and exchange reliable gradients across modalities without\nlabel. We conducted extensive experiments to analyze the effectiveness of each\nmodality and the significance of different components in complementary\nlearning. We demonstrate that our model outperforms the state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MC-LCR: Multi-modal contrastive classification by locally correlated representations for effective face forgery detection. (arXiv:2110.03290v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03290","description":"<p>As the remarkable development of facial manipulation technologies is\naccompanied by severe security concerns, face forgery detection has become a\nrecent research hotspot. Most existing detection methods train a binary\nclassifier under global supervision to judge real or fake. However, advanced\nmanipulations only perform small-scale tampering, posing challenges to\ncomprehensively capture subtle and local forgery artifacts, especially in high\ncompression settings and cross-dataset scenarios. To address such limitations,\nwe propose a novel framework named Multi-modal Contrastive Classification by\nLocally Correlated Representations(MC-LCR), for effective face forgery\ndetection. Instead of specific appearance features, our MC-LCR aims to amplify\nimplicit local discrepancies between authentic and forged faces from both\nspatial and frequency domains. Specifically, we design the shallow style\nrepresentation block that measures the pairwise correlation of shallow feature\nmaps, which encodes local style information to extract more discriminative\nfeatures in the spatial domain. Moreover, we make a key observation that subtle\nforgery artifacts can be further exposed in the patch-wise phase and amplitude\nspectrum and exhibit different clues. According to the complementarity of\namplitude and phase information, we develop a patch-wise amplitude and phase\ndual attention module to capture locally correlated inconsistencies with each\nother in the frequency domain. Besides the above two modules, we further\nintroduce the collaboration of supervised contrastive loss with cross-entropy\nloss. It helps the network learn more discriminative and generalized\nrepresentations. Through extensive experiments and comprehensive studies, we\nachieve state-of-the-art performance and demonstrate the robustness and\ngeneralization of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gaojian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiaohui Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Supermask Pruning: Learning to Prune Image Captioning Models. (arXiv:2110.03298v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03298","description":"<p>With the advancement of deep models, research work on image captioning has\nled to a remarkable gain in raw performance over the last decade, along with\nincreasing model complexity and computational cost. However, surprisingly works\non compression of deep networks for image captioning task has received little\nto no attention. For the first time in image captioning research, we provide an\nextensive comparison of various unstructured weight pruning methods on three\ndifferent popular image captioning architectures, namely Soft-Attention,\nUp-Down and Object Relation Transformer. Following this, we propose a novel\nend-to-end weight pruning method that performs gradual sparsification based on\nweight sensitivity to the training loss. The pruning schemes are then extended\nwith encoder pruning, where we show that conducting both decoder pruning and\ntraining simultaneously prior to the encoder pruning provides good overall\nperformance. Empirically, we show that an 80% to 95% sparse network (up to 75%\nreduction in model size) can either match or outperform its dense counterpart.\nThe code and pre-trained models for Up-Down and Object Relation Transformer\nthat are capable of achieving CIDEr scores &gt;120 on the MS-COCO dataset but with\nonly 8.7 MB and 14.5 MB in model size (size reduction of 96% and 94%\nrespectively against dense versions) are publicly available at\nhttps://github.com/jiahuei/sparse-image-captioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jia Huei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Chee Seng Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuah_J/0/1/0/all/0/1\">Joon Huang Chuah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MGPSN: Motion-Guided Pseudo Siamese Network for Indoor Video Head Detection. (arXiv:2110.03302v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03302","description":"<p>Head detection in real-world videos is an important research topic in\ncomputer vision. However, existing studies face some challenges in complex\nscenes. The performance of head detectors deteriorates when objects which have\nsimilar head appearance exist for indoor videos. Moreover, heads have small\nscales and diverse poses, which increases the difficulty in detection. To\nhandle these issues, we propose Motion-Guided Pseudo Siamese Network for Indoor\nVideo Head Detection (MGPSN), an end-to-end model to learn the robust head\nmotion features. MGPSN integrates spatial-temporal information on pixel level,\nguiding the model to extract effective head features. Experiments show that\nMGPSN is able to suppress static objects and enhance motion instances. Compared\nwith previous methods, it achieves state-of-the-art performance on the crowd\nBrainwash dataset. Different backbone networks and detectors are evaluated to\nverify the flexibility and generality of MGPSN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Kailai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaoteng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qianchuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Moment evolution equations and moment matching for stochastic image EPDiff. (arXiv:2110.03337v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03337","description":"<p>Models of stochastic image deformation allow study of time-continuous\nstochastic effects transforming images by deforming the image domain.\nApplications include longitudinal medical image analysis with both population\ntrends and random subject specific variation. Focusing on a stochastic\nextension of the LDDMM models with evolutions governed by a stochastic EPDiff\nequation, we use moment approximations of the corresponding Ito diffusion to\nconstruct estimators for statistical inference in the full stochastic model. We\nshow that this approach, when efficiently implemented with automatic\ndifferentiation tools, can successfully estimate parameters encoding the\nspatial correlation of the noise fields on the image\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christgau_A/0/1/0/all/0/1\">Alexander Christgau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnaudon_A/0/1/0/all/0/1\">Alexis Arnaudon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sommer_S/0/1/0/all/0/1\">Stefan Sommer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-aware GAN with Adaptive Loss for Robust MRI Image Enhancement. (arXiv:2110.03343v1 [eess.IV])","link":"http://arxiv.org/abs/2110.03343","description":"<p>Image-to-image translation is an ill-posed problem as unique one-to-one\nmapping may not exist between the source and target images. Learning-based\nmethods proposed in this context often evaluate the performance on test data\nthat is similar to the training data, which may be impractical. This demands\nrobust methods that can quantify uncertainty in the prediction for making\ninformed decisions, especially for critical areas such as medical imaging.\nRecent works that employ conditional generative adversarial networks (GANs)\nhave shown improved performance in learning photo-realistic image-to-image\nmappings between the source and the target images. However, these methods do\nnot focus on (i)~robustness of the models to out-of-distribution (OOD)-noisy\ndata and (ii)~uncertainty quantification. This paper proposes a GAN-based\nframework that (i)~models an adaptive loss function for robustness to OOD-noisy\ndata that automatically tunes the spatially varying norm for penalizing the\nresiduals and (ii)~estimates the per-voxel uncertainty in the predictions. We\ndemonstrate our method on two key applications in medical imaging:\n(i)~undersampled magnetic resonance imaging (MRI) reconstruction (ii)~MRI\nmodality propagation. Our experiments with two different real-world datasets\nshow that the proposed method (i)~is robust to OOD-noisy test data and provides\nimproved accuracy and (ii)~quantifies voxel-level uncertainty in the\npredictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Upadhyay_U/0/1/0/all/0/1\">Uddeshya Upadhyay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sudarshan_V/0/1/0/all/0/1\">Viswanath P. Sudarshan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Awate_S/0/1/0/all/0/1\">Suyash P. Awate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSHCNet: Multi-Stream Hybridized Convolutional Networks with Mixed Statistics in Euclidean/Non-Euclidean Spaces and Its Application to Hyperspectral Image Classification. (arXiv:2110.03346v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03346","description":"<p>It is well known that hyperspectral images (HSI) contain rich\nspatial-spectral contextual information, and how to effectively combine both\nspectral and spatial information using DNN for HSI classification has become a\nnew research hotspot. Compared with CNN with square kernels, GCN have exhibited\nexciting potential to model spatial contextual structure and conduct flexible\nconvolution on arbitrarily irregular image regions. However, current GCN only\nusing first-order spectral-spatial signatures can result in boundary blurring\nand isolated misclassification. To address these, we first designed the\ngraph-based second-order pooling (GSOP) operation to obtain contextual nodes\ninformation in non-Euclidean space for GCN. Further, we proposed a novel\nmulti-stream hybridized convolutional network (MSHCNet) with combination of\nfirst and second order statistics in Euclidean/non-Euclidean spaces to learn\nand fuse multi-view complementary information to segment HSIs. Specifically,\nour MSHCNet adopted four parallel streams, which contained G-stream, utilizing\nthe irregular correlation between adjacent land covers in terms of first-order\ngraph in non-Euclidean space; C-stream, adopting convolution operator to learn\nregular spatial-spectral features in Euclidean space; N-stream, combining first\nand second order features to learn representative and discriminative regular\nspatial-spectral features of Euclidean space; S-stream, using GSOP to capture\nboundary correlations and obtain graph representations from all nodes in graphs\nof non-Euclidean space. Besides, these feature representations learned from\nfour different streams were fused to integrate the multi-view complementary\ninformation for HSI classification. Finally, we evaluated our proposed MSHCNet\non three hyperspectral datasets, and experimental results demonstrated that our\nmethod significantly outperformed state-of-the-art eight methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shuang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haitong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xia Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hongjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nizhuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimized U-Net for Brain Tumor Segmentation. (arXiv:2110.03352v1 [eess.IV])","link":"http://arxiv.org/abs/2110.03352","description":"<p>We propose an optimized U-Net architecture for a brain \\mbox{tumor}\nsegmentation task in the BraTS21 Challenge. To find the \\mbox{optimal} model\narchitecture and learning schedule we ran an extensive ablation study to test:\ndeep supervision loss, Focal loss, decoder attention, drop block, and residual\nconnections. Additionally, we have searched for the optimal depth of the U-Net\nand number of convolutional channels. Our solution was the winner of the\nchallenge validation phase, with the normalized statistical ranking score of\n0.267 and mean Dice score of 0.8855\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Futrega_M/0/1/0/all/0/1\">Micha&#x142; Futrega</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milesi_A/0/1/0/all/0/1\">Alexandre Milesi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marcinkiewicz_M/0/1/0/all/0/1\">Michal Marcinkiewicz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ribalta_P/0/1/0/all/0/1\">Pablo Ribalta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse MoEs meet Efficient Ensembles. (arXiv:2110.03360v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03360","description":"<p>Machine learning models based on the aggregated outputs of submodels, either\nat the activation or prediction levels, lead to strong performance. We study\nthe interplay of two popular classes of such models: ensembles of neural\nnetworks and sparse mixture of experts (sparse MoEs). First, we show that these\ntwo approaches have complementary features whose combination is beneficial.\nThen, we present partitioned batch ensembles, an efficient ensemble of sparse\nMoEs that takes the best of both classes of models. Extensive experiments on\nfine-tuned vision transformers demonstrate the accuracy, log-likelihood,\nfew-shot learning, robustness, and uncertainty calibration improvements of our\napproach over several challenging baselines. Partitioned batch ensembles not\nonly scale to models with up to 2.7B parameters, but also provide larger\nperformance gains for larger models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Allingham_J/0/1/0/all/0/1\">James Urquhart Allingham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenzel_F/0/1/0/all/0/1\">Florian Wenzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mariet_Z/0/1/0/all/0/1\">Zelda E Mariet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1\">Basil Mustafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puigcerver_J/0/1/0/all/0/1\">Joan Puigcerver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jerfel_G/0/1/0/all/0/1\">Ghassen Jerfel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1\">Vincent Fortuin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_J/0/1/0/all/0/1\">Jasper Snoek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Dustin Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_C/0/1/0/all/0/1\">Carlos Riquelme Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1\">Rodolphe Jenatton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Baseline Framework for Part-level Action Parsing and Action Recognition. (arXiv:2110.03368v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03368","description":"<p>This technical report introduces our 2nd place solution to Kinetics-TPS Track\non Part-level Action Parsing in ICCV DeeperAction Workshop 2021. Our entry is\nmainly based on YOLOF for instance and part detection, HRNet for human pose\nestimation, and CSN for video-level action recognition and frame-level part\nstate parsing. We describe technical details for the Kinetics-TPS dataset,\ntogether with some experimental results. In the competition, we achieved 61.37%\nmAP on the test set of Kinetics-TPS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaodong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data. (arXiv:2110.03374v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03374","description":"<p>Unsupervised domain adaptation aims to align a labeled source domain and an\nunlabeled target domain, but it requires to access the source data which often\nraises concerns in data privacy, data portability and data transmission\nefficiency. We study unsupervised model adaptation (UMA), or called\nUnsupervised Domain Adaptation without Source Data, an alternative setting that\naims to adapt source-trained models towards target distributions without\naccessing source data. To this end, we design an innovative historical\ncontrastive learning (HCL) technique that exploits historical source hypothesis\nto make up for the absence of source data in UMA. HCL addresses the UMA\nchallenge from two perspectives. First, it introduces historical contrastive\ninstance discrimination (HCID) that learns from target samples by contrasting\ntheir embeddings which are generated by the currently adapted model and the\nhistorical models. With the source-trained and earlier-epoch models as the\nhistorical models, HCID encourages UMA to learn instance-discriminative target\nrepresentations while preserving the source hypothesis. Second, it introduces\nhistorical contrastive category discrimination (HCCD) that pseudo-labels target\nsamples to learn category-discriminative target representations. Instead of\nglobally thresholding pseudo labels, HCCD re-weights pseudo labels according to\ntheir prediction consistency across the current and historical models.\nExtensive experiments show that HCL outperforms and complements\nstate-of-the-art methods consistently across a variety of visual tasks (e.g.,\nsegmentation, classification and detection) and setups (e.g., close-set,\nopen-set and partial adaptation).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Dayan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Model Explainability for Inspection Accuracy Improvement in the Automotive Industry. (arXiv:2110.03384v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03384","description":"<p>The welding seams visual inspection is still manually operated by humans in\ndifferent companies, so the result of the test is still highly subjective and\nexpensive. At present, the integration of deep learning methods for welds\nclassification is a research focus in engineering applications. This work\nintends to apprehend and emphasize the contribution of deep learning model\nexplainability to the improvement of welding seams classification accuracy and\nreliability, two of the various metrics affecting the production lines and cost\nin the automotive industry. For this purpose, we implement a novel hybrid\nmethod that relies on combining the model prediction scores and visual\nexplanation heatmap of the model in order to make a more accurate\nclassification of welding seam defects and improve both its performance and its\nreliability. The results show that the hybrid model performance is relatively\nabove our target performance and helps to increase the accuracy by at least\n18%, which presents new perspectives to the developments of deep Learning\nexplainability and interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Houd_A/0/1/0/all/0/1\">Anass El Houd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hachem_C/0/1/0/all/0/1\">Charbel El Hachem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Painvin_L/0/1/0/all/0/1\">Loic Painvin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AnoSeg: Anomaly Segmentation Network Using Self-Supervised Learning. (arXiv:2110.03396v1 [eess.IV])","link":"http://arxiv.org/abs/2110.03396","description":"<p>Anomaly segmentation, which localizes defective areas, is an important\ncomponent in large-scale industrial manufacturing. However, most recent\nresearches have focused on anomaly detection. This paper proposes a novel\nanomaly segmentation network (AnoSeg) that can directly generate an accurate\nanomaly map using self-supervised learning. For highly accurate anomaly\nsegmentation, the proposed AnoSeg considers three novel techniques: Anomaly\ndata generation based on hard augmentation, self-supervised learning with\npixel-wise and adversarial losses, and coordinate channel concatenation. First,\nto generate synthetic anomaly images and reference masks for normal data, the\nproposed method uses hard augmentation to change the normal sample\ndistribution. Then, the proposed AnoSeg is trained in a self-supervised\nlearning manner from the synthetic anomaly data and normal data. Finally, the\ncoordinate channel, which represents the pixel location information, is\nconcatenated to an input of AnoSeg to consider the positional relationship of\neach pixel in the image. The estimated anomaly map can also be utilized to\nimprove the performance of anomaly detection. Our experiments show that the\nproposed method outperforms the state-of-the-art anomaly detection and anomaly\nsegmentation methods for the MVTec AD dataset. In addition, we compared the\nproposed method with the existing methods through the intersection over union\n(IoU) metric commonly used in segmentation tasks and demonstrated the\nsuperiority of our method for anomaly segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1\">Jouwon Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kong_K/0/1/0/all/0/1\">Kyeongbo Kong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_Y/0/1/0/all/0/1\">Ye-In Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Seong-Gyun Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_S/0/1/0/all/0/1\">Suk-Ju Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hierarchical Variational Neural Uncertainty Model for Stochastic Video Prediction. (arXiv:2110.03446v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03446","description":"<p>Predicting the future frames of a video is a challenging task, in part due to\nthe underlying stochastic real-world phenomena. Prior approaches to solve this\ntask typically estimate a latent prior characterizing this stochasticity,\nhowever do not account for the predictive uncertainty of the (deep learning)\nmodel. Such approaches often derive the training signal from the mean-squared\nerror (MSE) between the generated frame and the ground truth, which can lead to\nsub-optimal training, especially when the predictive uncertainty is high.\nTowards this end, we introduce Neural Uncertainty Quantifier (NUQ) - a\nstochastic quantification of the model's predictive uncertainty, and use it to\nweigh the MSE loss. We propose a hierarchical, variational framework to derive\nNUQ in a principled manner using a deep, Bayesian graphical model. Our\nexperiments on four benchmark stochastic video prediction datasets show that\nour proposed framework trains more effectively compared to the state-of-the-art\nmodels (especially when the training sets are small), while demonstrating\nbetter video generation quality and diversity against several evaluation\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_M/0/1/0/all/0/1\">Moitreya Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_N/0/1/0/all/0/1\">Narendra Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1\">Anoop Cherian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inter-Domain Alignment for Predicting High-Resolution Brain Networks Using Teacher-Student Learning. (arXiv:2110.03452v1 [eess.IV])","link":"http://arxiv.org/abs/2110.03452","description":"<p>Accurate and automated super-resolution image synthesis is highly desired\nsince it has the great potential to circumvent the need for acquiring high-cost\nmedical scans and a time-consuming preprocessing pipeline of neuroimaging data.\nHowever, existing deep learning frameworks are solely designed to predict\nhigh-resolution (HR) image from a low-resolution (LR) one, which limits their\ngeneralization ability to brain graphs (i.e., connectomes). A small body of\nworks has focused on superresolving brain graphs where the goal is to predict a\nHR graph from a single LR graph. Although promising, existing works mainly\nfocus on superresolving graphs belonging to the same domain (e.g., functional),\noverlooking the domain fracture existing between multimodal brain data\ndistributions (e.g., morphological and structural). To this aim, we propose a\nnovel inter-domain adaptation framework namely, Learn to SuperResolve Brain\nGraphs with Knowledge Distillation Network (L2S-KDnet), which adopts a\nteacher-student paradigm to superresolve brain graphs. Our teacher network is a\ngraph encoder-decoder that firstly learns the LR brain graph embeddings, and\nsecondly learns how to align the resulting latent representations to the HR\nground truth data distribution using an adversarial regularization. Ultimately,\nit decodes the HR graphs from the aligned embeddings. Next, our student network\nlearns the knowledge of the aligned brain graphs as well as the topological\nstructure of the predicted HR graphs transferred from the teacher. We further\nleverage the decoder of the teacher to optimize the student network. L2S-KDnet\npresents the first TS architecture tailored for brain graph super-resolution\nsynthesis that is based on inter-domain alignment. Our experimental results\ndemonstrate substantial performance gains over benchmark methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Demir_B/0/1/0/all/0/1\">Basar Demir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bessadok_A/0/1/0/all/0/1\">Alaa Bessadok</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rekik_I/0/1/0/all/0/1\">Islem Rekik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent Multigraph Integrator Network for Predicting the Evolution of Population-Driven Brain Connectivity Templates. (arXiv:2110.03453v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03453","description":"<p>Learning how to estimate a connectional brain template(CBT) from a population\nof brain multigraphs, where each graph (e.g., functional) quantifies a\nparticular relationship between pairs of brain regions of interest (ROIs),\nallows to pin down the unique connectivity patterns shared across individuals.\nSpecifically, a CBT is viewed as an integral representation of a set of highly\nheterogeneous graphs and ideally meeting the centeredness (i.e., minimum\ndistance to all graphs in the population) and discriminativeness (i.e.,\ndistinguishes the healthy from the disordered population) criteria. So far,\nexisting works have been limited to only integrating and fusing a population of\nbrain multigraphs acquired at a single timepoint. In this paper, we\nunprecedentedly tackle the question: Given a baseline multigraph population,\ncan we learn how to integrate and forecast its CBT representations at follow-up\ntimepoints? Addressing such question is of paramount in predicting common\nalternations across healthy and disordered populations. To fill this gap, we\npropose Recurrent Multigraph Integrator Network (ReMI-Net), the first graph\nrecurrent neural network which infers the baseline CBT of an input population\nt1 and predicts its longitudinal evolution over time (ti &gt; t1). Our ReMI-Net is\ncomposed of recurrent neural blocks with graph convolutional layers using a\ncross-node message passing to first learn hidden-states embeddings of each CBT\nnode (i.e., brain region of interest) and then predict its evolution at the\nconsecutive timepoint. Moreover, we design a novel time-dependent loss to\nregularize the CBT evolution trajectory over time and further introduce a\ncyclic recursion and learnable normalization layer to generate well-centered\nCBTs from time-dependent hidden-state embeddings. Finally, we derive the CBT\nadjacency matrix from the learned hidden state graph representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Demirbilek_O/0/1/0/all/0/1\">Oytun Demirbilek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekik_I/0/1/0/all/0/1\">Islem Rekik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differential Anomaly Detection for Facial Images. (arXiv:2110.03464v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03464","description":"<p>Due to their convenience and high accuracy, face recognition systems are\nwidely employed in governmental and personal security applications to\nautomatically recognise individuals. Despite recent advances, face recognition\nsystems have shown to be particularly vulnerable to identity attacks (i.e.,\ndigital manipulations and attack presentations). Identity attacks pose a big\nsecurity threat as they can be used to gain unauthorised access and spread\nmisinformation. In this context, most algorithms for detecting identity attacks\ngeneralise poorly to attack types that are unknown at training time. To tackle\nthis problem, we introduce a differential anomaly detection framework in which\ndeep face embeddings are first extracted from pairs of images (i.e., reference\nand probe) and then combined for identity attack detection. The experimental\nevaluation conducted over several databases shows a high generalisation\ncapability of the proposed method for detecting unknown attacks in both the\ndigital and physical domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ibsen_M/0/1/0/all/0/1\">Mathias Ibsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Soler_L/0/1/0/all/0/1\">L&#xe1;zaro J. Gonz&#xe1;lez-Soler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdowski_P/0/1/0/all/0/1\">Pawel Drozdowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Barrero_M/0/1/0/all/0/1\">Marta Gomez-Barrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Image Decomposition with Phase-Correlation Networks. (arXiv:2110.03473v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03473","description":"<p>The ability to decompose scenes into their object components is a desired\nproperty for autonomous agents, allowing them to reason and act in their\nsurroundings. Recently, different methods have been proposed to learn\nobject-centric representations from data in an unsupervised manner. These\nmethods often rely on latent representations learned by deep neural networks,\nhence requiring high computational costs and large amounts of curated data.\nSuch models are also difficult to interpret. To address these challenges, we\npropose the Phase-Correlation Decomposition Network (PCDNet), a novel model\nthat decomposes a scene into its object components, which are represented as\ntransformed versions of a set of learned object prototypes. The core building\nblock in PCDNet is the Phase-Correlation Cell (PC Cell), which exploits the\nfrequency-domain representation of the images in order to estimate the\ntransformation between an object prototype and its transformed version in the\nimage. In our experiments, we show how PCDNet outperforms state-of-the-art\nmethods for unsupervised object discovery and segmentation on simple benchmark\ndatasets and on more challenging data, while using a small number of learnable\nparameters and being fully interpretable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Villar_Corrales_A/0/1/0/all/0/1\">Angel Villar-Corrales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfoSeg: Unsupervised Semantic Image Segmentation with Mutual Information Maximization. (arXiv:2110.03477v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03477","description":"<p>We propose a novel method for unsupervised semantic image segmentation based\non mutual information maximization between local and global high-level image\nfeatures. The core idea of our work is to leverage recent progress in\nself-supervised image representation learning. Representation learning methods\ncompute a single high-level feature capturing an entire image. In contrast, we\ncompute multiple high-level features, each capturing image segments of one\nparticular semantic class. To this end, we propose a novel two-step learning\nprocedure comprising a segmentation and a mutual information maximization step.\nIn the first step, we segment images based on local and global features. In the\nsecond step, we maximize the mutual information between local features and\nhigh-level features of their respective class. For training, we provide solely\nunlabeled images and start from random network initialization. For quantitative\nand qualitative evaluation, we use established benchmarks, and COCO-Persons,\nwhereby we introduce the latter in this paper as a challenging novel benchmark.\nInfoSeg significantly outperforms the current state-of-the-art, e.g., we\nachieve a relative increase of 26% in the Pixel Accuracy metric on the\nCOCO-Stuff dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harb_R/0/1/0/all/0/1\">Robert Harb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knobelreiter_P/0/1/0/all/0/1\">Patrick Kn&#xf6;belreiter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Camera Calibration through Camera Projection Loss. (arXiv:2110.03479v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03479","description":"<p>Camera calibration is a necessity in various tasks including 3D\nreconstruction, hand-eye coordination for a robotic interaction, autonomous\ndriving, etc. In this work we propose a novel method to predict extrinsic\n(baseline, pitch, and translation), intrinsic (focal length and principal point\noffset) parameters using an image pair. Unlike existing methods, instead of\ndesigning an end-to-end solution, we proposed a new representation that\nincorporates camera model equations as a neural network in multi-task learning\nframework. We estimate the desired parameters via novel \\emph{camera projection\nloss} (CPL) that uses the camera model neural network to reconstruct the 3D\npoints and uses the reconstruction loss to estimate the camera parameters. To\nthe best of our knowledge, ours is the first method to jointly estimate both\nthe intrinsic and extrinsic parameters via a multi-task learning methodology\nthat combines analytical equations in learning framework for the estimation of\ncamera parameters. We also proposed a novel dataset using CARLA Simulator.\nEmpirically, we demonstrate that our proposed approach achieves better\nperformance with respect to both deep learning-based and traditional methods on\n7 out of 10 parameters evaluated using both synthetic and real data. Our code\nand generated dataset will be made publicly available to facilitate future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Butt_T/0/1/0/all/0/1\">Talha Hanif Butt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taj_M/0/1/0/all/0/1\">Murtaza Taj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Regress Bodies from Images using Differentiable Semantic Rendering. (arXiv:2110.03480v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03480","description":"<p>Learning to regress 3D human body shape and pose (e.g.~SMPL parameters) from\nmonocular images typically exploits losses on 2D keypoints, silhouettes, and/or\npart-segmentation when 3D training data is not available. Such losses, however,\nare limited because 2D keypoints do not supervise body shape and segmentations\nof people in clothing do not match projected minimally-clothed SMPL shapes. To\nexploit richer image information about clothed people, we introduce\nhigher-level semantic information about clothing to penalize clothed and\nnon-clothed regions of the image differently. To do so, we train a body\nregressor using a novel Differentiable Semantic Rendering - DSR loss. For\nMinimally-Clothed regions, we define the DSR-MC loss, which encourages a tight\nmatch between a rendered SMPL body and the minimally-clothed regions of the\nimage. For clothed regions, we define the DSR-C loss to encourage the rendered\nSMPL body to be inside the clothing mask. To ensure end-to-end differentiable\ntraining, we learn a semantic clothing prior for SMPL vertices from thousands\nof clothed human scans. We perform extensive qualitative and quantitative\nexperiments to evaluate the role of clothing semantics on the accuracy of 3D\nhuman pose and shape estimation. We outperform all previous state-of-the-art\nmethods on 3DPW and Human3.6M and obtain on par results on MPI-INF-3DHP. Code\nand trained models are available for research at https://dsr.is.tue.mpg.de/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dwivedi_S/0/1/0/all/0/1\">Sai Kumar Dwivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athanasiou_N/0/1/0/all/0/1\">Nikos Athanasiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocabas_M/0/1/0/all/0/1\">Muhammed Kocabas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cartoon Explanations of Image Classifiers. (arXiv:2110.03485v1 [cs.AI])","link":"http://arxiv.org/abs/2110.03485","description":"<p>We present CartoonX (Cartoon Explanation), a novel model-agnostic explanation\nmethod tailored towards image classifiers and based on the rate-distortion\nexplanation (RDE) framework. Natural images are roughly piece-wise smooth\nsignals -- also called cartoon images -- and tend to be sparse in the wavelet\ndomain. CartoonX is the first explanation method to exploit this by requiring\nits explanations to be sparse in the wavelet domain, thus extracting the\n\\emph{relevant piece-wise smooth} part of an image instead of relevant\npixel-sparse regions. We demonstrate experimentally that CartoonX is not only\nhighly interpretable due to its piece-wise smooth nature but also particularly\napt at explaining misclassifications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolek_S/0/1/0/all/0/1\">Stefan Kolek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levie_R/0/1/0/all/0/1\">Ron Levie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1\">Gitta Kutyniok</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale Invariant Domain Generalization Image Recapture Detection. (arXiv:2110.03496v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03496","description":"<p>Recapturing and rebroadcasting of images are common attack methods in\ninsurance frauds and face identification spoofing, and an increasing number of\ndetection techniques were introduced to handle this problem. However, most of\nthem ignored the domain generalization scenario and scale variances, with an\ninferior performance on domain shift situations, and normally were exacerbated\nby intra-domain and inter-domain scale variances. In this paper, we propose a\nscale alignment domain generalization framework (SADG) to address these\nchallenges. First, an adversarial domain discriminator is exploited to minimize\nthe discrepancies of image representation distributions among different\ndomains. Meanwhile, we exploit triplet loss as a local constraint to achieve a\nclearer decision boundary. Moreover, a scale alignment loss is introduced as a\nglobal relationship regularization to force the image representations of the\nsame class across different scales to be undistinguishable. Experimental\nresults on four databases and comparison with state-of-the-art approaches show\nthat better performance can be achieved using our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jinian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Weidong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_H/0/1/0/all/0/1\">Hong Hui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Few-shot Learning Graph Multi-Trajectory Evolution Network for Forecasting Multimodal Baby Connectivity Development from a Baseline Timepoint. (arXiv:2110.03535v1 [q-bio.NC])","link":"http://arxiv.org/abs/2110.03535","description":"<p>Charting the baby connectome evolution trajectory during the first year after\nbirth plays a vital role in understanding dynamic connectivity development of\nbaby brains. Such analysis requires acquisition of longitudinal connectomic\ndatasets. However, both neonatal and postnatal scans are rarely acquired due to\nvarious difficulties. A small body of works has focused on predicting baby\nbrain evolution trajectory from a neonatal brain connectome derived from a\nsingle modality. Although promising, large training datasets are essential to\nboost model learning and to generalize to a multi-trajectory prediction from\ndifferent modalities (i.e., functional and morphological connectomes). Here, we\nunprecedentedly explore the question: Can we design a few-shot learning-based\nframework for predicting brain graph trajectories across different modalities?\nTo this aim, we propose a Graph Multi-Trajectory Evolution Network (GmTE-Net),\nwhich adopts a teacher-student paradigm where the teacher network learns on\npure neonatal brain graphs and the student network learns on simulated brain\ngraphs given a set of different timepoints. To the best of our knowledge, this\nis the first teacher-student architecture tailored for brain graph\nmulti-trajectory growth prediction that is based on few-shot learning and\ngeneralized to graph neural networks (GNNs). To boost the performance of the\nstudent network, we introduce a local topology-aware distillation loss that\nforces the predicted graph topology of the student network to be consistent\nwith the teacher network. Experimental results demonstrate substantial\nperformance gains over benchmark methods. Hence, our GmTE-Net can be leveraged\nto predict atypical brain connectivity trajectory evolution across various\nmodalities. Our code is available at https: //github.com/basiralab/GmTE-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Bessadok_A/0/1/0/all/0/1\">Alaa Bessadok</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nebli_A/0/1/0/all/0/1\">Ahmed Nebli</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Mahjoub_M/0/1/0/all/0/1\">Mohamed Ali Mahjoub</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Li_G/0/1/0/all/0/1\">Gang Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lin_W/0/1/0/all/0/1\">Weili Lin</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rekik_I/0/1/0/all/0/1\">Islem Rekik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAR: Region-Aware Point Cloud Registration. (arXiv:2110.03544v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03544","description":"<p>This paper concerns the research problem of point cloud registration to find\nthe rigid transformation to optimally align the source point set with the\ntarget one. Learning robust point cloud registration models with deep neural\nnetworks has emerged as a powerful paradigm, offering promising performance in\npredicting the global geometric transformation for a pair of point sets.\nExisting methods firstly leverage an encoder to regress a latent shape\nembedding, which is then decoded into a shape-conditioned transformation via\nconcatenation-based conditioning. However, different regions of a 3D shape vary\nin their geometric structures which makes it more sense that we have a\nregion-conditioned transformation instead of the shape-conditioned one. In this\npaper we present a \\underline{R}egion-\\underline{A}ware point cloud\n\\underline{R}egistration, denoted as RAR, to predict transformation for\npairwise point sets in the self-supervised learning fashion. More specifically,\nwe develop a novel region-aware decoder (RAD) module that is formed with an\nimplicit neural region representation parameterized by neural networks. The\nimplicit neural region representation is learned with a self-supervised 3D\nshape reconstruction loss without the need for region labels. Consequently, the\nregion-aware decoder (RAD) module guides the training of the region-aware\ntransformation (RAT) module and region-aware weight (RAW) module, which predict\nthe transforms and weights for different regions respectively. The global\ngeometric transformation from source point set to target one is then formed by\nthe weighted fusion of region-aware transforms. Compared to the\nstate-of-the-art approaches, our experiments show that our RAR achieves\nsuperior registration performance over various benchmark datasets (e.g.\nModelNet40).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yu Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Human-Object Interaction Detection in Video via Contrastive Spatiotemporal Regions. (arXiv:2110.03562v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03562","description":"<p>We introduce the task of weakly supervised learning for detecting human and\nobject interactions in videos. Our task poses unique challenges as a system\ndoes not know what types of human-object interactions are present in a video or\nthe actual spatiotemporal location of the human and the object. To address\nthese challenges, we introduce a contrastive weakly supervised training loss\nthat aims to jointly associate spatiotemporal regions in a video with an action\nand object vocabulary and encourage temporal continuity of the visual\nappearance of moving objects as a form of self-supervision. To train our model,\nwe introduce a dataset comprising over 6.5k videos with human-object\ninteraction annotations that have been semi-automatically curated from sentence\ncaptions associated with the videos. We demonstrate improved performance over\nweakly supervised baselines adapted to our task on our video dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_B/0/1/0/all/0/1\">Bryan Russell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Simple Vision Algorithm for Detecting the Enzymic Browning Defects in Golden Delicious Apples. (arXiv:2110.03574v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03574","description":"<p>In this work, a simple vision algorithm is designed and implemented to\nextract and identify the surface defects on the Golden Delicious apples caused\nby the enzymic browning process. 34 Golden Delicious apples were selected for\nthe experiments, of which 17 had enzymic browning defects and the other 17 were\nsound. The image processing part of the proposed vision algorithm extracted the\ndefective surface area of the apples with high accuracy of 97.15%. The area and\nmean of the segmented images were selected as the 2x1 feature vectors to feed\ninto a designed artificial neural network. The analysis based on the above\nfeatures indicated that the images with a mean less than 0.0065 did not belong\nto the defective apples; rather, they were extracted as part of the calyx and\nstem of the healthy apples. The classification accuracy of the neural network\napplied in this study was 99.19%\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balanji_H/0/1/0/all/0/1\">Hamid Majidi Balanji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating Image Depth in the Comics Domain. (arXiv:2110.03575v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03575","description":"<p>Estimating the depth of comics images is challenging as such images a) are\nmonocular; b) lack ground-truth depth annotations; c) differ across different\nartistic styles; d) are sparse and noisy. We thus, use an off-the-shelf\nunsupervised image to image translation method to translate the comics images\nto natural ones and then use an attention-guided monocular depth estimator to\npredict their depth. This lets us leverage the depth annotations of existing\nnatural images to train the depth estimator. Furthermore, our model learns to\ndistinguish between text and images in the comics panels to reduce text-based\nartefacts in the depth estimates. Our method consistently outperforms the\nexisting state-ofthe-art approaches across all metrics on both the DCM and\neBDtheque images. Finally, we introduce a dataset to evaluate depth prediction\non comics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_D/0/1/0/all/0/1\">Deblina Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Everaert_M/0/1/0/all/0/1\">Martin Everaert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susstrunk_S/0/1/0/all/0/1\">Sabine S&#xfc;sstrunk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Accurate Cross-Domain In-Bed Human Pose Estimation. (arXiv:2110.03578v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03578","description":"<p>Human behavioral monitoring during sleep is essential for various medical\napplications. Majority of the contactless human pose estimation algorithms are\nbased on RGB modality, causing ineffectiveness in in-bed pose estimation due to\nocclusions by blankets and varying illumination conditions. Long-wavelength\ninfrared (LWIR) modality based pose estimation algorithms overcome the\naforementioned challenges; however, ground truth pose generations by a human\nannotator under such conditions are not feasible. A feasible solution to\naddress this issue is to transfer the knowledge learned from images with pose\nlabels and no occlusions, and adapt it towards real world conditions\n(occlusions due to blankets). In this paper, we propose a novel learning\nstrategy comprises of two-fold data augmentation to reduce the cross-domain\ndiscrepancy and knowledge distillation to learn the distribution of unlabeled\nimages in real world conditions. Our experiments and analysis show the\neffectiveness of our approach over multiple standard human pose estimation\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Afham_M/0/1/0/all/0/1\">Mohamed Afham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haputhanthri_U/0/1/0/all/0/1\">Udith Haputhanthri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradeepkumar_J/0/1/0/all/0/1\">Jathurshan Pradeepkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandakumar_M/0/1/0/all/0/1\">Mithunjha Anandakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1\">Ashwin De Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edussooriya_C/0/1/0/all/0/1\">Chamira Edussooriya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A transformer-based deep learning approach for classifying brain metastases into primary organ sites using clinical whole brain MRI images. (arXiv:2110.03588v1 [eess.IV])","link":"http://arxiv.org/abs/2110.03588","description":"<p>The treatment decisions for brain metastatic disease are driven by knowledge\nof the primary organ site cancer histology, often requiring invasive biopsy.\nThis study aims to develop a novel deep learning approach for accurate and\nrapid non-invasive identification of brain metastatic tumor histology with\nconventional whole-brain MRI. The use of clinical whole-brain data and the\nend-to-end pipeline obviate external human intervention. This IRB-approved\nsingle-site retrospective study was comprised of patients (n=1,293) referred\nfor MRI treatment-planning and gamma knife radiosurgery from July 2000 to May\n2019. Contrast-enhanced T1-weighted contrast enhanced and\nT2-weighted-Fluid-Attenuated Inversion Recovery brain MRI exams (n=1,428) were\nminimally preprocessed (voxel resolution unification and signal-intensity\nrescaling/normalization), requiring only seconds per an MRI scan, and input\ninto the proposed deep learning workflow for tumor segmentation, modality\ntransfer, and primary site classification associated with brain metastatic\ndisease in one of four classes (lung, melanoma, renal, and other). Ten-fold\ncross-validation generated the overall AUC of 0.941, lung class AUC of 0.899,\nmelanoma class AUC of 0.882, renal class AUC of 0.870, and other class AUC of\n0.885. It is convincingly established that whole-brain imaging features would\nbe sufficiently discriminative to allow accurate diagnosis of the primary organ\nsite of malignancy. Our end-to-end deep learning-based radiomic method has a\ngreat translational potential for classifying metastatic tumor types using\nwhole-brain MRI images, without additional human intervention. Further\nrefinement may offer invaluable tools to expedite primary organ site cancer\nidentification for treatment of brain metastatic disease and improvement of\npatient outcomes and survival.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Namjoshi_S/0/1/0/all/0/1\">Sanjeev V. Namjoshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McTyre_E/0/1/0/all/0/1\">Emory McTyre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Topaloglu_U/0/1/0/all/0/1\">Umit Topaloglu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barcus_R/0/1/0/all/0/1\">Richard Barcus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_M/0/1/0/all/0/1\">Michael D. Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cramer_C/0/1/0/all/0/1\">Christina K. Cramer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Debinski_W/0/1/0/all/0/1\">Waldemar Debinski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gurcan_M/0/1/0/all/0/1\">Metin N. Gurcan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lesser_G/0/1/0/all/0/1\">Glenn J. Lesser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_H/0/1/0/all/0/1\">Hui-Kuan Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munden_R/0/1/0/all/0/1\">Reginald F. Munden</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pasche_B/0/1/0/all/0/1\">Boris C. Pasche</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sai_K/0/1/0/all/0/1\">Kiran Kumar Solingapuram Sai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strowd_R/0/1/0/all/0/1\">Roy E. Strowd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tatter_S/0/1/0/all/0/1\">Stephen B. Tatter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watabe_K/0/1/0/all/0/1\">Kounosuke Watabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Whitlow_C/0/1/0/all/0/1\">Christopher T. Whitlow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TranSalNet: Visual saliency prediction using transformers. (arXiv:2110.03593v1 [cs.MM])","link":"http://arxiv.org/abs/2110.03593","description":"<p>Convolutional neural networks (CNNs) have significantly advanced\ncomputational modeling for saliency prediction. However, the inherent inductive\nbiases of convolutional architectures cause insufficient long-range contextual\nencoding capacity, which potentially makes a saliency model less humanlike.\nTransformers have shown great potential in encoding long-range information by\nleveraging the self-attention mechanism. In this paper, we propose a novel\nsaliency model integrating transformer components to CNNs to capture the\nlong-range contextual information. Experimental results show that the new\ncomponents make improvements, and the proposed model achieves promising results\nin predicting saliency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jianxun Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hanhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_D/0/1/0/all/0/1\">David Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saupe_D/0/1/0/all/0/1\">Dietmar Saupe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hantao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Thing to Fool them All: Generating Interpretable, Universal, and Physically-Realizable Adversarial Features. (arXiv:2110.03605v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03605","description":"<p>It is well understood that modern deep networks are vulnerable to adversarial\nattacks. However, conventional methods fail to produce adversarial\nperturbations that are intelligible to humans, and they pose limited threats in\nthe physical world. To study feature-class associations in networks and better\nunderstand the real-world threats they face, we develop feature-level\nadversarial perturbations using deep image generators and a novel optimization\nobjective. We term these feature-fool attacks. We show that they are versatile\nand use them to generate targeted feature-level attacks at the ImageNet scale\nthat are simultaneously interpretable, universal to any source image, and\nphysically-realizable. These attacks can also reveal spurious,\nsemantically-describable feature/class associations, and we use them to guide\nthe design of \"copy/paste\" adversaries in which one natural image is pasted\ninto another to cause a targeted misclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1\">Stephen Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeau_M/0/1/0/all/0/1\">Max Nadeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1\">Gabriel Kreiman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boxhead: A Dataset for Learning Hierarchical Representations. (arXiv:2110.03628v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03628","description":"<p>Disentanglement is hypothesized to be beneficial towards a number of\ndownstream tasks. However, a common assumption in learning disentangled\nrepresentations is that the data generative factors are statistically\nindependent. As current methods are almost solely evaluated on toy datasets\nwhere this ideal assumption holds, we investigate their performance in\nhierarchical settings, a relevant feature of real-world data. In this work, we\nintroduce Boxhead, a dataset with hierarchically structured ground-truth\ngenerative factors. We use this novel dataset to evaluate the performance of\nstate-of-the-art autoencoder-based disentanglement models and observe that\nhierarchical models generally outperform single-layer VAEs in terms of\ndisentanglement of hierarchically arranged factors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yukun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1\">Frederik Tr&#xe4;uble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1\">Andrea Dittadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Contrastive Learning and Pseudolabels to learn representations for Retail Product Image Classification. (arXiv:2110.03639v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03639","description":"<p>Retail product Image classification problems are often few shot\nclassification problems, given retail product classes cannot have the type of\nvariations across images like a cat or dog or tree could have. Previous works\nhave shown different methods to finetune Convolutional Neural Networks to\nachieve better classification accuracy on such datasets. In this work, we try\nto address the problem statement : Can we pretrain a Convolutional Neural\nNetwork backbone which yields good enough representations for retail product\nimages, so that training a simple logistic regression on these representations\ngives us good classifiers ? We use contrastive learning and pseudolabel based\nnoisy student training to learn representations that get accuracy in order of\nfinetuning the entire Convnet backbone for retail product image classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1\">Muktabh Mayank Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Keypoint Matching and Interactive Self Attention Network to verify Retail POSMs. (arXiv:2110.03646v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03646","description":"<p>Point of Sale Materials(POSM) are the merchandising and decoration items that\nare used by companies to communicate product information and offers in retail\nstores. POSMs are part of companies' retail marketing strategy and are often\napplied as stylized window displays around retail shelves. In this work, we\napply computer vision techniques to the task of verification of POSMs in\nsupermarkets by telling if all desired components of window display are present\nin a shelf image. We use Convolutional Neural Network based unsupervised\nkeypoint matching as a baseline to verify POSM components and propose a\nsupervised Neural Network based method to enhance the accuracy of baseline by a\nlarge margin. We also show that the supervised pipeline is not restricted to\nthe POSM material it is trained on and can generalize. We train and evaluate\nour model on a private dataset composed of retail shelf images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seth_H/0/1/0/all/0/1\">Harshita Seth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kant_S/0/1/0/all/0/1\">Sonaal Kant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1\">Muktabh Mayank Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design. (arXiv:2110.03659v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03659","description":"<p>An agent's functionality is largely determined by its design, i.e., skeletal\nstructure and joint attributes (e.g., length, size, strength). However, finding\nthe optimal agent design for a given function is extremely challenging since\nthe problem is inherently combinatorial and the design space is prohibitively\nlarge. Additionally, it can be costly to evaluate each candidate design which\nrequires solving for its optimal controller. To tackle these problems, our key\nidea is to incorporate the design procedure of an agent into its\ndecision-making process. Specifically, we learn a conditional policy that, in\nan episode, first applies a sequence of transform actions to modify an agent's\nskeletal structure and joint attributes, and then applies control actions under\nthe new design. To handle a variable number of joints across designs, we use a\ngraph-based policy where each graph node represents a joint and uses message\npassing with its neighbors to output joint-specific actions. Using policy\ngradient methods, our approach enables first-order optimization of agent design\nand control as well as experience sharing across different designs, which\nimproves sample efficiency tremendously. Experiments show that our approach,\nTransform2Act, outperforms prior methods significantly in terms of convergence\nspeed and final performance. Notably, Transform2Act can automatically discover\nplausible designs similar to giraffes, squids, and spiders. Our project website\nis at https://sites.google.com/view/transform2act.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuda Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhengyi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Gaussian Processes for Few-Shot Segmentation. (arXiv:2110.03674v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03674","description":"<p>Few-shot segmentation is a challenging dense prediction task, which entails\nsegmenting a novel query image given only a small annotated support set. The\nkey problem is thus to design a method that aggregates detailed information\nfrom the support set, while being robust to large variations in appearance and\ncontext. To this end, we propose a few-shot segmentation method based on dense\nGaussian process (GP) regression. Given the support set, our dense GP learns\nthe mapping from local deep image features to mask values, capable of capturing\ncomplex appearance distributions. Furthermore, it provides a principled means\nof capturing uncertainty, which serves as another powerful cue for the final\nsegmentation, obtained by a CNN decoder. Instead of a one-dimensional mask\noutput, we further exploit the end-to-end learning capabilities of our approach\nto learn a high-dimensional output space for the GP. Our approach sets a new\nstate-of-the-art for both 1-shot and 5-shot FSS on the PASCAL-5$^i$ and\nCOCO-20$^i$ benchmarks, achieving an absolute gain of $+14.9$ mIoU in the\nCOCO-20$^i$ 5-shot setting. Furthermore, the segmentation quality of our\napproach scales gracefully when increasing the support set size, while\nachieving robust cross-dataset transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Johnander_J/0/1/0/all/0/1\">Joakim Johnander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edstedt_J/0/1/0/all/0/1\">Johan Edstedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ATISS: Autoregressive Transformers for Indoor Scene Synthesis. (arXiv:2110.03675v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03675","description":"<p>The ability to synthesize realistic and diverse indoor furniture layouts\nautomatically or based on partial input, unlocks many applications, from better\ninteractive 3D tools to data synthesis for training and simulation. In this\npaper, we present ATISS, a novel autoregressive transformer architecture for\ncreating diverse and plausible synthetic indoor environments, given only the\nroom type and its floor plan. In contrast to prior work, which poses scene\nsynthesis as sequence generation, our model generates rooms as unordered sets\nof objects. We argue that this formulation is more natural, as it makes ATISS\ngenerally useful beyond fully automatic room layout synthesis. For example, the\nsame trained model can be used in interactive applications for general scene\ncompletion, partial room re-arrangement with any objects specified by the user,\nas well as object suggestions for any partial room. To enable this, our model\nleverages the permutation equivariance of the transformer when conditioning on\nthe partial scene, and is trained to be permutation-invariant across object\norderings. Our model is trained end-to-end as an autoregressive generative\nmodel using only labeled 3D bounding boxes as supervision. Evaluations on four\nroom types in the 3D-FRONT dataset demonstrate that our model consistently\ngenerates plausible room layouts that are more realistic than existing methods.\nIn addition, it has fewer parameters, is simpler to implement and train and\nruns up to 8 times faster than existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paschalidou_D/0/1/0/all/0/1\">Despoina Paschalidou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_A/0/1/0/all/0/1\">Amlan Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shugrina_M/0/1/0/all/0/1\">Maria Shugrina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreis_K/0/1/0/all/0/1\">Karsten Kreis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Burst Image Restoration and Enhancement. (arXiv:2110.03680v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03680","description":"<p>Modern handheld devices can acquire burst image sequence in a quick\nsuccession. However, the individual acquired frames suffer from multiple\ndegradations and are misaligned due to camera shake and object motions. The\ngoal of Burst Image Restoration is to effectively combine complimentary cues\nacross multiple burst frames to generate high-quality outputs. Towards this\ngoal, we develop a novel approach by solely focusing on the effective\ninformation exchange between burst frames, such that the degradations get\nfiltered out while the actual scene details are preserved and enhanced. Our\ncentral idea is to create a set of \\emph{pseudo-burst} features that combine\ncomplimentary information from all the input burst frames to seamlessly\nexchange information. The pseudo-burst representations encode channel-wise\nfeatures from the original burst images, thus making it easier for the model to\nlearn distinctive information offered by multiple burst frames. However, the\npseudo-burst cannot be successfully created unless the individual burst frames\nare properly aligned to discount inter-frame movements. Therefore, our approach\ninitially extracts preprocessed features from each burst frame and matches them\nusing an edge-boosting burst alignment module. The pseudo-burst features are\nthen created and enriched using multi-scale contextual information. Our final\nstep is to adaptively aggregate information from the pseudo-burst features to\nprogressively increase resolution in multiple stages while merging the\npseudo-burst features. In comparison to existing works that usually follow a\nlate fusion scheme with single-stage upsampling, our approach performs\nfavorably, delivering state of the art performance on burst super-resolution\nand low-light image enhancement tasks. Our codes and models will be released\npublicly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dudhane_A/0/1/0/all/0/1\">Akshay Dudhane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_S/0/1/0/all/0/1\">Syed Waqas Zamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vehicle Image Generation Going Well with The Surroundings. (arXiv:1807.02925v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1807.02925","description":"<p>Since the generative neural networks have made a breakthrough in the image\ngeneration problem, lots of researches on their applications have been studied\nsuch as image restoration, style transfer and image completion. However, there\nhas been few research generating objects in uncontrolled real-world\nenvironments. In this paper, we propose a novel approach for vehicle image\ngeneration in real-world scenes. Using a subnetwork based on a precedent work\nof image completion, our model makes the shape of an object. Details of objects\nare trained by an additional colorization and refinement subnetwork, resulting\nin a better quality of generated objects. Unlike many other works, our method\ndoes not require any segmentation layout but still makes a plausible vehicle in\nthe image. We evaluate our method by using images from Berkeley Deep Drive\n(BDD) and Cityscape datasets, which are widely used for object detection and\nimage segmentation problems. The adequacy of the generated images by the\nproposed method has also been evaluated using a widely utilized object\ndetection algorithm and the FID score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeesoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jangho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jaeyoung Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daesik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Graph Transformer Self-Attention Networks. (arXiv:1909.11855v12 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1909.11855","description":"<p>The transformer has been extensively used in research domains such as\ncomputer vision, image processing, and natural language processing. The\ntransformer, however, has not been actively used in graph neural networks. To\nthis end, we introduce a transformer-based advanced GNN model, named UGformer,\nto learn graph representations. In particular, given an input graph, we present\ntwo UGformer variants. The first variant is to leverage the transformer on a\nset of sampled neighbors for each node, while the second is to leverage the\ntransformer directly on the input graph. Experimental results demonstrate that\nthese two UGformer variants achieve state-of-the-art accuracies on well-known\nbenchmark datasets for graph classification and inductive text classification,\nrespectively. The code is available on Github:\n\\url{https://github.com/daiquocnguyen/Graph-Transformer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dai Quoc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Dinh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hypernetwork-Based Augmentation. (arXiv:2006.06320v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.06320","description":"<p>Data augmentation is an effective technique to improve the generalization of\ndeep neural networks. Recently, AutoAugment proposed a well-designed search\nspace and a search algorithm that automatically finds augmentation policies in\na data-driven manner. However, AutoAugment is computationally intensive. In\nthis paper, we propose an efficient gradient-based search algorithm, called\nHypernetwork-Based Augmentation (HBA), which simultaneously learns model\nparameters and augmentation hyperparameters in a single training. Our HBA uses\na hypernetwork to approximate a population-based training algorithm, which\nenables us to tune augmentation hyperparameters by gradient descent. Besides,\nwe introduce a weight sharing strategy that simplifies our hypernetwork\narchitecture and speeds up our search algorithm. We conduct experiments on\nCIFAR-10, CIFAR-100, SVHN, and ImageNet. Our results show that HBA is\ncompetitive to the state-of-the-art methods in terms of both search speed and\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chih-Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Che-Han Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reintroducing Straight-Through Estimators as Principled Methods for Stochastic Binary Networks. (arXiv:2006.06880v3 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2006.06880","description":"<p>Training neural networks with binary weights and activations is a challenging\nproblem due to the lack of gradients and difficulty of optimization over\ndiscrete weights. Many successful experimental results have been achieved with\nempirical straight-through (ST) approaches, proposing a variety of ad-hoc rules\nfor propagating gradients through non-differentiable activations and updating\ndiscrete weights. At the same time, ST methods can be truly derived as\nestimators in the stochastic binary network (SBN) model with Bernoulli weights.\nWe advance these derivations to a more complete and systematic study. We\nanalyze properties, estimation accuracy, obtain different forms of correct ST\nestimators for activations and weights, explain existing empirical approaches\nand their shortcomings, explain how latent weights arise from the mirror\ndescent method when optimizing over probabilities. This allows to reintroduce\nST methods, long known empirically, as sound approximations, apply them with\nclarity and develop further improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Shekhovtsov_A/0/1/0/all/0/1\">Alexander Shekhovtsov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yanush_V/0/1/0/all/0/1\">Viktor Yanush</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artificial Fingerprinting for Generative Models: Rooting Deepfake Attribution in Training Data. (arXiv:2007.08457v6 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2007.08457","description":"<p>Photorealistic image generation has reached a new level of quality due to the\nbreakthroughs of generative adversarial networks (GANs). Yet, the dark side of\nsuch deepfakes, the malicious use of generated media, raises concerns about\nvisual misinformation. While existing research work on deepfake detection\ndemonstrates high accuracy, it is subject to advances in generation techniques\nand adversarial iterations on detection countermeasure techniques. Thus, we\nseek a proactive and sustainable solution on deepfake detection, that is\nagnostic to the evolution of generative models, by introducing artificial\nfingerprints into the models.\n</p>\n<p>Our approach is simple and effective. We first embed artificial fingerprints\ninto training data, then validate a surprising discovery on the transferability\nof such fingerprints from training data to generative models, which in turn\nappears in the generated deepfakes. Experiments show that our fingerprinting\nsolution (1) holds for a variety of cutting-edge generative models, (2) leads\nto a negligible side effect on generation quality, (3) stays robust against\nimage-level and model-level perturbations, (4) stays hard to be detected by\nadversaries, and (5) converts deepfake detection and attribution into trivial\ntasks and outperforms the recent state-of-the-art baselines. Our solution\ncloses the responsibility loop between publishing pre-trained generative model\ninventions and their possible misuses, which makes it independent of the\ncurrent arms race.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Ning Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skripniuk_V/0/1/0/all/0/1\">Vladislav Skripniuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelnabi_S/0/1/0/all/0/1\">Sahar Abdelnabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-domain semantic segmentation with pyramidal fusion. (arXiv:2009.01636v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.01636","description":"<p>We present our submission to the semantic segmentation contest of the Robust\nVision Challenge held at ECCV 2020. The contest requires submitting the same\nmodel to seven benchmarks from three different domains. Our approach is based\non the SwiftNet architecture with pyramidal fusion. We address inconsistent\ntaxonomies with a single-level 193-dimensional softmax output. We strive to\ntrain with large batches in order to stabilize optimization of a hard\nrecognition problem, and to favour smooth evolution of batchnorm statistics. We\nachieve this by implementing a custom backward step through log-sum-prob loss,\nand by using small crops before freezing the population statistics. Our model\nranks first on the RVC semantic segmentation challenge as well as on the\nWildDash 2 leaderboard. This suggests that pyramidal fusion is competitive not\nonly for efficient inference with lightweight backbones, but also in\nlarge-scale setups for multi-domain application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bevandic_P/0/1/0/all/0/1\">Petra Bevandi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orsic_M/0/1/0/all/0/1\">Marin Or&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grubisic_I/0/1/0/all/0/1\">Ivan Grubi&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saric_J/0/1/0/all/0/1\">Josip &#x160;ari&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1\">Sini&#x161;a &#x160;egvi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Batch Normalization Increases Adversarial Vulnerability and Decreases Adversarial Transferability: A Non-Robust Feature Perspective. (arXiv:2010.03316v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.03316","description":"<p>Batch normalization (BN) has been widely used in modern deep neural networks\n(DNNs) due to improved convergence. BN is observed to increase the model\naccuracy while at the cost of adversarial robustness. There is an increasing\ninterest in the ML community to understand the impact of BN on DNNs, especially\nrelated to the model robustness. This work attempts to understand the impact of\nBN on DNNs from a non-robust feature perspective. Straightforwardly, the\nimproved accuracy can be attributed to the better utilization of useful\nfeatures. It remains unclear whether BN mainly favors learning robust features\n(RFs) or non-robust features (NRFs). Our work presents empirical evidence that\nsupports that BN shifts a model towards being more dependent on NRFs. To\nfacilitate the analysis of such a feature robustness shift, we propose a\nframework for disentangling robust usefulness into robustness and usefulness.\nExtensive analysis under the proposed framework yields valuable insight on the\nDNN behavior regarding robustness, e.g. DNNs first mainly learn RFs and then\nNRFs. The insight that RFs transfer better than NRFs, further inspires simple\ntechniques to strengthen transfer-based black-box attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benz_P/0/1/0/all/0/1\">Philipp Benz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Full-Glow: Fully conditional Glow for more realistic image generation. (arXiv:2012.05846v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.05846","description":"<p>Autonomous agents, such as driverless cars, require large amounts of labeled\nvisual data for their training. A viable approach for acquiring such data is\ntraining a generative model with collected real data, and then augmenting the\ncollected real dataset with synthetic images from the model, generated with\ncontrol of the scene layout and ground truth labeling. In this paper we propose\nFull-Glow, a fully conditional Glow-based architecture for generating plausible\nand realistic images of novel street scenes given a semantic segmentation map\nindicating the scene layout. Benchmark comparisons show our model to outperform\nrecent works in terms of the semantic segmentation performance of a pretrained\nPSPNet. This indicates that images from our model are, to a higher degree than\nfrom other models, similar to real images of the same kinds of scenes and\nobjects, making them suitable as training data for a visual semantic\nsegmentation or object recognition system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sorkhei_M/0/1/0/all/0/1\">Moein Sorkhei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1\">Gustav Eje Henter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1\">Hedvig Kjellstr&#xf6;m</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Two-stage Framework for Compound Figure Separation. (arXiv:2101.09903v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.09903","description":"<p>Scientific literature contains large volumes of complex, unstructured figures\nthat are compound in nature (i.e. composed of multiple images, graphs, and\ndrawings). Separation of these compound figures is critical for information\nretrieval from these figures. In this paper, we propose a new strategy for\ncompound figure separation, which decomposes the compound figures into\nconstituent subfigures while preserving the association between the subfigures\nand their respective caption components. We propose a two-stage framework to\naddress the proposed compound figure separation problem. In particular, the\nsubfigure label detection module detects all subfigure labels in the first\nstage. Then, in the subfigure detection module, the detected subfigure labels\nhelp to detect the subfigures by optimizing the feature selection process and\nproviding the global layout information as extra features. Extensive\nexperiments are conducted to validate the effectiveness and superiority of the\nproposed framework, which improves the detection precision by 9%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Weixin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwenker_E/0/1/0/all/0/1\">Eric Schwenker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spreadbury_T/0/1/0/all/0/1\">Trevor Spreadbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrier_N/0/1/0/all/0/1\">Nicola Ferrier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_M/0/1/0/all/0/1\">Maria K.Y. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cossairt_O/0/1/0/all/0/1\">Oliver Cossairt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Achieving Explainability for Plant Disease Classification with Disentangled Variational Autoencoders. (arXiv:2102.03082v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.03082","description":"<p>Agricultural image recognition tasks are becoming increasingly dependent on\ndeep learning (DL); however, despite the excellent performance of DL, it is\ndifficult to comprehend the type of logic or features of the input image it\nuses during decision making. Knowing the logic or features is highly crucial\nfor result verification, algorithm improvement, training data improvement, and\nknowledge extraction. However, the explanations from the current heatmap-based\nalgorithms are insufficient for the abovementioned requirements. To address\nthis, this paper details the development of a classification and explanation\nmethod based on a variational autoencoder (VAE) architecture, which can\nvisualize the variations of the most important features by visualizing the\ngenerated images that correspond to the variations of those features. Using the\nPlantVillage dataset, an acceptable level of explainability was achieved\nwithout sacrificing the classification accuracy. The proposed method can also\nbe extended to other crops as well as other image classification tasks.\nFurther, application systems using this method for disease identification\ntasks, such as the identification of potato blackleg disease, potato virus Y,\nand other image classification tasks, are currently being developed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Habaragamuwa_H/0/1/0/all/0/1\">Harshana Habaragamuwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oishi_Y/0/1/0/all/0/1\">Yu Oishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_K/0/1/0/all/0/1\">Kenichi Tanaka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VAE Approximation Error: ELBO and Conditional Independence. (arXiv:2102.09310v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.09310","description":"<p>The importance of Variational Autoencoders reaches far beyond standalone\ngenerative models -- the approach is also used for learning latent\nrepresentations and can be generalized to semi-supervised learning. This\nrequires a thorough analysis of their commonly known shortcomings: posterior\ncollapse and approximation errors. This paper analyzes VAE approximation errors\ncaused by the combination of the ELBO objective with the choice of the encoder\nprobability family, in particular under conditional independence assumptions.\nWe identify the subclass of generative models consistent with the encoder\nfamily. We show that the ELBO optimizer is pulled from the likelihood optimizer\ntowards this consistent subset. Furthermore, this subset can not be enlarged,\nand the respective error cannot be decreased, by only considering deeper\nencoder networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlesinger_D/0/1/0/all/0/1\">Dmitrij Schlesinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekhovtsov_A/0/1/0/all/0/1\">Alexander Shekhovtsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flach_B/0/1/0/all/0/1\">Boris Flach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Deep Learning to Automate the Detection of Flaws in Nuclear Fuel Channel UT Scans. (arXiv:2102.13635v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.13635","description":"<p>Nuclear reactor inspections are critical to ensure the safety and reliability\nof a nuclear facility's operation. In Canada, Ultrasonic Testing (UT) is used\nto inspect the health of pressure tubes which are part of Canada's Deuterium\nUranium (CANDU) reactor's fuel channels. Currently, analysis of UT scans is\nperformed by manual visualization and measurement to locate, characterize, and\ndisposition flaws. Therefore, there is motivation to develop an automated\nmethod that is fast and accurate. In this paper, a proof of concept (PoC) that\nautomates the detection of flaws in nuclear fuel channel UT scans using a\nconvolutional neural network (CNN) is presented. The CNN model was trained\nafter constructing a dataset using historical UT scans and the corresponding\ninspection results. The requirement for this prototype was to identify the\nlocation of at least a portion of each flaw in UT scans while minimizing false\npositives (FPs). The proposed CNN model achieves this target by automatically\nidentifying at least a portion of each flaw where further manual analysis is\nperformed to identify the width, the length, and the type of the flaw.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hammad_I/0/1/0/all/0/1\">Issam Hammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simpson_R/0/1/0/all/0/1\">Ryan Simpson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsague_H/0/1/0/all/0/1\">Hippolyte Djonon Tsague</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_S/0/1/0/all/0/1\">Sarah Hall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Architecture Search From Task Similarity Measure. (arXiv:2103.00241v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.00241","description":"<p>In this paper, we propose a neural architecture search framework based on a\nsimilarity measure between some baseline tasks and a target task. We first\ndefine the notion of the task similarity based on the log-determinant of the\nFisher Information matrix. Next, we compute the task similarity from each of\nthe baseline tasks to the target task. By utilizing the relation between a\ntarget and a set of learned baseline tasks, the search space of architectures\nfor the target task can be significantly reduced, making the discovery of the\nbest candidates in the set of possible architectures tractable and efficient,\nin terms of GPU days. This method eliminates the requirement for training the\nnetworks from scratch for a given target task as well as introducing the bias\nin the initialization of the search space from the human domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1\">Cat P. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltani_M/0/1/0/all/0/1\">Mohammadreza Soltani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravier_R/0/1/0/all/0/1\">Robert Ravier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1\">Vahid Tarokh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain and View-point Agnostic Hand Action Recognition. (arXiv:2103.02303v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.02303","description":"<p>Hand action recognition is a special case of action recognition with\napplications in human-robot interaction, virtual reality or life-logging\nsystems. Building action classifiers able to work for such heterogeneous action\ndomains is very challenging. There are very subtle changes across different\nactions from a given application but also large variations across domains (e.g.\nvirtual reality vs life-logging). This work introduces a novel skeleton-based\nhand motion representation model that tackles this problem. The framework we\npropose is agnostic to the application domain or camera recording view-point.\nWhen working on a single domain (intra-domain action classification) our\napproach performs better or similar to current state-of-the-art methods on\nwell-known hand action recognition benchmarks. And, more importantly, when\nperforming hand action recognition for action domains and camera perspectives\nwhich our approach has not been trained for (cross-domain action\nclassification), our proposed framework achieves comparable performance to\nintra-domain state-of-the-art methods. These experiments show the robustness\nand generalization capabilities of our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabater_A/0/1/0/all/0/1\">Alberto Sabater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_I/0/1/0/all/0/1\">I&#xf1;igo Alonso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesano_L/0/1/0/all/0/1\">Luis Montesano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1\">Ana C. Murillo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What's in My LiDAR Odometry Toolbox?. (arXiv:2103.09708v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2103.09708","description":"<p>With the democratization of 3D LiDAR sensors, precise LiDAR odometries and\nSLAM are in high demand. New methods regularly appear, proposing solutions\nranging from small variations in classical algorithms to radically new\nparadigms based on deep learning. Yet it is often difficult to compare these\nmethods, notably due to the few datasets on which the methods can be evaluated\nand compared. Furthermore, their weaknesses are rarely examined, often letting\nthe user discover the hard way whether a method would be appropriate for a use\ncase. In this paper, we review and organize the main 3D LiDAR odometries into\ndistinct categories. We implemented several approaches (geometric based, deep\nlearning based, and hybrid methods) to conduct an in-depth analysis of their\nstrengths and weaknesses on multiple datasets, guiding the reader through the\ndifferent LiDAR odometries available. Implementation of the methods has been\nmade publicly available at https://github.com/Kitware/pyLiDAR-SLAM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dellenbach_P/0/1/0/all/0/1\">Pierre Dellenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deschaud_J/0/1/0/all/0/1\">Jean-Emmanuel Deschaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacquet_B/0/1/0/all/0/1\">Bastien Jacquet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goulette_F/0/1/0/all/0/1\">Fran&#xe7;ois Goulette</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoordiNet: uncertainty-aware pose regressor for reliable vehicle localization. (arXiv:2103.10796v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.10796","description":"<p>In this paper, we investigate visual-based camera re-localization with neural\nnetworks for robotics and autonomous vehicles applications. Our solution is a\nCNN-based algorithm which predicts camera pose (3D translation and 3D rotation)\ndirectly from a single image. It also provides an uncertainty estimate of the\npose. Pose and uncertainty are learned together with a single loss function and\nare fused at test time with an EKF. Furthermore, we propose a new fully\nconvolutional architecture, named CoordiNet, designed to embed some of the\nscene geometry. Our framework outperforms comparable methods on the largest\navailable benchmark, the Oxford RobotCar dataset, with an average error of 8\nmeters where previous best was 19 meters. We have also investigated the\nperformance of our method on large scenes for real time (18 fps) vehicle\nlocalization. In this setup, structure-based methods require a large database,\nand we show that our proposal is a reliable alternative, achieving 29cm median\nerror in a 1.9km loop in a busy urban area\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moreau_A/0/1/0/all/0/1\">Arthur Moreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piasco_N/0/1/0/all/0/1\">Nathan Piasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsishkou_D/0/1/0/all/0/1\">Dzmitry Tsishkou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanciulescu_B/0/1/0/all/0/1\">Bogdan Stanciulescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortelle_A/0/1/0/all/0/1\">Arnaud de La Fortelle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent Forecasting. (arXiv:2103.14023v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2103.14023","description":"<p>Predicting accurate future trajectories of multiple agents is essential for\nautonomous systems, but is challenging due to the complex agent interaction and\nthe uncertainty in each agent's future behavior. Forecasting multi-agent\ntrajectories requires modeling two key dimensions: (1) time dimension, where we\nmodel the influence of past agent states over future states; (2) social\ndimension, where we model how the state of each agent affects others. Most\nprior methods model these two dimensions separately, e.g., first using a\ntemporal model to summarize features over time for each agent independently and\nthen modeling the interaction of the summarized features with a social model.\nThis approach is suboptimal since independent feature encoding over either the\ntime or social dimension can result in a loss of information. Instead, we would\nprefer a method that allows an agent's state at one time to directly affect\nanother agent's state at a future time. To this end, we propose a new\nTransformer, AgentFormer, that jointly models the time and social dimensions.\nThe model leverages a sequence representation of multi-agent trajectories by\nflattening trajectory features across time and agents. Since standard attention\noperations disregard the agent identity of each element in the sequence,\nAgentFormer uses a novel agent-aware attention mechanism that preserves agent\nidentities by attending to elements of the same agent differently than elements\nof other agents. Based on AgentFormer, we propose a stochastic multi-agent\ntrajectory prediction model that can attend to features of any agent at any\nprevious timestep when inferring an agent's future position. The latent intent\nof all agents is also jointly modeled, allowing the stochasticity in one\nagent's behavior to affect other agents. Our method substantially improves the\nstate of the art on well-established pedestrian and autonomous driving\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1\">Xinshuo Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1\">Yanglan Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LatentKeypointGAN: Controlling GANs via Latent Keypoints. (arXiv:2103.15812v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15812","description":"<p>Generative adversarial networks (GANs) have attained photo-realistic quality\nin image generation. However, how to best control the image content remains an\nopen challenge. We introduce LatentKeypointGAN, a two-stage GAN which is\ntrained end-to-end on the classical GAN objective with internal conditioning on\na set of space keypoints. These keypoints have associated appearance embeddings\nthat respectively control the position and style of the generated objects and\ntheir parts. A major difficulty that we address with suitable network\narchitectures and training schemes is disentangling the image into spatial and\nappearance factors without domain knowledge and supervision signals. We\ndemonstrate that LatentKeypointGAN provides an interpretable latent space that\ncan be used to re-arrange the generated images by re-positioning and exchanging\nkeypoint embeddings, such as generating portraits by combining the eyes, nose,\nand mouth from different images. In addition, the explicit generation of\nkeypoints and matching images enables a new, GAN-based method for unsupervised\nkeypoint detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xingzhe He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wandt_B/0/1/0/all/0/1\">Bastian Wandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1\">Helge Rhodin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Contrastive Loss and Attention for GANs. (arXiv:2103.16748v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16748","description":"<p>Generative Adversarial Networks (GANs) produce impressive results on\nunconditional image generation when powered with large-scale image datasets.\nYet generated images are still easy to spot especially on datasets with high\nvariance (e.g. bedroom, church). In this paper, we propose various improvements\nto further push the boundaries in image generation. Specifically, we propose a\nnovel dual contrastive loss and show that, with this loss, discriminator learns\nmore generalized and distinguishable representations to incentivize generation.\nIn addition, we revisit attention and extensively experiment with different\nattention blocks in the generator. We find attention to be still an important\nmodule for successful image generation even though it was not used in the\nrecent state-of-the-art models. Lastly, we study different attention\narchitectures in the discriminator, and propose a reference attention\nmechanism. By combining the strengths of these remedies, we improve the\ncompelling state-of-the-art Fr\\'{e}chet Inception Distance (FID) by at least\n17.5% on several benchmark datasets. We obtain even more significant\nimprovements on compositional synthetic scenes (up to 47.5% in FID).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Ning Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guilin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dundar_A/0/1/0/all/0/1\">Aysegul Dundar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_A/0/1/0/all/0/1\">Andrew Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1\">Larry Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Invariant Adversarial Learning. (arXiv:2104.00322v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.00322","description":"<p>The phenomenon of adversarial examples illustrates one of the most basic\nvulnerabilities of deep neural networks. Among the variety of techniques\nintroduced to surmount this inherent weakness, adversarial training has emerged\nas the most effective strategy to achieve robustness. Typically, this is\nachieved by balancing robust and natural objectives. In this work, we aim to\nfurther optimize the trade-off between robust and standard accuracy by\nenforcing a domain-invariant feature representation. We present a new\nadversarial training method, Domain Invariant Adversarial Learning (DIAL),\nwhich learns a feature representation that is both robust and domain invariant.\nDIAL uses a variant of Domain Adversarial Neural Network (DANN) on the natural\ndomain and its corresponding adversarial domain. In the case where the source\ndomain consists of natural examples and the target domain is the adversarially\nperturbed examples, our method learns a feature representation constrained not\nto discriminate between the natural and adversarial examples, and can therefore\nachieve a more robust representation. Our experiments indicate that our method\nimproves both robustness and standard accuracy, when compared to other\nstate-of-the-art adversarial training methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levi_M/0/1/0/all/0/1\">Matan Levi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attias_I/0/1/0/all/0/1\">Idan Attias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kontorovich_A/0/1/0/all/0/1\">Aryeh Kontorovich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfinityGAN: Towards Infinite-Pixel Image Synthesis. (arXiv:2104.03963v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03963","description":"<p>We present a novel framework, InfinityGAN, for arbitrary-sized image\ngeneration. The task is associated with several key challenges. First, scaling\nexisting models to an arbitrarily large image size is resource-constrained, in\nterms of both computation and availability of large-field-of-view training\ndata. InfinityGAN trains and infers in a seamless patch-by-patch manner with\nlow computational resources. Second, large images should be locally and\nglobally consistent, avoid repetitive patterns, and look realistic. To address\nthese, InfinityGAN disentangles global appearances, local structures, and\ntextures. With this formulation, we can generate images with spatial size and\nlevel of details not attainable before. Experimental evaluation validates that\nInfinityGAN generates images with superior realism compared to baselines and\nfeatures parallelizable inference. Finally, we show several applications\nunlocked by our approach, such as spatial style fusion, multi-modal\noutpainting, and image inbetweening. All applications can be operated with\narbitrary input and output sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chieh Hubert Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yen-Chi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point-Based Modeling of Human Clothing. (arXiv:2104.08230v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08230","description":"<p>We propose a new approach to human clothing modeling based on point clouds.\nWithin this approach, we learn a deep model that can predict point clouds of\nvarious outfits, for various human poses, and for various human body shapes.\nNotably, outfits of various types and topologies can be handled by the same\nmodel. Using the learned model, we can infer the geometry of new outfits from\nas little as a single image, and perform outfit retargeting to new bodies in\nnew poses. We complement our geometric model with appearance modeling that uses\nthe point cloud geometry as a geometric scaffolding and employs neural\npoint-based graphics to capture outfit appearance from videos and to re-render\nthe captured outfits. We validate both geometric modeling and appearance\nmodeling aspects of the proposed approach against recently proposed methods and\nestablish the viability of point-based clothing modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zakharkin_I/0/1/0/all/0/1\">Ilya Zakharkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazur_K/0/1/0/all/0/1\">Kirill Mazur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grigorev_A/0/1/0/all/0/1\">Artur Grigorev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lempitsky_V/0/1/0/all/0/1\">Victor Lempitsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations. (arXiv:2104.14548v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.14548","description":"<p>Self-supervised learning algorithms based on instance discrimination train\nencoders to be invariant to pre-defined transformations of the same instance.\nWhile most methods treat different views of the same image as positives for a\ncontrastive loss, we are interested in using positives from other instances in\nthe dataset. Our method, Nearest-Neighbor Contrastive Learning of visual\nRepresentations (NNCLR), samples the nearest neighbors from the dataset in the\nlatent space, and treats them as positives. This provides more semantic\nvariations than pre-defined transformations.\n</p>\n<p>We find that using the nearest-neighbor as positive in contrastive losses\nimproves performance significantly on ImageNet classification, from 71.7% to\n75.6%, outperforming previous state-of-the-art methods. On semi-supervised\nlearning benchmarks we improve performance significantly when only 1% ImageNet\nlabels are available, from 53.8% to 56.5%. On transfer learning benchmarks our\nmethod outperforms state-of-the-art methods (including supervised learning with\nImageNet) on 8 out of 12 downstream datasets. Furthermore, we demonstrate\nempirically that our method is less reliant on complex data augmentations. We\nsee a relative reduction of only 2.1% ImageNet Top-1 accuracy when we train\nusing only random crops.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dwibedi_D/0/1/0/all/0/1\">Debidatta Dwibedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aytar_Y/0/1/0/all/0/1\">Yusuf Aytar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tompson_J/0/1/0/all/0/1\">Jonathan Tompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sermanet_P/0/1/0/all/0/1\">Pierre Sermanet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. (arXiv:2105.02872v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.02872","description":"<p>This paper addresses the challenge of reconstructing an animatable human\nmodel from a multi-view video. Some recent works have proposed to decompose a\nnon-rigidly deforming scene into a canonical neural radiance field and a set of\ndeformation fields that map observation-space points to the canonical space,\nthereby enabling them to learn the dynamic scene from images. However, they\nrepresent the deformation field as translational vector field or SE(3) field,\nwhich makes the optimization highly under-constrained. Moreover, these\nrepresentations cannot be explicitly controlled by input motions. Instead, we\nintroduce neural blend weight fields to produce the deformation fields. Based\non the skeleton-driven deformation, blend weight fields are used with 3D human\nskeletons to generate observation-to-canonical and canonical-to-observation\ncorrespondences. Since 3D human skeletons are more observable, they can\nregularize the learning of deformation fields. Moreover, the learned blend\nweight fields can be combined with input skeletal motions to generate new\ndeformation fields to animate the human model. Experiments show that our\napproach significantly outperforms recent human synthesis methods. The code and\nsupplementary materials are available at\nhttps://zju3dv.github.io/animatable_nerf/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Sida Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junting Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qianqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shangzhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_Q/0/1/0/all/0/1\">Qing Shuai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLC: Accurate, Multi-purpose Learnt Low-dimensional Binary Codes. (arXiv:2106.01487v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.01487","description":"<p>Learning binary representations of instances and classes is a classical\nproblem with several high potential applications. In modern settings, the\ncompression of high-dimensional neural representations to low-dimensional\nbinary codes is a challenging task and often require large bit-codes to be\naccurate. In this work, we propose a novel method for Learning Low-dimensional\nbinary Codes (LLC) for instances as well as classes. Our method does not\nrequire any side-information, like annotated attributes or label meta-data, and\nlearns extremely low-dimensional binary codes (~20 bits for ImageNet-1K). The\nlearnt codes are super-efficient while still ensuring nearly optimal\nclassification accuracy for ResNet50 on ImageNet-1K. We demonstrate that the\nlearnt codes capture intrinsically important features in the data, by\ndiscovering an intuitive taxonomy over classes. We further quantitatively\nmeasure the quality of our codes by applying it to the efficient image\nretrieval as well as out-of-distribution (OOD) detection problems. For\nImageNet-100 retrieval problem, our learnt binary codes outperform 16 bit\nHashNet using only 10 bits and also are as accurate as 10 dimensional real\nrepresentations. Finally, our learnt binary codes can perform OOD detection,\nout-of-the-box, as accurately as a baseline that needs ~3000 samples to tune\nits threshold, while we require none. Code is open-sourced at\nhttps://github.com/RAIVNLab/LLC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1\">Aditya Kusupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallingford_M/0/1/0/all/0/1\">Matthew Wallingford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1\">Vivek Ramanujan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somani_R/0/1/0/all/0/1\">Raghav Somani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jae Sung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pillutla_K/0/1/0/all/0/1\">Krishna Pillutla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Prateek Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1\">Sham Kakade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Flow Regularization: Improving Structured Sparsity in Deep Neural Networks. (arXiv:2106.02914v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02914","description":"<p>Pruning is a model compression method that removes redundant parameters in\ndeep neural networks (DNNs) while maintaining accuracy. Most available filter\npruning methods require complex treatments such as iterative pruning, features\nstatistics/ranking, or additional optimization designs in the training process.\nIn this paper, we propose a simple and effective regularization strategy from a\nnew perspective of evolution of features, which we call feature flow\nregularization (FFR), for improving structured sparsity and filter pruning in\nDNNs. Specifically, FFR imposes controls on the gradient and curvature of\nfeature flow along the neural network, which implicitly increases the sparsity\nof the parameters. The principle behind FFR is that coherent and smooth\nevolution of features will lead to an efficient network that avoids redundant\nparameters. The high structured sparsity obtained from FFR enables us to prune\nfilters effectively. Experiments with VGGNets, ResNets on CIFAR-10/100, and\nTiny ImageNet datasets demonstrate that FFR can significantly improve both\nunstructured and structured sparsity. Our pruning results in terms of reduction\nof parameters and FLOPs are comparable to or even better than those of\nstate-of-the-art pruning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yuan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Luchan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yang Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Transformer: A unified architecture for predicting multiple agent trajectories. (arXiv:2106.08417v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.08417","description":"<p>Predicting the motion of multiple agents is necessary for planning in dynamic\nenvironments. This task is challenging for autonomous driving since agents\n(e.g. vehicles and pedestrians) and their associated behaviors may be diverse\nand influence one another. Most prior work have focused on predicting\nindependent futures for each agent based on all past motion, and planning\nagainst these independent predictions. However, planning against independent\npredictions can make it challenging to represent the future interaction\npossibilities between different agents, leading to sub-optimal planning. In\nthis work, we formulate a model for predicting the behavior of all agents\njointly, producing consistent futures that account for interactions between\nagents. Inspired by recent language modeling approaches, we use a masking\nstrategy as the query to our model, enabling one to invoke a single model to\npredict agent behavior in many ways, such as potentially conditioned on the\ngoal or full future trajectory of the autonomous vehicle or the behavior of\nother agents in the environment. Our model architecture employs attention to\ncombine features across road elements, agent interactions, and time steps. We\nevaluate our approach on autonomous driving datasets for both marginal and\njoint motion prediction, and achieve state of the art performance across two\npopular datasets. Through combining a scene-centric approach, agent permutation\nequivariant model, and a sequence masking strategy, we show that our model can\nunify a variety of motion prediction tasks from joint motion predictions to\nconditioned prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ngiam_J/0/1/0/all/0/1\">Jiquan Ngiam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caine_B/0/1/0/all/0/1\">Benjamin Caine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1\">Vijay Vasudevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1\">Hao-Tien Lewis Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_J/0/1/0/all/0/1\">Jeffrey Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bewley_A/0/1/0/all/0/1\">Alex Bewley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venugopal_A/0/1/0/all/0/1\">Ashish Venugopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_D/0/1/0/all/0/1\">David Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapp_B/0/1/0/all/0/1\">Ben Sapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1\">Jonathon Shlens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding the Security of Deepfake Detection. (arXiv:2107.02045v3 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2107.02045","description":"<p>Deepfakes pose growing challenges to the trust of information on the\nInternet. Thus, detecting deepfakes has attracted increasing attentions from\nboth academia and industry. State-of-the-art deepfake detection methods consist\nof two key components, i.e., face extractor and face classifier, which extract\nthe face region in an image and classify it to be real/fake, respectively.\nExisting studies mainly focused on improving the detection performance in\nnon-adversarial settings, leaving security of deepfake detection in adversarial\nsettings largely unexplored. In this work, we aim to bridge the gap. In\nparticular, we perform a systematic measurement study to understand the\nsecurity of the state-of-the-art deepfake detection methods in adversarial\nsettings. We use two large-scale public deepfakes data sources including\nFaceForensics++ and Facebook Deepfake Detection Challenge, where the deepfakes\nare fake face images; and we train state-of-the-art deepfake detection methods.\nThese detection methods can achieve 0.94--0.99 accuracies in non-adversarial\nsettings on these datasets. However, our measurement results uncover multiple\nsecurity limitations of the deepfake detection methods in adversarial settings.\nFirst, we find that an attacker can evade a face extractor, i.e., the face\nextractor fails to extract the correct face regions, via adding small Gaussian\nnoise to its deepfake images. Second, we find that a face classifier trained\nusing deepfakes generated by one method cannot detect deepfakes generated by\nanother method, i.e., an attacker can evade detection via generating deepfakes\nusing a new method. Third, we find that an attacker can leverage backdoor\nattacks developed by the adversarial machine learning community to evade a face\nclassifier. Our results highlight that deepfake detection should consider the\nadversarial nature of the problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaoyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation. (arXiv:2107.07651v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.07651","description":"<p>Large-scale vision and language representation learning has shown promising\nimprovements on various vision-language tasks. Most existing methods employ a\ntransformer-based multimodal encoder to jointly model visual tokens\n(region-based image features) and word tokens. Because the visual tokens and\nword tokens are unaligned, it is challenging for the multimodal encoder to\nlearn image-text interactions. In this paper, we introduce a contrastive loss\nto ALign the image and text representations BEfore Fusing (ALBEF) them through\ncross-modal attention, which enables more grounded vision and language\nrepresentation learning. Unlike most existing methods, our method does not\nrequire bounding box annotations nor high-resolution images. In order to\nimprove learning from noisy web data, we propose momentum distillation, a\nself-training method which learns from pseudo-targets produced by a momentum\nmodel. We provide a theoretical analysis of ALBEF from a mutual information\nmaximization perspective, showing that different training tasks can be\ninterpreted as different ways to generate views for an image-text pair. ALBEF\nachieves state-of-the-art performance on multiple downstream vision-language\ntasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained\non orders of magnitude larger datasets. On VQA and NLVR$^2$, ALBEF achieves\nabsolute improvements of 2.37% and 3.84% compared to the state-of-the-art,\nwhile enjoying faster inference speed. Code and pre-trained models are\navailable at https://github.com/salesforce/ALBEF/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selvaraju_R/0/1/0/all/0/1\">Ramprasaath R. Selvaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotmare_A/0/1/0/all/0/1\">Akhilesh Deepak Gotmare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PTT: Point-Track-Transformer Module for 3D Single Object Tracking in Point Clouds. (arXiv:2108.06455v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06455","description":"<p>3D single object tracking is a key issue for robotics. In this paper, we\npropose a transformer module called Point-Track-Transformer (PTT) for point\ncloud-based 3D single object tracking. PTT module contains three blocks for\nfeature embedding, position encoding, and self-attention feature computation.\nFeature embedding aims to place features closer in the embedding space if they\nhave similar semantic information. Position encoding is used to encode\ncoordinates of point clouds into high dimension distinguishable features.\nSelf-attention generates refined attention features by computing attention\nweights. Besides, we embed the PTT module into the open-source state-of-the-art\nmethod P2B to construct PTT-Net. Experiments on the KITTI dataset reveal that\nour PTT-Net surpasses the state-of-the-art by a noticeable margin (~10%).\nAdditionally, PTT-Net could achieve real-time performance (~40FPS) on NVIDIA\n1080Ti GPU. Our code is open-sourced for the robotics community at\nhttps://github.com/shanjiayao/PTT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shan_J/0/1/0/all/0/1\">Jiayao Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sifan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yubo Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Adversarially-Enhanced k-Nearest Neighbors. (arXiv:2108.06797v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.06797","description":"<p>Recent works have theoretically and empirically shown that deep neural\nnetworks (DNNs) have an inherent vulnerability to small perturbations. Applying\nthe Deep k-Nearest Neighbors (DkNN) classifier, we observe a dramatically\nincreasing robustness-accuracy trade-off as the layer goes deeper. In this\nwork, we propose a Deep Adversarially-Enhanced k-Nearest Neighbors (DAEkNN)\nmethod which achieves higher robustness than DkNN and mitigates the\nrobustness-accuracy trade-off in deep layers through two key elements. First,\nDAEkNN is based on an adversarially trained model. Second, DAEkNN makes\npredictions by leveraging a weighted combination of benign and adversarial\ntraining data. Empirically, we find that DAEkNN improves both the robustness\nand the robustness-accuracy trade-off on MNIST and CIFAR-10 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ren Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hero_A/0/1/0/all/0/1\">Alfred Hero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Scaling Law for Synthetic-to-Real Transfer: How Much Is Your Pre-training Effective?. (arXiv:2108.11018v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.11018","description":"<p>Synthetic-to-real transfer learning is a framework in which a synthetically\ngenerated dataset is used to pre-train a model to improve its performance on\nreal vision tasks. The most significant advantage of using synthetic images is\nthat the ground-truth labels are automatically available, enabling unlimited\nexpansion of the data size without human cost. However, synthetic data may have\na huge domain gap, in which case increasing the data size does not improve the\nperformance. How can we know that? In this study, we derive a simple scaling\nlaw that predicts the performance from the amount of pre-training data. By\nestimating the parameters of the law, we can judge whether we should increase\nthe data or change the setting of image synthesis. Further, we analyze the\ntheory of transfer learning by considering learning dynamics and confirm that\nthe derived generalization bound is consistent with our empirical findings. We\nempirically validated our scaling law on various experimental settings of\nbenchmark tasks, model sizes, and complexities of synthetic images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mikami_H/0/1/0/all/0/1\">Hiroaki Mikami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukumizu_K/0/1/0/all/0/1\">Kenji Fukumizu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murai_S/0/1/0/all/0/1\">Shogo Murai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_S/0/1/0/all/0/1\">Shuji Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kikuchi_Y/0/1/0/all/0/1\">Yuta Kikuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_T/0/1/0/all/0/1\">Taiji Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maeda_S/0/1/0/all/0/1\">Shin-ichi Maeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_K/0/1/0/all/0/1\">Kohei Hayashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eyes Tell All: Irregular Pupil Shapes Reveal GAN-generated Faces. (arXiv:2109.00162v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00162","description":"<p>Generative adversary network (GAN) generated high-realistic human faces have\nbeen used as profile images for fake social media accounts and are visually\nchallenging to discern from real ones. In this work, we show that GAN-generated\nfaces can be exposed via irregular pupil shapes. This phenomenon is caused by\nthe lack of physiological constraints in the GAN models. We demonstrate that\nsuch artifacts exist widely in high-quality GAN-generated faces and further\ndescribe an automatic method to extract the pupils from two eyes and analysis\ntheir shapes for exposing the GAN-generated faces. Qualitative and quantitative\nevaluations of our method suggest its simplicity and effectiveness in\ndistinguishing GAN-generated faces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Brand Label Albedo Extraction of eCommerce Products using Generative Adversarial Network. (arXiv:2109.02929v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02929","description":"<p>In this paper we present our solution to extract albedo of branded labels for\ne-commerce products. To this end, we generate a large-scale photo-realistic\nsynthetic data set for albedo extraction followed by training a generative\nmodel to translate images with diverse lighting conditions to albedo. We\nperformed an extensive evaluation to test the generalisation of our method to\nin-the-wild images. From the experimental results, we observe that our solution\ngeneralises well compared to the existing method both in the unseen rendered\nimages as well as in the wild image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sapkota_S/0/1/0/all/0/1\">Suman Sapkota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juneja_M/0/1/0/all/0/1\">Manish Juneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keleras_L/0/1/0/all/0/1\">Laurynas Keleras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotwal_P/0/1/0/all/0/1\">Pranav Kotwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattarai_B/0/1/0/all/0/1\">Binod Bhattarai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Compositional Feature Embedding and Similarity Metric for Ultra-Fine-Grained Visual Categorization. (arXiv:2109.12380v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12380","description":"<p>Fine-grained visual categorization (FGVC), which aims at classifying objects\nwith small inter-class variances, has been significantly advanced in recent\nyears. However, ultra-fine-grained visual categorization (ultra-FGVC), which\ntargets at identifying subclasses with extremely similar patterns, has not\nreceived much attention. In ultra-FGVC datasets, the samples per category are\nalways scarce as the granularity moves down, which will lead to overfitting\nproblems. Moreover, the difference among different categories is too subtle to\ndistinguish even for professional experts. Motivated by these issues, this\npaper proposes a novel compositional feature embedding and similarity metric\n(CECS). Specifically, in the compositional feature embedding module, we\nrandomly select patches in the original input image, and these patches are then\nreplaced by patches from the images of different categories or masked out. Then\nthe replaced and masked images are used to augment the original input images,\nwhich can provide more diverse samples and thus largely alleviate overfitting\nproblem resulted from limited training samples. Besides, learning with diverse\nsamples forces the model to learn not only the most discriminative features but\nalso other informative features in remaining regions, enhancing the\ngeneralization and robustness of the model. In the compositional similarity\nmetric module, a new similarity metric is developed to improve the\nclassification performance by narrowing the intra-category distance and\nenlarging the inter-category distance. Experimental results on two ultra-FGVC\ndatasets and one FGVC dataset with recent benchmark methods consistently\ndemonstrate that the proposed CECS method achieves the state of-the-art\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yajie Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miaohua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaohan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yongsheng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Student Helping Teacher: Teacher Evolution via Self-Knowledge Distillation. (arXiv:2110.00329v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00329","description":"<p>Knowledge distillation usually transfers the knowledge from a pre-trained\ncumbersome teacher network to a compact student network, which follows the\nclassical teacher-teaching-student paradigm. Based on this paradigm, previous\nmethods mostly focus on how to efficiently train a better student network for\ndeployment. Different from the existing practices, in this paper, we propose a\nnovel student-helping-teacher formula, Teacher Evolution via Self-Knowledge\nDistillation (TESKD), where the target teacher (for deployment) is learned with\nthe help of multiple hierarchical students by sharing the structural backbone.\nThe diverse feedback from multiple students allows the teacher to improve\nitself through the shared feature representations. The effectiveness of our\nproposed framework is demonstrated by extensive experiments with various\nnetwork settings on two standard benchmarks including CIFAR-100 and ImageNet.\nNotably, when trained together with our proposed method, ResNet-18 achieves\n79.15% and 71.14% accuracy on CIFAR-100 and ImageNet, outperforming the\nbaseline results by 4.74% and 1.43%, respectively. The code is available at:\nhttps://github.com/zhengli427/TESKD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lingfeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhigeng Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Unfolding Total Variation Network for Low-Light Image Enhancement. (arXiv:2110.00984v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.00984","description":"<p>Real-world low-light images suffer from two main degradations, namely,\ninevitable noise and poor visibility. Since the noise exhibits different\nlevels, its estimation has been implemented in recent works when enhancing\nlow-light images from raw Bayer space. When it comes to sRGB color space, the\nnoise estimation becomes more complicated due to the effect of the image\nprocessing pipeline. Nevertheless, most existing enhancing algorithms in sRGB\nspace only focus on the low visibility problem or suppress the noise under a\nhypothetical noise level, leading them impractical due to the lack of\nrobustness. To address this issue,we propose an adaptive unfolding total\nvariation network (UTVNet), which approximates the noise level from the real\nsRGB low-light image by learning the balancing parameter in the model-based\ndenoising method with total variation regularization. Meanwhile, we learn the\nnoise level map by unrolling the corresponding minimization process for\nproviding the inferences of smoothness and fidelity constraints. Guided by the\nnoise level map, our UTVNet can recover finer details and is more capable to\nsuppress noise in real captured low-light scenes. Extensive experiments on\nreal-world low-light images clearly demonstrate the superior performance of\nUTVNet over state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_C/0/1/0/all/0/1\">Chuanjun Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_D/0/1/0/all/0/1\">Daming Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_W/0/1/0/all/0/1\">Wentian Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Online Visual Invariances for Novel Objects via Supervised and Self-Supervised Training. (arXiv:2110.01476v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.01476","description":"<p>Humans can identify objects following various spatial transformations such as\nscale and viewpoint. This extends to novel objects, after a single presentation\nat a single pose, sometimes referred to as online invariance. CNNs have been\nproposed as a compelling model of human vision, but their ability to identify\nobjects across transformations is typically tested on held-out samples of\ntrained categories after extensive data augmentation. This paper assesses\nwhether standard CNNs can support human-like online invariance by training\nmodels to recognize images of synthetic 3D objects that undergo several\ntransformations: rotation, scaling, translation, brightness, contrast, and\nviewpoint. Through the analysis of models' internal representations, we show\nthat standard supervised CNNs trained on transformed objects can acquire strong\ninvariances on novel classes even when trained with as few as 50 objects taken\nfrom 10 classes. This extended to a different dataset of photographs of real\nobjects. We also show that these invariances can be acquired in a\nself-supervised way, through solving the same/different task. We suggest that\nthis latter approach may be similar to how humans acquire invariances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biscione_V/0/1/0/all/0/1\">Valerio Biscione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowers_J/0/1/0/all/0/1\">Jeffrey S. Bowers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantified Facial Expressiveness for Affective Behavior Analytics. (arXiv:2110.01758v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.01758","description":"<p>The quantified measurement of facial expressiveness is crucial to analyze\nhuman affective behavior at scale. Unfortunately, methods for expressiveness\nquantification at the video frame-level are largely unexplored, unlike the\nstudy of discrete expression. In this work, we propose an algorithm that\nquantifies facial expressiveness using a bounded, continuous expressiveness\nscore using multimodal facial features, such as action units (AUs), landmarks,\nhead pose, and gaze. The proposed algorithm more heavily weights AUs with high\nintensities and large temporal changes. The proposed algorithm can compute the\nexpressiveness in terms of discrete expression, and can be used to perform\ntasks including facial behavior tracking and subjectivity quantification in\ncontext. Our results on benchmark datasets show the proposed algorithm is\neffective in terms of capturing temporal changes and expressiveness, measuring\nsubjective differences in context, and extracting useful insight.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uddin_M/0/1/0/all/0/1\">Md Taufeeq Uddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canavan_S/0/1/0/all/0/1\">Shaun Canavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"2nd Place Solution to Google Landmark Recognition Competition 2021. (arXiv:2110.02638v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02638","description":"<p>As Transformer-based architectures have recently shown encouraging progresses\nin computer vision. In this work, we present the solution to the Google\nLandmark Recognition 2021 Challenge held on Kaggle, which is an improvement on\nour last year's solution by changing three designs, including (1) Using Swin\nand CSWin as backbone for feature extraction, (2) Train on full GLDv2, and (3)\nUsing full GLDv2 images as index image set for kNN search.\n</p>\n<p>With these modifications, our solution significantly improves last year\nsolution on this year competition. Our full pipeline, after ensembling Swin,\nCSWin, EfficientNet B7 models, scores 0.4907 on the private leaderboard which\nhelp us to get the 2nd place in the competition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Shubin Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3rd Place Solution to Google Landmark Recognition Competition 2021. (arXiv:2110.02794v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02794","description":"<p>In this paper, we show our solution to the Google Landmark Recognition 2021\nCompetition. Firstly, embeddings of images are extracted via various\narchitectures (i.e. CNN-, Transformer- and hybrid-based), which are optimized\nby ArcFace loss. Then we apply an efficient pipeline to re-rank predictions by\nadjusting the retrieval score with classification logits and non-landmark\ndistractors. Finally, the ensembled model scores 0.489 on the private\nleaderboard, achieving the 3rd place in the 2021 edition of the Google Landmark\nRecognition Competition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yuxiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_T/0/1/0/all/0/1\">Tianling Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yanyu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Cropped versus Uncropped Training Sets in Tabular Structure Detection. (arXiv:2110.02933v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02933","description":"<p>Automated document processing for tabular information extraction is highly\ndesired in many organizations, from industry to government. Prior works have\naddressed this problem under table detection and table structure detection\ntasks. Proposed solutions leveraging deep learning approaches have been giving\npromising results in these tasks. However, the impact of dataset structures on\ntable structure detection has not been investigated. In this study, we provide\na comparison of table structure detection performance with cropped and\nuncropped datasets. The cropped set consists of only table images that are\ncropped from documents assuming tables are detected perfectly. The uncropped\nset consists of regular document images. Experiments show that deep learning\nmodels can improve the detection performance by up to 9% in average precision\nand average recall on the cropped versions. Furthermore, the impact of cropped\nimages is negligible under the Intersection over Union (IoU) values of 50%-70%\nwhen compared to the uncropped versions. However, beyond 70% IoU thresholds,\ncropped datasets provide significantly higher detection performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akkaya_Y/0/1/0/all/0/1\">Yakup Akkaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simsek_M/0/1/0/all/0/1\">Murat Simsek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kantarci_B/0/1/0/all/0/1\">Burak Kantarci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Shahzad Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}