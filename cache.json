{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-11T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Behind the Mask: Demographic bias in name detection for PII masking. (arXiv:2205.04505v1 [cs.CL])","link":"http://arxiv.org/abs/2205.04505","description":"<p>Many datasets contain personally identifiable information, or PII, which\nposes privacy risks to individuals. PII masking is commonly used to redact\npersonal information such as names, addresses, and phone numbers from text\ndata. Most modern PII masking pipelines involve machine learning algorithms.\nHowever, these systems may vary in performance, such that individuals from\nparticular demographic groups bear a higher risk for having their personal\ninformation exposed. In this paper, we evaluate the performance of three\noff-the-shelf PII masking systems on name detection and redaction. We generate\ndata using names and templates from the customer service domain. We find that\nan open-source RoBERTa-based system shows fewer disparities than the commercial\nmodels we test. However, all systems demonstrate significant differences in\nerror rate based on demographics. In particular, the highest error rates\noccurred for names associated with Black and Asian/Pacific Islander\nindividuals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mansfield_C/0/1/0/all/0/1\">Courtney Mansfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paullada_A/0/1/0/all/0/1\">Amandalynne Paullada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howell_K/0/1/0/all/0/1\">Kristen Howell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Slot Schema Induction for Task-oriented Dialog. (arXiv:2205.04515v1 [cs.CL])","link":"http://arxiv.org/abs/2205.04515","description":"<p>Carefully-designed schemas describing how to collect and annotate dialog\ncorpora are a prerequisite towards building task-oriented dialog systems. In\npractical applications, manually designing schemas can be error-prone,\nlaborious, iterative, and slow, especially when the schema is complicated. To\nalleviate this expensive and time consuming process, we propose an unsupervised\napproach for slot schema induction from unlabeled dialog corpora. Leveraging\nin-domain language models and unsupervised parsing structures, our data-driven\napproach extracts candidate slots without constraints, followed by\ncoarse-to-fine clustering to induce slot types. We compare our method against\nseveral strong supervised baselines, and show significant performance\nimprovement in slot schema induction on MultiWoz and SGD datasets. We also\ndemonstrate the effectiveness of induced schemas on downstream applications\nincluding dialog state tracking and response generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingqiu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafran_I/0/1/0/all/0/1\">Izhak Shafran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafey_L/0/1/0/all/0/1\">Laurent El Shafey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltau_H/0/1/0/all/0/1\">Hagen Soltau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Song of (Dis)agreement: Evaluating the Evaluation of Explainable Artificial Intelligence in Natural Language Processing. (arXiv:2205.04559v1 [cs.CL])","link":"http://arxiv.org/abs/2205.04559","description":"<p>There has been significant debate in the NLP community about whether or not\nattention weights can be used as an explanation - a mechanism for interpreting\nhow important each input token is for a particular prediction. The validity of\n\"attention as explanation\" has so far been evaluated by computing the rank\ncorrelation between attention-based explanations and existing feature\nattribution explanations using LSTM-based models. In our work, we (i) compare\nthe rank correlation between five more recent feature attribution methods and\ntwo attention-based methods, on two types of NLP tasks, and (ii) extend this\nanalysis to also include transformer-based models. We find that attention-based\nexplanations do not correlate strongly with any recent feature attribution\nmethods, regardless of the model or task. Furthermore, we find that none of the\ntested explanations correlate strongly with one another for the\ntransformer-based model, leading us to question the underlying assumption that\nwe should measure the validity of attention-based explanations based on how\nwell they correlate with existing feature attribution explanation methods.\nAfter conducting experiments on five datasets using two different models, we\nargue that the community should stop using rank correlation as an evaluation\nmetric for attention-based explanations. We suggest that researchers and\npractitioners should instead test various explanation methods and employ a\nhuman-in-the-loop process to determine if the explanations align with human\nintuition for the particular use case at hand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neely_M/0/1/0/all/0/1\">Michael Neely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schouten_S/0/1/0/all/0/1\">Stefan F. Schouten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bleeker_M/0/1/0/all/0/1\">Maurits Bleeker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_A/0/1/0/all/0/1\">Ana Lucic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Model for Reverse Dictionary and Definition Modelling. (arXiv:2205.04602v1 [cs.CL])","link":"http://arxiv.org/abs/2205.04602","description":"<p>We train a dual-way neural dictionary to guess words from definitions\n(reverse dictionary), and produce definitions given words (definition\nmodelling). Our method learns the two tasks simultaneously, and handles unknown\nwords via embeddings. It casts a word or a definition to the same\nrepresentation space through a shared layer, then generates the other form from\nthere, in a multi-task fashion. The model achieves promising automatic scores\nwithout extra resources. Human annotators prefer the proposed model's outputs\nin both reference-less and reference-based evaluation, which indicates its\npracticality. Analysis suggests that multiple objectives benefit learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pinzhen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zheng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence-level Privacy for Document Embeddings. (arXiv:2205.04605v1 [cs.LG])","link":"http://arxiv.org/abs/2205.04605","description":"<p>User language data can contain highly sensitive personal content. As such, it\nis imperative to offer users a strong and interpretable privacy guarantee when\nlearning from their data. In this work, we propose SentDP: pure local\ndifferential privacy at the sentence level for a single user document. We\npropose a novel technique, DeepCandidate, that combines concepts from robust\nstatistics and language modeling to produce high-dimensional, general-purpose\n$\\epsilon$-SentDP document embeddings. This guarantees that any single sentence\nin a document can be substituted with any other sentence while keeping the\nembedding $\\epsilon$-indistinguishable. Our experiments indicate that these\nprivate document embeddings are useful for downstream tasks like sentiment\nanalysis and topic classification and even outperform baseline methods with\nweaker guarantees like word-level Metric DP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meehan_C/0/1/0/all/0/1\">Casey Meehan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mrini_K/0/1/0/all/0/1\">Khalil Mrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1\">Kamalika Chaudhuri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ParaCotta: Synthetic Multilingual Paraphrase Corpora from the Most Diverse Translation Sample Pair. (arXiv:2205.04651v1 [cs.CL])","link":"http://arxiv.org/abs/2205.04651","description":"<p>We release our synthetic parallel paraphrase corpus across 17 languages:\nArabic, Catalan, Czech, German, English, Spanish, Estonian, French, Hindi,\nIndonesian, Italian, Dutch, Romanian, Russian, Swedish, Vietnamese, and\nChinese. Our method relies only on monolingual data and a neural machine\ntranslation system to generate paraphrases, hence simple to apply. We generate\nmultiple translation samples using beam search and choose the most lexically\ndiverse pair according to their sentence BLEU. We compare our generated corpus\nwith the \\texttt{ParaBank2}. According to our evaluation, our synthetic\nparaphrase pairs are semantically similar and lexically diverse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatyanosa_T/0/1/0/all/0/1\">Tirana Noor Fatyanosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasojo_R/0/1/0/all/0/1\">Radityo Eko Prasojo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arthur_P/0/1/0/all/0/1\">Philip Arthur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fitriany_S/0/1/0/all/0/1\">Suci Fitriany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qonitah_S/0/1/0/all/0/1\">Salma Qonitah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zulfa_N/0/1/0/all/0/1\">Nadhifa Zulfa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santoso_T/0/1/0/all/0/1\">Tomi Santoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Data_M/0/1/0/all/0/1\">Mahendra Data</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SuMe: A Dataset Towards Summarizing Biomedical Mechanisms. (arXiv:2205.04652v1 [cs.CL])","link":"http://arxiv.org/abs/2205.04652","description":"<p>Can language models read biomedical texts and explain the biomedical\nmechanisms discussed? In this work we introduce a biomedical mechanism\nsummarization task. Biomedical studies often investigate the mechanisms behind\nhow one entity (e.g., a protein or a chemical) affects another in a biological\ncontext. The abstracts of these publications often include a focused set of\nsentences that present relevant supporting statements regarding such\nrelationships, associated experimental evidence, and a concluding sentence that\nsummarizes the mechanism underlying the relationship. We leverage this\nstructure and create a summarization task, where the input is a collection of\nsentences and the main entities in an abstract, and the output includes the\nrelationship and a sentence that summarizes the mechanism. Using a small amount\nof manually labeled mechanism sentences, we train a mechanism sentence\nclassifier to filter a large biomedical abstract collection and create a\nsummarization dataset with 22k instances. We also introduce conclusion sentence\ngeneration as a pretraining task with 611k instances. We benchmark the\nperformance of large bio-domain language models. We find that while the\npretraining task help improves performance, the best model produces acceptable\nmechanism outputs in only 32% of the instances, which shows the task presents\nsignificant challenges in biomedical language understanding and summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bastan_M/0/1/0/all/0/1\">Mohaddeseh Bastan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_N/0/1/0/all/0/1\">Nishant Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1\">Mihai Surdeanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdMix: A Mixed Sample Data Augmentation Method for Neural Machine Translation. (arXiv:2205.04686v1 [cs.CL])","link":"http://arxiv.org/abs/2205.04686","description":"<p>In Neural Machine Translation (NMT), data augmentation methods such as\nback-translation have proven their effectiveness in improving translation\nperformance. In this paper, we propose a novel data augmentation approach for\nNMT, which is independent of any additional training data. Our approach, AdMix,\nconsists of two parts: 1) introduce faint discrete noise (word replacement,\nword dropping, word swapping) into the original sentence pairs to form\naugmented samples; 2) generate new synthetic training data by softly mixing the\naugmented samples with their original samples in training corpus. Experiments\non three translation datasets of different scales show that AdMix achieves\nsignifi cant improvements (1.0 to 2.7 BLEU points) over strong Transformer\nbaseline. When combined with other data augmentation techniques (e.g.,\nback-translation), our approach can obtain further improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chang Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shigui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_N/0/1/0/all/0/1\">Nini Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1\">Hao Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Learning Based Knowledge Extrapolation for Knowledge Graphs in the Federated Setting. (arXiv:2205.04692v1 [cs.CL])","link":"http://arxiv.org/abs/2205.04692","description":"<p>We study the knowledge extrapolation problem to embed new components (i.e.,\nentities and relations) that come with emerging knowledge graphs (KGs) in the\nfederated setting. In this problem, a model trained on an existing KG needs to\nembed an emerging KG with unseen entities and relations. To solve this problem,\nwe introduce the meta-learning setting, where a set of tasks are sampled on the\nexisting KG to mimic the link prediction task on the emerging KG. Based on\nsampled tasks, we meta-train a graph neural network framework that can\nconstruct features for unseen components based on structural information and\noutput embeddings for them. Experimental results show that our proposed method\ncan effectively embed unseen components and outperforms models that consider\ninductive settings for KGs and baselines that directly use conventional KG\nembedding methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhen Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangnan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Mengxiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective. (arXiv:2205.04733v1 [cs.IR])","link":"http://arxiv.org/abs/2205.04733","description":"<p>Neural retrievers based on dense representations combined with Approximate\nNearest Neighbors search have recently received a lot of attention, owing their\nsuccess to distillation and/or better sampling of examples for training --\nwhile still relying on the same backbone architecture. In the meantime, sparse\nrepresentation learning fueled by traditional inverted indexing techniques has\nseen a growing interest, inheriting from desirable IR priors such as explicit\nlexical matching. While some architectural variants have been proposed, a\nlesser effort has been put in the training of such models. In this work, we\nbuild on SPLADE -- a sparse expansion-based retriever -- and show to which\nextent it is able to benefit from the same training improvements as dense\nmodels, by studying the effect of distillation, hard-negative mining as well as\nthe Pre-trained Language Model initialization. We furthermore study the link\nbetween effectiveness and efficiency, on in-domain and zero-shot settings,\nleading to state-of-the-art results in both scenarios for sufficiently\nexpressive models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Formal_T/0/1/0/all/0/1\">Thibault Formal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassance_C/0/1/0/all/0/1\">Carlos Lassance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piwowarski_B/0/1/0/all/0/1\">Benjamin Piwowarski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinchant_S/0/1/0/all/0/1\">St&#xe9;phane Clinchant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlling Extra-Textual Attributes about Dialogue Participants: A Case Study of English-to-Polish Neural Machine Translation. (arXiv:2205.04747v1 [cs.CL])","link":"http://arxiv.org/abs/2205.04747","description":"<p>Unlike English, morphologically rich languages can reveal characteristics of\nspeakers or their conversational partners, such as gender and number, via\npronouns, morphological endings of words and syntax. When translating from\nEnglish to such languages, a machine translation model needs to opt for a\ncertain interpretation of textual context, which may lead to serious\ntranslation errors if extra-textual information is unavailable. We investigate\nthis challenge in the English-to-Polish language direction. We focus on the\nunderresearched problem of utilising external metadata in automatic translation\nof TV dialogue, proposing a case study where a wide range of approaches for\ncontrolling attributes in translation is employed in a multi-attribute\nscenario. The best model achieves an improvement of +5.81 chrF++/+6.03 BLEU,\nwith other models achieving competitive performance. We additionally contribute\na novel attribute-annotated dataset of Polish TV dialogue and a morphological\nanalysis script used to evaluate attribute control in models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vincent_S/0/1/0/all/0/1\">Sebastian T. Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrault_L/0/1/0/all/0/1\">Lo&#xef;c Barrault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Importance of Context in Very Low Resource Language Modeling. (arXiv:2205.04810v1 [cs.CL])","link":"http://arxiv.org/abs/2205.04810","description":"<p>This paper investigates very low resource language model pretraining, when\nless than 100 thousand sentences are available. We find that, in very low\nresource scenarios, statistical n-gram language models outperform\nstate-of-the-art neural models. Our experiments show that this is mainly due to\nthe focus of the former on a local context. As such, we introduce three methods\nto improve a neural model's performance in the low-resource setting, finding\nthat limiting the model's self-attention is the most effective one, improving\non downstream tasks such as NLI and POS tagging by up to 5% for the languages\nwe test on: English, Hindi, and Turkish.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Edman_L/0/1/0/all/0/1\">Lukas Edman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1\">Antonio Toral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noord_G/0/1/0/all/0/1\">Gertjan van Noord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the prosody GAP: Genetic Algorithm with People to efficiently sample emotional prosody. (arXiv:2205.04820v1 [cs.CL])","link":"http://arxiv.org/abs/2205.04820","description":"<p>The human voice effectively communicates a range of emotions with nuanced\nvariations in acoustics. Existing emotional speech corpora are limited in that\nthey are either (a) highly curated to induce specific emotions with predefined\ncategories that may not capture the full extent of emotional experiences, or\n(b) entangled in their semantic and prosodic cues, limiting the ability to\nstudy these cues separately. To overcome this challenge, we propose a new\napproach called 'Genetic Algorithm with People' (GAP), which integrates human\ndecision and production into a genetic algorithm. In our design, we allow\ncreators and raters to jointly optimize the emotional prosody over generations.\nWe demonstrate that GAP can efficiently sample from the emotional speech space\nand capture a broad range of emotions, and show comparable results to\nstate-of-the-art emotional speech corpora. GAP is language-independent and\nsupports large crowd-sourcing, thus can support future large-scale\ncross-cultural research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rijn_P/0/1/0/all/0/1\">Pol van Rijn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Harin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1\">Nori Jacoby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALLSH: Active Learning Guided by Local Sensitivity and Hardness. (arXiv:2205.04980v1 [cs.CL])","link":"http://arxiv.org/abs/2205.04980","description":"<p>Active learning, which effectively collects informative unlabeled data for\nannotation, reduces the demand for labeled data. In this work, we propose to\nretrieve unlabeled samples with a local sensitivity and hardness-aware\nacquisition function. The proposed method generates data copies through local\nperturbations and selects data points whose predictive likelihoods diverge the\nmost from their copies. We further empower our acquisition function by\ninjecting the select-worst case perturbation. Our method achieves consistent\ngains over the commonly used active learning strategies in various\nclassification tasks. Furthermore, we observe consistent improvements over the\nbaselines on the study of prompt selection in prompt-based few-shot learning.\nThese experiments demonstrate that our acquisition guided by local sensitivity\nand hardness can be effective and beneficial for many NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shujian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chengyue Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingchao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Answer Visual Questions from Web Videos. (arXiv:2205.05019v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05019","description":"<p>Recent methods for visual question answering rely on large-scale annotated\ndatasets. Manual annotation of questions and answers for videos, however, is\ntedious, expensive and prevents scalability. In this work, we propose to avoid\nmanual annotation and generate a large-scale training dataset for video\nquestion answering making use of automatic cross-modal supervision. We leverage\na question generation transformer trained on text data and use it to generate\nquestion-answer pairs from transcribed video narrations. Given narrated videos,\nwe then automatically generate the HowToVQA69M dataset with 69M\nvideo-question-answer triplets. To handle the open vocabulary of diverse\nanswers in this dataset, we propose a training procedure based on a contrastive\nloss between a video-question multi-modal transformer and an answer\ntransformer. We introduce the zero-shot VideoQA task and the VideoQA feature\nprobe evaluation setting and show excellent results, in particular for rare\nanswers. Furthermore, our method achieves competitive results on MSRVTT-QA,\nActivityNet-QA, MSVD-QA and How2QA datasets. We also show that our VideoQA\ndataset generation approach generalizes to another source of web video and text\ndata. We use our method to generate the \\webdataname{} dataset from the WebVid\ndataset, i.e., videos with alt-text annotations, and show its benefits for\ntraining VideoQA models. Finally, for a detailed evaluation we introduce\n\\smalldatasetname{}, a new VideoQA dataset with reduced language bias and\nhigh-quality manual annotations. Code, datasets and trained models are\navailable at https://antoyang.github.io/just-ask.html\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Antoine Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1\">Antoine Miech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"White-box Testing of NLP models with Mask Neuron Coverage. (arXiv:2205.05050v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05050","description":"<p>Recent literature has seen growing interest in using black-box strategies\nlike CheckList for testing the behavior of NLP models. Research on white-box\ntesting has developed a number of methods for evaluating how thoroughly the\ninternal behavior of deep models is tested, but they are not applicable to NLP\nmodels. We propose a set of white-box testing methods that are customized for\ntransformer-based NLP models. These include Mask Neuron Coverage (MNCOVER) that\nmeasures how thoroughly the attention layers in models are exercised during\ntesting. We show that MNCOVER can refine testing suites generated by CheckList\nby substantially reduce them in size, for more than 60\\% on average, while\nretaining failing tests -- thereby concentrating the fault detection power of\nthe test suite. Further we show how MNCOVER can be used to guide CheckList\ninput generation, evaluate alternative NLP testing methods, and drive data\naugmentation to improve accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sekhon_A/0/1/0/all/0/1\">Arshdeep Sekhon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwyer_M/0/1/0/all/0/1\">Matthew B. Dwyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers. (arXiv:2205.05055v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05055","description":"<p>Large transformer-based language models are able to perform few-shot learning\n(also known as in-context learning), without having been explicitly trained for\nit. We hypothesized that specific distributional properties of natural language\nmight drive this emergent phenomenon, as these characteristics might lead to a\nkind of interpolation between few-shot meta-training (designed to elicit rapid\nfew-shot learning) and standard supervised training (designed to elicit gradual\nin-weights learning). We also hypothesized that these distributional properties\ncould lead to emergent few-shot learning in domains outside of language.\nInspired by this idea, we ran a series of experiments on a standard image-based\nfew-shot dataset. We discovered that a number of data properties did indeed\npromote the emergence of few-shot learning in transformer models. All of these\nproperties are present in natural language -- burstiness, long-tailedness, and\nmany-to-one or one-to-many label mappings. The data influenced whether models\nwere biased towards either few-shot learning vs. memorizing information in\ntheir weights; models could generally perform well at only one or the other.\nHowever, we discovered that an additional distributional property could allow\nthe two capabilities to co-exist in the same model -- a skewed, Zipfian\ndistribution over classes -- which occurs in language as well. Notably,\ntraining data that could elicit few-shot learning in transformers were unable\nto elicit few-shot learning in recurrent models. In sum, we find that few-shot\nlearning emerges only from applying the right architecture to the right data\ndistribution; neither component is sufficient on its own.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Stephanie C.Y. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santoro_A/0/1/0/all/0/1\">Adam Santoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1\">Andrew K. Lampinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jane X. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aaditya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richemond_P/0/1/0/all/0/1\">Pierre H. Richemond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McClelland_J/0/1/0/all/0/1\">Jay McClelland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Climate Awareness in NLP Research. (arXiv:2205.05071v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05071","description":"<p>The climate impact of AI, and NLP research in particular, has become a\nserious issue given the enormous amount of energy that is increasingly being\nused for training and running computational models. Consequently, increasing\nfocus is placed on efficient NLP. However, this important initiative lacks\nsimple guidelines that would allow for systematic climate reporting of NLP\nresearch. We argue that this deficiency is one of the reasons why very few\npublications in NLP report key figures that would allow a more thorough\nexamination of environmental impact. As a remedy, we propose a climate\nperformance model card with the primary purpose of being practically usable\nwith only limited information about experiments and the underlying computer\nhardware. We describe why this step is essential to increase awareness about\nthe environmental impact of NLP research and, thereby, paving the way for more\nthorough discussions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webersinke_N/0/1/0/all/0/1\">Nicolas Webersinke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraus_M/0/1/0/all/0/1\">Mathias Kraus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bingler_J/0/1/0/all/0/1\">Julia Anna Bingler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leippold_M/0/1/0/all/0/1\">Markus Leippold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrated Node Encoder for Labelled Textual Networks. (arXiv:2005.11694v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.11694","description":"<p>Voluminous works have been implemented to exploit content-enhanced network\nembedding models, with little focus on the labelled information of nodes.\nAlthough TriDNR leverages node labels by treating them as node attributes, it\nfails to enrich unlabelled node vectors with the labelled information, which\nleads to the weaker classification result on the test set in comparison to\nexisting unsupervised textual network embedding models. In this study, we\ndesign an integrated node encoder (INE) for textual networks which is jointly\ntrained on the structure-based and label-based objectives. As a result, the\nnode encoder preserves the integrated knowledge of not only the network text\nand structure, but also the labelled information. Furthermore, INE allows the\ncreation of label-enhanced vectors for unlabelled nodes by entering their node\ncontents. Our node embedding achieves state-of-the-art performances in the\nclassification task on two public citation networks, namely Cora and DBLP,\npushing benchmarks up by 10.0\\% and 12.1\\%, respectively, with the 70\\%\ntraining ratio. Additionally, a feasible solution that generalizes our model\nfrom textual networks to a broader range of networks is proposed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Ye Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_L/0/1/0/all/0/1\">Lu Zong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource Language Understanding Evaluation in Bangla. (arXiv:2101.00204v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00204","description":"<p>In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Abhik Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_T/0/1/0/all/0/1\">Tahmid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samin_K/0/1/0/all/0/1\">Kazi Samin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Saiful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_A/0/1/0/all/0/1\">Anindya Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">M. Sohel Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahriyar_R/0/1/0/all/0/1\">Rifat Shahriyar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MirrorAlign: A Super Lightweight Unsupervised Word Alignment Model via Cross-Lingual Contrastive Learning. (arXiv:2102.04009v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.04009","description":"<p>Word alignment is essential for the downstream cross-lingual language\nunderstanding and generation tasks. Recently, the performance of the neural\nword alignment models has exceeded that of statistical models. However, they\nheavily rely on sophisticated translation models. In this study, we propose a\nsuper lightweight unsupervised word alignment model named MirrorAlign, in which\nbidirectional symmetric attention trained with a contrastive learning objective\nis introduced, and an agreement loss is employed to bind the attention maps,\nsuch that the alignments follow mirror-like symmetry hypothesis. Experimental\nresults on several public benchmarks demonstrate that our model achieves\ncompetitive, if not better, performance compared to the state of the art in\nword alignment while significantly reducing the training and decoding time on\naverage. Further ablation analysis and case studies show the superiority of our\nproposed MirrorAlign. Notably, we recognize our model as a pioneer attempt to\nunify bilingual word embedding and word alignments. Encouragingly, our approach\nachieves {16.4X speedup} against GIZA++, and {50X parameter compression}\ncompared with the Transformer-based alignment methods. We release our code to\nfacilitate the community: https://github.com/moore3930/MirrorAlign.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingyang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Restoring Hebrew Diacritics Without a Dictionary. (arXiv:2105.05209v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.05209","description":"<p>We demonstrate that it is feasible to diacritize Hebrew script without any\nhuman-curated resources other than plain diacritized text. We present NAKDIMON,\na two-layer character level LSTM, that performs on par with much more\ncomplicated curation-dependent systems, across a diverse array of modern Hebrew\nsources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gershuni_E/0/1/0/all/0/1\">Elazar Gershuni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1\">Yuval Pinter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Abusive Albanian. (arXiv:2107.13592v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.13592","description":"<p>The ever growing usage of social media in the recent years has had a direct\nimpact on the increased presence of hate speech and offensive speech in online\nplatforms. Research on effective detection of such content has mainly focused\non English and a few other widespread languages, while the leftover majority\nfail to have the same work put into them and thus cannot benefit from the\nsteady advancements made in the field. In this paper we present \\textsc{Shaj},\nan annotated Albanian dataset for hate speech and offensive speech that has\nbeen constructed from user-generated content on various social media platforms.\nIts annotation follows the hierarchical schema introduced in OffensEval. The\ndataset is tested using three different classification models, the best of\nwhich achieves an F1 score of 0.77 for the identification of offensive\nlanguage, 0.64 F1 score for the automatic categorization of offensive types and\nlastly, 0.52 F1 score for the offensive language target identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nurce_E/0/1/0/all/0/1\">Erida Nurce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keci_J/0/1/0/all/0/1\">Jorgel Keci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WiC = TSV = WSD: On the Equivalence of Three Semantic Tasks. (arXiv:2107.14352v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.14352","description":"<p>The Word-in-Context (WiC) task has attracted considerable attention in the\nNLP community, as demonstrated by the popularity of the recent MCL-WiC SemEval\nshared task. Systems and lexical resources from word sense disambiguation (WSD)\nare often used for the WiC task and WiC dataset construction. In this paper, we\nestablish the exact relationship between WiC and WSD, as well as the related\ntask of target sense verification (TSV). Building upon a novel hypothesis on\nthe equivalence of sense and meaning distinctions, we demonstrate through the\napplication of tools from theoretical computer science that these three\nsemantic classification problems can be pairwise reduced to each other, and\ntherefore are equivalent. The results of experiments that involve systems and\ndatasets for both WiC and WSD provide strong empirical evidence that our\nproblem reductions work in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hauer_B/0/1/0/all/0/1\">Bradley Hauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondrak_G/0/1/0/all/0/1\">Grzegorz Kondrak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity Linking and Discovery via Arborescence-based Supervised Clustering. (arXiv:2109.01242v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01242","description":"<p>Previous work has shown promising results in performing entity linking by\nmeasuring not only the affinities between mentions and entities but also those\namongst mentions. In this paper, we present novel training and inference\nprocedures that fully utilize mention-to-mention affinities by building minimum\narborescences (i.e., directed spanning trees) over mentions and entities across\ndocuments in order to make linking decisions. We also show that this method\ngracefully extends to entity discovery, enabling the clustering of mentions\nthat do not have an associated entity in the knowledge base. We evaluate our\napproach on the Zero-Shot Entity Linking dataset and MedMentions, the largest\npublicly available biomedical dataset, and show significant improvements in\nperformance for both entity linking and discovery compared to identically\nparameterized models. We further show significant efficiency improvements with\nonly a small loss in accuracy over previous work, which use more\ncomputationally expensive models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Dhruv Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angell_R/0/1/0/all/0/1\">Rico Angell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monath_N/0/1/0/all/0/1\">Nicholas Monath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-Free Prosody-Aware Generative Spoken Language Modeling. (arXiv:2109.03264v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03264","description":"<p>Speech pre-training has primarily demonstrated efficacy on classification\ntasks, while its capability of generating novel speech, similar to how GPT-2\ncan generate coherent paragraphs, has barely been explored. Generative Spoken\nLanguage Modeling (GSLM) \\cite{Lakhotia2021} is the only prior work addressing\nthe generative aspects of speech pre-training, which replaces text with\ndiscovered phone-like units for language modeling and shows the ability to\ngenerate meaningful novel sentences. Unfortunately, despite eliminating the\nneed of text, the units used in GSLM discard most of the prosodic information.\nHence, GSLM fails to leverage prosody for better comprehension, and does not\ngenerate expressive speech. In this work, we present a prosody-aware generative\nspoken language model (pGSLM). It is composed of a multi-stream transformer\nlanguage model (MS-TLM) of speech, represented as discovered unit and prosodic\nfeature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to\nwaveforms. We devise a series of metrics for prosody modeling and generation,\nand re-use metrics from GSLM for content modeling. Experimental results show\nthat the pGSLM can utilize prosody to improve both prosody and content\nmodeling, and also generate natural, meaningful, and coherent speech given a\nspoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm.\nCodes and models are available at\nhttps://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu-Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riviere_M/0/1/0/all/0/1\">Morgane Rivi&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrollsWithOpinion: A Dataset for Predicting Domain-specific Opinion Manipulation in Troll Memes. (arXiv:2109.03571v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2109.03571","description":"<p>Research into the classification of Image with Text (IWT) troll memes has\nrecently become popular. Since the online community utilizes the refuge of\nmemes to express themselves, there is an abundance of data in the form of\nmemes. These memes have the potential to demean, harras, or bully targeted\nindividuals. Moreover, the targeted individual could fall prey to opinion\nmanipulation. To comprehend the use of memes in opinion manipulation, we define\nthree specific domains (product, political or others) which we classify into\ntroll or not-troll, with or without opinion manipulation. To enable this\nanalysis, we enhanced an existing dataset by annotating the data with our\ndefined classes, resulting in a dataset of 8,881 IWT or multimodal memes in the\nEnglish language (TrollsWithOpinion dataset). We perform baseline experiments\non the annotated dataset, and our result shows that existing state-of-the-art\ntechniques could only reach a weighted-average F1-score of 0.37. This shows the\nneed for a development of a specific technique to deal with multimodal troll\nmemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suryawanshi_S/0/1/0/all/0/1\">Shardul Suryawanshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arcan_M/0/1/0/all/0/1\">Mihael Arcan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Little_S/0/1/0/all/0/1\">Suzanne Little</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buitelaar_P/0/1/0/all/0/1\">Paul Buitelaar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding. (arXiv:2109.04947v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04947","description":"<p>Large-scale, pre-trained language models (LMs) have achieved human-level\nperformance on a breadth of language understanding tasks. However, evaluations\nonly based on end task performance shed little light on machines' true ability\nin language understanding and reasoning. In this paper, we highlight the\nimportance of evaluating the underlying reasoning process in addition to end\nperformance. Toward this goal, we introduce Tiered Reasoning for Intuitive\nPhysics (TRIP), a novel commonsense reasoning dataset with dense annotations\nthat enable multi-tiered evaluation of machines' reasoning process. Our\nempirical results show that while large LMs can achieve high end performance,\nthey struggle to support their predictions with valid supporting evidence. The\nTRIP dataset and our baseline results will motivate verifiable evaluation of\ncommonsense reasoning and facilitate future research toward developing better\nlanguage understanding and reasoning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Storks_S/0/1/0/all/0/1\">Shane Storks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiaozi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Control Prefixes for Parameter-Efficient Text Generation. (arXiv:2110.08329v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08329","description":"<p>Prefix-tuning is a powerful lightweight technique for adapting a large\npre-trained language model to a downstream application. However, it uses the\nsame dataset-level tuned prompt for all examples in the dataset. We extend this\nidea and propose a dynamic method, Control Prefixes, which allows for the\ninclusion of conditional input-dependent information, combining the benefits of\nprompt tuning and controlled generation. The method incorporates\nattribute-level learnable representations into different layers of a\npre-trained transformer, allowing for the generated text to be guided in a\nparticular direction. We provide a systematic evaluation of the technique and\napply it to five datasets from the GEM benchmark for natural language\ngeneration (NLG). Although the aim is to develop a parameter-efficient model,\nwe show Control Prefixes can even outperform full fine-tuning methods. We\npresent state-of-the-art results on several data-to-text datasets, including\nWebNLG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clive_J/0/1/0/all/0/1\">Jordan Clive</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_K/0/1/0/all/0/1\">Kris Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hey AI, Can You Solve Complex Tasks by Talking to Agents?. (arXiv:2110.08542v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08542","description":"<p>Training giant models from scratch for each complex task is resource- and\ndata-inefficient. To help develop models that can leverage existing systems, we\npropose a new challenge: Learning to solve complex tasks by communicating with\nexisting agents (or models) in natural language. We design a synthetic\nbenchmark, CommaQA, with three complex reasoning tasks (explicit, implicit,\nnumeric) designed to be solved by communicating with existing QA agents. For\ninstance, using text and table QA agents to answer questions such as \"Who had\nthe longest javelin throw from USA?\". We show that black-box models struggle to\nlearn this task from scratch (accuracy under 50\\%) even with access to each\nagent's knowledge and gold facts supervision. In contrast, models that learn to\ncommunicate with agents outperform black-box models, reaching scores of 100\\%\nwhen given gold decomposition supervision. However, we show that the challenge\nof learning to solve complex tasks by communicating with existing agents\n\\emph{without relying on any auxiliary supervision or data} still remains\nhighly elusive. We release CommaQA, along with a compositional generalization\ntest split, to advance research in this direction. Dataset and Code available\nat https://github.com/allenai/commaqa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richardson_K/0/1/0/all/0/1\">Kyle Richardson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Cross-Utterance Language Modeling for Conversational Speech Recognition. (arXiv:2111.03333v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.03333","description":"<p>Conversational speech normally is embodied with loose syntactic structures at\nthe utterance level but simultaneously exhibits topical coherence relations\nacross consecutive utterances. Prior work has shown that capturing longer\ncontext information with a recurrent neural network or long short-term memory\nlanguage model (LM) may suffer from the recent bias while excluding the\nlong-range context. In order to capture the long-term semantic interactions\namong words and across utterances, we put forward disparate conversation\nhistory fusion methods for language modeling in automatic speech recognition\n(ASR) of conversational speech. Furthermore, a novel audio-fusion mechanism is\nintroduced, which manages to fuse and utilize the acoustic embeddings of a\ncurrent utterance and the semantic content of its corresponding conversation\nhistory in a cooperative way. To flesh out our ideas, we frame the ASR N-best\nhypothesis rescoring task as a prediction problem, leveraging BERT, an iconic\npre-trained LM, as the ingredient vehicle to facilitate selection of the oracle\nhypothesis from a given N-best hypothesis list. Empirical experiments conducted\non the AMI benchmark dataset seem to demonstrate the feasibility and efficacy\nof our methods in relation to some current top-of-line methods. The proposed\nmethods not only achieve significant inference time reduction but also improve\nthe ASR performance for conversational speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bi-Cheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_S/0/1/0/all/0/1\">Shih-Hsuan Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1\">Hsuan-Sheng Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Berlin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Jargon: Combining Extraction and Generation for Definition Modeling. (arXiv:2111.07267v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.07267","description":"<p>Can machines know what twin prime is? From the composition of this phrase,\nmachines may guess twin prime is a certain kind of prime, but it is still\ndifficult to deduce exactly what twin stands for without additional knowledge.\nHere, twin prime is a jargon - a specialized term used by experts in a\nparticular field. Explaining jargon is challenging since it usually requires\ndomain knowledge to understand. Recently, there is an increasing interest in\nextracting and generating definitions of words automatically. However, existing\napproaches, either extraction or generation, perform poorly on jargon. In this\npaper, we propose to combine extraction and generation for jargon definition\nmodeling: first extract self- and correlative definitional information of\ntarget jargon from the Web and then generate the final definitions by\nincorporating the extracted definitional information. Our framework is\nremarkably simple but effective: experiments demonstrate our method can\ngenerate high-quality definitions for jargon and outperform state-of-the-art\nmodels significantly, e.g., BLEU score from 8.76 to 22.66 and human-annotated\nscore from 2.34 to 4.04.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1\">Hanyin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwu_W/0/1/0/all/0/1\">Wen-mei Hwu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection. (arXiv:2111.07997v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.07997","description":"<p>The perceived toxicity of language can vary based on someone's identity and\nbeliefs, but this variation is often ignored when collecting toxic language\ndatasets, resulting in dataset and model biases. We seek to understand the who,\nwhy, and what behind biases in toxicity annotations. In two online studies with\ndemographically and politically diverse participants, we investigate the effect\nof annotator identities (who) and beliefs (why), drawing from social psychology\nresearch about hate speech, free speech, racist beliefs, political leaning, and\nmore. We disentangle what is annotated as toxic by considering posts with three\ncharacteristics: anti-Black language, African American English (AAE) dialect,\nand vulgarity. Our results show strong associations between annotator identity\nand beliefs and their ratings of toxicity. Notably, more conservative\nannotators and those who scored highly on our scale for racist beliefs were\nless likely to rate anti-Black language as toxic, but more likely to rate AAE\nas toxic. We additionally present a case study illustrating how a popular\ntoxicity detection system's ratings inherently reflect only specific beliefs\nand perspectives. Our findings call for contextualizing toxicity labels in\nsocial variables, which raises immense implications for toxic language\nannotation and detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vianna_L/0/1/0/all/0/1\">Laura Vianna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xuhui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiVerS: Improving scientific claim verification with weak supervision and full-document context. (arXiv:2112.01640v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.01640","description":"<p>The scientific claim verification task requires an NLP system to label\nscientific documents which Support or Refute an input claim, and to select\nevidentiary sentences (or rationales) justifying each predicted label. In this\nwork, we present MultiVerS, which predicts a fact-checking label and identifies\nrationales in a multitask fashion based on a shared encoding of the claim and\nfull document context. This approach accomplishes two key modeling goals.\nFirst, it ensures that all relevant contextual information is incorporated into\neach labeling decision. Second, it enables the model to learn from instances\nannotated with a document-level fact-checking label, but lacking sentence-level\nrationales. This allows MultiVerS to perform weakly-supervised domain\nadaptation by training on scientific documents labeled using high-precision\nheuristics. Our approach outperforms two competitive baselines on three\nscientific claim verification datasets, with particularly strong performance in\nzero / few-shot domain adaptation experiments. Our code and data are available\nat https://github.com/dwadden/multivers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wadden_D/0/1/0/all/0/1\">David Wadden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lucy Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measure and Improve Robustness in NLP Models: A Survey. (arXiv:2112.08313v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08313","description":"<p>As NLP models achieved state-of-the-art performances over benchmarks and\ngained wide applications, it has been increasingly important to ensure the safe\ndeployment of these models in the real world, e.g., making sure the models are\nrobust against unseen or challenging scenarios. Despite robustness being an\nincreasingly studied topic, it has been separately explored in applications\nlike vision and NLP, with various definitions, evaluation and mitigation\nstrategies in multiple lines of research. In this paper, we aim to provide a\nunifying survey of how to define, measure and improve robustness in NLP. We\nfirst connect multiple definitions of robustness, then unify various lines of\nwork on identifying robustness failures and evaluating models' robustness.\nCorrespondingly, we present mitigation strategies that are data-driven,\nmodel-driven, and inductive-prior-based, with a more systematic view of how to\neffectively improve robustness in NLP models. Finally, we conclude by outlining\nopen challenges and future directions to motivate further research in this\narea.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Repair: Repairing model output errors after deployment using a dynamic memory of feedback. (arXiv:2112.09737v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.09737","description":"<p>Large language models (LMs), while powerful, are not immune to mistakes, but\ncan be difficult to retrain. Our goal is for an LM to continue to improve after\ndeployment, without retraining, using feedback from the user. Our approach\npairs an LM with (i) a growing memory of cases where the user identified an\noutput error and provided general feedback on how to correct it (ii) a\ncorrector model, trained to translate this general feedback into specific edits\nto repair the model output. Given a new, unseen input, our model can then use\nfeedback from similar, past cases to repair output errors that may occur. We\ninstantiate our approach using an existing, fixed model for script generation,\nthat takes a goal (e.g., \"bake a cake\") and generates a partially ordered\nsequence of actions to achieve that goal, sometimes containing errors. Our\nmemory-enhanced system, FBNet, learns to apply user feedback to repair such\nerrors (up to 30 points improvement), while making a start at avoiding similar\npast mistakes on new, unseen examples (up to 7 points improvement in a\ncontrolled setting). This is a first step towards strengthening deployed\nmodels, potentially broadening their utility. Our code and data is available at\nhttps://github.com/allenai/interscript/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1\">Niket Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotating the Tweebank Corpus on Named Entity Recognition and Building NLP Models for Social Media Analysis. (arXiv:2201.07281v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.07281","description":"<p>Social media data such as Twitter messages (\"tweets\") pose a particular\nchallenge to NLP systems because of their short, noisy, and colloquial nature.\nTasks such as Named Entity Recognition (NER) and syntactic parsing require\nhighly domain-matched training data for good performance. To date, there is no\ncomplete training corpus for both NER and syntactic analysis (e.g., part of\nspeech tagging, dependency parsing) of tweets. While there are some publicly\navailable annotated NLP datasets of tweets, they are only designed for\nindividual tasks. In this study, we aim to create Tweebank-NER, an English NER\ncorpus based on Tweebank V2 (TB2), train state-of-the-art (SOTA) Tweet NLP\nmodels on TB2, and release an NLP pipeline called Twitter-Stanza. We annotate\nnamed entities in TB2 using Amazon Mechanical Turk and measure the quality of\nour annotations. We train the Stanza pipeline on TB2 and compare with\nalternative NLP frameworks (e.g., FLAIR, spaCy) and transformer-based models.\nThe Stanza tokenizer and lemmatizer achieve SOTA performance on TB2, while the\nStanza NER tagger, part-of-speech (POS) tagger, and dependency parser achieve\ncompetitive performance against non-transformer models. The transformer-based\nmodels establish a strong baseline in Tweebank-NER and achieve the new SOTA\nperformance in POS tagging and dependency parsing on TB2. We release the\ndataset and make both the Stanza pipeline and BERTweet-based models available\n\"off-the-shelf\" for use in future Tweet NLP research. Our source code, data,\nand pre-trained models are available at:\n\\url{https://github.com/social-machines/TweebankNLP}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yining Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beeferman_D/0/1/0/all/0/1\">Doug Beeferman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1\">Deb Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Generative Pretraining for Multimodal Video Captioning. (arXiv:2201.08264v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08264","description":"<p>Recent video and language pretraining frameworks lack the ability to generate\nsentences. We present Multimodal Video Generative Pretraining (MV-GPT), a new\npretraining framework for learning from unlabelled videos which can be\neffectively used for generative tasks such as multimodal video captioning.\nUnlike recent video-language pretraining frameworks, our framework trains both\na multimodal video encoder and a sentence decoder jointly. To overcome the lack\nof captions in unlabelled videos, we leverage the future utterance as an\nadditional text source and propose a bidirectional generation objective -- we\ngenerate future utterances given the present mulitmodal context, and also the\npresent utterance given future observations. With this objective, we train an\nencoder-decoder model end-to-end to generate a caption from raw pixels and\ntranscribed speech directly. Our model achieves state-of-the-art performance\nfor multimodal video captioning on four standard benchmarks, as well as for\nother video understanding tasks such as VideoQA, video retrieval and action\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seo_P/0/1/0/all/0/1\">Paul Hongsuck Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey of Hallucination in Natural Language Generation. (arXiv:2202.03629v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.03629","description":"<p>Natural Language Generation (NLG) has improved exponentially in recent years\nthanks to the development of sequence-to-sequence deep learning technologies\nsuch as Transformer-based language models. This advancement has led to more\nfluent and coherent NLG, leading to improved development in downstream tasks\nsuch as abstractive summarization, dialogue generation and data-to-text\ngeneration. However, it is also apparent that deep learning based generation is\nprone to hallucinate unintended text, which degrades the system performance and\nfails to meet user expectations in many real-world scenarios. To address this\nissue, many studies have been presented in measuring and mitigating\nhallucinated texts, but these have never been reviewed in a comprehensive\nmanner before. In this survey, we thus provide a broad overview of the research\nprogress and challenges in the hallucination problem in NLG. The survey is\norganized into two parts: (1) a general overview of metrics, mitigation\nmethods, and future directions; and (2) an overview of task-specific research\nprogress on hallucinations in the following downstream tasks, namely\nabstractive summarization, dialogue generation, generative question answering,\ndata-to-text generation, and machine translation. This survey serves to\nfacilitate collaborative efforts among researchers in tackling the challenge of\nhallucinated texts in NLG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Ziwei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieske_R/0/1/0/all/0/1\">Rita Frieske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1\">Etsuko Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1\">Yejin Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic features of object concepts generated with GPT-3. (arXiv:2202.03753v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.03753","description":"<p>Semantic features have been playing a central role in investigating the\nnature of our conceptual representations. Yet the enormous time and effort\nrequired to empirically sample and norm features from human raters has\nrestricted their use to a limited set of manually curated concepts. Given\nrecent promising developments with transformer-based language models, here we\nasked whether it was possible to use such models to automatically generate\nmeaningful lists of properties for arbitrary object concepts and whether these\nmodels would produce features similar to those found in humans. To this end, we\nprobed a GPT-3 model to generate semantic features for 1,854 objects and\ncompared automatically-generated features to existing human feature norms.\nGPT-3 generated many more features than humans, yet showed a similar\ndistribution in the types of generated features. Generated feature norms\nrivaled human norms in predicting similarity, relatedness, and category\nmembership, while variance partitioning demonstrated that these predictions\nwere driven by similar variance in humans and GPT-3. Together, these results\nhighlight the potential of large language models to capture important facets of\nhuman knowledge and yield a new approach for automatically generating\ninterpretable feature sets, thus drastically expanding the potential use of\nsemantic features in psychological and linguistic studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hansen_H/0/1/0/all/0/1\">Hannes Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hebart_M/0/1/0/all/0/1\">Martin N. Hebart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building and curating conversational corpora for diversity-aware language science and technology. (arXiv:2203.03399v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03399","description":"<p>We present an analysis pipeline and best practice guidelines for building and\ncurating corpora of everyday conversation in diverse languages. Surveying\nlanguage documentation corpora and other resources that cover 67 languages and\nvarieties from 28 phyla, we describe the compilation and curation process,\nspecify minimal properties of a unified format for interactional data, and\ndevelop methods for quality control that take into account turn-taking and\ntiming. Two case studies show the broad utility of conversational data for (i)\ncharting human interactional infrastructure and (ii) tracing challenges and\nopportunities for current ASR solutions. Linguistically diverse conversational\ncorpora can provide new insights for the language sciences and stronger\nempirical foundations for language technology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liesenfeld_A/0/1/0/all/0/1\">Andreas Liesenfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dingemanse_M/0/1/0/all/0/1\">Mark Dingemanse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection. (arXiv:2203.09509v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09509","description":"<p>Toxic language detection systems often falsely flag text that contains\nminority group mentions as toxic, as those groups are often the targets of\nonline hate. Such over-reliance on spurious correlations also causes systems to\nstruggle with detecting implicitly toxic language. To help mitigate these\nissues, we create ToxiGen, a new large-scale and machine-generated dataset of\n274k toxic and benign statements about 13 minority groups. We develop a\ndemonstration-based prompting framework and an adversarial\nclassifier-in-the-loop decoding method to generate subtly toxic and benign text\nwith a massive pretrained language model. Controlling machine generation in\nthis way allows ToxiGen to cover implicitly toxic text at a larger scale, and\nabout more demographic groups, than previous resources of human-written text.\nWe conduct a human evaluation on a challenging subset of ToxiGen and find that\nannotators struggle to distinguish machine-generated text from human-written\nlanguage. We also find that 94.5% of toxic examples are labeled as hate speech\nby human annotators. Using three publicly-available datasets, we show that\nfinetuning a toxicity classifier on our data improves its performance on\nhuman-written data substantially. We also demonstrate that ToxiGen can be used\nto fight machine-generated toxicity as finetuning improves the classifier\nsignificantly on our evaluation subset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hartvigsen_T/0/1/0/all/0/1\">Thomas Hartvigsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabriel_S/0/1/0/all/0/1\">Saadia Gabriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1\">Hamid Palangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_D/0/1/0/all/0/1\">Dipankar Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1\">Ece Kamar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Converse: A Tree-Based Modular Task-Oriented Dialogue System. (arXiv:2203.12187v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12187","description":"<p>Creating a system that can have meaningful conversations with humans to help\naccomplish tasks is one of the ultimate goals of Artificial Intelligence (AI).\nIt has defined the meaning of AI since the beginning. A lot has been\naccomplished in this area recently, with voice assistant products entering our\ndaily lives and chat bot systems becoming commonplace in customer service. At\nfirst glance there seems to be no shortage of options for dialogue systems.\nHowever, the frequently deployed dialogue systems today seem to all struggle\nwith a critical weakness - they are hard to build and harder to maintain. At\nthe core of the struggle is the need to script every single turn of\ninteractions between the bot and the human user. This makes the dialogue\nsystems more difficult to maintain as the tasks become more complex and more\ntasks are added to the system. In this paper, we propose Converse, a flexible\ntree-based modular task-oriented dialogue system. Converse uses an and-or tree\nstructure to represent tasks and offers powerful multi-task dialogue\nmanagement. Converse supports task dependency and task switching, which are\nunique features compared to other open-source dialogue frameworks. At the same\ntime, Converse aims to make the bot building process easy and simple, for both\nprofessional and non-professional software developers. The code is available at\nhttps://github.com/salesforce/Converse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1\">Angela S. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feihong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kazuma Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Young Mo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1\">Michael Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1\">Richard Socher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation. (arXiv:2204.01171v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.01171","description":"<p>Current language generation models suffer from issues such as repetition,\nincoherence, and hallucinations. An often-repeated hypothesis is that this\nbrittleness of generation models is caused by the training and the generation\nprocedure mismatch, also referred to as exposure bias. In this paper, we verify\nthis hypothesis by analyzing exposure bias from an imitation learning\nperspective. We show that exposure bias leads to an accumulation of errors,\nanalyze why perplexity fails to capture this accumulation, and empirically show\nthat this accumulation results in poor generation quality. Source code to\nreproduce these experiments is available at\nhttps://github.com/kushalarora/quantifying_exposure_bias\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_K/0/1/0/all/0/1\">Kushal Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asri_L/0/1/0/all/0/1\">Layla El Asri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahuleyan_H/0/1/0/all/0/1\">Hareesh Bahuleyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie Chi Kit Cheung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Noisy Label Correction for Fine-Grained Entity Typing. (arXiv:2205.03011v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.03011","description":"<p>Fine-grained entity typing (FET) aims to assign proper semantic types to\nentity mentions according to their context, which is a fundamental task in\nvarious entity-leveraging applications. Current FET systems usually establish\non large-scale weakly-supervised/distantly annotation data, which may contain\nabundant noise and thus severely hinder the performance of the FET task.\nAlthough previous studies have made great success in automatically identifying\nthe noisy labels in FET, they usually rely on some auxiliary resources which\nmay be unavailable in real-world applications (e.g. pre-defined hierarchical\ntype structures, human-annotated subsets). In this paper, we propose a novel\napproach to automatically correct noisy labels for FET without external\nresources. Specifically, it first identifies the potentially noisy labels by\nestimating the posterior probability of a label being positive or negative\naccording to the logits output by the model, and then relabel candidate noisy\nlabels by training a robust model over the remaining clean labels. Experiments\non two popular benchmarks prove the effectiveness of our method. Our source\ncode can be obtained from https://github.com/CCIIPLab/DenoiseFET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Weiran Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feida Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniMorph 4.0: Universal Morphology. (arXiv:2205.03608v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.03608","description":"<p>The Universal Morphology (UniMorph) project is a collaborative effort\nproviding broad-coverage instantiated normalized morphological inflection\ntables for hundreds of diverse world languages. The project comprises two major\nthrusts: a language-independent feature schema for rich morphological\nannotation and a type-level resource of annotated data in diverse languages\nrealizing that schema. This paper presents the expansions and improvements made\non several fronts over the last couple of years (since McCarthy et al. (2020)).\nCollaborative efforts by numerous linguists have added 67 new languages,\nincluding 30 endangered languages. We have implemented several improvements to\nthe extraction pipeline to tackle some issues, e.g. missing gender and macron\ninformation. We have also amended the schema to use a hierarchical structure\nthat is needed for morphological phenomena like multiple-argument agreement and\ncase stacking, while adding some missing morphological features to make the\nschema more inclusive. In light of the last UniMorph release, we also augmented\nthe database with morpheme segmentation for 16 languages. Lastly, this new\nrelease makes a push towards inclusion of derivational morphology in UniMorph\nby enriching the data and annotation schema with instances representing\nderivational processes from MorphyNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Batsuren_K/0/1/0/all/0/1\">Khuyagbaatar Batsuren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1\">Omer Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalifa_S/0/1/0/all/0/1\">Salam Khalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kieras_W/0/1/0/all/0/1\">Witold Kiera&#x15b;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bella_G/0/1/0/all/0/1\">G&#xe1;bor Bella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonard_B/0/1/0/all/0/1\">Brian Leonard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicolai_G/0/1/0/all/0/1\">Garrett Nicolai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorman_K/0/1/0/all/0/1\">Kyle Gorman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ate_Y/0/1/0/all/0/1\">Yustinus Ghanggo Ate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryskina_M/0/1/0/all/0/1\">Maria Ryskina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mielke_S/0/1/0/all/0/1\">Sabrina J. Mielke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budianskaya_E/0/1/0/all/0/1\">Elena Budianskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Khaissi_C/0/1/0/all/0/1\">Charbel El-Khaissi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasser_M/0/1/0/all/0/1\">Michael Gasser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_W/0/1/0/all/0/1\">William Lane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_M/0/1/0/all/0/1\">Mohit Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coler_M/0/1/0/all/0/1\">Matt Coler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samame_J/0/1/0/all/0/1\">Jaime Rafael Montoya Samame</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camaiteri_D/0/1/0/all/0/1\">Delio Siticonatzi Camaiteri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rojas_E/0/1/0/all/0/1\">Esa&#xfa; Zumaeta Rojas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francis_D/0/1/0/all/0/1\">Didier L&#xf3;pez Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oncevay_A/0/1/0/all/0/1\">Arturo Oncevay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bautista_J/0/1/0/all/0/1\">Juan L&#xf3;pez Bautista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_G/0/1/0/all/0/1\">Gema Celeste Silva Villegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigen_L/0/1/0/all/0/1\">Lucas Torroba Hennigen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ek_A/0/1/0/all/0/1\">Adam Ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guriel_D/0/1/0/all/0/1\">David Guriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dirix_P/0/1/0/all/0/1\">Peter Dirix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernardy_J/0/1/0/all/0/1\">Jean-Philippe Bernardy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherbakov_A/0/1/0/all/0/1\">Andrey Scherbakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayyr_ool_A/0/1/0/all/0/1\">Aziyana Bayyr-ool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zariquiey_R/0/1/0/all/0/1\">Roberto Zariquiey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheifer_K/0/1/0/all/0/1\">Karina Sheifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganieva_S/0/1/0/all/0/1\">Sofya Ganieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_H/0/1/0/all/0/1\">Hilaria Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karahoga_R/0/1/0/all/0/1\">Ritv&#xe1;n Karah&#xf3;&#x1e7;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markantonatou_S/0/1/0/all/0/1\">Stella Markantonatou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlidis_G/0/1/0/all/0/1\">George Pavlidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plugaryov_M/0/1/0/all/0/1\">Matvey Plugaryov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klyachko_E/0/1/0/all/0/1\">Elena Klyachko</a>, et al. (52 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scheduled Multi-task Learning for Neural Chat Translation. (arXiv:2205.03766v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.03766","description":"<p>Neural Chat Translation (NCT) aims to translate conversational text into\ndifferent languages. Existing methods mainly focus on modeling the bilingual\ndialogue characteristics (e.g., coherence) to improve chat translation via\nmulti-task learning on small-scale chat translation data. Although the NCT\nmodels have achieved impressive success, it is still far from satisfactory due\nto insufficient chat translation data and simple joint training manners. To\naddress the above issues, we propose a scheduled multi-task learning framework\nfor NCT. Specifically, we devise a three-stage training framework to\nincorporate the large-scale in-domain chat translation data into training by\nadding a second pre-training stage between the original pre-training and\nfine-tuning stages. Further, we investigate where and how to schedule the\ndialogue-related auxiliary tasks in multiple training stages to effectively\nenhance the main chat translation task. Extensive experiments in four language\ndirections (English-Chinese and English-German) verify the effectiveness and\nsuperiority of the proposed approach. Additionally, we have made the\nlarge-scale in-domain paired bilingual dialogue dataset publicly available to\nthe research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Aware Abbreviation Expansion Using Large Language Models. (arXiv:2205.03767v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.03767","description":"<p>Motivated by the need for accelerating text entry in augmentative and\nalternative communication (AAC) for people with severe motor impairments, we\npropose a paradigm in which phrases are abbreviated aggressively as primarily\nword-initial letters. Our approach is to expand the abbreviations into\nfull-phrase options by leveraging conversation context with the power of\npretrained large language models (LLMs). Through zero-shot, few-shot, and\nfine-tuning experiments on four public conversation datasets, we show that for\nreplies to the initial turn of a dialog, an LLM with 64B parameters is able to\nexactly expand over 70% of phrases with abbreviation length up to 10, leading\nto an effective keystroke saving rate of up to about 77% on these exact\nexpansions. Including a small amount of context in the form of a single\nconversation turn more than doubles abbreviation expansion accuracies compared\nto having no context, an effect that is more pronounced for longer phrases.\nAdditionally, the robustness of models against typo noise can be enhanced\nthrough fine-tuning on noisy data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shanqing Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venugopalan_S/0/1/0/all/0/1\">Subhashini Venugopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomanek_K/0/1/0/all/0/1\">Katrin Tomanek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1\">Ajit Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_M/0/1/0/all/0/1\">Meredith R. Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brenner_M/0/1/0/all/0/1\">Michael P. Brenner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality. (arXiv:2205.04421v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2205.04421","description":"<p>Text to speech (TTS) has made rapid progress in both academia and industry in\nrecent years. Some questions naturally arise that whether a TTS system can\nachieve human-level quality, how to define/judge that quality and how to\nachieve it. In this paper, we answer these questions by first defining the\nhuman-level quality based on the statistical significance of subjective measure\nand introducing appropriate guidelines to judge it, and then developing a TTS\nsystem called NaturalSpeech that achieves human-level quality on a benchmark\ndataset. Specifically, we leverage a variational autoencoder (VAE) for\nend-to-end text to waveform generation, with several key modules to enhance the\ncapacity of the prior from text and reduce the complexity of the posterior from\nspeech, including phoneme pre-training, differentiable duration modeling,\nbidirectional prior/posterior modeling, and a memory mechanism in VAE.\nExperiment evaluations on popular LJSpeech dataset show that our proposed\nNaturalSpeech achieves -0.01 CMOS (comparative mean opinion score) to human\nrecordings at the sentence level, with Wilcoxon signed rank test at p-level p\n&gt;&gt; 0.05, which demonstrates no statistically significant difference from human\nrecordings for the first time on this dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Haohe Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cong_J/0/1/0/all/0/1\">Jian Cong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yanqing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yi_Y/0/1/0/all/0/1\">Yuanhao Yi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soong_F/0/1/0/all/0/1\">Frank Soong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Simplification by Tagging. (arXiv:2103.05070v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2103.05070","description":"<p>Edit-based approaches have recently shown promising results on multiple\nmonolingual sequence transduction tasks. In contrast to conventional\nsequence-to-sequence (Seq2Seq) models, which learn to generate text from\nscratch as they are trained on parallel corpora, these methods have proven to\nbe much more effective since they are able to learn to make fast and accurate\ntransformations while leveraging powerful pre-trained language models. Inspired\nby these ideas, we present TST, a simple and efficient Text Simplification\nsystem based on sequence Tagging, leveraging pre-trained Transformer-based\nencoders. Our system makes simplistic data augmentations and tweaks in training\nand inference on a pre-existing system, which makes it less reliant on large\namounts of parallel training data, provides more control over the outputs and\nenables faster inference speeds. Our best model achieves near state-of-the-art\nperformance on benchmark test datasets for the task. Since it is fully\nnon-autoregressive, it achieves faster inference speeds by over 11 times than\nthe current state-of-the-art text simplification system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Omelianchuk_K/0/1/0/all/0/1\">Kostiantyn Omelianchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raheja_V/0/1/0/all/0/1\">Vipul Raheja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skurzhanskyi_O/0/1/0/all/0/1\">Oleksandr Skurzhanskyi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Differentiable Electron Microscopy Simulation: Methods and Applications for Visualization. (arXiv:2205.04464v1 [q-bio.QM])","link":"http://arxiv.org/abs/2205.04464","description":"<p>We propose a new microscopy simulation system that can depict atomistic\nmodels in a micrograph visual style, similar to results of physical electron\nmicroscopy imaging. This system is scalable, able to represent simulation of\nelectron microscopy of tens of viral particles and synthesizes the image faster\nthan previous methods. On top of that, the simulator is differentiable, both\nits deterministic as well as stochastic stages that form signal and noise\nrepresentations in the micrograph. This notable property has the capability for\nsolving inverse problems by means of optimization and thus allows for\ngeneration of microscopy simulations using the parameter settings estimated\nfrom real data. We demonstrate this learning capability through two\napplications: (1) estimating the parameters of the modulation transfer function\ndefining the detector properties of the simulated and real micrographs, and (2)\ndenoising the real data based on parameters trained from the simulated\nexamples. While current simulators do not support any parameter estimation due\nto their forward design, we show that the results obtained using estimated\nparameters are very similar to the results of real micrographs. Additionally,\nwe evaluate the denoising capabilities of our approach and show that the\nresults showed an improvement over state-of-the-art methods. Denoised\nmicrographs exhibit less noise in the tilt-series tomography reconstructions,\nultimately reducing the visual dominance of noise in direct volume rendering of\nmicroscopy tomograms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Nguyen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liang_F/0/1/0/all/0/1\">Feng Liang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Engel_D/0/1/0/all/0/1\">Dominik Engel</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bohak_C/0/1/0/all/0/1\">Ciril Bohak</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wonka_P/0/1/0/all/0/1\">Peter Wonka</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ropinski_T/0/1/0/all/0/1\">Timo Ropinski</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Viola_I/0/1/0/all/0/1\">Ivan Viola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skin disease diagnosis using image analysis and natural language processing. (arXiv:2205.04468v1 [eess.IV])","link":"http://arxiv.org/abs/2205.04468","description":"<p>In Zambia, there is a serious shortage of medical staff where each\npractitioner attends to about 17000 patients in a given district while still,\nother patients travel over 10 km to access the basic medical services. In this\nresearch, we implement a deep learning model that can perform the clinical\ndiagnosis process. The study will prove whether image analysis is capable of\nperforming clinical diagnosis. It will also enable us to understand if we can\nuse image analysis to lessen the workload on medical practitioners by\ndelegating some tasks to an AI. The success of this study has the potential to\nincrease the accessibility of medical services to Zambians, which is one of the\nnational goals of Vision 2030.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chileshe_M/0/1/0/all/0/1\">Martin Chileshe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nyirenda_M/0/1/0/all/0/1\">Mayumbo Nyirenda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiview Stereo with Cascaded Epipolar RAFT. (arXiv:2205.04502v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04502","description":"<p>We address multiview stereo (MVS), an important 3D vision task that\nreconstructs a 3D model such as a dense point cloud from multiple calibrated\nimages. We propose CER-MVS (Cascaded Epipolar RAFT Multiview Stereo), a new\napproach based on the RAFT (Recurrent All-Pairs Field Transforms) architecture\ndeveloped for optical flow. CER-MVS introduces five new changes to RAFT:\nepipolar cost volumes, cost volume cascading, multiview fusion of cost volumes,\ndynamic supervision, and multiresolution fusion of depth maps. CER-MVS is\nsignificantly different from prior work in multiview stereo. Unlike prior work,\nwhich operates by updating a 3D cost volume, CER-MVS operates by updating a\ndisparity field. Furthermore, we propose an adaptive thresholding method to\nbalance the completeness and accuracy of the reconstructed point clouds.\nExperiments show that our approach achieves competitive performance on DTU (the\nsecond best among known results) and state-of-the-art performance on the\nTanks-and-Temples benchmark (both the intermediate and advanced set). Code is\navailable at https://github.com/princeton-vl/CER-MVS\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zeyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teed_Z/0/1/0/all/0/1\">Zachary Teed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jia Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image2Gif: Generating Continuous Realistic Animations with Warping NODEs. (arXiv:2205.04519v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04519","description":"<p>Generating smooth animations from a limited number of sequential observations\nhas a number of applications in vision. For example, it can be used to increase\nnumber of frames per second, or generating a new trajectory only based on first\nand last frames, e.g. a motion of face emotions. Despite the discrete observed\ndata (frames), the problem of generating a new trajectory is a continues\nproblem. In addition, to be perceptually realistic, the domain of an image\nshould not alter drastically through the trajectory of changes. In this paper,\nwe propose a new framework, Warping Neural ODE, for generating a smooth\nanimation (video frame interpolation) in a continuous manner, given two\n(\"farther apart\") frames, denoting the start and the end of the animation. The\nkey feature of our framework is utilizing the continuous spatial transformation\nof the image based on the vector field, derived from a system of differential\nequations. This allows us to achieve the smoothness and the realism of an\nanimation with infinitely small time steps between the frames. We show the\napplication of our work in generating an animation given two frames, in\ndifferent training settings, including Generative Adversarial Network (GAN) and\nwith $L_2$ loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nazarovs_J/0/1/0/all/0/1\">Jurijs Nazarovs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhichun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surreal-GAN:Semi-Supervised Representation Learning via GAN for uncovering heterogeneous disease-related imaging patterns. (arXiv:2205.04523v1 [cs.LG])","link":"http://arxiv.org/abs/2205.04523","description":"<p>A plethora of machine learning methods have been applied to imaging data,\nenabling the construction of clinically relevant imaging signatures of\nneurological and neuropsychiatric diseases. Oftentimes, such methods don't\nexplicitly model the heterogeneity of disease effects, or approach it via\nnonlinear models that are not interpretable. Moreover, unsupervised methods may\nparse heterogeneity that is driven by nuisance confounding factors that affect\nbrain structure or function, rather than heterogeneity relevant to a pathology\nof interest. On the other hand, semi-supervised clustering methods seek to\nderive a dichotomous subtype membership, ignoring the truth that disease\nheterogeneity spatially and temporally extends along a continuum. To address\nthe aforementioned limitations, herein, we propose a novel method, termed\nSurreal-GAN (Semi-SUpeRvised ReprEsentAtion Learning via GAN). Using\ncross-sectional imaging data, Surreal-GAN dissects underlying disease-related\nheterogeneity under the principle of semi-supervised clustering (cluster\nmappings from normal control to patient), proposes a continuously dimensional\nrepresentation, and infers the disease severity of patients at individual level\nalong each dimension. The model first learns a transformation function from\nnormal control (CN) domain to the patient (PT) domain with latent variables\ncontrolling transformation directions. An inverse mapping function together\nwith regularization on function continuity, pattern orthogonality and\nmonotonicity was also imposed to make sure that the transformation function\ncaptures necessarily meaningful imaging patterns with clinical significance. We\nfirst validated the model through extensive semi-synthetic experiments, and\nthen demonstrate its potential in capturing biologically plausible imaging\npatterns in Alzheimer's disease (AD).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhijian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Junhao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davatzikos_C/0/1/0/all/0/1\">Christos Davatzikos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does Frequency Bias Affect the Robustness of Neural Image Classifiers against Common Corruption and Adversarial Perturbations?. (arXiv:2205.04533v1 [cs.LG])","link":"http://arxiv.org/abs/2205.04533","description":"<p>Model robustness is vital for the reliable deployment of machine learning\nmodels in real-world applications. Recent studies have shown that data\naugmentation can result in model over-relying on features in the low-frequency\ndomain, sacrificing performance against low-frequency corruptions, highlighting\na connection between frequency and robustness. Here, we take one step further\nto more directly study the frequency bias of a model through the lens of its\nJacobians and its implication to model robustness. To achieve this, we propose\nJacobian frequency regularization for models' Jacobians to have a larger ratio\nof low-frequency components. Through experiments on four image datasets, we\nshow that biasing classifiers towards low (high)-frequency components can bring\nperformance gain against high (low)-frequency corruption and adversarial\nperturbation, albeit with a tradeoff in performance for low (high)-frequency\ncorruption. Our approach elucidates a more direct connection between the\nfrequency bias and robustness of deep learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Alvin Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1\">Yew-Soon Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Clement Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is my Depth Ground-Truth Good Enough? HAMMER -- Highly Accurate Multi-Modal Dataset for DEnse 3D Scene Regression. (arXiv:2205.04565v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04565","description":"<p>Depth estimation is a core task in 3D computer vision. Recent methods\ninvestigate the task of monocular depth trained with various depth sensor\nmodalities. Every sensor has its advantages and drawbacks caused by the nature\nof estimates. In the literature, mostly mean average error of the depth is\ninvestigated and sensor capabilities are typically not discussed. Especially\nindoor environments, however, pose challenges for some devices. Textureless\nregions pose challenges for structure from motion, reflective materials are\nproblematic for active sensing, and distances for translucent material are\nintricate to measure with existing sensors. This paper proposes HAMMER, a\ndataset comprising depth estimates from multiple commonly used sensors for\nindoor depth estimation, namely ToF, stereo, structured light together with\nmonocular RGB+P data. We construct highly reliable ground truth depth maps with\nthe help of 3D scanners and aligned renderings. A popular depth estimators is\ntrained on this data and typical depth senosors. The estimates are extensively\nanalyze on different scene structures. We notice generalization issues arising\nfrom various sensor technologies in household environments with challenging but\neveryday scene content. HAMMER, which we make publicly available, provides a\nreliable base to pave the way to targeted depth improvements and sensor fusion\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">HyunJun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruhkamp_P/0/1/0/all/0/1\">Patrick Ruhkamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangyao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brasch_N/0/1/0/all/0/1\">Nikolas Brasch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verdie_Y/0/1/0/all/0/1\">Yannick Verdie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jifei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armagan_A/0/1/0/all/0/1\">Anil Armagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilic_S/0/1/0/all/0/1\">Slobodan Ilic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ales Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When does dough become a bagel? Analyzing the remaining mistakes on ImageNet. (arXiv:2205.04596v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04596","description":"<p>Image classification accuracy on the ImageNet dataset has been a barometer\nfor progress in computer vision over the last decade. Several recent papers\nhave questioned the degree to which the benchmark remains useful to the\ncommunity, yet innovations continue to contribute gains to performance, with\ntoday's largest models achieving 90%+ top-1 accuracy. To help contextualize\nprogress on ImageNet and provide a more meaningful evaluation for today's\nstate-of-the-art models, we manually review and categorize every remaining\nmistake that a few top models make in order to provide insight into the\nlong-tail of errors on one of the most benchmarked datasets in computer vision.\nWe focus on the multi-label subset evaluation of ImageNet, where today's best\nmodels achieve upwards of 97% top-1 accuracy. Our analysis reveals that nearly\nhalf of the supposed mistakes are not mistakes at all, and we uncover new valid\nmulti-labels, demonstrating that, without careful review, we are significantly\nunderestimating the performance of these models. On the other hand, we also\nfind that today's best models still make a significant number of mistakes (40%)\nthat are obviously wrong to human reviewers. To calibrate future progress on\nImageNet, we provide an updated multi-label evaluation set, and we curate\nImageNet-Major: a 68-example \"major error\" slice of the obvious mistakes made\nby today's top models -- a slice where models should achieve near perfection,\nbut today are far from doing so.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1\">Vijay Vasudevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caine_B/0/1/0/all/0/1\">Benjamin Caine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gontijo_Lopes_R/0/1/0/all/0/1\">Raphael Gontijo-Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fridovich_Keil_S/0/1/0/all/0/1\">Sara Fridovich-Keil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoDo: Contrastive Learning with Downstream Background Invariance for Detection. (arXiv:2205.04617v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04617","description":"<p>The prior self-supervised learning researches mainly select image-level\ninstance discrimination as pretext task. It achieves a fantastic classification\nperformance that is comparable to supervised learning methods. However, with\ndegraded transfer performance on downstream tasks such as object detection. To\nbridge the performance gap, we propose a novel object-level self-supervised\nlearning method, called Contrastive learning with Downstream background\ninvariance (CoDo). The pretext task is converted to focus on instance location\nmodeling for various backgrounds, especially for downstream datasets. The\nability of background invariance is considered vital for object detection.\nFirstly, a data augmentation strategy is proposed to paste the instances onto\nbackground images, and then jitter the bounding box to involve background\ninformation. Secondly, we implement architecture alignment between our\npretraining network and the mainstream detection pipelines. Thirdly,\nhierarchical and multi views contrastive learning is designed to improve\nperformance of visual representation learning. Experiments on MSCOCO\ndemonstrate that the proposed CoDo with common backbones, ResNet50-FPN, yields\nstrong transfer learning results for object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hong Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KEMP: Keyframe-Based Hierarchical End-to-End Deep Model for Long-Term Trajectory Prediction. (arXiv:2205.04624v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04624","description":"<p>Predicting future trajectories of road agents is a critical task for\nautonomous driving. Recent goal-based trajectory prediction methods, such as\nDenseTNT and PECNet, have shown good performance on prediction tasks on public\ndatasets. However, they usually require complicated goal-selection algorithms\nand optimization. In this work, we propose KEMP, a hierarchical end-to-end deep\nlearning framework for trajectory prediction. At the core of our framework is\nkeyframe-based trajectory prediction, where keyframes are representative states\nthat trace out the general direction of the trajectory. KEMP first predicts\nkeyframes conditioned on the road context, and then fills in intermediate\nstates conditioned on the keyframes and the road context. Under our general\nframework, goal-conditioned methods are special cases in which the number of\nkeyframes equal to one. Unlike goal-conditioned methods, our keyframe predictor\nis learned automatically and does not require hand-crafted goal-selection\nalgorithms. We evaluate our model on public benchmarks and our model ranked 1st\non Waymo Open Motion Dataset Leaderboard (as of September 1, 2021).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qiujing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Weiqiao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_J/0/1/0/all/0/1\">Jeffrey Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varadarajan_B/0/1/0/all/0/1\">Balakrishnan Varadarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Covington_P/0/1/0/all/0/1\">Paul Covington</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using frequency attention to make adversarial patch powerful against person detector. (arXiv:2205.04638v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04638","description":"<p>Deep neural networks (DNNs) are vulnerable to adversarial attacks. In\nparticular, object detectors may be attacked by applying a particular\nadversarial patch to the image. However, because the patch shrinks during\npreprocessing, most existing approaches that employ adversarial patches to\nattack object detectors would diminish the attack success rate on small and\nmedium targets. This paper proposes a Frequency Module(FRAN), a\nfrequency-domain attention module for guiding patch generation. This is the\nfirst study to introduce frequency domain attention to optimize the attack\ncapabilities of adversarial patches. Our method increases the attack success\nrates of small and medium targets by 4.18% and 3.89%, respectively, over the\nstate-of-the-art attack method for fooling the human detector while assaulting\nYOLOv3 without reducing the attack success rate of big targets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1\">Xiaochun Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zetao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ting_G/0/1/0/all/0/1\">Gongzhao Ting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xiang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Linjun Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STDC-MA Network for Semantic Segmentation. (arXiv:2205.04639v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04639","description":"<p>Semantic segmentation is applied extensively in autonomous driving and\nintelligent transportation with methods that highly demand spatial and semantic\ninformation. Here, an STDC-MA network is proposed to meet these demands. First,\nthe STDC-Seg structure is employed in STDC-MA to ensure a lightweight and\nefficient structure. Subsequently, the feature alignment module (FAM) is\napplied to understand the offset between high-level and low-level features,\nsolving the problem of pixel offset related to upsampling on the high-level\nfeature map. Our approach implements the effective fusion between high-level\nfeatures and low-level features. A hierarchical multiscale attention mechanism\nis adopted to reveal the relationship among attention regions from two\ndifferent input sizes of one image. Through this relationship, regions\nreceiving much attention are integrated into the segmentation results, thereby\nreducing the unfocused regions of the input image and improving the effective\nutilization of multiscale features. STDC- MA maintains the segmentation speed\nas an STDC-Seg network while improving the segmentation accuracy of small\nobjects. STDC-MA was verified on the verification set of Cityscapes. The\nsegmentation result of STDC-MA attained 76.81% mIOU with the input of 0.5x\nscale, 3.61% higher than STDC-Seg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1\">Xiaochun Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Linjun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zetao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ting_G/0/1/0/all/0/1\">Gongzao Ting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaming Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Monitoring and Insect Behavioural Analysis Using Computer Vision for Precision Pollination. (arXiv:2205.04675v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04675","description":"<p>Insects are the most important global pollinator of crops and play a key role\nin maintaining the sustainability of natural ecosystems. Insect pollination\nmonitoring and management are therefore essential for improving crop production\nand food security. Computer vision facilitated pollinator monitoring can\nintensify data collection over what is feasible using manual approaches. The\nnew data it generates may provide a detailed understanding of insect\ndistributions and facilitate fine-grained analysis sufficient to predict their\npollination efficacy and underpin precision pollination. Current computer\nvision facilitated insect tracking in complex outdoor environments is\nrestricted in spatial coverage and often constrained to a single insect\nspecies. This limits its relevance to agriculture. Therefore, in this article\nwe introduce a novel system to facilitate markerless data capture for insect\ncounting, insect motion tracking, behaviour analysis and pollination prediction\nacross large agricultural areas. Our system is comprised of Edge Computing\nmulti-point video recording, offline automated multi-species insect counting,\ntracking and behavioural analysis. We implement and test our system on a\ncommercial berry farm to demonstrate its capabilities. Our system successfully\ntracked four insect varieties, at nine monitoring stations within a\npoly-tunnel, obtaining an F-score above 0.8 for each variety. The system\nenabled calculation of key metrics to assess the relative pollination impact of\neach insect variety. With this technological advancement, detailed, ongoing\ndata collection for precision pollination becomes achievable. This is important\nto inform growers and apiarists managing crop pollination, as it allows\ndata-driven decisions to be made to improve food production and food security.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ratnayake_M/0/1/0/all/0/1\">Malika Nisal Ratnayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amarathunga_D/0/1/0/all/0/1\">Don Chathurika Amarathunga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaman_A/0/1/0/all/0/1\">Asaduz Zaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyer_A/0/1/0/all/0/1\">Adrian G. Dyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorin_A/0/1/0/all/0/1\">Alan Dorin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNITS: Unsupervised Intermediate Training Stage for Scene Text Detection. (arXiv:2205.04683v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04683","description":"<p>Recent scene text detection methods are almost based on deep learning and\ndata-driven. Synthetic data is commonly adopted for pre-training due to\nexpensive annotation cost. However, there are obvious domain discrepancies\nbetween synthetic data and real-world data. It may lead to sub-optimal\nperformance to directly adopt the model initialized by synthetic data in the\nfine-tuning stage. In this paper, we propose a new training paradigm for scene\ntext detection, which introduces an \\textbf{UN}supervised \\textbf{I}ntermediate\n\\textbf{T}raining \\textbf{S}tage (UNITS) that builds a buffer path to\nreal-world data and can alleviate the gap between the pre-training stage and\nfine-tuning stage. Three training strategies are further explored to perceive\ninformation from real-world data in an unsupervised way. With UNITS, scene text\ndetectors are improved without introducing any parameters and computations\nduring inference. Extensive experimental results show consistent performance\nimprovements on three public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Youhui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xugong Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OTFPF: Optimal Transport-Based Feature Pyramid Fusion Network for Brain Age Estimation with 3D Overlapped ConvNeXt. (arXiv:2205.04684v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04684","description":"<p>Chronological age of healthy brain is able to be predicted using deep neural\nnetworks from T1-weighted magnetic resonance images (T1 MRIs), and the\npredicted brain age could serve as an effective biomarker for detecting\naging-related diseases or disorders. In this paper, we propose an end-to-end\nneural network architecture, referred to as optimal transport based feature\npyramid fusion (OTFPF) network, for the brain age estimation with T1 MRIs. The\nOTFPF consists of three types of modules: Optimal Transport based Feature\nPyramid Fusion (OTFPF) module, 3D overlapped ConvNeXt (3D OL-ConvNeXt) module\nand fusion module. These modules strengthen the OTFPF network's understanding\nof each brain's semi-multimodal and multi-level feature pyramid information,\nand significantly improve its estimation performances. Comparing with recent\nstate-of-the-art models, the proposed OTFPF converges faster and performs\nbetter. The experiments with 11,728 MRIs aged 3-97 years show that OTFPF\nnetwork could provide accurate brain age estimation, yielding mean absolute\nerror (MAE) of 2.097, Pearson's correlation coefficient (PCC) of 0.993 and\nSpearman's rank correlation coefficient (SRCC) of 0.989, between the estimated\nand chronological ages. Widespread quantitative experiments and ablation\nexperiments demonstrate the superiority and rationality of OTFPF network. The\ncodes and implement details will be released on GitHub:\nhttps://github.com/ZJU-Brain/OTFPF after final decision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yalin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shunjie Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1\">Le Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xunzhao Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qianqian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_C/0/1/0/all/0/1\">Cheng Zhuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An asynchronous event-based algorithm for periodic signals. (arXiv:2205.04691v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04691","description":"<p>In this paper, we present a simple event-oriented algorithm for detection of\npixel-size signals with a known frequency, by the novel technology of an event\ncamera. In addition, we analyze the ability of the algorithm to filter out the\ndesired periodic signals from random fluctuations. We demonstrate this ability\nand show how the algorithm can distinguish, during twilight, between the\nsignals of a streetlight that flicker with frequency of 100 Hz, and sun glitter\noriginating from windows in far-away buildings in the field of view.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Ezra_D/0/1/0/all/0/1\">David El-Chai Ben-Ezra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arad_R/0/1/0/all/0/1\">Ron Arad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padowicz_A/0/1/0/all/0/1\">Ayelet Padowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tugendhaft_I/0/1/0/all/0/1\">Israel Tugendhaft</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Detection of Microaneurysms in OCT Images Using Bag of Features. (arXiv:2205.04695v1 [eess.IV])","link":"http://arxiv.org/abs/2205.04695","description":"<p>Diabetic Retinopathy (DR) caused by diabetes occurs as a result of changes in\nthe retinal vessels and causes visual impairment. Microaneurysms (MAs) are the\nearly clinical signs of DR, whose timely diagnosis can help detecting DR in the\nearly stages of its development. It has been observed that MAs are more common\nin the inner retinal layers compared to the outer retinal layers in eyes\nsuffering from DR. Optical Coherence Tomography (OCT) is a noninvasive imaging\ntechnique that provides a cross-sectional view of the retina and it has been\nused in recent years to diagnose many eye diseases. As a result, in this paper\nhas attempted to identify areas with MA from normal areas of the retina using\nOCT images. This work is done using the dataset collected from FA and OCT\nimages of 20 patients with DR. In this regard, firstly Fluorescein Angiography\n(FA) and OCT images were registered. Then the MA and normal areas were\nseparated and the features of each of these areas were extracted using the Bag\nof Features (BOF) approach with Speeded-Up Robust Feature (SURF) descriptor.\nFinally, the classification process was performed using a multilayer perceptron\nnetwork. For each of the criteria of accuracy, sensitivity, specificity, and\nprecision, the obtained results were 96.33%, 97.33%, 95.4%, and 95.28%,\nrespectively. Utilizing OCT images to detect MAsautomatically is a new idea and\nthe results obtained as preliminary research in this field are promising .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nasab_E/0/1/0/all/0/1\">Elahe Sadat Kazemi Nasab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Almasi_R/0/1/0/all/0/1\">Ramin Almasi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shoushtarian_B/0/1/0/all/0/1\">Bijan Shoushtarian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Golkar_E/0/1/0/all/0/1\">Ehsan Golkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rabbani_H/0/1/0/all/0/1\">Hossein Rabbani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Burst Raw Denoising with Variance Stabilization and Multi-frequency Denoising Network. (arXiv:2205.04721v1 [eess.IV])","link":"http://arxiv.org/abs/2205.04721","description":"<p>With the growing popularity of smartphones, capturing high-quality images is\nof vital importance to smartphones. The cameras of smartphones have small\napertures and small sensor cells, which lead to the noisy images in low light\nenvironment. Denoising based on a burst of multiple frames generally\noutperforms single frame denoising but with the larger compututional cost. In\nthis paper, we propose an efficient yet effective burst denoising system. We\nadopt a three-stage design: noise prior integration, multi-frame alignment and\nmulti-frame denoising. First, we integrate noise prior by pre-processing raw\nsignals into a variance-stabilization space, which allows using a small-scale\nnetwork to achieve competitive performance. Second, we observe that it is\nessential to adopt an explicit alignment for burst denoising, but it is not\nnecessary to integrate a learning-based method to perform multi-frame\nalignment. Instead, we resort to a conventional and efficient alignment method\nand combine it with our multi-frame denoising network. At last, we propose a\ndenoising strategy that processes multiple frames sequentially. Sequential\ndenoising avoids filtering a large number of frames by decomposing multiple\nframes denoising into several efficient sub-network denoising. As for each\nsub-network, we propose an efficient multi-frequency denoising network to\nremove noise of different frequencies. Our three-stage design is efficient and\nshows strong performance on burst denoising. Experiments on synthetic and real\nraw datasets demonstrate that our method outperforms state-of-the-art methods,\nwith less computational cost. Furthermore, the low complexity and high-quality\nperformance make deployment on smartphones possible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_D/0/1/0/all/0/1\">Dasong Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Law_K/0/1/0/all/0/1\">Ka Lung Law</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_H/0/1/0/all/0/1\">Hongwei Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Medical Image Classification from Noisy Labeled Data with Global and Local Representation Guided Co-training. (arXiv:2205.04723v1 [eess.IV])","link":"http://arxiv.org/abs/2205.04723","description":"<p>Deep neural networks have achieved remarkable success in a wide variety of\nnatural image and medical image computing tasks. However, these achievements\nindispensably rely on accurately annotated training data. If encountering some\nnoisy-labeled images, the network training procedure would suffer from\ndifficulties, leading to a sub-optimal classifier. This problem is even more\nsevere in the medical image analysis field, as the annotation quality of\nmedical images heavily relies on the expertise and experience of annotators. In\nthis paper, we propose a novel collaborative training paradigm with global and\nlocal representation learning for robust medical image classification from\nnoisy-labeled data to combat the lack of high quality annotated medical data.\nSpecifically, we employ the self-ensemble model with a noisy label filter to\nefficiently select the clean and noisy samples. Then, the clean samples are\ntrained by a collaborative training strategy to eliminate the disturbance from\nimperfect labeled samples. Notably, we further design a novel global and local\nrepresentation learning scheme to implicitly regularize the networks to utilize\nnoisy samples in a self-supervised manner. We evaluated our proposed robust\nlearning strategy on four public medical image classification datasets with\nthree types of label noise,ie,random noise, computer-generated label noise, and\ninter-observer variability noise. Our method outperforms other learning from\nnoisy label methods and we also conducted extensive experiments to analyze each\ncomponent of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xue_C/0/1/0/all/0/1\">Cheng Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_L/0/1/0/all/0/1\">Lequan Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_P/0/1/0/all/0/1\">Pengfei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-supervised segmentation of referring expressions. (arXiv:2205.04725v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04725","description":"<p>Visual grounding localizes regions (boxes or segments) in the image\ncorresponding to given referring expressions. In this work we address image\nsegmentation from referring expressions, a problem that has so far only been\naddressed in a fully-supervised setting. A fully-supervised setup, however,\nrequires pixel-wise supervision and is hard to scale given the expense of\nmanual annotation. We therefore introduce a new task of weakly-supervised image\nsegmentation from referring expressions and propose Text grounded semantic\nSEGgmentation (TSEG) that learns segmentation masks directly from image-level\nreferring expressions without pixel-level annotations. Our transformer-based\nmethod computes patch-text similarities and guides the classification objective\nduring training with a new multi-label patch assignment mechanism. The\nresulting visual grounding model segments image regions corresponding to given\nnatural language expressions. Our approach TSEG demonstrates promising results\nfor weakly-supervised referring expression segmentation on the challenging\nPhraseCut and RefCOCO datasets. TSEG also shows competitive performance when\nevaluated in a zero-shot setting for semantic segmentation on Pascal VOC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Strudel_R/0/1/0/all/0/1\">Robin Strudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-Temporal Transformer for Dynamic Facial Expression Recognition in the Wild. (arXiv:2205.04749v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04749","description":"<p>Previous methods for dynamic facial expression in the wild are mainly based\non Convolutional Neural Networks (CNNs), whose local operations ignore the\nlong-range dependencies in videos. To solve this problem, we propose the\nspatio-temporal Transformer (STT) to capture discriminative features within\neach frame and model contextual relationships among frames. Spatio-temporal\ndependencies are captured and integrated by our unified Transformer.\nSpecifically, given an image sequence consisting of multiple frames as input,\nwe utilize the CNN backbone to translate each frame into a visual feature\nsequence. Subsequently, the spatial attention and the temporal attention within\neach block are jointly applied for learning spatio-temporal representations at\nthe sequence level. In addition, we propose the compact softmax cross entropy\nloss to further encourage the learned features have the minimum intra-class\ndistance and the maximum inter-class distance. Experiments on two in-the-wild\ndynamic facial expression datasets (i.e., DFEW and AFEW) indicate that our\nmethod provides an effective way to make use of the spatial and temporal\ndependencies for dynamic facial expression recognition. The source code and the\ntraining logs will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fuyan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WG-VITON: Wearing-Guide Virtual Try-On for Top and Bottom Clothes. (arXiv:2205.04759v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04759","description":"<p>Studies of virtual try-on (VITON) have been shown their effectiveness in\nutilizing the generative neural network for virtually exploring fashion\nproducts, and some of recent researches of VITON attempted to synthesize human\nimage wearing given multiple types of garments (e.g., top and bottom clothes).\nHowever, when replacing the top and bottom clothes of the target human,\nnumerous wearing styles are possible with a certain combination of the clothes.\nIn this paper, we address the problem of variation in wearing style when\nsimultaneously replacing the top and bottom clothes of the model. We introduce\nWearing-Guide VITON (i.e., WG-VITON) which utilizes an additional input binary\nmask to control the wearing styles of the generated image. Our experiments show\nthat WG-VITON effectively generates an image of the model wearing given top and\nbottom clothes, and create complicated wearing styles such as partly tucking in\nthe top to the bottom\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Soonchan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinah Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable Deep Learning Methods in Medical Diagnosis: A Survey. (arXiv:2205.04766v1 [eess.IV])","link":"http://arxiv.org/abs/2205.04766","description":"<p>The remarkable success of deep learning has prompted interest in its\napplication to medical diagnosis. Even tough state-of-the-art deep learning\nmodels have achieved human-level accuracy on the classification of different\ntypes of medical data, these models are hardly adopted in clinical workflows,\nmainly due to their lack of interpretability. The black-box-ness of deep\nlearning models has raised the need for devising strategies to explain the\ndecision process of these models, leading to the creation of the topic of\neXplainable Artificial Intelligence (XAI). In this context, we provide a\nthorough survey of XAI applied to medical diagnosis, including visual, textual,\nand example-based explanation methods. Moreover, this work reviews the existing\nmedical imaging datasets and the existing metrics for evaluating the quality of\nthe explanations . Complementary to most existing surveys, we include a\nperformance comparison among a set of report generation-based methods. Finally,\nthe major challenges in applying XAI to medical imaging are also discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Patricio_C/0/1/0/all/0/1\">Cristiano Patr&#xed;cio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Neves_J/0/1/0/all/0/1\">Jo&#xe3;o C. Neves</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teixeira_L/0/1/0/all/0/1\">Lu&#xed;s F. Teixeira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Invariant Masked Autoencoders for Self-supervised Learning from Multi-domains. (arXiv:2205.04771v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04771","description":"<p>Generalizing learned representations across significantly different visual\ndomains is a fundamental yet crucial ability of the human visual system. While\nrecent self-supervised learning methods have achieved good performances with\nevaluation set on the same domain as the training set, they will have an\nundesirable performance decrease when tested on a different domain. Therefore,\nthe self-supervised learning from multiple domains task is proposed to learn\ndomain-invariant features that are not only suitable for evaluation on the same\ndomain as the training set but also can be generalized to unseen domains. In\nthis paper, we propose a Domain-invariant Masked AutoEncoder (DiMAE) for\nself-supervised learning from multi-domains, which designs a new pretext task,\n\\emph{i.e.,} the cross-domain reconstruction task, to learn domain-invariant\nfeatures. The core idea is to augment the input image with style noise from\ndifferent domains and then reconstruct the image from the embedding of the\naugmented image, regularizing the encoder to learn domain-invariant features.\nTo accomplish the idea, DiMAE contains two critical designs, 1)\ncontent-preserved style mix, which adds style information from other domains to\ninput while persevering the content in a parameter-free manner, and 2) multiple\ndomain-specific decoders, which recovers the corresponding domain style of\ninput to the encoded domain-invariant features for reconstruction. Experiments\non PACS and DomainNet illustrate that DiMAE achieves considerable gains\ncompared with recent state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haiyang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meilin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shixiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Isometric Shape Matching via Functional Maps on Landmark-Adapted Bases. (arXiv:2205.04800v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04800","description":"<p>We propose a principled approach for non-isometric landmark-preserving\nnon-rigid shape matching. Our method is based on the functional maps framework,\nbut rather than promoting isometries we focus instead on near-conformal maps\nthat preserve landmarks exactly. We achieve this, first, by introducing a novel\nlandmark-adapted basis using an intrinsic Dirichlet-Steklov eigenproblem.\nSecond, we establish the functional decomposition of conformal maps expressed\nin this basis. Finally, we formulate a conformally-invariant energy that\npromotes high-quality landmark-preserving maps, and show how it can be solved\nvia a variant of the recently proposed ZoomOut method that we extend to our\nsetting. Our method is descriptor-free, efficient and robust to significant\nmesh variability. We evaluate our approach on a range of benchmark datasets and\ndemonstrate state-of-the-art performance on non-isometric benchmarks and near\nstate-of-the-art performance on isometric ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Panine_M/0/1/0/all/0/1\">Mikhail Panine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirgo_M/0/1/0/all/0/1\">Maxime Kirgo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1\">Maks Ovsjanikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Impact of Partial Occlusion on Pedestrian Detectability. (arXiv:2205.04812v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04812","description":"<p>Robust detection of vulnerable road users is a safety critical requirement\nfor the deployment of autonomous vehicles in heterogeneous traffic. One of the\nmost complex outstanding challenges is that of partial occlusion where a target\nobject is only partially available to the sensor due to obstruction by another\nforeground object. A number of leading pedestrian detection benchmarks provide\nannotation for partial occlusion, however each benchmark varies greatly in\ntheir definition of the occurrence and severity of occlusion. Recent research\ndemonstrates that a high degree of subjectivity is used to classify occlusion\nlevel in these cases and occlusion is typically categorized into 2 to 3 broad\ncategories such as partially and heavily occluded. This can lead to inaccurate\nor inconsistent reporting of pedestrian detection model performance depending\non which benchmark is used. This research introduces a novel, objective\nbenchmark for partially occluded pedestrian detection to facilitate the\nobjective characterization of pedestrian detection models. Characterization is\ncarried out on seven popular pedestrian detection models for a range of\nocclusion levels from 0-99%. Results demonstrate that pedestrian detection\nperformance degrades, and the number of false negative detections increase as\npedestrian occlusion level increases. Of the seven popular pedestrian detection\nroutines characterized, CenterNet has the greatest overall performance,\nfollowed by SSDlite. RetinaNet has the lowest overall detection performance\nacross the range of occlusion levels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gilroy_S/0/1/0/all/0/1\">Shane Gilroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullins_D/0/1/0/all/0/1\">Darragh Mullins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_E/0/1/0/all/0/1\">Edward Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parsi_A/0/1/0/all/0/1\">Ashkan Parsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavin_M/0/1/0/all/0/1\">Martin Glavin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised regression learning using domain knowledge: Applications to improving self-supervised denoising in imaging. (arXiv:2205.04821v1 [eess.IV])","link":"http://arxiv.org/abs/2205.04821","description":"<p>Regression that predicts continuous quantity is a central part of\napplications using computational imaging and computer vision technologies. Yet,\nstudying and understanding self-supervised learning for regression tasks -\nexcept for a particular regression task, image denoising - have lagged behind.\nThis paper proposes a general self-supervised regression learning (SSRL)\nframework that enables learning regression neural networks with only input data\n(but without ground-truth target data), by using a designable pseudo-predictor\nthat encapsulates domain knowledge of a specific application. The paper\nunderlines the importance of using domain knowledge by showing that under\ndifferent settings, the better pseudo-predictor can lead properties of SSRL\ncloser to those of ordinary supervised learning. Numerical experiments for\nlow-dose computational tomography denoising and camera image denoising\ndemonstrate that proposed SSRL significantly improves the denoising quality\nover several existing self-supervised denoising methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chun_I/0/1/0/all/0/1\">Il Yong Chun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_D/0/1/0/all/0/1\">Dongwon Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_X/0/1/0/all/0/1\">Xuehang Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chun_S/0/1/0/all/0/1\">Se Young Chun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Long_Y/0/1/0/all/0/1\">Yong Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Detection in Indian Food Platters using Transfer Learning with YOLOv4. (arXiv:2205.04841v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04841","description":"<p>Object detection is a well-known problem in computer vision. Despite this,\nits usage and pervasiveness in the traditional Indian food dishes has been\nlimited. Particularly, recognizing Indian food dishes present in a single photo\nis challenging due to three reasons: 1. Lack of annotated Indian food datasets\n2. Non-distinct boundaries between the dishes 3. High intra-class variation. We\nsolve these issues by providing a comprehensively labelled Indian food dataset-\nIndianFood10, which contains 10 food classes that appear frequently in a staple\nIndian meal and using transfer learning with YOLOv4 object detector model. Our\nmodel is able to achieve an overall mAP score of 91.8% and f1-score of 0.90 for\nour 10 class dataset. We also provide an extension of our 10 class dataset-\nIndianFood20, which contains 10 more traditional Indian food classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_D/0/1/0/all/0/1\">Deepanshu Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_P/0/1/0/all/0/1\">Purva Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toshniwal_G/0/1/0/all/0/1\">Gauri Toshniwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_M/0/1/0/all/0/1\">Mansi Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_V/0/1/0/all/0/1\">Vishesh Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhiman_S/0/1/0/all/0/1\">Shivangi Dhiman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_L/0/1/0/all/0/1\">Lavanya Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagler_G/0/1/0/all/0/1\">Ganesh Bagler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Streamline Plausibility Through Randomized Iterative Spherical-Deconvolution Informed Tractogram Filtering. (arXiv:2205.04843v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04843","description":"<p>Tractography has become an indispensable part of brain connectivity studies.\nHowever, it is currently facing problems with reliability. In particular, a\nsubstantial amount of nerve fiber reconstructions (streamlines) in tractograms\nproduced by state-of-the-art tractography methods are anatomically implausible.\nTo address this problem, tractogram filtering methods have been developed to\nremove faulty connections in a postprocessing step. This study takes a closer\nlook at one such method, \\textit{Spherical-deconvolution Informed Filtering of\nTractograms} (SIFT), which uses a global optimization approach to improve the\nagreement between the remaining streamlines after filtering and the underlying\ndiffusion magnetic resonance imaging data. SIFT is not suitable to judge the\nplausibility of individual streamlines since its results depend on the size and\ncomposition of the surrounding tractogram. To tackle this problem, we propose\napplying SIFT to randomly selected tractogram subsets in order to retrieve\nmultiple assessments for each streamline. This approach makes it possible to\nidentify streamlines with very consistent filtering results, which were used as\npseudo ground truths for training classifiers. The trained classifier is able\nto distinguish the obtained groups of plausible and implausible streamlines\nwith accuracy above 80%. The software code used in the paper and pretrained\nweights of the classifier are distributed freely via the Github repository\nhttps://github.com/djoerch/randomised_filtering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hain_A/0/1/0/all/0/1\">Antonia Hain</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Jorgens_D/0/1/0/all/0/1\">Daniel J&#xf6;rgens</a> (2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_R/0/1/0/all/0/1\">Rodrigo Moreno</a> (3) ((1) Saarland University, Faculty of Mathematics and Computer Science, Saarbr&#xfc;cken, Germany, (2) Division of Brain, Imaging, and Behaviour, Krembil Research Institute, Toronto Western Hospital, University Health Network, Toronto, Canada, (3) KTH Royal Institute of Technology, Department of Biomedical Engineering and Health Systems, Stockholm, Sweden)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MNet: Rethinking 2D/3D Networks for Anisotropic Medical Image Segmentation. (arXiv:2205.04846v1 [eess.IV])","link":"http://arxiv.org/abs/2205.04846","description":"<p>The nature of thick-slice scanning causes severe inter-slice discontinuities\nof 3D medical images, and the vanilla 2D/3D convolutional neural networks\n(CNNs) fail to represent sparse inter-slice information and dense intra-slice\ninformation in a balanced way, leading to severe underfitting to inter-slice\nfeatures (for vanilla 2D CNNs) and overfitting to noise from long-range slices\n(for vanilla 3D CNNs). In this work, a novel mesh network (MNet) is proposed to\nbalance the spatial representation inter axes via learning. 1) Our MNet\nlatently fuses plenty of representation processes by embedding\nmulti-dimensional convolutions deeply into basic modules, making the selections\nof representation processes flexible, thus balancing representation for sparse\ninter-slice information and dense intra-slice information adaptively. 2) Our\nMNet latently fuses multi-dimensional features inside each basic module,\nsimultaneously taking the advantages of 2D (high segmentation accuracy of the\neasily recognized regions in 2D view) and 3D (high smoothness of 3D organ\ncontour) representations, thus obtaining more accurate modeling for target\nregions. Comprehensive experiments are performed on four public datasets\n(CT\\&amp;MR), the results consistently demonstrate the proposed MNet outperforms\nthe other methods. The code and datasets are available at:\nhttps://github.com/zfdong-code/MNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dong_Z/0/1/0/all/0/1\">Zhangfu Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yuting He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_X/0/1/0/all/0/1\">Xiaoming Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shu_H/0/1/0/all/0/1\">Huazhong Shu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coatrieux_J/0/1/0/all/0/1\">Jean-Louis Coatrieux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guanyu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shuo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperparameter optimization of hybrid quantum neural networks for car classification. (arXiv:2205.04878v1 [quant-ph])","link":"http://arxiv.org/abs/2205.04878","description":"<p>Image recognition is one of the primary applications of machine learning\nalgorithms. Nevertheless, machine learning models used in modern image\nrecognition systems consist of millions of parameters that usually require\nsignificant computational time to be adjusted. Moreover, adjustment of model\nhyperparameters leads to additional overhead. Because of this, new developments\nin machine learning models and hyperparameter optimization techniques are\nrequired. This paper presents a quantum-inspired hyperparameter optimization\ntechnique and a hybrid quantum-classical machine learning model for supervised\nlearning. We benchmark our hyperparameter optimization method over standard\nblack-box objective functions and observe performance improvements in the form\nof reduced expected run times and fitness in response to the growth in the size\nof the search space. We test our approaches in a car image classification task,\nand demonstrate a full-scale implementation of the hybrid quantum neural\nnetwork model with the tensor train hyperparameter optimization. Our tests show\na qualitative and quantitative advantage over the corresponding standard\nclassical tabular grid search approach used with a deep neural network\nResNet34. A classification accuracy of 0.97 was obtained by the hybrid model\nafter 18 iterations, whereas the classical model achieved an accuracy of 0.92\nafter 75 iterations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Sagingalieva_A/0/1/0/all/0/1\">Asel Sagingalieva</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kurkin_A/0/1/0/all/0/1\">Andrii Kurkin</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Melnikov_A/0/1/0/all/0/1\">Artem Melnikov</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kuhmistrov_D/0/1/0/all/0/1\">Daniil Kuhmistrov</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Perelshtein_M/0/1/0/all/0/1\">Michael Perelshtein</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Melnikov_A/0/1/0/all/0/1\">Alexey Melnikov</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Skolik_A/0/1/0/all/0/1\">Andrea Skolik</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Dollen_D/0/1/0/all/0/1\">David Von Dollen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identical Image Retrieval using Deep Learning. (arXiv:2205.04883v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04883","description":"<p>In recent years, we know that the interaction with images has increased.\nImage similarity involves fetching similar-looking images abiding by a given\nreference image. The target is to find out whether the image searched as a\nquery can result in similar pictures. We are using the BigTransfer Model, which\nis a state-of-art model itself. BigTransfer(BiT) is essentially a ResNet but\npre-trained on a larger dataset like ImageNet and ImageNet-21k with additional\nmodifications. Using the fine-tuned pre-trained Convolution Neural Network\nModel, we extract the key features and train on the K-Nearest Neighbor model to\nobtain the nearest neighbor. The application of our model is to find similar\nimages, which are hard to achieve through text queries within a low inference\ntime. We analyse the benchmark of our model based on this application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nath_S/0/1/0/all/0/1\">Sayan Nath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_N/0/1/0/all/0/1\">Nikhil Nayak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Non-target Knowledge for Few-shot Semantic Segmentation. (arXiv:2205.04903v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04903","description":"<p>Existing studies in few-shot semantic segmentation only focus on mining the\ntarget object information, however, often are hard to tell ambiguous regions,\nespecially in non-target regions, which include background (BG) and Distracting\nObjects (DOs). To alleviate this problem, we propose a novel framework, namely\nNon-Target Region Eliminating (NTRE) network, to explicitly mine and eliminate\nBG and DO regions in the query. First, a BG Mining Module (BGMM) is proposed to\nextract the BG region via learning a general BG prototype. To this end, we\ndesign a BG loss to supervise the learning of BGMM only using the known target\nobject segmentation ground truth. Then, a BG Eliminating Module and a DO\nEliminating Module are proposed to successively filter out the BG and DO\ninformation from the query feature, based on which we can obtain a BG and\nDO-free target object segmentation result. Furthermore, we propose a\nprototypical contrastive learning algorithm to improve the model ability of\ndistinguishing the target object from DOs. Extensive experiments on both\nPASCAL-5i and COCO-20i datasets show that our approach is effective despite its\nsimplicity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qinglong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xiwen Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shadow-Aware Dynamic Convolution for Shadow Removal. (arXiv:2205.04908v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04908","description":"<p>With a wide range of shadows in many collected images, shadow removal has\naroused increasing attention since uncontaminated images are of vital\nimportance for many downstream multimedia tasks. Current methods consider the\nsame convolution operations for both shadow and non-shadow regions while\nignoring the large gap between the color mappings for the shadow region and the\nnon-shadow region, leading to poor quality of reconstructed images and a heavy\ncomputation burden. To solve this problem, this paper introduces a novel\nplug-and-play Shadow-Aware Dynamic Convolution (SADC) module to decouple the\ninterdependence between the shadow region and the non-shadow region. Inspired\nby the fact that the color mapping of the non-shadow region is easier to learn,\nour SADC processes the non-shadow region with a lightweight convolution module\nin a computationally cheap manner and recovers the shadow region with a more\ncomplicated convolution module to ensure the quality of image reconstruction.\nGiven that the non-shadow region often contains more background color\ninformation, we further develop a novel intra-convolution distillation loss to\nstrengthen the information flow from the non-shadow region to the shadow\nregion. Extensive experiments on the ISTD and SRD datasets show our method\nachieves better performance in shadow removal over many state-of-the-arts. Our\ncode is available at https://github.com/xuyimin0926/SADC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yimin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Closer Look at Blind Super-Resolution: Degradation Models, Baselines, and Performance Upper Bounds. (arXiv:2205.04910v1 [eess.IV])","link":"http://arxiv.org/abs/2205.04910","description":"<p>Degradation models play an important role in Blind super-resolution (SR). The\nclassical degradation model, which mainly involves blur degradation, is too\nsimple to simulate real-world scenarios. The recently proposed practical\ndegradation model includes a full spectrum of degradation types, but only\nconsiders complex cases that use all degradation types in the degradation\nprocess, while ignoring many important corner cases that are common in the real\nworld. To address this problem, we propose a unified gated degradation model to\ngenerate a broad set of degradation cases using a random gate controller. Based\non the gated degradation model, we propose simple baseline networks that can\neffectively handle non-blind, classical, practical degradation cases as well as\nmany other corner cases. To fairly evaluate the performance of our baseline\nnetworks against state-of-the-art methods and understand their limits, we\nintroduce the performance upper bound of an SR network for every degradation\ntype. Our empirical analysis shows that with the unified gated degradation\nmodel, the proposed baselines can achieve much better performance than existing\nmethods in quantitative and qualitative results, which are close to the\nperformance upper bounds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wenlong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_G/0/1/0/all/0/1\">Guangyuan Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yihao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Ming Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based Cross-Modal Recipe Embeddings with Large Batch Training. (arXiv:2205.04948v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04948","description":"<p>In this paper, we present a cross-modal recipe retrieval framework,\nTransformer-based Network for Large Batch Training (TNLBT), which is inspired\nby ACME~(Adversarial Cross-Modal Embedding) and H-T~(Hierarchical Transformer).\nTNLBT aims to accomplish retrieval tasks while generating images from recipe\nembeddings. We apply the Hierarchical Transformer-based recipe text encoder,\nthe Vision Transformer~(ViT)-based recipe image encoder, and an adversarial\nnetwork architecture to enable better cross-modal embedding learning for recipe\ntexts and images. In addition, we use self-supervised learning to exploit the\nrich information in the recipe texts having no corresponding images. Since\ncontrastive learning could benefit from a larger batch size according to the\nrecent literature on self-supervised learning, we adopt a large batch size\nduring training and have validated its effectiveness. In the experiments, the\nproposed framework significantly outperformed the current state-of-the-art\nframeworks in both cross-modal recipe retrieval and image generation tasks on\nthe benchmark Recipe1M. This is the first work which confirmed the\neffectiveness of large batch training on cross-modal recipe embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanai_K/0/1/0/all/0/1\">Keiji Yanai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeRF-Editing: Geometry Editing of Neural Radiance Fields. (arXiv:2205.04978v1 [cs.GR])","link":"http://arxiv.org/abs/2205.04978","description":"<p>Implicit neural rendering, especially Neural Radiance Field (NeRF), has shown\ngreat potential in novel view synthesis of a scene. However, current NeRF-based\nmethods cannot enable users to perform user-controlled shape deformation in the\nscene. While existing works have proposed some approaches to modify the\nradiance field according to the user's constraints, the modification is limited\nto color editing or object translation and rotation. In this paper, we propose\na method that allows users to perform controllable shape deformation on the\nimplicit representation of the scene, and synthesizes the novel view images of\nthe edited scene without re-training the network. Specifically, we establish a\ncorrespondence between the extracted explicit mesh representation and the\nimplicit neural representation of the target scene. Users can first utilize\nwell-developed mesh-based deformation methods to deform the mesh representation\nof the scene. Our method then utilizes user edits from the mesh representation\nto bend the camera rays by introducing a tetrahedra mesh as a proxy, obtaining\nthe rendering results of the edited scene. Extensive experiments demonstrate\nthat our framework can achieve ideal editing results not only on synthetic\ndata, but also on real scenes captured by users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yu-Jie Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yang-Tian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yu-Kun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuewen Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Rongfei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lin Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling A Single MR Modality. (arXiv:2205.04982v1 [eess.IV])","link":"http://arxiv.org/abs/2205.04982","description":"<p>Disentangling anatomical and contrast information from medical images has\ngained attention recently, demonstrating benefits for various image analysis\ntasks. Current methods learn disentangled representations using either paired\nmulti-modal images with the same underlying anatomy or auxiliary labels (e.g.,\nmanual delineations) to provide inductive bias for disentanglement. However,\nthese requirements could significantly increase the time and cost in data\ncollection and limit the applicability of these methods when such data are not\navailable. Moreover, these methods generally do not guarantee disentanglement.\nIn this paper, we present a novel framework that learns theoretically and\npractically superior disentanglement from single modality magnetic resonance\nimages. Moreover, we propose a new information-based metric to quantitatively\nevaluate disentanglement. Comparisons over existing disentangling methods\ndemonstrate that the proposed method achieves superior performance in both\ndisentanglement and cross-domain image-to-image translation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zuo_L/0/1/0/all/0/1\">Lianrui Zuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yihao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_Y/0/1/0/all/0/1\">Yuan Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_S/0/1/0/all/0/1\">Shuo Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bilgel_M/0/1/0/all/0/1\">Murat Bilgel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Resnick_S/0/1/0/all/0/1\">Susan M. Resnick</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prince_J/0/1/0/all/0/1\">Jerry L. Prince</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Carass_A/0/1/0/all/0/1\">Aaron Carass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints. (arXiv:2205.04992v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04992","description":"<p>Image-based volumetric avatars using pixel-aligned features promise\ngeneralization to unseen poses and identities. Prior work leverages global\nspatial encodings and multi-view geometric consistency to reduce spatial\nambiguity. However, global encodings often suffer from overfitting to the\ndistribution of the training data, and it is difficult to learn multi-view\nconsistent reconstruction from sparse views. In this work, we investigate\ncommon issues with existing spatial encodings and propose a simple yet highly\neffective approach to modeling high-fidelity volumetric avatars from sparse\nviews. One of the key ideas is to encode relative spatial 3D information via\nsparse 3D keypoints. This approach is robust to the sparsity of viewpoints and\ncross-dataset domain gap. Our approach outperforms state-of-the-art methods for\nhead reconstruction. On human body reconstruction for unseen subjects, we also\nachieve performance comparable to prior work that uses a parametric human body\nmodel and temporal feature aggregation. Our experiments show that a majority of\nerrors in prior work stem from an inappropriate choice of spatial encoding and\nthus we suggest a new direction for high-fidelity image-based avatar modeling.\nhttps://markomih.github.io/KeypointNeRF\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mihajlovic_M/0/1/0/all/0/1\">Marko Mihajlovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1\">Aayush Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1\">Michael Zollhoefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1\">Shunsuke Saito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Deep Learning-based Features Extracted from CT scans to Predict Outcomes in COVID-19 Patients. (arXiv:2205.05009v1 [eess.IV])","link":"http://arxiv.org/abs/2205.05009","description":"<p>The COVID-19 pandemic has had a considerable impact on day-to-day life.\nTackling the disease by providing the necessary resources to the affected is of\nparamount importance. However, estimation of the required resources is not a\ntrivial task given the number of factors which determine the requirement. This\nissue can be addressed by predicting the probability that an infected patient\nrequires Intensive Care Unit (ICU) support and the importance of each of the\nfactors that influence it. Moreover, to assist the doctors in determining the\npatients at high risk of fatality, the probability of death is also calculated.\nFor determining both the patient outcomes (ICU admission and death), a novel\nmethodology is proposed by combining multi-modal features, extracted from\nComputed Tomography (CT) scans and Electronic Health Record (EHR) data. Deep\nlearning models are leveraged to extract quantitative features from CT scans.\nThese features combined with those directly read from the EHR database are fed\ninto machine learning models to eventually output the probabilities of patient\noutcomes. This work demonstrates both the ability to apply a broad set of deep\nlearning methods for general quantification of Chest CT scans and the ability\nto link these quantitative metrics to patient outcomes. The effectiveness of\nthe proposed method is shown by testing it on an internally curated dataset,\nachieving a mean area under Receiver operating characteristic curve (AUC) of\n0.77 on ICU admission prediction and a mean AUC of 0.73 on death prediction\nusing the best performing classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nuthalapati_S/0/1/0/all/0/1\">Sai Vidyaranya Nuthalapati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vizcaychipi_M/0/1/0/all/0/1\">Marcela Vizcaychipi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shah_P/0/1/0/all/0/1\">Pallav Shah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chudzik_P/0/1/0/all/0/1\">Piotr Chudzik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leow_C/0/1/0/all/0/1\">Chee Hau Leow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yousefi_P/0/1/0/all/0/1\">Paria Yousefi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Selim_A/0/1/0/all/0/1\">Ahmed Selim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tait_K/0/1/0/all/0/1\">Keiran Tait</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Irving_B/0/1/0/all/0/1\">Ben Irving</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Answer Visual Questions from Web Videos. (arXiv:2205.05019v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05019","description":"<p>Recent methods for visual question answering rely on large-scale annotated\ndatasets. Manual annotation of questions and answers for videos, however, is\ntedious, expensive and prevents scalability. In this work, we propose to avoid\nmanual annotation and generate a large-scale training dataset for video\nquestion answering making use of automatic cross-modal supervision. We leverage\na question generation transformer trained on text data and use it to generate\nquestion-answer pairs from transcribed video narrations. Given narrated videos,\nwe then automatically generate the HowToVQA69M dataset with 69M\nvideo-question-answer triplets. To handle the open vocabulary of diverse\nanswers in this dataset, we propose a training procedure based on a contrastive\nloss between a video-question multi-modal transformer and an answer\ntransformer. We introduce the zero-shot VideoQA task and the VideoQA feature\nprobe evaluation setting and show excellent results, in particular for rare\nanswers. Furthermore, our method achieves competitive results on MSRVTT-QA,\nActivityNet-QA, MSVD-QA and How2QA datasets. We also show that our VideoQA\ndataset generation approach generalizes to another source of web video and text\ndata. We use our method to generate the \\webdataname{} dataset from the WebVid\ndataset, i.e., videos with alt-text annotations, and show its benefits for\ntraining VideoQA models. Finally, for a detailed evaluation we introduce\n\\smalldatasetname{}, a new VideoQA dataset with reduced language bias and\nhigh-quality manual annotations. Code, datasets and trained models are\navailable at https://antoyang.github.io/just-ask.html\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Antoine Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1\">Antoine Miech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification and mapping of low-statured 'shrubland' cover types in post-agricultural landscapes of the US Northeast. (arXiv:2205.05047v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05047","description":"<p>Context: Novel plant communities reshape landscapes and pose challenges for\nland cover classification and mapping that can constrain research and\nstewardship efforts. In the US Northeast, emergence of low-statured woody\nvegetation, or 'shrublands', instead of secondary forests in post-agricultural\nlandscapes is well-documented by field studies, but poorly understood from a\nlandscape perspective, which limits the ability to systematically study and\nmanage these lands. Objectives: To address gaps in classification/mapping of\nlow-statured cover types where they have been historically rare, we developed\nmodels to predict 'shrubland' distributions at 30m resolution across New York\nState (NYS), using machine learning and model ensembling techniques to\nintegrate remote sensing of structural (airborne LIDAR) and optical (satellite\nimagery) properties of vegetation cover. We first classified a 1m canopy height\nmodel (CHM), derived from a \"patchwork\" of available LIDAR coverages, to define\nshrubland presence/absence. Next, these non-contiguous maps were used to train\na model ensemble based on temporally-segmented imagery to predict 'shrubland'\nprobability for the entire study landscape (NYS). Results: Approximately 2.5%\nof the CHM coverage area was classified as shrubland. Models using Landsat\npredictors trained on the classified CHM were effective at identifying\nshrubland (test set AUC=0.893, real-world AUC=0.904), in discriminating between\nshrub/young forest and other cover classes, and produced qualitatively sensible\nmaps, even when extending beyond the original training data. Conclusions: After\nground-truthing, we expect these shrubland maps and models will have many\nresearch and stewardship applications including wildlife conservation, invasive\nspecies mitigation and natural climate solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael J Mahoney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_L/0/1/0/all/0/1\">Lucas K Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beier_C/0/1/0/all/0/1\">Colin M Beier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metric Learning based Interactive Modulation for Real-World Super-Resolution. (arXiv:2205.05065v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05065","description":"<p>Interactive image restoration aims to restore images by adjusting several\ncontrolling coefficients, which determine the restoration strength. Existing\nmethods are restricted in learning the controllable functions under the\nsupervision of known degradation types and levels. They usually suffer from a\nsevere performance drop when the real degradation is different from their\nassumptions. Such a limitation is due to the complexity of real-world\ndegradations, which can not provide explicit supervision to the interactive\nmodulation during training. However, how to realize the interactive modulation\nin real-world super-resolution has not yet been studied. In this work, we\npresent a Metric Learning based Interactive Modulation for Real-World\nSuper-Resolution (MM-RealSR). Specifically, we propose an unsupervised\ndegradation estimation strategy to estimate the degradation level in real-world\nscenarios. Instead of using known degradation levels as explicit supervision to\nthe interactive mechanism, we propose a metric learning strategy to map the\nunquantifiable degradation levels in real-world scenarios to a metric space,\nwhich is trained in an unsupervised manner. Moreover, we introduce an anchor\npoint strategy in the metric learning process to normalize the distribution of\nmetric space. Extensive experiments demonstrate that the proposed MM-RealSR\nachieves excellent modulation and restoration performance in real-world\nsuper-resolution. Codes are available at\nhttps://github.com/TencentARC/MM-RealSR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mou_C/0/1/0/all/0/1\">Chong Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanze Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating the Training of Video Super-Resolution. (arXiv:2205.05069v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05069","description":"<p>Despite that convolution neural networks (CNN) have recently demonstrated\nhigh-quality reconstruction for video super-resolution (VSR), efficiently\ntraining competitive VSR models remains a challenging problem. It usually takes\nan order of magnitude more time than training their counterpart image models,\nleading to long research cycles. Existing VSR methods typically train models\nwith fixed spatial and temporal sizes from beginning to end. The fixed sizes\nare usually set to large values for good performance, resulting to slow\ntraining. However, is such a rigid training strategy necessary for VSR? In this\nwork, we show that it is possible to gradually train video models from small to\nlarge spatial/temporal sizes, i.e., in an easy-to-hard manner. In particular,\nthe whole training is divided into several stages and the earlier stage has\nsmaller training spatial shape. Inside each stage, the temporal size also\nvaries from short to long while the spatial size remains unchanged. Training is\naccelerated by such a multigrid training strategy, as most of computation is\nperformed on smaller spatial and shorter temporal shapes. For further\nacceleration with GPU parallelization, we also investigate the large minibatch\ntraining without the loss in accuracy. Extensive experiments demonstrate that\nour method is capable of largely speeding up training (up to $6.2\\times$\nspeedup in wall-clock training time) without performance drop for various VSR\nmodels. The code is available at\nhttps://github.com/TencentARC/Efficient-VSR-Training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lijian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhongang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Visual Styles from Audio-Visual Associations. (arXiv:2205.05072v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05072","description":"<p>From the patter of rain to the crunch of snow, the sounds we hear often\nconvey the visual textures that appear within a scene. In this paper, we\npresent a method for learning visual styles from unlabeled audio-visual data.\nOur model learns to manipulate the texture of a scene to match a sound, a\nproblem we term audio-driven image stylization. Given a dataset of paired\naudio-visual data, we learn to modify input images such that, after\nmanipulation, they are more likely to co-occur with a given input sound. In\nquantitative and qualitative evaluations, our sound-based model outperforms\nlabel-based approaches. We also show that audio can be an intuitive\nrepresentation for manipulating images, as adjusting a sound's volume or mixing\ntwo sounds together results in predictable changes to visual style. Project\nwebpage: https://tinglok.netlify.app/files/avstyle\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tingle Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yichen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owens_A/0/1/0/all/0/1\">Andrew Owens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reduce Information Loss in Transformers for Pluralistic Image Inpainting. (arXiv:2205.05076v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05076","description":"<p>Transformers have achieved great success in pluralistic image inpainting\nrecently. However, we find existing transformer based solutions regard each\npixel as a token, thus suffer from information loss issue from two aspects: 1)\nThey downsample the input image into much lower resolutions for efficiency\nconsideration, incurring information loss and extra misalignment for the\nboundaries of masked regions. 2) They quantize $256^3$ RGB pixels to a small\nnumber (such as 512) of quantized pixels. The indices of quantized pixels are\nused as tokens for the inputs and prediction targets of transformer. Although\nan extra CNN network is used to upsample and refine the low-resolution results,\nit is difficult to retrieve the lost information back.To keep input information\nas much as possible, we propose a new transformer based framework \"PUT\".\nSpecifically, to avoid input downsampling while maintaining the computation\nefficiency, we design a patch-based auto-encoder P-VQVAE, where the encoder\nconverts the masked image into non-overlapped patch tokens and the decoder\nrecovers the masked regions from inpainted tokens while keeping the unmasked\nregions unchanged. To eliminate the information loss caused by quantization, an\nUn-Quantized Transformer (UQ-Transformer) is applied, which directly takes the\nfeatures from P-VQVAE encoder as input without quantization and regards the\nquantized tokens only as prediction targets. Extensive experiments show that\nPUT greatly outperforms state-of-the-art methods on image fidelity, especially\nfor large masked regions and complex large-scale datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiankun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhentao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EBBINNOT: A Hardware Efficient Hybrid Event-Frame Tracker for Stationary Dynamic Vision Sensors. (arXiv:2006.00422v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.00422","description":"<p>As an alternative sensing paradigm, dynamic vision sensors (DVS) have been\nrecently explored to tackle scenarios where conventional sensors result in high\ndata rate and processing time. This paper presents a hybrid event-frame\napproach for detecting and tracking objects recorded by a stationary\nneuromorphic sensor, thereby exploiting the sparse DVS output in a low-power\nsetting for traffic monitoring. Specifically, we propose a hardware efficient\nprocessing pipeline that optimizes memory and computational needs that enable\nlong-term battery powered usage for IoT applications. To exploit the background\nremoval property of a static DVS, we propose an event-based binary image\ncreation that signals presence or absence of events in a frame duration. This\nreduces memory requirement and enables usage of simple algorithms like median\nfiltering and connected component labeling for denoise and region proposal\nrespectively. To overcome the fragmentation issue, a YOLO inspired neural\nnetwork based detector and classifier to merge fragmented region proposals has\nbeen proposed. Finally, a new overlap based tracker was implemented, exploiting\noverlap between detections and tracks is proposed with heuristics to overcome\nocclusion. The proposed pipeline is evaluated with more than 5 hours of traffic\nrecording spanning three different locations on two different neuromorphic\nsensors (DVS and CeleX) and demonstrate similar performance. Compared to\nexisting event-based feature trackers, our method provides similar accuracy\nwhile needing approx 6 times less computes. To the best of our knowledge, this\nis the first time a stationary DVS based traffic monitoring solution is\nextensively compared to simultaneously recorded RGB frame-based methods while\nshowing tremendous promise by outperforming state-of-the-art deep learning\nsolutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohan_V/0/1/0/all/0/1\">Vivek Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_D/0/1/0/all/0/1\">Deepak Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulluri_T/0/1/0/all/0/1\">Tarun Pulluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ussa_A/0/1/0/all/0/1\">Andres Ussa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_P/0/1/0/all/0/1\">Pradeep Kumar Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Pao-Sheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_B/0/1/0/all/0/1\">Bharath Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1\">Arindam Basu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting. (arXiv:2010.04456v6 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2010.04456","description":"<p>Forecasting complex dynamical phenomena in settings where only partial\nknowledge of their dynamics is available is a prevalent problem across various\nscientific fields. While purely data-driven approaches are arguably\ninsufficient in this context, standard physical modeling based approaches tend\nto be over-simplistic, inducing non-negligible errors. In this work, we\nintroduce the APHYNITY framework, a principled approach for augmenting\nincomplete physical dynamics described by differential equations with deep\ndata-driven models. It consists in decomposing the dynamics into two\ncomponents: a physical component accounting for the dynamics for which we have\nsome prior knowledge, and a data-driven component accounting for errors of the\nphysical model. The learning problem is carefully formulated such that the\nphysical model explains as much of the data as possible, while the data-driven\ncomponent only describes information that cannot be captured by the physical\nmodel, no more, no less. This not only provides the existence and uniqueness\nfor this decomposition, but also ensures interpretability and benefits\ngeneralization. Experiments made on three important use cases, each\nrepresentative of a different family of phenomena, i.e. reaction-diffusion\nequations, wave equations and the non-linear damped pendulum, show that\nAPHYNITY can efficiently leverage approximate physical models to accurately\nforecast the evolution of the system and correctly identify relevant physical\nparameters. Code is available at https://github.com/yuan-yin/APHYNITY .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Yin_Y/0/1/0/all/0/1\">Yuan Yin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Guen_V/0/1/0/all/0/1\">Vincent Le Guen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dona_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;mie Dona</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bezenac_E/0/1/0/all/0/1\">Emmanuel de B&#xe9;zenac</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ayed_I/0/1/0/all/0/1\">Ibrahim Ayed</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thome_N/0/1/0/all/0/1\">Nicolas Thome</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ResNet-LDDMM: Advancing the LDDMM Framework using Deep Residual Networks. (arXiv:2102.07951v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2102.07951","description":"<p>In deformable registration, the geometric framework - large deformation\ndiffeomorphic metric mapping or LDDMM, in short - has inspired numerous\ntechniques for comparing, deforming, averaging and analyzing shapes or images.\nGrounded in flows, which are akin to the equations of motion used in fluid\ndynamics, LDDMM algorithms solve the flow equation in the space of plausible\ndeformations, i.e. diffeomorphisms. In this work, we make use of deep residual\nneural networks to solve the non-stationary ODE (flow equation) based on a\nEuler's discretization scheme. The central idea is to represent time-dependent\nvelocity fields as fully connected ReLU neural networks (building blocks) and\nderive optimal weights by minimizing a regularized loss function. Computing\nminimizing paths between deformations, thus between shapes, turns to find\noptimal network parameters by back-propagating over the intermediate building\nblocks. Geometrically, at each time step, ResNet-LDDMM searches for an optimal\npartition of the space into multiple polytopes, and then computes optimal\nvelocity vectors as affine transformations on each of these polytopes. As a\nresult, different parts of the shape, even if they are close (such as two\nfingers of a hand), can be made to belong to different polytopes, and therefore\nbe moved in different directions without costing too much energy. Importantly,\nwe show how diffeomorphic transformations, or more precisely bilipshitz\ntransformations, are predicted by our algorithm. We illustrate these ideas on\ndiverse registration problems of 3D shapes under complex topology-preserving\ntransformations. We thus provide essential foundations for more advanced shape\nvariability analysis under a novel joint geometric-neural networks\nRiemannian-like framework, i.e. ResNet-LDDMM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amor_B/0/1/0/all/0/1\">Boulbaba Ben Amor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arguillere_S/0/1/0/all/0/1\">Sylvain Arguill&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Generalization: A Survey. (arXiv:2103.02503v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.02503","description":"<p>Generalization to out-of-distribution (OOD) data is a capability natural to\nhumans yet challenging for machines to reproduce. This is because most learning\nalgorithms strongly rely on the i.i.d.~assumption on source/target data, which\nis often violated in practice due to domain shift. Domain generalization (DG)\naims to achieve OOD generalization by using only source data for model\nlearning. Over the last ten years, research in DG has made great progress,\nleading to a broad spectrum of methodologies, e.g., those based on domain\nalignment, meta-learning, data augmentation, or ensemble learning, to name a\nfew; DG has also been studied in various application areas including computer\nvision, speech recognition, natural language processing, medical imaging, and\nreinforcement learning. In this paper, for the first time a comprehensive\nliterature review in DG is provided to summarize the developments over the past\ndecade. Specifically, we first cover the background by formally defining DG and\nrelating it to other relevant fields like domain adaptation and transfer\nlearning. Then, we conduct a thorough review into existing methods and\ntheories. Finally, we conclude this survey with insights and discussions on\nfuture research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TubeR: Tubelet Transformer for Video Action Detection. (arXiv:2104.00969v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.00969","description":"<p>We propose TubeR: a simple solution for spatio-temporal video action\ndetection. Different from existing methods that depend on either an off-line\nactor detector or hand-designed actor-positional hypotheses like proposals or\nanchors, we propose to directly detect an action tubelet in a video by\nsimultaneously performing action localization and recognition from a single\nrepresentation. TubeR learns a set of tubelet-queries and utilizes a\ntubelet-attention module to model the dynamic spatio-temporal nature of a video\nclip, which effectively reinforces the model capacity compared to using\nactor-positional hypotheses in the spatio-temporal space. For videos containing\ntransitional states or scene changes, we propose a context aware classification\nhead to utilize short-term and long-term context to strengthen action\nclassification, and an action switch regression head for detecting the precise\ntemporal action extent. TubeR directly produces action tubelets with variable\nlengths and even maintains good results for long video clips. TubeR outperforms\nthe previous state-of-the-art on commonly used action detection datasets AVA,\nUCF101-24 and JHMDB51-21.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiaojiao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_S/0/1/0/all/0/1\">Shuai Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingze Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunhui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_K/0/1/0/all/0/1\">Kaustav Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1\">Davide Modolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marsic_I/0/1/0/all/0/1\">Ivan Marsic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G.M. Snoek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepTag: A General Framework for Fiducial Marker Design and Detection. (arXiv:2105.13731v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.13731","description":"<p>A fiducial marker system usually consists of markers, a detection algorithm,\nand a coding system. The appearance of markers and the detection robustness are\ngenerally limited by the existing detection algorithms, which are hand-crafted\nwith traditional low-level image processing techniques. Furthermore, a\nsophisticatedly designed coding system is required to overcome the shortcomings\nof both markers and detection algorithms. To improve the flexibility and\nrobustness in various applications, we propose a general deep learning based\nframework, DeepTag, for fiducial marker design and detection. DeepTag not only\nsupports detection of a wide variety of existing marker families, but also\nmakes it possible to design new marker families with customized local patterns.\nMoreover, we propose an effective procedure to synthesize training data on the\nfly without manual annotations. Thus, DeepTag can easily adapt to existing and\nnewly-designed marker families. To validate DeepTag and existing methods,\nbeside existing datasets, we further collect a new large and challenging\ndataset where markers are placed in different view distances and angles.\nExperiments show that DeepTag well supports different marker families and\ngreatly outperforms the existing methods in terms of both detection robustness\nand pose accuracy. Both code and dataset are available at\nhttps://herohuyongtao.github.io/research/publications/deep-tag/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yongtao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Guoxing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jingwen Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Streaming Egocentric Action Anticipation. (arXiv:2110.05386v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05386","description":"<p>Egocentric action anticipation is the task of predicting the future actions a\ncamera wearer will likely perform based on past video observations. While in a\nreal-world system it is fundamental to output such predictions before the\naction begins, past works have not generally paid attention to model runtime\nduring evaluation. Indeed, current evaluation schemes assume that predictions\ncan be made offline, and hence that computational resources are not limited. In\ncontrast, in this paper, we propose a \"streaming\" egocentric action\nanticipation evaluation protocol which explicitly considers model runtime for\nperformance assessment, assuming that predictions will be available only after\nthe current video segment is processed, which depends on the processing time of\na method. Following the proposed evaluation scheme, we benchmark different\nstate-of-the-art approaches for egocentric action anticipation on two popular\ndatasets. Our analysis shows that models with a smaller runtime tend to\noutperform heavier models in the considered streaming scenario, thus changing\nthe rankings generally observed in standard offline evaluations. Based on this\nobservation, we propose a lightweight action anticipation model consisting in a\nsimple feed-forward 3D CNN, which we propose to optimize using knowledge\ndistillation techniques and a custom loss. The results show that the proposed\napproach outperforms prior art in the streaming scenario, also in combination\nwith other lightweight models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1\">Antonino Furnari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1\">Giovanni Maria Farinella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skeleton-Based Mutually Assisted Interacted Object Localization and Human Action Recognition. (arXiv:2110.14994v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.14994","description":"<p>Skeleton data carries valuable motion information and is widely explored in\nhuman action recognition. However, not only the motion information but also the\ninteraction with the environment provides discriminative cues to recognize the\naction of persons. In this paper, we propose a joint learning framework for\nmutually assisted \"interacted object localization\" and \"human action\nrecognition\" based on skeleton data. The two tasks are serialized together and\ncollaborate to promote each other, where preliminary action type derived from\nskeleton alone helps improve interacted object localization, which in turn\nprovides valuable cues for the final human action recognition. Besides, we\nexplore the temporal consistency of interacted object as constraint to better\nlocalize the interacted object with the absence of ground-truth labels.\nExtensive experiments on the datasets of SYSU-3D, NTU60 RGB+D,\nNorthwestern-UCLA and UAV-Human show that our method achieves the best or\ncompetitive performance with the state-of-the-art methods for human action\nrecognition. Visualization results show that our method can also provide\nreasonable interacted object localization results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Authentication Attacks on Projection-based Cancelable Biometric Schemes (long version). (arXiv:2110.15163v4 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2110.15163","description":"<p>Cancelable biometric schemes aim at generating secure biometric templates by\ncombining user specific tokens, such as password, stored secret or salt, along\nwith biometric data. This type of transformation is constructed as a\ncomposition of a biometric transformation with a feature extraction algorithm.\nThe security requirements of cancelable biometric schemes concern the\nirreversibility, unlinkability and revocability of templates, without losing in\naccuracy of comparison. While several schemes were recently attacked regarding\nthese requirements, full reversibility of such a composition in order to\nproduce colliding biometric characteristics, and specifically presentation\nattacks, were never demonstrated to the best of our knowledge. In this paper,\nwe formalize these attacks for a traditional cancelable scheme with the help of\ninteger linear programming (ILP) and quadratically constrained quadratic\nprogramming (QCQP). Solving these optimization problems allows an adversary to\nslightly alter its fingerprint image in order to impersonate any individual.\nMoreover, in an even more severe scenario, it is possible to simultaneously\nimpersonate several individuals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Durbet_A/0/1/0/all/0/1\">Axel Durbet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lafourcade_P/0/1/0/all/0/1\">Pascal Lafourcade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Migdal_D/0/1/0/all/0/1\">Denis Migdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiry_Atighehchi_K/0/1/0/all/0/1\">Kevin Thiry-Atighehchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grollemund_P/0/1/0/all/0/1\">Paul-Marie Grollemund</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Category-orthogonal object features guide information processing in recurrent neural networks trained for object categorization. (arXiv:2111.07898v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.07898","description":"<p>Recurrent neural networks (RNNs) have been shown to perform better than\nfeedforward architectures in visual object categorization tasks, especially in\nchallenging conditions such as cluttered images. However, little is known about\nthe exact computational role of recurrent information flow in these conditions.\nHere we test RNNs trained for object categorization on the hypothesis that\nrecurrence iteratively aids object categorization via the communication of\ncategory-orthogonal auxiliary variables (the location, orientation, and scale\nof the object). Using diagnostic linear readouts, we find that: (a) information\nabout auxiliary variables increases across time in all network layers, (b) this\ninformation is indeed present in the recurrent information flow, and (c) its\nmanipulation significantly affects task performance. These observations confirm\nthe hypothesis that category-orthogonal auxiliary variable information is\nconveyed through recurrent connectivity and is used to optimize category\ninference in cluttered environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thorat_S/0/1/0/all/0/1\">Sushrut Thorat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldegheri_G/0/1/0/all/0/1\">Giacomo Aldegheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kietzmann_T/0/1/0/all/0/1\">Tim C. Kietzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Neural Light Fields with Ray-Space Embedding Networks. (arXiv:2112.01523v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01523","description":"<p>Neural radiance fields (NeRFs) produce state-of-the-art view synthesis\nresults. However, they are slow to render, requiring hundreds of network\nevaluations per pixel to approximate a volume rendering integral. Baking NeRFs\ninto explicit data structures enables efficient rendering, but results in a\nlarge increase in memory footprint and, in many cases, a quality reduction. In\nthis paper, we propose a novel neural light field representation that, in\ncontrast, is compact and directly predicts integrated radiance along rays. Our\nmethod supports rendering with a single network evaluation per pixel for small\nbaseline light field datasets and can also be applied to larger baselines with\nonly a few evaluations per pixel. At the core of our approach is a ray-space\nembedding network that maps the 4D ray-space manifold into an intermediate,\ninterpolable latent space. Our method achieves state-of-the-art quality on\ndense forward-facing datasets such as the Stanford Light Field dataset. In\naddition, for forward-facing scenes with sparser inputs we achieve results that\nare competitive with NeRF-based approaches in terms of quality while providing\na better speed/quality/memory trade-off with far fewer network evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Attal_B/0/1/0/all/0/1\">Benjamin Attal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jia-Bin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1\">Michael Zollhoefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopf_J/0/1/0/all/0/1\">Johannes Kopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changil Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GETAM: Gradient-weighted Element-wise Transformer Attention Map for Weakly-supervised Semantic segmentation. (arXiv:2112.02841v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02841","description":"<p>Weakly Supervised Semantic Segmentation (WSSS) is challenging, particularly\nwhen image-level labels are used to supervise pixel level prediction. To bridge\ntheir gap, a Class Activation Map (CAM) is usually generated to provide pixel\nlevel pseudo labels. CAMs in Convolutional Neural Networks suffer from partial\nactivation ie, only the most discriminative regions are activated. Transformer\nbased methods, on the other hand, are highly effective at exploring global\ncontext with long range dependency modeling, potentially alleviating the\n\"partial activation\" issue. In this paper, we propose the first transformer\nbased WSSS approach, and introduce the Gradient weighted Element wise\nTransformer Attention Map (GETAM). GETAM shows fine scale activation for all\nfeature map elements, revealing different parts of the object across\ntransformer layers. Further, we propose an activation aware label completion\nmodule to generate high quality pseudo labels. Finally, we incorporate our\nmethods into an end to end framework for WSSS using double backward\npropagation. Extensive experiments on PASCAL VOC and COCO demonstrate that our\nresults beat the state-of-the-art end-to-end approaches by a significant\nmargin, and outperform most multi-stage methods.m most multi-stage methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weixuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SVIP: Sequence VerIfication for Procedures in Videos. (arXiv:2112.06447v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06447","description":"<p>In this paper, we propose a novel sequence verification task that aims to\ndistinguish positive video pairs performing the same action sequence from\nnegative ones with step-level transformations but still conducting the same\ntask. Such a challenging task resides in an open-set setting without prior\naction detection or segmentation that requires event-level or even frame-level\nannotations. To that end, we carefully reorganize two publicly available\naction-related datasets with step-procedure-task structure. To fully\ninvestigate the effectiveness of any method, we collect a scripted video\ndataset enumerating all kinds of step-level transformations in chemical\nexperiments. Besides, a novel evaluation metric Weighted Distance Ratio is\nintroduced to ensure equivalence for different step-level transformations\nduring evaluation. In the end, a simple but effective baseline based on the\ntransformer encoder with a novel sequence alignment loss is introduced to\nbetter characterize long-term dependency between steps, which outperforms other\naction recognition methods. Codes and data will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yicheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weixin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Dongze Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shenghua Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Paced Deep Regression Forests with Consideration on Ranking Fairness. (arXiv:2112.06455v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06455","description":"<p>Deep discriminative models (DDMs), such as deep regression forests, deep\nneural decision forests, have been extensively studied recently to solve\nproblems like facial age estimation, head pose estimation, gaze estimation and\nso forth. Such problems are challenging in part because a large amount of\neffective training data without noise and bias is often not available. While\nsome progress has been achieved through learning more discriminative features,\nor reweighting samples, we argue what is more desirable is to learn gradually\nto discriminate like human beings. Then, we resort to self-paced learning\n(SPL). But a natural question arises: can self-paced regime lead DDMs to\nachieve more robust and less biased solutions? A serious problem with SPL,\nwhich is firstly discussed by this work, is it tends to aggravate the bias of\nsolutions, especially for obvious imbalanced data. To this end, this paper\nproposes a new self-paced paradigm for deep discriminative model, which\ndistinguishes noisy and underrepresented examples according to the output\nlikelihood and entropy associated with each example, and tackle the fundamental\nranking problem in SPL from a new perspective: fairness. This paradigm is\nfundamental, and could be easily combined with a variety of DDMs. Extensive\nexperiments on three computer vision tasks, such as facial age estimation, head\npose estimation and gaze estimation, demonstrate the efficacy of our paradigm.\nTo the best of our knowledge, our work is the first paper in the literature of\nSPL that considers ranking fairness for self-paced regime construction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lili Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1\">Mingming Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yazhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yali Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autoencoder-based background reconstruction and foreground segmentation with background noise estimation. (arXiv:2112.08001v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08001","description":"<p>Even after decades of research, dynamic scene background reconstruction and\nforeground object segmentation are still considered as open problems due\nvarious challenges such as illumination changes, camera movements, or\nbackground noise caused by air turbulence or moving trees. We propose in this\npaper to model the background of a frame sequence as a low dimensional manifold\nusing an autoencoder and compare the reconstructed background provided by this\nautoencoder with the original image to compute the foreground/background\nsegmentation masks. The main novelty of the proposed model is that the\nautoencoder is also trained to predict the background noise, which allows to\ncompute for each frame a pixel-dependent threshold to perform the foreground\nsegmentation. Although the proposed model does not use any temporal or motion\ninformation, it exceeds the state of the art for unsupervised background\nsubtraction on the CDnet 2014 and LASIESTA datasets, with a significant\nimprovement on videos where the camera is moving. It is also able to perform\nbackground reconstruction on some non-video image datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sauvalle_B/0/1/0/all/0/1\">Bruno Sauvalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortelle_A/0/1/0/all/0/1\">Arnaud de La Fortelle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Domain Adversarial Adaptation for Photon-efficient Imaging. (arXiv:2201.02475v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.02475","description":"<p>Photon-efficient imaging with the single-photon LiDAR captures the 3D\nstructure of a scene by only a few detected signal photons per pixel. However,\nthe existing computational methods for photon-efficient imaging are pre-tuned\non a restricted scenario or trained on simulated datasets. When applied to\nrealistic scenarios whose signal-to-background ratios (SBR) and other\nhardware-specific properties differ from those of the original task, the model\nperformance often significantly deteriorates. In this Letter, we present a\ndomain adversarial adaptation design to alleviate this domain shift problem by\nexploiting unlabeled real-world data, with significant resource savings. This\nmethod demonstrates superior performance on simulated and real-world\nexperiments using our home-built up-conversion single-photon imaging system,\nwhich provides an efficient approach to bypass the lack of ground-truth depth\ninformation in implementing computational imaging algorithms for realistic\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yiwei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_G/0/1/0/all/0/1\">Gongxin Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_H/0/1/0/all/0/1\">Hongye Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1\">Xiaomin Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1\">Yu Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Generative Pretraining for Multimodal Video Captioning. (arXiv:2201.08264v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08264","description":"<p>Recent video and language pretraining frameworks lack the ability to generate\nsentences. We present Multimodal Video Generative Pretraining (MV-GPT), a new\npretraining framework for learning from unlabelled videos which can be\neffectively used for generative tasks such as multimodal video captioning.\nUnlike recent video-language pretraining frameworks, our framework trains both\na multimodal video encoder and a sentence decoder jointly. To overcome the lack\nof captions in unlabelled videos, we leverage the future utterance as an\nadditional text source and propose a bidirectional generation objective -- we\ngenerate future utterances given the present mulitmodal context, and also the\npresent utterance given future observations. With this objective, we train an\nencoder-decoder model end-to-end to generate a caption from raw pixels and\ntranscribed speech directly. Our model achieves state-of-the-art performance\nfor multimodal video captioning on four standard benchmarks, as well as for\nother video understanding tasks such as VideoQA, video retrieval and action\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seo_P/0/1/0/all/0/1\">Paul Hongsuck Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing Temporal Information in a Single Frame: Channel Sampling Strategies for Action Recognition. (arXiv:2201.10394v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10394","description":"<p>We address the problem of capturing temporal information for video\nclassification in 2D networks, without increasing computational cost. Existing\napproaches focus on modifying the architecture of 2D networks (e.g. by\nincluding filters in the temporal dimension to turn them into 3D networks, or\nusing optical flow, etc.), which increases computation cost. Instead, we\npropose a novel sampling strategy, where we re-order the channels of the input\nvideo, to capture short-term frame-to-frame changes. We observe that without\nbells and whistles, the proposed sampling strategy improves performance on\nmultiple architectures (e.g. TSN, TRN, and TSM) and datasets (CATER,\nSomething-Something-V1 and V2), up to 24% over the baseline of using the\nstandard video input. In addition, our sampling strategies do not require\ntraining from scratch and do not increase the computational cost of training\nand testing. Given the generality of the results and the flexibility of the\napproach, we hope this can be widely useful to the video understanding\ncommunity. Code is available at https://github.com/kiyoon/PyVideoAI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kiyoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1\">Shreyank N Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1\">Oisin Mac Aodha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sevilla_Lara_L/0/1/0/all/0/1\">Laura Sevilla-Lara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System. (arXiv:2201.12604v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12604","description":"<p>Humans excel at continually learning from an ever-changing environment\nwhereas it remains a challenge for deep neural networks which exhibit\ncatastrophic forgetting. The complementary learning system (CLS) theory\nsuggests that the interplay between rapid instance-based learning and slow\nstructured learning in the brain is crucial for accumulating and retaining\nknowledge. Here, we propose CLS-ER, a novel dual memory experience replay (ER)\nmethod which maintains short-term and long-term semantic memories that interact\nwith the episodic memory. Our method employs an effective replay mechanism\nwhereby new knowledge is acquired while aligning the decision boundaries with\nthe semantic memories. CLS-ER does not utilize the task boundaries or make any\nassumption about the distribution of the data which makes it versatile and\nsuited for \"general continual learning\". Our approach achieves state-of-the-art\nperformance on standard benchmarks as well as more realistic general continual\nlearning settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarfraz_F/0/1/0/all/0/1\">Fahad Sarfraz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VOS: Learning What You Don't Know by Virtual Outlier Synthesis. (arXiv:2202.01197v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.01197","description":"<p>Out-of-distribution (OOD) detection has received much attention lately due to\nits importance in the safe deployment of neural networks. One of the key\nchallenges is that models lack supervision signals from unknown data, and as a\nresult, can produce overconfident predictions on OOD data. Previous approaches\nrely on real outlier datasets for model regularization, which can be costly and\nsometimes infeasible to obtain in practice. In this paper, we present VOS, a\nnovel framework for OOD detection by adaptively synthesizing virtual outliers\nthat can meaningfully regularize the model's decision boundary during training.\nSpecifically, VOS samples virtual outliers from the low-likelihood region of\nthe class-conditional distribution estimated in the feature space. Alongside,\nwe introduce a novel unknown-aware training objective, which contrastively\nshapes the uncertainty space between the ID data and synthesized outlier data.\nVOS achieves competitive performance on both object detection and image\nclassification models, reducing the FPR95 by up to 9.36% compared to the\nprevious best method on object detectors. Code is available at\nhttps://github.com/deeplearning-wisc/vos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xuefeng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Mu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Smooth Neural Functions via Lipschitz Regularization. (arXiv:2202.08345v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08345","description":"<p>Neural implicit fields have recently emerged as a useful representation for\n3D shapes. These fields are commonly represented as neural networks which map\nlatent descriptors and 3D coordinates to implicit function values. The latent\ndescriptor of a neural field acts as a deformation handle for the 3D shape it\nrepresents. Thus, smoothness with respect to this descriptor is paramount for\nperforming shape-editing operations. In this work, we introduce a novel\nregularization designed to encourage smooth latent spaces in neural fields by\npenalizing the upper bound on the field's Lipschitz constant. Compared with\nprior Lipschitz regularized networks, ours is computationally fast, can be\nimplemented in four lines of code, and requires minimal hyperparameter tuning\nfor geometric applications. We demonstrate the effectiveness of our approach on\nshape interpolation and extrapolation as well as partial shape reconstruction\nfrom 3D point clouds, showing both qualitative and quantitative improvements\nover existing state-of-the-art and non-regularized baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hsueh-Ti Derek Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_F/0/1/0/all/0/1\">Francis Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobson_A/0/1/0/all/0/1\">Alec Jacobson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litany_O/0/1/0/all/0/1\">Or Litany</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaker Extraction with Co-Speech Gestures Cue. (arXiv:2203.16840v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.16840","description":"<p>Speaker extraction seeks to extract the clean speech of a target speaker from\na multi-talker mixture speech. There have been studies to use a pre-recorded\nspeech sample or face image of the target speaker as the speaker cue. In human\ncommunication, co-speech gestures that are naturally timed with speech also\ncontribute to speech perception. In this work, we explore the use of co-speech\ngestures sequence, e.g. hand and body movements, as the speaker cue for speaker\nextraction, which could be easily obtained from low-resolution video\nrecordings, thus more available than face recordings. We propose two networks\nusing the co-speech gestures cue to perform attentive listening on the target\nspeaker, one that implicitly fuses the co-speech gestures cue in the speaker\nextraction process, the other performs speech separation first, followed by\nexplicitly using the co-speech gestures cue to associate a separated speech to\nthe target speaker. The experimental results show that the co-speech gestures\ncue is informative in associating with the target speaker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_Z/0/1/0/all/0/1\">Zexu Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_X/0/1/0/all/0/1\">Xinyuan Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A novel stereo matching pipeline with robustness and unfixed disparity search range. (arXiv:2204.04865v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04865","description":"<p>Stereo matching is an essential basis for various applications, but most\nstereo matching methods have poor generalization performance and require a\nfixed disparity search range. Moreover, current stereo matching methods focus\non the scenes that only have positive disparities, but ignore the scenes that\ncontain both positive and negative disparities, such as 3D movies. In this\npaper, we present a new stereo matching pipeline that first computes semi-dense\ndisparity maps based on binocular disparity, and then completes the rest\ndepending on monocular cues. The new stereo matching pipeline have the\nfollowing advantages: It 1) has better generalization performance than most of\nthe current stereo matching methods; 2) relaxes the limitation of a fixed\ndisparity search range; 3) can handle the scenes that involve both positive and\nnegative disparities, which has more potential applications, such as view\nsynthesis in 3D multimedia and VR/AR. Experimental results demonstrate the\neffectiveness of our new stereo matching pipeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiazhi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Image Relational Knowledge Distillation for Semantic Segmentation. (arXiv:2204.06986v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06986","description":"<p>Current Knowledge Distillation (KD) methods for semantic segmentation often\nguide the student to mimic the teacher's structured information generated from\nindividual data samples. However, they ignore the global semantic relations\namong pixels across various images that are valuable for KD. This paper\nproposes a novel Cross-Image Relational KD (CIRKD), which focuses on\ntransferring structured pixel-to-pixel and pixel-to-region relations among the\nwhole images. The motivation is that a good teacher network could construct a\nwell-structured feature space in terms of global pixel dependencies. CIRKD\nmakes the student mimic better structured semantic relations from the teacher,\nthus improving the segmentation performance. Experimental results over\nCityscapes, CamVid and Pascal VOC datasets demonstrate the effectiveness of our\nproposed approach against state-of-the-art distillation methods. The code is\navailable at https://github.com/winycg/CIRKD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chuanguang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Helong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1\">Zhulin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongjun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ResT V2: Simpler, Faster and Stronger. (arXiv:2204.07366v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07366","description":"<p>This paper proposes ResTv2, a simpler, faster, and stronger multi-scale\nvision Transformer for visual recognition. ResTv2 simplifies the EMSA structure\nin ResTv1 (i.e., eliminating the multi-head interaction part) and employs an\nupsample operation to reconstruct the lost medium- and high-frequency\ninformation caused by the downsampling operation. In addition, we explore\ndifferent techniques for better apply ResTv2 backbones to downstream tasks. We\nfound that although combining EMSAv2 and window attention can greatly reduce\nthe theoretical matrix multiply FLOPs, it may significantly decrease the\ncomputation density, thus causing lower actual speed. We comprehensively\nvalidate ResTv2 on ImageNet classification, COCO detection, and ADE20K semantic\nsegmentation. Experimental results show that the proposed ResTv2 can outperform\nthe recently state-of-the-art backbones by a large margin, demonstrating the\npotential of ResTv2 as solid backbones. The code and models will be made\npublicly available at \\url{https://github.com/wofmanaf/ResT}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qing-Long Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yu-Bin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application of Transfer Learning and Ensemble Learning in Image-level Classification for Breast Histopathology. (arXiv:2204.08311v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08311","description":"<p>Background: Breast cancer has the highest prevalence in women globally. The\nclassification and diagnosis of breast cancer and its histopathological images\nhave always been a hot spot of clinical concern. In Computer-Aided Diagnosis\n(CAD), traditional classification models mostly use a single network to extract\nfeatures, which has significant limitations. On the other hand, many networks\nare trained and optimized on patient-level datasets, ignoring the application\nof lower-level data labels.\n</p>\n<p>Method: This paper proposes a deep ensemble model based on image-level labels\nfor the binary classification of benign and malignant lesions of breast\nhistopathological images. First, the BreaKHis dataset is randomly divided into\na training, validation and test set. Then, data augmentation techniques are\nused to balance the number of benign and malignant samples. Thirdly,\nconsidering the performance of transfer learning and the complementarity\nbetween each network, VGG16, Xception, ResNet50, DenseNet201 are selected as\nthe base classifiers.\n</p>\n<p>Result: In the ensemble network model with accuracy as the weight, the\nimage-level binary classification achieves an accuracy of $98.90\\%$. In order\nto verify the capabilities of our method, the latest Transformer and Multilayer\nPerception (MLP) models have been experimentally compared on the same dataset.\nOur model wins with a $5\\%-20\\%$ advantage, emphasizing the ensemble model's\nfar-reaching significance in classification tasks.\n</p>\n<p>Conclusion: This research focuses on improving the model's classification\nperformance with an ensemble algorithm. Transfer learning plays an essential\nrole in small datasets, improving training speed and accuracy. Our model has\noutperformed many existing approaches in accuracy, providing a method for the\nfield of auxiliary medical diagnosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuchao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaomin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haiqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Model-Based Super-Resolution with Non-uniform Blur. (arXiv:2204.10109v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10109","description":"<p>We propose a state-of-the-art method for super-resolution with non-uniform\nblur. Single-image super-resolution methods seek to restore a high-resolution\nimage from blurred, subsampled, and noisy measurements. Despite their\nimpressive performance, existing techniques usually assume a uniform blur\nkernel. Hence, these techniques do not generalize well to the more general case\nof non-uniform blur. Instead, in this paper, we address the more realistic and\ncomputationally challenging case of spatially-varying blur. To this end, we\nfirst propose a fast deep plug-and-play algorithm, based on linearized ADMM\nsplitting techniques, which can solve the super-resolution problem with\nspatially-varying blur. Second, we unfold our iterative algorithm into a single\nnetwork and train it end-to-end. In this way, we overcome the intricacy of\nmanually tuning the parameters involved in the optimization scheme. Our\nalgorithm presents remarkable performance and generalizes well after a single\ntraining to a large family of spatially-varying blur kernels, noise levels and\nscale factors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laroche_C/0/1/0/all/0/1\">Charles Laroche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almansa_A/0/1/0/all/0/1\">Andr&#xe9;s Almansa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tassano_M/0/1/0/all/0/1\">Matias Tassano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BCI: Breast Cancer Immunohistochemical Image Generation through Pyramid Pix2pix. (arXiv:2204.11425v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.11425","description":"<p>The evaluation of human epidermal growth factor receptor 2 (HER2) expression\nis essential to formulate a precise treatment for breast cancer. The routine\nevaluation of HER2 is conducted with immunohistochemical techniques (IHC),\nwhich is very expensive. Therefore, for the first time, we propose a breast\ncancer immunohistochemical (BCI) benchmark attempting to synthesize IHC data\ndirectly with the paired hematoxylin and eosin (HE) stained images. The dataset\ncontains 4870 registered image pairs, covering a variety of HER2 expression\nlevels. Based on BCI, as a minor contribution, we further build a pyramid\npix2pix image generation method, which achieves better HE to IHC translation\nresults than the other current popular algorithms. Extensive experiments\ndemonstrate that BCI poses new challenges to the existing image translation\nresearch. Besides, BCI also opens the door for future pathology studies in HER2\nexpression evaluation based on the synthesized IHC images. BCI dataset can be\ndownloaded from https://bupt-ai-cz.github.io/BCI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Shengjie Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_C/0/1/0/all/0/1\">Chuang Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_F/0/1/0/all/0/1\">Feng Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_X/0/1/0/all/0/1\">Xinyu Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1\">Zhongyue Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_M/0/1/0/all/0/1\">Mulan Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaTriplet: Adaptive Gradient Triplet Loss with Automatic Margin Learning for Forensic Medical Image Matching. (arXiv:2205.02849v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.02849","description":"<p>This paper tackles the challenge of forensic medical image matching (FMIM)\nusing deep neural networks (DNNs). FMIM is a particular case of content-based\nimage retrieval (CBIR). The main challenge in FMIM compared to the general case\nof CBIR, is that the subject to whom a query image belongs may be affected by\naging and progressive degenerative disorders, making it difficult to match data\non a subject level. CBIR with DNNs is generally solved by minimizing a ranking\nloss, such as Triplet loss (TL), computed on image representations extracted by\na DNN from the original data. TL, in particular, operates on triplets: anchor,\npositive (similar to anchor) and negative (dissimilar to anchor). Although TL\nhas been shown to perform well in many CBIR tasks, it still has limitations,\nwhich we identify and analyze in this work. In this paper, we introduce (i) the\nAdaTriplet loss -- an extension of TL whose gradients adapt to different\ndifficulty levels of negative samples, and (ii) the AutoMargin method -- a\ntechnique to adjust hyperparameters of margin-based losses such as TL and our\nproposed loss dynamically. Our results are evaluated on two large-scale\nbenchmarks for FMIM based on the Osteoarthritis Initiative and Chest X-ray-14\ndatasets. The codes allowing replication of this study have been made publicly\navailable at \\url{https://github.com/Oulu-IMEDS/AdaTriplet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_K/0/1/0/all/0/1\">Khanh Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Huy Hoang Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tiulpin_A/0/1/0/all/0/1\">Aleksei Tiulpin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn-to-Race Challenge 2022: Benchmarking Safe Learning and Cross-domain Generalisation in Autonomous Racing. (arXiv:2205.02953v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2205.02953","description":"<p>We present the results of our autonomous racing virtual challenge, based on\nthe newly-released Learn-to-Race (L2R) simulation framework, which seeks to\nencourage interdisciplinary research in autonomous driving and to help advance\nthe state of the art on a realistic benchmark. Analogous to racing being used\nto test cutting-edge vehicles, we envision autonomous racing to serve as a\nparticularly challenging proving ground for autonomous agents as: (i) they need\nto make sub-second, safety-critical decisions in a complex, fast-changing\nenvironment; and (ii) both perception and control must be robust to\ndistribution shifts, novel road features, and unseen obstacles. Thus, the main\ngoal of the challenge is to evaluate the joint safety, performance, and\ngeneralisation capabilities of reinforcement learning agents on multi-modal\nperception, through a two-stage process. In the first stage of the challenge,\nwe evaluate an autonomous agent's ability to drive as fast as possible, while\nadhering to safety constraints. In the second stage, we additionally require\nthe agent to adapt to an unseen racetrack through safe exploration. In this\npaper, we describe the new L2R Task 2.0 benchmark, with refined metrics and\nbaseline approaches. We also provide an overview of deployment, evaluation, and\nrankings for the inaugural instance of the L2R Autonomous Racing Virtual\nChallenge (supported by Carnegie Mellon University, Arrival Ltd., AICrowd,\nAmazon Web Services, and Honda Research), which officially used the new L2R\nTask 2.0 benchmark and received over 20,100 views, 437 active participants, 46\nteams, and 733 model submissions -- from 88+ unique institutions, in 58+\ndifferent countries. Finally, we release leaderboard results from the challenge\nand provide description of the two top-ranking approaches in cross-domain model\ntransfer, across multiple sensor configurations and simulated races.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bingqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kathpal_S/0/1/0/all/0/1\">Sidharth Kathpal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poonganam_J/0/1/0/all/0/1\">Jyotish Poonganam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shivani_A/0/1/0/all/0/1\">Ayush Shivani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_V/0/1/0/all/0/1\">Vrushank Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genc_S/0/1/0/all/0/1\">Sahika Genc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhukov_I/0/1/0/all/0/1\">Ivan Zhukov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumskoy_M/0/1/0/all/0/1\">Max Kumskoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anirudh Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyberg_E/0/1/0/all/0/1\">Eric Nyberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement. (arXiv:2205.03569v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.03569","description":"<p>Compressed video action recognition has recently drawn growing attention,\nsince it remarkably reduces the storage and computational cost via replacing\nraw videos by sparsely sampled RGB frames and compressed motion cues (e.g.,\nmotion vectors and residuals). However, this task severely suffers from the\ncoarse and noisy dynamics and the insufficient fusion of the heterogeneous RGB\nand motion modalities. To address the two issues above, this paper proposes a\nnovel framework, namely Attentive Cross-modal Interaction Network with Motion\nEnhancement (MEACI-Net). It follows the two-stream architecture, i.e. one for\nthe RGB modality and the other for the motion modality. Particularly, the\nmotion stream employs a multi-scale block embedded with a denoising module to\nenhance representation learning. The interaction between the two streams is\nthen strengthened by introducing the Selective Motion Complement (SMC) and\nCross-Modality Augment (CMA) modules, where SMC complements the RGB modality\nwith spatio-temporally attentive local motion features and CMA further combines\nthe two modalities with selective feature augmentation. Extensive experiments\non the UCF-101, HMDB-51 and Kinetics-400 benchmarks demonstrate the\neffectiveness and efficiency of MEACI-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_X/0/1/0/all/0/1\">Xiuguo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HierAttn: Effectively Learn Representations from Stage Attention and Branch Attention for Skin Lesions Diagnosis. (arXiv:2205.04326v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.04326","description":"<p>An accurate and unbiased examination of skin lesions is critical for the\nearly diagnosis and treatment of skin cancers. The visual feature of the skin\nlesions varies significantly because skin images are collected from patients\nwith different skin colours by using various devices. Recent studies have\ndeveloped ensembled convolutional neural networks (CNNs) to classify the images\nfor early diagnosis. However, the practical use of CNNs is limited because\ntheir network structures are heavyweight and neglect contextual information.\nVision transformers (ViTs) learn the global features by self-attention\nmechanisms, but they also have comparatively large model sizes (more than\n100M). To address these limitations, we introduce HierAttn, a lite and\neffective neural network with hierarchical and self attention. HierAttn applies\na novel strategy based on learning local and global features by a multi-stage\nand hierarchical network. The efficacy of HierAttn was evaluated by using the\ndermoscopy images dataset ISIC2019 and smartphone photos dataset PAD-UFES-20.\nThe experimental results show that HierAttn achieves the best top-1 accuracy\nand AUC among state-of-the-art mobile networks, including MobileNetV3 and\nMobileViT. The code is available at https://github.com/anthonyweidai/HierAttn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_T/0/1/0/all/0/1\">Tianyi Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1\">Min Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}