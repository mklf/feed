{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-14T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Towards Highly Expressive Machine Learning Models of Non-Melanoma Skin Cancer. (arXiv:2207.05749v1 [cs.LG])","link":"http://arxiv.org/abs/2207.05749","description":"<p>Pathologists have a rich vocabulary with which they can describe all the\nnuances of cellular morphology. In their world, there is a natural pairing of\nimages and words. Recent advances demonstrate that machine learning models can\nnow be trained to learn high-quality image features and represent them as\ndiscrete units of information. This enables natural language, which is also\ndiscrete, to be jointly modelled alongside the imaging, resulting in a\ndescription of the contents of the imaging. Here we present experiments in\napplying discrete modelling techniques to the problem domain of non-melanoma\nskin cancer, specifically, histological images of Intraepidermal Carcinoma\n(IEC). Implementing a VQ-GAN model to reconstruct high-resolution (256x256)\nimages of IEC images, we trained a sequence-to-sequence transformer to generate\nnatural language descriptions using pathologist terminology. Combined with the\nidea of interactive concept vectors available by using continuous generative\nmethods, we demonstrate an additional angle of interpretability. The result is\na promising means of working towards highly expressive machine learning systems\nwhich are not only useful as predictive/classification tools, but also means to\nfurther our scientific understanding of disease.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Simon M. Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lefevre_J/0/1/0/all/0/1\">James G. Lefevre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baxter_G/0/1/0/all/0/1\">Glenn Baxter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_N/0/1/0/all/0/1\">Nicholas A.Hamilton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CQC: A Crosstalk-Aware Quantum Program Compilation Framework. (arXiv:2207.05751v1 [quant-ph])","link":"http://arxiv.org/abs/2207.05751","description":"<p>Near-term quantum systems are noisy. Crosstalk noise has been identified as\none of the major sources of noises in superconducting Noisy Intermediate-Scale\nQuantum (NISQ) devices. Crosstalk arises from the concurrent execution of\ntwo-qubit gates, such as \\texttt{CX}, on nearby qubits. It may significantly\nincrease the error rate of gates compared to running them individually.\nCrosstalk can be mitigated through scheduling or hardware tuning. Prior\nstudies, however, handle crosstalk at a very late stage in the compilation\nlater, typically after hardware mapping is done. It might miss great\nopportunities of optimizing algorithm logic, routing, and crosstalk at the same\ntime. In this paper, we push the envelope by considering all these factors\nsimultaneously at the very early compilation stage. We propose a\ncrosstalk-aware quantum program compilation framework called CQC that can\nenhance crosstalk-mitigation while achieving satisfactory circuit depth.\nMoreover, we identify opportunities for translation from intermediate\nrepresentation to the circuit for application-specific crosstalk mitigation,\nfor instance, the \\texttt{CX} ladder construction in variational quantum\neigensolvers (VQE). Evaluations through simulation and on real IBM-Q devices\nshow that our framework can significantly reduce the error rate by up to\n6$\\times$, with only $\\sim$60\\% circuit depth compared to state-of-the-art gate\nscheduling approaches. In particular for VQE, we demonstrate 49\\% circuit depth\nreduction with 9.6\\% fidelity improvement over prior art on the H4 molecule\nusing IBMQ Guadalupe. Our CQC framework will be released on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Hua_F/0/1/0/all/0/1\">Fei Hua</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Jin_Y/0/1/0/all/0/1\">Yuwei Jin</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chen_Y/0/1/0/all/0/1\">Yanhao Chen</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Hayes_A/0/1/0/all/0/1\">Ari Hayes</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Gao_H/0/1/0/all/0/1\">Hang Gao</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhang_E/0/1/0/all/0/1\">Eddy.Z Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OSLAT: Open Set Label Attention Transformer for Medical Entity Span Extraction. (arXiv:2207.05817v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05817","description":"<p>Identifying spans in medical texts that correspond to medical entities is one\nof the core steps for many healthcare NLP tasks such as ICD coding, medical\nfinding extraction, medical note contextualization, to name a few. Existing\nentity extraction methods rely on a fixed and limited vocabulary of medical\nentities and have difficulty with extracting entities represented by disjoint\nspans. In this paper, we present a new transformer-based architecture called\nOSLAT, Open Set Label Attention Transformer, that addresses many of the\nlimitations of the previous methods. Our approach uses the label-attention\nmechanism to implicitly learn spans associated with entities of interest. These\nentities can be provided as free text, including entities not seen during\nOSLAT's training, and the model can extract spans even when they are disjoint.\nTo test the generalizability of our method, we train two separate models on two\ndifferent datasets, which have very low entity overlap: (1) a public discharge\nnotes dataset from hNLP, and (2) a much more challenging proprietary patient\ntext dataset \"Reasons for Encounter\" (RFE). We find that OSLAT models trained\non either dataset outperform rule-based and fuzzy string matching baselines\nwhen applied to the RFE dataset as well as to the portion of hNLP dataset where\nentities are represented by disjoint spans. Our code can be found at\nhttps://github.com/curai/curai-research/tree/main/OSLAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Raymond Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valmianski_I/0/1/0/all/0/1\">Ilya Valmianski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Li Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amatriain_X/0/1/0/all/0/1\">Xavier Amatriain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_A/0/1/0/all/0/1\">Anitha Kannan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Social Graph Networks for Emotion Prediction. (arXiv:2207.05820v1 [cs.SI])","link":"http://arxiv.org/abs/2207.05820","description":"<p>Emotion prediction plays an essential role in mental health and emotion-aware\ncomputing. The complex nature of emotion resulting from its dependency on a\nperson's physiological health, mental state, and his surroundings makes its\nprediction a challenging task. In this work, we utilize mobile sensing data to\npredict happiness and stress. In addition to a person's physiological features,\nwe also incorporate the environment's impact through weather and social\nnetwork. To this end, we leverage phone data to construct social networks and\ndevelop a machine learning architecture that aggregates information from\nmultiple users of the graph network and integrates it with the temporal\ndynamics of data to predict emotion for all the users. The construction of\nsocial networks does not incur additional cost in terms of EMAs or data\ncollection from users and doesn't raise privacy concerns. We propose an\narchitecture that automates the integration of a user's social network affect\nprediction, is capable of dealing with the dynamic distribution of real-life\nsocial networks, making it scalable to large-scale networks. Our extensive\nevaluation highlights the improvement provided by the integration of social\nnetworks. We further investigate the impact of graph topology on model's\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalid_M/0/1/0/all/0/1\">Maryam Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sano_A/0/1/0/all/0/1\">Akane Sano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sockeye 3: Fast Neural Machine Translation with PyTorch. (arXiv:2207.05851v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05851","description":"<p>Sockeye 3 is the latest version of the Sockeye toolkit for Neural Machine\nTranslation (NMT). Now based on PyTorch, Sockeye 3 provides faster model\nimplementations and more advanced features with a further streamlined codebase.\nThis enables broader experimentation with faster iteration, efficient training\nof stronger and faster models, and the flexibility to move new ideas quickly\nfrom research to production. When running comparable models, Sockeye 3 is up to\n126% faster than other PyTorch implementations on GPUs and up to 292% faster on\nCPUs. Sockeye 3 is open source software released under the Apache 2.0 license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hieber_F/0/1/0/all/0/1\">Felix Hieber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denkowski_M/0/1/0/all/0/1\">Michael Denkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Domhan_T/0/1/0/all/0/1\">Tobias Domhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barros_B/0/1/0/all/0/1\">Barbara Darques Barros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Celina Dong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_C/0/1/0/all/0/1\">Cuong Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Ke Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_B/0/1/0/all/0/1\">Benjamin Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadejde_M/0/1/0/all/0/1\">Maria Nadejde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakew_S/0/1/0/all/0/1\">Surafel Lakew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_P/0/1/0/all/0/1\">Prashant Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federico_M/0/1/0/all/0/1\">Marcello Federico</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Currey_A/0/1/0/all/0/1\">Anna Currey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel DeBERTa-based Model for Financial Question Answering Task. (arXiv:2207.05875v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05875","description":"<p>As a rising star in the field of natural language processing, question\nanswering systems (Q&amp;A Systems) are widely used in all walks of life. Compared\nwith other scenarios, the applicationin financial scenario has strong\nrequirements in the traceability and interpretability of the Q&amp;A systems. In\naddition, since the demand for artificial intelligence technology has gradually\nshifted from the initial computational intelligence to cognitive intelligence,\nthis research mainly focuses on the financial numerical reasoning dataset -\nFinQA. In the shared task, the objective is to generate the reasoning program\nand the final answer according to the given financial report containing text\nand tables. We use the method based on DeBERTa pre-trained language model, with\nadditional optimization methods including multi-model fusion, training set\ncombination on this basis. We finally obtain an execution accuracy of 68.99 and\na program accuracy of 64.53, ranking No. 4 in the 2022 FinQA Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanbo J. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Hui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yuhang Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sheng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Word Semantics to Enrich Character Representations of Chinese Pre-trained Models. (arXiv:2207.05928v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05928","description":"<p>Most of the Chinese pre-trained models adopt characters as basic units for\ndownstream tasks. However, these models ignore the information carried by words\nand thus lead to the loss of some important semantics. In this paper, we\npropose a new method to exploit word structure and integrate lexical semantics\ninto character representations of pre-trained models. Specifically, we project\na word's embedding into its internal characters' embeddings according to the\nsimilarity weight. To strengthen the word boundary information, we mix the\nrepresentations of the internal characters within a word. After that, we apply\na word-to-character alignment attention mechanism to emphasize important\ncharacters by masking unimportant ones. Moreover, in order to reduce the error\npropagation caused by word segmentation, we present an ensemble approach to\ncombine segmentation results given by different tokenizers. The experimental\nresults show that our approach achieves superior performance over the basic\npre-trained models BERT, BERT-wwm and ERNIE on different Chinese NLP tasks:\nsentiment classification, sentence pair matching, natural language inference\nand machine reading comprehension. We make further analysis to prove the\neffectiveness of each component of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Rui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A General Contextualized Rewriting Framework for Text Summarization. (arXiv:2207.05948v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05948","description":"<p>The rewriting method for text summarization combines extractive and\nabstractive approaches, improving the conciseness and readability of extractive\nsummaries using an abstractive model. Exiting rewriting systems take each\nextractive sentence as the only input, which is relatively focused but can lose\nnecessary background knowledge and discourse context. In this paper, we\ninvestigate contextualized rewriting, which consumes the entire document and\nconsiders the summary context. We formalize contextualized rewriting as a\nseq2seq with group-tag alignments, introducing group-tag as a solution to model\nthe alignments, identifying extractive sentences through content-based\naddressing. Results show that our approach significantly outperforms\nnon-contextualized rewriting systems without requiring reinforcement learning,\nachieving strong improvements on ROUGE scores upon multiple extractors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_G/0/1/0/all/0/1\">Guangsheng Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Developing a Component Comment Extractor from Product Reviews on E-Commerce Sites. (arXiv:2207.05979v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05979","description":"<p>Consumers often read product reviews to inform their buying decision, as some\nconsumers want to know a specific component of a product. However, because\ntypical sentences on product reviews contain various details, users must\nidentify sentences about components they want to know amongst the many reviews.\nTherefore, we aimed to develop a system that identifies and collects component\nand aspect information of products in sentences. Our BERT-based classifiers\nassign labels referring to components and aspects to sentences in reviews and\nextract sentences with comments on specific components and aspects. We\ndetermined proper labels based for the words identified through pattern\nmatching from product reviews to create the training data. Because we could not\nuse the words as labels, we carefully created labels covering the meanings of\nthe words. However, the training data was imbalanced on component and aspect\npairs. We introduced a data augmentation method using WordNet to reduce the\nbias. Our evaluation demonstrates that the system can determine labels for road\nbikes using pattern matching, covering more than 88\\% of the indicators of\ncomponents and aspects on e-commerce sites. Moreover, our data augmentation\nmethod can improve the-F1-measure on insufficient data from 0.66 to 0.76.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anda_S/0/1/0/all/0/1\">Shogo Anda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kikuchi_M/0/1/0/all/0/1\">Masato Kikuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozono_T/0/1/0/all/0/1\">Tadachika Ozono</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DocCoder: Generating Code by Retrieving and Reading Docs. (arXiv:2207.05987v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05987","description":"<p>Natural-language-to-code models learn to generate a code snippet given a\nnatural language (NL) intent. However, the rapid growth of both publicly\navailable and proprietary libraries and functions makes it impossible to cover\nall APIs using training examples, as new libraries and functions are introduced\ndaily. Thus, existing models inherently cannot generalize to using unseen\nfunctions and libraries merely through incorporating them into the training\ndata. In contrast, when human programmers write programs, they frequently refer\nto textual resources such as code manuals, documentation, and tutorials, to\nexplore and understand available library functionality. Inspired by this\nobservation, we introduce DocCoder: an approach that explicitly leverages code\nmanuals and documentation by (1) retrieving the relevant documentation given\nthe NL intent, and (2) generating the code based on the NL intent and the\nretrieved documentation. Our approach is general, can be applied to any\nprogramming language, and is agnostic to the underlying neural model. We\ndemonstrate that DocCoder consistently improves NL-to-code models: DocCoder\nachieves 11x higher exact match accuracy than strong baselines on a new Bash\ndataset tldr; on the popular Python CoNaLa benchmark, DocCoder improves over\nstrong baselines by 1.65 BLEU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1\">Uri Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Frank F. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+JIang_Z/0/1/0/all/0/1\">Zhengbao JIang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-driven Emotional Style Control and Cross-speaker Style Transfer in Neural TTS. (arXiv:2207.06000v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06000","description":"<p>Expressive text-to-speech has shown improved performance in recent years.\nHowever, the style control of synthetic speech is often restricted to discrete\nemotion categories and requires training data recorded by the target speaker in\nthe target style. In many practical situations, users may not have reference\nspeech recorded in target emotion but still be interested in controlling speech\nstyle just by typing text description of desired emotional style. In this\npaper, we propose a text-based interface for emotional style control and\ncross-speaker style transfer in multi-speaker TTS. We propose the bi-modal\nstyle encoder which models the semantic relationship between text description\nembedding and speech style embedding with a pretrained language model. To\nfurther improve cross-speaker style transfer on disjoint, multi-style datasets,\nwe propose the novel style loss. The experimental results show that our model\ncan generate high-quality expressive speech even in unseen style.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1\">Yookyung Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Younggun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_S/0/1/0/all/0/1\">Suhee Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_Y/0/1/0/all/0/1\">Yeongtae Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taesu Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fuse It More Deeply! A Variational Transformer with Layer-Wise Latent Variable Inference for Text Generation. (arXiv:2207.06130v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06130","description":"<p>The past several years have witnessed Variational Auto-Encoder's superiority\nin various text generation tasks. However, due to the sequential nature of the\ntext, auto-regressive decoders tend to ignore latent variables and then reduce\nto simple language models, known as the KL vanishing problem, which would\nfurther deteriorate when VAE is combined with Transformer-based structures. To\nameliorate this problem, we propose DELLA, a novel variational Transformer\nframework. DELLA learns a series of layer-wise latent variables with each\ninferred from those of lower layers and tightly coupled with the hidden states\nby low-rank tensor product. In this way, DELLA forces these posterior latent\nvariables to be fused deeply with the whole computation path and hence\nincorporate more information. We theoretically demonstrate that our method can\nbe regarded as entangling latent variables to avoid posterior information\ndecrease through layers, enabling DELLA to get higher non-zero KL values even\nwithout any annealing or thresholding tricks. Experiments on four unconditional\nand three conditional generation tasks show that DELLA could better alleviate\nKL vanishing and improve both quality and diversity compared to several strong\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xiaoyuan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Hashtags to Analyze Purpose and Technology Application of Open-Source Project Related to COVID-19. (arXiv:2207.06219v1 [cs.IR])","link":"http://arxiv.org/abs/2207.06219","description":"<p>COVID-19 has had a profound impact on the lives of all human beings. Emerging\ntechnologies have made significant contributions to the fight against the\npandemic. An extensive review of the application of technology will help\nfacilitate future research and technology development to provide better\nsolutions for future pandemics. In contrast to the extensive surveys of\nacademic communities that have already been conducted, this study explores the\nIT community of practice. Using GitHub as the study target, we analyzed the\nmain functionalities of the projects submitted during the pandemic. This study\nexamines trends in projects with different functionalities and the relationship\nbetween functionalities and technologies. The study results show an imbalance\nin the number of projects with varying functionalities in the GitHub community,\ni.e., applications account for more than half of the projects. In contrast,\nother data analysis and AI projects account for a smaller share. This differs\nsignificantly from the survey of the academic community, where the findings\nfocus more on cutting-edge technologies while projects in the community of\npractice use more mature technologies. The spontaneous behavior of developers\nmay lack organization and make it challenging to target needs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Liang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengzhi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building a Relation Extraction Baseline for Gene-Disease Associations: A Reproducibility Study. (arXiv:2207.06226v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06226","description":"<p>Reproducibility is an important task in scientific research. It is crucial\nfor researchers to compare newly developed systems with the state-of-the-art to\nassess whether they made a breakthrough. However previous works may not be\nimmediately reproducible, for example due to the lack of source code. In this\nwork we reproduce DEXTER, a system to automatically extract Gene-Disease\nAssociations (GDAs) from biomedical abstracts. The goal is to provide a\nbenchmark for future works regarding Relation Extraction (RE), enabling\nresearchers to test and compare their results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Menotti_L/0/1/0/all/0/1\">Laura Menotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transfer Learning Based Model for Text Readability Assessment in German. (arXiv:2207.06265v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06265","description":"<p>Text readability assessment has a wide range of applications for different\ntarget people, from language learners to people with disabilities. The fast\npace of textual content production on the web makes it impossible to measure\ntext complexity without the benefit of machine learning and natural language\nprocessing techniques. Although various research addressed the readability\nassessment of English text in recent years, there is still room for improvement\nof the models for other languages. In this paper, we proposed a new model for\ntext complexity assessment for German text based on transfer learning. Our\nresults show that the model outperforms more classical solutions based on\nlinguistic features extraction from input text. The best model is based on the\nBERT pre-trained language model achieved the Root Mean Square Error (RMSE) of\n0.483.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohtaj_S/0/1/0/all/0/1\">Salar Mohtaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naderi_B/0/1/0/all/0/1\">Babak Naderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_S/0/1/0/all/0/1\">Sebastian M&#xf6;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maschhur_F/0/1/0/all/0/1\">Faraz Maschhur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reinhard_M/0/1/0/all/0/1\">Max Reinhard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re2G: Retrieve, Rerank, Generate. (arXiv:2207.06300v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06300","description":"<p>As demonstrated by GPT-3 and T5, transformers grow in capability as parameter\nspaces become larger and larger. However, for tasks that require a large amount\nof knowledge, non-parametric memory allows models to grow dramatically with a\nsub-linear increase in computational cost and GPU memory requirements. Recent\nmodels such as RAG and REALM have introduced retrieval into conditional\ngeneration. These models incorporate neural initial retrieval from a corpus of\npassages. We build on this line of research, proposing Re2G, which combines\nboth neural initial retrieval and reranking into a BART-based\nsequence-to-sequence generation. Our reranking approach also permits merging\nretrieval results from sources with incomparable scores, enabling an ensemble\nof BM25 and neural initial retrieval. To train our system end-to-end, we\nintroduce a novel variation of knowledge distillation to train the initial\nretrieval, reranker, and generation using only ground truth on the target\nsequence output. We find large gains in four diverse tasks: zero-shot slot\nfilling, question answering, fact-checking, and dialog, with relative gains of\n9% to 34% over the previous state-of-the-art on the KILT leaderboard. We make\nour code available as open source at\nhttps://github.com/IBM/kgi-slot-filling/tree/re2g.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glass_M/0/1/0/all/0/1\">Michael Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1\">Md Faisal Mahbub Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Ankita Rajaram Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1\">Pengshan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"N-Grammer: Augmenting Transformers with latent n-grams. (arXiv:2207.06366v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06366","description":"<p>Transformer models have recently emerged as one of the foundational models in\nnatural language processing, and as a byproduct, there is significant recent\ninterest and investment in scaling these models. However, the training and\ninference costs of these large Transformer language models are prohibitive,\nthus necessitating more research in identifying more efficient variants. In\nthis work, we propose a simple yet effective modification to the Transformer\narchitecture inspired by the literature in statistical language modeling, by\naugmenting the model with n-grams that are constructed from a discrete latent\nrepresentation of the text sequence. We evaluate our model, the N-Grammer on\nlanguage modeling on the C4 data-set as well as text classification on the\nSuperGLUE data-set, and find that it outperforms several strong baselines such\nas the Transformer and the Primer. We open-source our model for reproducibility\npurposes in Jax.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Aurko Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1\">Rohan Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_G/0/1/0/all/0/1\">Guangda Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Benjamin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jeffrey Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ye Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swavely_R/0/1/0/all/0/1\">Rigel Swavely</a>, Tao (Alex)Yu, <a href=\"http://arxiv.org/find/cs/1/au:+Dao_P/0/1/0/all/0/1\">Phuong Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fifty_C/0/1/0/all/0/1\">Christopher Fifty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Concept Grounding on Neural Fields. (arXiv:2207.06403v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06403","description":"<p>In this paper, we address the challenging problem of 3D concept grounding\n(i.e. segmenting and learning visual concepts) by looking at RGBD images and\nreasoning about paired questions and answers. Existing visual reasoning\napproaches typically utilize supervised methods to extract 2D segmentation\nmasks on which concepts are grounded. In contrast, humans are capable of\ngrounding concepts on the underlying 3D representation of images. However,\ntraditionally inferred 3D representations (e.g., point clouds, voxelgrids, and\nmeshes) cannot capture continuous 3D features flexibly, thus making it\nchallenging to ground concepts to 3D regions based on the language description\nof the object being referred to. To address both issues, we propose to leverage\nthe continuous, differentiable nature of neural fields to segment and learn\nconcepts. Specifically, each 3D coordinate in a scene is represented as a\nhigh-dimensional descriptor. Concept grounding can then be performed by\ncomputing the similarity between the descriptor vector of a 3D coordinate and\nthe vector embedding of a language concept, which enables segmentations and\nconcept learning to be jointly learned on neural fields in a differentiable\nfashion. As a result, both 3D semantic and instance segmentations can emerge\ndirectly from question answering supervision using a set of defined neural\noperators on top of neural fields (e.g., filtering and counting). Experimental\nresults show that our proposed framework outperforms\nunsupervised/language-mediated segmentation models on semantic and instance\nsegmentation tasks, as well as outperforms existing models on the challenging\n3D aware visual reasoning tasks. Furthermore, our framework can generalize well\nto unseen shape categories and real scans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yining Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunru Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic modeling of rational communication with conditionals. (arXiv:2105.05502v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.05502","description":"<p>While a large body of work has scrutinized the meaning of conditional\nsentences, considerably less attention has been paid to formal models of their\npragmatic use and interpretation. Here, we take a probabilistic approach to\npragmatic reasoning about indicative conditionals which flexibly integrates\ngradient beliefs about richly structured world states. We model listeners'\nupdate of their prior beliefs about the causal structure of the world and the\njoint probabilities of the consequent and antecedent based on assumptions about\nthe speaker's utterance production protocol. We show that, when supplied with\nnatural contextual assumptions, our model uniformly explains a number of\ninferences attested in the literature, including epistemic inferences,\nconditional perfection and the dependency between antecedent and consequent of\na conditional. We argue that this approach also helps explain three puzzles\nintroduced by Douven (2012) about updating with conditionals: depending on the\nutterance context, the listener's belief in the antecedent may increase,\ndecrease or remain unchanged.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grusdt_B/0/1/0/all/0/1\">Britta Grusdt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassiter_D/0/1/0/all/0/1\">Daniel Lassiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franke_M/0/1/0/all/0/1\">Michael Franke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Machines Learn Morality? The Delphi Experiment. (arXiv:2110.07574v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07574","description":"<p>As AI systems become increasingly powerful and pervasive, there are growing\nconcerns about machines' morality or a lack thereof. Yet, teaching morality to\nmachines is a formidable task, as morality remains among the most intensely\ndebated questions in humanity, let alone for AI. Existing AI systems deployed\nto millions of users, however, are already making decisions loaded with moral\nimplications, which poses a seemingly impossible challenge: teaching machines\nmoral sense, while humanity continues to grapple with it.\n</p>\n<p>To explore this challenge, we introduce Delphi, an experimental framework\nbased on deep neural networks trained directly to reason about descriptive\nethical judgments, e.g., \"helping a friend\" is generally good, while \"helping a\nfriend spread fake news\" is not. Empirical results shed novel insights on the\npromises and limits of machine ethics; Delphi demonstrates strong\ngeneralization capabilities in the face of novel ethical situations, while\noff-the-shelf neural network models exhibit markedly poor judgment including\nunjust biases, confirming the need for explicitly teaching machines moral\nsense.\n</p>\n<p>Yet, Delphi is not perfect, exhibiting susceptibility to pervasive biases and\ninconsistencies. Despite that, we demonstrate positive use cases of imperfect\nDelphi, including using it as a component model within other imperfect AI\nsystems. Importantly, we interpret the operationalization of Delphi in light of\nprominent ethical theories, which leads us to important future research\nquestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jenny Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1\">Jesse Dodge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1\">Keisuke Sakaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forbes_M/0/1/0/all/0/1\">Maxwell Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borchardt_J/0/1/0/all/0/1\">Jon Borchardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabriel_S/0/1/0/all/0/1\">Saadia Gabriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etzioni_O/0/1/0/all/0/1\">Oren Etzioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rini_R/0/1/0/all/0/1\">Regina Rini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks. (arXiv:2112.03227v4 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2112.03227","description":"<p>General-purpose robots coexisting with humans in their environment must learn\nto relate human language to their perceptions and actions to be useful in a\nrange of daily tasks. Moreover, they need to acquire a diverse repertoire of\ngeneral-purpose skills that allow composing long-horizon tasks by following\nunconstrained language instructions. In this paper, we present CALVIN\n(Composing Actions from Language and Vision), an open-source simulated\nbenchmark to learn long-horizon language-conditioned tasks. Our aim is to make\nit possible to develop agents that can solve many robotic manipulation tasks\nover a long horizon, from onboard sensors, and specified only via human\nlanguage. CALVIN tasks are more complex in terms of sequence length, action\nspace, and language than existing vision-and-language task datasets and\nsupports flexible specification of sensor suites. We evaluate the agents in\nzero-shot to novel language instructions and to novel environments and objects.\nWe show that a baseline model based on multi-context imitation learning\nperforms poorly on CALVIN, suggesting that there is significant room for\ndeveloping innovative agents that learn to relate human language to their world\nmodels with this benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mees_O/0/1/0/all/0/1\">Oier Mees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermann_L/0/1/0/all/0/1\">Lukas Hermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosete_Beas_E/0/1/0/all/0/1\">Erick Rosete-Beas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FRUIT: Faithfully Reflecting Updated Information in Text. (arXiv:2112.08634v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08634","description":"<p>Textual knowledge bases such as Wikipedia require considerable effort to keep\nup to date and consistent. While automated writing assistants could potentially\nease this burden, the problem of suggesting edits grounded in external\nknowledge has been under-explored. In this paper, we introduce the novel\ngeneration task of *faithfully reflecting updated information in text* (FRUIT)\nwhere the goal is to update an existing article given new evidence. We release\nthe FRUIT-WIKI dataset, a collection of over 170K distantly supervised data\nproduced from pairs of Wikipedia snapshots, along with our data generation\npipeline and a gold evaluation set of 914 instances whose edits are guaranteed\nto be supported by the evidence. We provide benchmark results for popular\ngeneration systems as well as EDIT5 -- a T5-based approach tailored to editing\nwe introduce that establishes the state of the art. Our analysis shows that\ndeveloping models that can update articles faithfully requires new capabilities\nfor neural generation models, and opens doors to many new applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Logan_R/0/1/0/all/0/1\">Robert L. Logan IV</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passos_A/0/1/0/all/0/1\">Alexandre Passos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Degendering Resumes for Fair Algorithmic Resume Screening. (arXiv:2112.08910v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08910","description":"<p>We investigate whether it is feasible to remove gendered information from\nresumes to mitigate potential bias in algorithmic resume screening. Using a\ncorpus of 709k resumes from IT firms, we first train a series of models to\nclassify the self-reported gender of the applicant, thereby measuring the\nextent and nature of gendered information encoded in resumes. We then conduct a\nseries of gender obfuscation experiments, where we iteratively remove gendered\ninformation from resumes. Finally, we train a resume screening algorithm and\ninvestigate the trade-off between gender obfuscation and screening algorithm\nperformance. Results show: (1) There is a significant amount of gendered\ninformation in resumes. (2) Lexicon-based gender obfuscation method (i.e.\nremoving tokens that are predictive of gender) can reduce the amount of\ngendered information to a large extent. However, after a certain point, the\nperformance of the resume screening algorithm starts suffering. (3)\nGeneral-purpose gender debiasing methods for NLP models such as removing gender\nsubspace from embeddings are not effective in obfuscating gender.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parasurama_P/0/1/0/all/0/1\">Prasanna Parasurama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Segmentation Optimization using Segmented Bilingual Speech Corpus for End-to-end Speech Translation. (arXiv:2203.15479v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.15479","description":"<p>Speech segmentation, which splits long speech into short segments, is\nessential for speech translation (ST). Popular VAD tools like WebRTC VAD have\ngenerally relied on pause-based segmentation. Unfortunately, pauses in speech\ndo not necessarily match sentence boundaries, and sentences can be connected by\na very short pause that is difficult to detect by VAD. In this study, we\npropose a speech segmentation method using a binary classification model\ntrained using a segmented bilingual speech corpus. We also propose a hybrid\nmethod that combines VAD and the above speech segmentation method. Experimental\nresults revealed that the proposed method is more suitable for cascade and\nend-to-end ST systems than conventional segmentation methods. The hybrid\napproach further improved the translation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fukuda_R/0/1/0/all/0/1\">Ryo Fukuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudoh_K/0/1/0/all/0/1\">Katsuhito Sudoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedNST: Federated Noisy Student Training for Automatic Speech Recognition. (arXiv:2206.02797v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2206.02797","description":"<p>Federated Learning (FL) enables training state-of-the-art Automatic Speech\nRecognition (ASR) models on user devices (clients) in distributed systems,\nhence preventing transmission of raw user data to a central server. A key\nchallenge facing practical adoption of FL for ASR is obtaining ground-truth\nlabels on the clients. Existing approaches rely on clients to manually\ntranscribe their speech, which is impractical for obtaining large training\ncorpora. A promising alternative is using semi-/self-supervised learning\napproaches to leverage unlabelled user data. To this end, we propose FedNST, a\nnovel method for training distributed ASR models using private and unlabelled\nuser data. We explore various facets of FedNST, such as training models with\ndifferent proportions of labelled and unlabelled data, and evaluate the\nproposed approach on 1173 simulated clients. Evaluating FedNST on LibriSpeech,\nwhere 960 hours of speech data is split equally into server (labelled) and\nclient (unlabelled) data, showed a 22.5% relative word error rate reduction}\n(WERR) over a supervised baseline trained only on server data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mehmood_H/0/1/0/all/0/1\">Haaris Mehmood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dobrowolska_A/0/1/0/all/0/1\">Agnieszka Dobrowolska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saravanan_K/0/1/0/all/0/1\">Karthikeyan Saravanan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozay_M/0/1/0/all/0/1\">Mete Ozay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why Robust Natural Language Understanding is a Challenge. (arXiv:2206.14575v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.14575","description":"<p>With the proliferation of Deep Machine Learning into real-life applications,\na particular property of this technology has been brought to attention:\nrobustness Neural Networks notoriously present low robustness and can be highly\nsensitive to small input perturbations. Recently, many methods for verifying\nnetworks' general properties of robustness have been proposed, but they are\nmostly applied in Computer Vision. In this paper we propose a Verification\nspecification for Natural Language Understanding classification based on larger\nregions of interest, and we discuss the challenges of such task. We observe\nthat, although the data is almost linearly separable, the verifier struggles to\noutput positive results and we explain the problems and implications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casadio_M/0/1/0/all/0/1\">Marco Casadio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komendantskaya_E/0/1/0/all/0/1\">Ekaterina Komendantskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1\">Verena Rieser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daggitt_M/0/1/0/all/0/1\">Matthew L. Daggitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kienitz_D/0/1/0/all/0/1\">Daniel Kienitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnaboldi_L/0/1/0/all/0/1\">Luca Arnaboldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokke_W/0/1/0/all/0/1\">Wen Kokke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code Translation with Compiler Representations. (arXiv:2207.03578v2 [cs.PL] UPDATED)","link":"http://arxiv.org/abs/2207.03578","description":"<p>In this paper, we leverage low-level compiler intermediate representations\n(IR) to improve code translation. Traditional transpilers rely on syntactic\ninformation and handcrafted rules, which limits their applicability and\nproduces unnatural-looking code. Applying neural machine translation (NMT)\napproaches to code has successfully broadened the set of programs on which one\ncan get a natural-looking translation. However, they treat the code as\nsequences of text tokens, and still do not differentiate well enough between\nsimilar pieces of code which have different semantics in different languages.\nThe consequence is low quality translation, reducing the practicality of NMT,\nand stressing the need for an approach significantly increasing its accuracy.\nHere we propose to augment code translation with IRs, specifically LLVM IR,\nwith results on the C++, Java, Rust, and Go languages. Our method improves upon\nthe state of the art for unsupervised code translation, increasing the number\nof correct translations by 11% on average, and up to 79% for the Java - Rust\npair. We extend previous test sets for code translation, by adding hundreds of\nGo and Rust functions. Additionally, we train models with high performance on\nthe problem of IR decompilation, generating programming source code from IR,\nand study using IRs as intermediary pivot for translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Szafraniec_M/0/1/0/all/0/1\">Marc Szafraniec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roziere_B/0/1/0/all/0/1\">Baptiste Roziere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leather_H/0/1/0/all/0/1\">Hugh Leather</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charton_F/0/1/0/all/0/1\">Francois Charton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labatut_P/0/1/0/all/0/1\">Patrick Labatut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastLTS: Non-Autoregressive End-to-End Unconstrained Lip-to-Speech Synthesis. (arXiv:2207.03800v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2207.03800","description":"<p>Unconstrained lip-to-speech synthesis aims to generate corresponding speeches\nfrom silent videos of talking faces with no restriction on head poses or\nvocabulary. Current works mainly use sequence-to-sequence models to solve this\nproblem, either in an autoregressive architecture or a flow-based\nnon-autoregressive architecture. However, these models suffer from several\ndrawbacks: 1) Instead of directly generating audios, they use a two-stage\npipeline that first generates mel-spectrograms and then reconstructs audios\nfrom the spectrograms. This causes cumbersome deployment and degradation of\nspeech quality due to error propagation; 2) The audio reconstruction algorithm\nused by these models limits the inference speed and audio quality, while neural\nvocoders are not available for these models since their output spectrograms are\nnot accurate enough; 3) The autoregressive model suffers from high inference\nlatency, while the flow-based model has high memory occupancy: neither of them\nis efficient enough in both time and memory usage. To tackle these problems, we\npropose FastLTS, a non-autoregressive end-to-end model which can directly\nsynthesize high-quality speech audios from unconstrained talking videos with\nlow latency, and has a relatively small model size. Besides, different from the\nwidely used 3D-CNN visual frontend for lip movement encoding, we for the first\ntime propose a transformer-based visual frontend for this task. Experiments\nshow that our model achieves $19.76\\times$ speedup for audio waveform\ngeneration compared with the current autoregressive model on input sequences of\n3 seconds, and obtains superior audio quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A description of Turkish Discourse Bank 1.2 and an examination of common dependencies in Turkish discourse. (arXiv:2207.05008v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.05008","description":"<p>We describe Turkish Discourse Bank 1.2, the latest version of a discourse\ncorpus annotated for explicitly or implicitly conveyed discourse relations,\ntheir constitutive units, and senses in the Penn Discourse Treebank style. We\npresent an evaluation of the recently added tokens and examine three commonly\noccurring dependency patterns that hold among the constitutive units of a pair\nof adjacent discourse relations, namely, shared arguments, full embedding and\npartial containment of a discourse relation. We present three major findings:\n(a) implicitly conveyed relations occur more often than explicitly conveyed\nrelations in the data; (b) it is much more common for two adjacent implicit\ndiscourse relations to share an argument than for two adjacent explicit\nrelations to do so; (c) both full embedding and partial containment of\ndiscourse relations are pervasive in the corpus, which can be partly due to\nsubordinator connectives whose preposed subordinate clause tends to be selected\ntogether with the matrix clause rather than being selected alone. Finally, we\nbriefly discuss the implications of our findings for Turkish discourse parsing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeyrek_D/0/1/0/all/0/1\">Deniz Zeyrek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Er_M/0/1/0/all/0/1\">Mustafa Erolcan Er</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Towards Highly Expressive Machine Learning Models of Non-Melanoma Skin Cancer. (arXiv:2207.05749v1 [cs.LG])","link":"http://arxiv.org/abs/2207.05749","description":"<p>Pathologists have a rich vocabulary with which they can describe all the\nnuances of cellular morphology. In their world, there is a natural pairing of\nimages and words. Recent advances demonstrate that machine learning models can\nnow be trained to learn high-quality image features and represent them as\ndiscrete units of information. This enables natural language, which is also\ndiscrete, to be jointly modelled alongside the imaging, resulting in a\ndescription of the contents of the imaging. Here we present experiments in\napplying discrete modelling techniques to the problem domain of non-melanoma\nskin cancer, specifically, histological images of Intraepidermal Carcinoma\n(IEC). Implementing a VQ-GAN model to reconstruct high-resolution (256x256)\nimages of IEC images, we trained a sequence-to-sequence transformer to generate\nnatural language descriptions using pathologist terminology. Combined with the\nidea of interactive concept vectors available by using continuous generative\nmethods, we demonstrate an additional angle of interpretability. The result is\na promising means of working towards highly expressive machine learning systems\nwhich are not only useful as predictive/classification tools, but also means to\nfurther our scientific understanding of disease.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Simon M. Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lefevre_J/0/1/0/all/0/1\">James G. Lefevre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baxter_G/0/1/0/all/0/1\">Glenn Baxter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_N/0/1/0/all/0/1\">Nicholas A.Hamilton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust and efficient computation of retinal fractal dimension through deep approximation. (arXiv:2207.05757v1 [q-bio.QM])","link":"http://arxiv.org/abs/2207.05757","description":"<p>A retinal trait, or phenotype, summarises a specific aspect of a retinal\nimage in a single number. This can then be used for further analyses, e.g. with\nstatistical methods. However, reducing an aspect of a complex image to a\nsingle, meaningful number is challenging. Thus, methods for calculating retinal\ntraits tend to be complex, multi-step pipelines that can only be applied to\nhigh quality images. This means that researchers often have to discard\nsubstantial portions of the available data. We hypothesise that such pipelines\ncan be approximated with a single, simpler step that can be made robust to\ncommon quality issues. We propose Deep Approximation of Retinal Traits (DART)\nwhere a deep neural network is used predict the output of an existing pipeline\non high quality images from synthetically degraded versions of these images. We\ndemonstrate DART on retinal Fractal Dimension (FD) calculated by VAMPIRE, using\nretinal images from UK Biobank that previous work identified as high quality.\nOur method shows very high agreement with FD VAMPIRE on unseen test images\n(Pearson r=0.9572). Even when those images are severely degraded, DART can\nstill recover an FD estimate that shows good agreement with FD VAMPIRE obtained\nfrom the original images (Pearson r=0.8817). This suggests that our method\ncould enable researchers to discard fewer images in the future. Our method can\ncompute FD for over 1,000img/s using a single GPU. We consider these to be very\nencouraging initial results and hope to develop this approach into a useful\ntool for retinal analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Engelmann_J/0/1/0/all/0/1\">Justin Engelmann</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Villaplana_Velasco_A/0/1/0/all/0/1\">Ana Villaplana-Velasco</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Storkey_A/0/1/0/all/0/1\">Amos Storkey</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bernabeu_M/0/1/0/all/0/1\">Miguel O. Bernabeu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Gap Estimation for Source Free Unsupervised Domain Adaptation with Many Classifiers. (arXiv:2207.05785v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05785","description":"<p>In theory, the success of unsupervised domain adaptation (UDA) largely relies\non domain gap estimation. However, for source free UDA, the source domain data\ncan not be accessed during adaptation, which poses great challenge of measuring\nthe domain gap. In this paper, we propose to use many classifiers to learn the\nsource domain decision boundaries, which provides a tighter upper bound of the\ndomain gap, even if both of the domain data can not be simultaneously accessed.\nThe source model is trained to push away each pair of classifiers whilst\nensuring the correctness of the decision boundaries. In this sense, our many\nclassifiers model separates the source different categories as far as possible\nwhich induces the maximum disagreement of many classifiers in the target\ndomain, thus the transferable source domain knowledge is maximized. For\nadaptation, the source model is adapted to maximize the agreement among pairs\nof the classifiers. Thus the target features are pushed away from the decision\nboundaries. Experiments on several datasets of UDA show that our approach\nachieves state of the art performance among source free UDA approaches and can\neven compete to source available UDA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1\">Ziyang Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huan_H/0/1/0/all/0/1\">Hai Huan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shape-Aware Masking for Inpainting in Medical Imaging. (arXiv:2207.05787v1 [eess.IV])","link":"http://arxiv.org/abs/2207.05787","description":"<p>Inpainting has recently been proposed as a successful deep learning technique\nfor unsupervised medical image model discovery. The masks used for inpainting\nare generally independent of the dataset and are not tailored to perform on\ndifferent given classes of anatomy. In this work, we introduce a method for\ngenerating shape-aware masks for inpainting, which aims at learning the\nstatistical shape prior. We hypothesize that although the variation of masks\nimproves the generalizability of inpainting models, the shape of the masks\nshould follow the topology of the organs of interest. Hence, we propose an\nunsupervised guided masking approach based on an off-the-shelf inpainting model\nand a superpixel over-segmentation algorithm to generate a wide range of\nshape-dependent masks. Experimental results on abdominal MR image\nreconstruction show the superiority of our proposed masking method over\nstandard methods using square-shaped or dataset of irregular shape masks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yeganeh_Y/0/1/0/all/0/1\">Yousef Yeganeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Farshad_A/0/1/0/all/0/1\">Azade Farshad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dam reservoir extraction from remote sensing imagery using tailored metric learning strategies. (arXiv:2207.05807v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05807","description":"<p>Dam reservoirs play an important role in meeting sustainable development\ngoals and global climate targets. However, particularly for small dam\nreservoirs, there is a lack of consistent data on their geographical location.\nTo address this data gap, a promising approach is to perform automated dam\nreservoir extraction based on globally available remote sensing imagery. It can\nbe considered as a fine-grained task of water body extraction, which involves\nextracting water areas in images and then separating dam reservoirs from\nnatural water bodies. We propose a novel deep neural network (DNN) based\npipeline that decomposes dam reservoir extraction into water body segmentation\nand dam reservoir recognition. Water bodies are firstly separated from\nbackground lands in a segmentation model and each individual water body is then\npredicted as either dam reservoir or natural water body in a classification\nmodel. For the former step, point-level metric learning with triplets across\nimages is injected into the segmentation model to address contour ambiguities\nbetween water areas and land regions. For the latter step, prior-guided metric\nlearning with triplets from clusters is injected into the classification model\nto optimize the image embedding space in a fine-grained level based on\nreservoir clusters. To facilitate future research, we establish a benchmark\ndataset with earth imagery data and human labelled reservoirs from river basins\nin West Africa and India. Extensive experiments were conducted on this\nbenchmark in the water body segmentation task, dam reservoir recognition task,\nand the joint dam reservoir extraction task. Superior performance has been\nobserved in the respective tasks when comparing our method with state of the\nart approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soesbergen_A/0/1/0/all/0/1\">Arnout van Soesbergen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1\">Zedong Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Miaojing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulligan_M/0/1/0/all/0/1\">Mark Mulligan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Look-ups are not (yet) all you need for deep learning inference. (arXiv:2207.05808v1 [cs.LG])","link":"http://arxiv.org/abs/2207.05808","description":"<p>Fast approximations to matrix multiplication have the potential to\ndramatically reduce the cost of neural network inference. Recent work on\napproximate matrix multiplication proposed to replace costly multiplications\nwith table-lookups by fitting a fast hash function from training data. In this\nwork, we propose improvements to this previous work, targeted to the deep\nlearning inference setting, where one has access to both training data and\nfixed (already learned) model weight matrices. We further propose a fine-tuning\nprocedure for accelerating entire neural networks while minimizing loss in\naccuracy. Finally, we analyze the proposed method on a simple image\nclassification task. While we show improvements to prior work, overall\nclassification accuracy remains substantially diminished compared to exact\nmatrix multiplication. Our work, despite this negative result, points the way\ntowards future efforts to accelerate inner products with fast nonlinear hashing\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McCarter_C/0/1/0/all/0/1\">Calvin McCarter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dronen_N/0/1/0/all/0/1\">Nicholas Dronen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Earthformer: Exploring Space-Time Transformers for Earth System Forecasting. (arXiv:2207.05833v1 [cs.LG])","link":"http://arxiv.org/abs/2207.05833","description":"<p>Conventionally, Earth system (e.g., weather and climate) forecasting relies\non numerical simulation with complex physical models and are hence both\nexpensive in computation and demanding on domain expertise. With the explosive\ngrowth of the spatiotemporal Earth observation data in the past decade,\ndata-driven models that apply Deep Learning (DL) are demonstrating impressive\npotential for various Earth system forecasting tasks. The Transformer as an\nemerging DL architecture, despite its broad success in other domains, has\nlimited adoption in this area. In this paper, we propose Earthformer, a\nspace-time Transformer for Earth system forecasting. Earthformer is based on a\ngeneric, flexible and efficient space-time attention block, named Cuboid\nAttention. The idea is to decompose the data into cuboids and apply\ncuboid-level self-attention in parallel. These cuboids are further connected\nwith a collection of global vectors. We conduct experiments on the MovingMNIST\ndataset and a newly proposed chaotic N-body MNIST dataset to verify the\neffectiveness of cuboid attention and figure out the best design of\nEarthformer. Experiments on two real-world benchmarks about precipitation\nnowcasting and El Nino/Southern Oscillation (ENSO) forecasting show Earthformer\nachieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhihan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xingjian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1\">Dit-Yan Yeung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REZCR: A Zero-shot Character Recognition Method via Radical Extraction. (arXiv:2207.05842v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05842","description":"<p>The long-tail effect is a common issue that limits the performance of deep\nlearning models on real-world datasets. Character image dataset development is\nalso affected by such unbalanced data distribution due to differences in\ncharacter usage frequency. Thus, current character recognition methods are\nlimited when applying to real-world datasets, in particular to the character\ncategories in the tail which are lacking training samples, e.g., uncommon\ncharacters, or characters from historical documents. In this paper, we propose\na zero-shot character recognition framework via radical extraction, i.e.,\nREZCR, to improve the recognition performance of few-sample character\ncategories, in which we exploit information on radicals, the graphical units of\ncharacters, by decomposing and reconstructing characters following orthography.\nREZCR consists of an attention-based radical information extractor (RIE) and a\nknowledge graph-based character reasoner (KGR). The RIE aims to recognize\ncandidate radicals and their possible structural relations from character\nimages. The results will be fed into KGR to recognize the target character by\nreasoning with a pre-designed character knowledge graph. We validate our method\non multiple datasets, REZCR shows promising experimental results, especially\nfor few-sample character datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diao_X/0/1/0/all/0/1\">Xiaolei Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Daqian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanzeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wayformer: Motion Forecasting via Simple & Efficient Attention Networks. (arXiv:2207.05844v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05844","description":"<p>Motion forecasting for autonomous driving is a challenging task because\ncomplex driving scenarios result in a heterogeneous mix of static and dynamic\ninputs. It is an open problem how best to represent and fuse information about\nroad geometry, lane connectivity, time-varying traffic light state, and history\nof a dynamic set of agents and their interactions into an effective encoding.\nTo model this diverse set of input features, many approaches proposed to design\nan equally complex system with a diverse set of modality specific modules. This\nresults in systems that are difficult to scale, extend, or tune in rigorous\nways to trade off quality and efficiency. In this paper, we present Wayformer,\na family of attention based architectures for motion forecasting that are\nsimple and homogeneous. Wayformer offers a compact model description consisting\nof an attention based scene encoder and a decoder. In the scene encoder we\nstudy the choice of early, late and hierarchical fusion of the input\nmodalities. For each fusion type we explore strategies to tradeoff efficiency\nand quality via factorized attention or latent query attention. We show that\nearly fusion, despite its simplicity of construction, is not only modality\nagnostic but also achieves state-of-the-art results on both Waymo Open\nMotionDataset (WOMD) and Argoverse leaderboards, demonstrating the\neffectiveness of our design philosophy\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayakanti_N/0/1/0/all/0/1\">Nigamaa Nayakanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Rfou_R/0/1/0/all/0/1\">Rami Al-Rfou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aurick Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_K/0/1/0/all/0/1\">Kratarth Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Refaat_K/0/1/0/all/0/1\">Khaled S. Refaat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapp_B/0/1/0/all/0/1\">Benjamin Sapp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Estimate External Forces of Human Motion in Video. (arXiv:2207.05845v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05845","description":"<p>Analyzing sports performance or preventing injuries requires capturing ground\nreaction forces (GRFs) exerted by the human body during certain movements.\nStandard practice uses physical markers paired with force plates in a\ncontrolled environment, but this is marred by high costs, lengthy\nimplementation time, and variance in repeat experiments; hence, we propose GRF\ninference from video. While recent work has used LSTMs to estimate GRFs from 2D\nviewpoints, these can be limited in their modeling and representation capacity.\nFirst, we propose using a transformer architecture to tackle the GRF from video\ntask, being the first to do so. Then we introduce a new loss to minimize high\nimpact peaks in regressed curves. We also show that pre-training and multi-task\nlearning on 2D-to-3D human pose estimation improves generalization to unseen\nmotions. And pre-training on this different task provides good initial weights\nwhen finetuning on smaller (rarer) GRF datasets. We evaluate on LAAS Parkour\nand a newly collected ForcePose dataset; we show up to 19% decrease in error\ncompared to prior approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Louis_N/0/1/0/all/0/1\">Nathan Louis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Templin_T/0/1/0/all/0/1\">Tylan N. Templin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eliason_T/0/1/0/all/0/1\">Travis D. Eliason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicolella_D/0/1/0/all/0/1\">Daniel P. Nicolella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corso_J/0/1/0/all/0/1\">Jason J. Corso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpOT: Spatiotemporal Modeling for 3D Object Tracking. (arXiv:2207.05856v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05856","description":"<p>3D multi-object tracking aims to uniquely and consistently identify all\nmobile entities through time. Despite the rich spatiotemporal information\navailable in this setting, current 3D tracking methods primarily rely on\nabstracted information and limited history, e.g. single-frame object bounding\nboxes. In this work, we develop a holistic representation of traffic scenes\nthat leverages both spatial and temporal information of the actors in the\nscene. Specifically, we reformulate tracking as a spatiotemporal problem by\nrepresenting tracked objects as sequences of time-stamped points and bounding\nboxes over a long temporal history. At each timestamp, we improve the location\nand motion estimates of our tracked objects through learned refinement over the\nfull sequence of object history. By considering time and space jointly, our\nrepresentation naturally encodes fundamental physical priors such as object\npermanence and consistency across time. Our spatiotemporal tracking framework\nachieves state-of-the-art performance on the Waymo and nuScenes benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stearns_C/0/1/0/all/0/1\">Colton Stearns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rempe_D/0/1/0/all/0/1\">Davis Rempe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1\">Rares Ambrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakharov_S/0/1/0/all/0/1\">Sergey Zakharov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guizilini_V/0/1/0/all/0/1\">Vitor Guizilini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J Guibas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Diffusion Priors for Accelerated MRI Reconstruction. (arXiv:2207.05876v1 [eess.IV])","link":"http://arxiv.org/abs/2207.05876","description":"<p>Deep MRI reconstruction is commonly performed with conditional models that\nmap undersampled data as input onto fully-sampled data as output. Conditional\nmodels perform de-aliasing under knowledge of the accelerated imaging operator,\nso they poorly generalize under domain shifts in the operator. Unconditional\nmodels are a powerful alternative that instead learn generative image priors to\nimprove reliability against domain shifts. Recent diffusion models are\nparticularly promising given their high representational diversity and sample\nquality. Nevertheless, projections through a static image prior can lead to\nsuboptimal performance. Here we propose a novel MRI reconstruction, AdaDiff,\nbased on an adaptive diffusion prior. To enable efficient image sampling, an\nadversarial mapper is introduced that enables use of large diffusion steps. A\ntwo-phase reconstruction is performed with the trained prior: a rapid-diffusion\nphase that produces an initial reconstruction, and an adaptation phase where\nthe diffusion prior is updated to minimize reconstruction loss on acquired\nk-space data. Demonstrations on multi-contrast brain MRI clearly indicate that\nAdaDiff achieves superior performance to competing models in cross-domain\ntasks, and superior or on par performance in within-domain tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dar_S/0/1/0/all/0/1\">Salman UH Dar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozturk_S/0/1/0/all/0/1\">&#x15e;aban &#xd6;zt&#xfc;rk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korkmaz_Y/0/1/0/all/0/1\">Yilmaz Korkmaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elmas_G/0/1/0/all/0/1\">Gokberk Elmas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozbey_M/0/1/0/all/0/1\">Muzaffer &#xd6;zbey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gungor_A/0/1/0/all/0/1\">Alper G&#xfc;ng&#xf6;r</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1\">Tolga &#xc7;ukur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Near Sensor Edge Computing System for Point Cloud Semantic Segmentation. (arXiv:2207.05888v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05888","description":"<p>Point cloud semantic segmentation has attracted attentions due to its\nrobustness to light condition. This makes it an ideal semantic solution for\nautonomous driving. However, considering the large computation burden and\nbandwidth demanding of neural networks, putting all the computing into vehicle\nElectronic Control Unit (ECU) is not efficient or practical. In this paper, we\nproposed a light weighted point cloud semantic segmentation network based on\nrange view. Due to its simple pre-processing and standard convolution, it is\nefficient when running on deep learning accelerator like DPU. Furthermore, a\nnear sensor computing system is built for autonomous vehicles. In this system,\na FPGA-based deep learning accelerator core (DPU) is placed next to the LiDAR\nsensor, to perform point cloud pre-processing and segmentation neural network.\nBy leaving only the post-processing step to ECU, this solution heavily\nalleviate the computation burden of ECU and consequently shortens the decision\nmaking and vehicles reaction latency. Our semantic segmentation network\nachieved 10 frame per second (fps) on Xilinx DPU with computation efficiency\n42.5 GOP/W.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinming Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Spatial-Temporal Entropy Modelling for Neural Video Compression. (arXiv:2207.05894v1 [eess.IV])","link":"http://arxiv.org/abs/2207.05894","description":"<p>For neural video codec, it is critical, yet challenging, to design an\nefficient entropy model which can accurately predict the probability\ndistribution of the quantized latent representation. However, most existing\nvideo codecs directly use the ready-made entropy model from image codec to\nencode the residual or motion, and do not fully leverage the spatial-temporal\ncharacteristics in video. To this end, this paper proposes a powerful entropy\nmodel which efficiently captures both spatial and temporal dependencies. In\nparticular, we introduce the latent prior which exploits the correlation among\nthe latent representation to squeeze the temporal redundancy. Meanwhile, the\ndual spatial prior is proposed to reduce the spatial redundancy in a\nparallel-friendly manner. In addition, our entropy model is also versatile.\nBesides estimating the probability distribution, our entropy model also\ngenerates the quantization step at spatial-channel-wise. This content-adaptive\nquantization mechanism not only helps our codec achieve the smooth rate\nadjustment in single model but also improves the final rate-distortion\nperformance by dynamic bit allocation. Experimental results show that, powered\nby the proposed entropy model, our neural codec can achieve 18.2% bitrate\nsaving on UVG dataset when compared with H.266 (VTM) using the highest\ncompression ratio configuration. It makes a new milestone in the development of\nneural video codec. The codes are at https://github.com/microsoft/DCVC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiahao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D-CBRS: Accounting For Intra-Class Diversity in Continual Learning. (arXiv:2207.05897v1 [cs.LG])","link":"http://arxiv.org/abs/2207.05897","description":"<p>Continual learning -- accumulating knowledge from a sequence of learning\nexperiences -- is an important yet challenging problem. In this paradigm, the\nmodel's performance for previously encountered instances may substantially drop\nas additional data are seen. When dealing with class-imbalanced data,\nforgetting is further exacerbated. Prior work has proposed replay-based\napproaches which aim at reducing forgetting by intelligently storing instances\nfor future replay. Although Class-Balancing Reservoir Sampling (CBRS) has been\nsuccessful in dealing with imbalanced data, the intra-class diversity has not\nbeen accounted for, implicitly assuming that each instance of a class is\nequally informative. We present Diverse-CBRS (D-CBRS), an algorithm that allows\nus to consider within class diversity when storing instances in the memory. Our\nresults show that D-CBRS outperforms state-of-the-art memory management\ncontinual learning algorithms on data sets with considerable intra-class\ndiversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Findik_Y/0/1/0/all/0/1\">Yasin Findik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pourkamali_Anaraki_F/0/1/0/all/0/1\">Farhad Pourkamali-Anaraki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Verifying Attention Robustness of Deep Neural Networks against Semantic Perturbations. (arXiv:2207.05902v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05902","description":"<p>It is known that deep neural networks (DNNs) classify an input image by\npaying particular attention to certain specific pixels; a graphical\nrepresentation of the magnitude of attention to each pixel is called a\nsaliency-map. Saliency-maps are used to check the validity of the\nclassification decision basis, e.g., it is not a valid basis for classification\nif a DNN pays more attention to the background rather than the subject of an\nimage. Semantic perturbations can significantly change the saliency-map. In\nthis work, we propose the first verification method for attention robustness,\ni.e., the local robustness of the changes in the saliency-map against\ncombinations of semantic perturbations. Specifically, our method determines the\nrange of the perturbation parameters (e.g., the brightness change) that\nmaintains the difference between the actual saliency-map change and the\nexpected saliency-map change below a given threshold value. Our method is based\non activation region traversals, focusing on the outermost robust boundary for\nscalability on larger DNNs. Experimental results demonstrate that our method\ncan show the extent to which DNNs can classify with the same basis regardless\nof semantic perturbations and report on performance and performance factors of\nactivation region traversals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Munakata_S/0/1/0/all/0/1\">Satoshi Munakata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urban_C/0/1/0/all/0/1\">Caterina Urban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoyama_H/0/1/0/all/0/1\">Haruki Yokoyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_K/0/1/0/all/0/1\">Koji Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munakata_K/0/1/0/all/0/1\">Kazuki Munakata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Dance Synthesis via Keyframes with Transformer Controllers. (arXiv:2207.05906v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05906","description":"<p>Existing keyframe-based motion synthesis mainly focuses on the generation of\ncyclic actions or short-term motion, such as walking, running, and transitions\nbetween close postures. However, these methods will significantly degrade the\nnaturalness and diversity of the synthesized motion when dealing with complex\nand impromptu movements, e.g., dance performance and martial arts. In addition,\ncurrent research lacks fine-grained control over the generated motion, which is\nessential for intelligent human-computer interaction and animation creation. In\nthis paper, we propose a novel keyframe-based motion generation network based\non multiple constraints, which can achieve diverse dance synthesis via learned\nknowledge. Specifically, the algorithm is mainly formulated based on the\nrecurrent neural network (RNN) and the Transformer architecture. The backbone\nof our network is a hierarchical RNN module composed of two long short-term\nmemory (LSTM) units, in which the first LSTM is utilized to embed the posture\ninformation of the historical frames into a latent space, and the second one is\nemployed to predict the human posture for the next frame. Moreover, our\nframework contains two Transformer-based controllers, which are used to model\nthe constraints of the root trajectory and the velocity factor respectively, so\nas to better utilize the temporal context of the frames and achieve\nfine-grained motion control. We verify the proposed approach on a dance dataset\ncontaining a wide range of contemporary dance. The results of three\nquantitative analyses validate the superiority of our algorithm. The video and\nqualitative experimental results demonstrate that the complex motion sequences\ngenerated by our algorithm can achieve diverse and smooth motion transitions\nbetween keyframes, even for long-term synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junjun Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Junxuan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Ju Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Appearance-guided Attentive Self-Paced Learning for Unsupervised Salient Object Detection. (arXiv:2207.05921v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05921","description":"<p>Existing Deep-Learning-based (DL-based) Unsupervised Salient Object Detection\n(USOD) methods learn saliency information in images based on the prior\nknowledge of traditional saliency methods and pretrained deep networks.\nHowever, these methods employ a simple learning strategy to train deep networks\nand therefore cannot properly incorporate the \"hidden\" information of the\ntraining samples into the learning process. Moreover, appearance information,\nwhich is crucial for segmenting objects, is only used as post-process after the\nnetwork training process. To address these two issues, we propose a novel\nappearance-guided attentive self-paced learning framework for unsupervised\nsalient object detection. The proposed framework integrates both self-paced\nlearning (SPL) and appearance guidance into a unified learning framework.\nSpecifically, for the first issue, we propose an Attentive Self-Paced Learning\n(ASPL) paradigm that organizes the training samples in a meaningful order to\nexcavate gradually more detailed saliency information. Our ASPL facilitates our\nframework capable of automatically producing soft attention weights that\nmeasure the learning difficulty of training samples in a purely self-learning\nway. For the second issue, we propose an Appearance Guidance Module (AGM),\nwhich formulates the local appearance contrast of each pixel as the probability\nof saliency boundary and finds the potential boundary of the target objects by\nmaximizing the probability. Furthermore, we further extend our framework to\nother multi-modality SOD tasks by aggregating the appearance vectors of other\nmodality data, such as depth map, thermal image or optical flow. Extensive\nexperiments on RGB, RGB-D, RGB-T and video SOD benchmarks prove that our\nframework achieves state-of-the-art performance against existing USOD methods\nand is comparable to the latest supervised SOD methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huajun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_B/0/1/0/all/0/1\">Bo Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lingxiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Jianhuang Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaohua Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rapid Person Re-Identification via Sub-space Consistency Regularization. (arXiv:2207.05933v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05933","description":"<p>Person Re-Identification (ReID) matches pedestrians across disjoint cameras.\nExisting ReID methods adopting real-value feature descriptors have achieved\nhigh accuracy, but they are low in efficiency due to the slow Euclidean\ndistance computation as well as complex quick-sort algorithms. Recently, some\nworks propose to yield binary encoded person descriptors which instead only\nrequire fast Hamming distance computation and simple counting-sort algorithms.\nHowever, the performances of such binary encoded descriptors, especially with\nshort code (e.g., 32 and 64 bits), are hardly satisfactory given the sparse\nbinary space. To strike a balance between the model accuracy and efficiency, we\npropose a novel Sub-space Consistency Regularization (SCR) algorithm that can\nspeed up the ReID procedure by $0.25$ times than real-value features under the\nsame dimensions whilst maintaining a competitive accuracy, especially under\nshort codes. SCR transforms real-value features vector (e.g., 2048 float32)\nwith short binary codes (e.g., 64 bits) by first dividing real-value features\nvector into $M$ sub-spaces, each with $C$ clustered centroids. Thus the\ndistance between two samples can be expressed as the summation of the\nrespective distance to the centroids, which can be sped up by offline\ncalculation and maintained via a look-up table. On the other side, these\nreal-value centroids help to achieve significantly higher accuracy than using\nbinary code. Lastly, we convert the distance look-up table to be integer and\napply the counting-sort algorithm to speed up the ranking stage.\n</p>\n<p>We also propose a novel consistency regularization with an iterative\nframework. Experimental results on Market-1501 and DukeMTMC-reID show promising\nand exciting results. Under short code, our proposed SCR enjoys\nReal-value-level accuracy and Hashing-level speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1\">Qingze Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guodong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qilei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shaogang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhenmin Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prediction of the motion of chest internal points using a recurrent neural network trained with real-time recurrent learning for latency compensation in lung cancer radiotherapy. (arXiv:2207.05951v1 [eess.IV])","link":"http://arxiv.org/abs/2207.05951","description":"<p>During the radiotherapy treatment of patients with lung cancer, the radiation\ndelivered to healthy tissue around the tumor needs to be minimized, which is\ndifficult because of respiratory motion and the latency of linear accelerator\nsystems. In the proposed study, we first use the Lucas-Kanade pyramidal optical\nflow algorithm to perform deformable image registration of chest computed\ntomography scan images of four patients with lung cancer. We then track three\ninternal points close to the lung tumor based on the previously computed\ndeformation field and predict their position with a recurrent neural network\n(RNN) trained using real-time recurrent learning (RTRL) and gradient clipping.\nThe breathing data is quite regular, sampled at approximately 2.5Hz, and\nincludes artificial drift in the spine direction. The amplitude of the motion\nof the tracked points ranged from 12.0mm to 22.7mm. Finally, we propose a\nsimple method for recovering and predicting 3D tumor images from the tracked\npoints and the initial tumor image based on a linear correspondence model and\nNadaraya-Watson non-linear regression. The root-mean-square error, maximum\nerror, and jitter corresponding to the RNN prediction on the test set were\nsmaller than the same performance measures obtained with linear prediction and\nleast mean squares (LMS). In particular, the maximum prediction error\nassociated with the RNN, equal to 1.51mm, is respectively 16.1% and 5.0% lower\nthan the maximum error associated with linear prediction and LMS. The average\nprediction time per time step with RTRL is equal to 119ms, which is less than\nthe 400ms marker position sampling time. The tumor position in the predicted\nimages appears visually correct, which is confirmed by the high mean\ncross-correlation between the original and predicted images, equal to 0.955.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pohl_M/0/1/0/all/0/1\">Michel Pohl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uesaka_M/0/1/0/all/0/1\">Mitsuru Uesaka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demachi_K/0/1/0/all/0/1\">Kazuyuki Demachi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chhatkuli_R/0/1/0/all/0/1\">Ritu Bhusal Chhatkuli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Orthogonal-Coding-Based Feature Generation for Transductive Open-Set Recognition via Dual-Space Consistent Sampling. (arXiv:2207.05957v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05957","description":"<p>Open-set recognition (OSR) aims to simultaneously detect unknown-class\nsamples and classify known-class samples. Most of the existing OSR methods are\ninductive methods, which generally suffer from the domain shift problem that\nthe learned model from the known-class domain might be unsuitable for the\nunknown-class domain. Addressing this problem, inspired by the success of\ntransductive learning for alleviating the domain shift problem in many other\nvisual tasks, we propose an Iterative Transductive OSR framework, called\nIT-OSR, which implements three explored modules iteratively, including a\nreliability sampling module, a feature generation module, and a baseline update\nmodule. Specifically, at each iteration, a dual-space consistent sampling\napproach is presented in the explored reliability sampling module for selecting\nsome relatively more reliable ones from the test samples according to their\npseudo labels assigned by a baseline method, which could be an arbitrary\ninductive OSR method. Then, a conditional dual-adversarial generative network\nunder an orthogonal coding condition is designed in the feature generation\nmodule to generate discriminative sample features of both known and unknown\nclasses according to the selected test samples with their pseudo labels.\nFinally, the baseline method is updated for sample re-prediction in the\nbaseline update module by jointly utilizing the generated features, the\nselected test samples with pseudo labels, and the training samples. Extensive\nexperimental results on both the standard-dataset and the cross-dataset\nsettings demonstrate that the derived transductive methods, by introducing two\ntypical inductive OSR methods into the proposed IT-OSR framework, achieve\nbetter performances than 15 state-of-the-art methods in most cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiayin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qiulei Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A new database of Houma Alliance Book ancient handwritten characters and its baseline algorithm. (arXiv:2207.05993v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05993","description":"<p>The Houma Alliance Book is one of the national treasures of the Museum in\nShanxi Museum Town in China. It has great historical significance in\nresearching ancient history. To date, the research on the Houma Alliance Book\nhas been staying in the identification of paper documents, which is inefficient\nto identify and difficult to display, study and publicize. Therefore, the\ndigitization of the recognized ancient characters of Houma League can\neffectively improve the efficiency of recognizing ancient characters and\nprovide more reliable technical support and text data. This paper proposes a\nnew database of Houma Alliance Book ancient handwritten characters and a\nmulti-modal fusion method to recognize ancient handwritten characters. In the\ndatabase, 297 classes and 3,547 samples of Houma Alliance ancient handwritten\ncharacters are collected from the original book collection and by human\nimitative writing. Furthermore, the decision-level classifier fusion strategy\nis applied to fuse three well-known deep neural network architectures for\nancient handwritten character recognition. Experiments are performed on our new\ndatabase. The experimental results first provide the baseline result of the new\ndatabase to the research community and then demonstrate the efficiency of our\nproposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaoyu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhibo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yabo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zekai Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_X/0/1/0/all/0/1\">Xiuyan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaohua Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Context-driven Audio Feature Enhancement for Robust End-to-End Audio-Visual Speech Recognition. (arXiv:2207.06020v1 [cs.SD])","link":"http://arxiv.org/abs/2207.06020","description":"<p>This paper focuses on designing a noise-robust end-to-end Audio-Visual Speech\nRecognition (AVSR) system. To this end, we propose Visual Context-driven Audio\nFeature Enhancement module (V-CAFE) to enhance the input noisy audio speech\nwith a help of audio-visual correspondence. The proposed V-CAFE is designed to\ncapture the transition of lip movements, namely visual context and to generate\na noise reduction mask by considering the obtained visual context. Through\ncontext-dependent modeling, the ambiguity in viseme-to-phoneme mapping can be\nrefined for mask generation. The noisy representations are masked out with the\nnoise reduction mask resulting in enhanced audio features. The enhanced audio\nfeatures are fused with the visual features and taken to an encoder-decoder\nmodel composed of Conformer and Transformer for speech recognition. We show the\nproposed end-to-end AVSR with the V-CAFE can further improve the\nnoise-robustness of AVSR. The effectiveness of the proposed method is evaluated\nin noisy speech recognition and overlapped speech recognition experiments using\nthe two largest audio-visual datasets, LRS2 and LRS3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Joanna Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_D/0/1/0/all/0/1\">Daehun Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1\">Yong Man Ro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perturbation Inactivation Based Adversarial Defense for Face Recognition. (arXiv:2207.06035v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06035","description":"<p>Deep learning-based face recognition models are vulnerable to adversarial\nattacks. To curb these attacks, most defense methods aim to improve the\nrobustness of recognition models against adversarial perturbations. However,\nthe generalization capacities of these methods are quite limited. In practice,\nthey are still vulnerable to unseen adversarial attacks. Deep learning models\nare fairly robust to general perturbations, such as Gaussian noises. A\nstraightforward approach is to inactivate the adversarial perturbations so that\nthey can be easily handled as general perturbations. In this paper, a\nplug-and-play adversarial defense method, named perturbation inactivation\n(PIN), is proposed to inactivate adversarial perturbations for adversarial\ndefense. We discover that the perturbations in different subspaces have\ndifferent influences on the recognition model. There should be a subspace,\ncalled the immune space, in which the perturbations have fewer adverse impacts\non the recognition model than in other subspaces. Hence, our method estimates\nthe immune space and inactivates the adversarial perturbations by restricting\nthem to this subspace. The proposed method can be generalized to unseen\nadversarial perturbations since it does not rely on a specific kind of\nadversarial attack method. This approach not only outperforms several\nstate-of-the-art adversarial defense methods but also demonstrates a superior\ngeneralization capacity through exhaustive experiments. Moreover, the proposed\nmethod can be successfully applied to four commercial APIs without additional\ntraining, indicating that it can be easily generalized to existing face\nrecognition systems. The source code is available at\nhttps://github.com/RenMin1991/Perturbation-Inactivate\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Min Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuhao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Experiments on Anomaly Detection in Autonomous Driving by Forward-Backward Style Transfers. (arXiv:2207.06055v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06055","description":"<p>Great progress has been achieved in the community of autonomous driving in\nthe past few years. As a safety-critical problem, however, anomaly detection is\na huge hurdle towards a large-scale deployment of autonomous vehicles in the\nreal world. While many approaches, such as uncertainty estimation or\nsegmentation-based image resynthesis, are extremely promising, there is more to\nbe explored. Especially inspired by works on anomaly detection based on image\nresynthesis, we propose a novel approach for anomaly detection through style\ntransfer. We leverage generative models to map an image from its original style\ndomain of road traffic to an arbitrary one and back to generate pixelwise\nanomaly scores. However, our experiments have proven our hypothesis wrong, and\nwe were unable to produce significant results. Nevertheless, we want to share\nour findings, so that others can learn from our experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bogdoll_D/0/1/0/all/0/1\">Daniel Bogdoll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nitsche_M/0/1/0/all/0/1\">Maximilian Nitsche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1\">J. Marius Z&#xf6;llner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure PLP-SLAM: Efficient Sparse Mapping and Localization using Point, Line and Plane for Monocular, RGB-D and Stereo Cameras. (arXiv:2207.06058v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06058","description":"<p>This paper demonstrates a visual SLAM system that utilizes point and line\ncloud for robust camera localization, simultaneously, with an embedded\npiece-wise planar reconstruction (PPR) module which in all provides a\nstructural map. To build a scale consistent map in parallel with tracking, such\nas employing a single camera brings the challenge of reconstructing geometric\nprimitives with scale ambiguity, and further introduces the difficulty in graph\noptimization of bundle adjustment (BA). We address these problems by proposing\nseveral run-time optimizations on the reconstructed lines and planes. The\nsystem is then extended with depth and stereo sensors based on the design of\nthe monocular framework. The results show that our proposed SLAM tightly\nincorporates the semantic features to boost both frontend tracking as well as\nbackend optimization. We evaluate our system exhaustively on various datasets,\nand open-source our code for the community\n(https://github.com/PeterFWS/Structure-PLP-SLAM).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_F/0/1/0/all/0/1\">Fangwen Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagani_A/0/1/0/all/0/1\">Alain Pagani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pyramid Transformer for Traffic Sign Detection. (arXiv:2207.06067v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06067","description":"<p>Traffic sign detection is a vital task in the visual system of self-driving\ncars and the automated driving system. Recently, novel Transformer-based models\nhave achieved encouraging results for various computer vision tasks. We still\nobserved that vanilla ViT could not yield satisfactory results in traffic sign\ndetection because the overall size of the datasets is very small and the class\ndistribution of traffic signs is extremely unbalanced. To overcome this\nproblem, a novel Pyramid Transformer with locality mechanisms is proposed in\nthis paper. Specifically, Pyramid Transformer has several spatial pyramid\nreduction layers to shrink and embed the input image into tokens with rich\nmulti-scale context by using atrous convolutions. Moreover, it inherits an\nintrinsic scale invariance inductive bias and is able to learn local feature\nrepresentation for objects at various scales, thereby enhancing the network\nrobustness against the size discrepancy of traffic signs. The experiments are\nconducted on the German Traffic Sign Detection Benchmark (GTSDB). The results\ndemonstrate the superiority of the proposed model in the traffic sign detection\ntasks. More specifically, Pyramid Transformer achieves 75.6% mAP in GTSDB when\napplied to the Cascade RCNN as the backbone and surpassing most well-known and\nwidely used SOTAs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manzari_O/0/1/0/all/0/1\">Omid Nejati Manzari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boudesh_A/0/1/0/all/0/1\">Amin Boudesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shokouhi_S/0/1/0/all/0/1\">Shahriar B. Shokouhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSPNet: Towards Slimmable Pretrained Networks based on Discriminative Self-supervised Learning. (arXiv:2207.06075v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06075","description":"<p>Self-supervised learning (SSL) has achieved promising downstream performance.\nHowever, when facing various resource budgets in real-world applications, it\ncosts a huge computation burden to pretrain multiple networks of various sizes\none by one. In this paper, we propose Discriminative-SSL-based Slimmable\nPretrained Networks (DSPNet), which can be trained at once and then slimmed to\nmultiple sub-networks of various sizes, each of which faithfully learns good\nrepresentation and can serve as good initialization for downstream tasks with\nvarious resource budgets. Specifically, we extend the idea of slimmable\nnetworks to a discriminative SSL paradigm, by integrating SSL and knowledge\ndistillation gracefully. We show comparable or improved performance of DSPNet\non ImageNet to the networks individually pretrained one by one under the linear\nevaluation and semi-supervised evaluation protocols, while reducing large\ntraining cost. The pretrained models also generalize well on downstream\ndetection and segmentation tasks. Code will be made public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaoru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiStream: A Simple and Fast Multiple Cameras Visual Monitor and Directly Streaming. (arXiv:2207.06078v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06078","description":"<p>Monitoring and streaming is one of the most important applications for the\nreal time cameras. The research of this has provided a novel design idea that\nuses the FFmpeg and Tkinter, combining with the libraries: OpenCV and PIL to\ndevelop a simple but fast streaming toolkit MultiSteam that can achieve the\nfunction of visible monitoring streaming for multiple simultaneously.\nMultiStream is able to automatically arrange the layout of the displays of\nmultiple camera windows and intelligently analyze the input streaming URL to\nselect the correct corresponding streaming communication protocol. Multiple\ncameras can be streamed with different communication protocols or the same\nprotocol. Besides, the paper has tested the different streaming speeds for\ndifferent protocols in camera streaming. MultiStream is able to gain the\ninformation of media equipment on the computer. The configuration information\nfor media-id selection and multiple cameras streaming can be saved as json\nfiles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jinwei Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teachers in concordance for pseudo-labeling of 3D sequential data. (arXiv:2207.06079v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06079","description":"<p>Automatic pseudo-labeling is a powerful tool to tap into large amounts of\nsequential unlabeled data. It is especially appealing in safety-critical\napplications of autonomous driving where performance requirements are extreme,\ndatasets large, and manual labeling is very challenging. We propose to leverage\nthe sequentiality of the captures to boost the pseudo-labeling technique in a\nteacher-student setup via training multiple teachers, each with access to\ndifferent temporal information. This set of teachers, dubbed Concordance,\nprovides higher quality pseudo-labels for the student training than standard\nmethods. The output of multiple teachers is combined via a novel pseudo-label\nconfidence-guided criterion. Our experimental evaluation focuses on the 3D\npoint cloud domain in urban driving scenarios. We show the performance of our\nmethod applied to multiple model architectures with tasks of 3D semantic\nsegmentation and 3D object detection on two benchmark datasets. Our method,\nusing only 20% of manual labels, outperforms some of the fully supervised\nmethods. Special performance boost is achieved for classes rarely appearing in\nthe training data, e.g., bicycles and pedestrians. The implementation of our\napproach is publicly available at https://github.com/ctu-vras/T-Concord3D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gebrehiwot_A/0/1/0/all/0/1\">Awet Haileslassie Gebrehiwot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vacek_P/0/1/0/all/0/1\">Patrik Vacek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hurych_D/0/1/0/all/0/1\">David Hurych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_K/0/1/0/all/0/1\">Karel Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svoboda_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Svoboda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Ranking for Object Image Blur Assessment. (arXiv:2207.06085v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06085","description":"<p>Assessing the blurriness of an object image is fundamentally important to\nimprove the performance for object recognition and retrieval. The main\nchallenge lies in the lack of abundant images with reliable labels and\neffective learning strategies. Current datasets are labeled with limited and\nconfused quality levels. To overcome this limitation, we propose to label the\nrank relationships between pairwise images rather their quality levels, since\nit is much easier for humans to label, and establish a large-scale realistic\nface image blur assessment dataset with reliable labels. Based on this dataset,\nwe propose a method to obtain the blur scores only with the pairwise rank\nlabels as supervision. Moreover, to further improve the performance, we propose\na self-supervised method based on quadruplet ranking consistency to leverage\nthe unlabeled data more effectively. The supervised and self-supervised methods\nconstitute a final semi-supervised learning framework, which can be trained\nend-to-end. Experimental results demonstrate the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhaoliang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingjing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Ye Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Pengju Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Di Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eliminating Gradient Conflict in Reference-based Line-art Colorization. (arXiv:2207.06095v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06095","description":"<p>Reference-based line-art colorization is a challenging task in computer\nvision. The color, texture, and shading are rendered based on an abstract\nsketch, which heavily relies on the precise long-range dependency modeling\nbetween the sketch and reference. Popular techniques to bridge the cross-modal\ninformation and model the long-range dependency employ the attention mechanism.\nHowever, in the context of reference-based line-art colorization, several\ntechniques would intensify the existing training difficulty of attention, for\ninstance, self-supervised training protocol and GAN-based losses. To understand\nthe instability in training, we detect the gradient flow of attention and\nobserve gradient conflict among attention branches. This phenomenon motivates\nus to alleviate the gradient issue by preserving the dominant gradient branch\nwhile removing the conflict ones. We propose a novel attention mechanism using\nthis training strategy, Stop-Gradient Attention (SGA), outperforming the\nattention baseline by a large margin with better training stability. Compared\nwith state-of-the-art modules in line-art colorization, our approach\ndemonstrates significant improvements in Fr\\'echet Inception Distance (FID, up\nto 27.21%) and structural similarity index measure (SSIM, up to 25.67%) on\nseveral benchmarks. The code of SGA is available at\nhttps://github.com/kunkun0w0/SGA .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zhengyang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global-local Motion Transformer for Unsupervised Skeleton-based Action Learning. (arXiv:2207.06101v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06101","description":"<p>We propose a new transformer model for the task of unsupervised learning of\nskeleton motion sequences. The existing transformer model utilized for\nunsupervised skeleton-based action learning is learned the instantaneous\nvelocity of each joint from adjacent frames without global motion information.\nThus, the model has difficulties in learning the attention globally over\nwhole-body motions and temporally distant joints. In addition, person-to-person\ninteractions have not been considered in the model. To tackle the learning of\nwhole-body motion, long-range temporal dynamics, and person-to-person\ninteractions, we design a global and local attention mechanism, where, global\nbody motions and local joint motions pay attention to each other. In addition,\nwe propose a novel pretraining strategy, multi-interval pose displacement\nprediction, to learn both global and local attention in diverse time ranges.\nThe proposed model successfully learns local dynamics of the joints and\ncaptures global context from the motion sequences. Our model outperforms\nstate-of-the-art models by notable margins in the representative benchmarks.\nCodes are available at https://github.com/Boeun-Kim/GL-Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Boeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hyung Jin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jungho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jin Young Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learnability Enhancement for Low-light Raw Denoising: Where Paired Real Data Meets Noise Modeling. (arXiv:2207.06103v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06103","description":"<p>Low-light raw denoising is an important and valuable task in computational\nphotography where learning-based methods trained with paired real data are\nmainstream. However, the limited data volume and complicated noise distribution\nhave constituted a learnability bottleneck for paired real data, which limits\nthe denoising performance of learning-based methods. To address this issue, we\npresent a learnability enhancement strategy to reform paired real data\naccording to noise modeling. Our strategy consists of two efficient techniques:\nshot noise augmentation (SNA) and dark shading correction (DSC). Through noise\nmodel decoupling, SNA improves the precision of data mapping by increasing the\ndata volume and DSC reduces the complexity of data mapping by reducing the\nnoise complexity. Extensive results on the public datasets and real imaging\nscenarios collectively demonstrate the state-of-the-art performance of our\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Hansen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hua Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Detection of Label Errors in Semantic Segmentation Datasets via Deep Learning and Uncertainty Quantification. (arXiv:2207.06104v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06104","description":"<p>In this work, we for the first time present a method for detecting label\nerrors in image datasets with semantic segmentation, i.e., pixel-wise class\nlabels. Annotation acquisition for semantic segmentation datasets is\ntime-consuming and requires plenty of human labor. In particular, review\nprocesses are time consuming and label errors can easily be overlooked by\nhumans. The consequences are biased benchmarks and in extreme cases also\nperformance degradation of deep neural networks (DNNs) trained on such\ndatasets. DNNs for semantic segmentation yield pixel-wise predictions, which\nmakes detection of label errors via uncertainty quantification a complex task.\nUncertainty is particularly pronounced at the transitions between connected\ncomponents of the prediction. By lifting the consideration of uncertainty to\nthe level of predicted components, we enable the usage of DNNs together with\ncomponent-level uncertainty quantification for the detection of label errors.\nWe present a principled approach to benchmarking the task of label error\ndetection by dropping labels from the Cityscapes dataset as well from a dataset\nextracted from the CARLA driving simulator, where in the latter case we have\nthe labels under control. Our experiments show that our approach is able to\ndetect the vast majority of label errors while controlling the number of false\nlabel error detections. Furthermore, we apply our method to semantic\nsegmentation datasets frequently used by the computer vision community and\npresent a collection of label errors along with sample statistics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1\">Matthias Rottmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reese_M/0/1/0/all/0/1\">Marco Reese</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DynaST: Dynamic Sparse Transformer for Exemplar-Guided Image Generation. (arXiv:2207.06124v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06124","description":"<p>One key challenge of exemplar-guided image generation lies in establishing\nfine-grained correspondences between input and guided images. Prior approaches,\ndespite the promising results, have relied on either estimating dense attention\nto compute per-point matching, which is limited to only coarse scales due to\nthe quadratic memory cost, or fixing the number of correspondences to achieve\nlinear complexity, which lacks flexibility. In this paper, we propose a dynamic\nsparse attention based Transformer model, termed Dynamic Sparse Transformer\n(DynaST), to achieve fine-level matching with favorable efficiency. The heart\nof our approach is a novel dynamic-attention unit, dedicated to covering the\nvariation on the optimal number of tokens one position should focus on.\nSpecifically, DynaST leverages the multi-layer nature of Transformer structure,\nand performs the dynamic attention scheme in a cascaded manner to refine\nmatching results and synthesize visually-pleasing outputs. In addition, we\nintroduce a unified training objective for DynaST, making it a versatile\nreference-based image translation framework for both supervised and\nunsupervised scenarios. Extensive experiments on three applications,\npose-guided person image generation, edge-based face synthesis, and undistorted\nimage style transfer, demonstrate that DynaST achieves superior performance in\nlocal details, outperforming the state of the art while reducing the\ncomputational cost significantly. Our code is available at\nhttps://github.com/Huage001/DynaST\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jingwen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust and accurate depth estimation by fusing LiDAR and Stereo. (arXiv:2207.06139v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06139","description":"<p>Depth estimation is one of the key technologies in some fields such as\nautonomous driving and robot navigation. However, the traditional method of\nusing a single sensor is inevitably limited by the performance of the sensor.\nTherefore, a precision and robust method for fusing the LiDAR and stereo\ncameras is proposed. This method fully combines the advantages of the LiDAR and\nstereo camera, which can retain the advantages of the high precision of the\nLiDAR and the high resolution of images respectively. Compared with the\ntraditional stereo matching method, the texture of the object and lighting\nconditions have less influence on the algorithm. Firstly, the depth of the\nLiDAR data is converted to the disparity of the stereo camera. Because the\ndensity of the LiDAR data is relatively sparse on the y-axis, the converted\ndisparity map is up-sampled using the interpolation method. Secondly, in order\nto make full use of the precise disparity map, the disparity map and stereo\nmatching are fused to propagate the accurate disparity. Finally, the disparity\nmap is converted to the depth map. Moreover, the converted disparity map can\nalso increase the speed of the algorithm. We evaluate the proposed pipeline on\nthe KITTI benchmark. The experiment demonstrates that our algorithm has higher\naccuracy than several classic methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangyao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Junfeng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">En Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1\">Xiaoyu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Rui Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating the Power Consumption of Heterogeneous Devices when performing AI Inference. (arXiv:2207.06150v1 [cs.AR])","link":"http://arxiv.org/abs/2207.06150","description":"<p>Modern-day life is driven by electronic devices connected to the internet.\nThe emerging research field of the Internet-of-Things (IoT) has become popular,\njust as there has been a steady increase in the number of connected devices -\nnow over 50 billion. Since many of these devices are utilised to perform\n\\gls*{cv} tasks, it is essential to understand their power consumption against\nperformance. We report the power consumption profile and analysis of the NVIDIA\nJetson Nano board while performing object classification. The authors present\nan extensive analysis regarding power consumption per frame and the output in\nframes per second (FPS) using YOLOv5 models. The results show that the YOLOv5n\noutperforms other YOLOV5 variants in terms of throughput (i.e. 12.34 fps) and\nlow power consumption (i.e. 0.154 mWh/frame).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Machado_P/0/1/0/all/0/1\">Pedro Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matic_I/0/1/0/all/0/1\">Ivica Matic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lemos_F/0/1/0/all/0/1\">Francisco de Lemos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihianle_I/0/1/0/all/0/1\">Isibor Kennedy Ihianle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adama_D/0/1/0/all/0/1\">David Ada Adama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A comparison between PMBM Bayesian track initiation and labelled RFS adaptive birth. (arXiv:2207.06156v1 [stat.AP])","link":"http://arxiv.org/abs/2207.06156","description":"<p>This paper provides a comparative analysis between the adaptive birth model\nused in the labelled random finite set literature and the track initiation in\nthe Poisson multi-Bernoulli mixture (PMBM) filter, with point-target models.\nThe PMBM track initiation is obtained via Bayes' rule applied on the predicted\nPMBM density, and creates one Bernoulli component for each received\nmeasurement, representing that this measurement may be clutter or a detection\nfrom a new target. Adaptive birth mimics this procedure by creating a Bernoulli\ncomponent for each measurement using a different rule to determine the\nprobability of existence and a user-defined single-target density. This paper\nfirst provides an analysis of the differences that arise in track initiation\nbased on isolated measurements. Then, it shows that adaptive birth\nunderestimates the number of objects present in the surveillance area under\ncommon modelling assumptions. Finally, we provide numerical simulations to\nfurther illustrate the differences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Garcia_Fernandez_A/0/1/0/all/0/1\">&#xc1;ngel F. Garc&#xed;a-Fern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xia_Y/0/1/0/all/0/1\">Yuxuan Xia</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Svensson_L/0/1/0/all/0/1\">Lennart Svensson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Visual Representation Learning by Synchronous Momentum Grouping. (arXiv:2207.06167v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06167","description":"<p>In this paper, we propose a genuine group-level contrastive visual\nrepresentation learning method whose linear evaluation performance on ImageNet\nsurpasses the vanilla supervised learning. Two mainstream unsupervised learning\nschemes are the instance-level contrastive framework and clustering-based\nschemes. The former adopts the extremely fine-grained instance-level\ndiscrimination whose supervisory signal is not efficient due to the false\nnegatives. Though the latter solves this, they commonly come with some\nrestrictions affecting the performance. To integrate their advantages, we\ndesign the SMoG method. SMoG follows the framework of contrastive learning but\nreplaces the contrastive unit from instance to group, mimicking\nclustering-based methods. To achieve this, we propose the momentum grouping\nscheme which synchronously conducts feature grouping with representation\nlearning. In this way, SMoG solves the problem of supervisory signal hysteresis\nwhich the clustering-based method usually faces, and reduces the false\nnegatives of instance contrastive methods. We conduct exhaustive experiments to\nshow that SMoG works well on both CNN and Transformer backbones. Results prove\nthat SMoG has surpassed the current SOTA unsupervised representation learning\nmethods. Moreover, its linear evaluation results surpass the performances\nobtained by vanilla supervised learning and the representation can be well\ntransferred to downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaoyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jia Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MRF-UNets: Searching UNet with Markov Random Fields. (arXiv:2207.06168v1 [cs.LG])","link":"http://arxiv.org/abs/2207.06168","description":"<p>UNet [27] is widely used in semantic segmentation due to its simplicity and\neffectiveness. However, its manually-designed architecture is applied to a\nlarge number of problem settings, either with no architecture optimizations, or\nwith manual tuning, which is time consuming and can be sub-optimal. In this\nwork, firstly, we propose Markov Random Field Neural Architecture Search\n(MRF-NAS) that extends and improves the recent Adaptive and Optimal Network\nWidth Search (AOWS) method [4] with (i) a more general MRF framework (ii)\ndiverse M-best loopy inference (iii) differentiable parameter learning. This\nprovides the necessary NAS framework to efficiently explore network\narchitectures that induce loopy inference graphs, including loops that arise\nfrom skip connections. With UNet as the backbone, we find an architecture,\nMRF-UNet, that shows several interesting characteristics. Secondly, through the\nlens of these characteristics, we identify the sub-optimality of the original\nUNet architecture and further improve our results with MRF-UNetV2. Experiments\nshow that our MRF-UNets significantly outperform several benchmarks on three\naerial image datasets and two medical image datasets while maintaining low\ncomputational costs. The code is available at:\nhttps://github.com/zifuwanggg/MRF-UNets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1\">Matthew B. Blaschko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RTN: Reinforced Transformer Network for Coronary CT Angiography Vessel-level Image Quality Assessment. (arXiv:2207.06177v1 [cs.MM])","link":"http://arxiv.org/abs/2207.06177","description":"<p>Coronary CT Angiography (CCTA) is susceptible to various distortions (e.g.,\nartifacts and noise), which severely compromise the exact diagnosis of\ncardiovascular diseases. The appropriate CCTA Vessel-level Image Quality\nAssessment (CCTA VIQA) algorithm can be used to reduce the risk of error\ndiagnosis. The primary challenges of CCTA VIQA are that the local part of\ncoronary that determines final quality is hard to locate. To tackle the\nchallenge, we formulate CCTA VIQA as a multiple-instance learning (MIL)\nproblem, and exploit Transformer-based MIL backbone (termed as T-MIL) to\naggregate the multiple instances along the coronary centerline into the final\nquality. However, not all instances are informative for final quality. There\nare some quality-irrelevant/negative instances intervening the exact quality\nassessment(e.g., instances covering only background or the coronary in\ninstances is not identifiable). Therefore, we propose a Progressive\nReinforcement learning based Instance Discarding module (termed as PRID) to\nprogressively remove quality-irrelevant/negative instances for CCTA VIQA. Based\non the above two modules, we propose a Reinforced Transformer Network (RTN) for\nautomatic CCTA VIQA based on end-to-end optimization. Extensive experimental\nresults demonstrate that our proposed method achieves the state-of-the-art\nperformance on the real-world CCTA dataset, exceeding previous MIL methods by a\nlarge margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yiting Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Congfu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Depression Estimation based on Sub-attentional Fusion. (arXiv:2207.06180v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06180","description":"<p>Failure to timely diagnose and effectively treat depression leads to over 280\nmillion people suffering from this psychological disorder worldwide. The\ninformation cues of depression can be harvested from diverse heterogeneous\nresources, e.g., audio, visual, and textual data, raising demand for new\neffective multi-modal fusion approaches for its automatic estimation. In this\nwork, we tackle the task of automatically identifying depression from\nmulti-modal data and introduce a sub-attention mechanism for linking\nheterogeneous information while leveraging Convolutional Bidirectional LSTM as\nour backbone. To validate this idea, we conduct extensive experiments on the\npublic DAIC-WOZ benchmark for depression assessment featuring different\nevaluation modes and taking gender-specific biases into account. The proposed\nmodel yields effective results with 0.89 precision and 0.70 F1-score in\ndetecting major depression and 4.92 MAE in estimating the severity. Our\nattention-based fusion module consistently outperforms conventional late fusion\napproaches and achieves a competitive performance compared to the previously\npublished depression estimation frameworks, while learning to diagnose the\ndisorder end-to-end and relying on far less preprocessing steps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1\">Ping-Cheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1\">Alina Roitberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collaborative Quantization Embeddings for Intra-Subject Prostate MR Image Registration. (arXiv:2207.06189v1 [eess.IV])","link":"http://arxiv.org/abs/2207.06189","description":"<p>Image registration is useful for quantifying morphological changes in\nlongitudinal MR images from prostate cancer patients. This paper describes a\ndevelopment in improving the learning-based registration algorithms, for this\nchallenging clinical application often with highly variable yet limited\ntraining data. First, we report that the latent space can be clustered into a\nmuch lower dimensional space than that commonly found as bottleneck features at\nthe deep layer of a trained registration network. Based on this observation, we\npropose a hierarchical quantization method, discretizing the learned feature\nvectors using a jointly-trained dictionary with a constrained size, in order to\nimprove the generalisation of the registration networks. Furthermore, a novel\ncollaborative dictionary is independently optimised to incorporate additional\nprior information, such as the segmentation of the gland or other regions of\ninterest, in the latent quantized space. Based on 216 real clinical images from\n86 prostate cancer patients, we show the efficacy of both the designed\ncomponents. Improved registration accuracy was obtained with statistical\nsignificance, in terms of both Dice on gland and target registration error on\ncorresponding landmarks, the latter of which achieved 5.46 mm, an improvement\nof 28.7\\% from the baseline without quantization. Experimental results also\nshow that the difference in performance was indeed minimised between training\nand testing data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shen_Z/0/1/0/all/0/1\">Ziyi Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1\">Qianye Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1\">Yuming Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Giganti_F/0/1/0/all/0/1\">Francesco Giganti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stavrinides_V/0/1/0/all/0/1\">Vasilis Stavrinides</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_R/0/1/0/all/0/1\">Richard Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moore_C/0/1/0/all/0/1\">Caroline Moore</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rusu_M/0/1/0/all/0/1\">Mirabela Rusu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sonn_G/0/1/0/all/0/1\">Geoffrey Sonn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barratt_D/0/1/0/all/0/1\">Dean Barratt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain adaptation strategies for cancer-independent detection of lymph node metastases. (arXiv:2207.06193v1 [eess.IV])","link":"http://arxiv.org/abs/2207.06193","description":"<p>Recently, large, high-quality public datasets have led to the development of\nconvolutional neural networks that can detect lymph node metastases of breast\ncancer at the level of expert pathologists. Many cancers, regardless of the\nsite of origin, can metastasize to lymph nodes. However, collecting and\nannotating high-volume, high-quality datasets for every cancer type is\nchallenging. In this paper we investigate how to leverage existing high-quality\ndatasets most efficiently in multi-task settings for closely related tasks.\nSpecifically, we will explore different training and domain adaptation\nstrategies, including prevention of catastrophic forgetting, for colon and\nhead-and-neck cancer metastasis detection in lymph nodes.\n</p>\n<p>Our results show state-of-the-art performance on both cancer metastasis\ndetection tasks. Furthermore, we show the effectiveness of repeated adaptation\nof networks from one cancer type to another to obtain multi-task metastasis\ndetection networks. Last, we show that leveraging existing high-quality\ndatasets can significantly boost performance on new target tasks and that\ncatastrophic forgetting can be effectively mitigated using regularization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bandi_P/0/1/0/all/0/1\">P&#xe9;ter B&#xe1;ndi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Balkenhol_M/0/1/0/all/0/1\">Maschenka Balkenhol</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dijk_M/0/1/0/all/0/1\">Marcory van Dijk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginneken_B/0/1/0/all/0/1\">Bram van Ginneken</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laak_J/0/1/0/all/0/1\">Jeroen van der Laak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Litjens_G/0/1/0/all/0/1\">Geert Litjens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarially-Aware Robust Object Detector. (arXiv:2207.06202v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06202","description":"<p>Object detection, as a fundamental computer vision task, has achieved a\nremarkable progress with the emergence of deep neural networks. Nevertheless,\nfew works explore the adversarial robustness of object detectors to resist\nadversarial attacks for practical applications in various real-world scenarios.\nDetectors have been greatly challenged by unnoticeable perturbation, with sharp\nperformance drop on clean images and extremely poor performance on adversarial\nimages. In this work, we empirically explore the model training for adversarial\nrobustness in object detection, which greatly attributes to the conflict\nbetween learning clean images and adversarial images. To mitigate this issue,\nwe propose a Robust Detector (RobustDet) based on adversarially-aware\nconvolution to disentangle gradients for model learning on clean and\nadversarial images. RobustDet also employs the Adversarial Image Discriminator\n(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliable\nrobustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate that\nour model effectively disentangles gradients and significantly enhances the\ndetection robustness with maintaining the detection ability on clean images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Ziyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1\">Pengxu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trans4Map: Revisiting Holistic Top-down Mapping from Egocentric Images to Allocentric Semantics with Vision Transformers. (arXiv:2207.06205v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06205","description":"<p>Humans have an innate ability to sense their surroundings, as they can\nextract the spatial representation from the egocentric perception and form an\nallocentric semantic map via spatial transformation and memory updating.\nHowever, endowing mobile agents with such a spatial sensing ability is still a\nchallenge, due to two difficulties: (1) the previous convolutional models are\nlimited by the local receptive field, thus, struggling to capture holistic\nlong-range dependencies during observation; (2) the excessive computational\nbudgets required for success, often lead to a separation of the mapping\npipeline into stages, resulting the entire mapping process inefficient. To\naddress these issues, we propose an end-to-end one-stage Transformer-based\nframework for Mapping, termed Trans4Map. Our egocentric-to-allocentric mapping\nprocess includes three steps: (1) the efficient transformer extracts the\ncontextual features from a batch of egocentric images; (2) the proposed\nBidirectional Allocentric Memory (BAM) module projects egocentric features into\nthe allocentric memory; (3) the map decoder parses the accumulated memory and\npredicts the top-down semantic segmentation map. In contrast, Trans4Map\nachieves state-of-the-art results, reducing 67.2% parameters, yet gaining a\n+3.25% mIoU and a +4.09% mBF1 improvements on the Matterport3D dataset. Code\nwill be made publicly available at https://github.com/jamycheung/Trans4Map.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sample-dependent Adaptive Temperature Scaling for Improved Calibration. (arXiv:2207.06211v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06211","description":"<p>It is now well known that neural networks can be wrong with high confidence\nin their predictions, leading to poor calibration. The most common post-hoc\napproach to compensate for this is to perform temperature scaling, which\nadjusts the confidences of the predictions on any input by scaling the logits\nby a fixed value. Whilst this approach typically improves the average\ncalibration across the whole test dataset, this improvement typically reduces\nthe individual confidences of the predictions irrespective of whether the\nclassification of a given input is correct or incorrect. With this insight, we\nbase our method on the observation that different samples contribute to the\ncalibration error by varying amounts, with some needing to increase their\nconfidence and others needing to decrease it. Therefore, for each input, we\npropose to predict a different temperature value, allowing us to adjust the\nmismatch between confidence and accuracy at a finer granularity. Furthermore,\nwe observe improved results on OOD detection and can also extract a notion of\nhardness for the data-points. Our method is applied post-hoc, consequently\nusing very little computation time and with a negligible memory footprint and\nis applied to off-the-shelf pre-trained classifiers. We test our method on the\nResNet50 and WideResNet28-10 architectures using the CIFAR10/100 and\nTiny-ImageNet datasets, showing that producing per-data-point temperatures is\nbeneficial also for the expected calibration error across the whole test set.\nCode is available at: https://github.com/thwjoy/adats.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joy_T/0/1/0/all/0/1\">Tom Joy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_F/0/1/0/all/0/1\">Francesco Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H. S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1\">Puneet K. Dokania</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is one annotation enough? A data-centric image classification benchmark for noisy and ambiguous label estimation. (arXiv:2207.06214v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06214","description":"<p>High-quality data is necessary for modern machine learning. However, the\nacquisition of such data is difficult due to noisy and ambiguous annotations of\nhumans. The aggregation of such annotations to determine the label of an image\nleads to a lower data quality. We propose a data-centric image classification\nbenchmark with nine real-world datasets and multiple annotations per image to\ninvestigate and quantify the impact of such data quality issues. We focus on a\ndata-centric perspective by asking how we could improve the data quality.\nAcross thousands of experiments, we show that multiple annotations allow a\nbetter approximation of the real underlying class distribution. We identify\nthat hard labels can not capture the ambiguity of the data and this might lead\nto the common issue of overconfident models. Based on the presented datasets,\nbenchmark baselines, and analysis, we create multiple research opportunities\nfor the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmarje_L/0/1/0/all/0/1\">Lars Schmarje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grossmann_V/0/1/0/all/0/1\">Vasco Grossmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelenka_C/0/1/0/all/0/1\">Claudius Zelenka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dippel_S/0/1/0/all/0/1\">Sabine Dippel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiko_R/0/1/0/all/0/1\">Rainer Kiko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oszust_M/0/1/0/all/0/1\">Mariusz Oszust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pastell_M/0/1/0/all/0/1\">Matti Pastell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stracke_J/0/1/0/all/0/1\">Jenny Stracke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valros_A/0/1/0/all/0/1\">Anna Valros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volkmann_N/0/1/0/all/0/1\">Nina Volkmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1\">Reinahrd Koch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLO2U-Net: Detection-Guided 3D Instance Segmentation for Microscopy. (arXiv:2207.06215v1 [eess.IV])","link":"http://arxiv.org/abs/2207.06215","description":"<p>Microscopy imaging techniques are instrumental for characterization and\nanalysis of biological structures. As these techniques typically render 3D\nvisualization of cells by stacking 2D projections, issues such as out-of-plane\nexcitation and low resolution in the $z$-axis may pose challenges (even for\nhuman experts) to detect individual cells in 3D volumes as these\nnon-overlapping cells may appear as overlapping. In this work, we introduce a\ncomprehensive method for accurate 3D instance segmentation of cells in the\nbrain tissue. The proposed method combines the 2D YOLO detection method with a\nmulti-view fusion algorithm to construct a 3D localization of the cells. Next,\nthe 3D bounding boxes along with the data volume are input to a 3D U-Net\nnetwork that is designed to segment the primary cell in each 3D bounding box,\nand in turn, to carry out instance segmentation of cells in the entire volume.\nThe promising performance of the proposed method is shown in comparison with\nsome current deep learning-based 3D instance segmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ziabari_A/0/1/0/all/0/1\">Amirkoushyar Ziabari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ros_D/0/1/0/all/0/1\">Derek C. Ros</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shirinifard_A/0/1/0/all/0/1\">Abbas Shirinifard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Solecki_D/0/1/0/all/0/1\">David Solecki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Hard Labels: Investigating data label distributions. (arXiv:2207.06224v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06224","description":"<p>High-quality data is a key aspect of modern machine learning. However, labels\ngenerated by humans suffer from issues like label noise and class ambiguities.\nWe raise the question of whether hard labels are sufficient to represent the\nunderlying ground truth distribution in the presence of these inherent\nimprecision. Therefore, we compare the disparity of learning with hard and soft\nlabels quantitatively and qualitatively for a synthetic and a real-world\ndataset. We show that the application of soft labels leads to improved\nperformance and yields a more regular structure of the internal feature space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grossmann_V/0/1/0/all/0/1\">Vasco Grossmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmarje_L/0/1/0/all/0/1\">Lars Schmarje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1\">Reinhard Koch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entry-Flipped Transformer for Inference and Prediction of Participant Behavior. (arXiv:2207.06235v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06235","description":"<p>Some group activities, such as team sports and choreographed dances, involve\nclosely coupled interaction between participants. Here we investigate the tasks\nof inferring and predicting participant behavior, in terms of motion paths and\nactions, under such conditions. We narrow the problem to that of estimating how\na set target participants react to the behavior of other observed participants.\nOur key idea is to model the spatio-temporal relations among participants in a\nmanner that is robust to error accumulation during frame-wise inference and\nprediction. We propose a novel Entry-Flipped Transformer (EF-Transformer),\nwhich models the relations of participants by attention mechanisms on both\nspatial and temporal domains. Unlike typical transformers, we tackle the\nproblem of error accumulation by flipping the order of query, key, and value\nentries, to increase the importance and fidelity of observed features in the\ncurrent frame. Comparative experiments show that our EF-Transformer achieves\nthe best performance on a newly-collected tennis doubles dataset, a Ceilidh\ndance dataset, and two pedestrian datasets. Furthermore, it is also\ndemonstrated that our EF-Transformer is better at limiting accumulated errors\nand recovering from wrong estimations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cham_T/0/1/0/all/0/1\">Tat-Jen Cham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SlimSeg: Slimmable Semantic Segmentation with Boundary Supervision. (arXiv:2207.06242v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06242","description":"<p>Accurate semantic segmentation models typically require significant\ncomputational resources, inhibiting their use in practical applications. Recent\nworks rely on well-crafted lightweight models to achieve fast inference.\nHowever, these models cannot flexibly adapt to varying accuracy and efficiency\nrequirements. In this paper, we propose a simple but effective slimmable\nsemantic segmentation (SlimSeg) method, which can be executed at different\ncapacities during inference depending on the desired accuracy-efficiency\ntradeoff. More specifically, we employ parametrized channel slimming by\nstepwise downward knowledge distillation during training. Motivated by the\nobservation that the differences between segmentation results of each submodel\nare mainly near the semantic borders, we introduce an additional boundary\nguided semantic segmentation loss to further improve the performance of each\nsubmodel. We show that our proposed SlimSeg with various mainstream networks\ncan produce flexible models that provide dynamic adjustment of computational\ncost and better performance than independent models. Extensive experiments on\nsemantic segmentation benchmarks, Cityscapes and CamVid, demonstrate the\ngeneralization ability of our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_D/0/1/0/all/0/1\">Danna Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herranz_L/0/1/0/all/0/1\">Luis Herranz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jinqiu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanning Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Consistent Semantic Image Editing with Style-Preserved Modulation. (arXiv:2207.06252v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06252","description":"<p>Semantic image editing utilizes local semantic label maps to generate the\ndesired content in the edited region. A recent work borrows SPADE block to\nachieve semantic image editing. However, it cannot produce pleasing results due\nto style discrepancy between the edited region and surrounding pixels. We\nattribute this to the fact that SPADE only uses an image-independent local\nsemantic layout but ignores the image-specific styles included in the known\npixels. To address this issue, we propose a style-preserved modulation (SPM)\ncomprising two modulations processes: The first modulation incorporates the\ncontextual style and semantic layout, and then generates two fused modulation\nparameters. The second modulation employs the fused parameters to modulate\nfeature maps. By using such two modulations, SPM can inject the given semantic\nlayout while preserving the image-specific context style. Moreover, we design a\nprogressive architecture for generating the edited content in a coarse-to-fine\nmanner. The proposed method can obtain context-consistent results and\nsignificantly alleviate the unpleasant boundary between the generated regions\nand the known pixels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wuyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Su Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weishan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image warp preserving content intensity. (arXiv:2207.06256v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06256","description":"<p>An accurate method for warping images is presented. Differently from most\ncommonly used techniques, this method guarantees the conservation of the\nintensity of the transformed image, evaluated as the sum of its pixel values\nover the whole image or over corresponding transformed subregions of it. Such\nproperty is mandatory for quantitative analysis, as, for instance, when\ndeformed images are used to assess radiances, to measure optical fluxes from\nlight sources, or to characterize material optical densities. The proposed\nmethod enforces area resampling by decomposing each rectangular pixel in two\ntriangles, and projecting the pixel intensity onto half pixels of the\ntransformed image, with weights proportional to the area of overlap of the\ntriangular half-pixels. The result is quantitatively exact, as long as the\noriginal pixel value is assumed to represent a constant image density within\nthe pixel area, and as long as the coordinate transformation is diffeomorphic.\nImplementation details and possible variations of the method are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Segre_E/0/1/0/all/0/1\">Enrico Segre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Appearance Free Action Recognition Possible?. (arXiv:2207.06261v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06261","description":"<p>Intuition might suggest that motion and dynamic information are key to\nvideo-based action recognition. In contrast, there is evidence that\nstate-of-the-art deep-learning video understanding architectures are biased\ntoward static information available in single frames. Presently, a methodology\nand corresponding dataset to isolate the effects of dynamic information in\nvideo are missing. Their absence makes it difficult to understand how well\ncontemporary architectures capitalize on dynamic vs. static information. We\nrespond with a novel Appearance Free Dataset (AFD) for action recognition. AFD\nis devoid of static information relevant to action recognition in a single\nframe. Modeling of the dynamics is necessary for solving the task, as the\naction is only apparent through consideration of the temporal dimension. We\nevaluated 11 contemporary action recognition architectures on AFD as well as\nits related RGB video. Our results show a notable decrease in performance for\nall architectures on AFD compared to RGB. We also conducted a complimentary\nstudy with humans that shows their recognition accuracy on AFD and RGB is very\nsimilar and much better than the evaluated architectures on AFD. Our results\nmotivate a novel architecture that revives explicit recovery of optical flow,\nwithin a contemporary design for best performance on AFD and RGB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ilic_F/0/1/0/all/0/1\">Filip Ilic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pock_T/0/1/0/all/0/1\">Thomas Pock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1\">Richard P. Wildes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Organic Priors in Non-Rigid Structure from Motion. (arXiv:2207.06262v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06262","description":"<p>This paper advocates the use of organic priors in classical non-rigid\nstructure from motion (NRSfM). By organic priors, we mean invaluable\nintermediate prior information intrinsic to the NRSfM matrix factorization\ntheory. It is shown that such priors reside in the factorized matrices, and\nquite surprisingly, existing methods generally disregard them. The paper's main\ncontribution is to put forward a simple, methodical, and practical method that\ncan effectively exploit such organic priors to solve NRSfM. The proposed method\ndoes not make assumptions other than the popular one on the low-rank shape and\noffers a reliable solution to NRSfM under orthographic projection. Our work\nreveals that the accessibility of organic priors is independent of the camera\nmotion and shape deformation type. Besides that, the paper provides insights\ninto the NRSfM factorization -- both in terms of shape, motion -- and is the\nfirst approach to show the benefit of single rotation averaging for NRSfM.\nFurthermore, we outline how to effectively recover motion and non-rigid 3D\nshape using the proposed organic prior based approach and demonstrate results\nthat outperform prior-free NRSfM performance by a significant margin. Finally,\nwe present the benefits of our method via extensive experiments and evaluations\non several benchmark dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Suryansh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task Agnostic Representation Consolidation: a Self-supervised based Continual Learning Approach. (arXiv:2207.06267v1 [cs.LG])","link":"http://arxiv.org/abs/2207.06267","description":"<p>Continual learning (CL) over non-stationary data streams remains one of the\nlong-standing challenges in deep neural networks (DNNs) as they are prone to\ncatastrophic forgetting. CL models can benefit from self-supervised\npre-training as it enables learning more generalizable task-agnostic features.\nHowever, the effect of self-supervised pre-training diminishes as the length of\ntask sequences increases. Furthermore, the domain shift between pre-training\ndata distribution and the task distribution reduces the generalizability of the\nlearned representations. To address these limitations, we propose Task Agnostic\nRepresentation Consolidation (TARC), a two-stage training paradigm for CL that\nintertwines task-agnostic and task-specific learning whereby self-supervised\ntraining is followed by supervised learning for each task. To further restrict\nthe deviation from the learned representations in the self-supervised stage, we\nemploy a task-agnostic auxiliary loss during the supervised stage. We show that\nour training paradigm can be easily added to memory- or regularization-based\napproaches and provides consistent performance gain across more challenging CL\nsettings. We further show that it leads to more robust and well-calibrated\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhat_P/0/1/0/all/0/1\">Prashant Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACLNet: An Attention and Clustering-based Cloud Segmentation Network. (arXiv:2207.06277v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06277","description":"<p>We propose a novel deep learning model named ACLNet, for cloud segmentation\nfrom ground images. ACLNet uses both deep neural network and machine learning\n(ML) algorithm to extract complementary features. Specifically, it uses\nEfficientNet-B0 as the backbone, \"`a trous spatial pyramid pooling\" (ASPP) to\nlearn at multiple receptive fields, and \"global attention module\" (GAM) to\nextract finegrained details from the image. ACLNet also uses k-means clustering\nto extract cloud boundaries more precisely. ACLNet is effective for both\ndaytime and nighttime images. It provides lower error rate, higher recall and\nhigher F1-score than state-of-art cloud segmentation models. The source-code of\nACLNet is available here: https://github.com/ckmvigil/ACLNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Makwana_D/0/1/0/all/0/1\">Dhruv Makwana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1\">Subhrajit Nag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susladkar_O/0/1/0/all/0/1\">Onkar Susladkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_G/0/1/0/all/0/1\">Gayatri Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+R_S/0/1/0/all/0/1\">Sai Chandra Teja R</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1\">Sparsh Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_C/0/1/0/all/0/1\">C Krishna Mohan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Neural Representations for Generative Modeling of Living Cell Shapes. (arXiv:2207.06283v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06283","description":"<p>Methods allowing the synthesis of realistic cell shapes could help generate\ntraining data sets to improve cell tracking and segmentation in biomedical\nimages. Deep generative models for cell shape synthesis require a light-weight\nand flexible representation of the cell shape. However, commonly used\nvoxel-based representations are unsuitable for high-resolution shape synthesis,\nand polygon meshes have limitations when modeling topology changes such as cell\ngrowth or mitosis. In this work, we propose to use level sets of signed\ndistance functions (SDFs) to represent cell shapes. We optimize a neural\nnetwork as an implicit neural representation of the SDF value at any point in a\n3D+time domain. The model is conditioned on a latent code, thus allowing the\nsynthesis of new and unseen shape sequences. We validate our approach\nquantitatively and qualitatively on C. elegans cells that grow and divide, and\nlung cancer cells with growing complex filopodial protrusions. Our results show\nthat shape descriptors of synthetic cells resemble those of real cells, and\nthat our model is able to generate topologically plausible sequences of complex\ncell shapes in 3D+time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wiesner_D/0/1/0/all/0/1\">David Wiesner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suk_J/0/1/0/all/0/1\">Julian Suk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dummer_S/0/1/0/all/0/1\">Sven Dummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svoboda_D/0/1/0/all/0/1\">David Svoboda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolterink_J/0/1/0/all/0/1\">Jelmer M. Wolterink</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PointNorm: Normalization is All You Need for Point Cloud Analysis. (arXiv:2207.06324v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06324","description":"<p>Point cloud analysis is challenging due to the irregularity of the point\ncloud data structure. Existing works typically employ the ad-hoc\nsampling-grouping operation of PointNet++, followed by sophisticated local\nand/or global feature extractors for leveraging the 3D geometry of the point\ncloud. Unfortunately, those intricate hand-crafted model designs have led to\npoor inference latency and performance saturation in the last few years. In\nthis paper, we point out that the classical sampling-grouping operations on the\nirregular point cloud cause learning difficulty for the subsequent MLP layers.\nTo reduce the irregularity of the point cloud, we introduce a DualNorm module\nafter the sampling-grouping operation. The DualNorm module consists of Point\nNormalization, which normalizes the grouped points to the sampled points, and\nReverse Point Normalization, which normalizes the sampled points to the grouped\npoints. The proposed PointNorm utilizes local mean and global standard\ndeviation to benefit from both local and global features while maintaining a\nfaithful inference speed. Experiments on point cloud classification show that\nwe achieved state-of-the-art accuracy on ModelNet40 and ScanObjectNN datasets.\nWe also generalize our model to point cloud part segmentation and demonstrate\ncompetitive performance on the ShapeNetPart dataset. Code is available at\nhttps://github.com/ShenZheng2000/PointNorm-for-Point-Cloud-Analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jinqian Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Changjie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gaurav Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Left Ventricle Contouring of Apical Three-Chamber Views on 2D Echocardiography. (arXiv:2207.06330v1 [eess.IV])","link":"http://arxiv.org/abs/2207.06330","description":"<p>We propose a new method to automatically contour the left ventricle on 2D\nechocardiographic images. Unlike most existing segmentation methods, which are\nbased on predicting segmentation masks, we focus at predicting the endocardial\ncontour and the key landmark points within this contour (basal points and\napex). This provides a representation that is closer to how experts perform\nmanual annotations and hence produce results that are physiologically more\nplausible.\n</p>\n<p>Our proposed method uses a two-headed network based on the U-Net\narchitecture. One head predicts the 7 contour points, and the other head\npredicts a distance map to the contour. This approach was compared to the U-Net\nand to a point based approach, achieving performance gains of up to 30\\% in\nterms of landmark localisation (&lt;4.5mm) and distance to the ground truth\ncontour (&lt;3.5mm).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gomez_A/0/1/0/all/0/1\">Alberto Gomez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Porumb_M/0/1/0/all/0/1\">Mihaela Porumb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mumith_A/0/1/0/all/0/1\">Angela Mumith</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Judge_T/0/1/0/all/0/1\">Thierry Judge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_S/0/1/0/all/0/1\">Shan Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_W/0/1/0/all/0/1\">Woo-Jin Cho Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_J/0/1/0/all/0/1\">Jorge Oliveira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chartsias_A/0/1/0/all/0/1\">Agis Chartsias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symmetry-Aware Transformer-based Mirror Detection. (arXiv:2207.06332v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06332","description":"<p>Mirror detection aims to identify the mirror regions in the given input\nimage. Existing works mainly focus on integrating the semantic features and\nstructural features to mine the similarity and discontinuity between mirror and\nnon-mirror regions, or introducing depth information to help analyze the\nexistence of mirrors. In this work, we observe that a real object typically\nforms a loose symmetry relationship with its corresponding reflection in the\nmirror, which is beneficial in distinguishing mirrors from real objects. Based\non this observation, we propose a dual-path Symmetry-Aware Transformer-based\nmirror detection Network (SATNet), which includes two novel modules:\nSymmetry-Aware Attention Module (SAAM) and Contrast and Fusion Decoder Module\n(CFDM). Specifically, we first introduce the transformer backbone to model\nglobal information aggregation in images, extracting multi-scale features in\ntwo paths. We then feed the high-level dual-path features to SAAMs to capture\nthe symmetry relations. Finally, we fuse the dual-path features and refine our\nprediction maps progressively with CFDMs to obtain the final mirror mask.\nExperimental results show that SATNet outperforms both RGB and RGB-D mirror\ndetection methods on all available mirror detection datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tianyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bowen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiaying Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaohui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_R/0/1/0/all/0/1\">Rynson W.H. Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"6D Camera Relocalization in Visually Ambiguous Extreme Environments. (arXiv:2207.06333v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06333","description":"<p>We propose a novel method to reliably estimate the pose of a camera given a\nsequence of images acquired in extreme environments such as deep seas or\nextraterrestrial terrains. Data acquired under these challenging conditions are\ncorrupted by textureless surfaces, image degradation, and presence of\nrepetitive and highly ambiguous structures. When naively deployed, the\nstate-of-the-art methods can fail in those scenarios as confirmed by our\nempirical analysis. In this paper, we attempt to make camera relocalization\nwork in these extreme situations. To this end, we propose: (i) a hierarchical\nlocalization system, where we leverage temporal information and (ii) a novel\nenvironment-aware image enhancement method to boost the robustness and\naccuracy. Our extensive experimental results demonstrate superior performance\nin favor of our method under two extreme settings: localizing an autonomous\nunderwater vehicle and localizing a planetary rover in a Mars-like desert. In\naddition, our method achieves comparable performance with state-of-the-art\nmethods on the indoor benchmark (7-Scenes dataset) using only 20% training\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1\">Tolga Birdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yueqi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Only Align Once: Bidirectional Interaction for Spatial-Temporal Video Super-Resolution. (arXiv:2207.06345v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06345","description":"<p>Spatial-Temporal Video Super-Resolution (ST-VSR) technology generates\nhigh-quality videos with higher resolution and higher frame rates. Existing\nadvanced methods accomplish ST-VSR tasks through the association of Spatial and\nTemporal video super-resolution (S-VSR and T-VSR). These methods require two\nalignments and fusions in S-VSR and T-VSR, which is obviously redundant and\nfails to sufficiently explore the information flow of consecutive spatial LR\nframes. Although bidirectional learning (future-to-past and past-to-future) was\nintroduced to cover all input frames, the direct fusion of final predictions\nfails to sufficiently exploit intrinsic correlations of bidirectional motion\nlearning and spatial information from all frames. We propose an effective yet\nefficient recurrent network with bidirectional interaction for ST-VSR, where\nonly one alignment and fusion is needed. Specifically, it first performs\nbackward inference from future to past, and then follows forward inference to\nsuper-resolve intermediate frames. The backward and forward inferences are\nassigned to learn structures and details to simplify the learning task with\njoint optimizations. Furthermore, a Hybrid Fusion Module (HFM) is designed to\naggregate and distill information to refine spatial information and reconstruct\nhigh-quality video frames. Extensive experiments on two public datasets\ndemonstrate that our method outperforms state-of-the-art methods in efficiency,\nand reduces calculation cost by about 22%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Mengshun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Z/0/1/0/all/0/1\">Zhixiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Prediction of Monocular Depth and Structure using Planar and Parallax Geometry. (arXiv:2207.06351v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06351","description":"<p>Supervised learning depth estimation methods can achieve good performance\nwhen trained on high-quality ground-truth, like LiDAR data. However, LiDAR can\nonly generate sparse 3D maps which causes losing information. Obtaining\nhigh-quality ground-truth depth data per pixel is difficult to acquire. In\norder to overcome this limitation, we propose a novel approach combining\nstructure information from a promising Plane and Parallax geometry pipeline\nwith depth information into a U-Net supervised learning network, which results\nin quantitative and qualitative improvement compared to existing popular\nlearning-based methods. In particular, the model is evaluated on two\nlarge-scale and challenging datasets: KITTI Vision Benchmark and Cityscapes\ndataset and achieve the best performance in terms of relative error. Compared\nwith pure depth supervision models, our model has impressive performance on\ndepth prediction of thin objects and edges, and compared to structure\nprediction baseline, our model performs more robustly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_H/0/1/0/all/0/1\">Hao Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yifan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biber_M/0/1/0/all/0/1\">Maximilian Biber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingchuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burschka_D/0/1/0/all/0/1\">Darius Burschka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Radar Image Reconstruction from Raw ADC Data using Parametric Variational Autoencoder with Domain Adaptation. (arXiv:2207.06379v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06379","description":"<p>This paper presents a parametric variational autoencoder-based human target\ndetection and localization framework working directly with the raw\nanalog-to-digital converter data from the frequency modulated continous wave\nradar. We propose a parametrically constrained variational autoencoder, with\nresidual and skip connections, capable of generating the clustered and\nlocalized target detections on the range-angle image. Furthermore, to\ncircumvent the problem of training the proposed neural network on all possible\nscenarios using real radar data, we propose domain adaptation strategies\nwhereby we first train the neural network using ray tracing based model data\nand then adapt the network to work on real sensor data. This strategy ensures\nbetter generalization and scalability of the proposed neural network even\nthough it is trained with limited radar data. We demonstrate the superior\ndetection and localization performance of our proposed solution compared to the\nconventional signal processing pipeline and earlier state-of-art deep U-Net\narchitecture with range-doppler images as inputs\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stephan_M/0/1/0/all/0/1\">Michael Stephan</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Stadelmayer_T/0/1/0/all/0/1\">Thomas Stadelmayer</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Santra_A/0/1/0/all/0/1\">Avik Santra</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_G/0/1/0/all/0/1\">Georg Fischer</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Weigel_R/0/1/0/all/0/1\">Robert Weigel</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lurz_F/0/1/0/all/0/1\">Fabian Lurz</a> (1) ((1) Friedrich-Alexander-University Erlangen-Nuremberg, (2) Infineon Technologies AG)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A General Framework for Partial to Full Image Registration. (arXiv:2207.06387v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06387","description":"<p>Image registration is a research field in which images must be compared and\naligned independently of the point of view or camera characteristics. In some\napplications (such as forensic biometrics, satellite photography or outdoor\nscene identification) classical image registration systems fail due to one of\nthe images compared represents a tiny piece of the other image. For instance,\nin forensics palmprint recognition, it is usual to find only a small piece of\nthe palmprint, but in the database, the whole palmprint has been enrolled. The\nmain reason of the poor behaviour of classical image registration methods is\nthe gap between the amounts of salient points of both images, which is related\nto the number of points to be considered as outliers. Usually, the difficulty\nof finding a good match increases when the image that represents the tiny part\nof the scene has been drastically rotated. Again, in the case of palmprint\nforensics, it is difficult to decide a priori the orientation of the found tiny\npalmprint image. We present a rotation invariant registration method that\nexplicitly considers that the image to be matched is a small piece of a larger\nimage. We have experimentally validated our method in two different scenarios;\npalmprint identification and outdoor image registration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Garcia_C/0/1/0/all/0/1\">Carlos Francisco Moreno-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serratosa_F/0/1/0/all/0/1\">Francesc Serratosa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Water Surface Patch Classification Using Mixture Augmentation for River Scum Index. (arXiv:2207.06388v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06388","description":"<p>Urban rivers provide a water environment that influences residential living.\nRiver surface monitoring has become crucial for making decisions about where to\nprioritize cleaning and when to automatically start the cleaning treatment. We\nfocus on the organic mud, or \"scum\" that accumulates on the river's surface and\ngives it its peculiar odor and external economic effects on the landscape.\nBecause of its feature of a sparsely distributed and unstable pattern of\norganic shape, automating the monitoring has proved difficult. We propose a\npatch classification pipeline to detect scum features on the river surface\nusing mixture image augmentation to increase the diversity between the scum\nfloating on the river and the entangled background on the river surface\nreflected by nearby structures like buildings, bridges, poles, and barriers.\nFurthermore, we propose a scum index covered on rivers to help monitor worse\ngrade online, collecting floating scum and deciding on chemical treatment\npolicies. Finally, we show how to use our method on a time series dataset with\nframes every ten minutes recording river scum events over several days. We\ndiscuss the value of our pipeline and its experimental findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yasuno_T/0/1/0/all/0/1\">Takato Yasuno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okano_M/0/1/0/all/0/1\">Masahiro Okano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goto_S/0/1/0/all/0/1\">Sanae Goto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_J/0/1/0/all/0/1\">Junichiro Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amakata_M/0/1/0/all/0/1\">Masazumi Amakata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PyMAF-X: Towards Well-aligned Full-body Model Regression from Monocular Images. (arXiv:2207.06400v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06400","description":"<p>Regression-based methods can estimate body, hand, and even full-body models\nfrom monocular images by directly mapping raw pixels to the model parameters in\na feed-forward manner. However, minor deviation in parameters may lead to\nnoticeable misalignment between the estimated meshes and input images,\nespecially in the context of full-body mesh recovery. To address this issue, we\npropose a Pyramidal Mesh Alignment Feedback (PyMAF) loop in our regression\nnetwork for well-aligned human mesh recovery and extend it to PyMAF-X for the\nrecovery of expressive full-body models. The core idea of PyMAF is to leverage\na feature pyramid and rectify the predicted parameters explicitly based on the\nmesh-image alignment status. Specifically, given the currently predicted\nparameters, mesh-aligned evidences will be extracted from finer-resolution\nfeatures accordingly and fed back for parameter rectification. To enhance the\nalignment perception, an auxiliary dense supervision is employed to provide\nmesh-image correspondence guidance while a spatial alignment attention is\nintroduced to enable the awareness of the global contexts for our network. When\nextending PyMAF for full-body mesh recovery, an adaptive integration strategy\nis proposed in PyMAF-X to adjust the elbow-twist rotations, which produces\nnatural wrist poses while maintaining the well-aligned performance of the\npart-specific estimations. The efficacy of our approach is validated on several\nbenchmark datasets for body-only and full-body mesh recovery, where PyMAF and\nPyMAF-X effectively improve the mesh-image alignment and achieve new\nstate-of-the-art results. The project page with code and video results can be\nfound at https://www.liuyebin.com/pymaf-x.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yating Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengcheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_L/0/1/0/all/0/1\">Liang An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Concept Grounding on Neural Fields. (arXiv:2207.06403v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06403","description":"<p>In this paper, we address the challenging problem of 3D concept grounding\n(i.e. segmenting and learning visual concepts) by looking at RGBD images and\nreasoning about paired questions and answers. Existing visual reasoning\napproaches typically utilize supervised methods to extract 2D segmentation\nmasks on which concepts are grounded. In contrast, humans are capable of\ngrounding concepts on the underlying 3D representation of images. However,\ntraditionally inferred 3D representations (e.g., point clouds, voxelgrids, and\nmeshes) cannot capture continuous 3D features flexibly, thus making it\nchallenging to ground concepts to 3D regions based on the language description\nof the object being referred to. To address both issues, we propose to leverage\nthe continuous, differentiable nature of neural fields to segment and learn\nconcepts. Specifically, each 3D coordinate in a scene is represented as a\nhigh-dimensional descriptor. Concept grounding can then be performed by\ncomputing the similarity between the descriptor vector of a 3D coordinate and\nthe vector embedding of a language concept, which enables segmentations and\nconcept learning to be jointly learned on neural fields in a differentiable\nfashion. As a result, both 3D semantic and instance segmentations can emerge\ndirectly from question answering supervision using a set of defined neural\noperators on top of neural fields (e.g., filtering and counting). Experimental\nresults show that our proposed framework outperforms\nunsupervised/language-mediated segmentation models on semantic and instance\nsegmentation tasks, as well as outperforms existing models on the challenging\n3D aware visual reasoning tasks. Furthermore, our framework can generalize well\nto unseen shape categories and real scans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yining Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunru Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open set learning with augmented category by exploiting unlabelled data (open-LACU). (arXiv:2002.01368v4 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2002.01368","description":"<p>Considering the nature of unlabelled data, it is common for partially\nlabelled training datasets to contain samples that belong to novel categories.\nAlthough these so-called observed novel categories exist in the training data,\nthey do not belong to any of the training labels. In contrast, open-sets define\nnovel categories as those unobserved during during training, but present during\ntesting. This research is the first to generalize between observed and\nunobserved novel categories within a new learning policy called open-set\nlearning with augmented category by exploiting unlabeled data or open-LACU.\nThis study conducts a high-level review on novelty detection so to\ndifferentiate between research fields that concern observed novel categories,\nand the research fields that concern unobserved novel categories. Open-LACU is\nthen introduced as a synthesis of the relevant fields to maintain the\nadvantages of each within a single learning policy. Currently, we are\nfinalising the first open-LACU network which will be combined with this\npre-print to be sent for publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Engelbrecht_E/0/1/0/all/0/1\">Emile R. Engelbrecht</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Preez_J/0/1/0/all/0/1\">Johan A. du Preez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Interpretable Microscopic Features of Tumor by Multi-task Adversarial CNNs Improves Generalization. (arXiv:2008.01478v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.01478","description":"<p>Adopting Convolutional Neural Networks (CNNs) in the daily routine of primary\ndiagnosis requires not only near-perfect precision, but also a sufficient\ndegree of generalization to data acquisition shifts and transparency. Existing\nCNN models act as black boxes, not ensuring to the physicians that important\ndiagnostic features are used by the model. Building on top of successfully\nexisting techniques such as multi-task learning, domain adversarial training\nand concept-based interpretability, this paper addresses the challenge of\nintroducing diagnostic factors in the training objectives. Here we show that\nour architecture, by learning end-to-end an uncertainty-based weighting\ncombination of multi-task and adversarial losses, is encouraged to focus on\npathology features such as density and pleomorphism of nuclei, e.g. variations\nin size and appearance, while discarding misleading features such as staining\ndifferences. Our results on breast lymph node tissue show significantly\nimproved generalization in the detection of tumorous tissue, with best average\nAUC 0.89 (0.01) against the baseline AUC 0.86 (0.005). By applying the\ninterpretability technique of linearly probing intermediate representations, we\nalso demonstrate that interpretable pathology features such as nuclei density\nare learned by the proposed CNN architecture, confirming the increased\ntransparency of this model. This result is a starting point towards building\ninterpretable multi-task architectures that are robust to data heterogeneity.\nOur code is available at https://bit.ly/356yQ2u.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Graziani_M/0/1/0/all/0/1\">Mara Graziani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otalora_S/0/1/0/all/0/1\">Sebastian Otalora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchand_Maillet_S/0/1/0/all/0/1\">Stephane Marchand-Maillet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_H/0/1/0/all/0/1\">Henning Muller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrearczyk_V/0/1/0/all/0/1\">Vincent Andrearczyk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Intermediate Flow Estimation for Video Frame Interpolation. (arXiv:2011.06294v12 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.06294","description":"<p>Real-time video frame interpolation (VFI) is very useful in video processing,\nmedia players, and display devices. We propose RIFE, a Real-time Intermediate\nFlow Estimation algorithm for VFI. To realize a high-quality flow-based VFI\nmethod, RIFE uses a neural network named IFNet that can estimate the\nintermediate flows end-to-end with much faster speed. A privileged distillation\nscheme is designed for stable IFNet training and improve the overall\nperformance. RIFE does not rely on pre-trained optical flow models and can\nsupport arbitrary-timestep frame interpolation with the temporal encoding\ninput. Experiments demonstrate that RIFE achieves state-of-the-art performance\non several public benchmarks. Compared with the popular SuperSlomo and DAIN\nmethods, RIFE is 4--27 times faster and produces better results. Furthermore,\nRIFE can be extended to wider applications thanks to temporal encoding. The\ncode is available at https://github.com/megvii-research/ECCV2022-RIFE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhewei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_W/0/1/0/all/0/1\">Wen Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Boxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuchang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review of Generalized Zero-Shot Learning Methods. (arXiv:2011.08641v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.08641","description":"<p>Generalized zero-shot learning (GZSL) aims to train a model for classifying\ndata samples under the condition that some output classes are unknown during\nsupervised learning. To address this challenging task, GZSL leverages semantic\ninformation of the seen (source) and unseen (target) classes to bridge the gap\nbetween both seen and unseen classes. Since its introduction, many GZSL models\nhave been formulated. In this review paper, we present a comprehensive review\non GZSL. Firstly, we provide an overview of GZSL including the problems and\nchallenges. Then, we introduce a hierarchical categorization for the GZSL\nmethods and discuss the representative methods in each category. In addition,\nwe discuss the available benchmark data sets and applications of GZSL, along\nwith a discussion on the research gaps and directions for future\ninvestigations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pourpanah_F/0/1/0/all/0/1\">Farhad Pourpanah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdar_M/0/1/0/all/0/1\">Moloud Abdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuxuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xinlei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_C/0/1/0/all/0/1\">Chee Peng Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi-Zhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Q. M. Jonathan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Label Noise Robust Collaborative Learning Method for Remote Sensing Image Classification. (arXiv:2012.10715v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2012.10715","description":"<p>The development of accurate methods for multi-label classification (MLC) of\nremote sensing (RS) images is one of the most important research topics in RS.\nThe MLC methods based on Convolutional Neural Networks (CNNs) have shown strong\nperformance gains in RS. However, they usually require a high number of\nreliable training images annotated with multiple land-cover class labels.\nCollecting such data is time-consuming and costly. To address this problem, the\npublicly available thematic products, which can include noisy labels, can be\nused to annotate RS images with zero-labeling cost. However, multi-label noise\n(which can be associated with wrong and missing label annotations) can distort\nthe learning process of the MLC methods. To address this problem, we propose a\nnovel multi-label noise robust collaborative learning (RCML) method to\nalleviate the negative effects of multi-label noise during the training phase\nof a CNN model. RCML identifies, ranks and excludes noisy multi-labels in RS\nimages based on three main modules: 1) the discrepancy module; 2) the group\nlasso module; and 3) the swap module. The discrepancy module ensures that the\ntwo networks learn diverse features, while producing the same predictions. The\ntask of the group lasso module is to detect the potentially noisy labels\nassigned to the multi-labeled training images, while the swap module is devoted\nto exchange the ranking information between two networks. Unlike existing\nmethods that make assumptions about the noise distribution, our proposed RCML\ndoes not make any prior assumption about the type of noise in the training set.\nThe experiments conducted on two multi-label RS image archives confirm the\nrobustness of the proposed RCML under extreme multi-label noise rates. Our code\nis publicly available at: <a href=\"http://www.noisy-labels-in-rs.org\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Aksoy_A/0/1/0/all/0/1\">Ahmet Kerem Aksoy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravanbakhsh_M/0/1/0/all/0/1\">Mahdyar Ravanbakhsh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demir_B/0/1/0/all/0/1\">Beg&#xfc;m Demir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vote from the Center: 6 DoF Pose Estimation in RGB-D Images by Radial Keypoint Voting. (arXiv:2104.02527v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02527","description":"<p>We propose a novel keypoint voting scheme based on intersecting spheres, that\nis more accurate than existing schemes and allows for fewer, more disperse\nkeypoints. The scheme is based upon the distance between points, which as a 1D\nquantity can be regressed more accurately than the 2D and 3D vector and offset\nquantities regressed in previous work, yielding more accurate keypoint\nlocalization. The scheme forms the basis of the proposed RCVPose method for 6\nDoF pose estimation of 3D objects in RGB-D data, which is particularly\neffective at handling occlusions. A CNN is trained to estimate the distance\nbetween the 3D point corresponding to the depth mode of each RGB pixel, and a\nset of 3 disperse keypoints defined in the object frame. At inference, a sphere\ncentered at each 3D point is generated, of radius equal to this estimated\ndistance. The surfaces of these spheres vote to increment a 3D accumulator\nspace, the peaks of which indicate keypoint locations. The proposed radial\nvoting scheme is more accurate than previous vector or offset schemes, and is\nrobust to disperse keypoints. Experiments demonstrate RCVPose to be highly\naccurate and competitive, achieving state-of-the-art results on the LINEMOD\n99.7% and YCB-Video 97.2% datasets, notably scoring +4.9% higher 71.1% than\nprevious methods on the challenging Occlusion LINEMOD dataset, and on average\noutperforming all other published results from the BOP benchmark for these 3\ndatasets. Our code is available at <a href=\"http://www.github.com/aaronwool/rcvpose.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yangzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zand_M/0/1/0/all/0/1\">Mohsen Zand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenspan_M/0/1/0/all/0/1\">Michael Greenspan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DnS: Distill-and-Select for Efficient and Accurate Video Indexing and Retrieval. (arXiv:2106.13266v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13266","description":"<p>In this paper, we address the problem of high performance and computationally\nefficient content-based video retrieval in large-scale datasets. Current\nmethods typically propose either: (i) fine-grained approaches employing\nspatio-temporal representations and similarity calculations, achieving high\nperformance at a high computational cost or (ii) coarse-grained approaches\nrepresenting/indexing videos as global vectors, where the spatio-temporal\nstructure is lost, providing low performance but also having low computational\ncost. In this work, we propose a Knowledge Distillation framework, called\nDistill-and-Select (DnS), that starting from a well-performing fine-grained\nTeacher Network learns: a) Student Networks at different retrieval performance\nand computational efficiency trade-offs and b) a Selector Network that at test\ntime rapidly directs samples to the appropriate student to maintain both high\nretrieval performance and high computational efficiency. We train several\nstudents with different architectures and arrive at different trade-offs of\nperformance and efficiency, i.e., speed and storage requirements, including\nfine-grained students that store/index videos using binary representations.\nImportantly, the proposed scheme allows Knowledge Distillation in large,\nunlabelled datasets -- this leads to good students. We evaluate DnS on five\npublic datasets on three different video retrieval tasks and demonstrate a)\nthat our students achieve state-of-the-art performance in several cases and b)\nthat the DnS framework provides an excellent trade-off between retrieval\nperformance, computational speed, and storage space. In specific\nconfigurations, the proposed method achieves similar mAP with the teacher but\nis 20 times faster and requires 240 times less storage space. The collected\ndataset and implementation are publicly available:\nhttps://github.com/mever-team/distill-and-select.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kordopatis_Zilos_G/0/1/0/all/0/1\">Giorgos Kordopatis-Zilos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1\">Christos Tzelepis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Symeon Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1\">Ioannis Kompatsiaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1\">Ioannis Patras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A survey on computational spectral reconstruction methods from RGB to hyperspectral imaging. (arXiv:2106.15944v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.15944","description":"<p>Hyperspectral imaging enables versatile applications due to its competence in\ncapturing abundant spatial and spectral information, which are crucial for\nidentifying substances. However, the devices for acquiring hyperspectral images\nare expensive and complicated. Therefore, many alternative spectral imaging\nmethods have been proposed by directly reconstructing the hyperspectral\ninformation from lower-cost, more available RGB images. We present a thorough\ninvestigation of these state-of-the-art spectral reconstruction methods from\nthe widespread RGB images. A systematic study and comparison of more than 25\nmethods has revealed that most of the data-driven deep learning methods are\nsuperior to prior-based methods in terms of reconstruction accuracy and quality\ndespite lower speeds. This comprehensive review can serve as a fruitful\nreference source for peer researchers, thus further inspiring future\ndevelopment directions in related domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jingang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_R/0/1/0/all/0/1\">Runmu Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_W/0/1/0/all/0/1\">Wenqi Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heide_F/0/1/0/all/0/1\">Felix Heide</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nie_Y/0/1/0/all/0/1\">Yunfeng Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A data-centric approach for improving ambiguous labels with combined semi-supervised classification and clustering. (arXiv:2106.16209v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.16209","description":"<p>Consistently high data quality is essential for the development of novel loss\nfunctions and architectures in the field of deep learning. The existence of\nsuch data and labels is usually presumed, while acquiring high-quality datasets\nis still a major issue in many cases. In real-world datasets we often encounter\nambiguous labels due to subjective annotations by annotators. In our\ndata-centric approach, we propose a method to relabel such ambiguous labels\ninstead of implementing the handling of this issue in a neural network. A hard\nclassification is by definition not enough to capture the real-world ambiguity\nof the data. Therefore, we propose our method \"Data-Centric Classification &amp;\nClustering (DC3)\" which combines semi-supervised classification and clustering.\nIt automatically estimates the ambiguity of an image and performs a\nclassification or clustering depending on that ambiguity. DC3 is general in\nnature so that it can be used in addition to many Semi-Supervised Learning\n(SSL) algorithms. On average, this results in a 7.6% better F1-Score for\nclassifications and 7.9% lower inner distance of clusters across multiple\nevaluated SSL algorithms and datasets. Most importantly, we give a\nproof-of-concept that the classifications and clusterings from DC3 are\nbeneficial as proposals for the manual refinement of such ambiguous labels.\nOverall, a combination of SSL with our method DC3 can lead to better handling\nof ambiguous labels during the annotation process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmarje_L/0/1/0/all/0/1\">Lars Schmarje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santarossa_M/0/1/0/all/0/1\">Monty Santarossa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroder_S/0/1/0/all/0/1\">Simon-Martin Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelenka_C/0/1/0/all/0/1\">Claudius Zelenka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiko_R/0/1/0/all/0/1\">Rainer Kiko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stracke_J/0/1/0/all/0/1\">Jenny Stracke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volkmann_N/0/1/0/all/0/1\">Nina Volkmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1\">Reinhard Koch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12636","description":"<p>Detection transformers have recently shown promising object detection results\nand attracted increasing attention. However, how to develop effective domain\nadaptation techniques to improve its cross-domain performance remains\nunexplored and unclear. In this paper, we delve into this topic and empirically\nfind that direct feature distribution alignment on the CNN backbone only brings\nlimited improvements, as it does not guarantee domain-invariant sequence\nfeatures in the transformer for prediction. To address this issue, we propose a\nnovel Sequence Feature Alignment (SFA) method that is specially designed for\nthe adaptation of detection transformers. Technically, SFA consists of a domain\nquery-based feature alignment (DQFA) module and a token-wise feature alignment\n(TDA) module. In DQFA, a novel domain query is used to aggregate and align\nglobal context from the token sequence of both domains. DQFA reduces the domain\ndiscrepancy in global feature representations and object relations when\ndeploying in the transformer encoder and decoder, respectively. Meanwhile, TDA\naligns token features in the sequence from both domains, which reduces the\ndomain gaps in local and instance-level feature representations in the\ntransformer encoder and decoder, respectively. Besides, a novel bipartite\nmatching consistency loss is proposed to enhance the feature discriminability\nfor robust object detection. Experiments on three challenging benchmarks show\nthat SFA outperforms state-of-the-art domain adaptive object detection methods.\nCode has been made available at: https://github.com/encounter1997/SFA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Fengxiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RigNet: Repetitive Image Guided Network for Depth Completion. (arXiv:2107.13802v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.13802","description":"<p>Depth completion deals with the problem of recovering dense depth maps from\nsparse ones, where color images are often used to facilitate this task. Recent\napproaches mainly focus on image guided learning frameworks to predict dense\ndepth. However, blurry guidance in the image and unclear structure in the depth\nstill impede the performance of the image guided frameworks. To tackle these\nproblems, we explore a repetitive design in our image guided network to\ngradually and sufficiently recover depth values. Specifically, the repetition\nis embodied in both the image guidance branch and depth generation branch. In\nthe former branch, we design a repetitive hourglass network to extract\ndiscriminative image features of complex environments, which can provide\npowerful contextual instruction for depth prediction. In the latter branch, we\nintroduce a repetitive guidance module based on dynamic convolution, in which\nan efficient convolution factorization is proposed to simultaneously reduce its\ncomplexity and progressively model high-frequency structures. Extensive\nexperiments show that our method achieves superior or competitive results on\nKITTI benchmark and NYUv2 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"View Vertically: A Hierarchical Network for Trajectory Prediction via Fourier Spectrums. (arXiv:2110.07288v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07288","description":"<p>Understanding and forecasting future trajectories of agents are critical for\nbehavior analysis, robot navigation, autonomous cars, and other related\napplications. Previous methods mostly treat trajectory prediction as time\nsequence generation. Different from them, this work studies agents'\ntrajectories in a \"vertical\" view, i.e., modeling and forecasting trajectories\nfrom the spectral domain. Different frequency bands in the trajectory spectrums\ncould hierarchically reflect agents' motion preferences at different scales.\nThe low-frequency and high-frequency portions could represent their coarse\nmotion trends and fine motion variations, respectively. Accordingly, we propose\na hierarchical network V$^2$-Net, which contains two sub-networks, to\nhierarchically model and predict agents' trajectories with trajectory\nspectrums. The coarse-level keypoints estimation sub-network first predicts the\n\"minimal\" spectrums of agents' trajectories on several \"key\" frequency\nportions. Then the fine-level spectrum interpolation sub-network interpolates\nthe spectrums to reconstruct the final predictions. Experimental results\ndisplay the competitiveness and superiority of V$^2$-Net on both ETH-UCY\nbenchmark and the Stanford Drone Dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Conghao Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Beihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Ziming Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qinmu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Wei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qiong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinge You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Vocabulary Object Detection with Pseudo Bounding-Box Labels. (arXiv:2111.09452v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09452","description":"<p>Despite great progress in object detection, most existing methods work only\non a limited set of object categories, due to the tremendous human effort\nneeded for bounding-box annotations of training data. To alleviate the problem,\nrecent open vocabulary and zero-shot detection methods attempt to detect novel\nobject categories beyond those seen during training. They achieve this goal by\ntraining on a pre-defined base categories to induce generalization to novel\nobjects. However, their potential is still constrained by the small set of base\ncategories available for training. To enlarge the set of base classes, we\npropose a method to automatically generate pseudo bounding-box annotations of\ndiverse objects from large-scale image-caption pairs. Our method leverages the\nlocalization ability of pre-trained vision-language models to generate pseudo\nbounding-box labels and then directly uses them for training object detectors.\nExperimental results show that our method outperforms the state-of-the-art open\nvocabulary detector by 8% AP on COCO novel categories, by 6.3% AP on PASCAL\nVOC, by 2.3% AP on Objects365 and by 2.8% AP on LVIS. Code is available at\nhttps://github.com/salesforce/PB-OVD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingfei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1\">Chen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niebles_J/0/1/0/all/0/1\">Juan Carlos Niebles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Clustering Learning for Large-scale Unsupervised Person Re-identification. (arXiv:2111.10032v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10032","description":"<p>Unsupervised Person Re-identification (U-ReID) with pseudo labeling recently\nreaches a competitive performance compared to fully-supervised ReID methods\nbased on modern clustering algorithms. However, such clustering-based scheme\nbecomes computationally prohibitive for large-scale datasets. How to\nefficiently leverage endless unlabeled data with limited computing resources\nfor better U-ReID is under-explored. In this paper, we make the first attempt\nto the large-scale U-ReID and propose a \"small data for big task\" paradigm\ndubbed Meta Clustering Learning (MCL). MCL only pseudo-labels a subset of the\nentire unlabeled data via clustering to save computing for the first-phase\ntraining. After that, the learned cluster centroids, termed as meta-prototypes\nin our MCL, are regarded as a proxy annotator to softly annotate the rest\nunlabeled data for further polishing the model. To alleviate the potential\nnoisy labeling issue in the polishment phase, we enforce two well-designed loss\nconstraints to promise intra-identity consistency and inter-identity strong\ncorrelation. For multiple widely-used U-ReID benchmarks, our method\nsignificantly saves computational cost while achieving a comparable or even\nbetter performance compared to prior works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AVA-AVD: Audio-Visual Speaker Diarization in the Wild. (arXiv:2111.14448v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14448","description":"<p>Audio-visual speaker diarization aims at detecting \"who spoke when\" using\nboth auditory and visual signals. Existing audio-visual diarization datasets\nare mainly focused on indoor environments like meeting rooms or news studios,\nwhich are quite different from in-the-wild videos in many scenarios such as\nmovies, documentaries, and audience sitcoms. To develop diarization methods for\nthese challenging videos, we create the AVA Audio-Visual Diarization (AVA-AVD)\ndataset. Our experiments demonstrate that adding AVA-AVD into training set can\nproduce significantly better diarization models for in-the-wild videos despite\nthat the data is relatively small. Moreover, this benchmark is challenging due\nto the diverse scenes, complicated acoustic conditions, and completely\noff-screen speakers. As a first step towards addressing the challenges, we\ndesign the Audio-Visual Relation Network (AVR-Net) which introduces a simple\nyet effective modality mask to capture discriminative information based on face\nvisibility. Experiments show that our method not only can outperform\nstate-of-the-art methods but is more robust as varying the ratio of off-screen\nspeakers. Our data and code has been made publicly available at\nhttps://github.com/showlab/AVA-AVD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_E/0/1/0/all/0/1\">Eric Zhongcong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zeyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsutsui_S/0/1/0/all/0/1\">Satoshi Tsutsui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Mang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks. (arXiv:2112.03227v4 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2112.03227","description":"<p>General-purpose robots coexisting with humans in their environment must learn\nto relate human language to their perceptions and actions to be useful in a\nrange of daily tasks. Moreover, they need to acquire a diverse repertoire of\ngeneral-purpose skills that allow composing long-horizon tasks by following\nunconstrained language instructions. In this paper, we present CALVIN\n(Composing Actions from Language and Vision), an open-source simulated\nbenchmark to learn long-horizon language-conditioned tasks. Our aim is to make\nit possible to develop agents that can solve many robotic manipulation tasks\nover a long horizon, from onboard sensors, and specified only via human\nlanguage. CALVIN tasks are more complex in terms of sequence length, action\nspace, and language than existing vision-and-language task datasets and\nsupports flexible specification of sensor suites. We evaluate the agents in\nzero-shot to novel language instructions and to novel environments and objects.\nWe show that a baseline model based on multi-context imitation learning\nperforms poorly on CALVIN, suggesting that there is significant room for\ndeveloping innovative agents that learn to relate human language to their world\nmodels with this benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mees_O/0/1/0/all/0/1\">Oier Mees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermann_L/0/1/0/all/0/1\">Lukas Hermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosete_Beas_E/0/1/0/all/0/1\">Erick Rosete-Beas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Instance and Task-Aware Dynamic Kernels for Few Shot Learning. (arXiv:2112.03494v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03494","description":"<p>Learning and generalizing to novel concepts with few samples (Few-Shot\nLearning) is still an essential challenge to real-world applications. A\nprinciple way of achieving few-shot learning is to realize a model that can\nrapidly adapt to the context of a given task. Dynamic networks have been shown\ncapable of learning content-adaptive parameters efficiently, making them\nsuitable for few-shot learning. In this paper, we propose to learn the dynamic\nkernels of a convolution network as a function of the task at hand, enabling\nfaster generalization. To this end, we obtain our dynamic kernels based on the\nentire task and each sample and develop a mechanism further conditioning on\neach individual channel and position independently. This results in dynamic\nkernels that simultaneously attend to the global information whilst also\nconsidering minuscule details available. We empirically show that our model\nimproves performance on few-shot classification and detection tasks, achieving\na tangible improvement over several baseline models. This includes\nstate-of-the-art results on 4 few-shot classification benchmarks:\nmini-ImageNet, tiered-ImageNet, CUB and FC100 and competitive results on a\nfew-shot detection dataset: MS COCO-PASCAL-VOC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rongkai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1\">Pengfei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avraham_G/0/1/0/all/0/1\">Gil Avraham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_Y/0/1/0/all/0/1\">Yan Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tianyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drummond_T/0/1/0/all/0/1\">Tom Drummond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1\">Mehrtash Harandi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polarimetric Pose Prediction. (arXiv:2112.03810v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03810","description":"<p>Light has many properties that vision sensors can passively measure.\nColour-band separated wavelength and intensity are arguably the most commonly\nused for monocular 6D object pose estimation. This paper explores how\ncomplementary polarisation information, i.e. the orientation of light wave\noscillations, influences the accuracy of pose predictions. A hybrid model that\nleverages physical priors jointly with a data-driven learning strategy is\ndesigned and carefully tested on objects with different levels of photometric\ncomplexity. Our design significantly improves the pose accuracy compared to\nstate-of-the-art photometric approaches and enables object pose estimation for\nhighly reflective and transparent objects. A new multi-modal instance-level 6D\nobject pose dataset with highly accurate pose annotations for multiple objects\nwith varying photometric complexity is introduced as a benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Daoyi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruhkamp_P/0/1/0/all/0/1\">Patrick Ruhkamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skobleva_I/0/1/0/all/0/1\">Iuliia Skobleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wysock_M/0/1/0/all/0/1\">Magdalena Wysock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">HyunJun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guridi_A/0/1/0/all/0/1\">Arturo Guridi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Hash Distillation for Image Retrieval. (arXiv:2112.08816v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08816","description":"<p>In hash-based image retrieval systems, degraded or transformed inputs usually\ngenerate different codes from the original, deteriorating the retrieval\naccuracy. To mitigate this issue, data augmentation can be applied during\ntraining. However, even if augmented samples of an image are similar in real\nfeature space, the quantization can scatter them far away in Hamming space.\nThis results in representation discrepancies that can impede training and\ndegrade performance. In this work, we propose a novel self-distilled hashing\nscheme to minimize the discrepancy while exploiting the potential of augmented\ndata. By transferring the hash knowledge of the weakly-transformed samples to\nthe strong ones, we make the hash code insensitive to various transformations.\nWe also introduce hash proxy-based similarity learning and binary cross\nentropy-based quantization loss to provide fine quality hash codes. Ultimately,\nwe construct a deep hashing framework that not only improves the existing deep\nhashing approaches, but also achieves the state-of-the-art retrieval results.\nExtensive experiments are conducted and confirm the effectiveness of our work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Young Kyun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_G/0/1/0/all/0/1\">Geonmo Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_B/0/1/0/all/0/1\">Byungsoo Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_I/0/1/0/all/0/1\">Isaac Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_N/0/1/0/all/0/1\">Nam Ik Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Navigating to Objects in Unseen Environments by Distance Prediction. (arXiv:2202.03735v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2202.03735","description":"<p>Object Goal Navigation (ObjectNav) task is to navigate an agent to an object\ncategory in unseen environments without a pre-built map. In this paper, we\nsolve this task by predicting the distance to the target using\nsemantically-related objects as cues. Based on the estimated distance to the\ntarget object, our method directly choose optimal mid-term goals that are more\nlikely to have a shorter path to the target. Specifically, based on the learned\nknowledge, our model takes a bird's-eye view semantic map as input, and\nestimates the path length from the frontier map cells to the target object.\nWith the estimated distance map, the agent could simultaneously explore the\nenvironment and navigate to the target objects based on a simple human-designed\nstrategy. Empirical results in visually realistic simulation environments show\nthat the proposed method outperforms a wide range of baselines on success rate\nand efficiency. Real-robot experiment also demonstrates that our method\ngeneralizes well to the real world. Video at\nhttps://www.youtube.com/watch?v=R79pWVGFKS4\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minzhao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Binglei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tao Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Atlas Segmentation and Spatial Alignment of the Human Embryo in First Trimester 3D Ultrasound. (arXiv:2202.06599v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.06599","description":"<p>Segmentation and spatial alignment of ultrasound (US) imaging data acquired\nin the in first trimester are crucial for monitoring human embryonic growth and\ndevelopment throughout this crucial period of life. Current approaches are\neither manual or semi-automatic and are therefore very time-consuming and prone\nto errors. To automate these tasks, we propose a multi-atlas framework for\nautomatic segmentation and spatial alignment of the embryo using deep learning\nwith minimal supervision. Our framework learns to register the embryo to an\natlas, which consists of the US images acquired at a range of gestational age\n(GA), segmented and spatially aligned to a predefined standard orientation.\nFrom this, we can derive the segmentation of the embryo and put the embryo in\nstandard orientation. US images acquired at 8+0 till 12+6 weeks GA were used\nand eight subjects were selected as atlas. We evaluated different fusion\nstrategies to incorporate multiple atlases: 1) training the framework using\natlas images from a single subject, 2) training the framework with data of all\navailable atlases and 3) ensembling of the frameworks trained per subject. To\nevaluate the performance, we calculated the Dice score over the test set. We\nfound that training the framework using all available atlases outperformed\nensembling and gave similar results compared to the best of all frameworks\ntrained on a single subject. Furthermore, we found that selecting images from\nthe four atlases closest in GA out of all available atlases, regardless of the\nindividual quality, gave the best results with a median Dice score of 0.72. We\nconclude that our framework can accurately segment and spatially align the\nembryo in first trimester 3D US images and is robust for the variation in\nquality that existed in the available atlases. Our code is publicly available\nat: https://github.com/wapbastiaansen/multi-atlas-seg-reg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bastiaansen_W/0/1/0/all/0/1\">W.A.P. Bastiaansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rousian_M/0/1/0/all/0/1\">M. Rousian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Steegers_Theunissen_R/0/1/0/all/0/1\">R.P.M. Steegers-Theunissen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niessen_W/0/1/0/all/0/1\">W.J. Niessen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koning_A/0/1/0/all/0/1\">A.H.J. Koning</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1\">S. Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Slicing Aided Hyper Inference and Fine-tuning for Small Object Detection. (arXiv:2202.06934v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06934","description":"<p>Detection of small objects and objects far away in the scene is a major\nchallenge in surveillance applications. Such objects are represented by small\nnumber of pixels in the image and lack sufficient details, making them\ndifficult to detect using conventional detectors. In this work, an open-source\nframework called Slicing Aided Hyper Inference (SAHI) is proposed that provides\na generic slicing aided inference and fine-tuning pipeline for small object\ndetection. The proposed technique is generic in the sense that it can be\napplied on top of any available object detector without any fine-tuning.\nExperimental evaluations, using object detection baselines on the Visdrone and\nxView aerial object detection datasets show that the proposed inference method\ncan increase object detection AP by 6.8%, 5.1% and 5.3% for FCOS, VFNet and\nTOOD detectors, respectively. Moreover, the detection accuracy can be further\nincreased with a slicing aided fine-tuning, resulting in a cumulative increase\nof 12.7%, 13.4% and 14.5% AP in the same order. Proposed technique has been\nintegrated with Detectron2, MMDetection and YOLOv5 models and it is publicly\navailable at https://github.com/obss/sahi.git .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akyon_F/0/1/0/all/0/1\">Fatih Cagatay Akyon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altinuc_S/0/1/0/all/0/1\">Sinan Onur Altinuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Temizel_A/0/1/0/all/0/1\">Alptekin Temizel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-scale Hybrid Vision Transformer for Learning Gastric Cancer Histology. (arXiv:2202.08510v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.08510","description":"<p>Gastric endoscopic screening is an effective way to decide appropriate\ngastric cancer (GC) treatment at an early stage, reducing GC-associated\nmortality rate. Although artificial intelligence (AI) has brought a great\npromise to assist pathologist to screen digitalized whole slide images,\nexisting AI systems are limited in fine-grained cancer subclassifications and\nhave little usability in planning cancer treatment. We propose a practical AI\nsystem that enables five subclassifications of GC pathology, which can be\ndirectly matched to general GC treatment guidance. The AI system is designed to\nefficiently differentiate multi-classes of GC through multi-scale\nself-attention mechanism using 2-stage hybrid Vision Transformer (ViT)\nnetworks, by mimicking the way how human pathologists understand histology. The\nAI system demonstrates reliable diagnostic performance by achieving\nclass-average sensitivity of above 0.85 on a total of 1,212 slides from\nmulticentric cohort. Furthermore, AI-assisted pathologists show significantly\nimproved diagnostic sensitivity by 12% in addition to 18% reduced screening\ntime compared to human pathologists. Our results demonstrate that AI-assisted\ngastric endoscopic screening has a great potential for providing presumptive\npathologic opinion and appropriate cancer treatment of gastric cancer in\npractical clinical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Oh_Y/0/1/0/all/0/1\">Yujin Oh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bae_G/0/1/0/all/0/1\">Go Eun Bae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1\">Kyung-Hee Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeo_M/0/1/0/all/0/1\">Min-Kyung Yeo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delving Deep into One-Shot Skeleton-based Action Recognition with Diverse Occlusions. (arXiv:2202.11423v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11423","description":"<p>Occlusions are universal disruptions constantly present in the real world.\nEspecially for sparse representations, such as human skeletons, a few occluded\npoints might destroy the geometrical and temporal continuity critically\naffecting the results. Yet, the research of data-scarce recognition from\nskeleton sequences, such as one-shot action recognition, does not explicitly\nconsider occlusions despite their everyday pervasiveness. In this work, we\nexplicitly tackle body occlusions for Skeleton-based One-shot Action\nRecognition (SOAR). We mainly consider two occlusion variants: 1) random\nocclusions and 2) more realistic occlusions caused by diverse everyday objects,\nwhich we generate by projecting the existing IKEA 3D furniture models into the\ncamera coordinate system of the 3D skeletons. We leverage the proposed pipeline\nto blend out portions of skeleton sequences of the three popular action\nrecognition datasets (NTU-120, NTU-60 and Toyota Smart Home) and formalize the\nfirst benchmark for SOAR from partially occluded body poses. This is the first\nbenchmark which considers occlusions for data-scarce action recognition.\nAnother key property of our benchmark are the more realistic occlusions\ngenerated by everyday objects, as even in standard recognition from 3D\nskeletons, only randomly missing joints were considered. We re-evaluate\nstate-of-the-art frameworks in the light of this new task and further introduce\nTrans4SOAR, a new transformer-based model which leverages three data streams\nand mixed attention fusion mechanism to alleviate the adverse effects caused by\nocclusions. While our experiments demonstrate a clear decline in accuracy with\nmissing skeleton portions, this effect is smaller with Trans4SOAR, which\noutperforms other architectures on all datasets. Trans4SOAR additionally yields\nstate-of-the-art in the standard SOAR, surpassing the best published approach\nby 2.85% on NTU-120.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1\">Alina Roitberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Adversarial Robustness of Adaptive Test-time Defenses. (arXiv:2202.13711v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.13711","description":"<p>Adaptive defenses, which optimize at test time, promise to improve\nadversarial robustness. We categorize such adaptive test-time defenses, explain\ntheir potential benefits and drawbacks, and evaluate a representative variety\nof the latest adaptive defenses for image classification. Unfortunately, none\nsignificantly improve upon static defenses when subjected to our careful case\nstudy evaluation. Some even weaken the underlying static model while\nsimultaneously increasing inference computation. While these results are\ndisappointing, we still believe that adaptive test-time defenses are a\npromising avenue of research and, as such, we provide recommendations for their\nthorough evaluation. We extend the checklist of Carlini et al. (2019) by\nproviding concrete steps specific to adaptive defenses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Croce_F/0/1/0/all/0/1\">Francesco Croce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gowal_S/0/1/0/all/0/1\">Sven Gowal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunner_T/0/1/0/all/0/1\">Thomas Brunner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1\">Evan Shelhamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1\">Matthias Hein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cemgil_T/0/1/0/all/0/1\">Taylan Cemgil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physical Backdoor Attacks to Lane Detection Systems in Autonomous Driving. (arXiv:2203.00858v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00858","description":"<p>Modern autonomous vehicles adopt state-of-the-art DNN models to interpret the\nsensor data and perceive the environment. However, DNN models are vulnerable to\ndifferent types of adversarial attacks, which pose significant risks to the\nsecurity and safety of the vehicles and passengers. One prominent threat is the\nbackdoor attack, where the adversary can compromise the DNN model by poisoning\nthe training samples. Although lots of effort has been devoted to the\ninvestigation of the backdoor attack to conventional computer vision tasks, its\npracticality and applicability to the autonomous driving scenario is rarely\nexplored, especially in the physical world.\n</p>\n<p>In this paper, we target the lane detection system, which is an indispensable\nmodule for many autonomous driving tasks, e.g., navigation, lane switching. We\ndesign and realize the first physical backdoor attacks to such system. Our\nattacks are comprehensively effective against different types of lane detection\nalgorithms. Specifically, we introduce two attack methodologies\n(poison-annotation and clean-annotation) to generate poisoned samples. With\nthose samples, the trained lane detection model will be infected with the\nbackdoor, and can be activated by common objects (e.g., traffic cones) to make\nwrong detections, leading the vehicle to drive off the road or onto the\nopposite lane. Extensive evaluations on public datasets and physical autonomous\nvehicles demonstrate that our backdoor attacks are effective, stealthy and\nrobust against various defense solutions. Our codes and experimental videos can\nbe found in https://sites.google.com/view/lane-detection-attack/lda.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xingshuo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guowen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuehuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering. (arXiv:2203.03949v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03949","description":"<p>Finding accurate correspondences among different views is the Achilles' heel\nof unsupervised Multi-View Stereo (MVS). Existing methods are built upon the\nassumption that corresponding pixels share similar photometric features.\nHowever, multi-view images in real scenarios observe non-Lambertian surfaces\nand experience occlusions. In this work, we propose a novel approach with\nneural rendering (RC-MVSNet) to solve such ambiguity issues of correspondences\namong views. Specifically, we impose a depth rendering consistency loss to\nconstrain the geometry features close to the object surface to alleviate\nocclusions. Concurrently, we introduce a reference view synthesis loss to\ngenerate consistent supervision, even for non-Lambertian surfaces. Extensive\nexperiments on DTU and Tanks\\&amp;Temples benchmarks demonstrate that our RC-MVSNet\napproach achieves state-of-the-art performance over unsupervised MVS frameworks\nand competitive performance to many supervised methods.The code is released at\nhttps://github.com/Boese0601/RC-MVSNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Di Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozic_A/0/1/0/all/0/1\">Alja&#x17e; Bo&#x17e;i&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qingsong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingcong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susstrunk_S/0/1/0/all/0/1\">Sabine S&#xfc;sstrunk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Detection as Probabilistic Set Prediction. (arXiv:2203.07980v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07980","description":"<p>Accurate uncertainty estimates are essential for deploying deep object\ndetectors in safety-critical systems. The development and evaluation of\nprobabilistic object detectors have been hindered by shortcomings in existing\nperformance measures, which tend to involve arbitrary thresholds or limit the\ndetector's choice of distributions. In this work, we propose to view object\ndetection as a set prediction task where detectors predict the distribution\nover the set of objects. Using the negative log-likelihood for random finite\nsets, we present a proper scoring rule for evaluating and training\nprobabilistic object detectors. The proposed method can be applied to existing\nprobabilistic detectors, is free from thresholds, and enables fair comparison\nbetween architectures. Three different types of detectors are evaluated on the\nCOCO dataset. Our results indicate that the training of existing detectors is\noptimized toward non-probabilistic metrics. We hope to encourage the\ndevelopment of new object detectors that can accurately estimate their own\nuncertainty. Code available at https://github.com/georghess/pmb-nll.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hess_G/0/1/0/all/0/1\">Georg Hess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_C/0/1/0/all/0/1\">Christoffer Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svensson_L/0/1/0/all/0/1\">Lennart Svensson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers. (arXiv:2203.17270v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.17270","description":"<p>3D visual perception tasks, including 3D detection and map segmentation based\non multi-camera images, are essential for autonomous driving systems. In this\nwork, we present a new framework termed BEVFormer, which learns unified BEV\nrepresentations with spatiotemporal transformers to support multiple autonomous\ndriving perception tasks. In a nutshell, BEVFormer exploits both spatial and\ntemporal information by interacting with spatial and temporal space through\npredefined grid-shaped BEV queries. To aggregate spatial information, we design\nspatial cross-attention that each BEV query extracts the spatial features from\nthe regions of interest across camera views. For temporal information, we\npropose temporal self-attention to recurrently fuse the history BEV\ninformation. Our approach achieves the new state-of-the-art 56.9\\% in terms of\nNDS metric on the nuScenes \\texttt{test} set, which is 9.0 points higher than\nprevious best arts and on par with the performance of LiDAR-based baselines. We\nfurther show that BEVFormer remarkably improves the accuracy of velocity\nestimation and recall of objects under low visibility conditions. The code is\navailable at \\url{https://github.com/zhiqi-li/BEVFormer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sima_C/0/1/0/all/0/1\">Chonghao Sima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qiao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dress Code: High-Resolution Multi-Category Virtual Try-On. (arXiv:2204.08532v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08532","description":"<p>Image-based virtual try-on strives to transfer the appearance of a clothing\nitem onto the image of a target person. Prior work focuses mainly on upper-body\nclothes (e.g. t-shirts, shirts, and tops) and neglects full-body or lower-body\nitems. This shortcoming arises from a main factor: current publicly available\ndatasets for image-based virtual try-on do not account for this variety, thus\nlimiting progress in the field. To address this deficiency, we introduce Dress\nCode, which contains images of multi-category clothes. Dress Code is more than\n3x larger than publicly available datasets for image-based virtual try-on and\nfeatures high-resolution paired images (1024x768) with front-view, full-body\nreference models. To generate HD try-on images with high visual quality and\nrich in details, we propose to learn fine-grained discriminating features.\nSpecifically, we leverage a semantic-aware discriminator that makes predictions\nat pixel-level instead of image- or patch-level. Extensive experimental\nevaluation demonstrates that the proposed approach surpasses the baselines and\nstate-of-the-art competitors in terms of visual quality and quantitative\nresults. The Dress Code dataset is publicly available at\nhttps://github.com/aimagelab/dress-code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morelli_D/0/1/0/all/0/1\">Davide Morelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fincato_M/0/1/0/all/0/1\">Matteo Fincato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landi_F/0/1/0/all/0/1\">Federico Landi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cesari_F/0/1/0/all/0/1\">Fabio Cesari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Negatives in Contrastive Learning for Unpaired Image-to-Image Translation. (arXiv:2204.11018v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.11018","description":"<p>Unpaired image-to-image translation aims to find a mapping between the source\ndomain and the target domain. To alleviate the problem of the lack of\nsupervised labels for the source images, cycle-consistency based methods have\nbeen proposed for image structure preservation by assuming a reversible\nrelationship between unpaired images. However, this assumption only uses\nlimited correspondence between image pairs. Recently, contrastive learning (CL)\nhas been used to further investigate the image correspondence in unpaired image\ntranslation by using patch-based positive/negative learning. Patch-based\ncontrastive routines obtain the positives by self-similarity computation and\nrecognize the rest patches as negatives. This flexible learning paradigm\nobtains auxiliary contextualized information at a low cost. As the negatives\nown an impressive sample number, with curiosity, we make an investigation based\non a question: are all negatives necessary for feature contrastive learning?\nUnlike previous CL approaches that use negatives as much as possible, in this\npaper, we study the negatives from an information-theoretic perspective and\nintroduce a new negative Pruning technology for Unpaired image-to-image\nTranslation (PUT) by sparsifying and ranking the patches. The proposed\nalgorithm is efficient, flexible and enables the model to learn essential\ninformation between corresponding patches stably. By putting quality over\nquantity, only a few negative patches are required to achieve better results.\nLastly, we validate the superiority, stability, and versatility of our model\nthrough comparative experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yupei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianshui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yongyi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guangping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yukai Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PointInst3D: Segmenting 3D Instances by Points. (arXiv:2204.11402v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.11402","description":"<p>The current state-of-the-art methods in 3D instance segmentation typically\ninvolve a clustering step, despite the tendency towards heuristics, greedy\nalgorithms, and a lack of robustness to the changes in data statistics. In\ncontrast, we propose a fully-convolutional 3D point cloud instance segmentation\nmethod that works in a per-point prediction fashion. In doing so it avoids the\nchallenges that clustering-based methods face: introducing dependencies among\ndifferent tasks of the model. We find the key to its success is assigning a\nsuitable target to each sampled point. Instead of the commonly used static or\ndistance-based assignment strategies, we propose to use an Optimal Transport\napproach to optimally assign target masks to the sampled points according to\nthe dynamic matching costs. Our approach achieves promising results on both\nScanNet and S3DIS benchmarks. The proposed approach removes intertask\ndependencies and thus represents a simpler and more flexible 3D instance\nsegmentation framework than other competing methods, while achieving improved\nsegmentation accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Closer Look at Branch Classifiers of Multi-exit Architectures. (arXiv:2204.13347v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13347","description":"<p>Multi-exit architectures consist of a backbone and branch classifiers that\noffer shortened inference pathways to reduce the run-time of deep neural\nnetworks. In this paper, we analyze different branching patterns that vary in\ntheir allocation of computational complexity for the branch classifiers.\nConstant-complexity branching keeps all branches the same, while\ncomplexity-increasing and complexity-decreasing branching place more complex\nbranches later or earlier in the backbone respectively. Through extensive\nexperimentation on multiple backbones and datasets, we find that\ncomplexity-decreasing branches are more effective than constant-complexity or\ncomplexity-increasing branches, which achieve the best accuracy-cost trade-off.\nWe investigate a cause by using knowledge consistency to probe the effect of\nadding branches onto a backbone. Our findings show that complexity-decreasing\nbranching yields the least disruption to the feature abstraction hierarchy of\nthe backbone, which explains the effectiveness of the branching patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shaohui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_B/0/1/0/all/0/1\">Bo Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn to Understand Negation in Video Retrieval. (arXiv:2205.00132v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2205.00132","description":"<p>Negation is a common linguistic skill that allows human to express what we do\nNOT want. Naturally, one might expect video retrieval to support\nnatural-language queries with negation, e.g., finding shots of kids sitting on\nthe floor and not playing with a dog. However, the state-of-the-art deep\nlearning based video retrieval models lack such ability, as they are typically\ntrained on video description datasets such as MSR-VTT and VATEX that lack\nnegated descriptions. Their retrieved results basically ignore the negator in\nthe sample query, incorrectly returning videos showing kids playing with dog.\nThis paper presents the first study on learning to understand negation in video\nretrieval and make contributions as follows. By re-purposing two existing\ndatasets (MSR-VTT and VATEX), we propose a new evaluation protocol for video\nretrieval with negation. We propose a learning based method for training a\nnegation-aware video retrieval model. The key idea is to first construct a soft\nnegative caption for a specific training video by partially negating its\noriginal caption, and then compute a bidirectionally constrained loss on the\ntriplet. This auxiliary loss is weightedly added to a standard retrieval loss.\nExperiments on the re-purposed benchmarks show that re-training the CLIP\n(Contrastive Language-Image Pre-Training) model by the proposed method clearly\nimproves its ability to handle queries with negation. In addition, the model\nperformance on the original benchmarks is also improved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Aozhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Composition-aware Graphic Layout GAN for Visual-textual Presentation Designs. (arXiv:2205.00303v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.00303","description":"<p>In this paper, we study the graphic layout generation problem of producing\nhigh-quality visual-textual presentation designs for given images. We note that\nimage compositions, which contain not only global semantics but also spatial\ninformation, would largely affect layout results. Hence, we propose a deep\ngenerative model, dubbed as composition-aware graphic layout GAN (CGL-GAN), to\nsynthesize layouts based on the global and spatial visual contents of input\nimages. To obtain training images from images that already contain manually\ndesigned graphic layout data, previous work suggests masking design elements\n(e.g., texts and embellishments) as model inputs, which inevitably leaves hint\nof the ground truth. We study the misalignment between the training inputs\n(with hint masks) and test inputs (without masks), and design a novel domain\nalignment module (DAM) to narrow this gap. For training, we built a large-scale\nlayout dataset which consists of 60,548 advertising posters with annotated\nlayout information. To evaluate the generated layouts, we propose three novel\nmetrics according to aesthetic intuitions. Through both quantitative and\nqualitative evaluations, we demonstrate that the proposed model can synthesize\nhigh-quality graphic layouts according to image compositions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Min Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenchen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Ye Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tiezheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-Aware Self-supervised Multi-Label Learning. (arXiv:2205.07028v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.07028","description":"<p>Multi-label Learning on Image data has been widely exploited with deep\nlearning models. However, supervised training on deep CNN models often cannot\ndiscover sufficient discriminative features for classification. As a result,\nnumerous self-supervision methods are proposed to learn more robust image\nrepresentations. However, most self-supervised approaches focus on\nsingle-instance single-label data and fall short on more complex images with\nmultiple objects. Therefore, we propose an Object-Aware Self-Supervision (OASS)\nmethod to obtain more fine-grained representations for multi-label learning,\ndynamically generating auxiliary tasks based on object locations. Secondly, the\nrobust representation learned by OASS can be leveraged to efficiently generate\nClass-Specific Instances (CSI) in a proposal-free fashion to better guide\nmulti-label supervision signal transfer to instances. Extensive experiments on\nthe VOC2012 dataset for multi-label classification demonstrate the\neffectiveness of the proposed method against the state-of-the-art counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaixin_X/0/1/0/all/0/1\">Xu Kaixin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liyang_L/0/1/0/all/0/1\">Liu Liyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziyuan_Z/0/1/0/all/0/1\">Zhao Ziyuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veeravalli_B/0/1/0/all/0/1\">Bharadwaj Veeravalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy Preserving Image Registration. (arXiv:2205.10120v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10120","description":"<p>Image registration is a key task in medical imaging applications, allowing to\nrepresent medical images in a common spatial reference frame. Current\nliterature on image registration is generally based on the assumption that\nimages are usually accessible to the researcher, from which the spatial\ntransformation is subsequently estimated. This common assumption may not be met\nin current practical applications, since the sensitive nature of medical images\nmay ultimately require their analysis under privacy constraints, preventing to\nshare the image content in clear form. In this work, we formulate the problem\nof image registration under a privacy preserving regime, where images are\nassumed to be confidential and cannot be disclosed in clear. We derive our\nprivacy preserving image registration framework by extending classical\nregistration paradigms to account for advanced cryptographic tools, such as\nsecure multi-party computation and homomorphic encryption, that enable the\nexecution of operations without leaking the underlying data. To overcome the\nproblem of performance and scalability of cryptographic tools in high\ndimensions, we first propose to optimize the underlying image registration\noperations using gradient approximations. We further revisit the use of\nhomomorphic encryption and use a packing method to allow the encryption and\nmultiplication of large matrices more efficiently. We demonstrate our privacy\npreserving framework in linear and non-linear registration problems, evaluating\nits accuracy and scalability with respect to standard image registration. Our\nresults show that privacy preserving image registration is feasible and can be\nadopted in sensitive medical imaging applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taiello_R/0/1/0/all/0/1\">Riccardo Taiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onen_M/0/1/0/all/0/1\">Melek &#xd6;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Humbert_O/0/1/0/all/0/1\">Olivier Humbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenzi_M/0/1/0/all/0/1\">Marco Lorenzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DH-GAN: A Physics-driven Untrained Generative Adversarial Network for 3D Microscopic Imaging using Digital Holography. (arXiv:2205.12920v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2205.12920","description":"<p>Digital holography is a 3D imaging technique by emitting a laser beam with a\nplane wavefront to an object and measuring the intensity of the diffracted\nwaveform, called holograms. The object's 3D shape can be obtained by numerical\nanalysis of the captured holograms and recovering the incurred phase. Recently,\ndeep learning (DL) methods have been used for more accurate holographic\nprocessing. However, most supervised methods require large datasets to train\nthe model, which is rarely available in most DH applications due to the\nscarcity of samples or privacy concerns. A few one-shot DL-based recovery\nmethods exist with no reliance on large datasets of paired images. Still, most\nof these methods often neglect the underlying physics law that governs wave\npropagation. These methods offer a black-box operation, which is not\nexplainable, generalizable, and transferrable to other samples and\napplications. In this work, we propose a new DL architecture based on\ngenerative adversarial networks that uses a discriminative network for\nrealizing a semantic measure for reconstruction quality while using a\ngenerative network as a function approximator to model the inverse of hologram\nformation. We impose smoothness on the background part of the recovered image\nusing a progressive masking module powered by simulated annealing to enhance\nthe reconstruction quality. The proposed method is one of its kind that\nexhibits high transferability to similar samples, which facilitates its fast\ndeployment in time-sensitive applications without the need for retraining the\nnetwork. The results show a considerable improvement to competitor methods in\nreconstruction quality (about 5 dB PSNR gain) and robustness to noise (about\n50% reduction in PSNR vs noise increase rate).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razi_A/0/1/0/all/0/1\">Abolfazl Razi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozicki_M/0/1/0/all/0/1\">Michael Kozicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mann_C/0/1/0/all/0/1\">Christopher Mann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedNST: Federated Noisy Student Training for Automatic Speech Recognition. (arXiv:2206.02797v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2206.02797","description":"<p>Federated Learning (FL) enables training state-of-the-art Automatic Speech\nRecognition (ASR) models on user devices (clients) in distributed systems,\nhence preventing transmission of raw user data to a central server. A key\nchallenge facing practical adoption of FL for ASR is obtaining ground-truth\nlabels on the clients. Existing approaches rely on clients to manually\ntranscribe their speech, which is impractical for obtaining large training\ncorpora. A promising alternative is using semi-/self-supervised learning\napproaches to leverage unlabelled user data. To this end, we propose FedNST, a\nnovel method for training distributed ASR models using private and unlabelled\nuser data. We explore various facets of FedNST, such as training models with\ndifferent proportions of labelled and unlabelled data, and evaluate the\nproposed approach on 1173 simulated clients. Evaluating FedNST on LibriSpeech,\nwhere 960 hours of speech data is split equally into server (labelled) and\nclient (unlabelled) data, showed a 22.5% relative word error rate reduction}\n(WERR) over a supervised baseline trained only on server data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mehmood_H/0/1/0/all/0/1\">Haaris Mehmood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dobrowolska_A/0/1/0/all/0/1\">Agnieszka Dobrowolska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saravanan_K/0/1/0/all/0/1\">Karthikeyan Saravanan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozay_M/0/1/0/all/0/1\">Mete Ozay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Adversarial Attacks and Defenses in Vision Transformers trained with DINO. (arXiv:2206.06761v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.06761","description":"<p>This work conducts the first analysis on the robustness against adversarial\nattacks on self-supervised Vision Transformers trained using DINO. First, we\nevaluate whether features learned through self-supervision are more robust to\nadversarial attacks than those emerging from supervised learning. Then, we\npresent properties arising for attacks in the latent space. Finally, we\nevaluate whether three well-known defense strategies can increase adversarial\nrobustness in downstream tasks by only fine-tuning the classification head to\nprovide robustness even in view of limited compute resources. These defense\nstrategies are: Adversarial Training, Ensemble Adversarial Training and\nEnsemble of Specialized Networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rando_J/0/1/0/all/0/1\">Javier Rando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naimi_N/0/1/0/all/0/1\">Nasib Naimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumann_T/0/1/0/all/0/1\">Thomas Baumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathys_M/0/1/0/all/0/1\">Max Mathys</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BadHash: Invisible Backdoor Attacks against Deep Hashing with Clean Label. (arXiv:2207.00278v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.00278","description":"<p>Due to its powerful feature learning capability and high efficiency, deep\nhashing has achieved great success in large-scale image retrieval. Meanwhile,\nextensive works have demonstrated that deep neural networks (DNNs) are\nsusceptible to adversarial examples, and exploring adversarial attack against\ndeep hashing has attracted many research efforts. Nevertheless, backdoor\nattack, another famous threat to DNNs, has not been studied for deep hashing\nyet. Although various backdoor attacks have been proposed in the field of image\nclassification, existing approaches failed to realize a truly imperceptive\nbackdoor attack that enjoys invisible triggers and clean label setting\nsimultaneously, and they also cannot meet the intrinsic demand of image\nretrieval backdoor. In this paper, we propose BadHash, the first\ngenerative-based imperceptible backdoor attack against deep hashing, which can\neffectively generate invisible and input-specific poisoned images with clean\nlabel. Specifically, we first propose a new conditional generative adversarial\nnetwork (cGAN) pipeline to effectively generate poisoned samples. For any given\nbenign image, it seeks to generate a natural-looking poisoned counterpart with\na unique invisible trigger. In order to improve the attack effectiveness, we\nintroduce a label-based contrastive learning network LabCLN to exploit the\nsemantic characteristics of different labels, which are subsequently used for\nconfusing and misleading the target model to learn the embedded trigger. We\nfinally explore the mechanism of backdoor attacks on image retrieval in the\nhash space. Extensive experiments on multiple benchmark datasets verify that\nBadHash can generate imperceptible poisoned samples with strong attack ability\nand transferability over state-of-the-art deep hashing schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengshan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Ziqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yechao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Leo Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yifeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+HE_Y/0/1/0/all/0/1\">Yuanyuan HE</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hai Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patient-specific modelling, simulation and real time processing for respiratory diseases. (arXiv:2207.01082v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.01082","description":"<p>Asthma is a common chronic disease of the respiratory system causing\nsignificant disability and societal burden. It affects over 500 million people\nworldwide and generates costs exceeding $USD 56 billion in 2011 in the United\nStates. Managing asthma involves controlling symptoms, preventing\nexacerbations, and maintaining lung function. Improving asthma control affects\nthe daily life of patients and is associated with a reduced risk of\nexacerbations and lung function impairment, reduces the cost of asthma care and\nindirect costs associated with reduced productivity. Understanding the complex\ndynamics of the pulmonary system and the lung's response to disease, injury,\nand treatment is fundamental to the advancement of Asthma treatment.\nComputational models of the respiratory system seek to provide a theoretical\nframework to understand the interaction between structure and function. Their\napplication can improve pulmonary medicine by a patient-specific approach to\nmedicinal methodologies optimizing the delivery given the personalized geometry\nand personalized ventilation patterns while introducing a patient-specific\ntechnique that maximizes drug delivery. A three-fold objective addressed within\nthis dissertation becomes prominent at this point. The first part refers to the\ncomprehension of pulmonary pathophysiology and the mechanics of Asthma and\nsubsequently of constrictive pulmonary conditions in general. The second part\nrefers to the design and implementation of tools that facilitate personalized\nmedicine to improve delivery and effectiveness. Finally, the third part refers\nto the self-management of the condition, meaning that medical personnel and\npatients have access to tools and methods that allow the first party to easily\ntrack the course of the condition and the second party, i.e. the patient to\neasily self-manage it alleviating the significant burden from the health\nsystem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nousias_S/0/1/0/all/0/1\">Stavros Nousias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Vehicle Detection and Tracking on Fisheye Traffic Monitoring Video using CNN and Bounding Box Propagation. (arXiv:2207.01183v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01183","description":"<p>We design a fast car detection and tracking algorithm for traffic monitoring\nfisheye video mounted on crossroads. We use ICIP 2020 VIP Cup dataset and adopt\nYOLOv5 as the object detection base model. The nighttime video of this dataset\nis very challenging, and the detection accuracy (AP50) of the base model is\nabout 54%. We design a reliable car detection and tracking algorithm based on\nthe concept of bounding box propagation among frames, which provides 17.9\npercentage points (pp) and 6.2 pp. accuracy improvement over the base model for\nthe nighttime and daytime videos, respectively. To speed up, the grayscale\nframe difference is used for the intermediate frames in a segment, which can\ndouble the processing speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ardianto_S/0/1/0/all/0/1\">Sandy Ardianto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hang_H/0/1/0/all/0/1\">Hsueh-Ming Hang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wen-Huang Cheng</a> (National Yang Ming Chiao Tung University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Much More Data Do I Need? Estimating Requirements for Downstream Tasks. (arXiv:2207.01725v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01725","description":"<p>Given a small training data set and a learning algorithm, how much more data\nis necessary to reach a target validation or test performance? This question is\nof critical importance in applications such as autonomous driving or medical\nimaging where collecting data is expensive and time-consuming. Overestimating\nor underestimating data requirements incurs substantial costs that could be\navoided with an adequate budget. Prior work on neural scaling laws suggest that\nthe power-law function can fit the validation performance curve and extrapolate\nit to larger data set sizes. We find that this does not immediately translate\nto the more difficult downstream task of estimating the required data set size\nto meet a target performance. In this work, we consider a broad class of\ncomputer vision tasks and systematically investigate a family of functions that\ngeneralize the power-law function to allow for better estimation of data\nrequirements. Finally, we show that incorporating a tuned correction factor and\ncollecting over multiple rounds significantly improves the performance of the\ndata estimators. Using our guidelines, practitioners can accurately estimate\ndata requirements of machine learning systems to gain savings in both\ndevelopment time and data acquisition costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_R/0/1/0/all/0/1\">Rafid Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_J/0/1/0/all/0/1\">James Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acuna_D/0/1/0/all/0/1\">David Acuna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Daiqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philion_J/0/1/0/all/0/1\">Jonah Philion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1\">Marc T. Law</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PatchZero: Defending against Adversarial Patch Attacks by Detecting and Zeroing the Patch. (arXiv:2207.01795v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01795","description":"<p>Adversarial patch attacks mislead neural networks by injecting adversarial\npixels within a local region. Patch attacks can be highly effective in a\nvariety of tasks and physically realizable via attachment (e.g. a sticker) to\nthe real-world objects. Despite the diversity in attack patterns, adversarial\npatches tend to be highly textured and different in appearance from natural\nimages. We exploit this property and present PatchZero, a general defense\npipeline against white-box adversarial patches without retraining the\ndownstream classifier or detector. Specifically, our defense detects\nadversaries at the pixel-level and \"zeros out\" the patch region by repainting\nwith mean pixel values. We further design a two-stage adversarial training\nscheme to defend against the stronger adaptive attacks. PatchZero achieves SOTA\ndefense performance on the image classification (ImageNet, RESISC45), object\ndetection (PASCAL VOC), and video classification (UCF101) tasks with little\ndegradation in benign performance. In addition, PatchZero transfers to\ndifferent patch shapes and attack types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhaoheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_K/0/1/0/all/0/1\">Kaijie Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nevatia_R/0/1/0/all/0/1\">Ram Nevatia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RepMix: Representation Mixing for Robust Attribution of Synthesized Images. (arXiv:2207.02063v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02063","description":"<p>Rapid advances in Generative Adversarial Networks (GANs) raise new challenges\nfor image attribution; detecting whether an image is synthetic and, if so,\ndetermining which GAN architecture created it. Uniquely, we present a solution\nto this task capable of 1) matching images invariant to their semantic content;\n2) robust to benign transformations (changes in quality, resolution, shape,\netc.) commonly encountered as images are re-shared online. In order to\nformalize our research, a challenging benchmark, Attribution88, is collected\nfor robust and practical image attribution. We then propose RepMix, our GAN\nfingerprinting technique based on representation mixing and a novel loss. We\nvalidate its capability of tracing the provenance of GAN-generated images\ninvariant to the semantic content of the image and also robust to\nperturbations. We show our approach improves significantly from existing GAN\nfingerprinting works on both semantic generalization and robustness. Data and\ncode are available at https://github.com/TuBui/image_attribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tu Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Ning Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling. (arXiv:2207.02196v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02196","description":"<p>Score-based generative models (SGMs) have recently emerged as a promising\nclass of generative models. However, a fundamental limitation is that their\ninference is very slow due to a need for many (e.g., 2000) iterations of\nsequential computations. An intuitive acceleration method is to reduce the\nsampling iterations which however causes severe performance degradation. We\ninvestigate this problem by viewing the diffusion sampling process as a\nMetropolis adjusted Langevin algorithm, which helps reveal the underlying cause\nto be ill-conditioned curvature. Under this insight, we propose a\nmodel-agnostic preconditioned diffusion sampling (PDS) method that leverages\nmatrix preconditioning to alleviate the aforementioned problem. Crucially, PDS\nis proven theoretically to converge to the original target distribution of a\nSGM, no need for retraining. Extensive experiments on three image datasets with\na variety of resolutions and diversity validate that PDS consistently\naccelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In\nparticular, PDS can accelerate by up to 29x on more challenging high resolution\n(1024x1024) image generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hengyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianfeng Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GaitTAKE: Gait Recognition by Temporal Attention and Keypoint-guided Embedding. (arXiv:2207.03608v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.03608","description":"<p>Gait recognition, which refers to the recognition or identification of a\nperson based on their body shape and walking styles, derived from video data\ncaptured from a distance, is widely used in crime prevention, forensic\nidentification, and social security. However, to the best of our knowledge,\nmost of the existing methods use appearance, posture and temporal feautures\nwithout considering a learned temporal attention mechanism for global and local\ninformation fusion. In this paper, we propose a novel gait recognition\nframework, called Temporal Attention and Keypoint-guided Embedding (GaitTAKE),\nwhich effectively fuses temporal-attention-based global and local appearance\nfeature and temporal aggregated human pose feature. Experimental results show\nthat our proposed method achieves a new SOTA in gait recognition with rank-1\naccuracy of 98.0% (normal), 97.5% (bag) and 92.2% (coat) on the CASIA-B gait\ndataset; 90.4% accuracy on the OU-MVLP gait dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_H/0/1/0/all/0/1\">Hung-Min Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng-Yen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thuc_H/0/1/0/all/0/1\">Hoang Le Uyen Thuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kwang-Ju Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastLTS: Non-Autoregressive End-to-End Unconstrained Lip-to-Speech Synthesis. (arXiv:2207.03800v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2207.03800","description":"<p>Unconstrained lip-to-speech synthesis aims to generate corresponding speeches\nfrom silent videos of talking faces with no restriction on head poses or\nvocabulary. Current works mainly use sequence-to-sequence models to solve this\nproblem, either in an autoregressive architecture or a flow-based\nnon-autoregressive architecture. However, these models suffer from several\ndrawbacks: 1) Instead of directly generating audios, they use a two-stage\npipeline that first generates mel-spectrograms and then reconstructs audios\nfrom the spectrograms. This causes cumbersome deployment and degradation of\nspeech quality due to error propagation; 2) The audio reconstruction algorithm\nused by these models limits the inference speed and audio quality, while neural\nvocoders are not available for these models since their output spectrograms are\nnot accurate enough; 3) The autoregressive model suffers from high inference\nlatency, while the flow-based model has high memory occupancy: neither of them\nis efficient enough in both time and memory usage. To tackle these problems, we\npropose FastLTS, a non-autoregressive end-to-end model which can directly\nsynthesize high-quality speech audios from unconstrained talking videos with\nlow latency, and has a relatively small model size. Besides, different from the\nwidely used 3D-CNN visual frontend for lip movement encoding, we for the first\ntime propose a transformer-based visual frontend for this task. Experiments\nshow that our model achieves $19.76\\times$ speedup for audio waveform\ngeneration compared with the current autoregressive model on input sequences of\n3 seconds, and obtains superior audio quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Snipper: A Spatiotemporal Transformer for Simultaneous Multi-Person 3D Pose Estimation Tracking and Forecasting on a Video Snippet. (arXiv:2207.04320v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04320","description":"<p>Multi-person pose understanding from RGB videos includes three complex tasks:\npose estimation, tracking and motion forecasting. Among these three tasks, pose\nestimation and tracking are correlated, and tracking is crucial to motion\nforecasting. Most existing works either focus on a single task or employ\ncascaded methods to solve each individual task separately. In this paper, we\npropose Snipper, a framework to perform multi-person 3D pose estimation,\ntracking and motion forecasting simultaneously in a single inference.\nSpecifically, we first propose a deformable attention mechanism to aggregate\nspatiotemporal information from video snippets. Building upon this deformable\nattention, a visual transformer is learned to encode the spatiotemporal\nfeatures from multi-frame images and to decode informative pose features to\nupdate multi-person pose queries. Last, these queries are regressed to predict\nmulti-person pose trajectories and future motions in one forward pass. In the\nexperiments, we show the effectiveness of Snipper on three challenging public\ndatasets where a generic model rivals specialized state-of-art baselines for\npose estimation, tracking, and forecasting. Code is available at\nhttps://github.com/JimmyZou/Snipper\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1\">Shihao Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanlu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lingni Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_M/0/1/0/all/0/1\">Minh Vo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Coding Using Learned Latent GAN Compression. (arXiv:2207.04324v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.04324","description":"<p>We propose in this paper a new paradigm for facial video compression. We\nleverage the generative capacity of GANs such as StyleGAN to represent and\ncompress a video, including intra and inter compression. Each frame is inverted\nin the latent space of StyleGAN, from which the optimal compression is learned.\nTo do so, a diffeomorphic latent representation is learned using a normalizing\nflows model, where an entropy model can be optimized for image coding. In\naddition, we propose a new perceptual loss that is more efficient than other\ncounterparts. Finally, an entropy model for video inter coding with residual is\nalso learned in the previously constructed latent representation. Our method\n(SGANC) is simple, faster to train, and achieves better results for image and\nvideo coding compared to state-of-the-art codecs such as VTM, AV1, and recent\ndeep learning techniques. In particular, it drastically minimizes perceptual\ndistortion at low bit rates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shukor_M/0/1/0/all/0/1\">Mustafa Shukor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Damodaran_B/0/1/0/all/0/1\">Bharath Bhushan Damodaran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_X/0/1/0/all/0/1\">Xu Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hellier_P/0/1/0/all/0/1\">Pierre Hellier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DCCF: Deep Comprehensible Color Filter Learning Framework for High-Resolution Image Harmonization. (arXiv:2207.04788v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04788","description":"<p>Image color harmonization algorithm aims to automatically match the color\ndistribution of foreground and background images captured in different\nconditions. Previous deep learning based models neglect two issues that are\ncritical for practical applications, namely high resolution (HR) image\nprocessing and model comprehensibility. In this paper, we propose a novel Deep\nComprehensible Color Filter (DCCF) learning framework for high-resolution image\nharmonization. Specifically, DCCF first downsamples the original input image to\nits low-resolution (LR) counter-part, then learns four human comprehensible\nneural filters (i.e. hue, saturation, value and attentive rendering filters) in\nan end-to-end manner, finally applies these filters to the original input image\nto get the harmonized result. Benefiting from the comprehensible neural\nfilters, we could provide a simple yet efficient handler for users to cooperate\nwith deep model to get the desired results with very little effort when\nnecessary. Extensive experiments demonstrate the effectiveness of DCCF learning\nframework and it outperforms state-of-the-art post-processing method on\niHarmony4 dataset on images' full-resolutions by achieving 7.63% and 1.69%\nrelative improvements on MSE and PSNR respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1\">Ben Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_S/0/1/0/all/0/1\">Shenghui Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Quan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Rongfei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Binqiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xing Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CCPL: Contrastive Coherence Preserving Loss for Versatile Style Transfer. (arXiv:2207.04808v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04808","description":"<p>In this paper, we aim to devise a universally versatile style transfer method\ncapable of performing artistic, photo-realistic, and video style transfer\njointly, without seeing videos during training. Previous single-frame methods\nassume a strong constraint on the whole image to maintain temporal consistency,\nwhich could be violated in many cases. Instead, we make a mild and reasonable\nassumption that global inconsistency is dominated by local inconsistencies and\ndevise a generic Contrastive Coherence Preserving Loss (CCPL) applied to local\npatches. CCPL can preserve the coherence of the content source during style\ntransfer without degrading stylization. Moreover, it owns a neighbor-regulating\nmechanism, resulting in a vast reduction of local distortions and considerable\nvisual quality improvement. Aside from its superior performance on versatile\nstyle transfer, it can be easily extended to other tasks, such as\nimage-to-image translation. Besides, to better fuse content and style features,\nwe propose Simple Covariance Transformation (SCT) to effectively align\nsecond-order statistics of the content feature with the style feature.\nExperiments demonstrate the effectiveness of the resulting model for versatile\nstyle transfer, when armed with CCPL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zijie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Junping Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Closer Look at Invariances in Self-supervised Pre-training for 3D Vision. (arXiv:2207.04997v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04997","description":"<p>Self-supervised pre-training for 3D vision has drawn increasing research\ninterest in recent years. In order to learn informative representations, a lot\nof previous works exploit invariances of 3D features, e.g.,\nperspective-invariance between views of the same scene, modality-invariance\nbetween depth and RGB images, format-invariance between point clouds and\nvoxels. Although they have achieved promising results, previous researches lack\na systematic and fair comparison of these invariances. To address this issue,\nour work, for the first time, introduces a unified framework, under which\nvarious pre-training methods can be investigated. We conduct extensive\nexperiments and provide a closer look at the contributions of different\ninvariances in 3D pre-training. Also, we propose a simple but effective method\nthat jointly pre-trains a 3D encoder and a depth map encoder using contrastive\nlearning. Models pre-trained with our method gain significant performance boost\nin downstream tasks. For instance, a pre-trained VoteNet outperforms previous\nmethods on SUN RGB-D and ScanNet object detection benchmarks with a clear\nmargin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lanxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heizmann_M/0/1/0/all/0/1\">Michael Heizmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intra-Modal Constraint Loss For Image-Text Retrieval. (arXiv:2207.05024v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.05024","description":"<p>Cross-modal retrieval has drawn much attention in both computer vision and\nnatural language processing domains. With the development of convolutional and\nrecurrent neural networks, the bottleneck of retrieval across image-text\nmodalities is no longer the extraction of image and text features but an\nefficient loss function learning in embedding space. Many loss functions try to\ncloser pairwise features from heterogeneous modalities. This paper proposes a\nmethod for learning joint embedding of images and texts using an intra-modal\nconstraint loss function to reduce the violation of negative pairs from the\nsame homogeneous modality. Experimental results show that our approach\noutperforms state-of-the-art bi-directional image-text retrieval methods on\nFlickr30K and Microsoft COCO datasets. Our code is publicly available:\nhttps://github.com/CanonChen/IMC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1\">Cong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kpalma_K/0/1/0/all/0/1\">Kidiyo Kpalma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Susceptibility of Continual Learning Against Adversarial Attacks. (arXiv:2207.05225v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2207.05225","description":"<p>The recent advances in continual (incremental or lifelong) learning have\nconcentrated on the prevention of forgetting that can lead to catastrophic\nconsequences, but there are two outstanding challenges that must be addressed.\nThe first is the evaluation of the robustness of the proposed methods. The\nsecond is ensuring the security of learned tasks remains largely unexplored.\nThis paper presents a comprehensive study of the susceptibility of the\ncontinually learned tasks (including both current and previously learned tasks)\nthat are vulnerable to forgetting. Such vulnerability of tasks against\nadversarial attacks raises profound issues in data integrity and privacy. We\nconsider the task incremental learning (Task-IL) scenario and explore three\nregularization-based experiments, three replay-based experiments, and one\nhybrid technique based on the reply and exemplar approach. We examine the\nrobustness of these methods. In particular, we consider cases where we\ndemonstrate that any class belonging to the current or previously learned tasks\nis prone to misclassification. Our observations highlight the potential\nlimitations of existing Task-IL approaches. Our empirical study recommends that\nthe research community consider the robustness of the proposed continual\nlearning approaches and invest extensive efforts in mitigating catastrophic\nforgetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_H/0/1/0/all/0/1\">Hikmat Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Pir Masoom Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_S/0/1/0/all/0/1\">Syed Farhan Alam Zaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1\">Saif ul Islam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Human Vision Inspired Action Recognition using Adaptive Spatiotemporal Sampling. (arXiv:2207.05249v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.05249","description":"<p>Adaptive sampling that exploits the spatiotemporal redundancy in videos is\ncritical for always-on action recognition on wearable devices with limited\ncomputing and battery resources. The commonly used fixed sampling strategy is\nnot context-aware and may under-sample the visual content, and thus adversely\nimpacts both computation efficiency and accuracy. Inspired by the concepts of\nfoveal vision and pre-attentive processing from the human visual perception\nmechanism, we introduce a novel adaptive spatiotemporal sampling scheme for\nefficient action recognition. Our system pre-scans the global scene context at\nlow-resolution and decides to skip or request high-resolution features at\nsalient regions for further processing. We validate the system on EPIC-KITCHENS\nand UCF-101 datasets for action recognition, and show that our proposed\napproach can greatly speed up inference with a tolerable loss of accuracy\ncompared with those from state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mac_K/0/1/0/all/0/1\">Khoi-Nguyen C. Mac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1\">Minh N. Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_M/0/1/0/all/0/1\">Minh P. Vo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios. (arXiv:2207.05501v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.05501","description":"<p>Due to the complex attention mechanisms and model design, most existing\nvision Transformers (ViTs) can not perform as efficiently as convolutional\nneural networks (CNNs) in realistic industrial deployment scenarios, e.g.\nTensorRT and CoreML. This poses a distinct challenge: Can a visual neural\nnetwork be designed to infer as fast as CNNs and perform as powerful as ViTs?\nRecent works have tried to design CNN-Transformer hybrid architectures to\naddress this issue, yet the overall performance of these works is far away from\nsatisfactory. To end these, we propose a next generation vision Transformer for\nefficient deployment in realistic industrial scenarios, namely Next-ViT, which\ndominates both CNNs and ViTs from the perspective of latency/accuracy\ntrade-off. In this work, the Next Convolution Block (NCB) and Next Transformer\nBlock (NTB) are respectively developed to capture local and global information\nwith deployment-friendly mechanisms. Then, Next Hybrid Strategy (NHS) is\ndesigned to stack NCB and NTB in an efficient hybrid paradigm, which boosts\nperformance in various downstream tasks. Extensive experiments show that\nNext-ViT significantly outperforms existing CNNs, ViTs and CNN-Transformer\nhybrid architectures with respect to the latency/accuracy trade-off across\nvarious vision tasks. On TensorRT, Next-ViT surpasses ResNet by 5.4 mAP (from\n40.4 to 45.8) on COCO detection and 8.2% mIoU (from 38.8% to 47.0%) on ADE20K\nsegmentation under similar latency. Meanwhile, it achieves comparable\nperformance with CSWin, while the inference speed is accelerated by 3.6x. On\nCoreML, Next-ViT surpasses EfficientFormer by 4.6 mAP (from 42.6 to 47.2) on\nCOCO detection and 3.5% mIoU (from 45.2% to 48.7%) on ADE20K segmentation under\nsimilar latency. Code will be released recently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiashi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huixia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuefeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Min Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xin Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LudVision -- Remote Detection of Exotic Invasive Aquatic Floral Species using Drone-Mounted Multispectral Data. (arXiv:2207.05620v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.05620","description":"<p>Remote sensing is the process of detecting and monitoring the physical\ncharacteristics of an area by measuring its reflected and emitted radiation at\na distance. It is being broadly used to monitor ecosystems, mainly for their\npreservation. Ever-growing reports of invasive species have affected the\nnatural balance of ecosystems. Exotic invasive species have a critical impact\nwhen introduced into new ecosystems and may lead to the extinction of native\nspecies. In this study, we focus on Ludwigia peploides, considered by the\nEuropean Union as an aquatic invasive species. Its presence can negatively\nimpact the surrounding ecosystem and human activities such as agriculture,\nfishing, and navigation. Our goal was to develop a method to identify the\npresence of the species. We used images collected by a drone-mounted\nmultispectral sensor to achieve this, creating our LudVision data set. To\nidentify the targeted species on the collected images, we propose a new method\nfor detecting Ludwigia p. in multispectral images. The method is based on\nexisting state-of-the-art semantic segmentation methods modified to handle\nmultispectral data. The proposed method achieved a producer's accuracy of 79.9%\nand a user's accuracy of 95.5%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abreu_A/0/1/0/all/0/1\">Ant&#xf3;nio J. Abreu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexandre_L/0/1/0/all/0/1\">Lu&#xed;s A. Alexandre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jo&#xe3;o A. Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basso_F/0/1/0/all/0/1\">Filippo Basso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}