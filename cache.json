{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-11T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Contextual Sentence Classification: Detecting Sustainability Initiatives in Company Reports. (arXiv:2110.03727v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03727","description":"<p>We introduce the novel task of detecting sustainability initiatives in\ncompany reports. Given a full report, the aim is to automatically identify\nmentions of practical activities that a company has performed in order to\ntackle specific societal issues. As a single initiative can often be described\nover multiples sentences, new methods for identifying continuous sentence spans\nneeds to be developed. We release a new dataset of company reports in which the\ntext has been manually annotated with sustainability initiatives. We also\nevaluate different models for initiative detection, introducing a novel\naggregation and evaluation methodology. Our proposed architecture uses\nsequences of five consecutive sentences to account for contextual information\nwhen making classification decisions at the individual sentence level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hirlea_D/0/1/0/all/0/1\">Dan Hirlea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bryant_C/0/1/0/all/0/1\">Christopher Bryant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UoB at SemEval-2021 Task 5: Extending Pre-Trained Language Models to Include Task and Domain-Specific Information for Toxic Span Prediction. (arXiv:2110.03730v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03730","description":"<p>Toxicity is pervasive in social media and poses a major threat to the health\nof online communities. The recent introduction of pre-trained language models,\nwhich have achieved state-of-the-art results in many NLP tasks, has transformed\nthe way in which we approach natural language processing. However, the inherent\nnature of pre-training means that they are unlikely to capture task-specific\nstatistical information or learn domain-specific knowledge. Additionally, most\nimplementations of these models typically do not employ conditional random\nfields, a method for simultaneous token classification. We show that these\nmodifications can improve model performance on the Toxic Spans Detection task\nat SemEval-2021 to achieve a score within 4 percentage points of the top\nperforming team.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_E/0/1/0/all/0/1\">Erik Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madabushi_H/0/1/0/all/0/1\">Harish Tayyar Madabushi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference. (arXiv:2110.03742v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03742","description":"<p>Sparse Mixture-of-Experts (MoE) has been a successful approach for scaling\nmultilingual translation models to billions of parameters without a\nproportional increase in training computation. However, MoE models are\nprohibitively large and practitioners often resort to methods such as\ndistillation for serving. In this work, we investigate routing strategies at\ndifferent granularity (token, sentence, task) in MoE models to bypass\ndistillation. Experiments on WMT and a web-scale dataset suggest that\ntask-level routing (task-MoE) enables us to extract smaller, ready-to-deploy\nsub-networks from large sparse models. On WMT, our task-MoE with 32 experts\n(533M parameters) outperforms the best performing token-level MoE model\n(token-MoE) by +1.0 BLEU on average across 30 language pairs. The peak\ninference throughput is also improved by a factor of 1.9x when we route by\ntasks instead of tokens. While distilling a token-MoE to a smaller dense model\npreserves only 32% of the BLEU gains, our sub-network task-MoE, by design,\npreserves all the gains with the same inference cost as the distilled student\nmodel. Finally, when scaling up to 200 language pairs, our 128-expert task-MoE\n(13B parameters) performs competitively with a token-level counterpart, while\nimproving the peak inference throughput by a factor of 2.6x.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kudugunta_S/0/1/0/all/0/1\">Sneha Kudugunta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krikun_M/0/1/0/all/0/1\">Maxim Krikun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepikhin_D/0/1/0/all/0/1\">Dmitry Lepikhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_M/0/1/0/all/0/1\">Minh-Thang Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sonorant spectra and coarticulation distinguish speakers with different dialects. (arXiv:2110.03756v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03756","description":"<p>The aim of this study is to determine the effect of language varieties on the\nspectral distribution of stressed and unstressed sonorants (nasals /m, n/,\nlateral approximants /l/, and rhotics /r/) and on their coarticulatory effects\non adjacent sounds. To quantify the shape of the spectral distribution, we\ncalculated the spectral moments from the sonorant spectra of nasals /m, n/,\nlateral approximants /l/, and rhotics /r/ produced by Athenian Greek and\nCypriot Greek speakers. To estimate the co-articulatory effects of sonorants on\nthe adjacent vowels' F1 - F4 formant frequencies, we developed polynomial\nmodels of the adjacent vowel's formant contours. We found significant effects\nof language variety (sociolinguistic information) on the spectral moments of\neach sonorant /m/, /n/, /l/, /r/ (except between /m/ and /n/) and on the\nformant contours of the adjacent vowel. All sonorants (including /m/ and /n/)\nhad distinct effects on adjacent vowel's formant contours, especially for F3\nand F4. The study highlights that the combination of spectral moments and\ncoarticulatory effects of sonorants determines linguistic (stress and phonemic\ncategory) and sociolinguistic (language variety) characteristics of sonorants.\nIt also provides the first comparative acoustic analysis of Athenian Greek and\nCypriot Greek sonorants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Themistocleous_C/0/1/0/all/0/1\">Charalambos Themistocleous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fyndanis_V/0/1/0/all/0/1\">Valantis Fyndanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsapkini_K/0/1/0/all/0/1\">Kyrana Tsapkini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Input Length Matters: An Empirical Study Of RNN-T And MWER Training For Long-form Telephony Speech Recognition. (arXiv:2110.03841v1 [eess.AS])","link":"http://arxiv.org/abs/2110.03841","description":"<p>End-to-end models have achieved state-of-the-art results on several automatic\nspeech recognition tasks. However, they perform poorly when evaluated on\nlong-form data, e.g., minutes long conversational telephony audio. One reason\nthe model fails on long-form speech is that it has only seen short utterances\nduring training. This paper presents an empirical study on the effect of\ntraining utterance length on the word error rate (WER) for RNN-transducer\n(RNN-T) model. We compare two widely used training objectives, log loss (or\nRNN-T loss) and minimum word error rate (MWER) loss. We conduct experiments on\ntelephony datasets in four languages. Our experiments show that for both\nlosses, the WER on long-form speech reduces substantially as the training\nutterance length increases. The average relative WER gain is 15.7% for log loss\nand 8.8% for MWER loss. When training on short utterances, MWER loss leads to a\nlower WER than the log loss. Such difference between the two losses diminishes\nwhen the input length increases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyun Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1\">Yanwei Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Doutre_T/0/1/0/all/0/1\">Thibault Doutre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_L/0/1/0/all/0/1\">Liangliang Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Translation Verbosity Control for Automatic Dubbing. (arXiv:2110.03847v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03847","description":"<p>Automatic dubbing aims at seamlessly replacing the speech in a video document\nwith synthetic speech in a different language. The task implies many\nchallenges, one of which is generating translations that not only convey the\noriginal content, but also match the duration of the corresponding utterances.\nIn this paper, we focus on the problem of controlling the verbosity of machine\ntranslation output, so that subsequent steps of our automatic dubbing pipeline\ncan generate dubs of better quality. We propose new methods to control the\nverbosity of MT output and compare them against the state of the art with both\nintrinsic and extrinsic evaluations. For our experiments we use a public data\nset to dub English speeches into French, Italian, German and Spanish. Finally,\nwe report extensive subjective tests that measure the impact of MT verbosity\ncontrol on the final quality of dubbed video clips.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lakew_S/0/1/0/all/0/1\">Surafel M. Lakew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federico_M/0/1/0/all/0/1\">Marcello Federico</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_C/0/1/0/all/0/1\">Cuong Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virkar_Y/0/1/0/all/0/1\">Yogesh Virkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barra_Chicote_R/0/1/0/all/0/1\">Roberto Barra-Chicote</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Enyedi_R/0/1/0/all/0/1\">Robert Enyedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speeding up Deep Model Training by Sharing Weights and Then Unsharing. (arXiv:2110.03848v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03848","description":"<p>We propose a simple and efficient approach for training the BERT model. Our\napproach exploits the special structure of BERT that contains a stack of\nrepeated modules (i.e., transformer encoders). Our proposed approach first\ntrains BERT with the weights shared across all the repeated modules till some\npoint. This is for learning the commonly shared component of weights across all\nrepeated layers. We then stop weight sharing and continue training until\nconvergence. We present theoretic insights for training by sharing weights then\nunsharing with analysis for simplified models. Empirical experiments on the\nBERT model show that our method yields better performance of trained models,\nand significantly reduces the number of training iterations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiaodan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A study on the efficacy of model pre-training in developing neural text-to-speech system. (arXiv:2110.03857v1 [eess.AS])","link":"http://arxiv.org/abs/2110.03857","description":"<p>In the development of neural text-to-speech systems, model pre-training with\na large amount of non-target speakers' data is a common approach. However, in\nterms of ultimately achieved system performance for target speaker(s), the\nactual benefits of model pre-training are uncertain and unstable, depending\nvery much on the quantity and text content of training data. This study aims to\nunderstand better why and how model pre-training can positively contribute to\nTTS system performance. It is postulated that the pre-training process plays a\ncritical role in learning text-related variation in speech, while further\ntraining with the target speaker's data aims to capture the speaker-related\nvariation. Different test sets are created with varying degrees of similarity\nto target speaker data in terms of text content. Experiments show that\nleveraging a speaker-independent TTS trained on speech data with diverse text\ncontent can improve the target speaker TTS on domain-mismatched text. We also\nattempt to reduce the amount of pre-training data for a new text domain and\nimprove the data and computational efficiency. It is found that the TTS system\ncould achieve comparable performance when the pre-training data is reduced to\n1/8 of its original size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1\">Guangyan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_D/0/1/0/all/0/1\">Daxin Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_Y/0/1/0/all/0/1\">Ying Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_T/0/1/0/all/0/1\">Tan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QTN-VQC: An End-to-End Learning framework for Quantum Neural Networks. (arXiv:2110.03861v1 [quant-ph])","link":"http://arxiv.org/abs/2110.03861","description":"<p>The advent of noisy intermediate-scale quantum (NISQ) computers raises a\ncrucial challenge to design quantum neural networks for fully quantum learning\ntasks. To bridge the gap, this work proposes an end-to-end learning framework\nnamed QTN-VQC, by introducing a trainable quantum tensor network (QTN) for\nquantum embedding on a variational quantum circuit (VQC). The architecture of\nQTN is composed of a parametric tensor-train network for feature extraction and\na tensor product encoding for quantum encoding. We highlight the QTN for\nquantum embedding in terms of two perspectives: (1) we theoretically\ncharacterize QTN by analyzing its representation power of input features; (2)\nQTN enables an end-to-end parametric model pipeline, namely QTN-VQC, from the\ngeneration of quantum embedding to the output measurement. Our experiments on\nthe MNIST dataset demonstrate the advantages of QTN for quantum embedding over\nother quantum embedding approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Qi_J/0/1/0/all/0/1\">Jun Qi</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Cross-Lingual Transfer of Structured Predictors without Source Data. (arXiv:2110.03866v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03866","description":"<p>Providing technologies to communities or domains where training data is\nscarce or protected e.g., for privacy reasons, is becoming increasingly\nimportant. To that end, we generalise methods for unsupervised transfer from\nmultiple input models for structured prediction. We show that the means of\naggregating over the input models is critical, and that multiplying marginal\nprobabilities of substructures to obtain high-probability structures for\ndistant supervision is substantially better than taking the union of such\nstructures over the input models, as done in prior work. Testing on 18\nlanguages, we demonstrate that the method works in a cross-lingual setting,\nconsidering both dependency parsing and part-of-speech structured prediction\nproblems. Our analyses show that the proposed method produces less noisy labels\nfor the distant supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurniawan_K/0/1/0/all/0/1\">Kemal Kurniawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frermann_L/0/1/0/all/0/1\">Lea Frermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_P/0/1/0/all/0/1\">Philip Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation of professions in entertainment media: Insights into frequency and sentiment trends through computational text analysis. (arXiv:2110.03873v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03873","description":"<p>Societal ideas and trends dictate media narratives and cinematic depictions\nwhich in turn influences people's beliefs and perceptions of the real world.\nMedia portrayal of culture, education, government, religion, and family affect\ntheir function and evolution over time as people interpret and perceive these\nrepresentations and incorporate them into their beliefs and actions. It is\nimportant to study media depictions of these social structures so that they do\nnot propagate or reinforce negative stereotypes, or discriminate against any\ndemographic section. In this work, we examine media representation of\nprofessions and provide computational insights into their incidence, and\nsentiment expressed, in entertainment media content. We create a searchable\ntaxonomy of professional groups and titles to facilitate their retrieval from\nspeaker-agnostic text passages like movie and television (TV) show subtitles.\nWe leverage this taxonomy and relevant natural language processing (NLP) models\nto create a corpus of professional mentions in media content, spanning more\nthan 136,000 IMDb titles over seven decades (1950-2017). We analyze the\nfrequency and sentiment trends of different occupations, study the effect of\nmedia attributes like genre, country of production, and title type on these\ntrends, and investigate if the incidence of professions in media subtitles\ncorrelate with their real-world employment statistics. We observe increased\nmedia mentions of STEM, arts, sports, and entertainment occupations in the\nanalyzed subtitles, and a decreased frequency of manual labor jobs and military\noccupations. The sentiment expressed toward lawyers, police, and doctors is\nbecoming negative over time, whereas astronauts, musicians, singers, and\nengineers are mentioned favorably. Professions that employ more people have\nincreased media frequency, supporting our hypothesis that media acts as a\nmirror to society.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baruah_S/0/1/0/all/0/1\">Sabyasachee Baruah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somandepalli_K/0/1/0/all/0/1\">Krishna Somandepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phone-to-audio alignment without text: A Semi-supervised Approach. (arXiv:2110.03876v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03876","description":"<p>The task of phone-to-audio alignment has many applications in speech\nresearch. Here we introduce two Wav2Vec2-based models for both text-dependent\nand text-independent phone-to-audio alignment. The proposed Wav2Vec2-FS, a\nsemi-supervised model, directly learns phone-to-audio alignment through\ncontrastive learning and a forward sum loss, and can be coupled with a\npretrained phone recognizer to achieve text-independent alignment. The other\nmodel, Wav2Vec2-FC, is a frame classification model trained on forced aligned\nlabels that can both perform forced alignment and text-independent\nsegmentation. Evaluation results suggest that both proposed methods, even when\ntranscriptions are not available, generate highly close results to existing\nforced alignment tools. Our work presents a neural pipeline of fully automated\nphone-to-audio alignment. Code and pretrained models are available at\nhttps://github.com/lingjzhu/charsiu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining the Attention Mechanism of End-to-End Speech Recognition Using Decision Trees. (arXiv:2110.03879v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03879","description":"<p>The attention mechanism has largely improved the performance of end-to-end\nspeech recognition systems. However, the underlying behaviours of attention is\nnot yet clearer. In this study, we use decision trees to explain how the\nattention mechanism impact itself in speech recognition. The results indicate\nthat attention levels are largely impacted by their previous states rather than\nthe encoder and decoder patterns. Additionally, the default attention mechanism\nseems to put more weights on closer states, but behaves poorly on modelling\nlong-term dependencies of attention states.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuanchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wenji Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chenghao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanyan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining. (arXiv:2110.03888v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03888","description":"<p>Recent expeditious developments in deep learning algorithms, distributed\ntraining, and even hardware design for large models have enabled training\nextreme-scale models, say GPT-3 and Switch Transformer possessing hundreds of\nbillions or even trillions of parameters. However, under limited resources,\nextreme-scale model training that requires enormous amounts of computes and\nmemory footprint suffers from frustratingly low efficiency in model\nconvergence. In this paper, we propose a simple training strategy called\n\"Pseudo-to-Real\" for high-memory-footprint-required large models.\nPseudo-to-Real is compatible with large models with architecture of sequential\nlayers. We demonstrate a practice of pretraining unprecedented\n10-trillion-parameter model, an order of magnitude larger than the\nstate-of-the-art, on solely 512 GPUs within 10 days. Besides demonstrating the\napplication of Pseudo-to-Real, we also provide a technique, Granular CPU\noffloading, to manage CPU memory for training large model and maintain high GPU\nutilities. Fast training of extreme-scale models on a decent amount of\nresources can bring much smaller carbon footprint and contribute to greener AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jinze Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Le Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xianyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALL-IN-ONE: Multi-Task Learning BERT models for Evaluating Peer Assessments. (arXiv:2110.03895v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03895","description":"<p>Peer assessment has been widely applied across diverse academic fields over\nthe last few decades and has demonstrated its effectiveness. However, the\nadvantages of peer assessment can only be achieved with high-quality peer\nreviews. Previous studies have found that high-quality review comments usually\ncomprise several features (e.g., contain suggestions, mention problems, use a\npositive tone). Thus, researchers have attempted to evaluate peer-review\ncomments by detecting different features using various machine learning and\ndeep learning models. However, there is no single study that investigates using\na multi-task learning (MTL) model to detect multiple features simultaneously.\nThis paper presents two MTL models for evaluating peer-review comments by\nleveraging the state-of-the-art pre-trained language representation models BERT\nand DistilBERT. Our results demonstrate that BERT-based models significantly\noutperform previous GloVe-based methods by around 6% in F1-score on tasks of\ndetecting a single feature, and MTL further improves performance while reducing\nmodel size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1\">Qinjin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jialin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yunkai Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_P/0/1/0/all/0/1\">Parvez Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehringer_E/0/1/0/all/0/1\">Edward F. Gehringer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CheerBots: Chatbots toward Empathy and Emotionusing Reinforcement Learning. (arXiv:2110.03949v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03949","description":"<p>Apart from the coherence and fluency of responses, an empathetic chatbot\nemphasizes more on people's feelings. By considering altruistic behaviors\nbetween human interaction, empathetic chatbots enable people to get a better\ninteractive and supportive experience. This study presents a framework whereby\nseveral empathetic chatbots are based on understanding users' implied feelings\nand replying empathetically for multiple dialogue turns. We call these chatbots\nCheerBots. CheerBots can be retrieval-based or generative-based and were\nfinetuned by deep reinforcement learning. To respond in an empathetic way, we\ndevelop a simulating agent, a Conceptual Human Model, as aids for CheerBots in\ntraining with considerations on changes in user's emotional states in the\nfuture to arouse sympathy. Finally, automatic metrics and human rating results\ndemonstrate that CheerBots outperform other baseline chatbots and achieves\nreciprocal altruism. The code and the pre-trained models will be made\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jhan_J/0/1/0/all/0/1\">Jiun-Hao Jhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chao-Peng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeng_S/0/1/0/all/0/1\">Shyh-Kang Jeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceived and Intended Sarcasm Detection with Graph Attention Networks. (arXiv:2110.04001v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04001","description":"<p>Existing sarcasm detection systems focus on exploiting linguistic markers,\ncontext, or user-level priors. However, social studies suggest that the\nrelationship between the author and the audience can be equally relevant for\nthe sarcasm usage and interpretation. In this work, we propose a framework\njointly leveraging (1) a user context from their historical tweets together\nwith (2) the social information from a user's conversational neighborhood in an\ninteraction graph, to contextualize the interpretation of the post. We use\ngraph attention networks (GAT) over users and tweets in a conversation thread,\ncombined with dense user history representations. Apart from achieving\nstate-of-the-art results on the recently published dataset of 19k Twitter users\nwith 30K labeled tweets, adding 10M unlabeled tweets as context, our results\nindicate that the model contributes to interpreting the sarcastic intentions of\nan author more than to predicting the sarcasm perception by others.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plepi_J/0/1/0/all/0/1\">Joan Plepi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flek_L/0/1/0/all/0/1\">Lucie Flek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Math-Aware Automated Classification and Similarity Search of Scientific Publications: Methods of Mathematical Content Representations. (arXiv:2110.04040v1 [cs.IR])","link":"http://arxiv.org/abs/2110.04040","description":"<p>In this paper, we investigate mathematical content representations suitable\nfor the automated classification of and the similarity search in STEM documents\nusing standard machine learning algorithms: the Latent Dirichlet Allocation\n(LDA) and the Latent Semantic Indexing (LSI). The methods are evaluated on a\nsubset of arXiv.org papers with the Mathematics Subject Classification (MSC) as\na reference classification and using the standard precision/recall/F1-measure\nmetrics. The results give insight into how different math representations may\ninfluence the performance of the classification and similarity search tasks in\nSTEM repositories. Non-surprisingly, machine learning methods are able to grab\ndistributional semantics from textual tokens. A proper selection of weighted\ntokens representing math may improve the quality of the results slightly. A\nstructured math representation that imitates successful text-processing\ntechniques with math is shown to yield better results than flat TeX tokens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+R%5Cr%7Bu%7Dzicka_M/0/1/0/all/0/1\">Michal R&#x16f;&#x17e;i&#x10d;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Do Things without Words: Modeling Semantic Drift of Emoji. (arXiv:2110.04093v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04093","description":"<p>Emoji have become a significant part of our informal textual communication.\nPrevious work addressing the societal and linguistic functions of emoji\noverlook the evolving meaning of the symbol. This evolution could be addressed\nthrough the framework of semantic drifts. In this paper we model and analyze\nthe semantic drift of emoji and discuss the features that may be contributing\nto the drift, some are unique to emoji and some are more general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arviv_E/0/1/0/all/0/1\">Eyal Arviv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsur_O/0/1/0/all/0/1\">Oren Tsur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Conditional End-to-End ASR with CTC and Multi-Granular Subword Units. (arXiv:2110.04109v1 [eess.AS])","link":"http://arxiv.org/abs/2110.04109","description":"<p>In end-to-end automatic speech recognition (ASR), a model is expected to\nimplicitly learn representations suitable for recognizing a word-level\nsequence. However, the huge abstraction gap between input acoustic signals and\noutput linguistic tokens makes it challenging for a model to learn the\nrepresentations. In this work, to promote the word-level representation\nlearning in end-to-end ASR, we propose a hierarchical conditional model that is\nbased on connectionist temporal classification (CTC). Our model is trained by\nauxiliary CTC losses applied to intermediate layers, where the vocabulary size\nof each target subword sequence is gradually increased as the layer becomes\nclose to the word-level output. Here, we make each level of sequence prediction\nexplicitly conditioned on the previous sequences predicted at lower levels.\nWith the proposed approach, we expect the proposed model to learn the\nword-level representations effectively by exploiting a hierarchy of linguistic\nstructures. Experimental results on LibriSpeech-{100h, 960h} and TEDLIUM2\ndemonstrate that the proposed model improves over a standard CTC-based model\nand other competitive models from prior work. We further analyze the results to\nconfirm the effectiveness of the intended representation learning with our\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_Y/0/1/0/all/0/1\">Yosuke Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karube_K/0/1/0/all/0/1\">Keita Karube</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ogawa_T/0/1/0/all/0/1\">Tetsuji Ogawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kobayashi_T/0/1/0/all/0/1\">Tetsunori Kobayashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I Do Not Understand What I Cannot Define: Automatic Question Generation With Pedagogically-Driven Content Selection. (arXiv:2110.04123v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04123","description":"<p>Most learners fail to develop deep text comprehension when reading textbooks\npassively. Posing questions about what learners have read is a well-established\nway of fostering their text comprehension. However, many textbooks lack\nself-assessment questions because authoring them is timeconsuming and\nexpensive. Automatic question generators may alleviate this scarcity by\ngenerating sound pedagogical questions. However, generating questions\nautomatically poses linguistic and pedagogical challenges. What should we ask?\nAnd, how do we phrase the question automatically? We address those challenges\nwith an automatic question generator grounded in learning theory. The paper\nintroduces a novel pedagogically meaningful content selection mechanism to find\nquestion-worthy sentences and answers in arbitrary textbook contents. We\nconducted an empirical evaluation study with educational experts, annotating\n150 generated questions in six different domains. Results indicate a high\nlinguistic quality of the generated questions. Furthermore, the evaluation\nresults imply that the majority of the generated questions inquire central\ninformation related to the given text and may foster text comprehension in\nspecific learning scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Steuer_T/0/1/0/all/0/1\">Tim Steuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filighera_A/0/1/0/all/0/1\">Anna Filighera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuser_T/0/1/0/all/0/1\">Tobias Meuser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rensing_C/0/1/0/all/0/1\">Christoph Rensing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text analysis and deep learning: A network approach. (arXiv:2110.04151v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04151","description":"<p>Much information available to applied researchers is contained within written\nlanguage or spoken text. Deep language models such as BERT have achieved\nunprecedented success in many applications of computational linguistics.\nHowever, much less is known about how these models can be used to analyze\nexisting text. We propose a novel method that combines transformer models with\nnetwork analysis to form a self-referential representation of language use\nwithin a corpus of interest. Our approach produces linguistic relations\nstrongly consistent with the underlying model as well as mathematically\nwell-defined operations on them, while reducing the amount of discretionary\nchoices of representation and distance measures. It represents, to the best of\nour knowledge, the first unsupervised method to extract semantic networks\ndirectly from deep language models. We illustrate our approach in a semantic\nanalysis of the term \"founder\". Using the entire corpus of Harvard Business\nReview from 1980 to 2020, we find that ties in our network track the semantics\nof discourse over time, and across contexts, identifying and relating clusters\nof semantic and syntactic relations. Finally, we discuss how this method can\nalso complement and inform analyses of the behavior of deep learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marquart_I/0/1/0/all/0/1\">Ingo Marquart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Decoding for Compositional Generalization in Transformers. (arXiv:2110.04169v1 [cs.LG])","link":"http://arxiv.org/abs/2110.04169","description":"<p>Deep learning models do well at generalizing to in-distribution data but\nstruggle to generalize compositionally, i.e., to combine a set of learned\nprimitives to solve more complex tasks. In particular, in sequence-to-sequence\n(seq2seq) learning, transformers are often unable to predict correct outputs\nfor even marginally longer examples than those seen during training. This paper\nintroduces iterative decoding, an alternative to seq2seq learning that (i)\nimproves transformer compositional generalization and (ii) evidences that, in\ngeneral, seq2seq transformers do not learn iterations that are not unrolled.\nInspired by the idea of compositionality -- that complex tasks can be solved by\ncomposing basic primitives -- training examples are broken down into a sequence\nof intermediate steps that the transformer then learns iteratively. At\ninference time, the intermediate outputs are fed back to the transformer as\nintermediate inputs until an end-of-iteration token is predicted. Through\nnumerical experiments, we show that transfomers trained via iterative decoding\noutperform their seq2seq counterparts on the PCFG dataset, and solve the\nproblem of calculating Cartesian products between vectors longer than those\nseen during training with 100% accuracy, a task at which seq2seq models have\nbeen shown to fail. We also illustrate a limitation of iterative decoding,\nspecifically, that it can make sorting harder to learn on the CFQ dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_L/0/1/0/all/0/1\">Luana Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development of an Extractive Title Generation System Using Titles of Papers of Top Conferences for Intermediate English Students. (arXiv:2110.04204v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04204","description":"<p>The formulation of good academic paper titles in English is challenging for\nintermediate English authors (particularly students). This is because such\nauthors are not aware of the type of titles that are generally in use. We aim\nto realize a support system for formulating more effective English titles for\nintermediate English and beginner authors. This study develops an extractive\ntitle generation system that formulates titles from keywords extracted from an\nabstract. Moreover, we realize a title evaluation model that can evaluate the\nappropriateness of paper titles. We train the model with titles of\ntop-conference papers by using BERT. This paper describes the training data,\nimplementation, and experimental results. The results show that our evaluation\nmodel can identify top-conference titles more effectively than intermediate\nEnglish and beginner students.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaku_K/0/1/0/all/0/1\">Kento Kaku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kikuchi_M/0/1/0/all/0/1\">Masato Kikuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozono_T/0/1/0/all/0/1\">Tadachika Ozono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shintani_T/0/1/0/all/0/1\">Toramatsu Shintani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive String Representation Learning using Synthetic Data. (arXiv:2110.04217v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04217","description":"<p>String representation Learning (SRL) is an important task in the field of\nNatural Language Processing, but it remains under-explored. The goal of SRL is\nto learn dense and low-dimensional vectors (or embeddings) for encoding\ncharacter sequences. The learned representation from this task can be used in\nmany downstream application tasks such as string similarity matching or lexical\nnormalization. In this paper, we propose a new method for to train a SRL model\nby only using synthetic data. Our approach makes use of Contrastive Learning in\norder to maximize similarity between related strings while minimizing it for\nunrelated strings. We demonstrate the effectiveness of our approach by\nevaluating the learned representation on the task of string similarity\nmatching. Codes, data and pretrained models will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaratiana_U/0/1/0/all/0/1\">Urchade Zaratiana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"lambeq: An Efficient High-Level Python Library for Quantum NLP. (arXiv:2110.04236v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04236","description":"<p>We present lambeq, the first high-level Python library for Quantum Natural\nLanguage Processing (QNLP). The open-source toolkit offers a detailed hierarchy\nof modules and classes implementing all stages of a pipeline for converting\nsentences to string diagrams, tensor networks, and quantum circuits ready to be\nused on a quantum computer. lambeq supports syntactic parsing, rewriting and\nsimplification of string diagrams, ansatz creation and manipulation, as well as\na number of compositional models for preparing quantum-friendly representations\nof sentences, employing various degrees of syntax sensitivity. We present the\ngeneric architecture and describe the most important modules in detail,\ndemonstrating the usage with illustrative examples. Further, we test the\ntoolkit in practice by using it to perform a number of experiments on simple\nNLP tasks, implementing both classical and quantum pipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kartsaklis_D/0/1/0/all/0/1\">Dimitri Kartsaklis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_I/0/1/0/all/0/1\">Ian Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_R/0/1/0/all/0/1\">Richie Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pearson_A/0/1/0/all/0/1\">Anna Pearson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenz_R/0/1/0/all/0/1\">Robin Lorenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toumi_A/0/1/0/all/0/1\">Alexis Toumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felice_G/0/1/0/all/0/1\">Giovanni de Felice</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meichanetzidis_K/0/1/0/all/0/1\">Konstantinos Meichanetzidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_S/0/1/0/all/0/1\">Stephen Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coecke_B/0/1/0/all/0/1\">Bob Coecke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VieSum: How Robust Are Transformer-based Models on Vietnamese Summarization?. (arXiv:2110.04257v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04257","description":"<p>Text summarization is a challenging task within natural language processing\nthat involves text generation from lengthy input sequences. While this task has\nbeen widely studied in English, there is very limited research on summarization\nfor Vietnamese text. In this paper, we investigate the robustness of\ntransformer-based encoder-decoder architectures for Vietnamese abstractive\nsummarization. Leveraging transfer learning and self-supervised learning, we\nvalidate the performance of the methods on two Vietnamese datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hieu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_L/0/1/0/all/0/1\">Long Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anibal_J/0/1/0/all/0/1\">James Anibal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peltekian_A/0/1/0/all/0/1\">Alec Peltekian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hieu Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taming Sparsely Activated Transformer with Stochastic Experts. (arXiv:2110.04260v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04260","description":"<p>Sparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can\neasily scale to have outrageously large amounts of parameters without\nsignificant increase in computational cost. However, SAMs are reported to be\nparameter inefficient such that larger models do not always lead to better\nperformance. While most on-going research focuses on improving SAMs models by\nexploring methods of routing inputs to experts, our analysis reveals that such\nresearch might not lead to the solution we expect, i.e., the commonly-used\nrouting methods based on gating mechanisms do not work better than randomly\nrouting inputs to experts. In this paper, we propose a new expert-based model,\nTHOR (Transformer witH StOchastic ExpeRts). Unlike classic expert-based models,\nsuch as the Switch Transformer, experts in THOR are randomly activated for each\ninput during training and inference. THOR models are trained using a\nconsistency regularized loss, where experts learn not only from training data\nbut also from other experts as teachers, such that all the experts make\nconsistent predictions. We validate the effectiveness of THOR on machine\ntranslation tasks. Results show that THOR models are more parameter efficient\nin that they significantly outperform the Transformer and MoE models across\nvarious settings. For example, in multilingual translation, THOR outperforms\nthe Switch Transformer by 2 BLEU scores, and obtains the same BLEU score as\nthat of a state-of-the-art MoE model that is 18 times larger. Our code is\npublicly available at: github.com/microsoft/Stochastic-Mixture-of-Experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_H/0/1/0/all/0/1\">Hany Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Heterogeneous Characteristics of Layers in ASR Models for More Efficient Training. (arXiv:2110.04267v1 [cs.LG])","link":"http://arxiv.org/abs/2110.04267","description":"<p>Transformer-based architectures have been the subject of research aimed at\nunderstanding their overparameterization and the non-uniform importance of\ntheir layers. Applying these approaches to Automatic Speech Recognition, we\ndemonstrate that the state-of-the-art Conformer models generally have multiple\nambient layers. We study the stability of these layers across runs and model\nsizes, propose that group normalization may be used without disrupting their\nformation, and examine their correlation with model weight updates in each\nlayer. Finally, we apply these findings to Federated Learning in order to\nimprove the training procedure, by targeting Federated Dropout to layers by\nimportance. This allows us to reduce the model size optimized by clients\nwithout quality degradation, and shows potential for future exploration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lillian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guliani_D/0/1/0/all/0/1\">Dhruv Guliani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabel_A/0/1/0/all/0/1\">Andreas Kabel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motta_G/0/1/0/all/0/1\">Giovanni Motta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local and Global Context-Based Pairwise Models for Sentence Ordering. (arXiv:2110.04291v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04291","description":"<p>Sentence Ordering refers to the task of rearranging a set of sentences into\nthe appropriate coherent order. For this task, most previous approaches have\nexplored global context-based end-to-end methods using Sequence Generation\ntechniques. In this paper, we put forward a set of robust local and global\ncontext-based pairwise ordering strategies, leveraging which our prediction\nstrategies outperform all previous works in this domain. Our proposed encoding\nmethod utilizes the paragraph's rich global contextual information to predict\nthe pairwise order using novel transformer architectures. Analysis of the two\nproposed decoding strategies helps better explain error propagation in pairwise\nmodels. This approach is the most accurate pure pairwise model and our encoding\nstrategy also significantly improves the performance of other recent approaches\nthat use pairwise models, including the previous state-of-the-art,\ndemonstrating the research novelty and generalizability of this work.\nAdditionally, we show how the pre-training task for ALBERT helps it to\nsignificantly outperform BERT, despite having considerably lesser parameters.\nThe extensive experimental results, architectural analysis and ablation studies\ndemonstrate the effectiveness and superiority of the proposed models compared\nto the previous state-of-the-art, besides providing a much better understanding\nof the functioning of pairwise models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manku_R/0/1/0/all/0/1\">Ruskin Raj Manku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1\">Aditya Jyoti Paul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From SCAN to Real Data: Systematic Generalization via Meaningful Learning. (arXiv:2003.06658v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2003.06658","description":"<p>Humans can systematically generalize to novel compositions of existing\nconcepts. There have been extensive conjectures into the extent to which neural\nnetworks can do the same. Recent arguments supported by evidence on the SCAN\ndataset claim that neural networks are inherently ineffective in such cognitive\ncapacity. In this paper, we revisit systematic generalization from the\nperspective of meaningful learning, an exceptional capability of humans to\nlearn new concepts by connecting them with other previously known knowledge. We\npropose to augment a training dataset in either an inductive or deductive\nmanner to build semantic links between new and old concepts. Our observations\non SCAN suggest that, following the meaningful learning principle, modern\nsequence-to-sequence models, including RNNs, CNNs, and Transformers, can\nsuccessfully generalize to compositions of new concepts. We further validate\nour findings on two real-world datasets on semantic parsing and consistent\ncompositional generalization is also observed. Moreover, our experiments\ndemonstrate that both prior knowledge and semantic linking play a key role to\nachieve systematic generalization. Meanwhile, inductive learning generally\nworks better than deductive learning in our experiments. Finally, we provide an\nexplanation for data augmentation techniques by concluding them into either\ninductive-based or deductive-based meaningful learning. We hope our findings\nwill encourage excavating existing neural networks' potential in systematic\ngeneralization through more advanced learning schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_N/0/1/0/all/0/1\">Ning Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"English Machine Reading Comprehension Datasets: A Survey. (arXiv:2101.10421v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.10421","description":"<p>This paper surveys 60 English Machine Reading Comprehension datasets, with a\nview to providing a convenient resource for other researchers interested in\nthis problem. We categorize the datasets according to their question and answer\nform and compare them across various dimensions including size, vocabulary,\ndata source, method of creation, human performance level, and first question\nword. Our analysis reveals that Wikipedia is by far the most common data source\nand that there is a relative lack of why, when, and where questions across\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dzendzik_D/0/1/0/all/0/1\">Daria Dzendzik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogel_C/0/1/0/all/0/1\">Carl Vogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jennifer Foster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based end-to-end speech recognition with residual Gaussian-based self-attention. (arXiv:2103.15722v4 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2103.15722","description":"<p>Self-attention (SA), which encodes vector sequences according to their\npairwise similarity, is widely used in speech recognition due to its strong\ncontext modeling ability. However, when applied to long sequence data, its\naccuracy is reduced. This is caused by the fact that its weighted average\noperator may lead to the dispersion of the attention distribution, which\nresults in the relationship between adjacent signals ignored. To address this\nissue, in this paper, we introduce relative-position-awareness self-attention\n(RPSA). It not only maintains the global-range dependency modeling ability of\nself-attention, but also improves the localness modeling ability. Because the\nlocal window length of the original RPSA is fixed and sensitive to different\ntest data, here we propose Gaussian-based self-attention (GSA) whose window\nlength is learnable and adaptive to the test data automatically. We further\ngeneralize GSA to a new residual Gaussian self-attention (resGSA) for the\nperformance improvement. We apply RPSA, GSA, and resGSA to Transformer-based\nspeech recognition respectively. Experimental results on the AISHELL-1 Mandarin\nspeech recognition corpus demonstrate the effectiveness of the proposed\nmethods. For example, the resGSA-Transformer achieves a character error rate\n(CER) of 5.86% on the test set, which is relative 7.8% lower than that of the\nSA-Transformer. Although the performance of the proposed resGSA-Transformer is\nonly slightly better than that of the RPSA-Transformer, it does not have to\ntune the window length manually.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chengdong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Menglong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relaxing the Conditional Independence Assumption of CTC-based ASR by Conditioning on Intermediate Predictions. (arXiv:2104.02724v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2104.02724","description":"<p>This paper proposes a method to relax the conditional independence assumption\nof connectionist temporal classification (CTC)-based automatic speech\nrecognition (ASR) models. We train a CTC-based ASR model with auxiliary CTC\nlosses in intermediate layers in addition to the original CTC loss in the last\nlayer. During both training and inference, each generated prediction in the\nintermediate layers is summed to the input of the next layer to condition the\nprediction of the last layer on those intermediate predictions. Our method is\neasy to implement and retains the merits of CTC-based ASR: a simple model\narchitecture and fast decoding speed. We conduct experiments on three different\nASR corpora. Our proposed method improves a standard CTC model significantly\n(e.g., more than 20 % relative word error rate reduction on the WSJ corpus)\nwith a little computational overhead. Moreover, for the TEDLIUM2 corpus and the\nAISHELL-1 corpus, it achieves a comparable performance to a strong\nautoregressive model with beam search, but the decoding speed is at least 30\ntimes faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nozaki_J/0/1/0/all/0/1\">Jumon Nozaki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Komatsu_T/0/1/0/all/0/1\">Tatsuya Komatsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Weakly Supervised Dataset of Fine-Grained Emotions in Portuguese. (arXiv:2108.07638v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.07638","description":"<p>Affective Computing is the study of how computers can recognize, interpret\nand simulate human affects. Sentiment Analysis is a common task inNLP related\nto this topic, but it focuses only on emotion valence (positive, negative,\nneutral). An emerging approach in NLP is Emotion Recognition, which relies on\nfined-grained classification. This research describes an approach to create a\nlexical-based weakly supervised corpus for fine-grained emotion in Portuguese.\nWe evaluated our dataset by fine-tuning a transformer-based language model\n(BERT) and validating it on a Gold Standard annotated validation set. Our\nresults (F1-score=.64) suggest lexical-based weak supervision as an appropriate\nstrategy for initial work in low resourced environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cortiz_D/0/1/0/all/0/1\">Diogo Cortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1\">Jefferson O. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calegari_N/0/1/0/all/0/1\">Newton Calegari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Ana Lu&#xed;sa Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_A/0/1/0/all/0/1\">Ana Ang&#xe9;lica Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botelho_C/0/1/0/all/0/1\">Carolina Botelho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rego_G/0/1/0/all/0/1\">Gabriel Gaudencio R&#xea;go</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampaio_W/0/1/0/all/0/1\">Waldir Sampaio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boggio_P/0/1/0/all/0/1\">Paulo Sergio Boggio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rule-based Morphological Inflection Improves Neural Terminology Translation. (arXiv:2109.04620v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04620","description":"<p>Current approaches to incorporating terminology constraints in machine\ntranslation (MT) typically assume that the constraint terms are provided in\ntheir correct morphological forms. This limits their application to real-world\nscenarios where constraint terms are provided as lemmas. In this paper, we\nintroduce a modular framework for incorporating lemma constraints in neural MT\n(NMT) in which linguistic knowledge and diverse types of NMT models can be\nflexibly applied. It is based on a novel cross-lingual inflection module that\ninflects the target lemma constraints based on the source context. We explore\nlinguistically motivated rule-based and data-driven neural-based inflection\nmodules and design English-German health and English-Lithuanian news test\nsuites to evaluate them in domain adaptation and low-resource MT settings.\nResults show that our rule-based inflection module helps NMT models incorporate\nlemma constraints more accurately than a neural module and outperforms the\nexisting end-to-end approach with lower training costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weijia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carpuat_M/0/1/0/all/0/1\">Marine Carpuat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation. (arXiv:2109.05729v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05729","description":"<p>In this paper, we take the advantage of previous pre-trained models (PTMs)\nand propose a novel Chinese Pre-trained Unbalanced Transformer (CPT). Different\nfrom previous Chinese PTMs, CPT is designed for both natural language\nunderstanding (NLU) and natural language generation (NLG) tasks. CPT consists\nof three parts: a shared encoder, an understanding decoder, and a generation\ndecoder. Two specific decoders with a shared encoder are pre-trained with\nmasked language modeling (MLM) and denoising auto-encoding (DAE) tasks,\nrespectively. With the partially shared architecture and multi-task\npre-training, CPT can (1) learn specific knowledge of both NLU or NLG tasks\nwith two decoders and (2) be fine-tuned flexibly that fully exploits the\npotential of the model. Moreover, the unbalanced Transformer saves the\ncomputational and storage cost, which makes CPT competitive and greatly\naccelerates the inference of text generation. Experimental results on a wide\nrange of Chinese NLU and NLG tasks show the effectiveness of CPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zhichao Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yitao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Junqi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_L/0/1/0/all/0/1\">Li Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LM-Critic: Language Models for Unsupervised Grammatical Error Correction. (arXiv:2109.06822v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06822","description":"<p>Training a model for grammatical error correction (GEC) requires a set of\nlabeled ungrammatical / grammatical sentence pairs, but manually annotating\nsuch pairs can be expensive. Recently, the Break-It-Fix-It (BIFI) framework has\ndemonstrated strong results on learning to repair a broken program without any\nlabeled examples, but this relies on a perfect critic (e.g., a compiler) that\nreturns whether an example is valid or not, which does not exist for the GEC\ntask. In this work, we show how to leverage a pretrained language model (LM) in\ndefining an LM-Critic, which judges a sentence to be grammatical if the LM\nassigns it a higher probability than its local perturbations. We apply this\nLM-Critic and BIFI along with a large set of unlabeled sentences to bootstrap\nrealistic ungrammatical / grammatical pairs for training a corrector. We\nevaluate our approach on GEC datasets across multiple domains (CoNLL-2014,\nBEA-2019, GMEG-wiki and GMEG-yahoo) and show that it outperforms existing\nmethods in both the unsupervised setting (+7.7 F0.5) and the supervised setting\n(+0.5 F0.5).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models. (arXiv:2109.11797v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.11797","description":"<p>Pre-Trained Vision-Language Models (VL-PTMs) have shown promising\ncapabilities in grounding natural language in image data, facilitating a broad\nvariety of cross-modal tasks. However, we note that there exists a significant\ngap between the objective forms of model pre-training and fine-tuning,\nresulting in a need for large amounts of labeled data to stimulate the visual\ngrounding capability of VL-PTMs for downstream tasks. To address the challenge,\nwe present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt\nTuning), a novel paradigm for tuning VL-PTMs, which reformulates visual\ngrounding into a fill-in-the-blank problem with color-based co-referential\nmarkers in image and text, maximally mitigating the gap. In this way, CPT\nenables strong few-shot and even zero-shot visual grounding capabilities of\nVL-PTMs. Comprehensive experimental results show that the prompt-tuned VL-PTMs\noutperform their fine-tuned counterparts by a large margin (e.g., 17.3%\nabsolute accuracy improvement, and 73.8% relative standard deviation reduction\non average with one shot in RefCOCO evaluation). All the data and codes will be\navailable to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FoodChem: A food-chemical relation extraction model. (arXiv:2110.02019v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02019","description":"<p>In this paper, we present FoodChem, a new Relation Extraction (RE) model for\nidentifying chemicals present in the composition of food entities, based on\ntextual information provided in biomedical peer-reviewed scientific literature.\nThe RE task is treated as a binary classification problem, aimed at identifying\nwhether the contains relation exists between a food-chemical entity pair. This\nis accomplished by fine-tuning BERT, BioBERT and RoBERTa transformer models.\nFor evaluation purposes, a novel dataset with annotated contains relations in\nfood-chemical entity pairs is generated, in a golden and silver version. The\nmodels are integrated into a voting scheme in order to produce the silver\nversion of the dataset which we use for augmenting the individual models, while\nthe manually annotated golden version is used for their evaluation. Out of the\nthree evaluated models, the BioBERT model achieves the best results, with a\nmacro averaged F1 score of 0.902 in the unbalanced augmentation setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cenikj_G/0/1/0/all/0/1\">Gjorgjina Cenikj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seljak_B/0/1/0/all/0/1\">Barbara Korou&#x161;i&#x107; Seljak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eftimov_T/0/1/0/all/0/1\">Tome Eftimov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Conditional Text Generation for Aspect-Based Sentiment Analysis. (arXiv:2110.02334v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02334","description":"<p>Aspect-based sentiment analysis (ABSA) is an NLP task that entails processing\nuser-generated reviews to determine (i) the target being evaluated, (ii) the\naspect category to which it belongs, and (iii) the sentiment expressed towards\nthe target and aspect pair. In this article, we propose transforming ABSA into\nan abstract summary-like conditional text generation task that uses targets,\naspects, and polarities to generate auxiliary statements. To demonstrate the\nefficacy of our task formulation and a proposed system, we fine-tune a\npre-trained model for conditional text generation tasks to get new\nstate-of-the-art results on a few restaurant domains and urban neighborhoods\ndomain benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chebolu_S/0/1/0/all/0/1\">Siva Uday Sampreeth Chebolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1\">Nedim Lipka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cut the CARP: Fishing for zero-shot story evaluation. (arXiv:2110.03111v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03111","description":"<p>Recent advances in large-scale language models (Raffel et al., 2019; Brown et\nal., 2020) have brought significant qualitative and quantitative improvements\nin machine-driven text generation. Despite this, generation and evaluation of\nmachine-generated narrative text remains a challenging problem. Objective\nevaluation of computationally-generated stories may be prohibitively expensive,\nrequire meticulously annotated datasets, or may not adequately measure the\nlogical coherence of a generated story's narratological structure.\n</p>\n<p>Informed by recent advances in contrastive learning (Radford et al., 2021),\nwe present Contrastive Authoring and Reviewing Pairing (CARP): a scalable,\nefficient method for performing qualitatively superior, zero-shot evaluation of\nstories. We show a strong correlation between human evaluation of stories and\nthose of CARP. Model outputs more significantly correlate with corresponding\nhuman input than those language-model based methods which utilize finetuning or\nprompt engineering approaches. We also present and analyze the Story-Critique\nDataset, a new corpora composed of 1.3 million aligned story-critique pairs\nderived from over 80,000 stories. We expect this corpus to be of interest to\nNLP researchers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matiana_S/0/1/0/all/0/1\">Shahbuland Matiana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">JR Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teehan_R/0/1/0/all/0/1\">Ryan Teehan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castricato_L/0/1/0/all/0/1\">Louis Castricato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Leo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frazier_S/0/1/0/all/0/1\">Spencer Frazier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Continual Knowledge Learning of Language Models. (arXiv:2110.03215v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03215","description":"<p>Large Language Models (LMs) are known to encode world knowledge in their\nparameters as they pretrain on a vast amount of web corpus, which is often\nutilized for performing knowledge-dependent downstream tasks such as question\nanswering, fact-checking, and open dialogue. In real-world scenarios, the world\nknowledge stored in the LMs can quickly become outdated as the world changes,\nbut it is non-trivial to avoid catastrophic forgetting and reliably acquire new\nknowledge while preserving invariant knowledge. To push the community towards\nbetter maintenance of ever-changing LMs, we formulate a new continual learning\n(CL) problem called Continual Knowledge Learning (CKL). We construct a new\nbenchmark and metric to quantify the retention of time-invariant world\nknowledge, the update of outdated knowledge, and the acquisition of new\nknowledge. We adopt applicable recent methods from literature to create several\nstrong baselines. Through extensive experiments, we find that CKL exhibits\nunique challenges that are not addressed in previous CL setups, where parameter\nexpansion is necessary to reliably retain and learn knowledge simultaneously.\nBy highlighting the critical causes of knowledge forgetting, we show that CKL\nis a challenging and important problem that helps us better understand and\ntrain ever-changing LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joel Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Seonghyeon Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sohee Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Joongbo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Janghoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyeonghun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Stanley Jungkyu Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Logic-Based Framework for Natural Language Inference in Dutch. (arXiv:2110.03323v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03323","description":"<p>We present a framework for deriving inference relations between Dutch\nsentence pairs. The proposed framework relies on logic-based reasoning to\nproduce inspectable proofs leading up to inference labels; its judgements are\ntherefore transparent and formally verifiable. At its core, the system is\npowered by two ${\\lambda}$-calculi, used as syntactic and semantic theories,\nrespectively. Sentences are first converted to syntactic proofs and terms of\nthe linear ${\\lambda}$-calculus using a choice of two parsers: an Alpino-based\npipeline, and Neural Proof Nets. The syntactic terms are then converted to\nsemantic terms of the simply typed ${\\lambda}$-calculus, via a set of hand\ndesigned type- and term-level transformations. Pairs of semantic terms are then\nfed to an automated theorem prover for natural logic which reasons with them\nwhile using lexical relations found in the Open Dutch WordNet. We evaluate the\nreasoning pipeline on the recently created Dutch natural language inference\ndataset, and achieve promising results, remaining only within a $1.1-3.2{\\%}$\nperformance margin to strong neural baselines. To the best of our knowledge,\nthe reasoning pipeline is the first logic-based system for Dutch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abzianidze_L/0/1/0/all/0/1\">Lasha Abzianidze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kogkalidis_K/0/1/0/all/0/1\">Konstantinos Kogkalidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Retriever-Ranker for dense text retrieval. (arXiv:2110.03611v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03611","description":"<p>Current dense text retrieval models face two typical challenges. First, it\nadopts a siamese dual-encoder architecture to encode query and document\nindependently for fast indexing and searching, whereas neglecting the\nfiner-grained term-wise interactions. This results in a sub-optimal recall\nperformance. Second, it highly relies on a negative sampling technique to build\nup the negative documents in its contrastive loss. To address these challenges,\nwe present Adversarial Retriever-Ranker (AR2), which consists of a dual-encoder\nretriever plus a cross-encoder ranker. The two models are jointly optimized\naccording to a minimax adversarial objective: the retriever learns to retrieve\nnegative documents to cheat the ranker, while the ranker learns to rank a\ncollection of candidates including both the ground-truth and the retrieved\nones, as well as providing progressive direct feedback to the dual-encoder\nretriever. Through this adversarial game, the retriever gradually produces\nharder negative documents to train a better ranker, whereas the cross-encoder\nranker provides progressive feedback to improve retriever. We evaluate AR2 on\nthree benchmarks. Experimental results show that AR2 consistently and\nsignificantly outperforms existing dense retriever methods and achieves new\nstate-of-the-art results on all of them. This includes the improvements on\nNatural Questions R@5 to 77.9%(+2.1%), TriviaQA R@5 to 78.2%(+1.4), and\nMS-MARCO MRR@10 to 39.5%(+1.3%). We will make our code, models, and data\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jiancheng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Learning Higher-Order Dynamics in Video-Based Cardiac Measurement. (arXiv:2110.03690v1 [eess.IV])","link":"http://arxiv.org/abs/2110.03690","description":"<p>Computer vision methods typically optimize for first-order dynamics (e.g.,\noptical flow). However, in many cases the properties of interest are subtle\nvariations in higher-order changes, such as acceleration. This is true in the\ncardiac pulse, where the second derivative can be used as an indicator of blood\npressure and arterial disease. Recent developments in camera-based vital sign\nmeasurement have shown that cardiac measurements can be recovered with\nimpressive accuracy from videos; however, the majority of research has focused\non extracting summary statistics such as heart rate. Less emphasis has been put\non the accuracy of waveform morphology that is necessary for many clinically\nimpactful scenarios. In this work, we provide evidence that higher-order\ndynamics are better estimated by neural models when explicitly optimized for in\nthe loss function. Furthermore, adding second-derivative inputs also improves\nperformance when estimating second-order dynamics. By incorporating the second\nderivative of both the input frames and the target vital sign signals into the\ntraining procedure, our model is better able to estimate left ventricle\nejection time (LVET) intervals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hill_B/0/1/0/all/0/1\">Brian L. Hill</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McDuff_D/0/1/0/all/0/1\">Daniel McDuff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SVG-Net: An SVG-based Trajectory Prediction Model. (arXiv:2110.03706v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03706","description":"<p>Anticipating motions of vehicles in a scene is an essential problem for safe\nautonomous driving systems. To this end, the comprehension of the scene's\ninfrastructure is often the main clue for predicting future trajectories. Most\nof the proposed approaches represent the scene with a rasterized format and\nsome of the more recent approaches leverage custom vectorized formats. In\ncontrast, we propose representing the scene's information by employing Scalable\nVector Graphics (SVG). SVG is a well-established format that matches the\nproblem of trajectory prediction better than rasterized formats while being\nmore general than arbitrary vectorized formats. SVG has the potential to\nprovide the convenience and generality of raster-based solutions if coupled\nwith a powerful tool such as CNNs, for which we introduce SVG-Net. SVG-Net is a\nTransformer-based Neural Network that can effectively capture the scene's\ninformation from SVG inputs. Thanks to the self-attention mechanism in its\nTransformers, SVG-Net can also adequately apprehend relations amongst the scene\nand the agents. We demonstrate SVG-Net's effectiveness by evaluating its\nperformance on the publicly available Argoverse forecasting dataset. Finally,\nwe illustrate how, by using SVG, one can benefit from datasets and advancements\nin other research fronts that also utilize the same input format. Our code is\navailable at https://vita-epfl.github.io/SVGNet/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahari_M/0/1/0/all/0/1\">Mohammadhossein Bahari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zehtab_V/0/1/0/all/0/1\">Vahid Zehtab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khorasani_S/0/1/0/all/0/1\">Sadegh Khorasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayramlou_S/0/1/0/all/0/1\">Sana Ayramlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saadatnejad_S/0/1/0/all/0/1\">Saeed Saadatnejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1\">Alexandre Alahi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Unlearning of Backdoors via Implicit Hypergradient. (arXiv:2110.03735v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03735","description":"<p>We propose a minimax formulation for removing backdoors from a given poisoned\nmodel based on a small set of clean data. This formulation encompasses much of\nprior work on backdoor removal. We propose the Implicit Bacdoor Adversarial\nUnlearning (I-BAU) algorithm to solve the minimax. Unlike previous work, which\nbreaks down the minimax into separate inner and outer problems, our algorithm\nutilizes the implicit hypergradient to account for the interdependence between\ninner and outer optimization. We theoretically analyze its convergence and the\ngeneralizability of the robustness gained by solving minimax on clean data to\nunseen test data. In our evaluation, we compare I-BAU with six state-of-art\nbackdoor defenses on seven backdoor attacks over two datasets and various\nattack settings, including the common setting where the attacker targets one\nclass as well as important but underexplored settings where multiple classes\nare targeted. I-BAU's performance is comparable to and most often significantly\nbetter than the best baseline. Particularly, its performance is more robust to\nthe variation on triggers, attack settings, poison ratio, and clean data size.\nMoreover, I-BAU requires less computation to take effect; particularly, it is\nmore than $13\\times$ faster than the most efficient baseline in the\nsingle-target attack setting. Furthermore, it can remain effective in the\nextreme case where the defender can only access 100 clean samples -- a setting\nwhere all the baselines fail to produce acceptable results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1\">Won Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Z. Morley Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_J/0/1/0/all/0/1\">Jin Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Early-Learning Correction for Segmentation from Noisy Annotations. (arXiv:2110.03740v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03740","description":"<p>Deep learning in the presence of noisy annotations has been studied\nextensively in classification, but much less in segmentation tasks. In this\nwork, we study the learning dynamics of deep segmentation networks trained on\ninaccurately-annotated data. We discover a phenomenon that has been previously\nreported in the context of classification: the networks tend to first fit the\nclean pixel-level labels during an \"early-learning\" phase, before eventually\nmemorizing the false annotations. However, in contrast to classification,\nmemorization in segmentation does not arise simultaneously for all semantic\ncategories. Inspired by these findings, we propose a new method for\nsegmentation from noisy annotations with two key elements. First, we detect the\nbeginning of the memorization phase separately for each category during\ntraining. This allows us to adaptively correct the noisy annotations in order\nto exploit early learning. Second, we incorporate a regularization term that\nenforces consistency across scales to boost robustness against annotation\nnoise. Our method outperforms standard approaches on a medical-imaging\nsegmentation task where noises are synthesized to mimic human annotation\nerrors. It also provides robustness to realistic noisy annotations present in\nweakly-supervised semantic segmentation, achieving state-of-the-art results on\nPASCAL VOC 2012.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kangning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Weicheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yiqiu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Attack by Limited Point Cloud Surface Modifications. (arXiv:2110.03745v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03745","description":"<p>Recent research has revealed that the security of deep neural networks that\ndirectly process 3D point clouds to classify objects can be threatened by\nadversarial samples. Although existing adversarial attack methods achieve high\nsuccess rates, they do not restrict the point modifications enough to preserve\nthe point cloud appearance. To overcome this shortcoming, two constraints are\nproposed. These include applying hard boundary constraints on the number of\nmodified points and on the point perturbation norms. Due to the restrictive\nnature of the problem, the search space contains many local maxima. The\nproposed method addresses this issue by using a high step-size at the beginning\nof the algorithm to search the main surface of the point cloud fast and\neffectively. Then, in order to converge to the desired output, the step-size is\ngradually decreased. To evaluate the performance of the proposed method, it is\nrun on the ModelNet40 and ScanObjectNN datasets by employing the\nstate-of-the-art point cloud classification models; including PointNet,\nPointNet++, and DGCNN. The obtained results show that it can perform successful\nattacks and achieve state-of-the-art results by only a limited number of point\nmodifications while preserving the appearance of the point cloud. Moreover, due\nto the effective search algorithm, it can perform successful attacks in just a\nfew steps. Additionally, the proposed step-size scheduling algorithm shows an\nimprovement of up to $14.5\\%$ when adopted by other methods as well. The\nproposed method also performs effectively against popular defense methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arya_A/0/1/0/all/0/1\">Atrin Arya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naderi_H/0/1/0/all/0/1\">Hanieh Naderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1\">Shohreh Kasaei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proposing a System Level Machine Learning Hybrid Architecture and Approach for a Comprehensive Autism Spectrum Disorder Diagnosis. (arXiv:2110.03775v1 [eess.IV])","link":"http://arxiv.org/abs/2110.03775","description":"<p>Autism Spectrum Disorder (ASD) is a severe neuropsychiatric disorder that\naffects intellectual development, social behavior, and facial features, and the\nnumber of cases is still significantly increasing. Due to the variety of\nsymptoms ASD displays, the diagnosis process remains challenging, with numerous\nmisdiagnoses as well as lengthy and expensive diagnoses. Fortunately, if ASD is\ndiagnosed and treated early, then the patient will have a much higher chance of\ndeveloping normally. For an ASD diagnosis, machine learning algorithms can\nanalyze both social behavior and facial features accurately and efficiently,\nproviding an ASD diagnosis in a drastically shorter amount of time than through\ncurrent clinical diagnosis processes. Therefore, we propose to develop a hybrid\narchitecture fully utilizing both social behavior and facial feature data to\nimprove the accuracy of diagnosing ASD. We first developed a Linear Support\nVector Machine for the social behavior based module, which analyzes Autism\nDiagnostic Observation Schedule (ADOS) social behavior data. For the facial\nfeature based module, a DenseNet model was utilized to analyze facial feature\nimage data. Finally, we implemented our hybrid model by incorporating different\nfeatures of the Support Vector Machine and the DenseNet into one model. Our\nresults show that the highest accuracy of 87% for ASD diagnosis has been\nachieved by our proposed hybrid model. The pros and cons of each module will be\ndiscussed in this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_R/0/1/0/all/0/1\">Ryan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_S/0/1/0/all/0/1\">Spencer He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning approaches to do size based reasoning on Retail Shelf objects to classify product variants. (arXiv:2110.03783v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03783","description":"<p>There has been a surge in the number of Machine Learning methods to analyze\nproducts kept on retail shelves images. Deep learning based computer vision\nmethods can be used to detect products on retail shelves and then classify\nthem. However, there are different sized variants of products which look\nexactly the same visually and the method to differentiate them is to look at\ntheir relative sizes with other products on shelves. This makes the process of\ndeciphering the sized based variants from each other using computer vision\nalgorithms alone impractical. In this work, we propose methods to ascertain the\nsize variant of the product as a downstream task to an object detector which\nextracts products from shelf and a classifier which determines product brand.\nProduct variant determination is the task which assigns a product variant to\nproducts of a brand based on the size of bounding boxes and brands predicted by\nclassifier. While gradient boosting based methods work well for products whose\nfacings are clear and distinct, a noise accommodating Neural Network method is\nproposed for cases where the products are stacked irregularly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1\">Muktabh Mayank Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient large-scale image retrieval with deep feature orthogonality and Hybrid-Swin-Transformers. (arXiv:2110.03786v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03786","description":"<p>We present an efficient end-to-end pipeline for largescale landmark\nrecognition and retrieval. We show how to combine and enhance concepts from\nrecent research in image retrieval and introduce two architectures especially\nsuited for large-scale landmark identification. A model with deep orthogonal\nfusion of local and global features (DOLG) using an EfficientNet backbone as\nwell as a novel Hybrid-Swin-Transformer is discussed and details how to train\nboth architectures efficiently using a step-wise approach and a sub-center\narcface loss with dynamic margins are provided. Furthermore, we elaborate a\nnovel discriminative re-ranking methodology for image retrieval. The\nsuperiority of our approach was demonstrated by winning the recognition and\nretrieval track of the Google Landmark Competition 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henkel_C/0/1/0/all/0/1\">Christof Henkel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Probabilistic Graphical Model Approach to the Structure-and-Motion Problem. (arXiv:2110.03792v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03792","description":"<p>We present a means of formulating and solving the well known\nstructure-and-motion problem in computer vision with probabilistic graphical\nmodels. We model the unknown camera poses and 3D feature coordinates as well as\nthe observed 2D projections as Gaussian random variables, using sigma point\nparameterizations to effectively linearize the nonlinear relationships between\nthese variables. Those variables involved in every projection are grouped into\na cluster, and we connect the clusters in a cluster graph. Loopy belief\npropagation is performed over this graph, in an iterative re-initialization and\nestimation procedure, and we find that our approach shows promise in both\nsimulation and on real-world data. The PGM is easily extendable to include\nadditional parameters or constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Streicher_S/0/1/0/all/0/1\">Simon Streicher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brink_W/0/1/0/all/0/1\">Willie Brink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preez_J/0/1/0/all/0/1\">Johan du Preez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FOCUS: Familiar Objects in Common and Uncommon Settings. (arXiv:2110.03804v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03804","description":"<p>Standard training datasets for deep learning often contain objects in common\nsettings (e.g., \"a horse on grass\" or \"a ship in water\") since they are usually\ncollected by randomly scraping the web. Uncommon and rare settings (e.g., \"a\nplane on water\", \"a car in snowy weather\") are thus severely under-represented\nin the training data. This can lead to an undesirable bias in model predictions\ntowards common settings and create a false sense of accuracy. In this paper, we\nintroduce FOCUS (Familiar Objects in Common and Uncommon Settings), a dataset\nfor stress-testing the generalization power of deep image classifiers. By\nleveraging the power of modern search engines, we deliberately gather data\ncontaining objects in common and uncommon settings in a wide range of\nlocations, weather conditions, and time of day. We present a detailed analysis\nof the performance of various popular image classifiers on our dataset and\ndemonstrate a clear drop in performance when classifying images in uncommon\nsettings. By analyzing deep features of these models, we show that such errors\ncan be due to the use of spurious features in model predictions. We believe\nthat our dataset will aid researchers in understanding the inability of deep\nmodels to generalize well to uncommon settings and drive future work on\nimproving their distributional robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kattakinda_P/0/1/0/all/0/1\">Priyatham Kattakinda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleGAN-induced data-driven regularization for inverse problems. (arXiv:2110.03814v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03814","description":"<p>Recent advances in generative adversarial networks (GANs) have opened up the\npossibility of generating high-resolution photo-realistic images that were\nimpossible to produce previously. The ability of GANs to sample from\nhigh-dimensional distributions has naturally motivated researchers to leverage\ntheir power for modeling the image prior in inverse problems. We extend this\nline of research by developing a Bayesian image reconstruction framework that\nutilizes the full potential of a pre-trained StyleGAN2 generator, which is the\ncurrently dominant GAN architecture, for constructing the prior distribution on\nthe underlying image. Our proposed approach, which we refer to as learned\nBayesian reconstruction with generative models (L-BRGM), entails joint\noptimization over the style-code and the input latent code, and enhances the\nexpressive power of a pre-trained StyleGAN2 generator by allowing the\nstyle-codes to be different for different generator layers. Considering the\ninverse problems of image inpainting and super-resolution, we demonstrate that\nthe proposed approach is competitive with, and sometimes superior to,\nstate-of-the-art GAN-based image reconstruction methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Conmy_A/0/1/0/all/0/1\">Arthur Conmy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhadip Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks. (arXiv:2110.03825v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03825","description":"<p>Deep neural networks (DNNs) are known to be vulnerable to adversarial\nattacks. A range of defense methods have been proposed to train adversarially\nrobust DNNs, among which adversarial training has demonstrated promising\nresults. However, despite preliminary understandings developed for adversarial\ntraining, it is still not clear, from the architectural perspective, what\nconfigurations can lead to more robust DNNs. In this paper, we address this gap\nvia a comprehensive investigation on the impact of network width and depth on\nthe robustness of adversarially trained DNNs. Specifically, we make the\nfollowing key observations: 1) more parameters (higher model capacity) does not\nnecessarily help adversarial robustness; 2) reducing capacity at the last stage\n(the last group of blocks) of the network can actually improve adversarial\nrobustness; and 3) under the same parameter budget, there exists an optimal\narchitectural configuration for adversarial robustness. We also provide a\ntheoretical analysis explaning why such network configuration can help\nrobustness. These architectural insights can help design adversarially robust\nDNNs. Code is available at \\url{https://github.com/HanxunH/RobustWRN}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hanxun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1\">Sarah Monazam Erfani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1\">James Bailey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xingjun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SkullEngine: A Multi-stage CNN Framework for Collaborative CBCT Image Segmentation and Landmark Detection. (arXiv:2110.03828v1 [eess.IV])","link":"http://arxiv.org/abs/2110.03828","description":"<p>We propose a multi-stage coarse-to-fine CNN-based framework, called\nSkullEngine, for high-resolution segmentation and large-scale landmark\ndetection through a collaborative, integrated, and scalable JSD model and three\nsegmentation and landmark detection refinement models. We evaluated our\nframework on a clinical dataset consisting of 170 CBCT/CT images for the task\nof segmenting 2 bones (midface and mandible) and detecting 175 clinically\ncommon landmarks on bones, teeth, and soft tissues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_H/0/1/0/all/0/1\">Han Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lian_C/0/1/0/all/0/1\">Chunfeng Lian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_D/0/1/0/all/0/1\">Deqiang Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuang_T/0/1/0/all/0/1\">Tianshu Kuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gateno_J/0/1/0/all/0/1\">Jaime Gateno</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yap_P/0/1/0/all/0/1\">Pew-Thian Yap</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_J/0/1/0/all/0/1\">James J. Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic annotation of visual deep neural networks. (arXiv:2110.03851v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03851","description":"<p>Computer vision is widely used in the fields of driverless, face recognition\nand 3D reconstruction as a technology to help or replace human eye perception\nimages or multidimensional data through computers. Nowadays, with the\ndevelopment and application of deep neural networks, the models of deep neural\nnetworks proposed for computer vision are becoming more and more abundant, and\ndevelopers will use the already trained models on the way to solve problems,\nand need to consult the relevant documents to understand the use of the model.\nThe class model, which creates the need to quickly and accurately find the\nrelevant models that you need. The automatic annotation method of visual depth\nneural network proposed in this paper is based on natural language processing\ntechnology such as semantic analysis, which realizes automatic labeling of\nmodel application fields. In the three top international conferences on\ncomputer vision: ICCV, CVPR and ECCV, the average correct rate of application\nof the papers of 72 papers reached 90%, indicating the effectiveness of the\nautomatic labeling system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Ming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">ChenHao Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Learning 3D Shape Segmentation Functions. (arXiv:2110.03854v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03854","description":"<p>Learning robust 3D shape segmentation functions with deep neural networks has\nemerged as a powerful paradigm, offering promising performance in producing a\nconsistent part segmentation of each 3D shape. Generalizing across 3D shape\nsegmentation functions requires robust learning of priors over the respective\nfunction space and enables consistent part segmentation of shapes in presence\nof significant 3D structure variations. Existing generalization methods rely on\nextensive training of 3D shape segmentation functions on large-scale labeled\ndatasets. In this paper, we proposed to formalize the learning of a 3D shape\nsegmentation function space as a meta-learning problem, aiming to predict a 3D\nsegmentation model that can be quickly adapted to new shapes with no or limited\ntraining data. More specifically, we define each task as unsupervised learning\nof shape-conditioned 3D segmentation function which takes as input points in 3D\nspace and predicts the part-segment labels. The 3D segmentation function is\ntrained by a self-supervised 3D shape reconstruction loss without the need for\npart labels. Also, we introduce an auxiliary deep neural network as a\nmeta-learner which takes as input a 3D shape and predicts the prior over the\nrespective 3D segmentation function space. We show in experiments that our\nmeta-learning approach, denoted as Meta-3DSeg, leads to improvements on\nunsupervised 3D shape segmentation over the conventional designs of deep neural\nnetworks for 3D shape segmentation functions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yu Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ABCP: Automatic Block-wise and Channel-wise Network Pruning via Joint Search. (arXiv:2110.03858v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03858","description":"<p>Currently, an increasing number of model pruning methods are proposed to\nresolve the contradictions between the computer powers required by the deep\nlearning models and the resource-constrained devices. However, most of the\ntraditional rule-based network pruning methods can not reach a sufficient\ncompression ratio with low accuracy loss and are time-consuming as well as\nlaborious. In this paper, we propose Automatic Block-wise and Channel-wise\nNetwork Pruning (ABCP) to jointly search the block-wise and channel-wise\npruning action with deep reinforcement learning. A joint sample algorithm is\nproposed to simultaneously generate the pruning choice of each residual block\nand the channel pruning ratio of each convolutional layer from the discrete and\ncontinuous search space respectively. The best pruning action taking both the\naccuracy and the complexity of the model into account is obtained finally.\nCompared with the traditional rule-based pruning method, this pipeline saves\nhuman labor and achieves a higher compression ratio with lower accuracy loss.\nTested on the mobile robot detection dataset, the pruned YOLOv3 model saves\n99.5% FLOPs, reduces 99.5% parameters, and achieves 37.3 times speed up with\nonly 2.8% mAP loss. The results of the transfer task on the sim2real detection\ndataset also show that our pruned model has much better robustness performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yaran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zixiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nannan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingjun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Zicheng Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongbing Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Token Pooling in Visual Transformers. (arXiv:2110.03860v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03860","description":"<p>Despite the recent success in many applications, the high computational\nrequirements of vision transformers limit their use in resource-constrained\nsettings. While many existing methods improve the quadratic complexity of\nattention, in most vision transformers, self-attention is not the major\ncomputation bottleneck, e.g., more than 80% of the computation is spent on\nfully-connected layers. To improve the computational complexity of all layers,\nwe propose a novel token downsampling method, called Token Pooling, efficiently\nexploiting redundancies in the images and intermediate token representations.\nWe show that, under mild assumptions, softmax-attention acts as a\nhigh-dimensional low-pass (smoothing) filter. Thus, its output contains\nredundancy that can be pruned to achieve a better trade-off between the\ncomputational cost and accuracy. Our new technique accurately approximates a\nset of tokens by minimizing the reconstruction error caused by downsampling. We\nsolve this optimization problem via cost-efficient clustering. We rigorously\nanalyze and compare to prior downsampling methods. Our experiments show that\nToken Pooling significantly improves the cost-accuracy trade-off over the\nstate-of-the-art downsampling. Token Pooling is a simple and effective operator\nthat can benefit many architectures. Applied to DeiT, it achieves the same\nImageNet top-1 accuracy using 42% fewer computations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marin_D/0/1/0/all/0/1\">Dmitrii Marin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jen-Hao Rick Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_A/0/1/0/all/0/1\">Anurag Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhu_A/0/1/0/all/0/1\">Anish Prabhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1\">Mohammad Rastegari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuzel_O/0/1/0/all/0/1\">Oncel Tuzel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QTN-VQC: An End-to-End Learning framework for Quantum Neural Networks. (arXiv:2110.03861v1 [quant-ph])","link":"http://arxiv.org/abs/2110.03861","description":"<p>The advent of noisy intermediate-scale quantum (NISQ) computers raises a\ncrucial challenge to design quantum neural networks for fully quantum learning\ntasks. To bridge the gap, this work proposes an end-to-end learning framework\nnamed QTN-VQC, by introducing a trainable quantum tensor network (QTN) for\nquantum embedding on a variational quantum circuit (VQC). The architecture of\nQTN is composed of a parametric tensor-train network for feature extraction and\na tensor product encoding for quantum encoding. We highlight the QTN for\nquantum embedding in terms of two perspectives: (1) we theoretically\ncharacterize QTN by analyzing its representation power of input features; (2)\nQTN enables an end-to-end parametric model pipeline, namely QTN-VQC, from the\ngeneration of quantum embedding to the output measurement. Our experiments on\nthe MNIST dataset demonstrate the advantages of QTN for quantum embedding over\nother quantum embedding approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Qi_J/0/1/0/all/0/1\">Jun Qi</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boundary-aware Transformers for Skin Lesion Segmentation. (arXiv:2110.03864v1 [eess.IV])","link":"http://arxiv.org/abs/2110.03864","description":"<p>Skin lesion segmentation from dermoscopy images is of great importance for\nimproving the quantitative analysis of skin cancer. However, the automatic\nsegmentation of melanoma is a very challenging task owing to the large\nvariation of melanoma and ambiguous boundaries of lesion areas. While\nconvolutional neutral networks (CNNs) have achieved remarkable progress in this\ntask, most of existing solutions are still incapable of effectively capturing\nglobal dependencies to counteract the inductive bias caused by limited\nreceptive fields. Recently, transformers have been proposed as a promising tool\nfor global context modeling by employing a powerful global attention mechanism,\nbut one of their main shortcomings when applied to segmentation tasks is that\nthey cannot effectively extract sufficient local details to tackle ambiguous\nboundaries. We propose a novel boundary-aware transformer (BAT) to\ncomprehensively address the challenges of automatic skin lesion segmentation.\nSpecifically, we integrate a new boundary-wise attention gate (BAG) into\ntransformers to enable the whole network to not only effectively model global\nlong-range dependencies via transformers but also, simultaneously, capture more\nlocal details by making full use of boundary-wise prior knowledge.\nParticularly, the auxiliary supervision of BAG is capable of assisting\ntransformers to learn position embedding as it provides much spatial\ninformation. We conducted extensive experiments to evaluate the proposed BAT\nand experiments corroborate its effectiveness, consistently outperforming\nstate-of-the-art methods in two famous datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiacheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_L/0/1/0/all/0/1\">Lan Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Q/0/1/0/all/0/1\">Qichao Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diabetic Retinopathy Screening Using Custom-Designed Convolutional Neural Network. (arXiv:2110.03877v1 [eess.IV])","link":"http://arxiv.org/abs/2110.03877","description":"<p>The prevalence of diabetic retinopathy (DR) has reached 34.6% worldwide and\nis a major cause of blindness among middle-aged diabetic patients. Regular DR\nscreening using fundus photography helps detect its complications and prevent\nits progression to advanced levels. As manual screening is time-consuming and\nsubjective, machine learning (ML) and deep learning (DL) have been employed to\naid graders. However, the existing CNN-based methods use either pre-trained CNN\nmodels or a brute force approach to design new CNN models, which are not\ncustomized to the complexity of fundus images. To overcome this issue, we\nintroduce an approach for custom-design of CNN models, whose architectures are\nadapted to the structural patterns of fundus images and better represent the\nDR-relevant features. It takes the leverage of k-medoid clustering, principal\ncomponent analysis (PCA), and inter-class and intra-class variations to\nautomatically determine the depth and width of a CNN model. The designed models\nare lightweight, adapted to the internal structures of fundus images, and\nencode the discriminative patterns of DR lesions. The technique is validated on\na local dataset from King Saud University Medical City, Saudi Arabia, and two\nchallenging benchmark datasets from Kaggle: EyePACS and APTOS2019. The\ncustom-designed models outperform the famous pre-trained CNN models like\nResNet152, Densnet121, and ResNeSt50 with a significant decrease in the number\nof parameters and compete well with the state-of-the-art CNN-based DR screening\nmethods. The proposed approach is helpful for DR screening under diverse\nclinical settings and referring the patients who may need further assessment\nand treatment to expert ophthalmologists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saeed_F/0/1/0/all/0/1\">Fahman Saeed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hussain_M/0/1/0/all/0/1\">Muhammad Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Member_S/0/1/0/all/0/1\">Senior Member</a>, <a href=\"http://arxiv.org/find/eess/1/au:+IEEE/0/1/0/all/0/1\">IEEE</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aboalsamh_H/0/1/0/all/0/1\">Hatim A Aboalsamh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Member_S/0/1/0/all/0/1\">Senior Member</a>, <a href=\"http://arxiv.org/find/eess/1/au:+IEEE/0/1/0/all/0/1\">IEEE</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adel_F/0/1/0/all/0/1\">Fadwa Al Adel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Owaifeer_A/0/1/0/all/0/1\">Adi Mohammed Al Owaifeer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BDC: Bounding-Box Deep Calibration for High Performance Face Detection. (arXiv:2110.03892v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03892","description":"<p>Modern CNN-based face detectors have achieved tremendous strides due to large\nannotated datasets. However, misaligned results with high detection confidence\nbut low localization accuracy restrict the further improvement of detection\nperformance. In this paper, we first generate detection results on training set\nitself. Surprisingly, a considerable part of them exist the same misalignment\nproblem. Then, we carefully examine these misaligned cases and point out\nannotation inconsistency is the main reason. Finally, we propose a novel\nBounding-Box Deep Calibration (BDC) method to reasonably replace inconsistent\nannotations with model predicted bounding-boxes and create a new annotation\nfile for training set. Extensive experiments on WIDER FACE dataset show the\neffectiveness of BDC on improving models' precision and recall rate. Our simple\nand effective method provides a new direction for improving face detection.\nSource code is available at https://github.com/shiluo1990/BDC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiongfei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoli Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Strokes: Stylized Line Drawing of 3D Shapes. (arXiv:2110.03900v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03900","description":"<p>This paper introduces a model for producing stylized line drawings from 3D\nshapes. The model takes a 3D shape and a viewpoint as input, and outputs a\ndrawing with textured strokes, with variations in stroke thickness,\ndeformation, and color learned from an artist's style. The model is fully\ndifferentiable. We train its parameters from a single training drawing of\nanother 3D shape. We show that, in contrast to previous image-based methods,\nthe use of a geometric representation of 3D shape and 2D strokes allows the\nmodel to transfer important aspects of shape and texture style while preserving\ncontours. Our method outputs the resulting drawing in a vector representation,\nenabling richer downstream analysis or editing in interactive applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Difan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1\">Matthew Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertzmann_A/0/1/0/all/0/1\">Aaron Hertzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalogerakis_E/0/1/0/all/0/1\">Evangelos Kalogerakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Monitoring System using Social Distancing and Face Mask Detection on Surveillance video datasets. (arXiv:2110.03905v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03905","description":"<p>In the current times, the fear and danger of COVID-19 virus still stands\nlarge. Manual monitoring of social distancing norms is impractical with a large\npopulation moving about and with insufficient task force and resources to\nadminister them. There is a need for a lightweight, robust and 24X7\nvideo-monitoring system that automates this process. This paper proposes a\ncomprehensive and effective solution to perform person detection, social\ndistancing violation detection, face detection and face mask classification\nusing object detection, clustering and Convolution Neural Network (CNN) based\nbinary classifier. For this, YOLOv3, Density-based spatial clustering of\napplications with noise (DBSCAN), Dual Shot Face Detector (DSFD) and\nMobileNetV2 based binary classifier have been employed on surveillance video\ndatasets. This paper also provides a comparative study of different face\ndetection and face mask classification models. Finally, a video dataset\nlabelling method is proposed along with the labelled video dataset to\ncompensate for the lack of dataset in the community and is used for evaluation\nof the system. The system performance is evaluated in terms of accuracy, F1\nscore as well as the prediction time, which has to be low for practical\napplicability. The system performs with an accuracy of 91.2% and F1 score of\n90.79% on the labelled video dataset and has an average prediction time of 7.12\nseconds for 78 frames of a video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+R_R/0/1/0/all/0/1\">Rujula Singh R</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_N/0/1/0/all/0/1\">Nikhil Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Sahana Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biradar_R/0/1/0/all/0/1\">Ruchita Biradar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Learning with Task-Adaptive Loss Function for Few-Shot Learning. (arXiv:2110.03909v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03909","description":"<p>In few-shot learning scenarios, the challenge is to generalize and perform\nwell on new unseen examples when only very few labeled examples are available\nfor each task. Model-agnostic meta-learning (MAML) has gained the popularity as\none of the representative few-shot learning methods for its flexibility and\napplicability to diverse problems. However, MAML and its variants often resort\nto a simple loss function without any auxiliary loss function or regularization\nterms that can help achieve better generalization. The problem lies in that\neach application and task may require different auxiliary loss function,\nespecially when tasks are diverse and distinct. Instead of attempting to\nhand-design an auxiliary loss function for each application and task, we\nintroduce a new meta-learning framework with a loss function that adapts to\neach task. Our proposed framework, named Meta-Learning with Task-Adaptive Loss\nFunction (MeTAL), demonstrates the effectiveness and the flexibility across\nvarious domains, such as few-shot classification and few-shot regression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baik_S/0/1/0/all/0/1\">Sungyong Baik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Janghoon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heewon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_D/0/1/0/all/0/1\">Dohee Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_J/0/1/0/all/0/1\">Jaesik Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stereo Dense Scene Reconstruction and Accurate Laparoscope Localization for Learning-Based Navigation in Robot-Assisted Surgery. (arXiv:2110.03912v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03912","description":"<p>The computation of anatomical information and laparoscope position is a\nfundamental block of robot-assisted surgical navigation in Minimally Invasive\nSurgery (MIS). Recovering a dense 3D structure of surgical scene using visual\ncues remains a challenge, and the online laparoscopic tracking mostly relies on\nexternal sensors, which increases system complexity. In this paper, we propose\na learning-driven framework, in which an image-guided laparoscopic localization\nwith 3D reconstructions of complex anatomical structures is hereby achieved. To\nreconstruct the 3D structure of the whole surgical environment, we first\nfine-tune a learning-based stereoscopic depth perception method, which is\nrobust to the texture-less and variant soft tissues, for depth estimation.\nThen, we develop a dense visual reconstruction algorithm to represent the scene\nby surfels, estimate the laparoscope pose and fuse the depth data into a\nunified reference coordinate for tissue reconstruction. To estimate poses of\nnew laparoscope views, we realize a coarse-to-fine localization method, which\nincorporates our reconstructed 3D model. We evaluate the reconstruction method\nand the localization module on three datasets, namely, the stereo\ncorrespondence and reconstruction of endoscopic data (SCARED), the ex-vivo\nphantom and tissue data collected with Universal Robot (UR) and Karl Storz\nLaparoscope, and the in-vivo DaVinci robotic surgery dataset. Extensive\nexperiments have been conducted to prove the superior performance of our method\nin 3D anatomy reconstruction and laparoscopic localization, which demonstrates\nits potential implementation to surgical navigation system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_R/0/1/0/all/0/1\">Ruofeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_H/0/1/0/all/0/1\">Hangjie Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1\">Bo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yonghao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bohan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunhui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Dong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCFlow: Optical Flow Estimation for Spiking Camera. (arXiv:2110.03916v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03916","description":"<p>As a bio-inspired sensor with high temporal resolution, Spiking camera has an\nenormous potential in real applications, especially for motion estimation in\nhigh-speed scenes. Optical flow estimation has achieved remarkable success in\nimage-based and event-based vision, but % existing methods cannot be directly\napplied in spike stream from spiking camera. conventional optical flow\nalgorithms are not well matched to the spike stream data. This paper presents,\nSCFlow, a novel deep learning pipeline for optical flow estimation for spiking\ncamera. Importantly, we introduce an proper input representation of a given\nspike stream, which is fed into SCFlow as the sole input. We introduce the\n\\textit{first} spiking camera simulator (SPCS). Furthermore, based on SPCS, we\nfirst propose two optical flow datasets for spiking camera (SPIkingly Flying\nThings and Photo-realistic High-speed Motion, denoted as SPIFT and PHM\nrespectively) corresponding to random high-speed and well-designed scenes.\nEmpirically, we show that the SCFlow can predict optical flow from spike stream\nin different high-speed scenes, and express superiority to existing methods on\nthe datasets. \\textit{All codes and constructed datasets will be released after\npublication}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Liwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Ziluo Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Ruiqin Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViDT: An Efficient and Effective Fully Transformer-based Object Detector. (arXiv:2110.03921v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03921","description":"<p>Transformers are transforming the landscape of computer vision, especially\nfor recognition tasks. Detection transformers are the first fully end-to-end\nlearning systems for object detection, while vision transformers are the first\nfully transformer-based architecture for image classification. In this paper,\nwe integrate Vision and Detection Transformers (ViDT) to build an effective and\nefficient object detector. ViDT introduces a reconfigured attention module to\nextend the recent Swin Transformer to be a standalone object detector, followed\nby a computationally efficient transformer decoder that exploits multi-scale\nfeatures and auxiliary techniques essential to boost the detection performance\nwithout much increase in computational load. Extensive evaluation results on\nthe Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP\nand latency trade-off among existing fully transformer-based object detectors,\nand achieves 49.2AP owing to its high scalability for large models. We will\nrelease the code and trained models athttps://github.com/naver-ai/vidt\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hwanjun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Deqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sanghyuk Chun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dongyoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_B/0/1/0/all/0/1\">Byeongho Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Wonjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Directionally Decomposing Structured Light for Projector Calibration. (arXiv:2110.03924v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03924","description":"<p>Intrinsic projector calibration is essential in projection mapping (PM)\napplications, especially in dynamic PM. However, due to the shallow\ndepth-of-field (DOF) of a projector, more work is needed to ensure accurate\ncalibration. We aim to estimate the intrinsic parameters of a projector while\navoiding the limitation of shallow DOF. As the core of our technique, we\npresent a practical calibration device that requires a minimal working volume\ndirectly in front of the projector lens regardless of the projector's focusing\ndistance and aperture size. The device consists of a flat-bed scanner and\npinhole-array masks. For calibration, a projector projects a series of\nstructured light patterns in the device. The pinholes directionally decompose\nthe structured light, and only the projected rays that pass through the\npinholes hit the scanner plane. For each pinhole, we extract a ray passing\nthrough the optical center of the projector. Consequently, we regard the\nprojector as a pinhole projector that projects the extracted rays only, and we\ncalibrate the projector by applying the standard camera calibration technique,\nwhich assumes a pinhole camera model. Using a proof-of-concept prototype, we\ndemonstrate that our technique can calibrate projectors with different focusing\ndistances and aperture sizes at the same accuracy as a conventional method.\nFinally, we confirm that our technique can provide intrinsic parameters\naccurate enough for a dynamic PM application, even when a projector is placed\ntoo far from a projection target for a conventional method to calibrate the\nprojector using a fiducial object of reasonable size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sugimoto_M/0/1/0/all/0/1\">Masatoki Sugimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwai_D/0/1/0/all/0/1\">Daisuke Iwai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishida_K/0/1/0/all/0/1\">Koki Ishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punpongsanon_P/0/1/0/all/0/1\">Parinya Punpongsanon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_K/0/1/0/all/0/1\">Kosuke Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pose Refinement with Joint Optimization of Visual Points and Lines. (arXiv:2110.03940v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03940","description":"<p>High-precision camera re-localization technology in a pre-established 3D\nenvironment map is the basis for many tasks, such as Augmented Reality,\nRobotics and Autonomous Driving. The point-based visual re-localization\napproaches are well-developed in recent decades, but are insufficient in some\nfeature-less cases. In this paper, we propose a point-line joint optimization\nmethod for pose refinement with the help of the innovatively designed line\nextracting CNN named VLSE, and the line matching and pose optimization\napproach. We adopt a novel line representation and customize a hybrid\nconvolutional block based on the Stacked Hourglass network, to detect accurate\nand stable line features on images. Then we apply a coarse-to-fine strategy to\nobtain precise 2D-3D line correspondences based on the geometric constraint. A\nfollowing point-line joint cost function is constructed to optimize the camera\npose with the initial coarse pose. Sufficient experiments are conducted on open\ndatasets, i.e, line extractor on Wireframe and YorkUrban, localization\nperformance on Aachen Day-Night v1.1 and InLoc, to confirm the effectiveness of\nour point-line joint pose optimization method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shuang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jixiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_Y/0/1/0/all/0/1\">Yishan Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xudong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shuzhou Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jijunnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GaitPrivacyON: Privacy-Preserving Mobile Gait Biometrics using Unsupervised Learning. (arXiv:2110.03967v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03967","description":"<p>Numerous studies in the literature have already shown the potential of\nbiometrics on mobile devices for authentication purposes. However, it has been\nshown that, the learning processes associated to biometric systems might expose\nsensitive personal information about the subjects. This study proposes\nGaitPrivacyON, a novel mobile gait biometrics verification approach that\nprovides accurate authentication results while preserving the sensitive\ninformation of the subject. It comprises two modules: i) a convolutional\nAutoencoder that transforms attributes of the biometric raw data, such as the\ngender or the activity being performed, into a new privacy-preserving\nrepresentation; and ii) a mobile gait verification system based on the\ncombination of Convolutional Neural Networks (CNNs) and Recurrent Neural\nNetworks (RNNs) with a Siamese architecture. The main advantage of\nGaitPrivacyON is that the first module (convolutional Autoencoder) is trained\nin an unsupervised way, without specifying the sensitive attributes of the\nsubject to protect. The experimental results achieved using two popular\ndatabases (MotionSense and MobiAct) suggest the potential of GaitPrivacyON to\nsignificantly improve the privacy of the subject while keeping user\nauthentication results higher than 99% Area Under the Curve (AUC). To the best\nof our knowledge, this is the first mobile gait verification approach that\nconsiders privacy-preserving methods trained in an unsupervised way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delgado_Santos_P/0/1/0/all/0/1\">Paula Delgado-Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1\">Ruben Tolosana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guest_R/0/1/0/all/0/1\">Richard Guest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_R/0/1/0/all/0/1\">Ruben Vera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deravi_F/0/1/0/all/0/1\">Farzin Deravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Build a Curb Dataset with LiDAR Data for Autonomous Driving. (arXiv:2110.03968v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03968","description":"<p>Curbs are one of the essential elements of urban and highway traffic\nenvironments. Robust curb detection provides road structure information for\nmotion planning in an autonomous driving system. Commonly, video cameras and 3D\nLiDARs are mounted on autonomous vehicles for curb detection. However,\ncamera-based methods suffer from challenging illumination conditions. During\nthe long period of time before wide application of Deep Neural Network (DNN)\nwith point clouds, LiDAR-based curb detection methods are based on hand-crafted\nfeatures, which suffer from poor detection in some complex scenes. Recently,\nDNN-based dynamic object detection using LiDAR data has become prevalent, while\nfew works pay attention to curb detection with a DNN approach due to lack of\nlabeled data. A dataset with curb annotations or an efficient curb labeling\napproach, hence, is of high demand...\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_D/0/1/0/all/0/1\">Dongfeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tongtong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jingming Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingbing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maximize the Exploration of Congeneric Semantics for Weakly Supervised Semantic Segmentation. (arXiv:2110.03982v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03982","description":"<p>With the increase in the number of image data and the lack of corresponding\nlabels, weakly supervised learning has drawn a lot of attention recently in\ncomputer vision tasks, especially in the fine-grained semantic segmentation\nproblem. To alleviate human efforts from expensive pixel-by-pixel annotations,\nour method focuses on weakly supervised semantic segmentation (WSSS) with\nimage-level tags, which are much easier to obtain. As a huge gap exists between\npixel-level segmentation and image-level labels, how to reflect the image-level\nsemantic information on each pixel is an important question. To explore the\ncongeneric semantic regions from the same class to the maximum, we construct\nthe patch-level graph neural network (P-GNN) based on the self-detected patches\nfrom different images that contain the same class labels. Patches can frame the\nobjects as much as possible and include as little background as possible. The\ngraph network that is established with patches as the nodes can maximize the\nmutual learning of similar objects. We regard the embedding vectors of patches\nas nodes, and use transformer-based complementary learning module to construct\nweighted edges according to the embedding similarity between different nodes.\nMoreover, to better supplement semantic information, we propose\nsoft-complementary loss functions matched with the whole network structure. We\nconduct experiments on the popular PASCAL VOC 2012 benchmarks, and our model\nyields state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_Q/0/1/0/all/0/1\">Qi Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yucong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xin He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Feature-Specific Tree Species Identification from Natural Images using Deep Semi-Supervised Learning. (arXiv:2110.03994v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03994","description":"<p>Prior work on plant species classification predominantly focuses on building\nmodels from isolated plant attributes. Hence, there is a need for tools that\ncan assist in species identification in the natural world. We present a novel\nand robust two-fold approach capable of identifying trees in a real-world\nnatural setting. Further, we leverage unlabelled data through deep\nsemi-supervised learning and demonstrate superior performance to supervised\nlearning. Our single-GPU implementation for feature recognition uses minimal\nannotated data and achieves accuracies of 93.96% and 93.11% for leaves and\nbark, respectively. Further, we extract feature-specific datasets of 50 species\nby employing this technique. Finally, our semi-supervised species\nclassification method attains 94.04% top-5 accuracy for leaves and 83.04% top-5\naccuracy for bark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Homan_D/0/1/0/all/0/1\">Dewald Homan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Preez_J/0/1/0/all/0/1\">Johan A. du Preez</a> (1) ((1) Faculty of Engineering, Stellenbosch University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi Proxy Anchor Loss and Effectiveness of Deep Metric Learning Performance Metrics. (arXiv:2110.03997v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03997","description":"<p>Deep metric learning (DML) learns the mapping, which maps into embedding\nspace in which similar data is near and dissimilar data is far. Most DML\nframeworks apply L2 normalization to feature vectors, and these feature vectors\nare non-sparse. In this paper, we propose to apply L1 regularization loss to\nfeature vectors. Proposed regularization emphasizes important features and\nrestraints unimportant features on L2 normalized features. L1 regularization\ncan combine with general DML losses because L1 regularization only regularizes\nfeature vectors. In this paper, we finally propose SparseSoftTriple loss, which\nis a combination of SoftTriple loss and L1 regularization. We demonstrate the\neffectiveness of the proposed SparseSoftTriple loss on some data sets for image\nretrieval tasks and fine-grained images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saeki_S/0/1/0/all/0/1\">Shozo Saeki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_M/0/1/0/all/0/1\">Minoru Kawahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aman_H/0/1/0/all/0/1\">Hirohisa Aman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trident Pyramid Networks: The importance of processing at the feature pyramid level for better object detection. (arXiv:2110.04004v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04004","description":"<p>Feature pyramids have become ubiquitous in multi-scale computer vision tasks\nsuch as object detection. Based on their importance, we divide a computer\nvision network into three parts: a backbone (generating a feature pyramid), a\ncore (refining the feature pyramid) and a head (generating the final output).\nMost existing networks operating on feature pyramids, named cores, are shallow\nand mostly focus on communication-based processing in the form of top-down and\nbottom-up operations. We present a new core architecture called Trident Pyramid\nNetwork (TPN), that allows for a deeper design and for a better balance between\ncommunication-based processing and self-processing. We show consistent\nimprovements when using our TPN core on the COCO object detection benchmark,\noutperforming the popular BiFPN baseline by 1.5 AP. Additionally, we\nempirically show that it is more beneficial to put additional computation into\nthe TPN core, rather than into the backbone, by outperforming a ResNet-101+FPN\nbaseline with our ResNet-50+TPN network by 1.7 AP, while operating under\nsimilar computation budgets. This emphasizes the importance of performing\ncomputation at the feature pyramid level in modern-day object detection\nsystems. Code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Picron_C/0/1/0/all/0/1\">C&#xe9;dric Picron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1\">Tinne Tuytelaars</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An End-to-End Trainable Video Panoptic Segmentation Method usingTransformers. (arXiv:2110.04009v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04009","description":"<p>In this paper, we present an algorithm to tackle a video panoptic\nsegmentation problem, a newly emerging area of research. The video panoptic\nsegmentation is a task that unifies the typical task of panoptic segmentation\nand multi-object tracking. In other words, it requires generating the instance\ntracking IDs along with panoptic segmentation results across video sequences.\nOur proposed video panoptic segmentation algorithm uses the transformer and it\ncan be trained in end-to-end with an input of multiple video frames. We test\nour method on the STEP dataset and report its performance with recently\nproposed STQ metric. The method archived 57.81\\% on the KITTI-STEP dataset and\n31.8\\% on the MOTChallenge-STEP dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ryu_J/0/1/0/all/0/1\">Jeongwon Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1\">Kwangjin Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multidirectional Conjugate Gradients for Scalable Bundle Adjustment. (arXiv:2110.04015v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04015","description":"<p>We revisit the problem of large-scale bundle adjustment and propose a\ntechnique called Multidirectional Conjugate Gradients that accelerates the\nsolution of the normal equation by up to 61%. The key idea is that we enlarge\nthe search space of classical preconditioned conjugate gradients to include\nmultiple search directions. As a consequence, the resulting algorithm requires\nfewer iterations, leading to a significant speedup of large-scale\nreconstruction, in particular for denser problems where traditional approaches\nnotoriously struggle. We provide a number of experimental ablation studies\nrevealing the robustness to variations in the hyper-parameters and the speedup\nas a function of problem density.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weber_S/0/1/0/all/0/1\">Simon Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demmel_N/0/1/0/all/0/1\">Nikolaus Demmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chromatic Aberration Recovery on Arbitrary Images. (arXiv:2110.04030v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04030","description":"<p>Digital imaging sensor technology has continued to outpace development in\noptical technology in modern imaging systems. The resulting quality loss\nattributable to lateral chromatic aberration is becoming increasingly\nsignificant as sensor resolution increases; other classes of aberration are\nless significant with classical image enhancement (e.g. sharpening), whereas\nlateral chromatic aberration becomes more significant. The goals of\nhigher-performance and lighter lens systems drive a recent need to find new\nways to overcome resulting image quality limitations.\n</p>\n<p>This work demonstrates the robust and automatic minimisation of lateral\nchromatic aberration, recovering the loss of image quality using both\nartificial and real-world images. A series of test images are used to validate\nthe functioning of the algorithm, and changes across a series of real-world\nimages are used to evaluate the performance of the approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blueman_D/0/1/0/all/0/1\">Daniel J. Blueman</a> (University of Bristol)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniNet: Unified Architecture Search with Convolution, Transformer, and MLP. (arXiv:2110.04035v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04035","description":"<p>Recently, transformer and multi-layer perceptron (MLP) architectures have\nachieved impressive results on various vision tasks. A few works investigated\nmanually combining those operators to design visual network architectures, and\ncan achieve satisfactory performances to some extent. In this paper, we propose\nto jointly search the optimal combination of convolution, transformer, and MLP\nfor building a series of all-operator network architectures with high\nperformances on visual tasks. We empirically identify that the widely-used\nstrided convolution or pooling based down-sampling modules become the\nperformance bottlenecks when the operators are combined to form a network. To\nbetter tackle the global context captured by the transformer and MLP operators,\nwe propose two novel context-aware down-sampling modules, which can better\nadapt to the global information encoded by transformer and MLP operators. To\nthis end, we jointly search all operators and down-sampling modules in a\nunified search space. Notably, Our searched network UniNet (Unified Network)\noutperforms state-of-the-art pure convolution-based architecture, EfficientNet,\nand pure transformer-based architecture, Swin-Transformer, on multiple public\nvisual benchmarks, ImageNet classification, COCO object detection, and ADE20K\nsemantic segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Guanglu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-LGM: Leveraging Object-Context Relation for Context-Aware Object Recognition. (arXiv:2110.04042v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04042","description":"<p>Context, as referred to situational factors related to the object of\ninterest, can help infer the object's states or properties in visual\nrecognition. As such contextual features are too diverse (across instances) to\nbe annotated, existing attempts simply exploit image labels as supervision to\nlearn them, resulting in various contextual tricks, such as features pyramid,\ncontext attention, etc. However, without carefully modeling the context's\nproperties, especially its relation to the object, their estimated context can\nsuffer from large inaccuracy. To amend this problem, we propose a novel\nContextual Latent Generative Model (Context-LGM), which considers the\nobject-context relation and models it in a hierarchical manner. Specifically,\nwe firstly introduce a latent generative model with a pair of correlated latent\nvariables to respectively model the object and context, and embed their\ncorrelation via the generative process. Then, to infer contextual features, we\nreformulate the objective function of Variational Auto-Encoder (VAE), where\ncontextual features are learned as a posterior distribution conditioned on the\nobject. Finally, to implement this contextual posterior, we introduce a\nTransformer that takes the object's information as a reference and locates\ncorrelated contextual factors. The effectiveness of our method is verified by\nstate-of-the-art performance on two context-aware object recognition tasks,\ni.e. lung cancer prediction and emotion recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingzhou Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xinwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fandong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curating Subject ID Labels using Keypoint Signatures. (arXiv:2110.04055v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04055","description":"<p>Subject ID labels are unique, anonymized codes that can be used to group all\nimages of a subject while maintaining anonymity. ID errors may be inadvertently\nintroduced manually error during enrollment and may lead to systematic error\ninto machine learning evaluation (e.g. due to double-dipping) or potential\npatient misdiagnosis in clinical contexts. Here we describe a highly efficient\nsystem for curating subject ID labels in large generic medical image datasets,\nbased on the 3D image keypoint representation, which recently led to the\ndiscovery of previously unknown labeling errors in widely-used public brain MRI\ndatasets\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chauvin_L/0/1/0/all/0/1\">Laurent Chauvin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toews_M/0/1/0/all/0/1\">Matthew Toews</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Weakly Supervised Learning Approach for Real-time Iron Ore Feed Load Estimation. (arXiv:2110.04063v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04063","description":"<p>Iron ore feed load control is one of the most critical settings in a mineral\ngrinding process, directly impacting the quality of final products. The setting\nof the feed load is mainly determined by the characteristics of the ore\npellets. However, the characterisation of ore is challenging to acquire in many\nproduction environments, leading to poor feed load settings and inefficient\nproduction processes. This paper presents our work using deep learning models\nfor direct ore feed load estimation from ore pellet images. To address the\nchallenges caused by the large size of a full ore pellets image and the\nshortage of accurately annotated data, we treat the whole modelling process as\na weakly supervised learning problem. A two-stage model training algorithm and\ntwo neural network architectures are proposed. The experiment results show\ncompetitive model performance, and the trained models can be used for real-time\nfeed load estimation for grind process optimisation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Li Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yonghong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingyu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Neural Anthropometer Learning from Body Dimensions Computed on Human 3D Meshes. (arXiv:2110.04064v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04064","description":"<p>Human shape estimation has become increasingly important both theoretically\nand practically, for instance, in 3D mesh estimation, distance garment\nproduction and computational forensics, to mention just a few examples. As a\nfurther specialization, \\emph{Human Body Dimensions Estimation} (HBDE) focuses\non estimating human body measurements like shoulder width or chest\ncircumference from images or 3D meshes usually using supervised learning\napproaches. The main obstacle in this context is the data scarcity problem, as\ncollecting this ground truth requires expensive and difficult procedures. This\nobstacle can be overcome by obtaining realistic human measurements from 3D\nhuman meshes. However, a) there are no well established methods to calculate\nHBDs from 3D meshes and b) there are no benchmarks to fairly compare results on\nthe HBDE task. Our contribution is twofold. On the one hand, we present a\nmethod to calculate right and left arm length, shoulder width, and inseam\n(crotch height) from 3D meshes with focus on potential medical, virtual try-on\nand distance tailoring applications. On the other hand, we use four additional\nbody dimensions calculated using recently published methods to assemble a set\nof eight body dimensions which we use as a supervision signal to our Neural\nAnthropometer: a convolutional neural network capable of estimating these\ndimensions. To assess the estimation, we train the Neural Anthropometer with\nsynthetic images of 3D meshes, from which we calculated the HBDs and observed\nthat the network's overall mean estimate error is $20.89$ mm (relative error of\n2.84\\%). The results we present are fully reproducible and establish a fair\nbaseline for research on the task of HBDE, therefore enabling the community\nwith a valuable method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tejeda_Y/0/1/0/all/0/1\">Yansel Gonz&#xe1;lez Tejeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayer_H/0/1/0/all/0/1\">Helmut A. Mayer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Test-time Batch Statistics Calibration for Covariate Shift. (arXiv:2110.04065v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04065","description":"<p>Deep neural networks have a clear degradation when applying to the unseen\nenvironment due to the covariate shift. Conventional approaches like domain\nadaptation requires the pre-collected target data for iterative training, which\nis impractical in real-world applications. In this paper, we propose to adapt\nthe deep models to the novel environment during inference. An previous solution\nis test time normalization, which substitutes the source statistics in BN\nlayers with the target batch statistics. However, we show that test time\nnormalization may potentially deteriorate the discriminative structures due to\nthe mismatch between target batch statistics and source parameters. To this\nend, we present a general formulation $\\alpha$-BN to calibrate the batch\nstatistics by mixing up the source and target statistics for both alleviating\nthe domain shift and preserving the discriminative structures. Based on\n$\\alpha$-BN, we further present a novel loss function to form a unified test\ntime adaptation framework Core, which performs the pairwise class correlation\nonline optimization. Extensive experiments show that our approaches achieve the\nstate-of-the-art performance on total twelve datasets from three topics,\nincluding model robustness to corruptions, domain generalization on image\nclassification and semantic segmentation. Particularly, our $\\alpha$-BN\nimproves 28.4\\% to 43.9\\% on GTA5 $\\rightarrow$ Cityscapes without any\ntraining, even outperforms the latest source-free domain adaptation method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_F/0/1/0/all/0/1\">Fuming You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MToFNet: Object Anti-Spoofing with Mobile Time-of-Flight Data. (arXiv:2110.04066v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04066","description":"<p>In online markets, sellers can maliciously recapture others' images on\ndisplay screens to utilize as spoof images, which can be challenging to\ndistinguish in human eyes. To prevent such harm, we propose an anti-spoofing\nmethod using the paired rgb images and depth maps provided by the mobile camera\nwith a Time-of-Fight sensor. When images are recaptured on display screens,\nvarious patterns differing by the screens as known as the moir\\'e patterns can\nbe also captured in spoof images. These patterns lead the anti-spoofing model\nto be overfitted and unable to detect spoof images recaptured on unseen media.\nTo avoid the issue, we build a novel representation model composed of two\nembedding models, which can be trained without considering the recaptured\nimages. Also, we newly introduce mToF dataset, the largest and most diverse\nobject anti-spoofing dataset, and the first to utilize ToF data. Experimental\nresults confirm that our model achieves robust generalization even across\nunseen domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Yonghyun Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaehyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1\">Minki Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Solbi Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jongwon Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Slap Fingerprint Segmentation for Juveniles and Adults. (arXiv:2110.04067v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04067","description":"<p>Many fingerprint recognition systems capture four fingerprints in one image.\nIn such systems, the fingerprint processing pipeline must first segment each\nfour-fingerprint slap into individual fingerprints. Note that most of the\ncurrent fingerprint segmentation algorithms have been designed and evaluated\nusing only adult fingerprint datasets. In this work, we have developed a\nhuman-annotated in-house dataset of 15790 slaps of which 9084 are adult samples\nand 6706 are samples drawn from children from ages 4 to 12. Subsequently, the\ndataset is used to evaluate the matching performance of the NFSEG, a slap\nfingerprint segmentation system developed by NIST, on slaps from adults and\njuvenile subjects. Our results reveal the lower performance of NFSEG on slaps\nfrom juvenile subjects. Finally, we utilized our novel dataset to develop the\nMask-RCNN based Clarkson Fingerprint Segmentation (CFSEG). Our matching results\nusing the Verifinger fingerprint matcher indicate that CFSEG outperforms NFSEG\nfor both adults and juvenile slaps. The CFSEG model is publicly available at\n\\url{https://github.com/keivanB/Clarkson_Finger_Segment}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murshed_M/0/1/0/all/0/1\">M. G. Sarwar Murshed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kline_R/0/1/0/all/0/1\">Robert Kline</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahmani_K/0/1/0/all/0/1\">Keivan Bahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_F/0/1/0/all/0/1\">Faraz Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuckers_S/0/1/0/all/0/1\">Stephanie Schuckers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BI-RADS-Net: An Explainable Multitask Learning Approach for Cancer Diagnosis in Breast Ultrasound Images. (arXiv:2110.04069v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04069","description":"<p>In healthcare, it is essential to explain the decision-making process of\nmachine learning models to establish the trustworthiness of clinicians. This\npaper introduces BI-RADS-Net, a novel explainable deep learning approach for\ncancer detection in breast ultrasound images. The proposed approach\nincorporates tasks for explaining and classifying breast tumors, by learning\nfeature representations relevant to clinical diagnosis. Explanations of the\npredictions (benign or malignant) are provided in terms of morphological\nfeatures that are used by clinicians for diagnosis and reporting in medical\npractice. The employed features include the BI-RADS descriptors of shape,\norientation, margin, echo pattern, and posterior features. Additionally, our\napproach predicts the likelihood of malignancy of the findings, which relates\nto the BI-RADS assessment category reported by clinicians. Experimental\nvalidation on a dataset consisting of 1,192 images indicates improved model\naccuracy, supported by explanations in clinical terms using the BI-RADS\nlexicon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Boyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakanski_A/0/1/0/all/0/1\">Aleksandar Vakanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_M/0/1/0/all/0/1\">Min Xian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataset Structural Index: Understanding a machine's perspective towards visual data. (arXiv:2110.04070v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04070","description":"<p>With advances in vision and perception architectures, we have realized that\nworking with data is equally crucial, if not more, than the algorithms. Till\ntoday, we have trained machines based on our knowledge and perspective of the\nworld. The entire concept of Dataset Structural Index(DSI) revolves around\nunderstanding a machine`s perspective of the dataset. With DSI, I show two meta\nvalues with which we can get more information over a visual dataset and use it\nto optimize data, create better architectures, and have an ability to guess\nwhich model would work best. These two values are the Variety contribution\nratio and Similarity matrix. In the paper, I show many applications of DSI, one\nof which is how the same level of accuracy can be achieved with the same model\narchitectures trained over less amount of data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Dishant Parikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KOHTD: Kazakh Offline Handwritten Text Dataset. (arXiv:2110.04075v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04075","description":"<p>Despite the transition to digital information exchange, many documents, such\nas invoices, taxes, memos and questionnaires, historical data, and answers to\nexam questions, still require handwritten inputs. In this regard, there is a\nneed to implement Handwritten Text Recognition (HTR) which is an automatic way\nto decrypt records using a computer. Handwriting recognition is challenging\nbecause of the virtually infinite number of ways a person can write the same\nmessage. For this proposal we introduce Kazakh handwritten text recognition\nresearch, a comprehensive dataset of Kazakh handwritten texts is necessary.\nThis is particularly true given the lack of a dataset for handwritten Kazakh\ntext. In this paper, we proposed our extensive Kazakh offline Handwritten Text\ndataset (KOHTD), which has 3000 handwritten exam papers and more than 140335\nsegmented images and there are approximately 922010 symbols. It can serve\nresearchers in the field of handwriting recognition tasks by using deep and\nmachine learning. We used a variety of popular text recognition methods for\nword and line recognition in our studies, including CTC-based and\nattention-based methods. The findings demonstrate KOHTD's diversity. Also, we\nproposed a Genetic Algorithm (GA) for line and word segmentation based on\nrandom enumeration of a parameter. The dataset and GA code are available at\nhttps://github.com/abdoelsayed2016/KOHTD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toiganbayeva_N/0/1/0/all/0/1\">Nazgul Toiganbayeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasem_M/0/1/0/all/0/1\">Mahmoud Kasem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdimanap_G/0/1/0/all/0/1\">Galymzhan Abdimanap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bostanbekov_K/0/1/0/all/0/1\">Kairat Bostanbekov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_A/0/1/0/all/0/1\">Abdelrahman Abdallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alimova_A/0/1/0/all/0/1\">Anel Alimova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurseitov_D/0/1/0/all/0/1\">Daniyar Nurseitov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Point Cloud Prediction Using 3D Spatio-temporal Convolutional Networks. (arXiv:2110.04076v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04076","description":"<p>Exploiting past 3D LiDAR scans to predict future point clouds is a promising\nmethod for autonomous mobile systems to realize foresighted state estimation,\ncollision avoidance, and planning. In this paper, we address the problem of\npredicting future 3D LiDAR point clouds given a sequence of past LiDAR scans.\nEstimating the future scene on the sensor level does not require any preceding\nsteps as in localization or tracking systems and can be trained\nself-supervised. We propose an end-to-end approach that exploits a 2D range\nimage representation of each 3D LiDAR scan and concatenates a sequence of range\nimages to obtain a 3D tensor. Based on such tensors, we develop an\nencoder-decoder architecture using 3D convolutions to jointly aggregate spatial\nand temporal information of the scene and to predict the future 3D point\nclouds. We evaluate our method on multiple datasets and the experimental\nresults suggest that our method outperforms existing point cloud prediction\narchitectures and generalizes well to new, unseen environments without\nadditional fine-tuning. Our method operates online and is faster than the\ncommon LiDAR frame rate of 10 Hz.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mersch_B/0/1/0/all/0/1\">Benedikt Mersch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xieyuanli Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behley_J/0/1/0/all/0/1\">Jens Behley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stachniss_C/0/1/0/all/0/1\">Cyrill Stachniss</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physical Context and Timing Aware Sequence Generating GANs. (arXiv:2110.04077v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04077","description":"<p>Generative Adversarial Networks (GANs) have shown remarkable successes in\ngenerating realistic images and interpolating changes between images. Existing\nmodels, however, do not take into account physical contexts behind images in\ngenerating the images, which may cause unrealistic changes. Furthermore, it is\ndifficult to generate the changes at a specific timing and they often do not\nmatch with actual changes. This paper proposes a novel GAN, named Physical\nContext and Timing aware sequence generating GANs (PCTGAN), that generates an\nimage in a sequence at a specific timing between two images with considering\nphysical contexts behind them. Our method consists of three components: an\nencoder, a generator, and a discriminator. The encoder estimates latent vectors\nfrom the beginning and ending images, their timings, and a target timing. The\ngenerator generates images and the physical contexts at the beginning, ending,\nand target timing from the corresponding latent vectors. The discriminator\ndiscriminates whether the generated images and contexts are real or not. In the\nexperiments, PCTGAN is applied to a data set of sequential changes of shapes in\ndie forging processes. We show that both timing and physical contexts are\neffective in generating sequential images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Futase_H/0/1/0/all/0/1\">Hayato Futase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsujimura_T/0/1/0/all/0/1\">Tomoki Tsujimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kajimoto_T/0/1/0/all/0/1\">Tetsuya Kajimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawarazaki_H/0/1/0/all/0/1\">Hajime Kawarazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_T/0/1/0/all/0/1\">Toshiyuki Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miwa_M/0/1/0/all/0/1\">Makoto Miwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_Y/0/1/0/all/0/1\">Yutaka Sasaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hybrid Spatial-temporal Sequence-to-one Neural Network Model for Lane Detection. (arXiv:2110.04079v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04079","description":"<p>Reliable and accurate lane detection is of vital importance for the safe\nperformance of Lane Keeping Assistance and Lane Departure Warning systems.\nHowever, under certain challenging peculiar circumstances (e.g., marking\ndegradation, serious vehicle occlusion), it is difficult to get satisfactory\nperformance in accurately detecting the lane markings from one single image\nwhich is often the case in current literature. Since road markings are\ncontinuous lines on the road, the lanes that are difficult to be accurately\ndetected in the current image frame might potentially be better inferred out if\ninformation from previous frames is incorporated. For this, we propose a novel\nhybrid spatial-temporal sequence-to-one deep learning architecture making full\nuse of the spatial-temporal information in multiple frames of a continuous\nsequence of images to detect lane markings in the very last current image\nframe. Specifically, the hybrid model integrates the spatial convolutional\nneural network (SCNN), which is powerful in extracting spatial features and\nrelationships in one single image, with convolutional long-short term memory\n(ConvLSTM) neural network, which can capture the spatial-temporal correlations\nand time dependencies among the image sequences. With the proposed model\narchitecture, the advantages of both SCNN and ConvLSTM are fully combined and\nthe spatial-temporal information is fully exploited. Treating lane detection as\nthe image segmentation problem, we applied encoder-decoder structures to make\nit work in an end-to-end way. Extensive experiments on two large-scale datasets\nreveal that our proposed model can effectively handle challenging driving\nscenes and outperforms previous state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yongqi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_S/0/1/0/all/0/1\">Sandeep Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arem_B/0/1/0/all/0/1\">Bart van Arem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farah_H/0/1/0/all/0/1\">Haneen Farah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Landslide Detection in Real-Time Social Media Image Streams. (arXiv:2110.04080v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04080","description":"<p>Lack of global data inventories obstructs scientific modeling of and response\nto landslide hazards which are oftentimes deadly and costly. To remedy this\nlimitation, new approaches suggest solutions based on citizen science that\nrequires active participation. However, as a non-traditional data source,\nsocial media has been increasingly used in many disaster response and\nmanagement studies in recent years. Inspired by this trend, we propose to\ncapitalize on social media data to mine landslide-related information\nautomatically with the help of artificial intelligence (AI) techniques.\nSpecifically, we develop a state-of-the-art computer vision model to detect\nlandslides in social media image streams in real time. To that end, we create a\nlarge landslide image dataset labeled by experts and conduct extensive model\ntraining experiments. The experimental results indicate that the proposed model\ncan be deployed in an online fashion to support global landslide susceptibility\nmaps and emergency response.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1\">Ferda Ofli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1\">Muhammad Imran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qazi_U/0/1/0/all/0/1\">Umair Qazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roch_J/0/1/0/all/0/1\">Julien Roch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pennington_C/0/1/0/all/0/1\">Catherine Pennington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banks_V/0/1/0/all/0/1\">Vanessa J. Banks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bossu_R/0/1/0/all/0/1\">Remy Bossu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flow Plugin Network for conditional generation. (arXiv:2110.04081v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04081","description":"<p>Generative models have gained many researchers' attention in the last years\nresulting in models such as StyleGAN for human face generation or PointFlow for\nthe 3D point cloud generation. However, by default, we cannot control its\nsampling process, i.e., we cannot generate a sample with a specific set of\nattributes. The current approach is model retraining with additional inputs and\ndifferent architecture, which requires time and computational resources. We\npropose a novel approach that enables to a generation of objects with a given\nset of attributes without retraining the base model. For this purpose, we\nutilize the normalizing flow models - Conditional Masked Autoregressive Flow\nand Conditional Real NVP, as a Flow Plugin Network (FPN).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wielopolski_P/0/1/0/all/0/1\">Patryk Wielopolski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koperski_M/0/1/0/all/0/1\">Micha&#x142; Koperski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zieba_M/0/1/0/all/0/1\">Maciej Zi&#x119;ba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discover, Hallucinate, and Adapt: Open Compound Domain Adaptation for Semantic Segmentation. (arXiv:2110.04111v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04111","description":"<p>Unsupervised domain adaptation (UDA) for semantic segmentation has been\nattracting attention recently, as it could be beneficial for various\nlabel-scarce real-world scenarios (e.g., robot control, autonomous driving,\nmedical imaging, etc.). Despite the significant progress in this field, current\nworks mainly focus on a single-source single-target setting, which cannot\nhandle more practical settings of multiple targets or even unseen targets. In\nthis paper, we investigate open compound domain adaptation (OCDA), which deals\nwith mixed and novel situations at the same time, for semantic segmentation. We\npresent a novel framework based on three main design principles: discover,\nhallucinate, and adapt. The scheme first clusters compound target data based on\nstyle, discovering multiple latent domains (discover). Then, it hallucinates\nmultiple latent target domains in source by using image-translation\n(hallucinate). This step ensures the latent domains in the source and the\ntarget to be paired. Finally, target-to-source alignment is learned separately\nbetween domains (adapt). In high-level, our solution replaces a hard OCDA\nproblem with much easier multiple UDA problems. We evaluate our solution on\nstandard benchmark GTA to C-driving, and achieved new state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">KwanYong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Sanghyun Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_I/0/1/0/all/0/1\">Inkyu Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-viewpoint Outdoor Dataset for Human Action Recognition. (arXiv:2110.04119v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04119","description":"<p>Advancements in deep neural networks have contributed to near perfect results\nfor many computer vision problems such as object recognition, face recognition\nand pose estimation. However, human action recognition is still far from\nhuman-level performance. Owing to the articulated nature of the human body, it\nis challenging to detect an action from multiple viewpoints, particularly from\nan aerial viewpoint. This is further compounded by a scarcity of datasets that\ncover multiple viewpoints of actions. To fill this gap and enable research in\nwider application areas, we present a multi-viewpoint outdoor action\nrecognition dataset collected from YouTube and our own drone. The dataset\nconsists of 20 dynamic human action classes, 2324 video clips and 503086\nframes. All videos are cropped and resized to 720x720 without distorting the\noriginal aspect ratio of the human subjects in videos. This dataset should be\nuseful to many research areas including action recognition, surveillance and\nsituational awareness. We evaluated the dataset with a two-stream CNN\narchitecture coupled with a recently proposed temporal pooling scheme called\nkernelized rank pooling that produces nonlinear feature subspace\nrepresentations. The overall baseline action recognition accuracy is 74.0%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perera_A/0/1/0/all/0/1\">Asanka G. Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_Y/0/1/0/all/0/1\">Yee Wei Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogunwa_T/0/1/0/all/0/1\">Titilayo T. Ogunwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chahl_J/0/1/0/all/0/1\">Javaan Chahl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rapid head-pose detection for automated slice prescription of fetal-brain MRI. (arXiv:2110.04140v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04140","description":"<p>In fetal-brain MRI, head-pose changes between prescription and acquisition\npresent a challenge to obtaining the standard sagittal, coronal and axial views\nessential to clinical assessment. As motion limits acquisitions to thick slices\nthat preclude retrospective resampling, technologists repeat ~55-second\nstack-of-slices scans (HASTE) with incrementally reoriented field of view\nnumerous times, deducing the head pose from previous stacks. To address this\ninefficient workflow, we propose a robust head-pose detection algorithm using\nfull-uterus scout scans (EPI) which take ~5 seconds to acquire. Our ~2-second\nprocedure automatically locates the fetal brain and eyes, which we derive from\nmaximally stable extremal regions (MSERs). The success rate of the method\nexceeds 94% in the third trimester, outperforming a trained technologist by up\nto 20%. The pipeline may be used to automatically orient the anatomical\nsequence, removing the need to estimate the head pose from 2D views and\nreducing delays during which motion can occur.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_M/0/1/0/all/0/1\">Malte Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turk_E/0/1/0/all/0/1\">Esra Abaci Turk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gagoski_B/0/1/0/all/0/1\">Borjan Gagoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_L/0/1/0/all/0/1\">Leah Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wighton_P/0/1/0/all/0/1\">Paul Wighton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tisdall_M/0/1/0/all/0/1\">M. Dylan Tisdall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuter_M/0/1/0/all/0/1\">Martin Reuter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adalsteinsson_E/0/1/0/all/0/1\">Elfar Adalsteinsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grant_P/0/1/0/all/0/1\">P. Ellen Grant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wald_L/0/1/0/all/0/1\">Lawrence L. Wald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouwe_A/0/1/0/all/0/1\">Andr&#xe9; J. W. van der Kouwe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainability-Aware One Point Attack for Point Cloud Neural Networks. (arXiv:2110.04158v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04158","description":"<p>With the proposition of neural networks for point clouds, deep learning has\nstarted to shine in the field of 3D object recognition while researchers have\nshown an increased interest to investigate the reliability of point cloud\nnetworks by fooling them with perturbed instances. However, most studies focus\non the imperceptibility or surface consistency, with humans perceiving no\nperturbations on the adversarial examples. This work proposes two new attack\nmethods: opa and cta, which go in the opposite direction: we restrict the\nperturbation dimensions to a human cognizable range with the help of\nexplainability methods, which enables the working principle or decision\nboundary of the models to be comprehensible through the observable perturbation\nmagnitude. Our results show that the popular point cloud networks can be\ndeceived with almost 100% success rate by shifting only one point from the\ninput instance. In addition, we attempt to provide a more persuasive viewpoint\nof comparing the robustness of point cloud models against adversarial attacks.\nWe also show the interesting impact of different point attribution\ndistributions on the adversarial robustness of point cloud networks. Finally,\nwe discuss how our approaches facilitate the explainability study for point\ncloud networks. To the best of our knowledge, this is the first\npoint-cloud-based adversarial approach concerning explainability. Our code is\navailable at https://github.com/Explain3D/Exp-One-Point-Atk-PC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hanxiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1\">Helena Kotthaus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Image Alignment for Vehicle Localization. (arXiv:2110.04162v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04162","description":"<p>Accurate and reliable localization is a fundamental requirement for\nautonomous vehicles to use map information in higher-level tasks such as\nnavigation or planning. In this paper, we present a novel approach to vehicle\nlocalization in dense semantic maps, including vectorized high-definition maps\nor 3D meshes, using semantic segmentation from a monocular camera. We formulate\nthe localization task as a direct image alignment problem on semantic images,\nwhich allows our approach to robustly track the vehicle pose in semantically\nlabeled maps by aligning virtual camera views rendered from the map to\nsequences of semantically segmented camera images. In contrast to existing\nvisual localization approaches, the system does not require additional keypoint\nfeatures, handcrafted localization landmark extractors or expensive LiDAR\nsensors. We demonstrate the wide applicability of our method on a diverse set\nof semantic mesh maps generated from stereo or LiDAR as well as manually\nannotated HD maps and show that it achieves reliable and accurate localization\nin real-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Herb_M/0/1/0/all/0/1\">Markus Herb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lemberger_M/0/1/0/all/0/1\">Matthias Lemberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1\">Marcel M. Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurz_A/0/1/0/all/0/1\">Alexander Kurz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiherer_T/0/1/0/all/0/1\">Tobias Weiherer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight Convolutional Neural Networks By Hypercomplex Parameterization. (arXiv:2110.04176v1 [cs.LG])","link":"http://arxiv.org/abs/2110.04176","description":"<p>Hypercomplex neural networks have proved to reduce the overall number of\nparameters while ensuring valuable performances by leveraging the properties of\nClifford algebras. Recently, hypercomplex linear layers have been further\nimproved by involving efficient parameterized Kronecker products. In this\npaper, we define the parameterization of hypercomplex convolutional layers to\ndevelop lightweight and efficient large-scale convolutional models. Our method\ngrasps the convolution rules and the filters organization directly from data\nwithout requiring a rigidly predefined domain structure to follow. The proposed\napproach is flexible to operate in any user-defined or tuned domain, from 1D to\n$n$D regardless of whether the algebra rules are preset. Such a malleability\nallows processing multidimensional inputs in their natural domain without\nannexing further dimensions, as done, instead, in quaternion neural networks\nfor 3D inputs like color images. As a result, the proposed method operates with\n$1/n$ free parameters as regards its analog in the real domain. We demonstrate\nthe versatility of this approach to multiple domains of application by\nperforming experiments on various image datasets as well as audio datasets in\nwhich our method outperforms real and quaternion-valued counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grassucci_E/0/1/0/all/0/1\">Eleonora Grassucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comminiello_D/0/1/0/all/0/1\">Danilo Comminiello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataset Condensation with Distribution Matching. (arXiv:2110.04181v1 [cs.LG])","link":"http://arxiv.org/abs/2110.04181","description":"<p>Computational cost to train state-of-the-art deep models in many learning\nproblems is rapidly increasing due to more sophisticated models and larger\ndatasets. A recent promising direction to reduce training time is dataset\ncondensation that aims to replace the original large training set with a\nsignificantly smaller learned synthetic set while preserving its information.\nWhile training deep models on the small set of condensed images can be\nextremely fast, their synthesis remains computationally expensive due to the\ncomplex bi-level optimization and second-order derivative computation. In this\nwork, we propose a simple yet effective dataset condensation technique that\nrequires significantly lower training cost with comparable performance by\nmatching feature distributions of the synthetic and original training images in\nsampled embedding spaces. Thanks to its efficiency, we apply our method to more\nrealistic and larger datasets with sophisticated neural architectures and\nachieve a significant performance boost while using larger synthetic training\nset. We also show various practical benefits of our method in continual\nlearning and neural architecture search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1\">Hakan Bilen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting the Intrinsic Neighborhood Structure for Source-free Domain Adaptation. (arXiv:2110.04202v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04202","description":"<p>Domain adaptation (DA) aims to alleviate the domain shift between source\ndomain and target domain. Most DA methods require access to the source data,\nbut often that is not possible (e.g. due to data privacy or intellectual\nproperty). In this paper, we address the challenging source-free domain\nadaptation (SFDA) problem, where the source pretrained model is adapted to the\ntarget domain in the absence of source data. Our method is based on the\nobservation that target data, which might no longer align with the source\ndomain classifier, still forms clear clusters. We capture this intrinsic\nstructure by defining local affinity of the target data, and encourage label\nconsistency among data with high local affinity. We observe that higher\naffinity should be assigned to reciprocal neighbors, and propose a self\nregularization loss to decrease the negative impact of noisy neighbors.\nFurthermore, to aggregate information with more context, we consider expanded\nneighborhoods with small affinity values. In the experimental results we verify\nthat the inherent structure of the target features is an important source of\ninformation for domain adaptation. We demonstrate that this local structure can\nbe efficiently captured by considering the local neighbors, the reciprocal\nneighbors, and the expanded neighborhood. Finally, we achieve state-of-the-art\nperformance on several 2D image and 3D point cloud recognition datasets. Code\nis available in https://github.com/Albert0147/SFDA_neighbors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shiqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaxing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herranz_L/0/1/0/all/0/1\">Luis Herranz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1\">Shangling Jui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward a Human-Level Video Understanding Intelligence. (arXiv:2110.04203v1 [cs.AI])","link":"http://arxiv.org/abs/2110.04203","description":"<p>We aim to develop an AI agent that can watch video clips and have a\nconversation with human about the video story. Developing video understanding\nintelligence is a significantly challenging task, and evaluation methods for\nadequately measuring and analyzing the progress of AI agent are lacking as\nwell. In this paper, we propose the Video Turing Test to provide effective and\npractical assessments of video understanding intelligence as well as\nhuman-likeness evaluation of AI agents. We define a general format and\nprocedure of the Video Turing Test and present a case study to confirm the\neffectiveness and usefulness of the proposed test.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heo_Y/0/1/0/all/0/1\">Yu-Jung Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minsu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seongho Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1\">Woo Suk Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1\">Minjung Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_M/0/1/0/all/0/1\">Minjoon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_J/0/1/0/all/0/1\">Jeh-Kwang Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Byoung-Tak Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inferring Offensiveness In Images From Natural Language Supervision. (arXiv:2110.04222v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04222","description":"<p>Probing or fine-tuning (large-scale) pre-trained models results in\nstate-of-the-art performance for many NLP tasks and, more recently, even for\ncomputer vision tasks when combined with image data. Unfortunately, these\napproaches also entail severe risks. In particular, large image datasets\nautomatically scraped from the web may contain derogatory terms as categories\nand offensive images, and may also underrepresent specific classes.\nConsequently, there is an urgent need to carefully document datasets and curate\ntheir content. Unfortunately, this process is tedious and error-prone. We show\nthat pre-trained transformers themselves provide a methodology for the\nautomated curation of large-scale vision datasets. Based on human-annotated\nexamples and the implicit knowledge of a CLIP based model, we demonstrate that\none can select relevant prompts for rating the offensiveness of an image. In\naddition to e.g. privacy violation and pornographic content previously\nidentified in ImageNet, we demonstrate that our approach identifies further\ninappropriate and potentially offensive content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Observations on K-image Expansion of Image-Mixing Augmentation for Classification. (arXiv:2110.04248v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04248","description":"<p>Image-mixing augmentations (e.g., Mixup or CutMix), which typically mix two\nimages, have become de-facto training tricks for image classification. Despite\ntheir huge success on image classification, the number of images to mix has not\nbeen profoundly investigated by the previous works, only showing the naive\nK-image expansion leads to poor performance degradation. This paper derives a\nnew K-image mixing augmentation based on the stick-breaking process under\nDirichlet prior. We show that our method can train more robust and generalized\nclassifiers through extensive experiments and analysis on classification\naccuracy, a shape of a loss landscape and adversarial robustness, than the\nusual two-image methods. Furthermore, we show that our probabilistic model can\nmeasure the sample-wise uncertainty and can boost the efficiency for Network\nArchitecture Search (NAS) with 7x reduced search time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Joonhyun Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sungmin Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Youngjoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sangdoo Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jongwon Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active learning for interactive satellite image change detection. (arXiv:2110.04250v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04250","description":"<p>We introduce in this paper a novel active learning algorithm for satellite\nimage change detection. The proposed solution is interactive and based on a\nquestion and answer model, which asks an oracle (annotator) the most\ninformative questions about the relevance of sampled satellite image pairs, and\naccording to the oracle's responses, updates a decision function iteratively.\nWe investigate a novel framework which models the probability that samples are\nrelevant; this probability is obtained by minimizing an objective function\ncapturing representativity, diversity and ambiguity. Only data with a high\nprobability according to these criteria are selected and displayed to the\noracle for further annotation. Extensive experiments on the task of satellite\nimage change detection after natural hazards (namely tornadoes) show the\nrelevance of the proposed method against the related work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahbi_H/0/1/0/all/0/1\">Hichem Sahbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deschamps_S/0/1/0/all/0/1\">Sebastien Deschamps</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoian_A/0/1/0/all/0/1\">Andrei Stoian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LCS: Learning Compressible Subspaces for Adaptive Network Compression at Inference Time. (arXiv:2110.04252v1 [cs.LG])","link":"http://arxiv.org/abs/2110.04252","description":"<p>When deploying deep learning models to a device, it is traditionally assumed\nthat available computational resources (compute, memory, and power) remain\nstatic. However, real-world computing systems do not always provide stable\nresource guarantees. Computational resources need to be conserved when load\nfrom other processes is high or battery power is low. Inspired by recent works\non neural network subspaces, we propose a method for training a \"compressible\nsubspace\" of neural networks that contains a fine-grained spectrum of models\nthat range from highly efficient to highly accurate. Our models require no\nretraining, thus our subspace of models can be deployed entirely on-device to\nallow adaptive network compression at inference time. We present results for\nachieving arbitrarily fine-grained accuracy-efficiency trade-offs at inference\ntime for structured and unstructured sparsity. We achieve accuracies on-par\nwith standard models when testing our uncompressed models, and maintain high\naccuracy for sparsity rates above 90% when testing our compressed models. We\nalso demonstrate that our algorithm extends to quantization at variable bit\nwidths, achieving accuracy on par with individually trained networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nunez_E/0/1/0/all/0/1\">Elvis Nunez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horton_M/0/1/0/all/0/1\">Maxwell Horton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhu_A/0/1/0/all/0/1\">Anish Prabhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_A/0/1/0/all/0/1\">Anurag Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1\">Mohammad Rastegari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StairwayGraphNet for Inter- and Intra-modality Multi-resolution Brain Graph Alignment and Synthesis. (arXiv:2110.04279v1 [eess.IV])","link":"http://arxiv.org/abs/2110.04279","description":"<p>Synthesizing multimodality medical data provides complementary knowledge and\nhelps doctors make precise clinical decisions. Although promising, existing\nmultimodal brain graph synthesis frameworks have several limitations. First,\nthey mainly tackle only one problem (intra- or inter-modality), limiting their\ngeneralizability to synthesizing inter- and intra-modality simultaneously.\nSecond, while few techniques work on super-resolving low-resolution brain\ngraphs within a single modality (i.e., intra), inter-modality graph\nsuper-resolution remains unexplored though this would avoid the need for costly\ndata collection and processing. More importantly, both target and source\ndomains might have different distributions, which causes a domain fracture\nbetween them. To fill these gaps, we propose a multi-resolution\nStairwayGraphNet (SG-Net) framework to jointly infer a target graph modality\nbased on a given modality and super-resolve brain graphs in both inter and\nintra domains. Our SG-Net is grounded in three main contributions: (i)\npredicting a target graph from a source one based on a novel graph generative\nadversarial network in both inter (e.g., morphological-functional) and intra\n(e.g., functional-functional) domains, (ii) generating high-resolution brain\ngraphs without resorting to the time consuming and expensive MRI processing\nsteps, and (iii) enforcing the source distribution to match that of the ground\ntruth graphs using an inter-modality aligner to relax the loss function to\noptimize. Moreover, we design a new Ground Truth-Preserving loss function to\nguide both generators in learning the topological structure of ground truth\nbrain graphs more accurately. Our comprehensive experiments on predicting\ntarget brain graphs from source graphs using a multi-resolution stairway showed\nthe outperformance of our method in comparison with its variants and\nstate-of-the-art method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mhiri_I/0/1/0/all/0/1\">Islem Mhiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahjoub_M/0/1/0/all/0/1\">Mohamed Ali Mahjoub</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rekik_I/0/1/0/all/0/1\">Islem Rekik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collaging Class-specific GANs for Semantic Image Synthesis. (arXiv:2110.04281v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04281","description":"<p>We propose a new approach for high resolution semantic image synthesis. It\nconsists of one base image generator and multiple class-specific generators.\nThe base generator generates high quality images based on a segmentation map.\nTo further improve the quality of different objects, we create a bank of\nGenerative Adversarial Networks (GANs) by separately training class-specific\nmodels. This has several benefits including -- dedicated weights for each\nclass; centrally aligned data for each model; additional training data from\nother sources, potential of higher resolution and quality; and easy\nmanipulation of a specific object in the scene. Experiments show that our\napproach can generate high quality images in high resolution while having\nflexibility of object-level control by using class-specific generators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yijun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingwan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Jae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Krishna Kumar Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Field Extraction from Forms with Unlabeled Data. (arXiv:2110.04282v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04282","description":"<p>We propose a novel framework to conduct field extraction from forms with\nunlabeled data. To bootstrap the training process, we develop a rule-based\nmethod for mining noisy pseudo-labels from unlabeled forms. Using the\nsupervisory signal from the pseudo-labels, we extract a discriminative token\nrepresentation from a transformer-based model by modeling the interaction\nbetween text in the form. To prevent the model from overfitting to label noise,\nwe introduce a refinement module based on a progressive pseudo-label ensemble.\nExperimental results demonstrate the effectiveness of our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingfei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_N/0/1/0/all/0/1\">Nikhil Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kazuma Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ran Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward a Visual Concept Vocabulary for GAN Latent Space. (arXiv:2110.04292v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04292","description":"<p>A large body of recent work has identified transformations in the latent\nspaces of generative adversarial networks (GANs) that consistently and\ninterpretably transform generated images. But existing techniques for\nidentifying these transformations rely on either a fixed vocabulary of\npre-specified visual concepts, or on unsupervised disentanglement techniques\nwhose alignment with human judgments about perceptual salience is unknown. This\npaper introduces a new method for building open-ended vocabularies of primitive\nvisual concepts represented in a GAN's latent space. Our approach is built from\nthree components: (1) automatic identification of perceptually salient\ndirections based on their layer selectivity; (2) human annotation of these\ndirections with free-form, compositional natural language descriptions; and (3)\ndecomposition of these annotations into a visual concept vocabulary, consisting\nof distilled directions labeled with single words. Experiments show that\nconcepts learned with our approach are reliable and composable -- generalizing\nacross classes, contexts, and observers, and enabling fine-grained manipulation\nof image style and content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwettmann_S/0/1/0/all/0/1\">Sarah Schwettmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_E/0/1/0/all/0/1\">Evan Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_S/0/1/0/all/0/1\">Samuel Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"2nd Place Solution to Google Landmark Retrieval 2021. (arXiv:2110.04294v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04294","description":"<p>This paper presents the 2nd place solution to the Google Landmark Retrieval\n2021 Competition on Kaggle. The solution is based on a baseline with training\ntricks from person re-identification, a continent-aware sampling strategy is\npresented to select training images according to their country tags and a\nLandmark-Country aware reranking is proposed for the retrieval task. With these\ncontributions, we achieve 0.52995 mAP@100 on private leaderboard. Code\navailable at\nhttps://github.com/WesleyZhang1991/Google_Landmark_Retrieval_2021_2nd_Place_Solution\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuqi_Z/0/1/0/all/0/1\">Zhang Yuqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xianzhe_X/0/1/0/all/0/1\">Xu Xianzhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weihua_C/0/1/0/all/0/1\">Chen Weihua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaohua_W/0/1/0/all/0/1\">Wang Yaohua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fangyi_Z/0/1/0/all/0/1\">Zhang Fangyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_L/0/1/0/all/0/1\">Li Hao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep localization of protein structures in fluorescence microscopy images. (arXiv:1910.04287v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1910.04287","description":"<p>Accurate localization of proteins from fluorescence microscopy images is\nchallenging due to the inter-class similarities and intra-class disparities\nintroducing grave concerns in addressing multi-class classification problems.\nConventional machine learning-based image prediction pipelines rely heavily on\npre-processing such as normalization and segmentation followed by hand-crafted\nfeature extraction to identify useful, informative, and application-specific\nfeatures. Here, we demonstrate that deep learning-based pipelines can\neffectively classify protein images from different datasets. We propose an\nend-to-end Protein Localization Convolutional Neural Network (PLCNN) that\nclassifies protein images more accurately and reliably. PLCNN processes raw\nimagery without involving any pre-processing steps and produces outputs without\nany customization or parameter adjustment for a particular dataset.\nExperimental analysis is performed on five benchmark datasets. PLCNN\nconsistently outperformed the existing state-of-the-art approaches from\ntraditional machine learning and deep architectures. This study highlights the\nimportance of deep learning for the analysis of fluorescence microscopy protein\nimagery. The proposed deep pipeline can better guide drug designing procedures\nin the pharmaceutical industry and open new avenues for researchers in\ncomputational biology and bioinformatics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tahir_M/0/1/0/all/0/1\">Muhammad Tahir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muzaffar_A/0/1/0/all/0/1\">Abdul Wahab Muzaffar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SynthMorph: learning contrast-invariant registration without acquired images. (arXiv:2004.10282v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2004.10282","description":"<p>We introduce a strategy for learning image registration without acquired\nimaging data, producing powerful networks agnostic to contrast introduced by\nmagnetic resonance imaging (MRI). While classical registration methods\naccurately estimate the spatial correspondence between images, they solve an\noptimization problem for every new image pair. Learning-based techniques are\nfast at test time but limited to registering images with contrasts and\ngeometric content similar to those seen during training. We propose to remove\nthis dependency on training data by leveraging a generative strategy for\ndiverse synthetic label maps and images that exposes networks to a wide range\nof variability, forcing them to learn more invariant features. This approach\nresults in powerful networks that accurately generalize to a broad array of MRI\ncontrasts. We present extensive experiments with a focus on 3D neuroimaging,\nshowing that this strategy enables robust and accurate registration of\narbitrary MRI contrasts even if the target contrast is not seen by the networks\nduring training. We demonstrate registration accuracy surpassing the state of\nthe art both within and across contrasts, using a single model. Critically,\ntraining on arbitrary shapes synthesized from noise distributions results in\ncompetitive performance, removing the dependency on acquired data of any kind.\nAdditionally, since anatomical label maps are often available for the anatomy\nof interest, we show that synthesizing images from these dramatically boosts\nperformance, while still avoiding the need for real intensity images. Our code\nis available at https://w3id.org/synthmorph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_M/0/1/0/all/0/1\">Malte Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Billot_B/0/1/0/all/0/1\">Benjamin Billot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Greve_D/0/1/0/all/0/1\">Douglas N. Greve</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1\">Juan Eugenio Iglesias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fischl_B/0/1/0/all/0/1\">Bruce Fischl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Appearance-free Tripartite Matching for Multiple Object Tracking. (arXiv:2008.03628v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.03628","description":"<p>Multiple Object Tracking (MOT) detects the trajectories of multiple objects\ngiven an input video. It has become more and more important for various\nresearch and industry areas, such as cell tracking for biomedical research and\nhuman tracking in video surveillance. Most existing algorithms depend on the\nuniqueness of the object's appearance, and the dominating bipartite matching\nscheme ignores the speed smoothness. Although several methods have incorporated\nthe velocity smoothness for tracking, they either fail to pursue global smooth\nvelocity or are often trapped in local optimums. We focus on the general MOT\nproblem regardless of the appearance and propose an appearance-free tripartite\nmatching to avoid the irregular velocity problem of the bipartite matching. The\ntripartite matching is formulated as maximizing the likelihood of the state\nvectors constituted of the position and velocity of objects, which results in a\nchain-dependent structure. We resort to the dynamic programming algorithm to\nfind such a maximum likelihood estimate. To overcome the high computational\ncost induced by the vast search space of dynamic programming when many objects\nare to be tracked, we decompose the space by the number of disappearing objects\nand propose a reduced-space approach by truncating the decomposition. Extensive\nsimulations have shown the superiority and efficiency of our proposed method,\nand the comparisons with top methods on Cell Tracking Challenge also\ndemonstrate our competence. We also applied our method to track the motion of\nnatural killer cells around tumor cells in a cancer study.\\footnote{The source\ncode is available on \\url{https://github.com/szcf-weiya/TriMatchMOT}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanting Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jue Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiaodan Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Selective Combinatorial Embedding and Consistency Regularization for Light Field Super-resolution. (arXiv:2009.12537v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2009.12537","description":"<p>Light field (LF) images acquired by hand-held devices usually suffer from low\nspatial resolution as the limited detector resolution has to be shared with the\nangular dimension. LF spatial super-resolution (SR) thus becomes an\nindispensable part of the LF camera processing pipeline. The\nhigh-dimensionality characteristic and complex geometrical structure of LF\nimages make the problem more challenging than traditional single-image SR. The\nperformance of existing methods is still limited as they fail to thoroughly\nexplore the coherence among LF sub-aperture images (SAIs) and are insufficient\nin accurately preserving the scene's parallax structure. To tackle this\nchallenge, we propose a novel learning-based LF spatial SR framework.\nSpecifically, each SAI of an LF image is first coarsely and individually\nsuper-resolved by exploring the complementary information among SAIs with\nselective combinatorial geometry embedding. To achieve efficient and effective\nselection of the complementary information, we propose two novel sub-modules\nconducted hierarchically: the patch selector provides an option of retrieving\nsimilar image patches based on offline disparity estimation to handle\nlarge-disparity correlations; and the SAI selector adaptively and flexibly\nselects the most informative SAIs to improve the embedding efficiency. To\npreserve the parallax structure among the reconstructed SAIs, we subsequently\nappend a consistency regularization network trained over a structure-aware loss\nfunction to refine the parallax relationships over the coarse estimation. In\naddition, we extend the proposed method to irregular LF data. To the best of\nour knowledge, this is the first learning-based SR method for irregular LF\ndata. Experimental results over both synthetic and real-world LF datasets\ndemonstrate the significant advantage of our approach over state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jin_J/0/1/0/all/0/1\">Jing Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhiyu Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation in LiDAR Semantic Segmentation by Aligning Class Distributions. (arXiv:2010.12239v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.12239","description":"<p>LiDAR semantic segmentation provides 3D semantic information about the\nenvironment, an essential cue for intelligent systems during their decision\nmaking processes. Deep neural networks are achieving state-of-the-art results\non large public benchmarks on this task. Unfortunately, finding models that\ngeneralize well or adapt to additional domains, where data distribution is\ndifferent, remains a major challenge. This work addresses the problem of\nunsupervised domain adaptation for LiDAR semantic segmentation models. Our\napproach combines novel ideas on top of the current state-of-the-art approaches\nand yields new state-of-the-art results. We propose simple but effective\nstrategies to reduce the domain shift by aligning the data distribution on the\ninput space. Besides, we propose a learning-based approach that aligns the\ndistribution of the semantic classes of the target domain to the source domain.\nThe presented ablation study shows how each part contributes to the final\nperformance. Our strategy is shown to outperform previous approaches for domain\nadaptation with comparisons run on three different domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alonso_I/0/1/0/all/0/1\">Inigo Alonso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesano_L/0/1/0/all/0/1\">Luis Riazuelo. Luis Montesano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1\">Ana C. Murillo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skeleton-based Relational Reasoning for Group Activity Analysis. (arXiv:2011.05653v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.05653","description":"<p>Research on group activity recognition mostly leans on the standard\ntwo-stream approach (RGB and Optical Flow) as their input features. Few have\nexplored explicit pose information, with none using it directly to reason about\nthe persons interactions. In this paper, we leverage the skeleton information\nto learn the interactions between the individuals straight from it. With our\nproposed method GIRN, multiple relationship types are inferred from independent\nmodules, that describe the relations between the body joints pair-by-pair.\nAdditionally to the joints relations, we also experiment with the previously\nunexplored relationship between individuals and relevant objects (e.g.\nvolleyball). The individuals distinct relations are then merged through an\nattention mechanism, that gives more importance to those individuals more\nrelevant for distinguishing the group activity. We evaluate our method in the\nVolleyball dataset, obtaining competitive results to the state-of-the-art. Our\nexperiments demonstrate the potential of skeleton-based approaches for modeling\nmulti-person interactions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_M/0/1/0/all/0/1\">Mauricio Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1\">Alex C. Kot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Train Neural Networks for Flare Removal. (arXiv:2011.12485v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2011.12485","description":"<p>When a camera is pointed at a strong light source, the resulting photograph\nmay contain lens flare artifacts. Flares appear in a wide variety of patterns\n(halos, streaks, color bleeding, haze, etc.) and this diversity in appearance\nmakes flare removal challenging. Existing analytical solutions make strong\nassumptions about the artifact's geometry or brightness, and therefore only\nwork well on a small subset of flares. Machine learning techniques have shown\nsuccess in removing other types of artifacts, like reflections, but have not\nbeen widely applied to flare removal due to the lack of training data. To solve\nthis problem, we explicitly model the optical causes of flare either\nempirically or using wave optics, and generate semi-synthetic pairs of\nflare-corrupted and clean images. This enables us to train neural networks to\nremove lens flare for the first time. Experiments show our data synthesis\napproach is critical for accurate flare removal, and that models trained with\nour technique generalize well to real lens flares across different scenes,\nlighting conditions, and cameras.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yicheng Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Q/0/1/0/all/0/1\">Qiurui He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_T/0/1/0/all/0/1\">Tianfan Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garg_R/0/1/0/all/0/1\">Rahul Garg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jiawen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veeraraghavan_A/0/1/0/all/0/1\">Ashok Veeraraghavan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of the Collapsing Problem in Semi-Supervised 2D Human Pose Estimation. (arXiv:2011.12498v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.12498","description":"<p>Semi-supervised learning aims to boost the accuracy of a model by exploring\nunlabeled images. The state-of-the-art methods are consistency-based which\nlearn about unlabeled images by encouraging the model to give consistent\npredictions for images under different augmentations. However, when applied to\npose estimation, the methods degenerate and predict every pixel in unlabeled\nimages as background. This is because contradictory predictions are gradually\npushed to the background class due to highly imbalanced class distribution. But\nthis is not an issue in supervised learning because it has accurate labels.\nThis inspires us to stabilize the training by obtaining reliable pseudo labels.\nSpecifically, we learn two networks to mutually teach each other. In\nparticular, for each image, we compose an easy-hard pair by applying different\naugmentations and feed them to both networks. The more reliable predictions on\neasy images in each network are used to teach the other network to learn about\nthe corresponding hard images. The approach successfully avoids degeneration\nand achieves promising results on public datasets. The source code and\npretrained models have been released at\nhttps://github.com/xierc/Semi_Human_Pose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Rongchang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Video Instance Segmentation with Transformers. (arXiv:2011.14503v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.14503","description":"<p>Video instance segmentation (VIS) is the task that requires simultaneously\nclassifying, segmenting and tracking object instances of interest in video.\nRecent methods typically develop sophisticated pipelines to tackle this task.\nHere, we propose a new video instance segmentation framework built upon\nTransformers, termed VisTR, which views the VIS task as a direct end-to-end\nparallel sequence decoding/prediction problem. Given a video clip consisting of\nmultiple image frames as input, VisTR outputs the sequence of masks for each\ninstance in the video in order directly. At the core is a new, effective\ninstance sequence matching and segmentation strategy, which supervises and\nsegments instances at the sequence level as a whole. VisTR frames the instance\nsegmentation and tracking in the same perspective of similarity learning, thus\nconsiderably simplifying the overall pipeline and is significantly different\nfrom existing approaches. Without bells and whistles, VisTR achieves the\nhighest speed among all existing VIS models, and achieves the best result among\nmethods using single model on the YouTube-VIS dataset. For the first time, we\ndemonstrate a much simpler and faster video instance segmentation framework\nbuilt upon Transformers, achieving competitive accuracy. We hope that VisTR can\nmotivate future research for more video understanding tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhaoliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Baoshan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1\">Huaxia Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Spatial Nonstationarity via Deformable Convolutions for Deep Traffic Flow Prediction. (arXiv:2101.12010v2 [physics.soc-ph] UPDATED)","link":"http://arxiv.org/abs/2101.12010","description":"<p>Deep neural networks are being increasingly used for short-term traffic flow\nprediction, which can be generally categorized as convolutional (CNNs) or graph\nneural networks (GNNs). CNNs are preferable for region-wise traffic prediction\nby taking advantage of localized spatial correlations, whilst GNNs achieves\nbetter performance for graph-structured traffic data. When applied to\nregion-wise traffic prediction, CNNs typically partition an underlying\nterritory into grid-like spatial units, and employ standard convolutions to\nlearn spatial dependence among the units. However, standard convolutions with\nfixed geometric structures cannot fully model the nonstationary characteristics\nof local traffic flows. To overcome the deficiency, we introduce deformable\nconvolution that augments the spatial sampling locations with additional\noffsets, to enhance the modeling capability of spatial nonstationarity. On this\nbasis, we design a deep deformable convolutional residual network, namely\nDeFlow-Net, that can effectively model global spatial dependence, local spatial\nnonstationarity, and temporal periodicity of traffic flows. Furthermore, to\nbetter fit with convolutions, we suggest to first aggregate traffic flows\naccording to pre-conceived regions or self-organized regions based on traffic\nflows, then dispose to sequentially organized raster images for network input.\nExtensive experiments on real-world traffic flows demonstrate that DeFlow-Net\noutperforms GNNs and existing CNNs using standard convolutions, and spatial\npartition by pre-conceived regions or self-organized regions further enhances\nthe performance. We also demonstrate the advantage of DeFlow-Net in maintaining\nspatial autocorrelation, and reveal the impacts of partition shapes and scales\non deep traffic flow prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Zeng_W/0/1/0/all/0/1\">Wei Zeng</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lin_C/0/1/0/all/0/1\">Chengqiao Lin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lin_J/0/1/0/all/0/1\">Juncong Lin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tung_A/0/1/0/all/0/1\">Anthony K. H. Tung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Scene Graph Classification by Exploiting Knowledge from Texts. (arXiv:2102.04760v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.04760","description":"<p>Training scene graph classification models requires a large amount of\nannotated image data. Meanwhile, scene graphs represent relational knowledge\nthat can be modeled with symbolic data from texts or knowledge graphs. While\nimage annotation demands extensive labor, collecting textual descriptions of\nnatural scenes requires less effort. In this work, we investigate whether\ntextual scene descriptions can substitute for annotated image data. To this\nend, we employ a scene graph classification framework that is trained not only\nfrom annotated images but also from symbolic data. In our architecture, the\nsymbolic entities are first mapped to their correspondent image-grounded\nrepresentations and then fed into the relational reasoning pipeline. Even\nthough a structured form of knowledge, such as the form in knowledge graphs, is\nnot always available, we can generate it from unstructured texts using a\ntransformer-based language model. We show that by fine-tuning the\nclassification pipeline with the extracted knowledge from texts, we can achieve\n~8x more accurate results in scene graph classification, ~3x in object\nclassification, and ~1.5x in predicate classification, compared to the\nsupervised baselines with only 1% of the annotated images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharifzadeh_S/0/1/0/all/0/1\">Sahand Sharifzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baharlou_S/0/1/0/all/0/1\">Sina Moayed Baharlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1\">Martin Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding self-supervised Learning Dynamics without Contrastive Pairs. (arXiv:2102.06810v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.06810","description":"<p>While contrastive approaches of self-supervised learning (SSL) learn\nrepresentations by minimizing the distance between two augmented views of the\nsame data point (positive pairs) and maximizing views from different data\npoints (negative pairs), recent \\emph{non-contrastive} SSL (e.g., BYOL and\nSimSiam) show remarkable performance {\\it without} negative pairs, with an\nextra learnable predictor and a stop-gradient operation. A fundamental question\narises: why do these methods not collapse into trivial representations? We\nanswer this question via a simple theoretical study and propose a novel\napproach, DirectPred, that \\emph{directly} sets the linear predictor based on\nthe statistics of its inputs, without gradient training. On ImageNet, it\nperforms comparably with more complex two-layer non-linear predictors that\nemploy BatchNorm and outperforms a linear predictor by $2.5\\%$ in 300-epoch\ntraining (and $5\\%$ in 60-epoch). DirectPred is motivated by our theoretical\nstudy of the nonlinear learning dynamics of non-contrastive SSL in simple\nlinear networks. Our study yields conceptual insights into how non-contrastive\nSSL methods learn, how they avoid representational collapse, and how multiple\nfactors, like predictor networks, stop-gradients, exponential moving averages,\nand weight decay all come into play. Our simple theory recapitulates the\nresults of real-world ablation studies in both STL-10 and ImageNet. Code is\nreleased https://github.com/facebookresearch/luckmatters/tree/master/ssl.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinlei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Surya Ganguli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VAE Approximation Error: ELBO and Conditional Independence. (arXiv:2102.09310v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.09310","description":"<p>The importance of Variational Autoencoders reaches far beyond standalone\ngenerative models -- the approach is also used for learning latent\nrepresentations and can be generalized to semi-supervised learning. This\nrequires a thorough analysis of their commonly known shortcomings: posterior\ncollapse and approximation errors. This paper analyzes VAE approximation errors\ncaused by the combination of the ELBO objective with the choice of the encoder\nprobability family, in particular under conditional independence assumptions.\nWe identify the subclass of generative models consistent with the encoder\nfamily. We show that the ELBO optimizer is pulled from the likelihood optimizer\ntowards this consistent subset. Furthermore, this subset can not be enlarged,\nand the respective error cannot be decreased, by only considering deeper\nencoder networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shekhovtsov_A/0/1/0/all/0/1\">Alexander Shekhovtsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlesinger_D/0/1/0/all/0/1\">Dmitrij Schlesinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flach_B/0/1/0/all/0/1\">Boris Flach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VMAF And Variants: Towards A Unified VQA. (arXiv:2103.07770v7 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.07770","description":"<p>Video quality assessment (VQA) is now a fast-growing subject, maturing in the\nfull reference (FR) case, yet challenging in the exploding no reference (NR)\ncase. We investigate variants of the popular VMAF video quality assessment\nalgorithm for the FR case, using both support vector regression and feedforward\nneural networks. We extend it to the NR case, using some different features but\nsimilar learning, to develop a partially unified framework for VQA. When fully\ntrained, FR algorithms such as VMAF perform very well on test datasets,\nreaching 90%+ match in PCC and SRCC; but for predicting performance in the\nwild, we train/test from scratch for each database. With an 80/20 train/test\nsplit, we still achieve about 90% performance on average in both PCC and SRCC,\nwith up to 7-9% gains over VMAF, using an improved motion feature and better\nregression. Moreover, we even get decent performance (about 75%) if we ignore\nthe reference, treating FR as NR, partly justifying our attempts at\nunification. In the true NR case, we reduce complexity vs. leading recent\nalgorithms VIDEVAL, RAPIQUE, yet achieve performance within 3-5%. Moreover, we\ndevelop a method to analyze the saliency of features, and conclude that for\nboth VIDEVAL and RAPIQUE, a small subset of their features are providing the\nbulk of the performance. In short, we find encouraging improvements in\ntrainability in FR, while constraining training complexity against leading\nmethods in NR, elucidating the saliency of features for feature selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Topiwala_P/0/1/0/all/0/1\">Pankaj Topiwala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pian_J/0/1/0/all/0/1\">Jiangfeng Pian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biondi_K/0/1/0/all/0/1\">Katalina Biondi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krovvidi_A/0/1/0/all/0/1\">Arvind Krovvidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unveiling the Power of Mixup for Stronger Classifiers. (arXiv:2103.13027v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.13027","description":"<p>Mixup-based data augmentations have achieved great success as regularizers\nfor deep neural networks. However, existing methods rely on deliberately\nhandcrafted mixup policies, which ignore or oversell the semantic matching\nbetween mixed samples and labels. Driven by their prior assumptions, early\nmethods attempt to smooth decision boundaries by random linear interpolation\nwhile others focus on maximizing class-related information via offline saliency\noptimization. As a result, the issue of label mismatch has not been well\naddressed. Additionally, the optimization stability of mixup training is\nconstantly troubled by the label mismatch. To address these challenges, we\nfirst reformulate mixup for supervised classification as two sub-tasks, mixup\nsample generation and classification, then propose Automatic Mixup (AutoMix), a\nrevolutionary mixup framework. Specifically, a learnable lightweight Mix Block\n(MB) with a cross-attention mechanism is proposed to generate a mixed sample by\nmodeling a fair relationship between the pair of samples under direct\nsupervision of the corresponding mixed label. Moreover, the proposed Momentum\nPipeline (MP) enhances training stability and accelerates convergence on top of\nmaking the Mix Block fully trained end-to-end. Extensive experiments on five\npopular classification benchmarks show that the proposed approach consistently\noutperforms leading methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianzhu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Robustness of Transformers for Image Classification. (arXiv:2103.14586v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14586","description":"<p>Deep Convolutional Neural Networks (CNNs) have long been the architecture of\nchoice for computer vision tasks. Recently, Transformer-based architectures\nlike Vision Transformer (ViT) have matched or even surpassed ResNets for image\nclassification. However, details of the Transformer architecture -- such as the\nuse of non-overlapping patches -- lead one to wonder whether these networks are\nas robust. In this paper, we perform an extensive study of a variety of\ndifferent measures of robustness of ViT models and compare the findings to\nResNet baselines. We investigate robustness to input perturbations as well as\nrobustness to model perturbations. We find that when pre-trained with a\nsufficient amount of data, ViT models are at least as robust as the ResNet\ncounterparts on a broad range of perturbations. We also find that Transformers\nare robust to the removal of almost any single layer, and that while\nactivations from later layers are highly correlated with each other, they\nnevertheless play an important role in classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1\">Srinadh Bhojanapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1\">Ayan Chakrabarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glasner_D/0/1/0/all/0/1\">Daniel Glasner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Daliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1\">Thomas Unterthiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1\">Andreas Veit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning with Memory-based Virtual Classes for Deep Metric Learning. (arXiv:2103.16940v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16940","description":"<p>The core of deep metric learning (DML) involves learning visual similarities\nin high-dimensional embedding space. One of the main challenges is to\ngeneralize from seen classes of training data to unseen classes of test data.\nRecent works have focused on exploiting past embeddings to increase the number\nof instances for the seen classes. Such methods achieve performance improvement\nvia augmentation, while the strong focus on seen classes still remains. This\ncan be undesirable for DML, where training and test data exhibit entirely\ndifferent classes. In this work, we present a novel training strategy for DML\ncalled MemVir. Unlike previous works, MemVir memorizes both embedding features\nand class weights to utilize them as additional virtual classes. The\nexploitation of virtual classes not only utilizes augmented information for\ntraining but also alleviates a strong focus on seen classes for better\ngeneralization. Moreover, we embed the idea of curriculum learning by slowly\nadding virtual classes for a gradual increase in learning difficulty, which\nimproves the learning stability as well as the final performance. MemVir can be\neasily applied to many existing loss functions without any modification.\nExtensive experimental results on famous benchmarks demonstrate the superiority\nof MemVir over state-of-the-art competitors. Code of MemVir is publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ko_B/0/1/0/all/0/1\">Byungsoo Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_G/0/1/0/all/0/1\">Geonmo Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Han-Gyu Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition. (arXiv:2104.03841v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03841","description":"<p>Object recognition has made great advances in the last decade, but\npredominately still relies on many high-quality training examples per object\ncategory. In contrast, learning new objects from only a few examples could\nenable many impactful applications from robotics to user personalization. Most\nfew-shot learning research, however, has been driven by benchmark datasets that\nlack the high variation that these applications will face when deployed in the\nreal-world. To close this gap, we present the ORBIT dataset and benchmark,\ngrounded in the real-world application of teachable object recognizers for\npeople who are blind/low-vision. The dataset contains 3,822 videos of 486\nobjects recorded by people who are blind/low-vision on their mobile phones. The\nbenchmark reflects a realistic, highly challenging recognition problem,\nproviding a rich playground to drive research in robustness to few-shot,\nhigh-variation conditions. We set the benchmark's first state-of-the-art and\nshow there is massive scope for further innovation, holding the potential to\nimpact a broad range of real-world vision applications including tools for the\nblind/low-vision community. We release the dataset at\nhttps://doi.org/10.25383/city.14294597 and benchmark code at\nhttps://github.com/microsoft/ORBIT-Dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Massiceti_D/0/1/0/all/0/1\">Daniela Massiceti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zintgraf_L/0/1/0/all/0/1\">Luisa Zintgraf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bronskill_J/0/1/0/all/0/1\">John Bronskill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theodorou_L/0/1/0/all/0/1\">Lida Theodorou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harris_M/0/1/0/all/0/1\">Matthew Tobias Harris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cutrell_E/0/1/0/all/0/1\">Edward Cutrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morrison_C/0/1/0/all/0/1\">Cecily Morrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stumpf_S/0/1/0/all/0/1\">Simone Stumpf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction. (arXiv:2104.10078v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10078","description":"<p>Neural implicit 3D representations have emerged as a powerful paradigm for\nreconstructing surfaces from multi-view images and synthesizing novel views.\nUnfortunately, existing methods such as DVR or IDR require accurate per-pixel\nobject masks as supervision. At the same time, neural radiance fields have\nrevolutionized novel view synthesis. However, NeRF's estimated volume density\ndoes not admit accurate surface reconstruction. Our key insight is that\nimplicit surface models and radiance fields can be formulated in a unified way,\nenabling both surface and volume rendering using the same model. This unified\nperspective enables novel, more efficient sampling procedures and the ability\nto reconstruct accurate surfaces without input masks. We compare our method on\nthe DTU, BlendedMVS, and a synthetic indoor dataset. Our experiments\ndemonstrate that we outperform NeRF in terms of reconstruction quality while\nperforming on par with IDR without requiring masks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oechsle_M/0/1/0/all/0/1\">Michael Oechsle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Songyou Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less is more: Selecting informative and diverse subsets with balancing constraints. (arXiv:2104.12835v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12835","description":"<p>Deep learning has yielded extraordinary results in vision and natural\nlanguage processing, but this achievement comes at a cost. Most models require\nenormous resources during training, both in terms of computation and in human\nlabeling effort. We show that we can identify informative and diverse subsets\nof data that lead to deep learning models with similar performance as the ones\ntrained with the original dataset. Prior methods have exploited diversity and\nuncertainty in submodular objective functions for choosing subsets. In addition\nto these measures, we show that balancing constraints on predicted class labels\nand decision boundaries are beneficial. We propose a novel formulation of these\nconstraints using matroids, an algebraic structure that generalizes linear\nindependence in vector spaces, and present an efficient greedy algorithm with\nconstant approximation guarantees. We outperform competing baselines on\nstandard classification datasets such as CIFAR-10, CIFAR-100, ImageNet, as well\nas long-tailed datasets such as CIFAR-100-LT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1\">Srikumar Ramalingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glasner_D/0/1/0/all/0/1\">Daniel Glasner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_K/0/1/0/all/0/1\">Kaushal Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemulapalli_R/0/1/0/all/0/1\">Raviteja Vemulapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayasumana_S/0/1/0/all/0/1\">Sadeep Jayasumana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RandCrowns: A Quantitative Metric for Imprecisely Labeled Tree Crown Delineation. (arXiv:2105.02186v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.02186","description":"<p>Supervised methods for object delineation in remote sensing require labeled\nground-truth data. Gathering sufficient high quality ground-truth data is\ndifficult, especially when targets are of irregular shape or difficult to\ndistinguish from background or neighboring objects. Tree crown delineation\nprovides key information from remote sensing images for forestry, ecology, and\nmanagement. However, tree crowns in remote sensing imagery are often difficult\nto label and annotate due to irregular shape, overlapping canopies, shadowing,\nand indistinct edges. There are also multiple approaches to annotation in this\nfield (e.g., rectangular boxes vs. convex polygons) that further contribute to\nannotation imprecision. However, current evaluation methods do not account for\nthis uncertainty in annotations, and quantitative metrics for evaluation can\nvary across multiple annotators. In this paper, we address these limitations by\ndeveloping an adaptation of the Rand index for weakly-labeled crown delineation\nthat we call RandCrowns. Our new RandCrowns evaluation metric provides a method\nto appropriately evaluate delineated tree crowns while taking into account\nimprecision in the ground-truth delineations. The RandCrowns metric\nreformulates the Rand index by adjusting the areas over which each term of the\nindex is computed to account for uncertain and imprecise object delineation\nlabels. Quantitative comparisons to the commonly used intersection over union\nmethod shows a decrease in the variance generated by differences among multiple\nannotators. Combined with qualitative examples, our results suggest that the\nRandCrowns metric is more robust for scoring target delineations in the\npresence of uncertainty and imprecision in annotations that are inherent to\ntree crown delineation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stewart_D/0/1/0/all/0/1\">Dylan Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zare_A/0/1/0/all/0/1\">Alina Zare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marconi_S/0/1/0/all/0/1\">Sergio Marconi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinstein_B/0/1/0/all/0/1\">Ben Weinstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_E/0/1/0/all/0/1\">Ethan White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graves_S/0/1/0/all/0/1\">Sarah Graves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohlman_S/0/1/0/all/0/1\">Stephanie Bohlman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aditya Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is aspect ratio of cells important in deep learning? A robust comparison of deep learning methods for multi-scale cytopathology cell image classification: from convolutional neural networks to visual transformers. (arXiv:2105.07402v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.07402","description":"<p>Cervical cancer is a very common and fatal cancer in women. Cytopathology\nimages are often used to screen this cancer. Since there is a possibility of a\nlarge number of errors in manual screening, the computer-aided diagnosis system\nbased on deep learning is developed. The deep learning methods required a fixed\nsize of input images, but the sizes of the clinical medical images are\ninconsistent. The aspect ratios of the images are suffered while resizing it\ndirectly. Clinically, the aspect ratios of cells inside cytopathological images\nprovide important information for doctors to diagnose cancer. Therefore, it is\nillogical to resize directly. However, many existing studies resized the images\ndirectly and obtained very robust classification results. To find a reasonable\ninterpretation, we have conducted a series of comparative experiments. First,\nthe raw data of the SIPaKMeD dataset are preprocessed to obtain the standard\nand scaled datasets. Then, the datasets are resized to 224 x 224 pixels.\nFinally, twenty-two deep learning models are used to classify standard and\nscaled datasets. The conclusion is that the deep learning models are robust to\nchanges in the aspect ratio of cells in cervical cytopathological images. This\nconclusion is also validated on the Herlev dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wanli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahamana_M/0/1/0/all/0/1\">Md Mamunur Rahamana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changhao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Remote Sensing Images Change Detection at Pixel-level. (arXiv:2105.08501v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.08501","description":"<p>Deep learning techniques have achieved great success in remote sensing image\nchange detection. Most of them are supervised techniques, which usually require\nlarge amounts of training data and are limited to a particular application.\nSelf-supervised methods as an unsupervised approach are popularly used to solve\nthis problem and are widely used in unsupervised binary change detection tasks.\nHowever, the existing self-supervised methods in change detection are based on\npre-tasks or at patch-level, which may be sub-optimal for pixel-wise change\ndetection tasks. Therefore, in this work, a pixel-wise contrastive approach is\nproposed to overcome this limitation. This is achieved by using contrastive\nloss in pixel-level features on an unlabeled multi-view setting. In this\napproach, a Siamese ResUnet is trained to obtain pixel-wise representations and\nto align features from shifted positive pairs. Meanwhile, vector quantization\nis used to augment the learned features in two branches. The final binary\nchange map is obtained by subtracting features of one branch from features of\nthe other branch and using the Rosin thresholding method. To overcome the\neffects of regular seasonal changes in binary change maps, we also used an\nuncertainty method to enhance the temporal robustness of the proposed approach.\nTwo homogeneous (OSCD and MUDS) datasets and one heterogeneous (California\nFlood) dataset are used to evaluate the performance of the proposed approach.\nResults demonstrate improvements in both efficiency and accuracy over the\npatch-wise multi-view contrastive method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxing Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bruzzone_L/0/1/0/all/0/1\">Lorenzo Bruzzone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VOILA: Visual-Observation-Only Imitation Learning for Autonomous Navigation. (arXiv:2105.09371v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2105.09371","description":"<p>While imitation learning for vision based autonomous mobile robot navigation\nhas recently received a great deal of attention in the research community,\nexisting approaches typically require state action demonstrations that were\ngathered using the deployment platform. However, what if one cannot easily\noutfit their platform to record these demonstration signals or worse yet the\ndemonstrator does not have access to the platform at all? Is imitation learning\nfor vision based autonomous navigation even possible in such scenarios? In this\nwork, we hypothesize that the answer is yes and that recent ideas from the\nImitation from Observation (IfO) literature can be brought to bear such that a\nrobot can learn to navigate using only ego centric video collected by a\ndemonstrator, even in the presence of viewpoint mismatch. To this end, we\nintroduce a new algorithm, Visual Observation only Imitation Learning for\nAutonomous navigation (VOILA), that can successfully learn navigation policies\nfrom a single video demonstration collected from a physically different agent.\nWe evaluate VOILA in the photorealistic AirSim simulator and show that VOILA\nnot only successfully imitates the expert, but that it also learns navigation\npolicies that can generalize to novel environments. Further, we demonstrate the\neffectiveness of VOILA in a real world setting by showing that it allows a\nwheeled Jackal robot to successfully imitate a human walking in an environment\nusing a video recorded using a mobile phone camera.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karnan_H/0/1/0/all/0/1\">Haresh Karnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1\">Garrett Warnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuesu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1\">Peter Stone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Unsupervised Document Image Blind Denoising. (arXiv:2105.09437v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.09437","description":"<p>Removing noise from scanned pages is a vital step before their submission to\nthe optical character recognition (OCR) system. Most available image denoising\nmethods are supervised where the pairs of noisy/clean pages are required.\nHowever, this assumption is rarely met in real settings. Besides, there is no\nsingle model that can remove various noise types from documents. Here, we\npropose a unified end-to-end unsupervised deep learning model, for the first\ntime, that can effectively remove multiple types of noise, including salt \\&amp;\npepper noise, blurred and/or faded text, as well as watermarks from documents\nat various levels of intensity. We demonstrate that the proposed model\nsignificantly improves the quality of scanned images and the OCR of the pages\non several test datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gangeh_M/0/1/0/all/0/1\">Mehrdad J Gangeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plata_M/0/1/0/all/0/1\">Marcin Plata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motahari_H/0/1/0/all/0/1\">Hamid Motahari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duffy_N/0/1/0/all/0/1\">Nigel P Duffy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation mitosis in wide neural networks. (arXiv:2106.03485v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2106.03485","description":"<p>Deep neural networks (DNNs) defy the classical bias-variance trade-off:\nadding parameters to a DNN that interpolates its training data will typically\nimprove its generalization performance. Explaining the mechanism behind this\n``benign overfitting'' in deep networks remains an outstanding challenge. Here,\nwe study the last hidden layer representations of various state-of-the-art\nconvolutional neural networks and find evidence for an underlying mechanism\nthat we call \"representation mitosis\": if the last hidden representation is\nwide enough, its neurons tend to split into groups which carry identical\ninformation, and differ from each other only by a statistically independent\nnoise. Like in a mitosis process, the number of such groups, or ``clones'',\nincreases linearly with the width of the layer, but only if the width is above\na critical value. We show that a key ingredient to activate mitosis is\ncontinuing the training process until the training error is zero.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Doimo_D/0/1/0/all/0/1\">Diego Doimo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Glielmo_A/0/1/0/all/0/1\">Aldo Glielmo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Goldt_S/0/1/0/all/0/1\">Sebastian Goldt</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Laio_A/0/1/0/all/0/1\">Alessandro Laio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FairCal: Fairness Calibration for Face Verification. (arXiv:2106.03761v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03761","description":"<p>Despite being widely used, face recognition models suffer from bias: the\nprobability of a false positive (incorrect face match) strongly depends on\nsensitive attributes such as the ethnicity of the face. As a result, these\nmodels can disproportionately and negatively impact minority groups,\nparticularly when used by law enforcement. The majority of bias reduction\nmethods have several drawbacks: they use an end-to-end retraining approach, may\nnot be feasible due to privacy issues, and often reduce accuracy. An\nalternative approach is post-processing methods that build fairer decision\nclassifiers using the features of pre-trained models. However, they still have\ndrawbacks: they reduce accuracy (AGENDA, FTC), or require retuning for\ndifferent false positive rates (FSN). In this work, we introduce the Fairness\nCalibration (FairCal) method, a post-training approach that: (i) increases\nmodel accuracy (improving the state-of-the-art), (ii) produces\nfairly-calibrated probabilities, (iii) significantly reduces the gap in the\nfalse positive rates, (iv) does not require knowledge of the sensitive\nattribute, and (v) does not require retraining, training an additional model,\nor retuning. We apply it to the task of Face Verification, and obtain\nstate-of-the-art results with all the above advantages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salvador_T/0/1/0/all/0/1\">Tiago Salvador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cairns_S/0/1/0/all/0/1\">Stephanie Cairns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1\">Vikram Voleti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_N/0/1/0/all/0/1\">Noah Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberman_A/0/1/0/all/0/1\">Adam Oberman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Space-Time Networks with Improved Memory Coverage for Efficient Video Object Segmentation. (arXiv:2106.05210v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05210","description":"<p>This paper presents a simple yet effective approach to modeling space-time\ncorrespondences in the context of video object segmentation. Unlike most\nexisting approaches, we establish correspondences directly between frames\nwithout re-encoding the mask features for every object, leading to a highly\nefficient and robust framework. With the correspondences, every node in the\ncurrent query frame is inferred by aggregating features from the past in an\nassociative fashion. We cast the aggregation process as a voting problem and\nfind that the existing inner-product affinity leads to poor use of memory with\na small (fixed) subset of memory nodes dominating the votes, regardless of the\nquery. In light of this phenomenon, we propose using the negative squared\nEuclidean distance instead to compute the affinities. We validated that every\nmemory node now has a chance to contribute, and experimentally showed that such\ndiversified voting is beneficial to both memory efficiency and inference\naccuracy. The synergy of correspondence networks and diversified voting works\nexceedingly well, achieves new state-of-the-art results on both DAVIS and\nYouTubeVOS datasets while running significantly faster at 20+ FPS for multiple\nobjects without bells and whistles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Ho Kei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yu-Wing Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chi-Keung Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Looking Outside the Window: Wide-Context Transformer for the Semantic Segmentation of High-Resolution Remote Sensing Images. (arXiv:2106.15754v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15754","description":"<p>Long-range contextual information is crucial for the semantic segmentation of\nHigh-Resolution (HR) Remote Sensing Images (RSIs). However, image cropping\noperations, commonly used for training neural networks, limit the perception of\nlong-range contexts in large RSIs. To overcome this limitation, we propose a\nWide-Context Network (WiCoNet) for the semantic segmentation of HR RSIs. Apart\nfrom extracting local features with a conventional CNN, the WiCoNet has an\nextra context branch to aggregate information from a larger image area.\nMoreover, we introduce a Context Transformer to embed contextual information\nfrom the context branch and selectively project it onto the local features. The\nContext Transformer extends the Vision Transformer, an emerging kind of neural\nnetworks, to model the dual-branch semantic correlations. It overcomes the\nlocality limitation of CNNs and enables the WiCoNet to see the bigger picture\nbefore segmenting the land-cover/land-use (LCLU) classes. Ablation studies and\ncomparative experiments conducted on several benchmark datasets demonstrate the\neffectiveness of the proposed method. In addition, we present a new Beijing\nLand-Use (BLU) dataset. This is a large-scale HR satellite dataset with\nhigh-quality and fine-grained reference labels, which can facilitate future\nstudies in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Lei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shaofu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiaojie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuebin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruzzone_L/0/1/0/all/0/1\">Lorenzo Bruzzone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Align Yourself: Self-supervised Pre-training for Fine-grained Recognition via Saliency Alignment. (arXiv:2106.15788v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15788","description":"<p>Self-supervised contrastive learning has demonstrated great potential in\nlearning visual representations. Despite their success on various downstream\ntasks such as image classification and object detection, self-supervised\npre-training for fine-grained scenarios is not fully explored. In this paper,\nwe first point out that current contrastive methods are prone to memorizing\nbackground/foreground texture and therefore have a limitation in localizing the\nforeground object. Analysis suggests that learning to extract discriminative\ntexture information and localization are equally crucial for self-supervised\npre-training in fine-grained scenarios. Based on our findings, we introduce\ncross-view saliency alignment (CVSA), a contrastive learning framework that\nfirst crops and swaps saliency regions of images as a novel view generation and\nthen guides the model to localize on the foreground object via a cross-view\nalignment loss. Extensive experiments on four popular fine-grained\nclassification benchmarks show that CVSA significantly improves the learned\nrepresentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1\">Zelin Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lei Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baigui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Source-Free Adaptation to Measurement Shift via Bottom-Up Feature Restoration. (arXiv:2107.05446v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.05446","description":"<p>Source-free domain adaptation (SFDA) aims to adapt a model trained on\nlabelled data in a source domain to unlabelled data in a target domain without\naccess to the source-domain data during adaptation. Existing methods for SFDA\nleverage entropy-minimization techniques which: (i) apply only to\nclassification; (ii) destroy model calibration; and (iii) rely on the source\nmodel achieving a good level of feature-space class-separation in the target\ndomain. We address these issues for a particularly pervasive type of domain\nshift called measurement shift -- characterized by a change in measurement\nsystem -- which can be resolved by restoring the source features. In the source\ndomain, we store a lightweight and flexible approximation of the feature\ndistribution under the source data. In the target domain, we adapt the\nfeature-extractor such that the approximate feature distribution under the\ntarget data realigns with that saved on the source. We call this method Feature\nRestoration (FR) as it seeks to extract features with the same semantics from\nthe target domain as were previously extracted from the source, rather than\nextracting new ones. We additionally propose Bottom-Up Feature Restoration\n(BUFR) -- a bottom-up training scheme for FR which boosts performance by\npreserving learnt structure in the later layers of a network. We demonstrate\nthat BUFR outperforms existing SFDA methods on real and synthetic data in terms\nof accuracy, calibration, and data efficiency, while being less reliant on the\nperformance of the source model in the target domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eastwood_C/0/1/0/all/0/1\">Cian Eastwood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mason_I/0/1/0/all/0/1\">Ian Mason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_C/0/1/0/all/0/1\">Christopher K. I. Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention. (arXiv:2108.00154v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00154","description":"<p>Transformers have made great progress in dealing with computer vision tasks.\nHowever, existing vision transformers do not yet possess the ability of\nbuilding the interactions among features of different scales, which is\nperceptually important to visual inputs. The reasons are two-fold: (1) Input\nembeddings of each layer are equal-scale, so no cross-scale feature can be\nextracted; (2) to lower the computational cost, some vision transformers merge\nadjacent embeddings inside the self-attention module, thus sacrificing\nsmall-scale (fine-grained) features of the embeddings and also disabling the\ncross-scale interactions. To this end, we propose Cross-scale Embedding Layer\n(CEL) and Long Short Distance Attention (LSDA). On the one hand, CEL blends\neach embedding with multiple patches of different scales, providing the\nself-attention module itself with cross-scale features. On the other hand, LSDA\nsplits the self-attention module into a short-distance one and a long-distance\ncounterpart, which not only reduces the computational burden but also keeps\nboth small-scale and large-scale features in the embeddings. Through the above\ntwo designs, we achieve cross-scale attention. Besides, we put forward a\ndynamic position bias for vision transformers to make the popular relative\nposition bias apply to variable-sized images. Hinging on the cross-scale\nattention module, we construct a versatile vision architecture, dubbed\nCrossFormer, which accommodates variable-sized inputs. Extensive experiments\nshow that CrossFormer outperforms the other vision transformers on image\nclassification, object detection, instance segmentation, and semantic\nsegmentation tasks. The code has been released:\nhttps://github.com/cheerss/CrossFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Binbin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Source-free Domain Adaptation. (arXiv:2108.01614v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01614","description":"<p>Domain adaptation (DA) aims to transfer the knowledge learned from a source\ndomain to an unlabeled target domain. Some recent works tackle source-free\ndomain adaptation (SFDA) where only a source pre-trained model is available for\nadaptation to the target domain. However, those methods do not consider keeping\nsource performance which is of high practical value in real world applications.\nIn this paper, we propose a new domain adaptation paradigm called Generalized\nSource-free Domain Adaptation (G-SFDA), where the learned model needs to\nperform well on both the target and source domains, with only access to current\nunlabeled target data during adaptation. First, we propose local structure\nclustering (LSC), aiming to cluster the target features with its semantically\nsimilar neighbors, which successfully adapts the model to the target domain in\nthe absence of source data. Second, we propose sparse domain attention (SDA),\nit produces a binary domain specific attention to activate different feature\nchannels for different domains, meanwhile the domain attention will be utilized\nto regularize the gradient during adaptation to keep source information. In the\nexperiments, for target performance our method is on par with or better than\nexisting DA and SFDA methods, specifically it achieves state-of-the-art\nperformance (85.4%) on VisDA, and our method works well for all domains after\nadapting to single or multiple target domains. Code is available in\nhttps://github.com/Albert0147/G-SFDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shiqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaxing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herranz_L/0/1/0/all/0/1\">Luis Herranz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1\">Shangling Jui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solo-learn: A Library of Self-supervised Methods for Visual Representation Learning. (arXiv:2108.01775v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01775","description":"<p>This paper presents solo-learn, a library of self-supervised methods for\nvisual representation learning. Implemented in Python, using Pytorch and\nPytorch lightning, the library fits both research and industry needs by\nfeaturing distributed training pipelines with mixed-precision, faster data\nloading via Nvidia DALI, online linear evaluation for better prototyping, and\nmany additional training tricks. Our goal is to provide an easy-to-use library\ncomprising a large amount of Self-supervised Learning (SSL) methods, that can\nbe easily extended and fine-tuned by the community. solo-learn opens up avenues\nfor exploiting large-budget SSL solutions on inexpensive smaller\ninfrastructures and seeks to democratize SSL by making it accessible to all.\nThe source code is available at https://github.com/vturrisi/solo-learn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Costa_V/0/1/0/all/0/1\">Victor G. Turrisi da Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1\">Enrico Fini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Embodied Vision Navigation: A Survey. (arXiv:2108.04097v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2108.04097","description":"<p>\"Embodied visual navigation\" problem requires an agent to navigate in a 3D\nenvironment mainly rely on its first-person observation. This problem has\nattracted rising attention in recent years due to its wide application in\nautonomous driving, vacuum cleaner, and rescue robot. A navigation agent is\nsupposed to have various intelligent skills, such as visual perceiving,\nmapping, planning, exploring and reasoning, etc. Building such an agent that\nobserves, thinks, and acts is a key to real intelligence. The remarkable\nlearning ability of deep learning methods empowered the agents to accomplish\nembodied visual navigation tasks. Despite this, embodied visual navigation is\nstill in its infancy since a lot of advanced skills are required, including\nperceiving partially observed visual input, exploring unseen areas, memorizing\nand modeling seen scenarios, understanding cross-modal instructions, and\nadapting to a new environment, etc. Recently, embodied visual navigation has\nattracted rising attention of the community, and numerous works has been\nproposed to learn these skills. This paper attempts to establish an outline of\nthe current works in the field of embodied visual navigation by providing a\ncomprehensive literature survey. We summarize the benchmarks and metrics,\nreview different methods, analysis the challenges, and highlight the\nstate-of-the-art methods. Finally, we discuss unresolved challenges in the\nfield of embodied visual navigation and give promising directions in pursuing\nfuture research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengda Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_V/0/1/0/all/0/1\">Vincent CS Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-domain Collaborative Feature Representation for Robust Visual Object Tracking. (arXiv:2108.04521v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04521","description":"<p>Jointly exploiting multiple different yet complementary domain information\nhas been proven to be an effective way to perform robust object tracking. This\npaper focuses on effectively representing and utilizing complementary features\nfrom the frame domain and event domain for boosting object tracking performance\nin challenge scenarios. Specifically, we propose Common Features Extractor\n(CFE) to learn potential common representations from the RGB domain and event\ndomain. For learning the unique features of the two domains, we utilize a\nUnique Extractor for Event (UEE) based on Spiking Neural Networks to extract\nedge cues in the event domain which may be missed in RGB in some challenging\nconditions, and a Unique Extractor for RGB (UER) based on Deep Convolutional\nNeural Networks to extract texture and semantic information in RGB domain.\nExtensive experiments on standard RGB benchmark and real event tracking dataset\ndemonstrate the effectiveness of the proposed approach. We show our approach\noutperforms all compared state-of-the-art tracking algorithms and verify\nevent-based data is a powerful cue for tracking in challenging scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingkai Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Baocai Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiagViB-6: A Diagnostic Benchmark Suite for Vision Models in the Presence of Shortcut and Generalization Opportunities. (arXiv:2108.05779v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05779","description":"<p>Common deep neural networks (DNNs) for image classification have been shown\nto rely on shortcut opportunities (SO) in the form of predictive and\neasy-to-represent visual factors. This is known as shortcut learning and leads\nto impaired generalization. In this work, we show that common DNNs also suffer\nfrom shortcut learning when predicting only basic visual object factors of\nvariation (FoV) such as shape, color, or texture. We argue that besides\nshortcut opportunities, generalization opportunities (GO) are also an inherent\npart of real-world vision data and arise from partial independence between\npredicted classes and FoVs. We also argue that it is necessary for DNNs to\nexploit GO to overcome shortcut learning. Our core contribution is to introduce\nthe Diagnostic Vision Benchmark suite DiagViB-6, which includes datasets and\nmetrics to study a network's shortcut vulnerability and generalization\ncapability for six independent FoV. In particular, DiagViB-6 allows controlling\nthe type and degree of SO and GO in a dataset. We benchmark a wide range of\npopular vision architectures and show that they can exploit GO only to a\nlimited extent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eulig_E/0/1/0/all/0/1\">Elias Eulig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saranrittichai_P/0/1/0/all/0/1\">Piyapat Saranrittichai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mummadi_C/0/1/0/all/0/1\">Chaithanya Kumar Mummadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambach_K/0/1/0/all/0/1\">Kilian Rambach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beluch_W/0/1/0/all/0/1\">William Beluch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiahan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_V/0/1/0/all/0/1\">Volker Fischer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAMA: A Rapid Multicut Algorithm on GPU. (arXiv:2109.01838v2 [cs.DC] UPDATED)","link":"http://arxiv.org/abs/2109.01838","description":"<p>We propose a highly parallel primal-dual algorithm for the multicut (a.k.a.\ncorrelation clustering) problem, a classical graph clustering problem widely\nused in machine learning and computer vision. Our algorithm consists of three\nsteps executed recursively: (1) Finding conflicted cycles that correspond to\nviolated inequalities of the underlying multicut relaxation, (2) Performing\nmessage passing between the edges and cycles to optimize the Lagrange\nrelaxation coming from the found violated cycles producing reduced costs and\n(3) Contracting edges with high reduced costs through matrix-matrix\nmultiplications.\n</p>\n<p>Our algorithm produces primal solutions and dual lower bounds that estimate\nthe distance to optimum. We implement our algorithm on GPUs and show resulting\none to two order-of-magnitudes improvements in execution speed without\nsacrificing solution quality compared to traditional serial algorithms that run\non CPUs. We can solve very large scale benchmark problems with up to\n$\\mathcal{O}(10^8)$ variables in a few seconds with small primal-dual gaps. We\nmake our code available at https://github.com/pawelswoboda/RAMA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abbas_A/0/1/0/all/0/1\">Ahmed Abbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swoboda_P/0/1/0/all/0/1\">Paul Swoboda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identification of Driver Phone Usage Violations via State-of-the-Art Object Detection with Tracking. (arXiv:2109.02119v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02119","description":"<p>The use of mobiles phones when driving have been a major factor when it comes\nto road traffic incidents and the process of capturing such violations can be a\nlaborious task. Advancements in both modern object detection frameworks and\nhigh-performance hardware has paved the way for a more automated approach when\nit comes to video surveillance. In this work, we propose a custom-trained\nstate-of-the-art object detector to work with roadside cameras to capture\ndriver phone usage without the need for human intervention. The proposed\napproach also addresses the issues caused by windscreen glare and introduces\nthe steps required to remedy this. Twelve pre-trained models are fine-tuned\nwith our custom dataset using four popular object detection methods: YOLO, SSD,\nFaster R-CNN, and CenterNet. Out of all the object detectors tested, the YOLO\nyields the highest accuracy levels of up to 96% (AP10) and frame rates of up to\n~30 FPS. DeepSort object tracking algorithm is also integrated into the\nbest-performing model to collect records of only the unique violations, and\nenable the proposed approach to count the number of vehicles. The proposed\nautomated system will collect the output images of the identified violations,\ntimestamps of each violation, and total vehicle count. Data can be accessed via\na purpose-built user interface.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carrell_S/0/1/0/all/0/1\">Steven Carrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atapour_Abarghouei_A/0/1/0/all/0/1\">Amir Atapour-Abarghouei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation. (arXiv:2109.06165v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06165","description":"<p>Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from\na labeled source domain to a different unlabeled target domain. Most existing\nUDA methods focus on learning domain-invariant feature representation, either\nfrom the domain level or category level, using convolution neural networks\n(CNNs)-based frameworks. One fundamental problem for the category level based\nUDA is the production of pseudo labels for samples in target domain, which are\nusually too noisy for accurate domain alignment, inevitably compromising the\nUDA performance. With the success of Transformer in various tasks, we find that\nthe cross-attention in Transformer is robust to the noisy input pairs for\nbetter feature alignment, thus in this paper Transformer is adopted for the\nchallenging UDA task. Specifically, to generate accurate input pairs, we design\na two-way center-aware labeling algorithm to produce pseudo labels for target\nsamples. Along with the pseudo labels, a weight-sharing triple-branch\ntransformer framework is proposed to apply self-attention and cross-attention\nfor source/target feature learning and source-target domain alignment,\nrespectively. Such design explicitly enforces the framework to learn\ndiscriminative domain-specific and domain-invariant representations\nsimultaneously. The proposed method is dubbed CDTrans (cross-domain\ntransformer), and it provides one of the first attempts to solve UDA tasks with\na pure transformer solution. Extensive experiments show that our proposed\nmethod achieves the best performance on Office-Home, VisDA-2017, and DomainNet\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tongkun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto White-Balance Correction for Mixed-Illuminant Scenes. (arXiv:2109.08750v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.08750","description":"<p>Auto white balance (AWB) is applied by camera hardware at capture time to\nremove the color cast caused by the scene illumination. The vast majority of\nwhite-balance algorithms assume a single light source illuminates the scene;\nhowever, real scenes often have mixed lighting conditions. This paper presents\nan effective AWB method to deal with such mixed-illuminant scenes. A unique\ndeparture from conventional AWB, our method does not require illuminant\nestimation, as is the case in traditional camera AWB modules. Instead, our\nmethod proposes to render the captured scene with a small set of predefined\nwhite-balance settings. Given this set of rendered images, our method learns to\nestimate weighting maps that are used to blend the rendered images to generate\nthe final corrected image. Through extensive experiments, we show this proposed\nmethod produces promising results compared to other alternatives for single-\nand mixed-illuminant scene color correction. Our source code and trained models\nare available at https://github.com/mahmoudnafifi/mixedillWB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1\">Michael S. Brown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Well Googled is Half Done: Multimodal Forecasting of New Fashion Product Sales with Image-based Google Trends. (arXiv:2109.09824v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09824","description":"<p>This paper investigates the effectiveness of systematically probing Google\nTrendsagainst textual translations of visual aspects as exogenous knowledge to\npredict the sales of brand-new fashion items, where past sales data is not\navailable, but only an image and few metadata are available. In particular, we\npropose GTM-Transformer, standing for Google Trends Multimodal Transformer,\nwhose encoder works on the representation of the exogenous time series, while\nthe decoder forecasts the sales using the Google Trends encoding, and the\navailable visual and metadata information. Our model works in a\nnon-autoregressive manner, avoiding the compounding effect of the first-step\nerrors. As a second contribution, we present the VISUELLE dataset, which is the\nfirst publicly available dataset for the task of new fashion product sales\nforecasting, containing the sales of 5577 new products sold between 2016-2019,\nderived from genuine historical data ofNunalie, an Italian fast-fashion\ncompany. Our dataset is equipped with images of products, metadata, related\nsales, and associated Google Trends. We use VISUELLE to compare our approach\nagainst state-of-the-art alternatives and numerous baselines, showing that\nGTM-Transformer is the most accurate in terms of both percentage and absolute\nerror. It is worth noting that the addition of exogenous knowledge boosts the\nforecasting accuracy by 1.5% WAPE wise, showing the importance of exploiting\nGoogle Trends. The code and dataset are both available at\nhttps://github.com/HumaticsLAB/GTM-Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Skenderi_G/0/1/0/all/0/1\">Geri Skenderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joppi_C/0/1/0/all/0/1\">Christian Joppi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denitto_M/0/1/0/all/0/1\">Matteo Denitto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1\">Marco Cristani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DVC-P: Deep Video Compression with Perceptual Optimizations. (arXiv:2109.10849v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.10849","description":"<p>Recent years have witnessed the significant development of learning-based\nvideo compression methods, which aim at optimizing objective or perceptual\nquality and bit rates. In this paper, we introduce deep video compression with\nperceptual optimizations (DVC-P), which aims at increasing perceptual quality\nof decoded videos. Our proposed DVC-P is based on Deep Video Compression (DVC)\nnetwork, but improves it with perceptual optimizations. Specifically, a\ndiscriminator network and a mixed loss are employed to help our network trade\noff among distortion, perception and rate. Furthermore, nearest-neighbor\ninterpolation is used to eliminate checkerboard artifacts which can appear in\nsequences encoded with DVC frameworks. Thanks to these two improvements, the\nperceptual quality of decoded sequences is improved. Experimental results\ndemonstrate that, compared with the baseline DVC, our proposed method can\ngenerate videos with higher perceptual quality achieving 12.27% reduction in a\nperceptual BD-rate equivalent, on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Saiping Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1\">Marta Mrak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herranz_L/0/1/0/all/0/1\">Luis Herranz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gorriz_M/0/1/0/all/0/1\">Marc G&#xf3;rriz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_S/0/1/0/all/0/1\">Shuai Wan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_F/0/1/0/all/0/1\">Fuzheng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models. (arXiv:2109.11797v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.11797","description":"<p>Pre-Trained Vision-Language Models (VL-PTMs) have shown promising\ncapabilities in grounding natural language in image data, facilitating a broad\nvariety of cross-modal tasks. However, we note that there exists a significant\ngap between the objective forms of model pre-training and fine-tuning,\nresulting in a need for large amounts of labeled data to stimulate the visual\ngrounding capability of VL-PTMs for downstream tasks. To address the challenge,\nwe present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt\nTuning), a novel paradigm for tuning VL-PTMs, which reformulates visual\ngrounding into a fill-in-the-blank problem with color-based co-referential\nmarkers in image and text, maximally mitigating the gap. In this way, CPT\nenables strong few-shot and even zero-shot visual grounding capabilities of\nVL-PTMs. Comprehensive experimental results show that the prompt-tuned VL-PTMs\noutperform their fine-tuned counterparts by a large margin (e.g., 17.3%\nabsolute accuracy improvement, and 73.8% relative standard deviation reduction\non average with one shot in RefCOCO evaluation). All the data and codes will be\navailable to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HSVA: Hierarchical Semantic-Visual Adaptation for Zero-Shot Learning. (arXiv:2109.15163v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.15163","description":"<p>Zero-shot learning (ZSL) tackles the unseen class recognition problem,\ntransferring semantic knowledge from seen classes to unseen ones. Typically, to\nguarantee desirable knowledge transfer, a common (latent) space is adopted for\nassociating the visual and semantic domains in ZSL. However, existing common\nspace learning methods align the semantic and visual domains by merely\nmitigating distribution disagreement through one-step adaptation. This strategy\nis usually ineffective due to the heterogeneous nature of the feature\nrepresentations in the two domains, which intrinsically contain both\ndistribution and structure variations. To address this and advance ZSL, we\npropose a novel hierarchical semantic-visual adaptation (HSVA) framework.\nSpecifically, HSVA aligns the semantic and visual domains by adopting a\nhierarchical two-step adaptation, i.e., structure adaptation and distribution\nadaptation. In the structure adaptation step, we take two task-specific\nencoders to encode the source data (visual domain) and the target data\n(semantic domain) into a structure-aligned common space. To this end, a\nsupervised adversarial discrepancy (SAD) module is proposed to adversarially\nminimize the discrepancy between the predictions of two task-specific\nclassifiers, thus making the visual and semantic feature manifolds more closely\naligned. In the distribution adaptation step, we directly minimize the\nWasserstein distance between the latent multivariate Gaussian distributions to\nalign the visual and semantic distributions using a common encoder. Finally,\nthe structure and distribution adaptation are derived in a unified framework\nunder two partially-aligned variational autoencoders. Extensive experiments on\nfour benchmark datasets demonstrate that HSVA achieves superior performance on\nboth conventional and generalized ZSL. The code is available at\n\\url{https://github.com/shiming-chen/HSVA} .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guo-Sen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qinmu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baigui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinge You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Efficient Instance Segmentation with a Single GPU. (arXiv:2110.00242v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00242","description":"<p>Not everyone is wealthy enough to have hundreds of GPUs or TPUs. Therefore,\nwe've got to find a way out. In this paper, we introduce a data-efficient\ninstance segmentation method we used in the 2021 VIPriors Instance Segmentation\nChallenge. Our solution is a modified version of Swin Transformer, based on the\nmmdetection which is a powerful toolbox. To solve the problem of lack of data,\nwe utilize data augmentation including random flip and multiscale training to\ntrain our model. During inference, multiscale fusion is used to boost the\nperformance. We only use a single GPU during the whole training and testing\nstages. In the end, our team named THU_IVG_2018 achieved the result of 0.366\nfor AP@0.50:0.95 on the test set, which is competitive with other top-ranking\nmethods while only one GPU is used. Besides, our method achieved the\nAP@0.50:0.95 (medium) of 0.592, which ranks second among all contestants. In\nthe end, our team ranked third among all the contestants, as announced by the\norganizers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pengyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wanhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Transfer Learning for Land Use Land Cover Classification: A Comparative Study. (arXiv:2110.02580v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02580","description":"<p>Efficiently implementing remote sensing image classification with high\nspatial resolution imagery can provide great significant value in land-use\nland-cover classification (LULC). The developments in remote sensing and deep\nlearning technologies have facilitated the extraction of spatiotemporal\ninformation for LULC classification. Moreover, the diverse disciplines of\nscience, including remote sensing, have utilised tremendous improvements in\nimage classification by CNNs with Transfer Learning. In this study, instead of\ntraining CNNs from scratch, we make use of transfer learning to fine-tune\npre-trained networks a) VGG16 and b) Wide Residual Networks (WRNs), by\nreplacing the final layer with additional layers, for LULC classification with\nEuroSAT dataset. Further, the performance and computational time were compared\nand optimized with techniques like early stopping, gradient clipping, adaptive\nlearning rates and data augmentation. With the proposed approaches we were able\nto address the limited-data problem and achieved very good accuracy.\nComprehensive comparisons over the EuroSAT RGB version benchmark have\nsuccessfully established that our method outperforms the previous best-stated\nresults, with a significant improvement over the accuracy from 98.57% to\n99.17%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naushad_R/0/1/0/all/0/1\">Raoof Naushad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaur_T/0/1/0/all/0/1\">Tarunpreet Kaur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MovingFashion: a Benchmark for the Video-to-Shop Challenge. (arXiv:2110.02627v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02627","description":"<p>Retrieving clothes which are worn in social media videos (Instagram, TikTok)\nis the latest frontier of e-fashion, referred to as \"video-to-shop\" in the\ncomputer vision literature. In this paper we present MovingFashion, the first\npublicly available dataset to cope with this challenge. MovingFashion is\ncomposed of 14855 social videos, each one of them associated to e-commerce\n\"shop\" images where the corresponding clothing items are clearly portrayed. In\naddition, we present a network for retrieving the shop images in this scenario,\ndubbed SEAM Match-RCNN. The model is trained by image-to-video domain\nadaptation, allowing to use video sequences where only their association with a\nshop image is given, eliminating the need of millions of annotated bounding\nboxes. SEAM Match-RCNN builds an embedding, where an attention-based weighted\nsum of few frames (10) of a social video is enough to individuate the correct\nproduct within the first 5 retrieved items in a 14K+ shop element gallery with\nan accuracy of 80%. This provides the best performance on MovingFashion,\ncomparing exhaustively against the related state-of-the-art approaches and\nalternative baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Godi_M/0/1/0/all/0/1\">Marco Godi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joppi_C/0/1/0/all/0/1\">Christian Joppi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skenderi_G/0/1/0/all/0/1\">Geri Skenderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1\">Marco Cristani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Propagating State Uncertainty Through Trajectory Forecasting. (arXiv:2110.03267v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2110.03267","description":"<p>Uncertainty pervades through the modern robotic autonomy stack, with nearly\nevery component (e.g., sensors, detection, classification, tracking, behavior\nprediction) producing continuous or discrete probabilistic distributions.\nTrajectory forecasting, in particular, is surrounded by uncertainty as its\ninputs are produced by (noisy) upstream perception and its outputs are\npredictions that are often probabilistic for use in downstream planning.\nHowever, most trajectory forecasting methods do not account for upstream\nuncertainty, instead taking only the most-likely values. As a result,\nperceptual uncertainties are not propagated through forecasting and predictions\nare frequently overconfident. To address this, we present a novel method for\nincorporating perceptual state uncertainty in trajectory forecasting, a key\ncomponent of which is a new statistical distance-based loss function which\nencourages predicting uncertainties that better match upstream perception. We\nevaluate our approach both in illustrative simulations and on large-scale,\nreal-world data, demonstrating its efficacy in propagating perceptual state\nuncertainty through prediction and producing more calibrated predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivanovic_B/0/1/0/all/0/1\">Boris Ivanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yifeng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_S/0/1/0/all/0/1\">Shubham Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarty_P/0/1/0/all/0/1\">Punarjay Chakravarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Image Decomposition with Phase-Correlation Networks. (arXiv:2110.03473v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03473","description":"<p>The ability to decompose scenes into their object components is a desired\nproperty for autonomous agents, allowing them to reason and act in their\nsurroundings. Recently, different methods have been proposed to learn\nobject-centric representations from data in an unsupervised manner. These\nmethods often rely on latent representations learned by deep neural networks,\nhence requiring high computational costs and large amounts of curated data.\nSuch models are also difficult to interpret. To address these challenges, we\npropose the Phase-Correlation Decomposition Network (PCDNet), a novel model\nthat decomposes a scene into its object components, which are represented as\ntransformed versions of a set of learned object prototypes. The core building\nblock in PCDNet is the Phase-Correlation Cell (PC Cell), which exploits the\nfrequency-domain representation of the images in order to estimate the\ntransformation between an object prototype and its transformed version in the\nimage. In our experiments, we show how PCDNet outperforms state-of-the-art\nmethods for unsupervised object discovery and segmentation on simple benchmark\ndatasets and on more challenging data, while using a small number of learnable\nparameters and being fully interpretable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Villar_Corrales_A/0/1/0/all/0/1\">Angel Villar-Corrales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Sharpness-aware Minimization for Improved Training of Neural Networks. (arXiv:2110.03141v1 [cs.AI] CROSS LISTED)","link":"http://arxiv.org/abs/2110.03141","description":"<p>Overparametrized Deep Neural Networks (DNNs) often achieve astounding\nperformances, but may potentially result in severe generalization error.\nRecently, the relation between the sharpness of the loss landscape and the\ngeneralization error has been established by Foret et al. (2020), in which the\nSharpness Aware Minimizer (SAM) was proposed to mitigate the degradation of the\ngeneralization. Unfortunately, SAM s computational cost is roughly double that\nof base optimizers, such as Stochastic Gradient Descent (SGD). This paper thus\nproposes Efficient Sharpness Aware Minimizer (ESAM), which boosts SAM s\nefficiency at no cost to its generalization performance. ESAM includes two\nnovel and efficient training strategies-StochasticWeight Perturbation and\nSharpness-Sensitive Data Selection. In the former, the sharpness measure is\napproximated by perturbing a stochastically chosen set of weights in each\niteration; in the latter, the SAM loss is optimized using only a judiciously\nselected subset of data that is sensitive to the sharpness. We provide\ntheoretical explanations as to why these strategies perform well. We also show,\nvia extensive experiments on the CIFAR and ImageNet datasets, that ESAM\nenhances the efficiency over SAM from requiring 100% extra computations to 40%\nvis-a-vis base optimizers, while test accuracies are preserved or even\nimproved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jiawei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hanshu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_L/0/1/0/all/0/1\">Liangli Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1\">Rick Siow Mong Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}