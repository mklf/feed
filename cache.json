{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-01-28T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"OntoProtein: Protein Pretraining With Gene Ontology Embedding. (arXiv:2201.11147v1 [q-bio.BM])","link":"http://arxiv.org/abs/2201.11147","description":"<p>Self-supervised protein language models have proved their effectiveness in\nlearning the proteins representations. With the increasing computational power,\ncurrent protein language models pre-trained with millions of diverse sequences\ncan advance the parameter scale from million-level to billion-level and achieve\nremarkable improvement. However, those prevailing approaches rarely consider\nincorporating knowledge graphs (KGs), which can provide rich structured\nknowledge facts for better protein representations. We argue that informative\nbiology knowledge in KGs can enhance protein representation with external\nknowledge. In this work, we propose OntoProtein, the first general framework\nthat makes use of structure in GO (Gene Ontology) into protein pre-training\nmodels. We construct a novel large-scale knowledge graph that consists of GO\nand its related proteins, and gene annotation texts or protein sequences\ndescribe all nodes in the graph. We propose novel contrastive learning with\nknowledge-aware negative sampling to jointly optimize the knowledge graph and\nprotein embedding during pre-training. Experimental results show that\nOntoProtein can surpass state-of-the-art methods with pre-trained protein\nlanguage models in TAPE benchmark and yield better performance compared with\nbaselines in protein-protein interaction and protein function prediction. Code\nand datasets are available in https://github.com/zjunlp/OntoProtein.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hong_H/0/1/0/all/0/1\">Haosen Hong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lian_J/0/1/0/all/0/1\">Jiazhang Lian</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Addressing Issues of Cross-Linguality in Open-Retrieval Question Answering Systems For Emergent Domains. (arXiv:2201.11153v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11153","description":"<p>Open-retrieval question answering systems are generally trained and tested on\nlarge datasets in well-established domains. However, low-resource settings such\nas new and emerging domains would especially benefit from reliable question\nanswering systems. Furthermore, multilingual and cross-lingual resources in\nemergent domains are scarce, leading to few or no such systems. In this paper,\nwe demonstrate a cross-lingual open-retrieval question answering system for the\nemergent domain of COVID-19. Our system adopts a corpus of scientific articles\nto ensure that retrieved documents are reliable. To address the scarcity of\ncross-lingual training data in emergent domains, we present a method utilizing\nautomatic translation, alignment, and filtering to produce English-to-all\ndatasets. We show that a deep semantic retriever greatly benefits from training\non our English-to-all data and significantly outperforms a BM25 baseline in the\ncross-lingual setting. We illustrate the capabilities of our system with\nexamples and release all code necessary to train and deploy such a system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1\">Alon Albalak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Sharon Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable Patterns for Distinction and Prediction of Moral Judgement on Reddit. (arXiv:2201.11155v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11155","description":"<p>The forum r/AmITheAsshole in Reddit hosts discussion on moral issues based on\nconcrete narratives presented by users. Existing analysis of the forum focuses\non its comments, and does not make the underlying data publicly available. In\nthis paper we build a new dataset of comments and also investigate the\nclassification of the posts in the forum. Further, we identify textual patterns\nassociated with the provocation of moral judgement by posts, with the\nexpression of moral stance in comments, and with the decisions of trained\nclassifiers of posts and comments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Efstathiadis_I/0/1/0/all/0/1\">Ion Stagkos Efstathiadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulino_Passos_G/0/1/0/all/0/1\">Guilherme Paulino-Passos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1\">Francesca Toni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tackling data scarcity in speech translation using zero-shot multilingual machine translation techniques. (arXiv:2201.11172v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11172","description":"<p>Recently, end-to-end speech translation (ST) has gained significant attention\nas it avoids error propagation. However, the approach suffers from data\nscarcity. It heavily depends on direct ST data and is less efficient in making\nuse of speech transcription and text translation data, which is often more\neasily available. In the related field of multilingual text translation,\nseveral techniques have been proposed for zero-shot translation. A main idea is\nto increase the similarity of semantically similar sentences in different\nlanguages. We investigate whether these ideas can be applied to speech\ntranslation, by building ST models trained on speech transcription and text\ntranslation data. We investigate the effects of data augmentation and auxiliary\nloss function. The techniques were successfully applied to few-shot ST using\nlimited ST data, with improvements of up to +12.9 BLEU points compared to\ndirect end-to-end ST and +3.1 BLEU points compared to ST models fine-tuned from\nASR model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1\">Tu Anh Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Danni Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1\">Jan Niehues</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence. (arXiv:2201.11176v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11176","description":"<p>Recently has there been a growing interest in the creation of text generation\nsystems from a discourse coherence perspective, e.g., modeling the\ninterdependence between sentences. Still, recent BERT-based evaluation metrics\ncannot recognize coherence and fail to punish incoherent elements in system\noutputs. In this work, we introduce DiscoScore, a discourse metric with\nmultiple variants, which uses BERT to model discourse coherence from different\nperspectives, driven by Centering theory. Our experiments encompass 16\nnon-discourse and discourse metrics, including DiscoScore and popular coherence\nmodels, evaluated on summarization and document-level machine translation (MT).\nWe find that (i) the majority of BERT-based metrics correlate much worse with\nhuman rated coherence than early discourse metrics, invented a decade ago; (ii)\nthe recent state-of-the-art BARTScore is weak when operated at system level --\nwhich is particularly problematic as systems are typically compared in this\nmanner. DiscoScore, in contrast, achieves strong system-level correlation with\nhuman ratings, not only in coherence but also in factual consistency and other\naspects, and surpasses BARTScore by over 10 correlation points on average.\nFurther, aiming to understand DiscoScore, we provide justifications to the\nimportance of discourse coherence for evaluation metrics, and explain the\nsuperiority of one variant over another. Our code is available at\n\\url{https://github.com/AIPHES/DiscoScore}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strube_M/0/1/0/all/0/1\">Michael Strube</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Phonetic Inventories with Crosslingual Automatic Speech Recognition. (arXiv:2201.11207v1 [cs.SD])","link":"http://arxiv.org/abs/2201.11207","description":"<p>The high cost of data acquisition makes Automatic Speech Recognition (ASR)\nmodel training problematic for most existing languages, including languages\nthat do not even have a written script, or for which the phone inventories\nremain unknown. Past works explored multilingual training, transfer learning,\nas well as zero-shot learning in order to build ASR systems for these\nlow-resource languages. While it has been shown that the pooling of resources\nfrom multiple languages is helpful, we have not yet seen a successful\napplication of an ASR model to a language unseen during training. A crucial\nstep in the adaptation of ASR from seen to unseen languages is the creation of\nthe phone inventory of the unseen language. The ultimate goal of our work is to\nbuild the phone inventory of a language unseen during training in an\nunsupervised way without any knowledge about the language. In this paper, we 1)\ninvestigate the influence of different factors (i.e., model architecture,\nphonotactic model, type of speech representation) on phone recognition in an\nunknown language; 2) provide an analysis of which phones transfer well across\nlanguages and which do not in order to understand the limitations of and areas\nfor further improvement for automatic phone inventory creation; and 3) present\ndifferent methods to build a phone inventory of an unseen language in an\nunsupervised way. To that end, we conducted mono-, multi-, and crosslingual\nexperiments on a set of 13 phonetically diverse languages and several in-depth\nanalyses. We found a number of universal phone tokens (IPA symbols) that are\nwell-recognized cross-linguistically. Through a detailed analysis of results,\nwe conclude that unique sounds, similar sounds, and tone languages remain a\nmajor challenge for phonetic inventory discovery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zelasko_P/0/1/0/all/0/1\">Piotr &#x17b;elasko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Siyuan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velazquez_L/0/1/0/all/0/1\">Laureano Moro Velazquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abavisani_A/0/1/0/all/0/1\">Ali Abavisani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhati_S/0/1/0/all/0/1\">Saurabhchand Bhati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharenborg_O/0/1/0/all/0/1\">Odette Scharenborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1\">Mark Hasegawa-Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehak_N/0/1/0/all/0/1\">Najim Dehak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jointly Learning Knowledge Embedding and Neighborhood Consensus with Relational Knowledge Distillation for Entity Alignment. (arXiv:2201.11249v1 [cs.LG])","link":"http://arxiv.org/abs/2201.11249","description":"<p>Entity alignment aims at integrating heterogeneous knowledge from different\nknowledge graphs. Recent studies employ embedding-based methods by first\nlearning the representation of Knowledge Graphs and then performing entity\nalignment via measuring the similarity between entity embeddings. However, they\nfailed to make good use of the relation semantic information due to the\ntrade-off problem caused by the different objectives of learning knowledge\nembedding and neighborhood consensus. To address this problem, we propose\nRelational Knowledge Distillation for Entity Alignment (RKDEA), a Graph\nConvolutional Network (GCN) based model equipped with knowledge distillation\nfor entity alignment. We adopt GCN-based models to learn the representation of\nentities by considering the graph structure and incorporating the relation\nsemantic information into GCN via knowledge distillation. Then, we introduce a\nnovel adaptive mechanism to transfer relational knowledge so as to jointly\nlearn entity embedding and neighborhood consensus. Experimental results on\nseveral benchmarking datasets demonstrate the effectiveness of our proposed\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1\">Chunxiao Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning How to Translate North Korean through South Korean. (arXiv:2201.11258v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11258","description":"<p>South and North Korea both use the Korean language. However, Korean NLP\nresearch has focused on South Korean only, and existing NLP systems of the\nKorean language, such as neural machine translation (NMT) models, cannot\nproperly handle North Korean inputs. Training a model using North Korean data\nis the most straightforward approach to solving this problem, but there is\ninsufficient data to train NMT models. In this study, we create data for North\nKorean NMT models using a comparable corpus. First, we manually create\nevaluation data for automatic alignment and machine translation. Then, we\ninvestigate automatic alignment methods suitable for North Korean. Finally, we\nverify that a model trained by North Korean bilingual data without human\nannotation can significantly boost North Korean translation accuracy compared\nto existing South Korean models in zero-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hwichan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Sangwhan Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komachi_M/0/1/0/all/0/1\">Mamoru Komachi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Highly Generalizable Models for Multilingual Hate Speech Detection. (arXiv:2201.11294v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11294","description":"<p>Hate speech detection has become an important research topic within the past\ndecade. More private corporations are needing to regulate user generated\ncontent on different platforms across the globe. In this paper, we introduce a\nstudy of multilingual hate speech classification. We compile a dataset of 11\nlanguages and resolve different taxonomies by analyzing the combined data with\nbinary labels: hate speech or not hate speech. Defining hate speech in a single\nway across different languages and datasets may erase cultural nuances to the\ndefinition, therefore, we utilize language agnostic embeddings provided by\nLASER and MUSE in order to develop models that can use a generalized definition\nof hate speech across datasets. Furthermore, we evaluate prior state of the art\nmethodologies for hate speech detection under our expanded dataset. We conduct\nthree types of experiments for a binary hate speech classification task:\nMultilingual-Train Monolingual-Test, MonolingualTrain Monolingual-Test and\nLanguage-Family-Train Monolingual Test scenarios to see if performance\nincreases for each language due to learning more from other language data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_N/0/1/0/all/0/1\">Neha Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farris_N/0/1/0/all/0/1\">Nicholas Farris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vidhur Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Higher-Order Semantic Dependency Parser. (arXiv:2201.11312v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11312","description":"<p>Higher-order features bring significant accuracy gains in semantic dependency\nparsing. However, modeling higher-order features with exact inference is\nNP-hard. Graph neural networks (GNNs) have been demonstrated to be an effective\ntool for solving NP-hard problems with approximate inference in many graph\nlearning tasks. Inspired by the success of GNNs, we investigate building a\nhigher-order semantic dependency parser by applying GNNs. Instead of explicitly\nextracting higher-order features from intermediate parsing graphs, GNNs\naggregate higher-order information concisely by stacking multiple GNN layers.\nExperimental results show that our model outperforms the previous\nstate-of-the-art parser on the SemEval 2015 Task 18 English datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yunlong Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sataer_Y/0/1/0/all/0/1\">Yikemaiti Sataer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhiqiang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Deep Semantic Model for Code Search using CodeSearchNet Corpus. (arXiv:2201.11313v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11313","description":"<p>Semantic code search is the task of retrieving relevant code snippet given a\nnatural language query. Different from typical information retrieval tasks,\ncode search requires to bridge the semantic gap between the programming\nlanguage and natural language, for better describing intrinsic concepts and\nsemantics. Recently, deep neural network for code search has been a hot\nresearch topic. Typical methods for neural code search first represent the code\nsnippet and query text as separate embeddings, and then use vector distance\n(e.g. dot-product or cosine) to calculate the semantic similarity between them.\nThere exist many different ways for aggregating the variable length of code or\nquery tokens into a learnable embedding, including bi-encoder, cross-encoder,\nand poly-encoder. The goal of the query encoder and code encoder is to produce\nembeddings that are close with each other for a related pair of query and the\ncorresponding desired code snippet, in which the choice and design of encoder\nis very significant.\n</p>\n<p>In this paper, we propose a novel deep semantic model which makes use of the\nutilities of not only the multi-modal sources, but also feature extractors such\nas self-attention, the aggregated vectors, combination of the intermediate\nrepresentations. We apply the proposed model to tackle the CodeSearchNet\nchallenge about semantic code search. We align cross-lingual embedding for\nmulti-modality learning with large batches and hard example mining, and combine\ndifferent learned representations for better enhancing the representation\nlearning. Our model is trained on CodeSearchNet corpus and evaluated on the\nheld-out data, the final model achieves 0.384 NDCG and won the first place in\nthis benchmark. Models and code are available at\nhttps://github.com/overwindows/SemanticCodeSearch.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ontology-enhanced Prompt-tuning for Few-shot Learning. (arXiv:2201.11332v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11332","description":"<p>Few-shot Learning (FSL) is aimed to make predictions based on a limited\nnumber of samples. Structured data such as knowledge graphs and ontology\nlibraries has been leveraged to benefit the few-shot setting in various tasks.\nHowever, the priors adopted by the existing methods suffer from challenging\nknowledge missing, knowledge noise, and knowledge heterogeneity, which hinder\nthe performance for few-shot learning. In this study, we explore knowledge\ninjection for FSL with pre-trained language models and propose\nontology-enhanced prompt-tuning (OntoPrompt). Specifically, we develop the\nontology transformation based on the external knowledge graph to address the\nknowledge missing issue, which fulfills and converts structure knowledge to\ntext. We further introduce span-sensitive knowledge injection via a visible\nmatrix to select informative knowledge to handle the knowledge noise issue. To\nbridge the gap between knowledge and text, we propose a collective training\nalgorithm to optimize representations jointly. We evaluate our proposed\nOntoPrompt in three tasks, including relation extraction, event extraction, and\nknowledge graph completion, with eight datasets. Experimental results\ndemonstrate that our approach can obtain better few-shot performance than\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pan More Gold from the Sand: Refining Open-domain Dialogue Training with Noisy Self-Retrieval Generation. (arXiv:2201.11367v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11367","description":"<p>Real human conversation data are complicated, heterogeneous, and noisy, from\nwhom building open-domain dialogue systems remains a challenging task. In fact,\nsuch dialogue data can still contain a wealth of information and knowledge,\nhowever, they are not fully explored. In this paper, we show existing\nopen-domain dialogue generation methods by memorizing context-response paired\ndata with causal or encode-decode language models underutilize the training\ndata. Different from current approaches, using external knowledge, we explore a\nretrieval-generation training framework that can increase the usage of training\ndata by directly considering the heterogeneous and noisy training data as the\n\"evidence\". Experiments over publicly available datasets demonstrate that our\nmethod can help models generate better responses, even such training data are\nusually impressed as low-quality data. Such performance gain is comparable with\nthose improved by enlarging the training set, even better. We also found that\nthe model performance has a positive correlation with the relevance of the\nretrieved evidence. Moreover, our method performed well on zero-shot\nexperiments, which indicates that our method can be more robust to real-world\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yihe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pingyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Systematic Investigation of Strategies Tailored for Low-Resource Settings for Sanskrit Dependency Parsing. (arXiv:2201.11374v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11374","description":"<p>Existing state of the art approaches for Sanskrit Dependency Parsing (SDP),\nare hybrid in nature, and rely on a lexicon-driven shallow parser for\nlinguistically motivated feature engineering. However, these methods fail to\nhandle out of vocabulary (OOV) words, which limits their applicability in\nrealistic scenarios. On the other hand, purely data-driven approaches do not\nmatch the performance of hybrid approaches due to the labelled data sparsity.\nThus, in this work, we investigate the following question: How far can we push\na purely data-driven approach using recently proposed strategies for\nlow-resource settings? We experiment with five strategies, namely, data\naugmentation, sequential transfer learning, cross-lingual/mono-lingual\npretraining, multi-task learning and self-training. Our proposed ensembled\nsystem outperforms the purely data-driven state of the art system by 2.8/3.9\npoints (Unlabelled Attachment Score (UAS)/Labelled Attachment Score (LAS))\nabsolute gain. Interestingly, it also supersedes the performance of the state\nof the art hybrid system by 1.2 points (UAS) absolute gain and shows comparable\nperformance in terms of LAS. Code and data will be publicly available at:\n\\url{https://github.com/Jivnesh/SanDP}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sandhan_J/0/1/0/all/0/1\">Jivnesh Sandhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behera_L/0/1/0/all/0/1\">Laxmidhar Behera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prabhupadavani: A Code-mixed Speech Translation Data for 25 Languages. (arXiv:2201.11391v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11391","description":"<p>Nowadays, code-mixing has become ubiquitous in Natural Language Processing\n(NLP); however, no efforts have been made to address this phenomenon for Speech\nTranslation (ST) task. This can be solely attributed to the lack of code-mixed\nST task labelled data. Thus, we introduce Prabhupadavani, a multilingual\ncode-mixed ST dataset for 25 languages, covering ten language families,\ncontaining 94 hours of speech by 130+ speakers, manually aligned with\ncorresponding text in the target language. Prabhupadvani is the first\ncode-mixed ST dataset available in the ST literature to the best of our\nknowledge. This data also can be used for a code-mixed machine translation\ntask. All the dataset and code can be accessed at:\n\\url{https://github.com/frozentoad9/CMST}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sandhan_J/0/1/0/all/0/1\">Jivnesh Sandhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daksh_A/0/1/0/all/0/1\">Ayush Daksh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paranjay_O/0/1/0/all/0/1\">Om Adideva Paranjay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behera_L/0/1/0/all/0/1\">Laxmidhar Behera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Yes-Yes-Yes: Donation-based Peer Reviewing Data Collection for ACL Rolling Review and Beyond. (arXiv:2201.11443v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11443","description":"<p>Peer review is the primary gatekeeper of scientific merit and quality, yet it\nis prone to bias and suffers from low efficiency. This demands\ncross-disciplinary scrutiny of the processes that underlie peer reviewing;\nhowever, quantitative research is limited by the data availability, as most of\nthe peer reviewing data across research disciplines is never made public.\nExisting data collection efforts focus on few scientific domains and do not\naddress a range of ethical, license- and confidentiality-related issues\nassociated with peer reviewing data, preventing wide-scale research and\napplication development. While recent methods for peer review analysis and\nprocessing show promise, a solid data foundation for computational research in\npeer review is still missing. To address this, we present an in-depth\ndiscussion of peer reviewing data, outline the ethical and legal desiderata for\npeer reviewing data collection, and propose the first continuous,\ndonation-based data collection workflow that meets these requirements. We\nreport on the ongoing implementation of this workflow at the ACL Rolling Review\nand deliver the first insights obtained with the newly collected data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dycke_N/0/1/0/all/0/1\">Nils Dycke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_I/0/1/0/all/0/1\">Ilia Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning Like Program Executors. (arXiv:2201.11473v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11473","description":"<p>Reasoning over natural language is a long-standing goal for the research\ncommunity. However, studies have shown that existing language models are\ninadequate in reasoning. To address the issue, we present POET, a new\npre-training paradigm. Through pre-training language models with programs and\ntheir execution results, POET empowers language models to harvest the reasoning\nknowledge possessed in program executors via a data-driven approach. POET is\nconceptually simple and can be instantiated by different kinds of programs. In\nthis paper, we show three empirically powerful instances, i.e., POET-Math,\nPOET-Logic, and POET-SQL. Experimental results on six benchmarks demonstrate\nthat POET can significantly boost model performance on natural language\nreasoning, such as numerical reasoning, logical reasoning, and multi-hop\nreasoning. Taking the DROP benchmark as a representative example, POET improves\nthe F1 metric of BART from 69.2% to 80.6%. Furthermore, POET shines in giant\nlanguage models, pushing the F1 metric of T5-11B to 87.6% and achieving a new\nstate-of-the-art performance on DROP. POET opens a new gate on\nreasoning-enhancement pre-training and we hope our analysis would shed light on\nthe future research of reasoning like program executors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pi_X/0/1/0/all/0/1\">Xinyu Pi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziyadi_M/0/1/0/all/0/1\">Morteza Ziyadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zeqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Interpretation of Saliency-based Explanation Over Text. (arXiv:2201.11569v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11569","description":"<p>While a lot of research in explainable AI focuses on producing effective\nexplanations, less work is devoted to the question of how people understand and\ninterpret the explanation. In this work, we focus on this question through a\nstudy of saliency-based explanations over textual data. Feature-attribution\nexplanations of text models aim to communicate which parts of the input text\nwere more influential than others towards the model decision. Many current\nexplanation methods, such as gradient-based or Shapley value-based methods,\nprovide measures of importance which are well-understood mathematically. But\nhow does a person receiving the explanation (the explainee) comprehend it? And\ndoes their understanding match what the explanation attempted to communicate?\nWe empirically investigate the effect of various factors of the input, the\nfeature-attribution explanation, and visualization procedure, on laypeople's\ninterpretation of the explanation. We query crowdworkers for their\ninterpretation on tasks in English and German, and fit a GAMM model to their\nresponses considering the factors of interest. We find that people often\nmis-interpret the explanations: superficial and unrelated factors, such as word\nlength, influence the explainees' importance assignment despite the explanation\ncommunicating importance directly. We then show that some of this distortion\ncan be attenuated: we propose a method to adjust saliencies based on model\nestimates of over- and under-perception, and explore bar charts as an\nalternative to heatmap saliency visualization. We find that both approaches can\nattenuate the distorting effect of specific factors, leading to\nbetter-calibrated understanding of the explanation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuff_H/0/1/0/all/0/1\">Hendrik Schuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacovi_A/0/1/0/all/0/1\">Alon Jacovi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthesizing Dysarthric Speech Using Multi-talker TTS for Dysarthric Speech Recognition. (arXiv:2201.11571v1 [eess.AS])","link":"http://arxiv.org/abs/2201.11571","description":"<p>Dysarthria is a motor speech disorder often characterized by reduced speech\nintelligibility through slow, uncoordinated control of speech production\nmuscles. Automatic Speech recognition (ASR) systems may help dysarthric talkers\ncommunicate more effectively. To have robust dysarthria-specific ASR,\nsufficient training speech is required, which is not readily available. Recent\nadvances in Text-To-Speech (TTS) synthesis multi-speaker end-to-end TTS systems\nsuggest the possibility of using synthesis for data augmentation. In this\npaper, we aim to improve multi-speaker end-to-end TTS systems to synthesize\ndysarthric speech for improved training of a dysarthria-specific DNN-HMM ASR.\nIn the synthesized speech, we add dysarthria severity level and pause insertion\nmechanisms to other control parameters such as pitch, energy, and duration.\nResults show that a DNN-HMM model trained on additional synthetic dysarthric\nspeech achieves WER improvement of 12.2% compared to the baseline, the addition\nof the severity level and pause insertion controls decrease WER by 6.5%,\nshowing the effectiveness of adding these parameters. Audio samples are\navailable at\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Soleymanpour_M/0/1/0/all/0/1\">Mohammad Soleymanpour</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Johnson_M/0/1/0/all/0/1\">Michael T. Johnson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soleymanpour_R/0/1/0/all/0/1\">Rahim Soleymanpour</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Berry_J/0/1/0/all/0/1\">Jeffrey Berry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grad2Task: Improved Few-shot Text Classification Using Gradients for Task Representation. (arXiv:2201.11576v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11576","description":"<p>Large pretrained language models (LMs) like BERT have improved performance in\nmany disparate natural language processing (NLP) tasks. However, fine tuning\nsuch models requires a large number of training examples for each target task.\nSimultaneously, many realistic NLP problems are \"few shot\", without a\nsufficiently large training set. In this work, we propose a novel conditional\nneural process-based approach for few-shot text classification that learns to\ntransfer from other diverse tasks with rich annotation. Our key idea is to\nrepresent each task using gradient information from a base model and to train\nan adaptation network that modulates a text classifier conditioned on the task\nrepresentation. While previous task-aware few-shot learners represent tasks by\ninput encoding, our novel task representation is more powerful, as the gradient\ncaptures input-output relationships of a task. Experimental results show that\nour approach outperforms traditional fine-tuning, sequential transfer learning,\nand state-of-the-art meta learning approaches on a collection of diverse\nfew-shot tasks. We further conducted analysis and ablations to justify our\ndesign choices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kuan-Chieh Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1\">Frank Rudzicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brudno_M/0/1/0/all/0/1\">Michael Brudno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GUDN A novel guide network for extreme multi-label text classification. (arXiv:2201.11582v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11582","description":"<p>The problem of extreme multi-label text classification (XMTC) is to recall\nsome most relevant labels for a text from an extremely large label set. Though\nthe methods based on deep pre-trained models have reached significant\nachievement, the pre-trained models are still not fully utilized. Label\nsemantics has not attracted much attention so far, and the latent space between\ntexts and labels has not been effectively explored. This paper constructs a\nnovel guide network (GUDN) to help fine-tune the pre-trained model to instruct\nclassification later. Also, we use the raw label semantics to effectively\nexplore the latent space between texts and labels, which can further improve\npredicted accuracy. Experimental results demonstrate that GUDN outperforms\nstate-of-the-art methods on several popular datasets. Our source code is\nreleased at https://github.com/wq2581/GUDN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_H/0/1/0/all/0/1\">Hongji Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jia Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages. (arXiv:2201.11732v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11732","description":"<p>Reliable evaluation benchmarks designed for replicability and\ncomprehensiveness have driven progress in machine learning. Due to the lack of\na multilingual benchmark, however, vision-and-language research has mostly\nfocused on English language tasks. To fill this gap, we introduce the\nImage-Grounded Language Understanding Evaluation benchmark. IGLUE brings\ntogether - by both aggregating pre-existing datasets and creating new ones -\nvisual question answering, cross-modal retrieval, grounded reasoning, and\ngrounded entailment tasks across 20 diverse languages. Our benchmark enables\nthe evaluation of multilingual multimodal models for transfer learning, not\nonly in a zero-shot setting, but also in newly defined few-shot learning\nsetups. Based on the evaluation of the available state-of-the-art models, we\nfind that translate-test transfer is superior to zero-shot transfer and that\nfew-shot learning is hard to harness for many tasks. Moreover, downstream\nperformance is partially explained by the amount of available unlabelled\ntextual data for pretraining, and only weakly by the typological distance of\ntarget-source languages. We hope to encourage future research efforts in this\narea by releasing the benchmark to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Maria Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Named Entity Recognition and Relation Extraction using Enhanced Table Filling by Contextualized Representations. (arXiv:2010.07522v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.07522","description":"<p>In this study, a novel method for extracting named entities and relations\nfrom unstructured text based on the table representation is presented. By using\ncontextualized word embeddings, the proposed method computes representations\nfor entity mentions and long-range dependencies without complicated\nhand-crafted features or neural-network architectures. We also adapt a tensor\ndot-product to predict relation labels all at once without resorting to\nhistory-based predictions or search strategies. These advances significantly\nsimplify the model and algorithm for the extraction of named entities and\nrelations. Despite its simplicity, the experimental results demonstrate that\nthe proposed method outperforms the state-of-the-art methods on the CoNLL04 and\nACE05 English datasets. We also confirm that the proposed method achieves a\ncomparable performance with the state-of-the-art NER models on the ACE05\ndatasets when multiple sentences are provided for context aggregation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Youmi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hiraoka_T/0/1/0/all/0/1\">Tatsuya Hiraoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains. (arXiv:2102.12206v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.12206","description":"<p>Natural Language Processing algorithms have made incredible progress, but\nthey still struggle when applied to out-of-distribution examples. We address a\nchallenging and underexplored version of this domain adaptation problem, where\nan algorithm is trained on several source domains, and then applied to examples\nfrom unseen domains that are unknown at training time. Particularly, no\nexamples, labeled or unlabeled, or any other knowledge about the target domain\nare available to the algorithm at training time. We present PADA: An\nexample-based autoregressive Prompt learning algorithm for on-the-fly\nAny-Domain Adaptation, based on the T5 language model. Given a test example,\nPADA first generates a unique prompt for it and then, conditioned on this\nprompt, labels the example with respect to the NLP prediction task. PADA is\ntrained to generate a prompt which is a token sequence of unrestricted length,\nconsisting of Domain Related Features (DRFs) that characterize each of the\nsource domains. Intuitively, the generated prompt is a unique signature that\nmaps the test example to a semantic space spanned by the source domains. In\nexperiments with 3 tasks (text classification and sequence tagging), for a\ntotal of 14 multi-source adaptation scenarios, PADA substantially outperforms\nstrong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ben_David_E/0/1/0/all/0/1\">Eyal Ben-David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oved_N/0/1/0/all/0/1\">Nadav Oved</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Neighbourhood Framework for Resource-Lean Content Flagging. (arXiv:2103.17055v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.17055","description":"<p>We propose a novel framework for cross-lingual content flagging with limited\ntarget-language data, which significantly outperforms prior work in terms of\npredictive performance. The framework is based on a nearest-neighbour\narchitecture. It is a modern instantiation of the vanilla k-nearest neighbour\nmodel, as we use Transformer representations in all its components. Our\nframework can adapt to new source-language instances, without the need to be\nretrained from scratch. Unlike prior work on neighbourhood-based approaches, we\nencode the neighbourhood information based on query--neighbour interactions. We\npropose two encoding schemes and we show their effectiveness using both\nqualitative and quantitative analysis. Our evaluation results on eight\nlanguages from two different datasets for abusive language detection show\nsizable improvements of up to 9.5 F1 points absolute (for Italian) over strong\nbaselines. On average, we achieve 3.6 absolute F1 points of improvement for the\nthree languages in the Jigsaw Multilingual dataset and 2.14 points for the WUL\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarwar_S/0/1/0/all/0/1\">Sheikh Muhammad Sarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zlatkova_D/0/1/0/all/0/1\">Dimitrina Zlatkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1\">Momchil Hardalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinkov_Y/0/1/0/all/0/1\">Yoan Dinkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grover's Algorithm for Question Answering. (arXiv:2106.05299v3 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2106.05299","description":"<p>Grover's algorithm, a well-know quantum search algorithm, allows one to find\nthe correct item in a database, with quadratic speedup. In this paper we adapt\nGrover's algorithm to the problem of finding a correct answer to a natural\nlanguage question in English, thus contributing to the growing field of Quantum\nNatural Language Processing. Using a grammar that can be interpreted as tensor\ncontractions, each word is represented as a quantum state that serves as input\nto the quantum circuit. We here introduce a quantum measurement to contract the\nrepresentations of words, resulting in the representation of larger text\nfragments. Using this framework, a representation for the question is found\nthat contains all the possible answers in equal quantum superposition, and\nallows for the building of an oracle that can detect a correct answer, being\nagnostic to the specific question. Furthermore, we show that our construction\ncan deal with certain types of ambiguous phrases by keeping the various\ndifferent meanings in quantum superposition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Correia_A/0/1/0/all/0/1\">A. D. Correia</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Moortgat_M/0/1/0/all/0/1\">M. Moortgat</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Stoof_H/0/1/0/all/0/1\">H. T. C. Stoof</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners. (arXiv:2108.13161v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13161","description":"<p>Large-scale pre-trained language models have contributed significantly to\nnatural language processing by demonstrating remarkable abilities as few-shot\nlearners. However, their effectiveness depends mainly on scaling the model\nparameters and prompt design, hindering their implementation in most real-world\napplications. This study proposes a novel pluggable, extensible, and efficient\napproach named DifferentiAble pRompT (DART), which can convert small language\nmodels into better few-shot learners without any prompt engineering. The main\nprinciple behind this approach involves reformulating potential natural\nlanguage processing tasks into the task of a pre-trained language model and\ndifferentially optimizing the prompt template as well as the target label with\nbackpropagation. Furthermore, the proposed approach can be: (i) Plugged to any\npre-trained language models; (ii) Extended to widespread classification tasks.\nA comprehensive evaluation of standard NLP tasks demonstrates that the proposed\napproach achieves a better few-shot performance. Code is available in\nhttps://github.com/zjunlp/DART.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Audio Captions Be Evaluated with Image Caption Metrics?. (arXiv:2110.04684v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2110.04684","description":"<p>Automated audio captioning aims at generating textual descriptions for an\naudio clip. To evaluate the quality of generated audio captions, previous works\ndirectly adopt image captioning metrics like SPICE and CIDEr, without\njustifying their suitability in this new domain, which may mislead the\ndevelopment of advanced models. This problem is still unstudied due to the lack\nof human judgment datasets on caption quality. Therefore, we firstly construct\ntwo evaluation benchmarks, AudioCaps-Eval and Clotho-Eval. They are established\nwith pairwise comparison instead of absolute rating to achieve better\ninter-annotator agreement. Current metrics are found in poor correlation with\nhuman annotations on these datasets. To overcome their limitations, we propose\na metric named FENSE, where we combine the strength of Sentence-BERT in\ncapturing similarity, and a novel Error Detector to penalize erroneous\nsentences for robustness. On the newly established benchmarks, FENSE\noutperforms current metrics by 14-25% accuracy. Code, data and web demo\navailable at: https://github.com/blmoistawinde/fense\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zelin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiling Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuenan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zeyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kenny Q. Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Multi-hop Question Answering over Knowledge Base. (arXiv:2112.11909v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.11909","description":"<p>KBQA is a task that requires to answer questions by using semantic structured\ninformation in knowledge base. Previous work in this area has been restricted\ndue to the lack of large semantic parsing dataset and the exponential growth of\nsearching space with the increasing hops of relation paths. In this paper, we\npropose an efficient pipeline method equipped with a pre-trained language\nmodel. By adopting Beam Search algorithm, the searching space will not be\nrestricted in subgraph of 3 hops. Besides, we propose a data generation\nstrategy, which enables our model to generalize well from few training samples.\nWe evaluate our model on an open-domain complex Chinese Question Answering task\nCCKS2019 and achieve F1-score of 62.55% on the test dataset. In addition, in\norder to test the few-shot learning capability of our model, we ramdomly select\n10% of the primary data to train our model, the result shows that our model can\nstill achieves F1-score of 58.54%, which verifies the capability of our model\nto process KBQA task and the advantage in few-shot Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1\">Meihao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Siyao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuru Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZeroBERTo: Leveraging Zero-Shot Text Classification by Topic Modeling. (arXiv:2201.01337v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.01337","description":"<p>Traditional text classification approaches often require a good amount of\nlabeled data, which is difficult to obtain, especially in restricted domains or\nless widespread languages. This lack of labeled data has led to the rise of\nlow-resource methods, that assume low data availability in natural language\nprocessing. Among them, zero-shot learning stands out, which consists of\nlearning a classifier without any previously labeled data. The best results\nreported with this approach use language models such as Transformers, but fall\ninto two problems: high execution time and inability to handle long texts as\ninput. This paper proposes a new model, ZeroBERTo, which leverages an\nunsupervised clustering step to obtain a compressed data representation before\nthe classification task. We show that ZeroBERTo has better performance for long\ninputs and shorter execution time, outperforming XLM-R by about 12% in the F1\nscore in the FolhaUOL dataset. Keywords: Low-Resource NLP, Unlabeled data,\nZero-Shot Learning, Topic Modeling, Transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alcoforado_A/0/1/0/all/0/1\">Alexandre Alcoforado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraz_T/0/1/0/all/0/1\">Thomas Palmeira Ferraz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerber_R/0/1/0/all/0/1\">Rodrigo Gerber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bustos_E/0/1/0/all/0/1\">Enzo Bustos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1\">Andr&#xe9; Seidel Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veloso_B/0/1/0/all/0/1\">Bruno Miguel Veloso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siqueira_F/0/1/0/all/0/1\">Fabio Levy Siqueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1\">Anna Helena Reali Costa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Table Pre-training: A Survey on Model Architectures, Pretraining Objectives, and Downstream Tasks. (arXiv:2201.09745v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.09745","description":"<p>Since a vast number of tables can be easily collected from web pages,\nspreadsheets, PDFs, and various other document types, a flurry of table\npre-training frameworks have been proposed following the success of text and\nimages, and they have achieved new state-of-the-arts on various tasks such as\ntable question answering, table type recognition, column relation\nclassification, table search, formula prediction, etc. To fully use the\nsupervision signals in unlabeled tables, a variety of pre-training objectives\nhave been designed and evaluated, for example, denoising cell values,\npredicting numerical relationships, and implicitly executing SQLs. And to best\nleverage the characteristics of (semi-)structured tables, various tabular\nlanguage models, particularly with specially-designed attention mechanisms,\nhave been explored. Since tables usually appear and interact with free-form\ntext, table pre-training usually takes the form of table-text joint\npre-training, which attracts significant research interests from multiple\ndomains. This survey aims to provide a comprehensive review of different model\ndesigns, pre-training objectives, and downstream tasks for table pre-training,\nand we further share our thoughts and vision on existing challenges and future\nopportunities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhoujun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xinyi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Anda Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Ao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"DIREG3D: DIrectly REGress 3D Hands from Multiple Cameras. (arXiv:2201.11187v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11187","description":"<p>In this paper, we present DIREG3D, a holistic framework for 3D Hand Tracking.\nThe proposed framework is capable of utilizing camera intrinsic parameters, 3D\ngeometry, intermediate 2D cues, and visual information to regress parameters\nfor accurately representing a Hand Mesh model. Our experiments show that\ninformation like the size of the 2D hand, its distance from the optical center,\nand radial distortion is useful for deriving highly reliable 3D poses in camera\nspace from just monocular information. Furthermore, we extend these results to\na multi-view camera setup by fusing features from different viewpoints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ashar Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahbub_U/0/1/0/all/0/1\">Upal Mahbub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dane_G/0/1/0/all/0/1\">Gokce Dane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reitmayr_G/0/1/0/all/0/1\">Gerhard Reitmayr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReforesTree: A Dataset for Estimating Tropical Forest Carbon Stock with Deep Learning and Aerial Imagery. (arXiv:2201.11192v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11192","description":"<p>Forest biomass is a key influence for future climate, and the world urgently\nneeds highly scalable financing schemes, such as carbon offsetting\ncertifications, to protect and restore forests. Current manual forest carbon\nstock inventory methods of measuring single trees by hand are time, labour, and\ncost-intensive and have been shown to be subjective. They can lead to\nsubstantial overestimation of the carbon stock and ultimately distrust in\nforest financing. The potential for impact and scale of leveraging advancements\nin machine learning and remote sensing technologies is promising but needs to\nbe of high quality in order to replace the current forest stock protocols for\ncertifications.\n</p>\n<p>In this paper, we present ReforesTree, a benchmark dataset of forest carbon\nstock in six agro-forestry carbon offsetting sites in Ecuador. Furthermore, we\nshow that a deep learning-based end-to-end model using individual tree\ndetection from low cost RGB-only drone imagery is accurately estimating forest\ncarbon stock within official carbon offsetting certification standards.\nAdditionally, our baseline CNN model outperforms state-of-the-art\nsatellite-based forest biomass and carbon stock estimates for this type of\nsmall-scale, tropical agro-forestry sites. We present this dataset to encourage\nmachine learning research in this area to increase accountability and\ntransparency of monitoring, verification and reporting (MVR) in carbon\noffsetting projects, as well as scaling global reforestation financing through\naccurate remote sensing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reiersen_G/0/1/0/all/0/1\">Gyri Reiersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dao_D/0/1/0/all/0/1\">David Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1\">Bj&#xf6;rn L&#xfc;tjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1\">Konstantin Klemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amara_K/0/1/0/all/0/1\">Kenza Amara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinegger_A/0/1/0/all/0/1\">Attila Steinegger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoxiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges and Opportunities for Machine Learning Classification of Behavior and Mental State from Images. (arXiv:2201.11197v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11197","description":"<p>Computer Vision (CV) classifiers which distinguish and detect nonverbal\nsocial human behavior and mental state can aid digital diagnostics and\ntherapeutics for psychiatry and the behavioral sciences. While CV classifiers\nfor traditional and structured classification tasks can be developed with\nstandard machine learning pipelines for supervised learning consisting of data\nlabeling, preprocessing, and training a convolutional neural network, there are\nseveral pain points which arise when attempting this process for behavioral\nphenotyping. Here, we discuss the challenges and corresponding opportunities in\nthis space, including handling heterogeneous data, avoiding biased models,\nlabeling massive and repetitive data sets, working with ambiguous or compound\nclass labels, managing privacy concerns, creating appropriate representations,\nand personalizing models. We discuss current state-of-the-art research\nendeavors in CV such as data curation, data augmentation, crowdsourced\nlabeling, active learning, reinforcement learning, generative models,\nrepresentation learning, federated learning, and meta-learning. We highlight at\nleast some of the machine learning advancements needed for imaging classifiers\nto detect human social cues successfully and reliably.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1\">Peter Washington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutlu_C/0/1/0/all/0/1\">Cezmi Onur Mutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kline_A/0/1/0/all/0/1\">Aaron Kline</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paskov_K/0/1/0/all/0/1\">Kelley Paskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stockham_N/0/1/0/all/0/1\">Nate Tyler Stockham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrisman_B/0/1/0/all/0/1\">Brianna Chrisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deveau_N/0/1/0/all/0/1\">Nick Deveau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surhabi_M/0/1/0/all/0/1\">Mourya Surhabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haber_N/0/1/0/all/0/1\">Nick Haber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wall_D/0/1/0/all/0/1\">Dennis P. Wall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Examination by Automatic Quiz Assessment Using Spiral Codes and Image Processing. (arXiv:2201.11228v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11228","description":"<p>We describe a technical solution implemented at Halmstad University to\nautomatise assessment and reporting of results of paper-based quiz exams. Paper\nquizzes are affordable and within reach of campus education in classrooms.\nOffering and taking them is accepted as they cause fewer issues with\nreliability and democratic access, e.g. a large number of students can take\nthem without a trusted mobile device, internet, or battery. By contrast,\ncorrection of the quiz is a considerable obstacle. We suggest mitigating the\nissue by a novel image processing technique using harmonic spirals that aligns\nanswer sheets in sub-pixel accuracy to read student identity and answers and to\nemail results within minutes, all fully automatically. Using the described\nmethod, we carry out regular weekly examinations in two master courses at the\nmentioned centre without a significant workload increase. The employed solution\nalso enables us to assign a unique identifier to each quiz (e.g. week 1, week\n2. . . ) while allowing us to have an individualised quiz for each student.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1\">Fernando Alonso-Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigun_J/0/1/0/all/0/1\">Josef Bigun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HistoKT: Cross Knowledge Transfer in Computational Pathology. (arXiv:2201.11246v1 [eess.IV])","link":"http://arxiv.org/abs/2201.11246","description":"<p>The lack of well-annotated datasets in computational pathology (CPath)\nobstructs the application of deep learning techniques for classifying medical\nimages. %Since pathologist time is expensive, dataset curation is intrinsically\ndifficult. Many CPath workflows involve transferring learned knowledge between\nvarious image domains through transfer learning. Currently, most transfer\nlearning research follows a model-centric approach, tuning network parameters\nto improve transfer results over few datasets. In this paper, we take a\ndata-centric approach to the transfer learning problem and examine the\nexistence of generalizable knowledge between histopathological datasets. First,\nwe create a standardization workflow for aggregating existing histopathological\ndata. We then measure inter-domain knowledge by training ResNet18 models across\nmultiple histopathological datasets, and cross-transferring between them to\ndetermine the quantity and quality of innate shared knowledge. Additionally, we\nuse weight distillation to share knowledge between models without additional\ntraining. We find that hard to learn, multi-class datasets benefit most from\npretraining, and a two stage learning framework incorporating a large source\ndomain such as ImageNet allows for better utilization of smaller datasets.\nFurthermore, we find that weight distillation enables models trained on purely\nhistopathological features to outperform models using external natural image\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1\">Ryan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1\">Jiadai Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Stephen Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hosseini_M/0/1/0/all/0/1\">Mahdi S. Hosseini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Genovese_A/0/1/0/all/0/1\">Angelo Genovese</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Lina Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rowsell_C/0/1/0/all/0/1\">Corwyn Rowsell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Damaskinos_S/0/1/0/all/0/1\">Savvas Damaskinos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Varma_S/0/1/0/all/0/1\">Sonal Varma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlling Directions Orthogonal to a Classifier. (arXiv:2201.11259v1 [cs.LG])","link":"http://arxiv.org/abs/2201.11259","description":"<p>We propose to identify directions invariant to a given classifier so that\nthese directions can be controlled in tasks such as style transfer. While\northogonal decomposition is directly identifiable when the given classifier is\nlinear, we formally define a notion of orthogonality in the non-linear case. We\nalso provide a surprisingly simple method for constructing the orthogonal\nclassifier (a classifier utilizing directions other than those of the given\nclassifier). Empirically, we present three use cases where controlling\northogonal variation is important: style transfer, domain adaptation, and\nfairness. The orthogonal classifier enables desired style transfer when domains\nvary in multiple aspects, improves domain adaptation with label shifts and\nmitigates the unfairness as a predictor. The code is available at\n<a href=\"http://github.com/Newbeeer/orthogonal_classifier\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yilun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tianxiao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1\">Tommi Jaakkola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting RCAN: Improved Training for Image Super-Resolution. (arXiv:2201.11279v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11279","description":"<p>Image super-resolution (SR) is a fast-moving field with novel architectures\nattracting the spotlight. However, most SR models were optimized with dated\ntraining strategies. In this work, we revisit the popular RCAN model and\nexamine the effect of different training options in SR. Surprisingly (or\nperhaps as expected), we show that RCAN can outperform or match nearly all the\nCNN-based SR architectures published after RCAN on standard benchmarks with a\nproper training strategy and minimal architecture change. Besides, although\nRCAN is a very large SR architecture with more than four hundred convolutional\nlayers, we draw a notable conclusion that underfitting is still the main\nproblem restricting the model capability instead of overfitting. We observe\nsupportive evidence that increasing training iterations clearly improves the\nmodel performance while applying regularization techniques generally degrades\nthe predictions. We denote our simply revised RCAN as RCAN-it and recommend\npractitioners to use it as baselines for future research. Code is publicly\navailable at https://github.com/zudi-lin/rcan-it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zudi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_P/0/1/0/all/0/1\">Prateek Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Atmadeep Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magid_S/0/1/0/all/0/1\">Salma Abdel Magid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Deqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Donglai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive 3D Character Modeling from 2D Orthogonal Drawings with Annotations. (arXiv:2201.11284v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11284","description":"<p>We propose an interactive 3D character modeling approach from orthographic\ndrawings (e.g., front and side views) based on 2D-space annotations. First, the\nsystem builds partial correspondences between the input drawings and generates\na base mesh with sweeping splines according to edge information in 2D images.\nNext, users annotates the desired parts on the input drawings (e.g., the eyes\nand mouth) by using two type of strokes, called addition and erosion, and the\nsystem re-optimizes the shape of the base mesh. By repeating the 2D-space\noperations (i.e., revising and modifying the annotations), users can design a\ndesired character model. To validate the efficiency and quality of our system,\nwe verified the generated results with state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhengyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haoran Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukusato_T/0/1/0/all/0/1\">Tsukasa Fukusato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient divide-and-conquer registration of UAV and ground LiDAR point clouds through canopy shape context. (arXiv:2201.11296v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11296","description":"<p>Registration of unmanned aerial vehicle laser scanning (ULS) and ground light\ndetection and ranging (LiDAR) point clouds in forests is critical to create a\ndetailed representation of a forest structure and an accurate inversion of\nforest parameters. However, forest occlusion poses challenges for marker-based\nregistration methods, and some marker-free automated registration methods have\nlow efficiency due to the process of object (e.g., tree, crown) segmentation.\nTherefore, we use a divide-and-conquer strategy and propose an automated and\nefficient method to register ULS and ground LiDAR point clouds in forests.\nRegistration involves coarse alignment and fine registration, where the coarse\nalignment of point clouds is divided into vertical and horizontal alignment.\nThe vertical alignment is achieved by ground alignment, which is achieved by\nthe transformation relationship between normal vectors of the ground point\ncloud and the horizontal plane, and the horizontal alignment is achieved by\ncanopy projection image matching. During image matching, vegetation points are\nfirst distinguished by the ground filtering algorithm, and then, vegetation\npoints are projected onto the horizontal plane to obtain two binary images. To\nmatch the two images, a matching strategy is used based on canopy shape context\nfeatures, which are described by a two-point congruent set and canopy overlap.\nFinally, we implement coarse alignment of ULS and ground LiDAR datasets by\ncombining the results of ground alignment and image matching and finish fine\nregistration. Also, the effectiveness, accuracy, and efficiency of the proposed\nmethod are demonstrated by field measurements of forest plots. Experimental\nresults show that the ULS and ground LiDAR data in different plots are\nregistered, of which the horizontal alignment errors are less than 0.02 m, and\nthe average runtime of the proposed method is less than 1 second.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jie Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1\">Peng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Lei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1\">Jiaxin Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wuming Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dissecting the impact of different loss functions with gradient surgery. (arXiv:2201.11307v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11307","description":"<p>Pair-wise loss is an approach to metric learning that learns a semantic\nembedding by optimizing a loss function that encourages images from the same\nsemantic class to be mapped closer than images from different classes. The\nliterature reports a large and growing set of variations of the pair-wise loss\nstrategies. Here we decompose the gradient of these loss functions into\ncomponents that relate to how they push the relative feature positions of the\nanchor-positive and anchor-negative pairs. This decomposition allows the\nunification of a large collection of current pair-wise loss functions.\nAdditionally, explicitly constructing pair-wise gradient updates to separate\nout these effects gives insights into which have the biggest impact, and leads\nto a simple algorithm that beats the state of the art for image retrieval on\nthe CAR, CUB and Stanford Online products datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xuan_H/0/1/0/all/0/1\">Hong Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pless_R/0/1/0/all/0/1\">Robert Pless</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Module Networks for Systematic Generalization in Visual Question Answering. (arXiv:2201.11316v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11316","description":"<p>Transformer-based models achieve great performance on Visual Question\nAnswering (VQA). However, when we evaluate them on systematic generalization,\ni.e., handling novel combinations of known concepts, their performance\ndegrades. Neural Module Networks (NMNs) are a promising approach for systematic\ngeneralization that consists on composing modules, i.e., neural networks that\ntackle a sub-task. Inspired by Transformers and NMNs, we propose Transformer\nModule Network (TMN), a novel Transformer-based model for VQA that dynamically\ncomposes modules into a question-specific Transformer network. TMNs achieve\nstate-of-the-art systematic generalization performance in three VQA datasets,\nnamely, CLEVR-CoGenT, CLOSURE and GQA-SGL, in some cases improving more than\n30% over standard Transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamada_M/0/1/0/all/0/1\">Moyuru Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAmario_V/0/1/0/all/0/1\">Vanessa D&#x27;Amario</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takemoto_K/0/1/0/all/0/1\">Kentaro Takemoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Rectification Knowledge Distillation. (arXiv:2201.11319v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11319","description":"<p>Knowledge Distillation is a technique which aims to utilize dark knowledge to\ncompress and transfer information from a vast, well-trained neural network\n(teacher model) to a smaller, less capable neural network (student model) with\nimproved inference efficiency. This approach of distilling knowledge has gained\npopularity as a result of the prohibitively complicated nature of such\ncumbersome models for deployment on edge computing devices. Generally, the\nteacher models used to teach smaller student models are cumbersome in nature\nand expensive to train. To eliminate the necessity for a cumbersome teacher\nmodel completely, we propose a simple yet effective knowledge distillation\nframework that we termed Dynamic Rectification Knowledge Distillation (DR-KD).\nOur method transforms the student into its own teacher, and if the self-teacher\nmakes wrong predictions while distilling information, the error is rectified\nprior to the knowledge being distilled. Specifically, the teacher targets are\ndynamically tweaked by the agency of ground-truth while distilling the\nknowledge gained from traditional training. Our proposed DR-KD performs\nremarkably well in the absence of a sophisticated cumbersome teacher model and\nachieves comparable performance to existing state-of-the-art teacher-free\nknowledge distillation frameworks when implemented by a low-cost dynamic\nmannered teacher. Our approach is all-encompassing and can be utilized for any\ndeep neural network training that requires categorization or object\nrecognition. DR-KD enhances the test accuracy on Tiny ImageNet by 2.65% over\nprominent baseline models, which is significantly better than any other\nknowledge distillation approach while requiring no additional training costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amik_F/0/1/0/all/0/1\">Fahad Rahman Amik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tasin_A/0/1/0/all/0/1\">Ahnaf Ismat Tasin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Silvia Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elahi_M/0/1/0/all/0/1\">M. M. Lutfe Elahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1\">Nabeel Mohammed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Transfer Learning for Holographic Image Reconstruction using a Recurrent Neural Network. (arXiv:2201.11333v1 [eess.IV])","link":"http://arxiv.org/abs/2201.11333","description":"<p>Deep learning-based methods in computational microscopy have been shown to be\npowerful but in general face some challenges due to limited generalization to\nnew types of samples and requirements for large and diverse training data.\nHere, we demonstrate a few-shot transfer learning method that helps a\nholographic image reconstruction deep neural network rapidly generalize to new\ntypes of samples using small datasets. We pre-trained a convolutional recurrent\nneural network on a large dataset with diverse types of samples, which serves\nas the backbone model. By fixing the recurrent blocks and transferring the rest\nof the convolutional blocks of the pre-trained model, we reduced the number of\ntrainable parameters by ~90% compared with standard transfer learning, while\nachieving equivalent generalization. We validated the effectiveness of this\napproach by successfully generalizing to new types of samples using small\nholographic datasets for training, and achieved (i) ~2.5-fold convergence speed\nacceleration, (ii) ~20% computation time reduction per epoch, and (iii)\nimproved reconstruction performance over baseline network models trained from\nscratch. This few-shot transfer learning approach can potentially be applied in\nother microscopic imaging methods, helping to generalize to new types of\nsamples without the need for extensive training time and data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1\">Luzhe Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xilin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tairan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozcan_A/0/1/0/all/0/1\">Aydogan Ozcan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Global Diversity and Local Context for Video Summarization. (arXiv:2201.11345v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11345","description":"<p>Video summarization aims to automatically generate a diverse and concise\nsummary which is useful in large-scale video processing. Most of methods tend\nto adopt self attention mechanism across video frames, which fails to model the\ndiversity of video frames. To alleviate this problem, we revisit the pairwise\nsimilarity measurement in self attention mechanism and find that the existing\ninner-product affinity leads to discriminative features rather than diversified\nfeatures. In light of this phenomenon, we propose global diverse attention by\nusing the squared Euclidean distance instead to compute the affinities.\nMoreover, we model the local contextual information by proposing local\ncontextual attention to remove the redundancy in the video. By combining these\ntwo attention mechanism, a video \\textbf{SUM}marization model with Diversified\nContextual Attention scheme is developed and named as SUM-DCA. Extensive\nexperiments are conducted on benchmark data sets to verify the effectiveness\nand the superiority of SUM-DCA in terms of F-score and rank-based evaluation\nwithout any bells and whistles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingchao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_O/0/1/0/all/0/1\">Ouhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinghao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongjin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenjiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guodun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxing Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Shortcut Technique for GAN. (arXiv:2201.11351v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11351","description":"<p>In recent years, generative adversarial network (GAN)-based image generation\ntechniques design their generators by stacking up multiple residual blocks. The\nresidual block generally contains a shortcut, \\ie skip connection, which\neffectively supports information propagation in the network. In this paper, we\npropose a novel shortcut method, called the gated shortcut, which not only\nembraces the strength point of the residual block but also further boosts the\nGAN performance. More specifically, based on the gating mechanism, the proposed\nmethod leads the residual block to keep (or remove) information that is\nrelevant (or irrelevant) to the image being generated. To demonstrate that the\nproposed method brings significant improvements in the GAN performance, this\npaper provides extensive experimental results on the various standard datasets\nsuch as CIFAR-10, CIFAR-100, LSUN, and tiny-ImageNet. Quantitative evaluations\nshow that the gated shortcut achieves the impressive GAN performance in terms\nof Frechet inception distance (FID) and Inception score (IS). For instance, the\nproposed method improves the FID and IS scores on the tiny-ImageNet dataset\nfrom 35.13 to 27.90 and 20.23 to 23.42, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1\">Cheol-Hwan Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1\">Yong-Goo Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Confidence Guided Distance for 3D Partial Shape Registration. (arXiv:2201.11379v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11379","description":"<p>We present a novel non-iterative learnable method for partial-to-partial 3D\nshape registration. The partial alignment task is extremely complex, as it\njointly tries to match between points and identify which points do not appear\nin the corresponding shape, causing the solution to be non-unique and ill-posed\nin most cases.\n</p>\n<p>Until now, two principal methodologies have been suggested to solve this\nproblem: sample a subset of points that are likely to have correspondences or\nperform soft alignment between the point clouds and try to avoid a match to an\noccluded part. These heuristics work when the partiality is mild or when the\ntransformation is small but fails for severe occlusions or when outliers are\npresent. We present a unique approach named Confidence Guided Distance Network\n(CGD-net), where we fuse learnable similarity between point embeddings and\nspatial distance between point clouds, inducing an optimized solution for the\noverlapping points while ignoring parts that only appear in one of the shapes.\nThe point feature generation is done by a self-supervised architecture that\nrepels far points to have different embeddings, therefore succeeds to align\npartial views of shapes, even with excessive internal symmetries or acute\nrotations. We compare our network to recently presented learning-based and\naxiomatic methods and report a fundamental boost in performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ginzburg_D/0/1/0/all/0/1\">Dvir Ginzburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raviv_D/0/1/0/all/0/1\">Dan Raviv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Embedding Distribution Refinement and Entropy-Aware Attention for 3D Point Cloud Classification. (arXiv:2201.11388v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11388","description":"<p>Learning a powerful representation from point clouds is a fundamental and\nchallenging problem in the field of computer vision. Different from images\nwhere RGB pixels are stored in the regular grid, for point clouds, the\nunderlying semantic and structural information of point clouds is the spatial\nlayout of the points. Moreover, the properties of challenging in-context and\nbackground noise pose more challenges to point cloud analysis. One assumption\nis that the poor performance of the classification model can be attributed to\nthe indistinguishable embedding feature that impedes the search for the optimal\nclassifier. This work offers a new strategy for learning powerful\nrepresentations via a contrastive learning approach that can be embedded into\nany point cloud classification network. First, we propose a supervised\ncontrastive classification method to implement embedding feature distribution\nrefinement by improving the intra-class compactness and inter-class\nseparability. Second, to solve the confusion problem caused by small\ninter-class compactness and inter-class separability. Second, to solve the\nconfusion problem caused by small inter-class variations between some\nsimilar-looking categories, we propose a confusion-prone class mining strategy\nto alleviate the confusion effect. Finally, considering that outliers of the\nsample clusters in the embedding space may cause performance degradation, we\ndesign an entropy-aware attention module with information entropy theory to\nidentify the outlier cases and the unstable samples by measuring the\nuncertainty of predicted probability. The results of extensive experiments\ndemonstrate that our method outperforms the state-of-the-art approaches by\nachieving 82.9% accuracy on the real-world ScanObjectNN dataset and substantial\nperformance gains up to 2.9% in DCGNN, 3.1% in PointNet++, and 2.4% in GBNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Feng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yichao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Q/0/1/0/all/0/1\">Qifan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Shuai Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weigong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Frame Quality Enhancement On Compressed Video Using Quantised Data of Deep Belief Networks. (arXiv:2201.11389v1 [eess.IV])","link":"http://arxiv.org/abs/2201.11389","description":"<p>In the age of streaming and surveillance compressed video enhancement has\nbecome a problem in need of constant improvement. Here, we investigate a way of\nimproving the Multi-Frame Quality Enhancement approach. This approach consists\nof making use of the frames that have the peak quality in the region to improve\nthose that have a lower quality in that region. This approach consists of\nobtaining quantized data from the videos using a deep belief network. The\nquantized data is then fed into the MF-CNN architecture to improve the\ncompressed video. We further investigate the impact of using a Bi-LSTM for\ndetecting the peak quality frames. Our approach obtains better results than the\nfirst approach of the MFQE which uses an SVM for PQF detection. On the other\nhand, our MFQE approach does not outperform the latest version of the MQFE\napproach that uses a Bi-LSTM for PQF detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chasi_D/0/1/0/all/0/1\">Dionne Takudzwa Chasi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ngxande_M/0/1/0/all/0/1\">Mkhuseli Ngxande</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalised Image Outpainting with U-Transformer. (arXiv:2201.11403v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11403","description":"<p>While most present image outpainting conducts horizontal extrapolation, we\nstudy the generalised image outpainting problem that extrapolates visual\ncontext all-side around a given image. To this end, we develop a novel\ntransformer-based generative adversarial network called U-Transformer able to\nextend image borders with plausible structure and details even for complicated\nscenery images. Specifically, we design a generator as an encoder-to-decoder\nstructure embedded with the popular Swin Transformer blocks. As such, our novel\nframework can better cope with image long-range dependencies which are\ncrucially important for generalised image outpainting. We propose additionally\na U-shaped structure and multi-view Temporal Spatial Predictor network to\nreinforce image self-reconstruction as well as unknown-part prediction smoothly\nand realistically. We experimentally demonstrate that our proposed method could\nproduce visually appealing results for generalized image outpainting against\nthe state-of-the-art image outpainting approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Penglei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1\">Yujie Geng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-linear Motion Estimation for Video Frame Interpolation using Space-time Convolutions. (arXiv:2201.11407v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11407","description":"<p>Video frame interpolation aims to synthesize one or multiple frames between\ntwo consecutive frames in a video. It has a wide range of applications\nincluding slow-motion video generation, frame-rate up-scaling and developing\nvideo codecs. Some older works tackled this problem by assuming per-pixel\nlinear motion between video frames. However, objects often follow a non-linear\nmotion pattern in the real domain and some recent methods attempt to model\nper-pixel motion by non-linear models (e.g., quadratic). A quadratic model can\nalso be inaccurate, especially in the case of motion discontinuities over time\n(i.e. sudden jerks) and occlusions, where some of the flow information may be\ninvalid or inaccurate.\n</p>\n<p>In our paper, we propose to approximate the per-pixel motion using a\nspace-time convolution network that is able to adaptively select the motion\nmodel to be used. Specifically, we are able to softly switch between a linear\nand a quadratic model. Towards this end, we use an end-to-end 3D CNN\nencoder-decoder architecture over bidirectional optical flows and occlusion\nmaps to estimate the non-linear motion model of each pixel. Further, a motion\nrefinement module is employed to refine the non-linear motion and the\ninterpolated frames are estimated by a simple warping of the neighboring frames\nwith the estimated per-pixel motion. Through a set of comprehensive\nexperiments, we validate the effectiveness of our model and show that our\nmethod outperforms state-of-the-art algorithms on four datasets (Vimeo, DAVIS,\nHD and GoPro).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Saikat Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramaniam_A/0/1/0/all/0/1\">Arulkumar Subramaniam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Anurag Mittal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DocSegTr: An Instance-Level End-to-End Document Image Segmentation Transformer. (arXiv:2201.11438v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11438","description":"<p>Understanding documents with rich layouts is an essential step towards\ninformation extraction. Business intelligence processes often require the\nextraction of useful semantic content from documents at a large scale for\nsubsequent decision-making tasks. In this context, instance-level segmentation\nof different document objects(title, sections, figures, tables and so on) has\nemerged as an interesting problem for the document layout analysis community.\nTo advance the research in this direction, we present a transformer-based model\nfor end-to-end segmentation of complex layouts in document images. To our\nknowledge, this is the first work on transformer-based document segmentation.\nExtensive experimentation on the PubLayNet dataset shows that our model\nachieved comparable or better segmentation performance than the existing\nstate-of-the-art approaches. We hope our simple and flexible framework could\nserve as a promising baseline for instance-level recognition tasks in document\nimages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Sanket Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Ayan Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1\">Josep Llad&#xf3;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1\">Umapada Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks. (arXiv:2201.11440v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11440","description":"<p>Novel and high-performance medical image classification pipelines are heavily\nutilizing ensemble learning strategies. The idea of ensemble learning is to\nassemble diverse models or multiple predictions and, thus, boost prediction\nperformance. However, it is still an open question to what extent as well as\nwhich ensemble learning strategies are beneficial in deep learning based\nmedical image classification pipelines. In this work, we proposed a\nreproducible medical image classification pipeline for analyzing the\nperformance impact of the following ensemble learning techniques: Augmenting,\nStacking, and Bagging. The pipeline consists of state-of-the-art preprocessing\nand image augmentation methods as well as 9 deep convolution neural network\narchitectures. It was applied on four popular medical imaging datasets with\nvarying complexity. Furthermore, 12 pooling functions for combining multiple\npredictions were analyzed, ranging from simple statistical functions like\nunweighted averaging up to more complex learning-based functions like support\nvector machines. Our results revealed that Stacking achieved the largest\nperformance gain of up to 13% F1-score increase. Augmenting showed consistent\nimprovement capabilities by up to 4% and is also applicable to single model\nbased pipelines. Cross-validation based Bagging demonstrated to be the most\ncomplex ensemble learning method, which resulted in an F1-score decrease in all\nanalyzed datasets (up to -10%). Furthermore, we demonstrated that simple\nstatistical pooling functions are equal or often even better than more complex\npooling functions. We concluded that the integration of Stacking and\nAugmentation ensemble learning techniques is a powerful method for any medical\nimage classification pipeline to improve robustness and boost performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_D/0/1/0/all/0/1\">Dominik M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_Rey_I/0/1/0/all/0/1\">I&#xf1;aki Soto-Rey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kramer_F/0/1/0/all/0/1\">Frank Kramer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pan-Tumor CAnine cuTaneous Cancer Histology (CATCH) Dataset. (arXiv:2201.11446v1 [eess.IV])","link":"http://arxiv.org/abs/2201.11446","description":"<p>Due to morphological similarities, the differentiation of histologic sections\nof cutaneous tumors into individual subtypes can be challenging. Recently, deep\nlearning-based approaches have proven their potential for supporting\npathologists in this regard. However, many of these supervised algorithms\nrequire a large amount of annotated data for robust development. We present a\npublicly available dataset consisting of 350 whole slide images of seven\ndifferent canine cutaneous tumors complemented by 12,424 polygon annotations\nfor 13 histologic classes including seven cutaneous tumor subtypes. Regarding\nsample size and annotation extent, this exceeds most publicly available\ndatasets which are oftentimes limited to the tumor area or merely provide\npatch-level annotations. We validated our model for tissue segmentation,\nachieving a class-averaged Jaccard coefficient of 0.7047, and 0.9044 for tumor\nin particular. For tumor subtype classification, we achieve a slide-level\naccuracy of 0.9857. Since canine cutaneous tumors possess various histologic\nhomologies to human tumors, we believe that the added value of this dataset is\nnot limited to veterinary pathology but extends to more general fields of\napplication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wilm_F/0/1/0/all/0/1\">Frauke Wilm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fragoso_M/0/1/0/all/0/1\">Marco Fragoso</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marzahl_C/0/1/0/all/0/1\">Christian Marzahl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_J/0/1/0/all/0/1\">Jingna Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bertram_C/0/1/0/all/0/1\">Christof A. Bertram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klopfleisch_R/0/1/0/all/0/1\">Robert Klopfleisch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aubreville_M/0/1/0/all/0/1\">Marc Aubreville</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In Defense of Kalman Filtering for Polyp Tracking from Colonoscopy Videos. (arXiv:2201.11450v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11450","description":"<p>Real-time and robust automatic detection of polyps from colonoscopy videos\nare essential tasks to help improve the performance of doctors during this\nexam. The current focus of the field is on the development of accurate but\ninefficient detectors that will not enable a real-time application. We advocate\nthat the field should instead focus on the development of simple and efficient\ndetectors that an be combined with effective trackers to allow the\nimplementation of real-time polyp detectors. In this paper, we propose a Kalman\nfiltering tracker that can work together with powerful, but efficient\ndetectors, enabling the implementation of real-time polyp detectors. In\nparticular, we show that the combination of our Kalman filtering with the\ndetector PP-YOLO shows state-of-the-art (SOTA) detection accuracy and real-time\nprocessing. More specifically, our approach has SOTA results on the\nCVC-ClinicDB dataset, with a recall of 0.740, precision of 0.869, $F_1$ score\nof 0.799, an average precision (AP) of 0.837, and can run in real time (i.e.,\n30 frames per second). We also evaluate our method on a subset of the\nHyper-Kvasir annotated by our clinical collaborators, resulting in SOTA\nresults, with a recall of 0.956, precision of 0.875, $F_1$ score of 0.914, AP\nof 0.952, and can run in real time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Butler_D/0/1/0/all/0/1\">David Butler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tim Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seon Ho Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rajvinder Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RelTR: Relation Transformer for Scene Graph Generation. (arXiv:2201.11460v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11460","description":"<p>Different objects in the same scene are more or less related to each other,\nbut only a limited number of these relationships are noteworthy. Inspired by\nDETR, which excels in object detection, we view scene graph generation as a set\nprediction problem and propose an end-to-end scene graph generation model RelTR\nwhich has an encoder-decoder architecture. The encoder reasons about the visual\nfeature context while the decoder infers a fixed-size set of triplets\nsubject-predicate-object using different types of attention mechanisms with\ncoupled subject and object queries. We design a set prediction loss performing\nthe matching between the ground truth and predicted triplets for the end-to-end\ntraining. In contrast to most existing scene graph generation methods, RelTR is\na one-stage method that predicts a set of relationships directly only using\nvisual appearance without combining entities and labeling all possible\npredicates. Extensive experiments on the Visual Genome and Open Images V6\ndatasets demonstrate the superior performance and fast inference of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1\">Yuren Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Michael Ying Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eye-focused Detection of Bell's Palsy in Videos. (arXiv:2201.11479v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11479","description":"<p>In this paper, we present how Bell's Palsy, a neurological disorder, can be\ndetected just from a subject's eyes in a video. We notice that Bell's Palsy\npatients often struggle to blink their eyes on the affected side. As a result,\nwe can observe a clear contrast between the blinking patterns of the two eyes.\nAlthough previous works did utilize images/videos to detect this disorder, none\nhave explicitly focused on the eyes. Most of them require the entire face. One\nobvious advantage of having an eye-focused detection system is that subjects'\nanonymity is not at risk. Also, our AI decisions based on simple blinking\npatterns make them explainable and straightforward. Specifically, we develop a\nnovel feature called blink similarity, which measures the similarity between\nthe two blinking patterns. Our extensive experiments demonstrate that the\nproposed feature is quite robust, for it helps in Bell's Palsy detection even\nwith very few labels. Our proposed eye-focused detection system is not only\ncheaper but also more convenient than several existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ansari_S/0/1/0/all/0/1\">Sharik Ali Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jerripothula_K/0/1/0/all/0/1\">Koteswar Rao Jerripothula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagpal_P/0/1/0/all/0/1\">Pragya Nagpal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Ankush Mittal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Head and eye egocentric gesture recognition for human-robot interaction using eyewear cameras. (arXiv:2201.11500v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11500","description":"<p>Non-verbal communication plays a particularly important role in a wide range\nof scenarios in Human-Robot Interaction (HRI). Accordingly, this work addresses\nthe problem of human gesture recognition. In particular, we focus on head and\neye gestures, and adopt an egocentric (first-person) perspective using eyewear\ncameras. We argue that this egocentric view offers a number of conceptual and\ntechnical benefits over scene- or robot-centric perspectives.\n</p>\n<p>A motion-based recognition approach is proposed, which operates at two\ntemporal granularities. Locally, frame-to-frame homographies are estimated with\na convolutional neural network (CNN). The output of this CNN is input to a long\nshort-term memory (LSTM) to capture longer-term temporal visual relationships,\nwhich are relevant to characterize gestures.\n</p>\n<p>Regarding the configuration of the network architecture, one particularly\ninteresting finding is that using the output of an internal layer of the\nhomography CNN increases the recognition rate with respect to using the\nhomography matrix itself. While this work focuses on action recognition, and no\nrobot or user study has been conducted yet, the system has been de signed to\nmeet real-time constraints. The encouraging results suggest that the proposed\negocentric perspective is viable, and this proof-of-concept work provides novel\nand useful contributions to the exciting area of HRI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marina_Miranda_J/0/1/0/all/0/1\">Javier Marina-Miranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Traver_V/0/1/0/all/0/1\">V. Javier Traver</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly Detection in Retinal Images using Multi-Scale Deep Feature Sparse Coding. (arXiv:2201.11506v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11506","description":"<p>Convolutional Neural Network models have successfully detected retinal\nillness from optical coherence tomography (OCT) and fundus images. These CNN\nmodels frequently rely on vast amounts of labeled data for training, difficult\nto obtain, especially for rare diseases. Furthermore, a deep learning system\ntrained on a data set with only one or a few diseases cannot detect other\ndiseases, limiting the system's practical use in disease identification. We\nhave introduced an unsupervised approach for detecting anomalies in retinal\nimages to overcome this issue. We have proposed a simple, memory efficient,\neasy to train method which followed a multi-step training technique that\nincorporated autoencoder training and Multi-Scale Deep Feature Sparse Coding\n(MDFSC), an extended version of normal sparse coding, to accommodate diverse\ntypes of retinal datasets. We achieve relative AUC score improvement of 7.8\\%,\n6.7\\% and 12.1\\% over state-of-the-art SPADE on Eye-Q, IDRiD and OCTID datasets\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sourya Dipta Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Saikat Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nisarg A. Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahapatra_D/0/1/0/all/0/1\">Dwarikanath Mahapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Density-Aware Hyper-Graph Neural Networks for Graph-based Semi-supervised Node Classification. (arXiv:2201.11511v1 [cs.LG])","link":"http://arxiv.org/abs/2201.11511","description":"<p>Graph-based semi-supervised learning, which can exploit the connectivity\nrelationship between labeled and unlabeled data, has been shown to outperform\nthe state-of-the-art in many artificial intelligence applications. One of the\nmost challenging problems for graph-based semi-supervised node classification\nis how to use the implicit information among various data to improve the\nperformance of classifying. Traditional studies on graph-based semi-supervised\nlearning have focused on the pairwise connections among data. However, the data\ncorrelation in real applications could be beyond pairwise and more complicated.\nThe density information has been demonstrated to be an important clue, but it\nis rarely explored in depth among existing graph-based semi-supervised node\nclassification methods. To develop a flexible and effective model for\ngraph-based semi-supervised node classification, we propose a novel\nDensity-Aware Hyper-Graph Neural Networks (DA-HGNN). In our proposed approach,\nhyper-graph is provided to explore the high-order semantic correlation among\ndata, and a density-aware hyper-graph attention network is presented to explore\nthe high-order connection relationship. Extensive experiments are conducted in\nvarious benchmark datasets, and the results demonstrate the effectiveness of\nthe proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jianpeng Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Q/0/1/0/all/0/1\">Qian Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ResiDualGAN: Resize-Residual DualGAN for Cross-Domain Remote Sensing Images Semantic Segmentation. (arXiv:2201.11523v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11523","description":"<p>The performance of a semantic segmentation model for remote sensing (RS)\nimages pretrained on an annotated dataset would greatly decrease when testing\non another unannotated dataset because of the domain gap. Adversarial\ngenerative methods, e.g., DualGAN, are utilized for unpaired image-to-image\ntranslation to minimize the pixel-level domain gap, which is one of the common\napproaches for unsupervised domain adaptation (UDA). However, existing image\ntranslation methods are facing two problems when performing RS images\ntranslation: 1) ignoring the scale discrepancy between two RS datasets which\ngreatly affect the accuracy performance of scale-invariant objects, 2) ignoring\nthe characteristic of real-to-real translation of RS images which brings an\nunstable factor for the training of the models. In this paper, ResiDualGAN is\nproposed for RS images translation, where a resizer module is used for\naddressing the scale discrepancy of RS datasets, and a residual connection is\nused for strengthening the stability of real-to-real images translation and\nimproving the performance in cross-domain semantic segmentation tasks.\nCombining with an output space adaptation method, the proposed method greatly\nimproves the accuracy performance on common benchmarks, which demonstrates the\nsuperiority and reliability of ResiDuanGAN. At the end of the paper, a thorough\ndiscussion is also conducted to give a reasonable explanation for the\nimprovement of ResiDualGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Han Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Peng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zihao Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains. (arXiv:2201.11528v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11528","description":"<p>Adversarial examples have posed a severe threat to deep neural networks due\nto their transferable nature. Currently, various works have paid great efforts\nto enhance the cross-model transferability, which mostly assume the substitute\nmodel is trained in the same domain as the target model. However, in reality,\nthe relevant information of the deployed model is unlikely to leak. Hence, it\nis vital to build a more practical black-box threat model to overcome this\nlimitation and evaluate the vulnerability of deployed models. In this paper,\nwith only the knowledge of the ImageNet domain, we propose a Beyond ImageNet\nAttack (BIA) to investigate the transferability towards black-box domains\n(unknown classification tasks). Specifically, we leverage a generative model to\nlearn the adversarial function for disrupting low-level features of input\nimages. Based on this framework, we further propose two variants to narrow the\ngap between the source and target domains from the data and model perspectives,\nrespectively. Extensive experiments on coarse-grained and fine-grained domains\ndemonstrate the effectiveness of our proposed methods. Notably, our methods\noutperform state-of-the-art approaches by up to 7.71\\% (towards coarse-grained\ndomains) and 25.91\\% (towards fine-grained domains) on average. Our code is\navailable at \\url{https://github.com/qilong-zhang/Beyond-ImageNet-Attack}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaodan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASOC: Adaptive Self-aware Object Co-localization. (arXiv:2201.11547v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11547","description":"<p>The primary goal of this paper is to localize objects in a group of\nsemantically similar images jointly, also known as the object co-localization\nproblem. Most related existing works are essentially weakly-supervised, relying\nprominently on the neighboring images' weak-supervision. Although weak\nsupervision is beneficial, it is not entirely reliable, for the results are\nquite sensitive to the neighboring images considered. In this paper, we combine\nit with a self-awareness phenomenon to mitigate this issue. By self-awareness\nhere, we refer to the solution derived from the image itself in the form of\nsaliency cue, which can also be unreliable if applied alone. Nevertheless,\ncombining these two paradigms together can lead to a better co-localization\nability. Specifically, we introduce a dynamic mediator that adaptively strikes\na proper balance between the two static solutions to provide an optimal\nsolution. Therefore, we call this method \\textit{ASOC}: Adaptive Self-aware\nObject Co-localization. We perform exhaustive experiments on several benchmark\ndatasets and validate that weak-supervision supplemented with self-awareness\nhas superior performance outperforming several compared competing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jerripothula_K/0/1/0/all/0/1\">Koteswar Rao Jerripothula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_P/0/1/0/all/0/1\">Prerana Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Probabilistic Framework for Dynamic Object Recognition in 3D Environment With A Novel Continuous Ground Estimation Method. (arXiv:2201.11608v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11608","description":"<p>In this thesis a probabilistic framework is developed and proposed for\nDynamic Object Recognition in 3D Environments. A software package is developed\nusing C++ and Python in ROS that performs the detection and tracking task.\nFurthermore, a novel Gaussian Process Regression (GPR) based method is\ndeveloped to detect ground points in different urban scenarios of regular,\nsloped and rough. The ground surface behavior is assumed to only demonstrate\nlocal input-dependent smoothness. kernel's length-scales are obtained. Bayesian\ninference is implemented sing \\textit{Maximum a Posteriori} criterion. The\nlog-marginal likelihood function is assumed to be a multi-task objective\nfunction, to represent a whole-frame unbiased view of the ground at each frame\nbecause adjacent segments may not have similar ground structure in an uneven\nscene while having shared hyper-parameter values. Simulation results shows the\neffectiveness of the proposed method in uneven and rough scenes which\noutperforms similar Gaussian process based ground segmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehrabi_P/0/1/0/all/0/1\">Pouria Mehrabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain generalization in deep learning-based mass detection in mammography: A large-scale multi-center study. (arXiv:2201.11620v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11620","description":"<p>Computer-aided detection systems based on deep learning have shown great\npotential in breast cancer detection. However, the lack of domain\ngeneralization of artificial neural networks is an important obstacle to their\ndeployment in changing clinical environments. In this work, we explore the\ndomain generalization of deep learning methods for mass detection in digital\nmammography and analyze in-depth the sources of domain shift in a large-scale\nmulti-center setting. To this end, we compare the performance of eight\nstate-of-the-art detection methods, including Transformer-based models, trained\nin a single domain and tested in five unseen domains. Moreover, a single-source\nmass detection training pipeline is designed to improve the domain\ngeneralization without requiring images from the new domain. The results show\nthat our workflow generalizes better than state-of-the-art transfer\nlearning-based approaches in four out of five domains while reducing the domain\nshift caused by the different acquisition protocols and scanner manufacturers.\nSubsequently, an extensive analysis is performed to identify the covariate\nshifts with bigger effects on the detection performance, such as due to\ndifferences in patient age, breast density, mass size, and mass malignancy.\nUltimately, this comprehensive study provides key insights and best practices\nfor future research on domain generalization in deep learning-based breast\ncancer detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garrucho_L/0/1/0/all/0/1\">Lidia Garrucho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushibar_K/0/1/0/all/0/1\">Kaisar Kushibar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jouide_S/0/1/0/all/0/1\">Socayna Jouide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_O/0/1/0/all/0/1\">Oliver Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igual_L/0/1/0/all/0/1\">Laura Igual</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Classification of Neuromuscular Diseases in Children Using Photoacoustic Imaging. (arXiv:2201.11630v1 [eess.IV])","link":"http://arxiv.org/abs/2201.11630","description":"<p>Neuromuscular diseases (NMDs) cause a significant burden for both healthcare\nsystems and society. They can lead to severe progressive muscle weakness,\nmuscle degeneration, contracture, deformity and progressive disability. The\nNMDs evaluated in this study often manifest in early childhood. As subtypes of\ndisease, e.g. Duchenne Muscular Dystropy (DMD) and Spinal Muscular Atrophy\n(SMA), are difficult to differentiate at the beginning and worsen quickly, fast\nand reliable differential diagnosis is crucial. Photoacoustic and ultrasound\nimaging has shown great potential to visualize and quantify the extent of\ndifferent diseases. The addition of automatic classification of such image data\ncould further improve standard diagnostic procedures. We compare deep\nlearning-based 2-class and 3-class classifiers based on VGG16 for\ndifferentiating healthy from diseased muscular tissue. This work shows\npromising results with high accuracies above 0.86 for the 3-class problem and\ncan be used as a proof of concept for future approaches for earlier diagnosis\nand therapeutic monitoring of NMDs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Schlereth_M/0/1/0/all/0/1\">Maja Schlereth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stromer_D/0/1/0/all/0/1\">Daniel Stromer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wagner_A/0/1/0/all/0/1\">Alexandra Wagner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_L/0/1/0/all/0/1\">Lina Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Knieling_F/0/1/0/all/0/1\">Ferdinand Knieling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Video Prior for Video Consistency and Propagation. (arXiv:2201.11632v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11632","description":"<p>Applying an image processing algorithm independently to each video frame\noften leads to temporal inconsistency in the resulting video. To address this\nissue, we present a novel and general approach for blind video temporal\nconsistency. Our method is only trained on a pair of original and processed\nvideos directly instead of a large dataset. Unlike most previous methods that\nenforce temporal consistency with optical flow, we show that temporal\nconsistency can be achieved by training a convolutional neural network on a\nvideo with Deep Video Prior (DVP). Moreover, a carefully designed iteratively\nreweighted training strategy is proposed to address the challenging multimodal\ninconsistency problem. We demonstrate the effectiveness of our approach on 7\ncomputer vision tasks on videos. Extensive quantitative and perceptual\nexperiments show that our approach obtains superior performance than\nstate-of-the-art methods on blind video temporal consistency. We further extend\nDVP to video propagation and demonstrate its effectiveness in propagating three\ndifferent types of information (color, artistic style, and object\nsegmentation). A progressive propagation strategy with pseudo labels is also\nproposed to enhance DVP's performance on video propagation. Our source codes\nare publicly available at https://github.com/ChenyangLEI/deep-video-prior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1\">Chenyang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1\">Yazhou Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_H/0/1/0/all/0/1\">Hao Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Team Yao at Factify 2022: Utilizing Pre-trained Models and Co-attention Networks for Multi-Modal Fact Verification. (arXiv:2201.11664v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11664","description":"<p>In recent years, social media has enabled users to get exposed to a myriad of\nmisinformation and disinformation; thus, misinformation has attracted a great\ndeal of attention in research fields and as a social issue. To address the\nproblem, we propose a framework, Pre-CoFact, composed of two pre-trained models\nfor extracting features from text and images, and multiple co-attention\nnetworks for fusing the same modality but different sources and different\nmodalities. Besides, we adopt the ensemble method by using different\npre-trained models in Pre-CoFact to achieve better performance. We further\nillustrate the effectiveness from the ablation study and examine different\npre-trained models for comparison. Our team, Yao, won the fifth prize\n(F1-score: 74.585\\%) in the Factify challenge hosted by De-Factify @ AAAI 2022,\nwhich demonstrates that our model achieved competitive performance without\nusing auxiliary tasks or extra information. The source code of our work is\npublicly available at\nhttps://github.com/wywyWang/Multi-Modal-Fact-Verification-2021\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei-Yao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wen-Chih Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Checklist: Towards Testable Error Analysis of Image Models to Help System Designers Interrogate Model Capabilities. (arXiv:2201.11674v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11674","description":"<p>Using large pre-trained models for image recognition tasks is becoming\nincreasingly common owing to the well acknowledged success of recent models\nlike vision transformers and other CNN-based models like VGG and Resnet. The\nhigh accuracy of these models on benchmark tasks has translated into their\npractical use across many domains including safety-critical applications like\nautonomous driving and medical diagnostics. Despite their widespread use, image\nmodels have been shown to be fragile to changes in the operating environment,\nbringing their robustness into question. There is an urgent need for methods\nthat systematically characterise and quantify the capabilities of these models\nto help designers understand and provide guarantees about their safety and\nrobustness. In this paper, we propose Vision Checklist, a framework aimed at\ninterrogating the capabilities of a model in order to produce a report that can\nbe used by a system designer for robustness evaluations. This framework\nproposes a set of perturbation operations that can be applied on the underlying\ndata to generate test samples of different types. The perturbations reflect\npotential changes in operating environments, and interrogate various properties\nranging from the strictly quantitative to more qualitative. Our framework is\nevaluated on multiple datasets like Tinyimagenet, CIFAR10, CIFAR100 and\nCamelyon17 and for models like ViT and Resnet. Our Vision Checklist proposes a\nspecific set of evaluations that can be integrated into the previously proposed\nconcept of a model card. Robustness evaluations like our checklist will be\ncrucial in future safety evaluations of visual perception modules, and be\nuseful for a wide range of stakeholders including designers, deployers, and\nregulators involved in the certification of these systems. Source code of\nVision Checklist would be open for public use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xin Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Legastelois_B/0/1/0/all/0/1\">Benedicte Legastelois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesh_B/0/1/0/all/0/1\">Bhargavi Ganesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_A/0/1/0/all/0/1\">Ajitha Rajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chockler_H/0/1/0/all/0/1\">Hana Chockler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belle_V/0/1/0/all/0/1\">Vaishak Belle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_S/0/1/0/all/0/1\">Stuart Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthy_S/0/1/0/all/0/1\">Subramanian Ramamoorthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Change Detection using DRE-CUSUM. (arXiv:2201.11678v1 [cs.LG])","link":"http://arxiv.org/abs/2201.11678","description":"<p>This paper presents DRE-CUSUM, an unsupervised density-ratio estimation (DRE)\nbased approach to determine statistical changes in time-series data when no\nknowledge of the pre-and post-change distributions are available. The core idea\nbehind the proposed approach is to split the time-series at an arbitrary point\nand estimate the ratio of densities of distribution (using a parametric model\nsuch as a neural network) before and after the split point. The DRE-CUSUM\nchange detection statistic is then derived from the cumulative sum (CUSUM) of\nthe logarithm of the estimated density ratio. We present a theoretical\njustification as well as accuracy guarantees which show that the proposed\nstatistic can reliably detect statistical changes, irrespective of the split\npoint. While there have been prior works on using density ratio based methods\nfor change detection, to the best of our knowledge, this is the first\nunsupervised change detection approach with a theoretical justification and\naccuracy guarantees. The simplicity of the proposed framework makes it readily\napplicable in various practical settings (including high-dimensional\ntime-series data); we also discuss generalizations for online change detection.\nWe experimentally show the superiority of DRE-CUSUM using both synthetic and\nreal-world datasets over existing state-of-the-art unsupervised algorithms\n(such as Bayesian online change detection, its variants as well as several\nother heuristic methods).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adiga_S/0/1/0/all/0/1\">Sudarshan Adiga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_R/0/1/0/all/0/1\">Ravi Tandon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DropNAS: Grouped Operation Dropout for Differentiable Architecture Search. (arXiv:2201.11679v1 [cs.LG])","link":"http://arxiv.org/abs/2201.11679","description":"<p>Neural architecture search (NAS) has shown encouraging results in automating\nthe architecture design. Recently, DARTS relaxes the search process with a\ndifferentiable formulation that leverages weight-sharing and SGD where all\ncandidate operations are trained simultaneously. Our empirical results show\nthat such procedure results in the co-adaption problem and Matthew Effect:\noperations with fewer parameters would be trained maturely earlier. This causes\ntwo problems: firstly, the operations with more parameters may never have the\nchance to express the desired function since those with less have already done\nthe job; secondly, the system will punish those underperforming operations by\nlowering their architecture parameter, and they will get smaller loss\ngradients, which causes the Matthew Effect. In this paper, we systematically\nstudy these problems and propose a novel grouped operation dropout algorithm\nnamed DropNAS to fix the problems with DARTS. Extensive experiments demonstrate\nthat DropNAS solves the above issues and achieves promising performance.\nSpecifically, DropNAS achieves 2.26% test error on CIFAR-10, 16.39% on\nCIFAR-100 and 23.4% on ImageNet (with the same training hyperparameters as\nDARTS for a fair comparison). It is also observed that DropNAS is robust across\nvariants of the DARTS search space. Code is available at\nhttps://github.com/wiljohnhong/DropNAS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1\">Weijun Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Ruiming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constrained Structure Learning for Scene Graph Generation. (arXiv:2201.11697v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11697","description":"<p>As a structured prediction task, scene graph generation aims to build a\nvisually-grounded scene graph to explicitly model objects and their\nrelationships in an input image. Currently, the mean field variational Bayesian\nframework is the de facto methodology used by the existing methods, in which\nthe unconstrained inference step is often implemented by a message passing\nneural network. However, such formulation fails to explore other inference\nstrategies, and largely ignores the more general constrained optimization\nmodels. In this paper, we present a constrained structure learning method, for\nwhich an explicit constrained variational inference objective is proposed.\nInstead of applying the ubiquitous message-passing strategy, a generic\nconstrained optimization method - entropic mirror descent - is utilized to\nsolve the constrained variational inference step. We validate the proposed\ngeneric model on various popular scene graph generation benchmarks and show\nthat it outperforms the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bober_M/0/1/0/all/0/1\">Miroslaw Bober</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1\">Josef Kittler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matched Illumination. (arXiv:2201.11700v1 [eess.IV])","link":"http://arxiv.org/abs/2201.11700","description":"<p>In previous work, it was shown that a camera can theoretically be made more\ncolorimetric - its RGBs become more linearly related to XYZ tristimuli - by\nplacing a specially designed color filter in the optical path. While the prior\nart demonstrated the principle, the optimal color-correction filters were not\nactually manufactured. In this paper, we provide a novel way of creating the\ncolor filtering effect without making a physical filter: we modulate the\nspectrum of the light source by using a spectrally tunable lighting system to\nrecast the prefiltering effect from a lighting perspective. According to our\nmethod, if we wish to measure color under a D65 light, we relight the scene\nwith a modulated D65 spectrum where the light modulation mimics the effect of\ncolor prefiltering in the prior art. We call our optimally modulated light, the\nmatched illumination. In the experiments, using synthetic and real\nmeasurements, we show that color measurement errors can be reduced by about 50%\nor more on simulated data and 25% or more on real images when the matched\nillumination is used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuteng Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Finlayson_G/0/1/0/all/0/1\">Graham D. Finlayson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Study of Bias Amplification. (arXiv:2201.11706v1 [cs.LG])","link":"http://arxiv.org/abs/2201.11706","description":"<p>Recent research suggests that predictions made by machine-learning models can\namplify biases present in the training data. When a model amplifies bias, it\nmakes certain predictions at a higher rate for some groups than expected based\non training-data statistics. Mitigating such bias amplification requires a deep\nunderstanding of the mechanics in modern machine learning that give rise to\nthat amplification. We perform the first systematic, controlled study into when\nand how bias amplification occurs. To enable this study, we design a simple\nimage-classification problem in which we can tightly control (synthetic)\nbiases. Our study of this problem reveals that the strength of bias\namplification is correlated to measures such as model accuracy, model capacity,\nmodel overconfidence, and amount of training data. We also find that bias\namplification can vary greatly during training. Finally, we find that bias\namplification may depend on the difficulty of the classification task relative\nto the difficulty of recognizing group membership: bias amplification appears\nto occur primarily when it is easier to recognize group membership than class\nmembership. Our results suggest best practices for training machine-learning\nmodels that we hope will help pave the way for the development of better\nmitigation strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hall_M/0/1/0/all/0/1\">Melissa Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maaten_L/0/1/0/all/0/1\">Laurens van der Maaten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gustafson_L/0/1/0/all/0/1\">Laura Gustafson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adcock_A/0/1/0/all/0/1\">Aaron Adcock</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages. (arXiv:2201.11732v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11732","description":"<p>Reliable evaluation benchmarks designed for replicability and\ncomprehensiveness have driven progress in machine learning. Due to the lack of\na multilingual benchmark, however, vision-and-language research has mostly\nfocused on English language tasks. To fill this gap, we introduce the\nImage-Grounded Language Understanding Evaluation benchmark. IGLUE brings\ntogether - by both aggregating pre-existing datasets and creating new ones -\nvisual question answering, cross-modal retrieval, grounded reasoning, and\ngrounded entailment tasks across 20 diverse languages. Our benchmark enables\nthe evaluation of multilingual multimodal models for transfer learning, not\nonly in a zero-shot setting, but also in newly defined few-shot learning\nsetups. Based on the evaluation of the available state-of-the-art models, we\nfind that translate-test transfer is superior to zero-shot transfer and that\nfew-shot learning is hard to harness for many tasks. Moreover, downstream\nperformance is partially explained by the amount of available unlabelled\ntextual data for pretraining, and only weakly by the typological distance of\ntarget-source languages. We hope to encourage future research efforts in this\narea by releasing the benchmark to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Maria Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ranking Info Noise Contrastive Estimation: Boosting Contrastive Learning via Ranked Positives. (arXiv:2201.11736v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11736","description":"<p>This paper introduces Ranking Info Noise Contrastive Estimation (RINCE), a\nnew member in the family of InfoNCE losses that preserves a ranked ordering of\npositive samples. In contrast to the standard InfoNCE loss, which requires a\nstrict binary separation of the training pairs into similar and dissimilar\nsamples, RINCE can exploit information about a similarity ranking for learning\na corresponding embedding space. We show that the proposed loss function learns\nfavorable embeddings compared to the standard InfoNCE whenever at least noisy\nranking information can be obtained or when the definition of positives and\nnegatives is blurry. We demonstrate this for a supervised classification task\nwith additional superclass labels and noisy similarity scores. Furthermore, we\nshow that RINCE can also be applied to unsupervised training with experiments\non unsupervised representation learning from videos. In particular, the\nembedding yields higher classification accuracy, retrieval rates and performs\nbetter in out-of-distribution detection than the standard InfoNCE loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_D/0/1/0/all/0/1\">David T. Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behrmann_N/0/1/0/all/0/1\">Nadine Behrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1\">Thomas Brox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noroozi_M/0/1/0/all/0/1\">Mehdi Noroozi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PRNU Based Source Camera Identification for Webcam and Smartphone Videos. (arXiv:2201.11737v1 [eess.IV])","link":"http://arxiv.org/abs/2201.11737","description":"<p>This communication is about an application of image forensics where we use\ncamera sensor fingerprints to identify source camera (SCI: Source Camera\nIdentification) in webcam/smartphone videos. Sensor or camera fingerprints are\nbased on computing the intrinsic noise that is always present in this kind of\nsensors due to manufacturing imperfections. This is an unavoidable\ncharacteristic that links each sensor with its noise pattern. PRNU (Photo\nResponse Non-Uniformity) has become the default technique to compute a camera\nfingerprint. There are many applications nowadays dealing with PRNU patterns\nfor camera identification using still images. In this work we focus on video,\nfirst on webcam video and afterwards on smartphone video. Webcams and\nsmartphones are the most used video cameras nowadays. Three possible methods\nfor SCI are implemented and assessed in this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Martin_Rodriguez_F/0/1/0/all/0/1\">Fernando Mart&#xed;n-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Isasi_de_Vicente_F/0/1/0/all/0/1\">Fernando Isasi-de-Vicente</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Agents in Networks: Estimation and Targeting. (arXiv:1808.04878v3 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/1808.04878","description":"<p>We consider a network of agents. Associated with each agent are her covariate\nand outcome. Agents influence each other's outcomes according to a certain\nconnection/influence structure. A subset of the agents participate on a\nplatform, and hence, are observable to it. The rest are not observable to the\nplatform and are called the latent agents. The platform does not know the\ninfluence structure of the observable or the latent parts of the network. It\nonly observes the data on past covariates and decisions of the observable\nagents. Observable agents influence each other both directly and indirectly\nthrough the influence they exert on the latent agents.\n</p>\n<p>We investigate how the platform can estimate the dependence of the observable\nagents' outcomes on their covariates, taking the latent agents into account.\nFirst, we show that this relationship can be succinctly captured by a matrix\nand provide an algorithm for estimating it under a suitable approximate\nsparsity condition using historical data of covariates and outcomes for the\nobservable agents. We also obtain convergence rates for the proposed estimator\ndespite the high dimensionality that allows more agents than observations.\nSecond, we show that the approximate sparsity condition holds under the\nstandard conditions used in the literature. Hence, our results apply to a large\nclass of networks. Finally, we apply our results to two practical settings:\ntargeted advertising and promotional pricing. We show that by using the\navailable historical data with our estimator, it is possible to obtain\nasymptotically optimal advertising/pricing decisions, despite the presence of\nlatent agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ata_B/0/1/0/all/0/1\">Baris Ata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belloni_A/0/1/0/all/0/1\">Alexandre Belloni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Candogan_O/0/1/0/all/0/1\">Ozan Candogan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis and algorithms for $\\ell_p$-based semi-supervised learning on graphs. (arXiv:1901.05031v4 [math.NA] UPDATED)","link":"http://arxiv.org/abs/1901.05031","description":"<p>This paper addresses theory and applications of $\\ell_p$-based Laplacian\nregularization in semi-supervised learning. The graph $p$-Laplacian for $p&gt;2$\nhas been proposed recently as a replacement for the standard ($p=2$) graph\nLaplacian in semi-supervised learning problems with very few labels, where\nLaplacian learning is degenerate.\n</p>\n<p>In the first part of the paper we prove new discrete to continuum convergence\nresults for $p$-Laplace problems on $k$-nearest neighbor ($k$-NN) graphs, which\nare more commonly used in practice than random geometric graphs. Our analysis\nshows that, on $k$-NN graphs, the $p$-Laplacian retains information about the\ndata distribution as $p\\to \\infty$ and Lipschitz learning ($p=\\infty$) is\nsensitive to the data distribution. This situation can be contrasted with\nrandom geometric graphs, where the $p$-Laplacian forgets the data distribution\nas $p\\to \\infty$. We also present a general framework for proving discrete to\ncontinuum convergence results in graph-based learning that only requires\npointwise consistency and monotonicity.\n</p>\n<p>In the second part of the paper, we develop fast algorithms for solving the\nvariational and game-theoretic $p$-Laplace equations on weighted graphs for\n$p&gt;2$. We present several efficient and scalable algorithms for both\nformulations, and present numerical results on synthetic data indicating their\nconvergence properties. Finally, we conduct extensive numerical experiments on\nthe MNIST, FashionMNIST and EMNIST datasets that illustrate the effectiveness\nof the $p$-Laplacian formulation for semi-supervised learning with few labels.\nIn particular, we find that Lipschitz learning ($p=\\infty$) performs well with\nvery few labels on $k$-NN graphs, which experimentally validates our\ntheoretical findings that Lipschitz learning retains information about the data\ndistribution (the unlabeled data) on $k$-NN graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Flores_M/0/1/0/all/0/1\">Mauricio Flores</a>, <a href=\"http://arxiv.org/find/math/1/au:+Calder_J/0/1/0/all/0/1\">Jeff Calder</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lerman_G/0/1/0/all/0/1\">Gilad Lerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Adjacency Matrix for Video Relocalization. (arXiv:2008.08977v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.08977","description":"<p>In this paper, we continue our work on video relocalization task. Based on\nusing graph convolution to extract intra-video and inter-video frame features,\nwe improve the method by using similarity-metric based graph convolution, whose\nweighted adjacency matrix is achieved by calculating similarity metric between\nfeatures of any two different time steps in the graph. Experiments on\nActivityNet v1.2 and Thumos14 dataset show the effectiveness of this\nimprovement, and it outperforms the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruolin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_S/0/1/0/all/0/1\">Shuwei Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Colorization: A Survey and Dataset. (arXiv:2008.10774v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.10774","description":"<p>Image colorization is the process of estimating RGB colors for grayscale\nimages or video frames to improve their aesthetic and perceptual quality. Deep\nlearning techniques for image colorization have progressed notably over the\nlast decade, calling the need for a systematic survey and benchmarking of these\ntechniques. This article presents a comprehensive survey of recent\nstate-of-the-art deep learning-based image colorization techniques, describing\ntheir fundamental block architectures, inputs, optimizers, loss functions,\ntraining protocols, and training data \\textit{etc.} It categorizes the existing\ncolorization techniques into seven classes and discusses important factors\ngoverning their performance, such as benchmark datasets and evaluation metrics.\nWe highlight the limitations of existing datasets and introduce a new dataset\nspecific to colorization. Using the existing datasets and our new one, we\nperform an extensive experimental evaluation of existing image colorization\nmethods. Finally, we discuss the limitations of existing methods and recommend\npossible solutions as well as future research directions for this rapidly\nevolving topic of deep image colorization. Dataset and codes for evaluation are\npublicly available at https://github.com/saeed-anwar/ColorSurvey\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tahir_M/0/1/0/all/0/1\">Muhammad Tahir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chongyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muzaffar_A/0/1/0/all/0/1\">Abdul Wahab Muzaffar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can we Generalize and Distribute Private Representation Learning?. (arXiv:2010.01792v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.01792","description":"<p>We study the problem of learning representations that are private yet\ninformative i.e., provide information about intended \"ally\" targets while\nhiding sensitive \"adversary\" attributes). We propose Exclusion-Inclusion\nGenerative Adversarial Network (EIGAN), a generalized private representation\nlearning (PRL) architecture that accounts for multiple ally and adversary\nattributes unlike existing PRL solutions. While centrally-aggregated dataset is\na prerequisite for most PRL techniques, data in real-world is often siloed\nacross multiple distributed nodes unwilling to share the raw data because of\nprivacy concerns. We address this practical constraint by developing D-EIGAN,\nthe first distributed PRL method that learns representations at each node\nwithout transmitting the source data. We theoretically analyze the behavior of\nadversaries under the optimal EIGAN and D-EIGAN encoders and the impact of\ndependencies among ally and adversary tasks on the optimization objective. Our\nexperiments on various datasets demonstrate the advantages of EIGAN in terms of\nperformance, robustness, and scalability. In particular, EIGAN outperforms the\nprevious state-of-the-art by a significant accuracy margin (47% improvement),\nand D-EIGAN's performance is consistently on par with EIGAN under different\nnetwork settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azam_S/0/1/0/all/0/1\">Sheikh Shams Azam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taejin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseinalipour_S/0/1/0/all/0/1\">Seyyedali Hosseinalipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joe_Wong_C/0/1/0/all/0/1\">Carlee Joe-Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1\">Saurabh Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1\">Christopher Brinton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixMix: All You Need for Data-Free Compression Are Feature and Data Mixing. (arXiv:2011.09899v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.09899","description":"<p>User data confidentiality protection is becoming a rising challenge in the\npresent deep learning research. Without access to data, conventional\ndata-driven model compression faces a higher risk of performance degradation.\nRecently, some works propose to generate images from a specific pretrained\nmodel to serve as training data. However, the inversion process only utilizes\nbiased feature statistics stored in one model and is from low-dimension to\nhigh-dimension. As a consequence, it inevitably encounters the difficulties of\ngeneralizability and inexact inversion, which leads to unsatisfactory\nperformance. To address these problems, we propose MixMix based on two simple\nyet effective techniques: (1) Feature Mixing: utilizes various models to\nconstruct a universal feature space for generalized inversion; (2) Data Mixing:\nmixes the synthesized images and labels to generate exact label information. We\nprove the effectiveness of MixMix from both theoretical and empirical\nperspectives. Extensive experiments show that MixMix outperforms existing\nmethods on the mainstream compression tasks, including quantization, knowledge\ndistillation, and pruning. Specifically, MixMix achieves up to 4% and 20%\naccuracy uplift on quantization and pruning, respectively, compared to existing\ndata-free compression work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ruihao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1\">Mingzhu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fengwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shaoqing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shi Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MODNet: Real-Time Trimap-Free Portrait Matting via Objective Decomposition. (arXiv:2011.11961v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11961","description":"<p>Existing portrait matting methods either require auxiliary inputs that are\ncostly to obtain or involve multiple stages that are computationally expensive,\nmaking them less suitable for real-time applications. In this work, we present\na light-weight matting objective decomposition network (MODNet) for portrait\nmatting in real-time with a single input image. The key idea behind our\nefficient design is by optimizing a series of sub-objectives simultaneously via\nexplicit constraints. In addition, MODNet includes two novel techniques for\nimproving model efficiency and robustness. First, an Efficient Atrous Spatial\nPyramid Pooling (e-ASPP) module is introduced to fuse multi-scale features for\nsemantic estimation. Second, a self-supervised sub-objectives consistency (SOC)\nstrategy is proposed to adapt MODNet to real-world data to address the domain\nshift problem common to trimap-free methods. MODNet is easy to be trained in an\nend-to-end manner. It is much faster than contemporaneous methods and runs at\n67 frames per second on a 1080Ti GPU. Experiments show that MODNet outperforms\nprior trimap-free methods by a large margin on both Adobe Matting Dataset and a\ncarefully designed photographic portrait matting (PPM-100) benchmark proposed\nby us. Further, MODNet achieves remarkable results on daily photos and videos.\nOur code and models are available at https://github.com/ZHKKKe/MODNet, and the\nPPM-100 benchmark is released at https://github.com/ZHKKKe/PPM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_Z/0/1/0/all/0/1\">Zhanghan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiayu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kaican Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_R/0/1/0/all/0/1\">Rynson W.H. Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPAA: Stealthy Projector-based Adversarial Attacks on Deep Image Classifiers. (arXiv:2012.05858v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.05858","description":"<p>Light-based adversarial attacks use spatial augmented reality (SAR)\ntechniques to fool image classifiers by altering the physical light condition\nwith a controllable light source, e.g., a projector. Compared with physical\nattacks that place hand-crafted adversarial objects, projector-based ones\nobviate modifying the physical entities, and can be performed transiently and\ndynamically by altering the projection pattern. However, subtle light\nperturbations are insufficient to fool image classifiers, due to the complex\nenvironment and project-and-capture process. Thus, existing approaches focus on\nprojecting clearly perceptible adversarial patterns, while the more interesting\nyet challenging goal, stealthy projector-based attack, remains open. In this\npaper, for the first time, we formulate this problem as an end-to-end\ndifferentiable process and propose a Stealthy Projector-based Adversarial\nAttack (SPAA) solution. In SPAA, we approximate the real Project-and-Capture\nprocess using a deep neural network named PCNet, then we include PCNet in the\noptimization of projector-based attacks such that the generated adversarial\nprojection is physically plausible. Finally, to generate both robust and\nstealthy adversarial projections, we propose an algorithm that uses minimum\nperturbation and adversarial confidence thresholds to alternate between the\nadversarial loss and stealthiness loss optimization. Our experimental\nevaluations show that SPAA clearly outperforms other methods by achieving\nhigher attack success rates and meanwhile being stealthier, for both targeted\nand untargeted attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Bingyao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Non-linear Wavelet Transformation via Normalizing Flow. (arXiv:2101.11306v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.11306","description":"<p>Wavelet transformation stands as a cornerstone in modern data analysis and\nsignal processing. Its mathematical essence is an invertible transformation\nthat discerns slow patterns from fast ones in the frequency domain. Such an\ninvertible transformation can be learned by a designed normalizing flow model.\nWith a generalized lifting scheme as coupling layers, a factor-out layer\nresembling the downsampling, and parameter sharing at different levels of the\nmodel, one can train the normalizing flow to filter high-frequency elements at\ndifferent levels, thus extending traditional linear wavelet transformations to\nlearnable non-linear deep learning models. In this paper, a way of building\nsuch flow is proposed, along with a numerical analysis of the learned\ntransformation. Then, we demonstrate the model's ability in image lossless\ncompression, show it can achieve SOTA compression scores while achieving a\nsmall model size, substantial generalization ability, and the ability to handle\nhigh-dimensional data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuo-Hui Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Looking Beyond Two Frames: End-to-End Multi-Object Tracking Using Spatial and Temporal Transformers. (arXiv:2103.14829v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14829","description":"<p>Tracking a time-varying indefinite number of objects in a video sequence over\ntime remains a challenge despite recent advances in the field. Ignoring\nlong-term temporal information, most existing approaches are not able to\nproperly handle multi-object tracking challenges such as occlusion. To address\nthese shortcomings, we present MO3TR: a truly end-to-end Transformer-based\nonline multi-object tracking (MOT) framework that learns to handle occlusions,\ntrack initiation and termination without the need for an explicit data\nassociation module or any heuristics/post-processing. MO3TR encodes object\ninteractions into long-term temporal embeddings using a combination of spatial\nand temporal Transformers, and recursively uses the information jointly with\nthe input data to estimate the states of all tracked objects over time. The\nspatial attention mechanism enables our framework to learn implicit\nrepresentations between all the objects and the objects to the measurements,\nwhile the temporal attention mechanism focuses on specific parts of past\ninformation, allowing our approach to resolve occlusions over multiple frames.\nOur experiments demonstrate the potential of this new approach, reaching new\nstate-of-the-art results on multiple MOT metrics for two popular multi-object\ntracking benchmarks. Our code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tianyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hiller_M/0/1/0/all/0/1\">Markus Hiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsanpour_M/0/1/0/all/0/1\">Mahsa Ehsanpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rongkai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drummond_T/0/1/0/all/0/1\">Tom Drummond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Ian Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezatofighi_H/0/1/0/all/0/1\">Hamid Rezatofighi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning from Semantically Imprecise Data. (arXiv:2104.10901v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10901","description":"<p>Learning from imprecise labels such as \"animal\" or \"bird\", but making precise\npredictions like \"snow bunting\" at inference time is an important capability\nfor any classifier when expertly labeled training data is scarce. Contributions\nby volunteers or results of web crawling lack precision in this manner, but are\nstill valuable. And crucially, these weakly labeled examples are available in\nlarger quantities for lower cost than high-quality bespoke training data.\nCHILLAX, a recently proposed method to tackle this task, leverages a\nhierarchical classifier to learn from imprecise labels. However, it has two\nmajor limitations. First, it does not learn from examples labeled as the root\nof the hierarchy, e.g., \"object\". Second, an extrapolation of annotations to\nprecise labels is only performed at test time, where confident extrapolations\ncould be already used as training data. In this work, we extend CHILLAX with a\nself-supervised scheme using constrained semantic extrapolation to generate\npseudo-labels. This addresses the second concern, which in turn solves the\nfirst problem, enabling an even weaker supervision requirement than CHILLAX. We\nevaluate our approach empirically, showing that our method allows for a\nconsistent accuracy improvement of 0.84 to 1.19 percent points over CHILLAX and\nis suitable as a drop-in replacement without any negative consequences such as\nlonger training times.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brust_C/0/1/0/all/0/1\">Clemens-Alexander Brust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barz_B/0/1/0/all/0/1\">Bj&#xf6;rn Barz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1\">Joachim Denzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Prediction and Evaluation of Brassica Growth in the Field using Conditional Generative Adversarial Networks. (arXiv:2105.07789v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.07789","description":"<p>Farmers frequently assess plant growth and performance as basis for making\ndecisions when to take action in the field, such as fertilization, weed\ncontrol, or harvesting. The prediction of plant growth is a major challenge, as\nit is affected by numerous and highly variable environmental factors. This\npaper proposes a novel monitoring approach that comprises high-throughput\nimaging sensor measurements and their automatic analysis to predict future\nplant growth. Our approach's core is a novel machine learning-based generative\ngrowth model based on conditional generative adversarial networks, which is\nable to predict the future appearance of individual plants. In experiments with\nRGB time-series images of laboratory-grown Arabidopsis thaliana and field-grown\ncauliflower plants, we show that our approach produces realistic, reliable, and\nreasonable images of future growth stages. The automatic interpretation of the\ngenerated images through neural network-based instance segmentation allows the\nderivation of various phenotypic traits that describe plant growth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Drees_L/0/1/0/all/0/1\">Lukas Drees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junker_Frohn_L/0/1/0/all/0/1\">Laura Verena Junker-Frohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kierdorf_J/0/1/0/all/0/1\">Jana Kierdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1\">Ribana Roscher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLO5Face: Why Reinventing a Face Detector. (arXiv:2105.12931v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.12931","description":"<p>Tremendous progress has been made on face detection in recent years using\nconvolutional neural networks. While many face detectors use designs designated\nfor detecting faces, we treat face detection as a generic object detection\ntask. We implement a face detector based on the YOLOv5 object detector and call\nit YOLO5Face. We make a few key modifications to the YOLOv5 and optimize it for\nface detection. These modifications include adding a five-point landmark\nregression head, using a stem block at the input of the backbone, using\nsmaller-size kernels in the SPP, and adding a P6 output in the PAN block. We\ndesign detectors of different model sizes, from an extra-large model to achieve\nthe best performance to a super small model for real-time detection on an\nembedded or mobile device. Experiment results on the WiderFace dataset show\nthat on VGA images, our face detectors can achieve state-of-the-art performance\nin almost all the Easy, Medium, and Hard subsets, exceeding the more complex\ndesignated face detectors. The code is available at\n\\url{https://github.com/deepcam-cn/yolov5-face}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_D/0/1/0/all/0/1\">Delong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Weijun Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Qi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingfeng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The effectiveness of feature attribution methods and its correlation with automatic evaluation scores. (arXiv:2105.14944v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14944","description":"<p>Explaining the decisions of an Artificial Intelligence (AI) model is\nincreasingly critical in many real-world, high-stake applications. Hundreds of\npapers have either proposed new feature attribution methods, discussed or\nharnessed these tools in their work. However, despite humans being the target\nend-users, most attribution methods were only evaluated on proxy\nautomatic-evaluation metrics (Zhang et al. 2018; Zhou et al. 2016; Petsiuk et\nal. 2018). In this paper, we conduct the first user study to measure\nattribution map effectiveness in assisting humans in ImageNet classification\nand Stanford Dogs fine-grained classification, and when an image is natural or\nadversarial (i.e., contains adversarial perturbations). Overall, feature\nattribution is surprisingly not more effective than showing humans nearest\ntraining-set examples. On a harder task of fine-grained dog categorization,\npresenting attribution maps to humans does not help, but instead hurts the\nperformance of human-AI teams compared to AI alone. Importantly, we found\nautomatic attribution-map evaluation measures to correlate poorly with the\nactual human-AI team performance. Our findings encourage the community to\nrigorously test their methods on the downstream human-in-the-loop applications\nand to rethink the existing evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_G/0/1/0/all/0/1\">Giang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mutual Distillation of Confident Knowledge. (arXiv:2106.01489v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.01489","description":"<p>Mutual knowledge distillation (MKD) improves a model by distilling knowledge\nfrom another model. However, \\textit{not all knowledge is certain and correct},\nespecially under adverse conditions. For example, label noise usually leads to\nless reliable models due to undesired memorization\n\\cite{zhang2017understanding,arpit2017closer}. Wrong knowledge misleads the\nlearning rather than helps. This problem can be handled by two aspects: (i)\nimproving the reliability of a model where the knowledge is from (i.e.,\nknowledge source's reliability); (ii) selecting reliable knowledge for\ndistillation. In the literature, making a model more reliable is widely studied\nwhile selective MKD receives little attention. Therefore, we focus on studying\nselective MKD. Concretely, a generic MKD framework, \\underline{C}onfident\nknowledge selection followed by \\underline{M}utual \\underline{D}istillation\n(CMD), is designed. The key component of CMD is a generic knowledge selection\nformulation, making the selection threshold either static (CMD-S) or\nprogressive (CMD-P). Additionally, CMD covers two special cases: zero-knowledge\nand all knowledge, leading to a unified MKD framework. Extensive experiments\nare present to demonstrate the effectiveness of CMD and thoroughly justify the\ndesign of CMD. For example, CMD-P obtains new state-of-the-art results in\nrobustness against label noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziyun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinshao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Di Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_N/0/1/0/all/0/1\">Neil M. Robertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1\">David A. Clifton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1\">Christoph Meinel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haojin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation-Based Associative Joint Location for Human Pose Estimation in Videos. (arXiv:2107.03591v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.03591","description":"<p>Video-based human pose estimation (VHPE) is a vital yet challenging task.\nWhile deep learning methods have made significant progress for the VHPE, most\napproaches to this task implicitly model the long-range interaction between\njoints by enlarging the receptive field of the convolution. Unlike prior\nmethods, we design a lightweight and plug-and-play joint relation extractor\n(JRE) to model the associative relationship between joints explicitly and\nautomatically. The JRE takes the pseudo heatmaps of joints as input and\ncalculates the similarity between pseudo heatmaps. In this way, the JRE\nflexibly learns the relationship between any two joints, allowing it to learn\nthe rich spatial configuration of human poses. Moreover, the JRE can infer\ninvisible joints according to the relationship between joints, which is\nbeneficial for the model to locate occluded joints. Then, combined with\ntemporal semantic continuity modeling, we propose a Relation-based Pose\nSemantics Transfer Network (RPSTN) for video-based human pose estimation.\nSpecifically, to capture the temporal dynamics of poses, the pose semantic\ninformation of the current frame is transferred to the next with a joint\nrelation guided pose semantics propagator (JRPSP). The proposed model can\ntransfer the pose semantic features from the non-occluded frame to the occluded\nframe, making our method robust to the occlusion. Furthermore, the proposed JRE\nmodule is also suitable for image-based human pose estimation. The proposed\nRPSTN achieves state-of-the-art results on the video-based Penn Action dataset,\nSub-JHMDB dataset, and PoseTrack2018 dataset. Moreover, the proposed JRE\nimproves the performance of backbones on the image-based COCO2017 dataset. Code\nis available at https://github.com/YHDang/pose-estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dang_Y/0/1/0/all/0/1\">Yonghao Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaojie Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modality specific U-Net variants for biomedical image segmentation: A survey. (arXiv:2107.04537v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.04537","description":"<p>With the advent of advancements in deep learning approaches, such as deep\nconvolution neural network, residual neural network, adversarial network; U-Net\narchitectures are most widely utilized in biomedical image segmentation to\naddress the automation in identification and detection of the target regions or\nsub-regions. In recent studies, U-Net based approaches have illustrated\nstate-of-the-art performance in different applications for the development of\ncomputer-aided diagnosis systems for early diagnosis and treatment of diseases\nsuch as brain tumor, lung cancer, alzheimer, breast cancer, etc., using various\nmodalities. This article contributes in presenting the success of these\napproaches by describing the U-Net framework, followed by the comprehensive\nanalysis of the U-Net variants by performing 1) inter-modality, and 2)\nintra-modality categorization to establish better insights into the associated\nchallenges and solutions. Besides, this article also highlights the\ncontribution of U-Net based frameworks in the ongoing pandemic, severe acute\nrespiratory syndrome coronavirus 2 (SARS-CoV-2) also known as COVID-19.\nFinally, the strengths and similarities of these U-Net variants are analysed\nalong with the challenges involved in biomedical image segmentation to uncover\npromising future research directions in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Neural Network (CNN) vs Vision Transformer (ViT) for Digital Holography. (arXiv:2108.09147v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09147","description":"<p>In Digital Holography (DH), it is crucial to extract the object distance from\na hologram in order to reconstruct its amplitude and phase. This step is called\nauto-focusing and it is conventionally solved by first reconstructing a stack\nof images and then by sharpening each reconstructed image using a focus metric\nsuch as entropy or variance. The distance corresponding to the sharpest image\nis considered the focal position. This approach, while effective, is\ncomputationally demanding and time-consuming. In this paper, the determination\nof the distance is performed by Deep Learning (DL). Two deep learning (DL)\narchitectures are compared: Convolutional Neural Network (CNN) and Vision\nTransformer (ViT). ViT and CNN are used to cope with the problem of\nauto-focusing as a classification problem. Compared to a first attempt [11] in\nwhich the distance between two consecutive classes was 100$\\mu$m, our proposal\nallows us to drastically reduce this distance to 1$\\mu$m. Moreover, ViT reaches\nsimilar accuracy and is more robust than CNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cuenat_S/0/1/0/all/0/1\">St&#xe9;phane Cuenat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couturier_R/0/1/0/all/0/1\">Rapha&#xeb;l Couturier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Relative Confidence and Label-Free Model Selection for Convolutional Neural Networks. (arXiv:2108.11845v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11845","description":"<p>This letter is concerned with image classification with deep convolutional\nneural networks (CNNs). The focus is on the following question: given a set of\ncandidate CNN models, how to select the right one with the best generalization\nproperty for the current task? Present model selection methods require access\nto a batch of labeled data for computing a pre-specified performance metric,\nsuch as the cross-entropy loss, the classification error rate, the negative\nlog-likelihood. In many practical cases, labels are not available in time as\nlabeling itself is a time-consuming and expensive task. To this end, this\nletter presents an approach to CNN model selection using only unlabeled data.\nThis method is developed based on a principle termed consistent relative\nconfidence. The effectiveness and efficiency of the proposed method are\ndemonstrated by experiments using benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Aggregate and Refine Noisy Labels for Visual Sentiment Analysis. (arXiv:2109.07509v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07509","description":"<p>Visual sentiment analysis has received increasing attention in recent years.\nHowever, the dataset's quality is a concern because the sentiment labels are\ncrowd-sourcing, subjective, and prone to mistakes, and poses a severe threat to\nthe data-driven models, especially the deep neural networks. The deep models\nwould generalize poorly on the testing cases when trained to over-fit the\ntraining samples with noisy sentiment labels. Inspired by the recent progress\non learning with noisy labels, we propose a robust learning method to perform\nrobust visual sentiment analysis. Our method relies on external memory to\naggregate and filters noisy labels during training. The memory is composed of\nthe prototypes with corresponding labels, which can be updated online. The\nlearned prototypes and their labels can be regarded as denoising features and\nlabels for the local regions and can guide the training process to prevent the\nmodel from overfitting the noisy cases. We establish a benchmark for visual\nsentiment analysis with label noise using publicly available datasets. The\nexperiment results of the proposed benchmark settings comprehensively show the\neffectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zihe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitian Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1\">Hanjia Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Protecting Face Embeddings in Mobile Face Verification Scenarios. (arXiv:2110.00434v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00434","description":"<p>This paper proposes PolyProtect, a method for protecting the sensitive face\nembeddings that are used to represent people's faces in neural-network-based\nface verification systems. PolyProtect transforms a face embedding to a more\nsecure template, using a mapping based on multivariate polynomials\nparameterised by user-specific coefficients and exponents. In this work,\nPolyProtect is evaluated on two open-source face recognition systems in a\ncooperative-user mobile face verification context, under the toughest threat\nmodel that assumes a fully-informed attacker with complete knowledge of the\nsystem and all its parameters. Results indicate that PolyProtect can be tuned\nto achieve a satisfactory trade-off between the recognition accuracy of the\nPolyProtected face verification system and the irreversibility of the\nPolyProtected templates. Furthermore, PolyProtected templates are shown to be\neffectively unlinkable, especially if the user-specific parameters employed in\nthe PolyProtect mapping are selected in a non-naive manner. The evaluation is\nconducted using practical methodologies with tangible results, to present\nrealistic insight into the method's robustness as a face embedding protection\nscheme in practice. This work is fully reproducible using the publicly\navailable code at: https://gitlab.idiap.ch/bob/bob.paper.polyprotect_2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hahn_V/0/1/0/all/0/1\">Vedrana Krivoku&#x107;a Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcel_S/0/1/0/all/0/1\">S&#xe9;bastien Marcel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FOD-A: A Dataset for Foreign Object Debris in Airports. (arXiv:2110.03072v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03072","description":"<p>Foreign Object Debris (FOD) detection has attracted increased attention in\nthe area of machine learning and computer vision. However, a robust and\npublicly available image dataset for FOD has not been initialized. To this end,\nthis paper introduces an image dataset of FOD, named FOD in Airports (FOD-A).\nFOD-A object categories have been selected based on guidance from prior\ndocumentation and related research by the Federal Aviation Administration\n(FAA). In addition to the primary annotations of bounding boxes for object\ndetection, FOD-A provides labeled environmental conditions. As such, each\nannotation instance is further categorized into three light level categories\n(bright, dim, and dark) and two weather categories (dry and wet). Currently,\nFOD-A has released 31 object categories and over 30,000 annotation instances.\nThis paper presents the creation methodology, discusses the publicly available\ndataset extension process, and demonstrates the practicality of FOD-A with\nwidely used machine learning models for object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Munyer_T/0/1/0/all/0/1\">Travis Munyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Pei-Chi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1\">Xin Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biometric Template Protection for Neural-Network-based Face Recognition Systems: A Survey of Methods and Evaluation Techniques. (arXiv:2110.05044v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05044","description":"<p>This paper presents a survey of biometric template protection (BTP) methods\nfor securing face templates in neural-network-based face recognition systems.\nThe BTP methods are categorised into two types: Non-NN and NN-learned. Non-NN\nmethods use a neural network (NN) as a feature extractor, but the BTP part is\nbased on a non-NN algorithm applied at image-level or feature-level. In\ncontrast, NN-learned methods specifically employ a NN to learn a protected\ntemplate from the unprotected face image/features. We present examples of\nNon-NN and NN-learned face BTP methods from the literature, along with a\ndiscussion of the two categories' comparative strengths and weaknesses. We also\ninvestigate the techniques used to evaluate these BTP methods, in terms of the\nthree most common criteria: recognition accuracy, irreversibility, and\nrenewability/unlinkability. As expected, the recognition accuracy of protected\nface recognition systems is generally evaluated using the same (empirical)\ntechniques employed for evaluating standard (unprotected) biometric systems. On\nthe contrary, most irreversibility and renewability/unlinkability evaluations\nare based on theoretical assumptions/estimates or verbal implications, with no\nempirical validation in a practical face recognition context. So, we recommend\na greater focus on empirical evaluation strategies, to provide more concrete\ninsights into the irreversibility and renewability/unlinkability of face BTP\nmethods in practice. An exploration of the reproducibility of the studied BTP\nworks, in terms of the public availability of their implementation code and\nevaluation datasets/procedures, suggests that it would currently be difficult\nfor the BTP community to faithfully replicate (and thus validate) most of the\nreported findings. So, we advocate for a push towards reproducibility, in the\nhope of furthering our understanding of the face BTP research field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hahn_V/0/1/0/all/0/1\">Vedrana Krivoku&#x107;a Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcel_S/0/1/0/all/0/1\">S&#xe9;bastien Marcel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CVAD: A generic medical anomaly detector based on Cascade VAE. (arXiv:2110.15811v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.15811","description":"<p>Detecting out-of-distribution (OOD) samples in medical imaging plays an\nimportant role for downstream medical diagnosis. However, existing OOD\ndetectors are demonstrated on natural images composed of inter-classes and have\ndifficulty generalizing to medical images. The key issue is the granularity of\nOOD data in the medical domain, where intra-class OOD samples are predominant.\nWe focus on the generalizability of OOD detection for medical images and\npropose a self-supervised Cascade Variational autoencoder-based Anomaly\nDetector (CVAD). We use a variational autoencoders' cascade architecture, which\ncombines latent representation at multiple scales, before being fed to a\ndiscriminator to distinguish the OOD data from the in-distribution (ID) data.\nFinally, both the reconstruction error and the OOD probability predicted by the\nbinary discriminator are used to determine the anomalies. We compare the\nperformance with the state-of-the-art deep learning models to demonstrate our\nmodel's efficacy on various open-access medical imaging datasets for both\nintra- and inter-class OOD. Further extensive results on datasets including\ncommon natural datasets show our model's effectiveness and generalizability.\nThe code is available at https://github.com/XiaoyuanGuo/CVAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1\">Xiaoyuan Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gichoya_J/0/1/0/all/0/1\">Judy Wawira Gichoya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Purkayastha_S/0/1/0/all/0/1\">Saptarshi Purkayastha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Banerjee_I/0/1/0/all/0/1\">Imon Banerjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Radiograph Representation Learning via Cross-supervision between Images and Free-text Radiology Reports. (arXiv:2111.03452v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.03452","description":"<p>Pre-training lays the foundation for recent successes in radiograph analysis\nsupported by deep learning. It learns transferable image representations by\nconducting large-scale fully-supervised or self-supervised learning on a source\ndomain. However, supervised pre-training requires a complex and labor intensive\ntwo-stage human-assisted annotation process while self-supervised learning\ncannot compete with the supervised paradigm. To tackle these issues, we propose\na cross-supervised methodology named REviewing FreE-text Reports for\nSupervision (REFERS), which acquires free supervision signals from original\nradiology reports accompanying the radiographs. The proposed approach employs a\nvision transformer and is designed to learn joint representations from multiple\nviews within every patient study. REFERS outperforms its transfer learning and\nself-supervised learning counterparts on 4 well-known X-ray datasets under\nextremely limited supervision. Moreover, REFERS even surpasses methods based on\na source domain of radiographs with human-assisted structured labels. Thus\nREFERS has the potential to replace canonical pre-training methodologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Hong-Yu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinghao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_R/0/1/0/all/0/1\">Ruibang Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iBOT: Image BERT Pre-Training with Online Tokenizer. (arXiv:2111.07832v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.07832","description":"<p>The success of language Transformers is primarily attributed to the pretext\ntask of masked language modeling (MLM), where texts are first tokenized into\nsemantically meaningful pieces. In this work, we study masked image modeling\n(MIM) and indicate the advantages and challenges of using a semantically\nmeaningful visual tokenizer. We present a self-supervised framework iBOT that\ncan perform masked prediction with an online tokenizer. Specifically, we\nperform self-distillation on masked patch tokens and take the teacher network\nas the online tokenizer, along with self-distillation on the class token to\nacquire visual semantics. The online tokenizer is jointly learnable with the\nMIM objective and dispenses with a multi-stage training pipeline where the\ntokenizer needs to be pre-trained beforehand. We show the prominence of iBOT by\nachieving an 82.3% linear probing accuracy and an 87.8% fine-tuning accuracy\nevaluated on ImageNet-1K. Beyond the state-of-the-art image classification\nresults, we underline emerging local semantic patterns, which helps the models\nto obtain strong robustness against common corruptions and achieve leading\nresults on dense downstream tasks, eg., object detection, instance\nsegmentation, and semantic segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinghao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tao Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient deep learning models for land cover image classification. (arXiv:2111.09451v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09451","description":"<p>The availability of the sheer volume of Copernicus Sentinel-2 imagery has\ncreated new opportunities for exploiting deep learning methods for land use\nland cover (LULC) mapping at large scales. However, an extensive set of\nbenchmark experiments is currently lacking, i.e. deep learning models tested on\nthe same dataset, with a common and consistent set of metrics, and in the same\nhardware. In this work, we use the BigEarthNet Sentinel-2 multispectral dataset\nto benchmark for the first time different state-of-the-art deep learning models\nfor the multi-label, multi-class LULC classification problem, contributing with\nan exhaustive zoo of 56 trained models. Our benchmark includes standard\nConvolution Neural Network architectures, but we also test non-convolutional\nmethods, such as Multi-Layer Perceptrons and Vision Transformers. We put to the\ntest EfficientNets and Wide Residual Networks (WRN) architectures, and leverage\nclassification accuracy, training time and inference rate. Furthermore, we\npropose to use the EfficientNet framework for the compound scaling of a\nlightweight WRN, by varying network depth, width, and input data resolution.\nEnhanced with an Efficient Channel Attention mechanism, our scaled lightweight\nmodel emerged as the new state-of-the-art. It achieves 4.5% higher averaged\nF-Score classification accuracy for all 19 LULC classes compared to a standard\nResNet50 baseline model, with an order of magnitude less trainable parameters.\nWe provide access to all trained models, along with our code for distributed\ntraining on multiple GPU nodes. This model zoo of pre-trained encoders can be\nused for transfer learning and rapid prototyping in different remote sensing\ntasks that use Sentinel-2 data, instead of exploiting backbone models trained\nwith data from a different domain, e.g., from ImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papoutsis_I/0/1/0/all/0/1\">Ioannis Papoutsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bountos_N/0/1/0/all/0/1\">Nikolaos-Ioannis Bountos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zavras_A/0/1/0/all/0/1\">Angelos Zavras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michail_D/0/1/0/all/0/1\">Dimitrios Michail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tryfonopoulos_C/0/1/0/all/0/1\">Christos Tryfonopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D High-Quality Magnetic Resonance Image Restoration in Clinics Using Deep Learning. (arXiv:2111.14259v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.14259","description":"<p>Shortening acquisition time and reducing the motion artifacts are two of the\nmost essential concerns in magnetic resonance imaging. As a promising solution,\ndeep learning-based high-quality MR image restoration has been investigated to\ngenerate higher resolution and motion artifact-free MR images from lower\nresolution images acquired with shortened acquisition time, without costing\nadditional acquisition time or modifying the pulse sequences. However, numerous\nproblems still exist to prevent deep learning approaches from becoming\npractical in the clinic environment. Specifically, most of the prior works\nfocus solely on the network model but ignore the impact of various downsampling\nstrategies on the acquisition time. Besides, the long inference time and high\nGPU consumption are also the bottlenecks to deploy most of the prior works in\nclinics. Furthermore, prior studies employ random movement in retrospective\nmotion artifact generation, resulting in uncontrollable severity of motion\nartifact. More importantly, doctors are unsure whether the generated MR images\nare trustworthy, making diagnosis difficult. To overcome all these problems, we\nemployed a unified 2D deep learning neural network for both 3D MRI super\nresolution and motion artifact reduction, demonstrating such a framework can\nachieve better performance in 3D MRI restoration tasks compared to other states\nof the art methods and remains the GPU consumption and inference time\nsignificantly low, thus easier to deploy. We also analyzed several downsampling\nstrategies based on the acceleration factor, including multiple combinations of\nin-plane and through-plane downsampling, and developed a controllable and\nquantifiable motion artifact generation method. At last, the pixel-wise\nuncertainty was calculated and used to estimate the accuracy of the generated\nimage, providing additional information for reliable diagnosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jianan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Prediction of MGMT Promoter Methylation from MRI Scans using Adversarial Learning. (arXiv:2201.04416v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.04416","description":"<p>Glioblastoma Multiforme (GBM) is a malignant brain cancer forming around 48%\nof al brain and Central Nervous System (CNS) cancers. It is estimated that\nannually over 13,000 deaths occur in the US due to GBM, making it crucial to\nhave early diagnosis systems that can lead to predictable and effective\ntreatment. The most common treatment after GBM diagnosis is chemotherapy, which\nworks by sending rapidly dividing cells to apoptosis. However, this form of\ntreatment is not effective when the MGMT promoter sequence is methylated, and\ninstead leads to severe side effects decreasing patient survivability.\nTherefore, it is important to be able to identify the MGMT promoter methylation\nstatus through non-invasive magnetic resonance imaging (MRI) based machine\nlearning (ML) models. This is accomplished using the Brain Tumor Segmentation\n(BraTS) 2021 dataset, which was recently used for an international Kaggle\ncompetition. We developed four primary models - two radiomic models and two CNN\nmodels - each solving the binary classification task with progressive\nimprovements. We built a novel ML model termed as the Intermediate State\nGenerator which was used to normalize the slice thicknesses of all MRI scans.\nWith further improvements, our best model was able to achieve performance\nsignificantly ($p &lt; 0.05$) better than the best performing Kaggle model with a\n6% increase in average cross-validation accuracy. This improvement could\npotentially lead to a more informed choice of chemotherapy as a treatment\noption, prolonging lives of thousands of patients with GBM each year.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Das_S/0/1/0/all/0/1\">Sauman Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Robust are Discriminatively Trained Zero-Shot Learning Models?. (arXiv:2201.10972v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10972","description":"<p>Data shift robustness has been primarily investigated from a fully supervised\nperspective, and robustness of zero-shot learning (ZSL) models have been\nlargely neglected. In this paper, we present novel analyses on the robustness\nof discriminative ZSL to image corruptions. We subject several ZSL models to a\nlarge set of common corruptions and defenses. In order to realize the\ncorruption analysis, we curate and release the first ZSL corruption robustness\ndatasets SUN-C, CUB-C and AWA2-C. We analyse our results by taking into account\nthe dataset characteristics, class imbalance, class transitions between seen\nand unseen classes and the discrepancies between ZSL and GZSL performances. Our\nresults show that discriminative ZSL suffers from corruptions and this trend is\nfurther exacerbated by the severe class imbalance and model weakness inherent\nin ZSL methods. We then combine our findings with those based on adversarial\nattacks in ZSL, and highlight the different effects of corruptions and\nadversarial examples, such as the pseudo-robustness effect present under\nadversarial attacks. We also obtain new strong baselines for both models with\nthe defense methods. Finally, our experiments show that although existing\nmethods to improve robustness somewhat work for ZSL models, they do not produce\na tangible effect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yucel_M/0/1/0/all/0/1\">Mehmet Kerim Yucel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cinbis_R/0/1/0/all/0/1\">Ramazan Gokberk Cinbis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duygulu_P/0/1/0/all/0/1\">Pinar Duygulu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Attribute Balanced Sampling for Disentangled GAN Controls. (arXiv:2111.00909v2 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2111.00909","description":"<p>Various controls over the generated data can be extracted from the latent\nspace of a pre-trained GAN, as it implicitly encodes the semantics of the\ntraining data. The discovered controls allow to vary semantic attributes in the\ngenerated images but usually lead to entangled edits that affect multiple\nattributes at the same time. Supervised approaches typically sample and\nannotate a collection of latent codes, then train classifiers in the latent\nspace to identify the controls. Since the data generated by GANs reflects the\nbiases of the original dataset, so do the resulting semantic controls. We\npropose to address disentanglement by subsampling the generated data to remove\nover-represented co-occuring attributes thus balancing the semantics of the\ndataset before training the classifiers. We demonstrate the effectiveness of\nthis approach by extracting disentangled linear directions for face\nmanipulation on two popular GAN architectures, PGGAN and StyleGAN, and two\ndatasets, CelebAHQ and FFHQ. We show that this approach outperforms\nstate-of-the-art classifier-based methods while avoiding the need for\ndisentanglement-enforcing post-processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doubinsky_P/0/1/0/all/0/1\">Perla Doubinsky</a> (CEDRIC - VERTIGO, CNAM), <a href=\"http://arxiv.org/find/cs/1/au:+Audebert_N/0/1/0/all/0/1\">Nicolas Audebert</a> (CEDRIC - VERTIGO, CNAM), <a href=\"http://arxiv.org/find/cs/1/au:+Crucianu_M/0/1/0/all/0/1\">Michel Crucianu</a> (CEDRIC - VERTIGO, CNAM), <a href=\"http://arxiv.org/find/cs/1/au:+Borgne_H/0/1/0/all/0/1\">Herv&#xe9; Le Borgne</a> (LIST)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jacobian Computation for Cumulative B-splines on SE(3) and Application to Continuous-Time Object Tracking. (arXiv:2201.10602v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2201.10602","description":"<p>In this paper we propose a method that estimates the $SE(3)$ continuous\ntrajectories (orientation and translation) of the dynamic rigid objects present\nin a scene, from multiple RGB-D views. Specifically, we fit the object\ntrajectories to cumulative B-Splines curves, which allow us to interpolate, at\nany intermediate time stamp, not only their poses but also their linear and\nangular velocities and accelerations. Additionally, we derive in this work the\nanalytical $SE(3)$ Jacobians needed by the optimization, being applicable to\nany other approach that uses this type of curves. To the best of our knowledge\nthis is the first work that proposes 6-DoF continuous-time object tracking,\nwhich we endorse with significant computational cost reduction thanks to our\nanalytical derivations. We evaluate our proposal in synthetic data and in a\npublic benchmark, showing competitive results in localization and significant\nimprovements in velocity estimation in comparison to discrete-time approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tirado_J/0/1/0/all/0/1\">Javier Tirado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Javier Civera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}