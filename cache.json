{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-17T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Cross-Register Projection for Headline Part of Speech Tagging. (arXiv:2109.07483v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07483","description":"<p>Part of speech (POS) tagging is a familiar NLP task. State of the art taggers\nroutinely achieve token-level accuracies of over 97% on news body text,\nevidence that the problem is well understood. However, the register of English\nnews headlines, \"headlinese\", is very different from the register of long-form\ntext, causing POS tagging models to underperform on headlines. In this work, we\nautomatically annotate news headlines with POS tags by projecting predicted\ntags from corresponding sentences in news bodies. We train a multi-domain POS\ntagger on both long-form and headline text and show that joint training on both\nregisters improves over training on just one or naively concatenating training\nsets. We evaluate on a newly-annotated corpus of over 5,248 English news\nheadlines from the Google sentence compression corpus, and show that our model\nyields a 23% relative error reduction per token and 19% per headline. In\naddition, we demonstrate that better headline POS tags can improve the\nperformance of a syntax-based open information extraction system. We make POSH,\nthe POS-tagged Headline corpus, available to encourage research in improved NLP\nmodels for news headlines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benton_A/0/1/0/all/0/1\">Adrian Benton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malioutov_I/0/1/0/all/0/1\">Igor Malioutov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Euclidean and Hyperbolic Embeddings on the WordNet Nouns Hypernymy Graph. (arXiv:2109.07488v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07488","description":"<p>Nickel and Kiela (2017) present a new method for embedding tree nodes in the\nPoincare ball, and suggest that these hyperbolic embeddings are far more\neffective than Euclidean embeddings at embedding nodes in large, hierarchically\nstructured graphs like the WordNet nouns hypernymy tree. This is especially\ntrue in low dimensions (Nickel and Kiela, 2017, Table 1). In this work, we seek\nto reproduce their experiments on embedding and reconstructing the WordNet\nnouns hypernymy graph. Counter to what they report, we find that Euclidean\nembeddings are able to represent this tree at least as well as Poincare\nembeddings, when allowed at least 50 dimensions. We note that this does not\ndiminish the significance of their work given the impressive performance of\nhyperbolic embeddings in very low-dimensional settings. However, given the wide\ninfluence of their work, our aim here is to present an updated and more\naccurate comparison between the Euclidean and hyperbolic embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1\">Sameer Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benton_A/0/1/0/all/0/1\">Adrian Benton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Heads and Tails of Models with Marginal Calibration for Sparse Tagsets. (arXiv:2109.07494v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07494","description":"<p>For interpreting the behavior of a probabilistic model, it is useful to\nmeasure a model's calibration--the extent to which it produces reliable\nconfidence scores. We address the open problem of calibration for tagging\nmodels with sparse tagsets, and recommend strategies to measure and reduce\ncalibration error (CE) in such models. We show that several post-hoc\nrecalibration techniques all reduce calibration error across the marginal\ndistribution for two existing sequence taggers. Moreover, we propose tag\nfrequency grouping (TFG) as a way to measure calibration error in different\nfrequency bands. Further, recalibrating each group separately promotes a more\nequitable reduction of calibration error across the tag frequency spectrum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kranzlein_M/0/1/0/all/0/1\">Michael Kranzlein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nelson F. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialogue State Tracking with a Language Model using Schema-Driven Prompting. (arXiv:2109.07506v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07506","description":"<p>Task-oriented conversational systems often use dialogue state tracking to\nrepresent the user's intentions, which involves filling in values of\npre-defined slots. Many approaches have been proposed, often using\ntask-specific architectures with special-purpose classifiers. Recently, good\nresults have been obtained using more general architectures based on pretrained\nlanguage models. Here, we introduce a new variation of the language modeling\napproach that uses schema-driven prompting to provide task-aware history\nencoding that is used for both categorical and non-categorical slots. We\nfurther improve performance by augmenting the prompting with schema\ndescriptions, a naturally occurring source of in-domain knowledge. Our purely\ngenerative system achieves state-of-the-art performance on MultiWOZ 2.2 and\nachieves competitive performance on two other benchmarks: MultiWOZ 2.1 and M2M.\nThe data and code will be available at\nhttps://github.com/chiahsuan156/DST-as-Prompting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chia-Hsuan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tied & Reduced RNN-T Decoder. (arXiv:2109.07513v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07513","description":"<p>Previous works on the Recurrent Neural Network-Transducer (RNN-T) models have\nshown that, under some conditions, it is possible to simplify its prediction\nnetwork with little or no loss in recognition accuracy (<a href=\"/abs/2003.07705\">arXiv:2003.07705</a>\n[eess.AS], [2], <a href=\"/abs/2012.06749\">arXiv:2012.06749</a> [cs.CL]). This is done by limiting the context\nsize of previous labels and/or using a simpler architecture for its layers\ninstead of LSTMs. The benefits of such changes include reduction in model size,\nfaster inference and power savings, which are all useful for on-device\napplications.\n</p>\n<p>In this work, we study ways to make the RNN-T decoder (prediction network +\njoint network) smaller and faster without degradation in recognition\nperformance. Our prediction network performs a simple weighted averaging of the\ninput embeddings, and shares its embedding matrix weights with the joint\nnetwork's output layer (a.k.a. weight tying, commonly used in language modeling\n<a href=\"/abs/1611.01462\">arXiv:1611.01462</a> [cs.LG]). This simple design, when used in conjunction with\nadditional Edit-based Minimum Bayes Risk (EMBR) training, reduces the RNN-T\nDecoder from 23M parameters to just 2M, without affecting word-error rate\n(WER).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Botros_R/0/1/0/all/0/1\">Rami Botros</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+David_R/0/1/0/all/0/1\">Robert David</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_E/0/1/0/all/0/1\">Emmanuel Guzman</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yanzhang He</a> (1) ((1) Google Inc. USA)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text as Causal Mediators: Research Design for Causal Estimates of Differential Treatment of Social Groups via Language Aspects. (arXiv:2109.07542v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07542","description":"<p>Using observed language to understand interpersonal interactions is important\nin high-stakes decision making. We propose a causal research design for\nobservational (non-experimental) data to estimate the natural direct and\nindirect effects of social group signals (e.g. race or gender) on speakers'\nresponses with separate aspects of language as causal mediators. We illustrate\nthe promises and challenges of this framework via a theoretical case study of\nthe effect of an advocate's gender on interruptions from justices during U.S.\nSupreme Court oral arguments. We also discuss challenges conceptualizing and\noperationalizing causal variables such as gender and language that comprise of\nmany components, and we articulate technical open challenges such as temporal\ndependence between language mediators in conversational settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keith_K/0/1/0/all/0/1\">Katherine A. Keith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rice_D/0/1/0/all/0/1\">Douglas Rice</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_B/0/1/0/all/0/1\">Brendan O&#x27;Connor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"It doesn't look good for a date\": Transforming Critiques into Preferences for Conversational Recommendation Systems. (arXiv:2109.07576v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07576","description":"<p>Conversations aimed at determining good recommendations are iterative in\nnature. People often express their preferences in terms of a critique of the\ncurrent recommendation (e.g., \"It doesn't look good for a date\"), requiring\nsome degree of common sense for a preference to be inferred. In this work, we\npresent a method for transforming a user critique into a positive preference\n(e.g., \"I prefer more romantic\") in order to retrieve reviews pertaining to\npotentially better recommendations (e.g., \"Perfect for a romantic dinner\"). We\nleverage a large neural language model (LM) in a few-shot setting to perform\ncritique-to-preference transformation, and we test two methods for retrieving\nrecommendations: one that matches embeddings, and another that fine-tunes an LM\nfor the task. We instantiate this approach in the restaurant domain and\nevaluate it using a new dataset of restaurant critiques. In an ablation study,\nwe show that utilizing critique-to-preference transformation improves\nrecommendations, and that there are at least three general cases that explain\nthis improved performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bursztyn_V/0/1/0/all/0/1\">Victor S. Bursztyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Healey_J/0/1/0/all/0/1\">Jennifer Healey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1\">Nedim Lipka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_E/0/1/0/all/0/1\">Eunyee Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birnbaum_L/0/1/0/all/0/1\">Larry Birnbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An influencer-based approach to understanding radical right viral tweets. (arXiv:2109.07588v1 [cs.SI])","link":"http://arxiv.org/abs/2109.07588","description":"<p>Radical right influencers routinely use social media to spread highly\ndivisive, disruptive and anti-democratic messages. Assessing and countering the\nchallenge that such content poses is crucial for ensuring that online spaces\nremain open, safe and accessible. Previous work has paid little attention to\nunderstanding factors associated with radical right content that goes viral. We\ninvestigate this issue with a new dataset ROT which provides insight into the\ncontent, engagement and followership of a set of 35 radical right influencers.\nIt includes over 50,000 original entries and over 40 million retweets, quotes,\nreplies and mentions. We use a multilevel model to measure engagement with\ntweets, which are nested in each influencer. We show that it is crucial to\naccount for the influencer-level structure, and find evidence of the importance\nof both influencer- and content-level factors, including the number of\nfollowers each influencer has, the type of content (original posts, quotes and\nreplies), the length and toxicity of content, and whether influencers request\nretweets. We make ROT available for other researchers to use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sprejer_L/0/1/0/all/0/1\">Laila Sprejer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Margetts_H/0/1/0/all/0/1\">Helen Margetts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_K/0/1/0/all/0/1\">Kleber Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OSullivan_D/0/1/0/all/0/1\">David O&#x27;Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1\">Bertie Vidgen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning. (arXiv:2109.07589v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07589","description":"<p>Named Entity Recognition (NER) in Few-Shot setting is imperative for entity\ntagging in low resource domains. Existing approaches only learn class-specific\nsemantic features and intermediate representations from source domains. This\naffects generalizability to unseen target domains, resulting in suboptimal\nperformances. To this end, we present CONTaiNER, a novel contrastive learning\ntechnique that optimizes the inter-token distribution distance for Few-Shot\nNER. Instead of optimizing class-specific attributes, CONTaiNER optimizes a\ngeneralized objective of differentiating between token categories based on\ntheir Gaussian-distributed embeddings. This effectively alleviates overfitting\nissues originating from training domains. Our experiments in several\ntraditional test domains (OntoNotes, CoNLL'03, WNUT '17, GUM) and a new large\nscale Few-Shot NER dataset (Few-NERD) demonstrate that on average, CONTaiNER\noutperforms previous methods by 3%-13% absolute F1 points while showing\nconsistent performance trends, even in challenging scenarios where previous\napproaches could not achieve appreciable performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sarkar Snigdha Sarathi Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katiyar_A/0/1/0/all/0/1\">Arzoo Katiyar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1\">Rebecca J. Passonneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Complementarity of Data Selection and Fine Tuning for Domain Adaptation. (arXiv:2109.07591v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07591","description":"<p>Domain adaptation of neural networks commonly relies on three training\nphases: pretraining, selected data training and then fine tuning. Data\nselection improves target domain generalization by training further on\npretraining data identified by relying on a small sample of target domain data.\nThis work examines the benefit of data selection for language modeling and\nmachine translation. Our experiments assess the complementarity of selection\nwith fine tuning and result in practical recommendations: (i) selected data\nmust be similar to the fine-tuning domain but not so much as to erode the\ncomplementary effect of fine-tuning; (ii) there is a trade-off between\nselecting little data for fast but limited progress or much data for slow but\nlong lasting progress; (iii) data selection can be applied early during\npretraining, with performance gains comparable to long pretraining session;\n(iv) data selection from domain classifiers is often more effective than the\npopular contrastive data selection method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iter_D/0/1/0/all/0/1\">Dan Iter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grangier_D/0/1/0/all/0/1\">David Grangier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Feature-Engineering and Feature-Learning Approaches for Multilingual Translationese Classification. (arXiv:2109.07604v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07604","description":"<p>Traditional hand-crafted linguistically-informed features have often been\nused for distinguishing between translated and original non-translated texts.\nBy contrast, to date, neural architectures without manual feature engineering\nhave been less explored for this task. In this work, we (i) compare the\ntraditional feature-engineering-based approach to the feature-learning-based\none and (ii) analyse the neural architectures in order to investigate how well\nthe hand-crafted features explain the variance in the neural models'\npredictions. We use pre-trained neural word embeddings, as well as several\nend-to-end neural architectures in both monolingual and multilingual settings\nand compare them to feature-engineering-based SVM classifiers. We show that (i)\nneural architectures outperform other approaches by more than 20 accuracy\npoints, with the BERT-based model performing the best in both the monolingual\nand multilingual settings; (ii) while many individual hand-crafted\ntranslationese features correlate with neural model predictions, feature\nimportance analysis shows that the most important features for neural and\nclassical architectures differ; and (iii) our multilingual experiments provide\nempirical evidence for translationese universals across languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pylypenko_D/0/1/0/all/0/1\">Daria Pylypenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amponsah_Kaakyire_K/0/1/0/all/0/1\">Kwabena Amponsah-Kaakyire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_K/0/1/0/all/0/1\">Koel Dutta Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genabith_J/0/1/0/all/0/1\">Josef van Genabith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espana_Bonet_C/0/1/0/all/0/1\">Cristina Espa&#xf1;a-Bonet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Ontology-Based Information Extraction System for Residential Land Use Suitability Analysis. (arXiv:2109.07672v1 [cs.AI])","link":"http://arxiv.org/abs/2109.07672","description":"<p>We propose an Ontology-Based Information Extraction (OBIE) system to automate\nthe extraction of the criteria and values applied in Land Use Suitability\nAnalysis (LUSA) from bylaw and regulation documents related to the geographic\narea of interest. The results obtained by our proposed LUSA OBIE system (land\nuse suitability criteria and their values) are presented as an ontology\npopulated with instances of the extracted criteria and property values. This\nlatter output ontology is incorporated into a Multi-Criteria Decision Making\n(MCDM) model applied for constructing suitability maps for different kinds of\nland uses. The resulting maps may be the final desired product or can be\nincorporated into the cellular automata urban modeling and simulation for\npredicting future urban growth. A case study has been conducted where the\noutput from LUSA OBIE is applied to help produce a suitability map for the City\nof Regina, Saskatchewan, to assist in the identification of suitable areas for\nresidential development. A set of Saskatchewan bylaw and regulation documents\nwere downloaded and input to the LUSA OBIE system. We accessed the extracted\ninformation using both the populated LUSA ontology and the set of annotated\ndocuments. In this regard, the LUSA OBIE system was effective in producing a\nfinal suitability map.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Ageili_M/0/1/0/all/0/1\">Munira Al-Ageili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouhoub_M/0/1/0/all/0/1\">Malek Mouhoub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Commonsense Knowledge Base Population with an Effective Evaluation Dataset. (arXiv:2109.07679v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07679","description":"<p>Reasoning over commonsense knowledge bases (CSKB) whose elements are in the\nform of free-text is an important yet hard task in NLP. While CSKB completion\nonly fills the missing links within the domain of the CSKB, CSKB population is\nalternatively proposed with the goal of reasoning unseen assertions from\nexternal resources. In this task, CSKBs are grounded to a large-scale\neventuality (activity, state, and event) graph to discriminate whether novel\ntriples from the eventuality graph are plausible or not. However, existing\nevaluations on the population task are either not accurate (automatic\nevaluation with randomly sampled negative examples) or of small scale (human\nannotation). In this paper, we benchmark the CSKB population task with a new\nlarge-scale dataset by first aligning four popular CSKBs, and then presenting a\nhigh-quality human-annotated evaluation set to probe neural models' commonsense\nreasoning ability. We also propose a novel inductive commonsense reasoning\nmodel that reasons over graphs. Experimental results show that generalizing\ncommonsense reasoning on unseen assertions is inherently a hard task. Models\nachieving high accuracy during training perform poorly on the evaluation set,\nwith a large gap between human performance. We will make the data publicly\navailable for future contributions. Codes and data are available at\nhttps://github.com/HKUST-KnowComp/CSKB-Population.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tianqing Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Sehyun Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1\">Shibo Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bin He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jointly Modeling Aspect and Polarity for Aspect-based Sentiment Analysis in Persian Reviews. (arXiv:2109.07680v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07680","description":"<p>Identification of user's opinions from natural language text has become an\nexciting field of research due to its growing applications in the real world.\nThe research field is known as sentiment analysis and classification, where\naspect category detection (ACD) and aspect category polarity (ACP) are two\nimportant sub-tasks of aspect-based sentiment analysis. The goal in ACD is to\nspecify which aspect of the entity comes up in opinion while ACP aims to\nspecify the polarity of each aspect category from the ACD task. The previous\nworks mostly propose separate solutions for these two sub-tasks. This paper\nfocuses on the ACD and ACP sub-tasks to solve both problems simultaneously. The\nproposed method carries out multi-label classification where four different\ndeep models were employed and comparatively evaluated to examine their\nperformance. A dataset of Persian reviews was collected from CinemaTicket\nwebsite including 2200 samples from 14 categories. The developed models were\nevaluated using the collected dataset in terms of example-based and label-based\nmetrics. The results indicate the high applicability and preference of the CNN\nand GRU models in comparison to LSTM and Bi-LSTM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vazan_M/0/1/0/all/0/1\">Milad Vazan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razmara_J/0/1/0/all/0/1\">Jafar Razmara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models are Few-shot Multilingual Learners. (arXiv:2109.07684v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07684","description":"<p>General-purpose language models have demonstrated impressive capabilities,\nperforming on par with state-of-the-art approaches on a range of downstream\nnatural language processing (NLP) tasks and benchmarks when inferring\ninstructions from very few examples. Here, we evaluate the multilingual skills\nof the GPT and T5 models in conducting multi-class classification on\nnon-English languages without any parameter updates. We show that, given a few\nEnglish examples as context, pre-trained language models can predict not only\nEnglish test samples but also non-English ones. Finally, we find the in-context\nfew-shot cross-lingual prediction results of language models are significantly\nbetter than random prediction, and they are competitive compared to the\nexisting state-of-the-art cross-lingual models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhaojiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rosanne Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yosinski_J/0/1/0/all/0/1\">Jason Yosinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferable Persona-Grounded Dialogues via Grounded Minimal Edits. (arXiv:2109.07713v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07713","description":"<p>Grounded dialogue models generate responses that are grounded on certain\nconcepts. Limited by the distribution of grounded dialogue data, models trained\non such data face the transferability challenges in terms of the data\ndistribution and the type of grounded concepts. To address the challenges, we\npropose the grounded minimal editing framework, which minimally edits existing\nresponses to be grounded on the given concept. Focusing on personas, we propose\nGrounded Minimal Editor (GME), which learns to edit by disentangling and\nrecombining persona-related and persona-agnostic parts of the response. To\nevaluate persona-grounded minimal editing, we present the PersonaMinEdit\ndataset, and experimental results show that GME outperforms competitive\nbaselines by a large margin. To evaluate the transferability, we experiment on\nthe test set of BlendedSkillTalk and show that GME can edit dialogue models'\nresponses to largely improve their persona consistency while preserving the use\nof knowledge and empathy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Henry Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xiaoxi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sister Help: Data Augmentation for Frame-Semantic Role Labeling. (arXiv:2109.07725v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07725","description":"<p>While FrameNet is widely regarded as a rich resource of semantics in natural\nlanguage processing, a major criticism concerns its lack of coverage and the\nrelative paucity of its labeled data compared to other commonly used lexical\nresources such as PropBank and VerbNet. This paper reports on a pilot study to\naddress these gaps. We propose a data augmentation approach, which uses\nexisting frame-specific annotation to automatically annotate other lexical\nunits of the same frame which are unannotated. Our rule-based approach defines\nthe notion of a sister lexical unit and generates frame-specific augmented data\nfor training. We present experiments on frame-semantic role labeling which\ndemonstrate the importance of this data augmentation: we obtain a large\nimprovement to prior results on frame identification and argument\nidentification for FrameNet, utilizing both full-text and lexicographic\nannotations under FrameNet. Our findings on data augmentation highlight the\nvalue of automatic resource creation for improved models in frame-semantic\nparsing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pancholy_A/0/1/0/all/0/1\">Ayush Pancholy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petruck_M/0/1/0/all/0/1\">Miriam R. L. Petruck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOVER: Mask, Over-generate and Rank for Hyperbole Generation. (arXiv:2109.07726v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07726","description":"<p>Despite being a common figure of speech, hyperbole is under-researched with\nonly a few studies addressing its identification task. In this paper, we\nintroduce a new task of hyperbole generation to transfer a literal sentence\ninto its hyperbolic paraphrase. To tackle the lack of available hyperbolic\nsentences, we construct HYPO-XL, the first large-scale hyperbole corpus\ncontaining 17,862 hyperbolic sentences in a non-trivial way. Based on our\ncorpus, we propose an unsupervised method for hyperbole generation with no need\nfor parallel literal-hyperbole pairs. During training, we fine-tune BART to\ninfill masked hyperbolic spans of sentences from HYPO-XL. During inference, we\nmask part of an input literal sentence and over-generate multiple possible\nhyperbolic versions. Then a BERT-based ranker selects the best candidate by\nhyperbolicity and paraphrase quality. Human evaluation results show that our\nmodel is capable of generating hyperbolic paraphrase sentences and outperforms\nseveral baseline systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Laws for Neural Machine Translation. (arXiv:2109.07740v1 [cs.LG])","link":"http://arxiv.org/abs/2109.07740","description":"<p>We present an empirical study of scaling properties of encoder-decoder\nTransformer models used in neural machine translation (NMT). We show that\ncross-entropy loss as a function of model size follows a certain scaling law.\nSpecifically (i) We propose a formula which describes the scaling behavior of\ncross-entropy loss as a bivariate function of encoder and decoder size, and\nshow that it gives accurate predictions under a variety of scaling approaches\nand languages; we show that the total number of parameters alone is not\nsufficient for such purposes. (ii) We observe different power law exponents\nwhen scaling the decoder vs scaling the encoder, and provide recommendations\nfor optimal allocation of encoder/decoder capacity based on this observation.\n(iii) We also report that the scaling behavior of the model is acutely\ninfluenced by composition bias of the train/test sets, which we define as any\ndeviation from naturally generated text (either via machine generated or human\ntranslated text). We observe that natural text on the target side enjoys\nscaling, which manifests as successful reduction of the cross-entropy loss.\n(iv) Finally, we investigate the relationship between the cross-entropy loss\nand the quality of the generated translations. We find two different behaviors,\ndepending on the nature of the test data. For test sets which were originally\ntranslated from target language to source language, both loss and BLEU score\nimprove as model size increases. In contrast, for test sets originally\ntranslated from source language to target language, the loss improves, but the\nBLEU score stops improving after a certain threshold. We release generated text\nfrom all models used in this study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_B/0/1/0/all/0/1\">Behrooz Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krikun_M/0/1/0/all/0/1\">Maxim Krikun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chelba_C/0/1/0/all/0/1\">Ciprian Chelba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherry_C/0/1/0/all/0/1\">Colin Cherry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spanish Biomedical Crawled Corpus: A Large, Diverse Dataset for Spanish Biomedical Language Models. (arXiv:2109.07765v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07765","description":"<p>We introduce CoWeSe (the Corpus Web Salud Espa\\~nol), the largest Spanish\nbiomedical corpus to date, consisting of 4.5GB (about 750M tokens) of clean\nplain text. CoWeSe is the result of a massive crawler on 3000 Spanish domains\nexecuted in 2020. The corpus is openly available and already preprocessed.\nCoWeSe is an important resource for biomedical and health NLP in Spanish and\nhas already been employed to train domain-specific language models and to\nproduce word embbedings. We released the CoWeSe corpus under a Creative Commons\nAttribution 4.0 International license, both in Zenodo\n(\\url{https://zenodo.org/record/4561971\\#.YTI5SnVKiEA}).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1\">Casimiro Pio Carrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonet_O/0/1/0/all/0/1\">Ona de Gibert Bonet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krallinger_M/0/1/0/all/0/1\">Martin Krallinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Emotion Consensus and Utilizing Unpaired Data for Empathetic Dialogue Generation. (arXiv:2109.07779v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07779","description":"<p>Researches on dialogue empathy aim to endow an agent with the capacity of\naccurate understanding and proper responding for emotions. Existing models for\nempathetic dialogue generation focus on the emotion flow in one direction, that\nis, from the context to response. We argue that conducting an empathetic\nconversation is a bidirectional process, where empathy occurs when the emotions\nof two interlocutors could converge on the same point, i.e., reaching an\nemotion consensus. Besides, we also find that the empathetic dialogue corpus is\nextremely limited, which further restricts the model performance. To address\nthe above issues, we propose a dual-generative model, Dual-Emp, to\nsimultaneously construct the emotion consensus and utilize some external\nunpaired data. Specifically, our model integrates a forward dialogue model, a\nbackward dialogue model, and a discrete latent variable representing the\nemotion consensus into a unified architecture. Then, to alleviate the\nconstraint of paired data, we extract unpaired emotional data from open-domain\nconversations and employ Dual-Emp to produce pseudo paired empathetic samples,\nwhich is more efficient and low-cost than the human annotation. Automatic and\nhuman evaluations demonstrate that our method outperforms competitive baselines\nin producing coherent and empathetic responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_J/0/1/0/all/0/1\">Jiao Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Neural Machine Translation by Bidirectional Training. (arXiv:2109.07780v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07780","description":"<p>We present a simple and effective pretraining strategy -- bidirectional\ntraining (BiT) for neural machine translation. Specifically, we bidirectionally\nupdate the model parameters at the early stage and then tune the model\nnormally. To achieve bidirectional updating, we simply reconstruct the training\nsamples from \"src$\\rightarrow$tgt\" to \"src+tgt$\\rightarrow$tgt+src\" without any\ncomplicated model modifications. Notably, our approach does not increase any\nparameters or training steps, requiring the parallel data merely. Experimental\nresults show that BiT pushes the SOTA neural machine translation performance\nacross 15 translation tasks on 8 language pairs (data sizes range from 160K to\n38M) significantly higher. Encouragingly, our proposed model can complement\nexisting data manipulation strategies, i.e. back translation, data\ndistillation, and data diversification. Extensive analyses show that our\napproach functions as a novel bilingual code-switcher, obtaining better\nbilingual alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transductive Learning for Unsupervised Text Style Transfer. (arXiv:2109.07812v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07812","description":"<p>Unsupervised style transfer models are mainly based on an inductive learning\napproach, which represents the style as embeddings, decoder parameters, or\ndiscriminator parameters and directly applies these general rules to the test\ncases. However, the lacking of parallel corpus hinders the ability of these\ninductive learning methods on this task. As a result, it is likely to cause\nsevere inconsistent style expressions, like `the salad is rude`. To tackle this\nproblem, we propose a novel transductive learning approach in this paper, based\non a retrieval-based context-aware style representation. Specifically, an\nattentional encoder-decoder with a retriever framework is utilized. It involves\ntop-K relevant sentences in the target style in the transfer process. In this\nway, we can learn a context-aware style embedding to alleviate the above\ninconsistency problem. In this paper, both sparse (BM25) and dense retrieval\nfunctions (MIPS) are used, and two objective functions are designed to\nfacilitate joint learning. Experimental results show that our method\noutperforms several strong baselines. The proposed transductive learning\napproach is general and effective to the task of unsupervised style transfer,\nand we will apply it to the other two typical methods in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1\">Fei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reframing Instructional Prompts to GPTk's Language. (arXiv:2109.07830v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07830","description":"<p>How can model designers turn task instructions into effective prompts for\nlanguage models? Backed by extensive empirical analysis on GPT3, we observe\nimportant features for successful instructional prompts, and propose several\nreframing techniques for model designers to create such prompts. For example, a\ncomplex task can be decomposed into multiple simpler tasks. We experiment over\n12 NLP tasks across 6 diverse categories (question generation, classification,\netc.). Our results show that reframing improves few-shot learning performance\nby 14\\% while reducing sample complexity over existing few-shot baselines. The\nperformance gains are particularly important on large language models, such as\nGPT3 where tuning models or prompts on large datasets is not feasible.\nFurthermore, we observe that such gains are not limited to GPT3; the reframed\ntasks remain superior over raw instructions across different model\narchitectures, underscoring the cross-model generality of these guidelines. We\nhope these empirical-driven techniques will pave way for more effective ways to\nprompt LMs in future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings. (arXiv:2109.07833v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07833","description":"<p>Natural language inference (NLI) requires models to learn and apply\ncommonsense knowledge. These reasoning abilities are particularly important for\nexplainable NLI systems that generate a natural language explanation in\naddition to their label prediction. The integration of external knowledge has\nbeen shown to improve NLI systems, here we investigate whether it can also\nimprove their explanation capabilities. For this, we investigate different\nsources of external knowledge and evaluate the performance of our models on\nin-domain data as well as on special transfer datasets that are designed to\nassess fine-grained reasoning capabilities. We find that different sources of\nknowledge have a different effect on reasoning abilities, for example, implicit\nknowledge stored in language models can hinder reasoning on numbers and\nnegations. Finally, we conduct the largest and most fine-grained explainable\nNLI crowdsourcing study to date. It reveals that even large differences in\nautomatic performance scores do neither reflect in human ratings of label,\nexplanation, commonsense nor grammar correctness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuff_H/0/1/0/all/0/1\">Hendrik Schuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hsiu-Yu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Language Model Understood the Prompt was Ambiguous: Probing Syntactic Uncertainty Through Generation. (arXiv:2109.07848v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07848","description":"<p>Temporary syntactic ambiguities arise when the beginning of a sentence is\ncompatible with multiple syntactic analyses. We inspect to which extent neural\nlanguage models (LMs) exhibit uncertainty over such analyses when processing\ntemporarily ambiguous inputs, and how that uncertainty is modulated by\ndisambiguating cues. We probe the LM's expectations by generating from it: we\nuse stochastic decoding to derive a set of sentence completions, and estimate\nthe probability that the LM assigns to each interpretation based on the\ndistribution of parses across completions. Unlike scoring-based methods for\ntargeted syntactic evaluation, this technique makes it possible to explore\ncompletions that are not hypothesized in advance by the researcher. We apply\nthis method to study the behavior of two LMs (GPT2 and an LSTM) on three types\nof temporary ambiguity, using materials from human sentence processing\nexperiments. We find that LMs can track multiple analyses simultaneously; the\ndegree of uncertainty varies across constructions and contexts. As a response\nto disambiguating cues, the LMs often select the correct interpretation, but\noccasional errors point to potential areas of improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aina_L/0/1/0/all/0/1\">Laura Aina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translation Transformers Rediscover Inherent Data Domains. (arXiv:2109.07864v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07864","description":"<p>Many works proposed methods to improve the performance of Neural Machine\nTranslation (NMT) models in a domain/multi-domain adaptation scenario. However,\nan understanding of how NMT baselines represent text domain information\ninternally is still lacking. Here we analyze the sentence representations\nlearned by NMT Transformers and show that these explicitly include the\ninformation on text domains, even after only seeing the input sentences without\ndomains labels. Furthermore, we show that this internal information is enough\nto cluster sentences by their underlying domains without supervision. We show\nthat NMT models produce clusters better aligned to the actual domains compared\nto pre-trained language models (LMs). Notably, when computed on document-level,\nNMT cluster-to-domain correspondence nears 100%. We use these findings together\nwith an approach to NMT domain adaptation using automatically extracted\ndomains. Whereas previous work relied on external LMs for text clustering, we\npropose re-using the NMT model as a source of unsupervised clusters. We perform\nan extensive experimental study comparing two approaches across two data\nscenarios, three language pairs, and both sentence-level and document-level\nclustering, showing equal or significantly superior performance compared to\nLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Del_M/0/1/0/all/0/1\">Maksym Del</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korotkova_E/0/1/0/all/0/1\">Elizaveta Korotkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fishel_M/0/1/0/all/0/1\">Mark Fishel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Humanly Certifying Superhuman Classifiers. (arXiv:2109.07867v1 [cs.LG])","link":"http://arxiv.org/abs/2109.07867","description":"<p>Estimating the performance of a machine learning system is a longstanding\nchallenge in artificial intelligence research. Today, this challenge is\nespecially relevant given the emergence of systems which appear to increasingly\noutperform human beings. In some cases, this \"superhuman\" performance is\nreadily demonstrated; for example by defeating legendary human players in\ntraditional two player games. On the other hand, it can be challenging to\nevaluate classification models that potentially surpass human performance.\nIndeed, human annotations are often treated as a ground truth, which implicitly\nassumes the superiority of the human over any models trained on human\nannotations. In reality, human annotators can make mistakes and be subjective.\nEvaluating the performance with respect to a genuine oracle may be more\nobjective and reliable, even when querying the oracle is expensive or\nimpossible. In this paper, we first raise the challenge of evaluating the\nperformance of both humans and models with respect to an oracle which is\nunobserved. We develop a theory for estimating the accuracy compared to the\noracle, using only imperfect human annotations for reference. Our analysis\nprovides a simple recipe for detecting and certifying superhuman performance in\nthis setting, which we believe will assist in understanding the stage of\ncurrent research on classification. We validate the convergence of the bounds\nand the assumptions of our theory on carefully designed toy experiments with\nknown oracles. Moreover, we demonstrate the utility of our theory by\nmeta-analyzing large-scale natural language processing tasks, for which an\noracle does not exist, and show that under our assumptions a number of models\nfrom recent years are with high probability superhuman.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiongkai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walder_C/0/1/0/all/0/1\">Christian Walder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenchen Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MFE-NER: Multi-feature Fusion Embedding for Chinese Named Entity Recognition. (arXiv:2109.07877v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07877","description":"<p>Pre-trained language models lead Named Entity Recognition (NER) into a new\nera, while some more knowledge is needed to improve their performance in\nspecific problems. In Chinese NER, character substitution is a complicated\nlinguistic phenomenon. Some Chinese characters are quite similar for sharing\nthe same components or having similar pronunciations. People replace characters\nin a named entity with similar characters to generate a new collocation but\nreferring to the same object. It becomes even more common in the Internet age\nand is often used to avoid Internet censorship or just for fun. Such character\nsubstitution is not friendly to those pre-trained language models because the\nnew collocations are occasional. As a result, it always leads to unrecognizable\nor recognition errors in the NER task. In this paper, we propose a new method,\nMulti-Feature Fusion Embedding for Chinese Named Entity Recognition (MFE-NER),\nto strengthen the language pattern of Chinese and handle the character\nsubstitution problem in Chinese Named Entity Recognition. MFE fuses semantic,\nglyph, and phonetic features together. In the glyph domain, we disassemble\nChinese characters into components to denote structure features so that\ncharacters with similar structures can have close embedding space\nrepresentation. Meanwhile, an improved phonetic system is also proposed in our\nwork, making it reasonable to calculate phonetic similarity among Chinese\ncharacters. Experiments demonstrate that our method improves the overall\nperformance of Chinese NER and especially performs well in informal language\nenvironments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiatong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_K/0/1/0/all/0/1\">Kui Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surveying the Research on Fake News in Social Media: a Tale of Networks and Language. (arXiv:2109.07909v1 [cs.CY])","link":"http://arxiv.org/abs/2109.07909","description":"<p>The history of journalism and news diffusion is tightly coupled with the\neffort to dispel hoaxes, misinformation, propaganda, unverified rumours, poor\nreporting, and messages containing hate and divisions. With the explosive\ngrowth of online social media and billions of individuals engaged with\nconsuming, creating, and sharing news, this ancient problem has surfaced with a\nrenewed intensity threatening our democracies, public health, and news outlets\ncredibility. This has triggered many researchers to develop new methods for\nstudying, understanding, detecting, and preventing fake-news diffusion; as a\nconsequence, thousands of scientific papers have been published in a relatively\nshort period, making researchers of different disciplines to struggle in search\nof open problems and most relevant trends. The aim of this survey is threefold:\nfirst, we want to provide the researchers interested in this multidisciplinary\nand challenging area with a network-based analysis of the existing literature\nto assist them with a visual exploration of papers that can be of interest;\nsecond, we present a selection of the main results achieved so far adopting the\nnetwork as an unifying framework to represent and make sense of data, to model\ndiffusion processes, and to evaluate different debunking strategies. Finally,\nwe present an outline of the most relevant research trends focusing on the\nmoving target of fake-news, bots, and trolls identification by means of data\nmining and text technologies; despite scholars working on computational\nlinguistics and networks traditionally belong to different scientific\ncommunities, we expect that forthcoming computational approaches to prevent\nfake news from polluting the social media must be developed using hybrid and\nup-to-date methodologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruffo_G/0/1/0/all/0/1\">Giancarlo Ruffo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Semeraro_A/0/1/0/all/0/1\">Alfonso Semeraro</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Giachanou_A/0/1/0/all/0/1\">Anastasia Giachanou</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_P/0/1/0/all/0/1\">Paolo Rosso</a> (3) ((1) Universit&#xe0; degli Studi di Torino, (2) Utrecht University, (3) Universitat Polit&#xe8;cnica de Val&#xe8;ncia)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Search for a Search Method -- Simple Heuristics Suffice for Adversarial Text Attacks. (arXiv:2109.07926v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07926","description":"<p>Recently more attention has been given to adversarial attacks on neural\nnetworks for natural language processing (NLP). A central research topic has\nbeen the investigation of search algorithms and search constraints, accompanied\nby benchmark algorithms and tasks. We implement an algorithm inspired by zeroth\norder optimization-based attacks and compare with the benchmark results in the\nTextAttack framework. Surprisingly, we find that optimization-based methods do\nnot yield any improvement in a constrained setup and slightly benefit from\napproximate gradient information only in unconstrained setups where search\nspaces are larger. In contrast, simple heuristics exploiting nearest neighbors\nwithout querying the target function yield substantial success rates in\nconstrained setups, and nearly full success rate in unconstrained setups, at an\norder of magnitude fewer queries. We conclude from these results that current\nTextAttack benchmark tasks are too easy and constraints are too strict,\npreventing meaningful research on black-box adversarial text attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berger_N/0/1/0/all/0/1\">Nathaniel Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1\">Stefan Riezler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolov_A/0/1/0/all/0/1\">Artem Sokolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebert_S/0/1/0/all/0/1\">Sebastian Ebert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RetrievalSum: A Retrieval Enhanced Framework for Abstractive Summarization. (arXiv:2109.07943v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07943","description":"<p>Existing summarization systems mostly generate summaries purely relying on\nthe content of the source document. However, even for humans, we usually need\nsome references or exemplars to help us fully understand the source document\nand write summaries in a particular format. But how to find the high-quality\nexemplars and incorporate them into summarization systems is still challenging\nand worth exploring. In this paper, we propose RetrievalSum, a novel retrieval\nenhanced abstractive summarization framework consisting of a dense Retriever\nand a Summarizer. At first, several closely related exemplars are retrieved as\nsupplementary input to help the generation model understand the text more\ncomprehensively. Furthermore, retrieved exemplars can also play a role in\nguiding the model to capture the writing style of a specific corpus. We\nvalidate our method on a wide range of summarization datasets across multiple\ndomains and two backbone models: BERT and BART. Results show that our framework\nobtains significant improvement by 1.38~4.66 in ROUGE-1 score when compared\nwith the powerful pre-trained models, and achieve new state-of-the-art on\nBillSum. Human evaluation demonstrates that our retrieval enhanced model can\nbetter capture the domain-specific writing style.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_C/0/1/0/all/0/1\">Chenxin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zhichao Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianqiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Attribute Injection for Pretrained Language Models. (arXiv:2109.07953v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07953","description":"<p>Metadata attributes (e.g., user and product IDs from reviews) can be\nincorporated as additional inputs to neural-based NLP models, by modifying the\narchitecture of the models, in order to improve their performance. Recent\nmodels however rely on pretrained language models (PLMs), where previously used\ntechniques for attribute injection are either nontrivial or ineffective. In\nthis paper, we propose a lightweight and memory-efficient method to inject\nattributes to PLMs. We extend adapters, i.e. tiny plug-in feed-forward modules,\nto include attributes both independently of or jointly with the text. To limit\nthe increase of parameters especially when the attribute vocabulary is large,\nwe use low-rank approximations and hypercomplex multiplications, significantly\ndecreasing the total parameters. We also introduce training mechanisms to\nhandle domains in which attributes can be multi-labeled or sparse. Extensive\nexperiments and analyses on eight datasets from different domains show that our\nmethod outperforms previous attribute injection methods and achieves\nstate-of-the-art performance on various datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amplayo_R/0/1/0/all/0/1\">Reinald Kim Amplayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Unsupervised Question Answering via Summarization-Informed Question Generation. (arXiv:2109.07954v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07954","description":"<p>Question Generation (QG) is the task of generating a plausible question for a\ngiven &lt;passage, answer&gt; pair. Template-based QG uses linguistically-informed\nheuristics to transform declarative sentences into interrogatives, whereas\nsupervised QG uses existing Question Answering (QA) datasets to train a system\nto generate a question given a passage and an answer. A disadvantage of the\nheuristic approach is that the generated questions are heavily tied to their\ndeclarative counterparts. A disadvantage of the supervised approach is that\nthey are heavily tied to the domain/language of the QA dataset used as training\ndata. In order to overcome these shortcomings, we propose an unsupervised QG\nmethod which uses questions generated heuristically from summaries as a source\nof training data for a QG system. We make use of freely available news summary\ndata, transforming declarative summary sentences into appropriate questions\nusing heuristics informed by dependency parsing, named entity recognition and\nsemantic role labeling. The resulting questions are then combined with the\noriginal news articles to train an end-to-end neural QG model. We extrinsically\nevaluate our approach using unsupervised QA: our QG model is used to generate\nsynthetic QA pairs for training a QA model. Experimental results show that,\ntrained with only 20k English Wikipedia-based synthetic QA pairs, the QA model\nsubstantially outperforms previous unsupervised models on three in-domain\ndatasets (SQuAD1.1, Natural Questions, TriviaQA) and three out-of-domain\ndatasets (NewsQA, BioASQ, DuoRC), demonstrating the transferability of the\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chenyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_Y/0/1/0/all/0/1\">Yvette Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jennifer Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods. (arXiv:2109.07958v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07958","description":"<p>We propose a benchmark to measure whether a language model is truthful in\ngenerating answers to questions. The benchmark comprises 817 questions that\nspan 38 categories, including health, law, finance and politics. We crafted\nquestions that some humans would answer falsely due to a false belief or\nmisconception. To perform well, models must avoid generating false answers\nlearned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a\nT5-based model. The best model was truthful on 58% of questions, while human\nperformance was 94%. Models generated many false answers that mimic popular\nmisconceptions and have the potential to deceive humans. The largest models\nwere generally the least truthful. For example, the 6B-parameter GPT-J model\nwas 17% less truthful than its 125M-parameter counterpart. This contrasts with\nother NLP tasks, where performance improves with model size. However, this\nresult is expected if false answers are learned from the training distribution.\nWe suggest that scaling up models alone is less promising for improving\ntruthfulness than fine-tuning using training objectives other than imitation of\ntext from the web.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephanie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilton_J/0/1/0/all/0/1\">Jacob Hilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_O/0/1/0/all/0/1\">Owain Evans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alquist 4.0: Towards Social Intelligence Using Generative Models and Dialogue Personalization. (arXiv:2109.07968v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07968","description":"<p>The open domain-dialogue system Alquist has a goal to conduct a coherent and\nengaging conversation that can be considered as one of the benchmarks of social\nintelligence. The fourth version of the system, developed within the Alexa\nPrize Socialbot Grand Challenge 4, brings two main innovations. The first\naddresses coherence, and the second addresses the engagingness of the\nconversation. For innovations regarding coherence, we propose a novel hybrid\napproach combining hand-designed responses and a generative model. The proposed\napproach utilizes hand-designed dialogues, out-of-domain detection, and a\nneural response generator. Hand-designed dialogues walk the user through\nhigh-quality conversational flows. The out-of-domain detection recognizes that\nthe user diverges from the predefined flow and prevents the system from\nproducing a scripted response that might not make sense for unexpected user\ninput. Finally, the neural response generator generates a response based on the\ncontext of the dialogue that correctly reacts to the unexpected user input and\nreturns the dialogue to the boundaries of hand-designed dialogues. The\ninnovations for engagement that we propose are mostly inspired by the famous\nexploration-exploitation dilemma. To conduct an engaging conversation with the\ndialogue partners, one has to learn their preferences and interests --\nexploration. Moreover, to engage the partner, we have to utilize the knowledge\nwe have already learned -- exploitation. In this work, we present the\nprinciples and inner workings of individual components of the open-domain\ndialogue system Alquist developed within the Alexa Prize Socialbot Grand\nChallenge 4 and the experiments we have conducted to evaluate them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Konrad_J/0/1/0/all/0/1\">Jakub Konr&#xe1;d</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pichl_J/0/1/0/all/0/1\">Jan Pichl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marek_P/0/1/0/all/0/1\">Petr Marek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenc_P/0/1/0/all/0/1\">Petr Lorenc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ta_V/0/1/0/all/0/1\">Van Duy Ta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobza_O/0/1/0/all/0/1\">Ond&#x159;ej Kobza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hylova_L/0/1/0/all/0/1\">Lenka H&#xfd;lov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedivy_J/0/1/0/all/0/1\">Jan &#x160;ediv&#xfd;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Language Models Know the Way to Rome?. (arXiv:2109.07971v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07971","description":"<p>The global geometry of language models is important for a range of\napplications, but language model probes tend to evaluate rather local\nrelations, for which ground truths are easily obtained. In this paper we\nexploit the fact that in geography, ground truths are available beyond local\nrelations. In a series of experiments, we evaluate the extent to which language\nmodel representations of city and country names are isomorphic to real-world\ngeography, e.g., if you tell a language model where Paris and Berlin are, does\nit know the way to Rome? We find that language models generally encode limited\ngeographic information, but with larger models performing the best, suggesting\nthat geographic knowledge can be induced from higher-order co-occurrence\nstatistics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lietard_B/0/1/0/all/0/1\">Bastien Li&#xe9;tard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdou_M/0/1/0/all/0/1\">Mostafa Abdou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Let the CAT out of the bag: Contrastive Attributed explanations for Text. (arXiv:2109.07983v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07983","description":"<p>Contrastive explanations for understanding the behavior of black box models\nhas gained a lot of attention recently as they provide potential for recourse.\nIn this paper, we propose a method Contrastive Attributed explanations for Text\n(CAT) which provides contrastive explanations for natural language text data\nwith a novel twist as we build and exploit attribute classifiers leading to\nmore semantically meaningful explanations. To ensure that our contrastive\ngenerated text has the fewest possible edits with respect to the original text,\nwhile also being fluent and close to a human generated contrastive, we resort\nto a minimal perturbation approach regularized using a BERT language model and\nattribute classifiers trained on available attributes. We show through\nqualitative examples and a user study that our method not only conveys more\ninsight because of these attributes, but also leads to better quality\n(contrastive) text. Moreover, quantitatively we show that our method is more\nefficient than other state-of-the-art methods with it also scoring higher on\nbenchmark metrics such as flip rate, (normalized) Levenstein distance, fluency\nand content preservation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chemmengath_S/0/1/0/all/0/1\">Saneem Chemmengath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azad_A/0/1/0/all/0/1\">Amar Prakash Azad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luss_R/0/1/0/all/0/1\">Ronny Luss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhurandhar_A/0/1/0/all/0/1\">Amit Dhurandhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-aware Entity Typing in Knowledge Graphs. (arXiv:2109.07990v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07990","description":"<p>Knowledge graph entity typing aims to infer entities' missing types in\nknowledge graphs which is an important but under-explored issue. This paper\nproposes a novel method for this task by utilizing entities' contextual\ninformation. Specifically, we design two inference mechanisms: i) N2T:\nindependently use each neighbor of an entity to infer its type; ii) Agg2T:\naggregate the neighbors of an entity to infer its type. Those mechanisms will\nproduce multiple inference results, and an exponentially weighted pooling\nmethod is used to generate the final inference result. Furthermore, we propose\na novel loss function to alleviate the false-negative problem during training.\nExperiments on two real-world KGs demonstrate the effectiveness of our method.\nThe source code and data of this paper can be obtained from\nhttps://github.com/CCIIPLab/CET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Weiran Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KnowMAN: Weakly Supervised Multinomial Adversarial Networks. (arXiv:2109.07994v1 [cs.LG])","link":"http://arxiv.org/abs/2109.07994","description":"<p>The absence of labeled data for training neural models is often addressed by\nleveraging knowledge about the specific task, resulting in heuristic but noisy\nlabels. The knowledge is captured in labeling functions, which detect certain\nregularities or patterns in the training samples and annotate corresponding\nlabels for training. This process of weakly supervised training may result in\nan over-reliance on the signals captured by the labeling functions and hinder\nmodels to exploit other signals or to generalize well. We propose KnowMAN, an\nadversarial scheme that enables to control influence of signals associated with\nspecific labeling functions. KnowMAN forces the network to learn\nrepresentations that are invariant to those signals and to pick up other\nsignals that are more generally associated with an output label. KnowMAN\nstrongly improves results compared to direct weakly supervised learning with a\npre-trained transformer language model and a feature-based baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marz_L/0/1/0/all/0/1\">Luisa M&#xe4;rz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgari_E/0/1/0/all/0/1\">Ehsaneddin Asgari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braune_F/0/1/0/all/0/1\">Fabienne Braune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_F/0/1/0/all/0/1\">Franziska Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The NiuTrans System for the WMT21 Efficiency Task. (arXiv:2109.08003v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08003","description":"<p>This paper describes the NiuTrans system for the WMT21 translation efficiency\ntask (<a href=\"http://statmt.org/wmt21/efficiency-task.html\">this http URL</a>). Following last year's\nwork, we explore various techniques to improve efficiency while maintaining\ntranslation quality. We investigate the combinations of lightweight Transformer\narchitectures and knowledge distillation strategies. Also, we improve the\ntranslation efficiency with graph optimization, low precision, dynamic\nbatching, and parallel pre/post-processing. Our system can translate 247,000\nwords per second on an NVIDIA A100, being 3$\\times$ faster than last year's\nsystem. Our system is the fastest and has the lowest memory consumption on the\nGPU-throughput track. The code, model, and pipeline will be available at\nNiuTrans.NMT (https://github.com/NiuTrans/NiuTrans.NMT).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenglong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yongyu Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhongxiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Siming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Minyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Ye Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The NiuTrans System for WNGT 2020 Efficiency Task. (arXiv:2109.08008v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08008","description":"<p>This paper describes the submissions of the NiuTrans Team to the WNGT 2020\nEfficiency Shared Task. We focus on the efficient implementation of deep\nTransformer models \\cite{wang-etal-2019-learning, li-etal-2019-niutrans} using\nNiuTensor (https://github.com/NiuTrans/NiuTensor), a flexible toolkit for NLP\ntasks. We explored the combination of deep encoder and shallow decoder in\nTransformer models via model compression and knowledge distillation. The neural\nmachine translation decoding also benefits from FP16 inference, attention\ncaching, dynamic batching, and batch pruning. Our systems achieve promising\nresults in both translation quality and efficiency, e.g., our fastest system\ncan translate more than 40,000 tokens per second with an RTX 2080 Ti while\nmaintaining 42.9 BLEU on \\textit{newstest2018}. The code, models, and docker\nimages are available at NiuTrans.NMT\n(https://github.com/NiuTrans/NiuTrans.NMT).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Ye Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinqiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenglong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Propaganda Techniques in Memes. (arXiv:2109.08013v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08013","description":"<p>Propaganda can be defined as a form of communication that aims to influence\nthe opinions or the actions of people towards a specific goal; this is achieved\nby means of well-defined rhetorical and psychological devices. Propaganda, in\nthe form we know it today, can be dated back to the beginning of the 17th\ncentury. However, it is with the advent of the Internet and the social media\nthat it has started to spread on a much larger scale than before, thus becoming\nmajor societal and political issue. Nowadays, a large fraction of propaganda in\nsocial media is multimodal, mixing textual with visual content. With this in\nmind, here we propose a new multi-label multimodal task: detecting the type of\npropaganda techniques used in memes. We further create and release a new corpus\nof 950 memes, carefully annotated with 22 propaganda techniques, which can\nappear in the text, in the image, or in both. Our analysis of the corpus shows\nthat understanding both modalities together is essential for detecting these\ntechniques. This is further confirmed in our experiments with several\nstate-of-the-art multimodal models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1\">Dimitar Dimitrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_B/0/1/0/all/0/1\">Bishr Bin Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1\">Fabrizio Silvestri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Concept of Semantic Value in Social Network Analysis: an Application to Comparative Mythology. (arXiv:2109.08023v1 [cs.SI])","link":"http://arxiv.org/abs/2109.08023","description":"<p>Human sciences have traditionally relied on human reasoning and intelligence\nto infer knowledge from a wide range of sources, such as oral and written\nnarrations, reports, and traditions. Here we develop an extension of classical\nsocial network analysis approaches to incorporate the concept of meaning in\neach actor, as a mean to quantify and infer further knowledge from the original\nsource of the network. This extension is based on a new affinity function, the\nsemantic affinity, that establishes fuzzy-like relationships between the\ndifferent actors in the network, using combinations of affinity functions. We\nalso propose a new heuristic algorithm based on the shortest capacity problem\nto compute this affinity function. We use these concept of meaning and semantic\naffinity to analyze and compare the gods and heroes from three different\nclassical mythologies: Greek, Celtic and Nordic. We study the relationships of\neach individual mythology and those of common structure that is formed when we\nfuse the three of them. We show a strong connection between the Celtic and\nNordic gods and that Greeks put more emphasis on heroic characters rather than\ndeities. Our approach provides a technique to highlight and quantify important\nrelationships in the original domain of the network not deducible from its\nstructural properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fumanal_Idocin_J/0/1/0/all/0/1\">Javier Fumanal-Idocin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordon_O/0/1/0/all/0/1\">Oscar Cord&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimuro_G/0/1/0/all/0/1\">Gra&#xe7;aliz Dimuro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minarova_M/0/1/0/all/0/1\">Mar&#xed;a Min&#xe1;rov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bustince_H/0/1/0/all/0/1\">Humberto Bustince</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Locating Language-Specific Information in Contextualized Embeddings. (arXiv:2109.08040v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08040","description":"<p>Multilingual pretrained language models (MPLMs) exhibit multilinguality and\nare well suited for transfer across languages. Most MPLMs are trained in an\nunsupervised fashion and the relationship between their objective and\nmultilinguality is unclear. More specifically, the question whether MPLM\nrepresentations are language-agnostic or they simply interleave well with\nlearned task prediction heads arises. In this work, we locate language-specific\ninformation in MPLMs and identify its dimensionality and the layers where this\ninformation occurs. We show that language-specific information is scattered\nacross many dimensions, which can be projected into a linear subspace. Our\nstudy contributes to a better understanding of MPLM representations, going\nbeyond treating them as unanalyzable blobs of information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Sheng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufter_P/0/1/0/all/0/1\">Philipp Dufter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Error Type Annotation for Arabic. (arXiv:2109.08068v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08068","description":"<p>We present ARETA, an automatic error type annotation system for Modern\nStandard Arabic. We design ARETA to address Arabic's morphological richness and\northographic ambiguity. We base our error taxonomy on the Arabic Learner Corpus\n(ALC) Error Tagset with some modifications. ARETA achieves a performance of\n85.8% (micro average F1 score) on a manually annotated blind test portion of\nALC. We also demonstrate ARETA's usability by applying it to a number of\nsubmissions from the QALB 2014 shared task for Arabic grammatical error\ncorrection. The resulting analyses give helpful insights on the strengths and\nweaknesses of different submissions, which is more useful than the opaque M2\nscoring metrics used in the shared task. ARETA employs a large Arabic\nmorphological analyzer, but is completely unsupervised otherwise. We make ARETA\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belkebir_R/0/1/0/all/0/1\">Riadh Belkebir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Open Information Extraction using Question Generation and Reading Comprehension. (arXiv:2109.08079v1 [cs.IR])","link":"http://arxiv.org/abs/2109.08079","description":"<p>Typically, Open Information Extraction (OpenIE) focuses on extracting\ntriples, representing a subject, a relation, and the object of the relation.\nHowever, most of the existing techniques are based on a predefined set of\nrelations in each domain which limits their applicability to newer domains\nwhere these relations may be unknown such as financial documents. This paper\npresents a zero-shot open information extraction technique that extracts the\nentities (value) and their descriptions (key) from a sentence, using off the\nshelf machine reading comprehension (MRC) Model. The input questions to this\nmodel are created using a novel noun phrase generation method. This method\ntakes the context of the sentence into account and can create a wide variety of\nquestions making our technique domain independent. Given the questions and the\nsentence, our technique uses the MRC model to extract entities (value). The\nnoun phrase corresponding to the question, with the highest confidence, is\ntaken as the description (key).\n</p>\n<p>This paper also introduces the EDGAR10-Q dataset which is based on publicly\navailable financial documents from corporations listed in US securities and\nexchange commission (SEC). The dataset consists of paragraphs, tagged values\n(entities), and their keys (descriptions) and is one of the largest among\nentity extraction datasets. This dataset will be a valuable addition to the\nresearch community, especially in the financial domain. Finally, the paper\ndemonstrates the efficacy of the proposed technique on the EDGAR10-Q and Ade\ncorpus drug dosage datasets, where it obtained 86.84 % and 97% accuracy,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badugu_A/0/1/0/all/0/1\">Amogh Badugu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_T/0/1/0/all/0/1\">Tamanna Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_H/0/1/0/all/0/1\">Himanshu Sharad Bhatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MeLT: Message-Level Transformer with Masked Document Representations as Pre-Training for Stance Detection. (arXiv:2109.08113v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08113","description":"<p>Much of natural language processing is focused on leveraging large capacity\nlanguage models, typically trained over single messages with a task of\npredicting one or more tokens. However, modeling human language at\nhigher-levels of context (i.e., sequences of messages) is under-explored. In\nstance detection and other social media tasks where the goal is to predict an\nattribute of a message, we have contextual data that is loosely semantically\nconnected by authorship. Here, we introduce Message-Level Transformer (MeLT) --\na hierarchical message-encoder pre-trained over Twitter and applied to the task\nof stance prediction. We focus on stance prediction as a task benefiting from\nknowing the context of the message (i.e., the sequence of previous messages).\nThe model is trained using a variant of masked-language modeling; where instead\nof predicting tokens, it seeks to generate an entire masked (aggregated)\nmessage vector via reconstruction loss. We find that applying this pre-trained\nmasked message-level transformer to the downstream task of stance detection\nachieves F1 performance of 67%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matero_M/0/1/0/all/0/1\">Matthew Matero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soni_N/0/1/0/all/0/1\">Nikita Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1\">H. Andrew Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Online Hate Speech through the Causal Lens. (arXiv:2109.08120v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08120","description":"<p>The societal issue of digital hostility has previously attracted a lot of\nattention. The topic counts an ample body of literature, yet remains prominent\nand challenging as ever due to its subjective nature. We posit that a better\nunderstanding of this problem will require the use of causal inference\nframeworks. This survey summarises the relevant research that revolves around\nestimations of causal effects related to online hate speech. Initially, we\nprovide an argumentation as to why re-establishing the exploration of hate\nspeech in causal terms is of the essence. Following that, we give an overview\nof the leading studies classified with respect to the direction of their\noutcomes, as well as an outline of all related research, and a summary of open\nresearch problems that can influence future work on the topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Founta_A/0/1/0/all/0/1\">Antigoni-Maria Founta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Tri-training of Dependency Parsers. (arXiv:2109.08122v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08122","description":"<p>We compare two orthogonal semi-supervised learning techniques, namely\ntri-training and pretrained word embeddings, in the task of dependency parsing.\nWe explore language-specific FastText and ELMo embeddings and multilingual BERT\nembeddings. We focus on a low resource scenario as semi-supervised learning can\nbe expected to have the most impact here. Based on treebank size and available\nELMo models, we select Hungarian, Uyghur (a zero-shot language for mBERT) and\nVietnamese. Furthermore, we include English in a simulated low-resource\nsetting. We find that pretrained word embeddings make more effective use of\nunlabelled data than tri-training but that the two approaches can be\nsuccessfully combined.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1\">Joachim Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jennifer Foster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Summary Evaluation Survive Translation to Other Languages?. (arXiv:2109.08129v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08129","description":"<p>The creation of a large summarization quality dataset is a considerable,\nexpensive, time-consuming effort, requiring careful planning and setup. It\nincludes producing human-written and machine-generated summaries and evaluation\nof the summaries by humans, preferably by linguistic experts, and by automatic\nevaluation tools. If such effort is made in one language, it would be\nbeneficial to be able to use it in other languages. To investigate how much we\ncan trust the translation of such dataset without repeating human annotations\nin another language, we translated an existing English summarization dataset,\nSummEval dataset, to four different languages and analyzed the scores from the\nautomatic evaluation metrics in translated languages, as well as their\ncorrelation with human annotations in the source language. Our results reveal\nthat although translation changes the absolute value of automatic scores, the\nscores keep the same rank order and approximately the same correlations with\nhuman annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iskender_N/0/1/0/all/0/1\">Neslihan Iskender</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilyev_O/0/1/0/all/0/1\">Oleg Vasilyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polzehl_T/0/1/0/all/0/1\">Tim Polzehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohannon_J/0/1/0/all/0/1\">John Bohannon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_S/0/1/0/all/0/1\">Sebastian M&#xf6;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phrase Retrieval Learns Passage Retrieval, Too. (arXiv:2109.08133v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08133","description":"<p>Dense retrieval methods have shown great promise over sparse retrieval\nmethods in a range of NLP problems. Among them, dense phrase retrieval-the most\nfine-grained retrieval unit-is appealing because phrases can be directly used\nas the output for question answering and slot filling tasks. In this work, we\nfollow the intuition that retrieving phrases naturally entails retrieving\nlarger text blocks and study whether phrase retrieval can serve as the basis\nfor coarse-level retrieval including passages and documents. We first observe\nthat a dense phrase-retrieval system, without any retraining, already achieves\nbetter passage retrieval accuracy (+3-5% in top-5 accuracy) compared to passage\nretrievers, which also helps achieve superior end-to-end QA performance with\nfewer passages. Then, we provide an interpretation for why phrase-level\nsupervision helps learn better fine-grained entailment compared to\npassage-level supervision, and also show that phrase retrieval can be improved\nto achieve competitive performance in document-retrieval tasks such as entity\nlinking and knowledge-grounded dialogue. Finally, we demonstrate how phrase\nfiltering and vector quantization can reduce the size of our index by 4-10x,\nmaking dense phrase retrieval a practical and versatile solution in\nmulti-granularity retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhyuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wettig_A/0/1/0/all/0/1\">Alexander Wettig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resources for Turkish Dependency Parsing: Introducing the BOUN Treebank and the BoAT Annotation Tool. (arXiv:2002.10416v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2002.10416","description":"<p>In this paper, we introduce the resources that we developed for Turkish\ndependency parsing, which include a novel manually annotated treebank (BOUN\nTreebank), along with the guidelines we adopted, and a new annotation tool\n(BoAT). The manual annotation process we employed was shaped and implemented by\na team of four linguists and five Natural Language Processing (NLP)\nspecialists. Decisions regarding the annotation of the BOUN Treebank were made\nin line with the Universal Dependencies (UD) framework as well as our recent\nefforts for unifying the Turkish UD treebanks through manual re-annotation. To\nthe best of our knowledge, BOUN Treebank is the largest Turkish treebank. It\ncontains a total of 9,761 sentences from various topics including biographical\ntexts, national newspapers, instructional texts, popular culture articles, and\nessays. In addition, we report the parsing results of a state-of-the-art\ndependency parser obtained over the BOUN Treebank as well as two other\ntreebanks in Turkish. Our results demonstrate that the unification of the\nTurkish annotation scheme and the introduction of a more comprehensive treebank\nlead to improved performance with regard to dependency parsing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Turk_U/0/1/0/all/0/1\">Utku T&#xfc;rk</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Atmaca_F/0/1/0/all/0/1\">Furkan Atmaca</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ozates_S/0/1/0/all/0/1\">&#x15e;aziye Bet&#xfc;l &#xd6;zate&#x15f;</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Berk_G/0/1/0/all/0/1\">G&#xf6;zde Berk</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Bedir_S/0/1/0/all/0/1\">Seyyit Talha Bedir</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Koksal_A/0/1/0/all/0/1\">Abdullatif K&#xf6;ksal</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Basaran_B/0/1/0/all/0/1\">Balk&#x131;z &#xd6;zt&#xfc;rk Ba&#x15f;aran</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gungor_T/0/1/0/all/0/1\">Tunga G&#xfc;ng&#xf6;r</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Ozgur_A/0/1/0/all/0/1\">Arzucan &#xd6;zg&#xfc;r</a> (2) ((1) Department of Linguistics Bo&#x11f;azi&#xe7;i University, (2) Department of Computer Engineering Bo&#x11f;azi&#xe7;i University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regex Queries over Incomplete Knowledge Bases. (arXiv:2005.00480v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.00480","description":"<p>We propose the novel task of answering regular expression queries (containing\ndisjunction ($\\vee$) and Kleene plus ($+$) operators) over incomplete KBs. The\nanswer set of these queries potentially has a large number of entities, hence\nprevious works for single-hop queries in KBC that model a query as a point in\nhigh-dimensional space are not as effective. In response, we develop RotatE-Box\n-- a novel combination of RotatE and box embeddings. It can model more\nrelational inference patterns compared to existing embedding based models.\nFurthermore, we define baseline approaches for embedding based KBC models to\nhandle regex operators. We demonstrate performance of RotatE-Box on two new\nregex-query datasets introduced in this paper, including one where the queries\nare harvested based on actual user query logs. We find that our final\nRotatE-Box model significantly outperforms models based on just RotatE and just\nbox embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adlakha_V/0/1/0/all/0/1\">Vaibhav Adlakha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Parth Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedathur_S/0/1/0/all/0/1\">Srikanta Bedathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Learning for End-to-End Automatic Speech Recognition. (arXiv:2005.04288v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2005.04288","description":"<p>In this paper, we propose an incremental learning method for end-to-end\nAutomatic Speech Recognition (ASR) which enables an ASR system to perform well\non new tasks while maintaining the performance on its originally learned ones.\nTo mitigate catastrophic forgetting during incremental learning, we design a\nnovel explainability-based knowledge distillation for ASR models, which is\ncombined with a response-based knowledge distillation to maintain the original\nmodel's predictions and the \"reason\" for the predictions. Our method works\nwithout access to the training data of original tasks, which addresses the\ncases where the previous data is no longer available or joint training is\ncostly. Results on a multi-stage sequential training task show that our method\noutperforms existing ones in mitigating forgetting. Furthermore, in two\npractical scenarios, compared to the target-reference joint training method,\nthe performance drop of our method is 0.02% Character Error Rate (CER), which\nis 97% smaller than the drops of the baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fu_L/0/1/0/all/0/1\">Li Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zi_L/0/1/0/all/0/1\">Libo Zi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengchen Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MLQE-PE: A Multilingual Quality Estimation and Post-Editing Dataset. (arXiv:2010.04480v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.04480","description":"<p>We present MLQE-PE, a new dataset for Machine Translation (MT) Quality\nEstimation (QE) and Automatic Post-Editing (APE). The dataset contains eleven\nlanguage pairs, with human labels for up to 10,000 translations per language\npair in the following formats: sentence-level direct assessments and\npost-editing effort, and word-level good/bad labels. It also contains the\npost-edited sentences, as well as titles of the articles where the sentences\nwere extracted from, and the neural MT models used to translate the text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fomicheva_M/0/1/0/all/0/1\">Marina Fomicheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shuo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_E/0/1/0/all/0/1\">Erick Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zerva_C/0/1/0/all/0/1\">Chrysoula Zerva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blain_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Blain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopatina_N/0/1/0/all/0/1\">Nina Lopatina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enabling Zero-shot Multilingual Spoken Language Translation with Language-Specific Encoders and Decoders. (arXiv:2011.01097v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.01097","description":"<p>Current end-to-end approaches to Spoken Language Translation (SLT) rely on\nlimited training resources, especially for multilingual settings. On the other\nhand, Multilingual Neural Machine Translation (MultiNMT) approaches rely on\nhigher-quality and more massive data sets. Our proposed method extends a\nMultiNMT architecture based on language-specific encoders-decoders to the task\nof Multilingual SLT (MultiSLT). Our method entirely eliminates the dependency\nfrom MultiSLT data and it is able to translate while training only on ASR and\nMultiNMT data.\n</p>\n<p>Our experiments on four different languages show that coupling the speech\nencoder to the MultiNMT architecture produces similar quality translations\ncompared to a bilingual baseline ($\\pm 0.2$ BLEU) while effectively allowing\nfor zero-shot MultiSLT. Additionally, we propose using an Adapter module for\ncoupling the speech inputs. This Adapter module produces consistent\nimprovements up to +6 BLEU points on the proposed architecture and +1 BLEU\npoint on the end-to-end baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Escolano_C/0/1/0/all/0/1\">Carlos Escolano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonollosa_J/0/1/0/all/0/1\">Jos&#xe9; A. R. Fonollosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segura_C/0/1/0/all/0/1\">Carlos Segura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I Wish I Would Have Loved This One, But I Didn't -- A Multilingual Dataset for Counterfactual Detection in Product Reviews. (arXiv:2104.06893v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06893","description":"<p>Counterfactual statements describe events that did not or cannot take place.\nWe consider the problem of counterfactual detection (CFD) in product reviews.\nFor this purpose, we annotate a multilingual CFD dataset from Amazon product\nreviews covering counterfactual statements written in English, German, and\nJapanese languages. The dataset is unique as it contains counterfactuals in\nmultiple languages, covers a new application area of e-commerce reviews, and\nprovides high quality professional annotations. We train CFD models using\ndifferent text representation methods and classifiers. We find that these\nmodels are robust against the selectional biases introduced due to cue\nphrase-based sentence selection. Moreover, our CFD dataset is compatible with\nprior datasets and can be merged to learn accurate CFD models. Applying machine\ntranslation on English counterfactual examples to create multilingual data\nperforms poorly, demonstrating the language-specificity of this problem, which\nhas been ignored so far.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+ONeill_J/0/1/0/all/0/1\">James O&#x27;Neill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozenshtein_P/0/1/0/all/0/1\">Polina Rozenshtein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiryo_R/0/1/0/all/0/1\">Ryuichi Kiryo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubota_M/0/1/0/all/0/1\">Motoko Kubota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TWEAC: Transformer with Extendable QA Agent Classifiers. (arXiv:2104.07081v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07081","description":"<p>Question answering systems should help users to access knowledge on a broad\nrange of topics and to answer a wide array of different questions. Most systems\nfall short of this expectation as they are only specialized in one particular\nsetting, e.g., answering factual questions with Wikipedia data. To overcome\nthis limitation, we propose composing multiple QA agents within a meta-QA\nsystem. We argue that there exist a wide range of specialized QA agents in\nliterature. Thus, we address the central research question of how to\neffectively and efficiently identify suitable QA agents for any given question.\nWe study both supervised and unsupervised approaches to address this challenge,\nshowing that TWEAC -- Transformer with Extendable Agent Classifiers -- achieves\nthe best performance overall with 94% accuracy. We provide extensive insights\non the scalability of TWEAC, demonstrating that it scales robustly to over 100\nQA agents with each providing just 1000 examples of questions they can answer.\nOur code and data is available:\nhttps://github.com/UKPLab/TWEAC-qa-agent-selection\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geigle_G/0/1/0/all/0/1\">Gregor Geigle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruckle_A/0/1/0/all/0/1\">Andreas R&#xfc;ckl&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation. (arXiv:2104.08678v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08678","description":"<p>Despite recent progress, state-of-the-art question answering models remain\nvulnerable to a variety of adversarial attacks. While dynamic adversarial data\ncollection, in which a human annotator tries to write examples that fool a\nmodel-in-the-loop, can improve model robustness, this process is expensive\nwhich limits the scale of the collected data. In this work, we are the first to\nuse synthetic adversarial data generation to make question answering models\nmore robust to human adversaries. We develop a data generation pipeline that\nselects source passages, identifies candidate answers, generates questions,\nthen finally filters or re-labels them to improve quality. Using this approach,\nwe amplify a smaller human-written adversarial dataset to a much larger set of\nsynthetic question-answer pairs. By incorporating our synthetic data, we\nimprove the state-of-the-art on the AdversarialQA dataset by 3.7F1 and improve\nmodel generalisation on nine of the twelve MRQA datasets. We further conduct a\nnovel human-in-the-loop evaluation to show that our models are considerably\nmore robust to new human-written adversarial examples: crowdworkers can fool\nour model only 8.8% of the time on average, compared to 17.6% for a model\ntrained without synthetic data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartolo_M/0/1/0/all/0/1\">Max Bartolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1\">Tristan Thrush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discrete representations in neural models of spoken language. (arXiv:2105.05582v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.05582","description":"<p>The distributed and continuous representations used by neural networks are at\nodds with representations employed in linguistics, which are typically\nsymbolic. Vector quantization has been proposed as a way to induce discrete\nneural representations that are closer in nature to their linguistic\ncounterparts. However, it is not clear which metrics are the best-suited to\nanalyze such discrete representations. We compare the merits of four commonly\nused metrics in the context of weakly supervised models of spoken language. We\ncompare the results they show when applied to two different models, while\nsystematically studying the effect of the placement and size of the\ndiscretization layer. We find that different evaluation regimes can give\ninconsistent results. While we can attribute them to the properties of the\ndifferent metrics in most cases, one point of concern remains: the use of\nminimal pairs of phoneme triples as stimuli disadvantages larger discrete unit\ninventories, unlike metrics applied to complete utterances. Furthermore, while\nin general vector quantization induces representations that correlate with\nunits posited in linguistics, the strength of this correlation is only\nmoderate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Higy_B/0/1/0/all/0/1\">Bertrand Higy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelderloos_L/0/1/0/all/0/1\">Lieke Gelderloos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alishahi_A/0/1/0/all/0/1\">Afra Alishahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrupala_G/0/1/0/all/0/1\">Grzegorz Chrupa&#x142;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Divided We Rule: Influencer Polarization on Twitter During Political Crises in India. (arXiv:2105.08361v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2105.08361","description":"<p>Influencers are key to the nature and networks of information propagation on\nsocial media. Influencers are particularly important in political discourse\nthrough their engagement with issues, and may derive their legitimacy either\nsolely or in large part through online operation, or have an offline sphere of\nexpertise such as entertainers, journalists etc. To quantify influencers'\npolitical engagement and polarity, we use Google's Universal Sentence Encoder\n(USE) to encode the tweets of 6k influencers and 26k Indian politicians during\npolitical crises in India. We then obtain aggregate vector representations of\nthe influencers based on their tweet embeddings, which alongside retweet graphs\nhelp compute their stance and polarity with respect to these political issues.\nWe find that influencers engage with the topics in a partisan manner, with\npolarized influencers being rewarded with increased retweeting and following.\nMoreover, we observe that specific groups of influencers are consistently\npolarized across all events. We conclude by discussing how our study provides\ninsights into the political schisms of present-day India, but also offers a\nmeans to study the role of influencers in exacerbating political polarization\nin other contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1\">Saloni Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_D/0/1/0/all/0/1\">Dibyendu Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekhawat_G/0/1/0/all/0/1\">Gazal Shekhawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_J/0/1/0/all/0/1\">Joyojeet Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Directed Acyclic Graph Network for Conversational Emotion Recognition. (arXiv:2105.12907v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.12907","description":"<p>The modeling of conversational context plays a vital role in emotion\nrecognition from conversation (ERC). In this paper, we put forward a novel idea\nof encoding the utterances with a directed acyclic graph (DAG) to better model\nthe intrinsic structure within a conversation, and design a directed acyclic\nneural network, namely DAG-ERC, to implement this idea. In an attempt to\ncombine the strengths of conventional graph-based neural models and\nrecurrence-based neural models, DAG-ERC provides a more intuitive way to model\nthe information flow between long-distance conversation background and nearby\ncontext. Extensive experiments are conducted on four ERC benchmarks with\nstate-of-the-art models employed as baselines for comparison. The empirical\nresults demonstrate the superiority of this new model and confirm the\nmotivation of the directed acyclic graph architecture for ERC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Weizhou Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Siyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information. (arXiv:2106.05707v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.05707","description":"<p>Fact verification has attracted a lot of attention in the machine learning\nand natural language processing communities, as it is one of the key methods\nfor detecting misinformation. Existing large-scale benchmarks for this task\nhave focused mostly on textual sources, i.e. unstructured information, and thus\nignored the wealth of information available in structured formats, such as\ntables. In this paper we introduce a novel dataset and benchmark, Fact\nExtraction and VERification Over Unstructured and Structured information\n(FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated\nwith evidence in the form of sentences and/or cells from tables in Wikipedia,\nas well as a label indicating whether this evidence supports, refutes, or does\nnot provide enough information to reach a verdict. Furthermore, we detail our\nefforts to track and minimize the biases present in the dataset and could be\nexploited by models, e.g. being able to predict the label without using\nevidence. Finally, we develop a baseline for verifying claims against text and\ntables which predicts both the correct evidence and verdict for 18% of the\nclaims.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aly_R/0/1/0/all/0/1\">Rami Aly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhijiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1\">Michael Schlichtkrull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1\">James Thorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christodoulopoulos_C/0/1/0/all/0/1\">Christos Christodoulopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cocarascu_O/0/1/0/all/0/1\">Oana Cocarascu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Arpit Mittal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-utterance Reranking Models with BERT and Graph Convolutional Networks for Conversational Speech Recognition. (arXiv:2106.06922v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.06922","description":"<p>How to effectively incorporate cross-utterance information cues into a neural\nlanguage model (LM) has emerged as one of the intriguing issues for automatic\nspeech recognition (ASR). Existing research efforts on improving\ncontextualization of an LM typically regard previous utterances as a sequence\nof additional input and may fail to capture complex global structural\ndependencies among these utterances. In view of this, we in this paper seek to\nrepresent the historical context information of an utterance as\ngraph-structured data so as to distill cross-utterances, global word\ninteraction relationships. To this end, we apply a graph convolutional network\n(GCN) on the resulting graph to obtain the corresponding GCN embeddings of\nhistorical words. GCN has recently found its versatile applications on\nsocial-network analysis, text summarization, and among others due mainly to its\nability of effectively capturing rich relational information among elements.\nHowever, GCN remains largely underexplored in the context of ASR, especially\nfor dealing with conversational speech. In addition, we frame ASR N-best\nreranking as a prediction problem, leveraging bidirectional encoder\nrepresentations from transformers (BERT) as the vehicle to not only seize the\nlocal intrinsic word regularity patterns inherent in a candidate hypothesis but\nalso incorporate the cross-utterance, historical word interaction cues\ndistilled by GCN for promoting performance. Extensive experiments conducted on\nthe AMI benchmark dataset seem to confirm the pragmatic utility of our methods,\nin relation to some current top-of-the-line methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_S/0/1/0/all/0/1\">Shih-Hsuan Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_T/0/1/0/all/0/1\">Tien-Hong Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fu-An Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Berlin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education. (arXiv:2106.07340v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07340","description":"<p>Since the introduction of the original BERT (i.e., BASE BERT), researchers\nhave developed various customized BERT models with improved performance for\nspecific domains and tasks by exploiting the benefits of transfer learning. Due\nto the nature of mathematical texts, which often use domain specific vocabulary\nalong with equations and math symbols, we posit that the development of a new\nBERT model for mathematics would be useful for many mathematical downstream\ntasks. In this resource paper, we introduce our multi-institutional effort\n(i.e., two learning platforms and three academic institutions in the US) toward\nthis need: MathBERT, a model created by pre-training the BASE BERT model on a\nlarge mathematical corpus ranging from pre-kindergarten (pre-k), to\nhigh-school, to college graduate level mathematical content. In addition, we\nselect three general NLP tasks that are often used in mathematics education:\nprediction of knowledge component, auto-grading open-ended Q&amp;A, and knowledge\ntracing, to demonstrate the superiority of MathBERT over BASE BERT. Our\nexperiments show that MathBERT outperforms prior best methods by 1.2-22% and\nBASE BERT by 2-8% on these tasks. In addition, we build a mathematics specific\nvocabulary 'mathVocab' to train with MathBERT. We discover that MathBERT\npre-trained with 'mathVocab' outperforms MathBERT trained with the BASE BERT\nvocabulary (i.e., 'origVocab'). MathBERT is currently being adopted at the\nparticipated leaning platforms: Stride, Inc, a commercial educational resource\nprovider, and ASSISTments.org, a free online educational platform. We release\nMathBERT for public usage at: https://github.com/tbs17/MathBERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jia Tracy Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_M/0/1/0/all/0/1\">Michiharu Yamashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prihar_E/0/1/0/all/0/1\">Ethan Prihar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_N/0/1/0/all/0/1\">Neil Heffernan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xintao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graff_B/0/1/0/all/0/1\">Ben Graff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN. (arXiv:2107.01366v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.01366","description":"<p>Despite their practical success, modern seq2seq architectures are unable to\ngeneralize systematically on several SCAN tasks. Hence, it is not clear if\nSCAN-style compositional generalization is useful in realistic NLP tasks. In\nthis work, we study the benefit that such compositionality brings about to\nseveral machine translation tasks. We present several focused modifications of\nTransformer that greatly improve generalization capabilities on SCAN and select\none that remains on par with a vanilla Transformer on a standard machine\ntranslation (MT) task. Next, we study its performance in low-resource settings\nand on a newly introduced distribution-shifted English-French translation task.\nOverall, we find that improvements of a SCAN-capable model do not directly\ntransfer to the resource-rich MT setup. In contrast, in the low-resource setup,\ngeneral modifications lead to an improvement of up to 13.1% BLEU score w.r.t. a\nvanilla Transformer. Similarly, an improvement of 14% in an accuracy-based\nmetric is achieved in the introduced compositional English-French translation\ntask. This provides experimental evidence that the compositional generalization\nassessed in SCAN is particularly useful in resource-starved and domain-shifted\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaabouni_R/0/1/0/all/0/1\">Rahma Chaabouni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dessi_R/0/1/0/all/0/1\">Roberto Dess&#xec;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Contrastive Learning with Adversarial Perturbations for Robust Pretrained Language Models. (arXiv:2107.07610v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.07610","description":"<p>This paper improves the robustness of the pretrained language model, BERT,\nagainst word substitution-based adversarial attacks by leveraging\nself-supervised contrastive learning with adversarial perturbations. One\nadvantage of our method compared to previous works is that it is capable of\nimproving model robustness without using any labels. Additionally, we also\ncreate an adversarial attack for word-level adversarial training on BERT. The\nattack is efficient, allowing adversarial training for BERT on adversarial\nexamples generated \\textit{on the fly} during training. Experimental results\nshow that our method improves the robustness of BERT against four different\nword substitution-based adversarial attacks. Additionally, combining our method\nwith adversarial training gives higher robustness than adversarial training\nalone. Furthermore, to understand why our method can improve the model\nrobustness against adversarial attacks, we study vector representations of\nclean examples and their corresponding adversarial examples before and after\napplying our method. As our method improves model robustness with unlabeled raw\ndata, it opens up the possibility of using large text datasets to train robust\nlanguage models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yihan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1\">Roger Wattenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis. (arXiv:2109.00412v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00412","description":"<p>In multimodal sentiment analysis (MSA), the performance of a model highly\ndepends on the quality of synthesized embeddings. These embeddings are\ngenerated from the upstream process called multimodal fusion, which aims to\nextract and combine the input unimodal raw data to produce a richer multimodal\nrepresentation. Previous work either back-propagates the task loss or\nmanipulates the geometric property of feature spaces to produce favorable\nfusion results, which neglects the preservation of critical task-related\ninformation that flows from input to the fusion results. In this work, we\npropose a framework named MultiModal InfoMax (MMIM), which hierarchically\nmaximizes the Mutual Information (MI) in unimodal input pairs (inter-modality)\nand between multimodal fusion result and unimodal input in order to maintain\ntask-related information through multimodal fusion. The framework is jointly\ntrained with the main task (MSA) to improve the performance of the downstream\nMSA task. To address the intractable issue of MI bounds, we further formulate a\nset of computationally simple parametric and non-parametric methods to\napproximate their truth value. Experimental results on the two widely used\ndatasets demonstrate the efficacy of our approach. The implementation of this\nwork is publicly available at\nhttps://github.com/declare-lab/Multimodal-Infomax.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Establishing Interlingua in Multilingual Language Models. (arXiv:2109.01207v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01207","description":"<p>Large multilingual language models show remarkable zero-shot cross-lingual\ntransfer performance on a range of tasks. Follow-up works hypothesized that\nthese models internally project representations of different languages into a\nshared interlingual space. However, they produced contradictory results. In\nthis paper, we correct the famous prior work claiming that \"BERT is not an\nInterlingua\" and show that with the proper choice of sentence representation\ndifferent languages actually do converge to a shared space in such language\nmodels. Furthermore, we demonstrate that this convergence pattern is robust\nacross four measures of correlation similarity and six mBERT-like models. We\nthen extend our analysis to 28 diverse languages and find that the interlingual\nspace exhibits a particular structure similar to the linguistic relatedness of\nlanguages. We also highlight a few outlier languages that seem to fail to\nconverge to the shared space. The code for replicating our results is available\nat the following URL: https://github.com/maksym-del/interlingua.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Del_M/0/1/0/all/0/1\">Maksym Del</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fishel_M/0/1/0/all/0/1\">Mark Fishel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self- and Pseudo-self-supervised Prediction of Speaker and Key-utterance for Multi-party Dialogue Reading Comprehension. (arXiv:2109.03772v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03772","description":"<p>Multi-party dialogue machine reading comprehension (MRC) brings tremendous\nchallenge since it involves multiple speakers at one dialogue, resulting in\nintricate speaker information flows and noisy dialogue contexts. To alleviate\nsuch difficulties, previous models focus on how to incorporate these\ninformation using complex graph-based modules and additional manually labeled\ndata, which is usually rare in real scenarios. In this paper, we design two\nlabour-free self- and pseudo-self-supervised prediction tasks on speaker and\nkey-utterance to implicitly model the speaker information flows, and capture\nsalient clues in a long dialogue. Experimental results on two benchmark\ndatasets have justified the effectiveness of our method over competitive\nbaselines and current state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Recipe For Arbitrary Text Style Transfer with Large Language Models. (arXiv:2109.03910v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03910","description":"<p>In this paper, we leverage large language models (LMs) to perform zero-shot\ntext style transfer. We present a prompting method that we call augmented\nzero-shot learning, which frames style transfer as a sentence rewriting task\nand requires only a natural language instruction, without model fine-tuning or\nexemplars in the target style. Augmented zero-shot learning is simple and\ndemonstrates promising results not just on standard style transfer tasks such\nas sentiment, but also on arbitrary transformations such as \"make this\nmelodramatic\" or \"insert a metaphor.\"\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1\">Emily Reif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_A/0/1/0/all/0/1\">Ann Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coenen_A/0/1/0/all/0/1\">Andy Coenen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation Schemes for Building ASR in Low-resource Languages. (arXiv:2109.05494v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05494","description":"<p>Building an automatic speech recognition (ASR) system from scratch requires a\nlarge amount of annotated speech data, which is difficult to collect in many\nlanguages. However, there are cases where the low-resource language shares a\ncommon acoustic space with a high-resource language having enough annotated\ndata to build an ASR. In such cases, we show that the domain-independent\nacoustic models learned from the high-resource language through unsupervised\ndomain adaptation (UDA) schemes can enhance the performance of the ASR in the\nlow-resource language. We use the specific example of Hindi in the source\ndomain and Sanskrit in the target domain. We explore two architectures: i)\ndomain adversarial training using gradient reversal layer (GRL) and ii) domain\nseparation networks (DSN). The GRL and DSN architectures give absolute\nimprovements of 6.71% and 7.32%, respectively, in word error rate over the\nbaseline deep neural network model when trained on just 5.5 hours of data in\nthe target domain. We also show that choosing a proper language (Telugu) in the\nsource domain can bring further improvement. The results suggest that UDA\nschemes can be helpful in the development of ASR systems for low-resource\nlanguages, mitigating the hassle of collecting large amounts of annotated\nspeech data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+S_A/0/1/0/all/0/1\">Anoop C S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+P_P/0/1/0/all/0/1\">Prathosh A P</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1\">A G Ramakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Levenshtein Training for Word-level Quality Estimation. (arXiv:2109.05611v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05611","description":"<p>We propose a novel scheme to use the Levenshtein Transformer to perform the\ntask of word-level quality estimation. A Levenshtein Transformer is a natural\nfit for this task: trained to perform decoding in an iterative manner, a\nLevenshtein Transformer can learn to post-edit without explicit supervision. To\nfurther minimize the mismatch between the translation task and the word-level\nQE task, we propose a two-stage transfer learning procedure on both augmented\ndata and human post-editing data. We also propose heuristics to construct\nreference labels that are compatible with subword-level finetuning and\ninference. Results on WMT 2020 QE shared task dataset show that our proposed\nmethod has superior data efficiency under the data-constrained setting and\ncompetitive performance under the unconstrained setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shuoyang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junczys_Dowmunt_M/0/1/0/all/0/1\">Marcin Junczys-Dowmunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1\">Matt Post</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Talking Space: inference from spatial linguistic meanings. (arXiv:2109.06554v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06554","description":"<p>This paper concerns the intersection of natural language and the physical\nspace around us in which we live, that we observe and/or imagine things within.\nMany important features of language have spatial connotations, for example,\nmany prepositions (like in, next to, after, on, etc.) are fundamentally\nspatial. Space is also a key factor of the meanings of many\nwords/phrases/sentences/text, and space is a, if not the key, context for\nreferencing (e.g. pointing) and embodiment.\n</p>\n<p>We propose a mechanism for how space and linguistic structure can be made to\ninteract in a matching compositional fashion. Examples include Cartesian space,\nsubway stations, chesspieces on a chess-board, and Penrose's staircase. The\nstarting point for our construction is the DisCoCat model of compositional\nnatural language meaning, which we relax to accommodate physical space. We\naddress the issue of having multiple agents/objects in a space, including the\ncase that each agent has different capabilities with respect to that space,\ne.g., the specific moves each chesspiece can make, or the different velocities\none may be able to reach.\n</p>\n<p>Once our model is in place, we show how inferences drawing from the structure\nof physical space can be made. We also how how linguistic model of space can\ninteract with other such models related to our senses and/or embodiment, such\nas the conceptual spaces of colour, taste and smell, resulting in a rich\ncompositional model of meaning that is close to human experience and embodiment\nin the world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Mascianica_V/0/1/0/all/0/1\">Vincent Wang-Mascianica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coecke_B/0/1/0/all/0/1\">Bob Coecke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientBERT: Progressively Searching Multilayer Perceptron via Warm-up Knowledge Distillation. (arXiv:2109.07222v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07222","description":"<p>Pre-trained language models have shown remarkable results on various NLP\ntasks. Nevertheless, due to their bulky size and slow inference speed, it is\nhard to deploy them on edge devices. In this paper, we have a critical insight\nthat improving the feed-forward network (FFN) in BERT has a higher gain than\nimproving the multi-head attention (MHA) since the computational cost of FFN is\n2$\\sim$3 times larger than MHA. Hence, to compact BERT, we are devoted to\ndesigning efficient FFN as opposed to previous works that pay attention to MHA.\nSince FFN comprises a multilayer perceptron (MLP) that is essential in BERT\noptimization, we further design a thorough search space towards an advanced MLP\nand perform a coarse-to-fine mechanism to search for an efficient BERT\narchitecture. Moreover, to accelerate searching and enhance model\ntransferability, we employ a novel warm-up knowledge distillation strategy at\neach search stage. Extensive experiments show our searched EfficientBERT is\n6.9$\\times$ smaller and 4.4$\\times$ faster than BERT$\\rm_{BASE}$, and has\ncompetitive performances on GLUE and SQuAD Benchmarks. Concretely,\nEfficientBERT attains a 77.7 average score on GLUE \\emph{test}, 0.7 higher than\nMobileBERT$\\rm_{TINY}$, and achieves an 85.3/74.5 F1 score on SQuAD v1.1/v2.0\n\\emph{dev}, 3.2/2.7 higher than TinyBERT$_4$ even without data augmentation.\nThe code is released at https://github.com/cheneydon/efficient-bert.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chenhe Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangrun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jiefeng Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HEIDL: Learning Linguistic Expressions with Deep Learning and Human-in-the-Loop. (arXiv:1907.11184v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/1907.11184","description":"<p>While the role of humans is increasingly recognized in machine learning\ncommunity, representation of and interaction with models in current\nhuman-in-the-loop machine learning (HITL-ML) approaches are too low-level and\nfar-removed from human's conceptual models. We demonstrate HEIDL, a prototype\nHITL-ML system that exposes the machine-learned model through high-level,\nexplainable linguistic expressions formed of predicates representing semantic\nstructure of text. In HEIDL, human's role is elevated from simply evaluating\nmodel predictions to interpreting and even updating the model logic directly by\nenabling interaction with rule predicates themselves. Raising the currency of\ninteraction to such semantic levels calls for new interaction paradigms between\nhumans and machines that result in improved productivity for text analytics\nmodel development process. Moreover, by involving humans in the process, the\nhuman-machine co-created models generalize better to unseen data as domain\nexperts are able to instill their expertise by extrapolating from what has been\nlearned by automated algorithms from few labelled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandogan_E/0/1/0/all/0/1\">Eser Kandogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasecki_W/0/1/0/all/0/1\">Walter S. Lasecki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_P/0/1/0/all/0/1\">Prithviraj Sen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-16T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Federated Contrastive Learning for Decentralized Unlabeled Medical Images. (arXiv:2109.07504v1 [cs.LG])","link":"http://arxiv.org/abs/2109.07504","description":"<p>A label-efficient paradigm in computer vision is based on self-supervised\ncontrastive pre-training on unlabeled data followed by fine-tuning with a small\nnumber of labels. Making practical use of a federated computing environment in\nthe clinical domain and learning on medical images poses specific challenges.\nIn this work, we propose FedMoCo, a robust federated contrastive learning (FCL)\nframework, which makes efficient use of decentralized unlabeled medical data.\nFedMoCo has two novel modules: metadata transfer, an inter-node statistical\ndata augmentation module, and self-adaptive aggregation, an aggregation module\nbased on representational similarity analysis. To the best of our knowledge,\nthis is the first FCL work on medical images. Our experiments show that FedMoCo\ncan consistently outperform FedAvg, a seminal federated learning framework, in\nextracting meaningful representations for downstream tasks. We further show\nthat FedMoCo can substantially reduce the amount of labeled data required in a\ndownstream task, such as COVID-19 detection, to achieve a reasonable\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Nanqing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voiculescu_I/0/1/0/all/0/1\">Irina Voiculescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Aggregate and Refine Noisy Labels for Visual Sentiment Analysis. (arXiv:2109.07509v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07509","description":"<p>Visual sentiment analysis has received increasing attention in recent years.\nHowever, the quality of the dataset is a concern because the sentiment labels\nare crowd-sourcing, subjective, and prone to mistakes. This poses a severe\nthreat to the data-driven models including the deep neural networks which would\ngeneralize poorly on the testing cases if they are trained to over-fit the\nsamples with noisy sentiment labels. Inspired by the recent progress on\nlearning with noisy labels, we propose a robust learning method to perform\nrobust visual sentiment analysis. Our method relies on an external memory to\naggregate and filter noisy labels during training and thus can prevent the\nmodel from overfitting the noisy cases. The memory is composed of the\nprototypes with corresponding labels, both of which can be updated online. We\nestablish a benchmark for visual sentiment analysis with label noise using\npublicly available datasets. The experiment results of the proposed benchmark\nsettings comprehensively show the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zihe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitian Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1\">Hanjia Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pose Transformers (POTR): Human Motion Prediction with Non-Autoregressive Transformers. (arXiv:2109.07531v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07531","description":"<p>We propose to leverage Transformer architectures for non-autoregressive human\nmotion prediction. Our approach decodes elements in parallel from a query\nsequence, instead of conditioning on previous predictions such as\ninstate-of-the-art RNN-based approaches. In such a way our approach is less\ncomputational intensive and potentially avoids error accumulation to long term\nelements in the sequence. In that context, our contributions are fourfold: (i)\nwe frame human motion prediction as a sequence-to-sequence problem and propose\na non-autoregressive Transformer to infer the sequences of poses in parallel;\n(ii) we propose to decode sequences of 3D poses from a query sequence generated\nin advance with elements from the input sequence;(iii) we propose to perform\nskeleton-based activity classification from the encoder memory, in the hope\nthat identifying the activity can improve predictions;(iv) we show that despite\nits simplicity, our approach achieves competitive results in two public\ndatasets, although surprisingly more for short term predictions rather than for\nlong term ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Gonzalez_A/0/1/0/all/0/1\">Angel Mart&#xed;nez-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villamizar_M/0/1/0/all/0/1\">Michael Villamizar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odobez_J/0/1/0/all/0/1\">Jean-Marc Odobez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAFT-Stereo: Multilevel Recurrent Field Transforms for Stereo Matching. (arXiv:2109.07547v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07547","description":"<p>We introduce RAFT-Stereo, a new deep architecture for rectified stereo based\non the optical flow network RAFT. We introduce multi-level convolutional GRUs,\nwhich more efficiently propagate information across the image. A modified\nversion of RAFT-Stereo can perform accurate real-time inference. RAFT-stereo\nranks first on the Middlebury leaderboard, outperforming the next best method\non 1px error by 29% and outperforms all published work on the ETH3D two-view\nstereo benchmark. Code is available at\nhttps://github.com/princeton-vl/RAFT-Stereo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lipson_L/0/1/0/all/0/1\">Lahav Lipson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teed_Z/0/1/0/all/0/1\">Zachary Teed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jia Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning the Regularization in DCE-MR Image Reconstruction for Functional Imaging of Kidneys. (arXiv:2109.07548v1 [cs.LG])","link":"http://arxiv.org/abs/2109.07548","description":"<p>Kidney DCE-MRI aims at both qualitative assessment of kidney anatomy and\nquantitative assessment of kidney function by estimating the tracer kinetic\n(TK) model parameters. Accurate estimation of TK model parameters requires an\naccurate measurement of the arterial input function (AIF) with high temporal\nresolution. Accelerated imaging is used to achieve high temporal resolution,\nwhich yields under-sampling artifacts in the reconstructed images. Compressed\nsensing (CS) methods offer a variety of reconstruction options. Most commonly,\nsparsity of temporal differences is encouraged for regularization to reduce\nartifacts. Increasing regularization in CS methods removes the ambient\nartifacts but also over-smooths the signal temporally which reduces the\nparameter estimation accuracy. In this work, we propose a single image trained\ndeep neural network to reduce MRI under-sampling artifacts without reducing the\naccuracy of functional imaging markers. Instead of regularizing with a penalty\nterm in optimization, we promote regularization by generating images from a\nlower dimensional representation. In this manuscript we motivate and explain\nthe lower dimensional input design. We compare our approach to CS\nreconstructions with multiple regularization weights. Proposed approach results\nin kidney biomarkers that are highly correlated with the ground truth markers\nestimated using the CS reconstruction which was optimized for functional\nanalysis. At the same time, the proposed approach reduces the artifacts in the\nreconstructed images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocanaogullari_A/0/1/0/all/0/1\">Aziz Ko&#xe7;anao&#x11f;ullar&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ariyurek_C/0/1/0/all/0/1\">Cemre Ariyurek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afacan_O/0/1/0/all/0/1\">Onur Afacan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurugol_S/0/1/0/all/0/1\">Sila Kurugol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Pathology Deep Learning System Capable of Triage of Melanoma Specimens Utilizing Dermatopathologist Consensus as Ground Truth. (arXiv:2109.07554v1 [eess.IV])","link":"http://arxiv.org/abs/2109.07554","description":"<p>Although melanoma occurs more rarely than several other skin cancers,\npatients' long term survival rate is extremely low if the diagnosis is missed.\nDiagnosis is complicated by a high discordance rate among pathologists when\ndistinguishing between melanoma and benign melanocytic lesions. A tool that\nallows pathology labs to sort and prioritize melanoma cases in their workflow\ncould improve turnaround time by prioritizing challenging cases and routing\nthem directly to the appropriate subspecialist. We present a pathology deep\nlearning system (PDLS) that performs hierarchical classification of digitized\nwhole slide image (WSI) specimens into six classes defined by their\nmorphological characteristics, including classification of \"Melanocytic\nSuspect\" specimens likely representing melanoma or severe dysplastic nevi. We\ntrained the system on 7,685 images from a single lab (the reference lab),\nincluding the the largest set of triple-concordant melanocytic specimens\ncompiled to date, and tested the system on 5,099 images from two distinct\nvalidation labs. We achieved Area Underneath the ROC Curve (AUC) values of 0.93\nclassifying Melanocytic Suspect specimens on the reference lab, 0.95 on the\nfirst validation lab, and 0.82 on the second validation lab. We demonstrate\nthat the PDLS is capable of automatically sorting and triaging skin specimens\nwith high sensitivity to Melanocytic Suspect cases and that a pathologist would\nonly need between 30% and 60% of the caseload to address all melanoma\nspecimens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sankarapandian_S/0/1/0/all/0/1\">Sivaramakrishnan Sankarapandian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kohn_S/0/1/0/all/0/1\">Saul Kohn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Spurrier_V/0/1/0/all/0/1\">Vaughn Spurrier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grullon_S/0/1/0/all/0/1\">Sean Grullon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soans_R/0/1/0/all/0/1\">Rajath E. Soans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ayyagari_K/0/1/0/all/0/1\">Kameswari D. Ayyagari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chamarthi_R/0/1/0/all/0/1\">Ramachandra V. Chamarthi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Motaparthi_K/0/1/0/all/0/1\">Kiran Motaparthi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Jason B. Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shon_W/0/1/0/all/0/1\">Wonwoo Shon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bonham_M/0/1/0/all/0/1\">Michael Bonham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ianni_J/0/1/0/all/0/1\">Julianna D. Ianni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid ICP. (arXiv:2109.07559v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07559","description":"<p>ICP algorithms typically involve a fixed choice of data association method\nand a fixed choice of error metric. In this paper, we propose Hybrid ICP, a\nnovel and flexible ICP variant which dynamically optimises both the data\nassociation method and error metric based on the live image of an object and\nthe current ICP estimate. We show that when used for object pose estimation,\nHybrid ICP is more accurate and more robust to noise than other commonly used\nICP variants. We also consider the setting where ICP is applied sequentially\nwith a moving camera, and we study the trade-off between the accuracy of each\nICP estimate and the number of ICP estimates available within a fixed amount of\ntime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dreczkowski_K/0/1/0/all/0/1\">Kamil Dreczkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1\">Edward Johns</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Framework for Multisensory Foresight for Embodied Agents. (arXiv:2109.07561v1 [cs.RO])","link":"http://arxiv.org/abs/2109.07561","description":"<p>Predicting future sensory states is crucial for learning agents such as\nrobots, drones, and autonomous vehicles. In this paper, we couple multiple\nsensory modalities with exploratory actions and propose a predictive neural\nnetwork architecture to address this problem. Most existing approaches rely on\nlarge, manually annotated datasets, or only use visual data as a single\nmodality. In contrast, the unsupervised method presented here uses multi-modal\nperceptions for predicting future visual frames. As a result, the proposed\nmodel is more comprehensive and can better capture the spatio-temporal dynamics\nof the environment, leading to more accurate visual frame prediction. The other\nnovelty of our framework is the use of sub-networks dedicated to anticipating\nfuture haptic, audio, and tactile signals. The framework was tested and\nvalidated with a dataset containing 4 sensory modalities (vision, haptic,\naudio, and tactile) on a humanoid robot performing 9 behaviors multiple times\non a large set of objects. While the visual information is the dominant\nmodality, utilizing the additional non-visual modalities improves the accuracy\nof predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_R/0/1/0/all/0/1\">Ramtin Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panetta_K/0/1/0/all/0/1\">Karen Panetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinapov_J/0/1/0/all/0/1\">Jivko Sinapov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting 3D shapes, masks, and properties of materials, liquids, and objects inside transparent containers, using the TransProteus CGI dataset. (arXiv:2109.07577v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07577","description":"<p>We present TransProteus, a dataset, and methods for predicting the 3D\nstructure, masks, and properties of materials, liquids, and objects inside\ntransparent vessels from a single image without prior knowledge of the image\nsource and camera parameters. Manipulating materials in transparent containers\nis essential in many fields and depends heavily on vision. This work supplies a\nnew procedurally generated dataset consisting of 50k images of liquids and\nsolid objects inside transparent containers. The image annotations include 3D\nmodels, material properties (color/transparency/roughness...), and segmentation\nmasks for the vessel and its content. The synthetic (CGI) part of the dataset\nwas procedurally generated using 13k different objects, 500 different\nenvironments (HDRI), and 1450 material textures (PBR) combined with simulated\nliquids and procedurally generated vessels. In addition, we supply 104\nreal-world images of objects inside transparent vessels with depth maps of both\nthe vessel and its content. We propose a camera agnostic method that predicts\n3D models from an image as an XYZ map. This allows the trained net to predict\nthe 3D model as a map with XYZ coordinates per pixel without prior knowledge of\nthe image source. To calculate the training loss, we use the distance between\npairs of points inside the 3D model instead of the absolute XYZ coordinates.\nThis makes the loss function translation invariant. We use this to predict 3D\nmodels of vessels and their content from a single image. Finally, we\ndemonstrate a net that uses a single image to predict the material properties\nof the vessel content and surface.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eppel_S/0/1/0/all/0/1\">Sagi Eppel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoping Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Ru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aspuru_Guzik_A/0/1/0/all/0/1\">Alan Aspuru-Guzik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UCP-Net: Unstructured Contour Points for Instance Segmentation. (arXiv:2109.07592v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07592","description":"<p>The goal of interactive segmentation is to assist users in producing\nsegmentation masks as fast and as accurately as possible. Interactions have to\nbe simple and intuitive and the number of interactions required to produce a\nsatisfactory segmentation mask should be as low as possible. In this paper, we\npropose a novel approach to interactive segmentation based on unconstrained\ncontour clicks for initial segmentation and segmentation refinement. Our method\nis class-agnostic and produces accurate segmentation masks (IoU &gt; 85%) for a\nlower number of user interactions than state-of-the-art methods on popular\nsegmentation datasets (COCO MVal, SBD and Berkeley).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dupont_C/0/1/0/all/0/1\">Camille Dupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouakrim_Y/0/1/0/all/0/1\">Yanis Ouakrim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1\">Quoc Cuong Pham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partner-Assisted Learning for Few-Shot Image Classification. (arXiv:2109.07607v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07607","description":"<p>Few-shot Learning has been studied to mimic human visual capabilities and\nlearn effective models without the need of exhaustive human annotation. Even\nthough the idea of meta-learning for adaptation has dominated the few-shot\nlearning methods, how to train a feature extractor is still a challenge. In\nthis paper, we focus on the design of training strategy to obtain an elemental\nrepresentation such that the prototype of each novel class can be estimated\nfrom a few labeled samples. We propose a two-stage training scheme,\nPartner-Assisted Learning (PAL), which first trains a partner encoder to model\npair-wise similarities and extract features serving as soft-anchors, and then\ntrains a main encoder by aligning its outputs with soft-anchors while\nattempting to maximize classification performance. Two alignment constraints\nfrom logit-level and feature-level are designed individually. For each few-shot\ntask, we perform prototype classification. Our method consistently outperforms\nthe state-of-the-art method on four benchmarks. Detailed ablation studies of\nPAL are provided to justify the selection of each component involved in\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiawei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Hanchen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Guangxing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abd_Almageed_W/0/1/0/all/0/1\">Wael Abd-Almageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication. (arXiv:2109.07644v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07644","description":"<p>Employing Vehicle-to-Vehicle communication to enhance perception performance\nin self-driving technology has attracted considerable attention recently;\nhowever, the absence of a suitable open dataset for benchmarking algorithms has\nmade it difficult to develop and assess cooperative perception technologies. To\nthis end, we present the first large-scale open simulated dataset for\nVehicle-to-Vehicle perception. It contains over 70 interesting scenes, 111,464\nframes, and 232,913 annotated 3D vehicle bounding boxes, collected from 8 towns\nin CARLA and a digital town of Culver City, Los Angeles. We then construct a\ncomprehensive benchmark with a total of 16 implemented models to evaluate\nseveral information fusion strategies~(i.e. early, late, and intermediate\nfusion) with state-of-the-art LiDAR detection algorithms. Moreover, we propose\na new Attentive Intermediate Fusion pipeline to aggregate information from\nmultiple connected vehicles. Our experiments show that the proposed pipeline\ncan be easily integrated with existing 3D LiDAR detectors and achieve\noutstanding performance even with large compression rates. To encourage more\nresearchers to investigate Vehicle-to-Vehicle perception, we will release the\ndataset, benchmark methods, and all related codes in\nhttps://mobility-lab.seas.ucla.edu/opv2v/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runsheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_H/0/1/0/all/0/1\">Hao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinlong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiaqi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"METEOR: A Massive Dense & Heterogeneous Behavior Dataset for Autonomous Driving. (arXiv:2109.07648v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07648","description":"<p>We present a new and complex traffic dataset, METEOR, which captures traffic\npatterns in unstructured scenarios in India. METEOR consists of more than 1000\none-minute video clips, over 2 million annotated frames with ego-vehicle\ntrajectories, and more than 13 million bounding boxes for surrounding vehicles\nor traffic agents. METEOR is a unique dataset in terms of capturing the\nheterogeneity of microscopic and macroscopic traffic characteristics.\nFurthermore, we provide annotations for rare and interesting driving behaviors\nsuch as cut-ins, yielding, overtaking, overspeeding, zigzagging, sudden lane\nchanging, running traffic signals, driving in the wrong lanes, taking wrong\nturns, lack of right-of-way rules at intersections, etc. We also present\ndiverse traffic scenarios corresponding to rainy weather, nighttime driving,\ndriving in rural areas with unmarked roads, and high-density traffic scenarios.\nWe use our novel dataset to evaluate the performance of object detection and\nbehavior prediction algorithms. We show that state-of-the-art object detectors\nfail in these challenging conditions and also propose a new benchmark test:\naction-behavior prediction with a baseline mAP score of 70.74.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohan Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_M/0/1/0/all/0/1\">Mridul Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kala_R/0/1/0/all/0/1\">Rahul Kala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palugulla_R/0/1/0/all/0/1\">Rishitha Palugulla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naidu_C/0/1/0/all/0/1\">Chandrababu Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Alok Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Fusion Network for RGBT Tracking. (arXiv:2109.07662v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07662","description":"<p>For both visible and infrared images have their own advantages and\ndisadvantages, RGBT tracking has attracted more and more attention. The key\npoints of RGBT tracking lie in feature extraction and feature fusion of visible\nand infrared images. Current RGBT tracking methods mostly pay attention to both\nindividual features (features extracted from images of a single camera) and\ncommon features (features extracted and fused from an RGB camera and a thermal\ncamera), while pay less attention to the different and dynamic contributions of\nindividual features and common features for different sequences of registered\nimage pairs. This paper proposes a novel RGBT tracking method, called Dynamic\nFusion Network (DFNet), which adopts a two-stream structure, in which two\nnon-shared convolution kernels are employed in each layer to extract individual\nfeatures. Besides, DFNet has shared convolution kernels for each layer to\nextract common features. Non-shared convolution kernels and shared convolution\nkernels are adaptively weighted and summed according to different image pairs,\nso that DFNet can deal with different contributions for different sequences.\nDFNet has a fast speed, which is 28.658 FPS. The experimental results show that\nwhen DFNet only increases the Mult-Adds of 0.02% than the\nnon-shared-convolution-kernel-based fusion method, Precision Rate (PR) and\nSuccess Rate (SR) reach 88.1% and 71.9% respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jingchao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haitao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhengwei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPIN Road Mapper: Extracting Roads from Aerial Images via Spatial and Interaction Space Graph Reasoning for Autonomous Driving. (arXiv:2109.07701v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07701","description":"<p>Road extraction is an essential step in building autonomous navigation\nsystems. Detecting road segments is challenging as they are of varying widths,\nbifurcated throughout the image, and are often occluded by terrain, cloud, or\nother weather conditions. Using just convolution neural networks (ConvNets) for\nthis problem is not effective as it is inefficient at capturing distant\ndependencies between road segments in the image which is essential to extract\nroad connectivity. To this end, we propose a Spatial and Interaction Space\nGraph Reasoning (SPIN) module which when plugged into a ConvNet performs\nreasoning over graphs constructed on spatial and interaction spaces projected\nfrom the feature maps. Reasoning over spatial space extracts dependencies\nbetween different spatial regions and other contextual information. Reasoning\nover a projected interaction space helps in appropriate delineation of roads\nfrom other topographies present in the image. Thus, SPIN extracts long-range\ndependencies between road segments and effectively delineates roads from other\nsemantics. We also introduce a SPIN pyramid which performs SPIN graph reasoning\nacross multiple scales to extract multi-scale features. We propose a network\nbased on stacked hourglass modules and SPIN pyramid for road segmentation which\nachieves better performance compared to existing methods. Moreover, our method\nis computationally efficient and significantly boosts the convergence speed\nduring training, making it feasible for applying on large-scale high-resolution\naerial images. Code available at:\nhttps://github.com/wgcban/SPIN_RoadMapper.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Task Cross-Task Learning Architecture for Ad-hoc Uncertainty Estimation in 3D Cardiac MRI Image Segmentation. (arXiv:2109.07702v1 [eess.IV])","link":"http://arxiv.org/abs/2109.07702","description":"<p>Medical image segmentation has significantly benefitted thanks to deep\nlearning architectures. Furthermore, semi-supervised learning (SSL) has\nrecently been a growing trend for improving a model's overall performance by\nleveraging abundant unlabeled data. Moreover, learning multiple tasks within\nthe same model further improves model generalizability. To generate smoother\nand accurate segmentation masks from 3D cardiac MR images, we present a\nMulti-task Cross-task learning consistency approach to enforce the correlation\nbetween the pixel-level (segmentation) and the geometric-level (distance map)\ntasks. Our extensive experimentation with varied quantities of labeled data in\nthe training sets justifies the effectiveness of our model for the segmentation\nof the left atrial cavity from Gadolinium-enhanced magnetic resonance (GE-MR)\nimages. With the incorporation of uncertainty estimates to detect failures in\nthe segmentation masks generated by CNNs, our study further showcases the\npotential of our model to flag low-quality segmentation from a given model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hasan_S/0/1/0/all/0/1\">S. M. Kamrul Hasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Linte_C/0/1/0/all/0/1\">Cristian A. Linte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ROS-X-Habitat: Bridging the ROS Ecosystem with Embodied AI. (arXiv:2109.07703v1 [cs.RO])","link":"http://arxiv.org/abs/2109.07703","description":"<p>We introduce ROS-X-Habitat, a software interface that bridges the AI Habitat\nplatform for embodied reinforcement learning agents with other robotics\nresources via ROS. This interface not only offers standardized communication\nprotocols between embodied agents and simulators, but also enables\nphysics-based simulation. With this interface, roboticists are able to train\ntheir own Habitat RL agents in another simulation environment or to develop\ntheir own robotic algorithms inside Habitat Sim. Through in silico experiments,\nwe demonstrate that ROS-X-Habitat has minimal impact on the navigation\nperformance and simulation speed of Habitat agents; that a standard set of ROS\nmapping, planning and navigation tools can run in the Habitat simulator, and\nthat a Habitat agent can run in the standard ROS simulator Gazebo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanxiong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haoyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_I/0/1/0/all/0/1\">Ian M. Mitchell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Pruning of Pointwise Convolutions in the Frequency Domain. (arXiv:2109.07707v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07707","description":"<p>Depthwise separable convolutions and frequency-domain convolutions are two\nrecent ideas for building efficient convolutional neural networks. They are\nseemingly incompatible: the vast majority of operations in depthwise separable\nCNNs are in pointwise convolutional layers, but pointwise layers use 1x1\nkernels, which do not benefit from frequency transformation. This paper unifies\nthese two ideas by transforming the activations, not the kernels. Our key\ninsights are that 1) pointwise convolutions commute with frequency\ntransformation and thus can be computed in the frequency domain without\nmodification, 2) each channel within a given layer has a different level of\nsensitivity to frequency domain pruning, and 3) each channel's sensitivity to\nfrequency pruning is approximately monotonic with respect to frequency. We\nleverage this knowledge by proposing a new technique which wraps each pointwise\nlayer in a discrete cosine transform (DCT) which is truncated to selectively\nprune coefficients above a given threshold as per the needs of each channel. To\nlearn which frequencies should be pruned from which channels, we introduce a\nnovel learned parameter which specifies each channel's pruning threshold. We\nadd a new regularization term which incentivizes the model to decrease the\nnumber of retained frequencies while still maintaining task accuracy. Unlike\nweight pruning techniques which rely on sparse operators, our contiguous\nfrequency band pruning results in fully dense computation. We apply our\ntechnique to MobileNetV2 and in the process reduce computation time by 22% and\nincur &lt;1% accuracy degradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buckler_M/0/1/0/all/0/1\">Mark Buckler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adit_N/0/1/0/all/0/1\">Neil Adit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuwei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampson_A/0/1/0/all/0/1\">Adrian Sampson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Activation based Gradient Output Sparsity to Accelerate Backpropagation in CNNs. (arXiv:2109.07710v1 [cs.LG])","link":"http://arxiv.org/abs/2109.07710","description":"<p>Machine/deep-learning (ML/DL) based techniques are emerging as a driving\nforce behind many cutting-edge technologies, achieving high accuracy on\ncomputer vision workloads such as image classification and object detection.\nHowever, training these models involving large parameters is both\ntime-consuming and energy-hogging. In this regard, several prior works have\nadvocated for sparsity to speed up the of DL training and more so, the\ninference phase. This work begins with the observation that during training,\nsparsity in the forward and backward passes are correlated. In that context, we\ninvestigate two types of sparsity (input and output type) inherent in gradient\ndescent-based optimization algorithms and propose a hardware micro-architecture\nto leverage the same. Our experimental results use five state-of-the-art CNN\nmodels on the Imagenet dataset, and show back propagation speedups in the range\nof 1.69$\\times$ to 5.43$\\times$, compared to the dense baseline execution. By\nexploiting sparsity in both the forward and backward passes, speedup\nimprovements range from 1.68$\\times$ to 3.30$\\times$ over the sparsity-agnostic\nbaseline execution. Our work also achieves significant reduction in training\niteration time over several previously proposed dense as well as sparse\naccelerator based platforms, in addition to achieving order of magnitude energy\nefficiency improvements over GPU based execution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarma_A/0/1/0/all/0/1\">Anup Sarma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sonali Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huaipan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pattnaik_A/0/1/0/all/0/1\">Ashutosh Pattnaik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Asit K Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1\">Vijaykrishnan Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandemir_M/0/1/0/all/0/1\">Mahmut T Kandemir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_C/0/1/0/all/0/1\">Chita R Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepMTS: Deep Multi-task Learning for Survival Prediction in Patients with Advanced Nasopharyngeal Carcinoma using Pretreatment PET/CT. (arXiv:2109.07711v1 [eess.IV])","link":"http://arxiv.org/abs/2109.07711","description":"<p>Nasopharyngeal Carcinoma (NPC) is a worldwide malignant epithelial cancer.\nSurvival prediction is a major concern for NPC patients, as it provides early\nprognostic information that is needed to guide treatments. Recently, deep\nlearning, which leverages Deep Neural Networks (DNNs) to learn deep\nrepresentations of image patterns, has been introduced to the survival\nprediction in various cancers including NPC. It has been reported that\nimage-derived end-to-end deep survival models have the potential to outperform\nclinical prognostic indicators and traditional radiomics-based survival models\nin prognostic performance. However, deep survival models, especially 3D models,\nrequire large image training data to avoid overfitting. Unfortunately, medical\nimage data is usually scarce, especially for Positron Emission\nTomography/Computed Tomography (PET/CT) due to the high cost of PET/CT\nscanning. Compared to Magnetic Resonance Imaging (MRI) or Computed Tomography\n(CT) providing only anatomical information of tumors, PET/CT that provides both\nanatomical (from CT) and metabolic (from PET) information is promising to\nachieve more accurate survival prediction. However, we have not identified any\n3D end-to-end deep survival model that applies to small PET/CT data of NPC\npatients. In this study, we introduced the concept of multi-task leaning into\ndeep survival models to address the overfitting problem resulted from small\ndata. Tumor segmentation was incorporated as an auxiliary task to enhance the\nmodel's efficiency of learning from scarce PET/CT data. Based on this idea, we\nproposed a 3D end-to-end Deep Multi-Task Survival model (DeepMTS) for joint\nsurvival prediction and tumor segmentation. Our DeepMTS can jointly learn\nsurvival prediction and tumor segmentation using PET/CT data of only 170\npatients with advanced NPC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meng_M/0/1/0/all/0/1\">Mingyuan Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_B/0/1/0/all/0/1\">Bingxin Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bi_L/0/1/0/all/0/1\">Lei Bi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_S/0/1/0/all/0/1\">Shaoli Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_D/0/1/0/all/0/1\">David Dagan Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1\">Jinman Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Object Detection by Attending to Per-Sample-Prototype. (arXiv:2109.07734v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07734","description":"<p>Few-shot object detection aims to detect instances of specific categories in\na query image with only a handful of support samples. Although this takes less\neffort than obtaining enough annotated images for supervised object detection,\nit results in a far inferior performance compared to the conventional object\ndetection methods. In this paper, we propose a meta-learning-based approach\nthat considers the unique characteristics of each support sample. Rather than\nsimply averaging the information of the support samples to generate a single\nprototype per category, our method can better utilize the information of each\nsupport sample by treating each support sample as an individual prototype.\nSpecifically, we introduce two types of attention mechanisms for aggregating\nthe query and support feature maps. The first is to refine the information of\nfew-shot samples by extracting shared information between the support samples\nthrough attention. Second, each support sample is used as a class code to\nleverage the information by comparing similarities between each support feature\nand query features. Our proposed method is complementary to the previous\nmethods, making it easy to plug and play for further improvement. We have\nevaluated our method on PASCAL VOC and COCO benchmarks, and the results verify\nthe effectiveness of our method. In particular, the advantages of our method\nare maximized when there is more diversity among support data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hojun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Myunggi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Machine Learning Methods for Predicting the Evolution of Brain Connectivity from a Baseline Timepoint. (arXiv:2109.07739v1 [cs.LG])","link":"http://arxiv.org/abs/2109.07739","description":"<p>Predicting the evolution of the brain network, also called connectome, by\nforeseeing changes in the connectivity weights linking pairs of anatomical\nregions makes it possible to spot connectivity-related neurological disorders\nin earlier stages and detect the development of potential connectomic\nanomalies. Remarkably, such a challenging prediction problem remains least\nexplored in the predictive connectomics literature. It is a known fact that\nmachine learning (ML) methods have proven their predictive abilities in a wide\nvariety of computer vision problems. However, ML techniques specifically\ntailored for the prediction of brain connectivity evolution trajectory from a\nsingle timepoint are almost absent. To fill this gap, we organized a Kaggle\ncompetition where 20 competing teams designed advanced machine learning\npipelines for predicting the brain connectivity evolution from a single\ntimepoint. The competing teams developed their ML pipelines with a combination\nof data pre-processing, dimensionality reduction, and learning methods.\nUtilizing an inclusive evaluation approach, we ranked the methods based on two\ncomplementary evaluation metrics (mean absolute error (MAE) and Pearson\nCorrelation Coefficient (PCC)) and their performances using different training\nand testing data perturbation strategies (single random split and\ncross-validation). The final rank was calculated using the rank product for\neach competing team across all evaluation measures and validation strategies.\nIn support of open science, the developed 20 ML pipelines along with the\nconnectomic dataset are made available on GitHub. The outcomes of this\ncompetition are anticipated to lead to the further development of predictive\nmodels that can foresee the evolution of brain connectivity over time, as well\nas other types of networks (e.g., genetic networks).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akti_S/0/1/0/all/0/1\">&#x15e;eymanur Akt&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamar_D/0/1/0/all/0/1\">Do&#x11f;ay Kamar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozlu_O/0/1/0/all/0/1\">&#xd6;zg&#xfc;r An&#x131;l &#xd6;zl&#xfc;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soydemir_I/0/1/0/all/0/1\">Ihsan Soydemir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akcan_M/0/1/0/all/0/1\">Muhammet Akcan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kul_A/0/1/0/all/0/1\">Abdullah Kul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekik_I/0/1/0/all/0/1\">Islem Rekik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Partially Observable Visual Navigation in a Diverse Environment. (arXiv:2109.07752v1 [cs.RO])","link":"http://arxiv.org/abs/2109.07752","description":"<p>How can a robot navigate successfully in a rich and diverse environment,\nindoors or outdoors, along an office corridor or a trail in the park, on the\nflat ground, the staircase, or the elevator, etc.? To this end, this work aims\nat three challenges: (i) complex visual observations, (ii) partial\nobservability of local sensing, and (iii) multimodal navigation behaviors that\ndepend on both the local environment and the high-level goal. We propose a\nnovel neural network (NN) architecture to represent a local controller and\nleverage the flexibility of the end-to-end approach to learn a powerful policy.\nTo tackle complex visual observations, we extract multiscale spatial\ninformation through convolution layers. To deal with partial observability, we\nencode rich history information in LSTM-like modules. Importantly, we integrate\nthe two into a single unified architecture that exploits convolutional memory\ncells to track the observation history at multiple spatial scales, which can\ncapture the complex spatiotemporal dependencies between observations and\ncontrols. We additionally condition the network on the high-level goal in order\nto generate different navigation behavior modes. Specifically, we propose to\nuse independent memory cells for different modes to prevent mode collapse in\nthe learned policy. We implemented the NN controller on the SPOT robot and\nevaluate it on three challenging tasks with partial observations: adversarial\npedestrian avoidance, blind-spot obstacle avoidance, and elevator riding. Our\nmodel significantly outperforms CNNs, conventional LSTMs, or the ablated\nversions of our model. A demo video will be publicly available, showing our\nSPOT robot traversing many different locations on our university campus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ai_B/0/1/0/all/0/1\">Bo Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinay/0/1/0/all/0/1\">Vinay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1\">David Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mask-Guided Feature Extraction and Augmentation for Ultra-Fine-Grained Visual Categorization. (arXiv:2109.07755v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07755","description":"<p>While the fine-grained visual categorization (FGVC) problems have been\ngreatly developed in the past years, the Ultra-fine-grained visual\ncategorization (Ultra-FGVC) problems have been understudied. FGVC aims at\nclassifying objects from the same species (very similar categories), while the\nUltra-FGVC targets at more challenging problems of classifying images at an\nultra-fine granularity where even human experts may fail to identify the visual\ndifference. The challenges for Ultra-FGVC mainly comes from two aspects: one is\nthat the Ultra-FGVC often arises overfitting problems due to the lack of\ntraining samples; and another lies in that the inter-class variance among\nimages is much smaller than normal FGVC tasks, which makes it difficult to\nlearn discriminative features for each class. To solve these challenges, a\nmask-guided feature extraction and feature augmentation method is proposed in\nthis paper to extract discriminative and informative regions of images which\nare then used to augment the original feature map. The advantage of the\nproposed method is that the feature detection and extraction model only\nrequires a small amount of target region samples with bounding boxes for\ntraining, then it can automatically locate the target area for a large number\nof images in the dataset at a high detection accuracy. Experimental results on\ntwo public datasets and ten state-of-the-art benchmark methods consistently\ndemonstrate the effectiveness of the proposed method both visually and\nquantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zicheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaohan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miaohua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yongsheng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Semantic Contrast for Self-Supervised Visual Representation Learning. (arXiv:2109.07756v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07756","description":"<p>Self-supervised representation learning for visual pre-training has achieved\nremarkable success with sample (instance or pixel) discrimination and semantics\ndiscovery of instance, whereas there still exists a non-negligible gap between\npre-trained model and downstream dense prediction tasks. Concretely, these\ndownstream tasks require more accurate representation, in other words, the\npixels from the same object must belong to a shared semantic category, which is\nlacking in the previous methods. In this work, we present Dense Semantic\nContrast (DSC) for modeling semantic category decision boundaries at a dense\nlevel to meet the requirement of these tasks. Furthermore, we propose a dense\ncross-image semantic contrastive learning framework for multi-granularity\nrepresentation learning. Specially, we explicitly explore the semantic\nstructure of the dataset by mining relations among pixels from different\nperspectives. For intra-image relation modeling, we discover pixel neighbors\nfrom multiple views. And for inter-image relations, we enforce pixel\nrepresentation from the same semantic class to be more similar than the\nrepresentation from different classes in one mini-batch. Experimental results\nshow that our DSC model outperforms state-of-the-art methods when transferring\nto downstream dense prediction tasks, including object detection, semantic\nsegmentation, and instance segmentation. Code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoni Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aoting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Ning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haiying Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Non-Line-of-Sight Photography. (arXiv:2109.07783v1 [eess.IV])","link":"http://arxiv.org/abs/2109.07783","description":"<p>Non-line-of-sight (NLOS) imaging is based on capturing the multi-bounce\nindirect reflections from the hidden objects. Active NLOS imaging systems rely\non the capture of the time of flight of light through the scene, and have shown\ngreat promise for the accurate and robust reconstruction of hidden scenes\nwithout the need for specialized scene setups and prior assumptions. Despite\nthat existing methods can reconstruct 3D geometries of the hidden scene with\nexcellent depth resolution, accurately recovering object textures and\nappearance with high lateral resolution remains an challenging problem. In this\nwork, we propose a new problem formulation, called NLOS photography, to\nspecifically address this deficiency. Rather than performing an intermediate\nestimate of the 3D scene geometry, our method follows a data-driven approach\nand directly reconstructs 2D images of a NLOS scene that closely resemble the\npictures taken with a conventional camera from the location of the relay wall.\nThis formulation largely simplifies the challenging reconstruction problem by\nbypassing the explicit modeling of 3D geometry, and enables the learning of a\ndeep model with a relatively small training dataset. The results are NLOS\nreconstructions of unprecedented lateral resolution and image quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_J/0/1/0/all/0/1\">Jiayong Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mu_F/0/1/0/all/0/1\">Fangzhou Mu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nam_J/0/1/0/all/0/1\">Ji Hyun Nam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raghavan_S/0/1/0/all/0/1\">Siddeshwar Raghavan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Velten_A/0/1/0/all/0/1\">Andreas Velten</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhiwei Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MHFC: Multi-Head Feature Collaboration for Few-Shot Learning. (arXiv:2109.07785v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07785","description":"<p>Few-shot learning (FSL) aims to address the data-scarce problem. A standard\nFSL framework is composed of two components: (1) Pre-train. Employ the base\ndata to generate a CNN-based feature extraction model (FEM). (2) Meta-test.\nApply the trained FEM to acquire the novel data's features and recognize them.\nFSL relies heavily on the design of the FEM. However, various FEMs have\ndistinct emphases. For example, several may focus more attention on the contour\ninformation, whereas others may lay particular emphasis on the texture\ninformation. The single-head feature is only a one-sided representation of the\nsample. Besides the negative influence of cross-domain (e.g., the trained FEM\ncan not adapt to the novel class flawlessly), the distribution of novel data\nmay have a certain degree of deviation compared with the ground truth\ndistribution, which is dubbed as distribution-shift-problem (DSP). To address\nthe DSP, we propose Multi-Head Feature Collaboration (MHFC) algorithm, which\nattempts to project the multi-head features (e.g., multiple features extracted\nfrom a variety of FEMs) to a unified space and fuse them to capture more\ndiscriminative information. Typically, first, we introduce a subspace learning\nmethod to transform the multi-head features to aligned low-dimensional\nrepresentations. It corrects the DSP via learning the feature with more\npowerful discrimination and overcomes the problem of inconsistent measurement\nscales from different head features. Then, we design an attention block to\nupdate combination weights for each head feature automatically. It\ncomprehensively considers the contribution of various perspectives and further\nimproves the discrimination of features. We evaluate the proposed method on\nfive benchmark datasets (including cross-domain experiments) and achieve\nsignificant improvements of 2.1%-7.8% compared with state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1\">Shuai Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Lei Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Rui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chunyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan-Jiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bao-Di Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Marginal MAP Estimation for Inverse RL under Occlusion with Observer Noise. (arXiv:2109.07788v1 [cs.RO])","link":"http://arxiv.org/abs/2109.07788","description":"<p>We consider the problem of learning the behavioral preferences of an expert\nengaged in a task from noisy and partially-observable demonstrations. This is\nmotivated by real-world applications such as a line robot learning from\nobserving a human worker, where some observations are occluded by environmental\nobjects that cannot be removed. Furthermore, robotic perception tends to be\nimperfect and noisy. Previous techniques for inverse reinforcement learning\n(IRL) take the approach of either omitting the missing portions or inferring it\nas part of expectation-maximization, which tends to be slow and prone to local\noptima. We present a new method that generalizes the well-known Bayesian\nmaximum-a-posteriori (MAP) IRL method by marginalizing the occluded portions of\nthe trajectory. This is additionally extended with an observation model to\naccount for perception noise. We show that the marginal MAP (MMAP) approach\nsignificantly improves on the previous IRL technique under occlusion in both\nformative evaluations on a toy problem and in a summative evaluation on an\nonion sorting line task by a robot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suresh_P/0/1/0/all/0/1\">Prasanth Sengadu Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_P/0/1/0/all/0/1\">Prashant Doshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label-Attention Transformer with Geometrically Coherent Objects for Image Captioning. (arXiv:2109.07799v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07799","description":"<p>Automatic transcription of scene understanding in images and videos is a step\ntowards artificial general intelligence. Image captioning is a nomenclature for\ndescribing meaningful information in an image using computer vision techniques.\nAutomated image captioning techniques utilize encoder and decoder architecture,\nwhere the encoder extracts features from an image and the decoder generates a\ntranscript. In this work, we investigate two unexplored ideas for image\ncaptioning using transformers: First, we demonstrate the enforcement of using\nobjects' relevance in the surrounding environment. Second, learning an explicit\nassociation between labels and language constructs. We propose label-attention\nTransformer with geometrically coherent objects (LATGeO). The proposed\ntechnique acquires a proposal of geometrically coherent objects using a deep\nneural network (DNN) and generates captions by investigating their\nrelationships using a label-attention module. Object coherence is defined using\nthe localized ratio of the geometrical properties of the proposals. The\nlabel-attention module associates the extracted objects classes to the\navailable dictionary using self-attention layers. The experimentation results\nshow that objects' relevance in surroundings and binding of their visual\nfeature with their geometrically localized ratios combined with its associated\nlabels help in defining meaningful captions. The proposed framework is tested\non the MSCOCO dataset, and a thorough evaluation resulting in overall better\nquantitative scores pronounces its superiority.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shikha Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olimov_F/0/1/0/all/0/1\">Farrukh Olimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafique_M/0/1/0/all/0/1\">Muhammad Aasim Rafique</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joonmo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1\">Moongu Jeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compact Binary Fingerprint for Image Copy Re-Ranking. (arXiv:2109.07802v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07802","description":"<p>Image copy detection is challenging and appealing topic in computer vision\nand signal processing. Recent advancements in multimedia have made distribution\nof image across the global easy and fast: that leads to many other issues such\nas forgery and image copy retrieval.\n</p>\n<p>Local keypoint descriptors such as SIFT are used to represent the images, and\nbased on those descriptors matching, images are matched and retrieved. Features\nare quantized so that searching/matching may be made feasible for large\ndatabases at the cost of accuracy loss. In this paper, we propose binary\nfeature that is obtained by quantizing the SIFT into binary, and rank list is\nre-examined to remove the false positives. Experiments on challenging dataset\nshows the gain in accuracy and time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_N/0/1/0/all/0/1\">Nazar Mohammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baber_J/0/1/0/all/0/1\">Junaid Baber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakhtyar_M/0/1/0/all/0/1\">Maheen Bakhtyar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandio_B/0/1/0/all/0/1\">Bilal Ahmed Chandio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanjrani_A/0/1/0/all/0/1\">Anwar Ali Sanjrani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection Accuracy for Evaluating Compositional Explanations of Units. (arXiv:2109.07804v1 [cs.LG])","link":"http://arxiv.org/abs/2109.07804","description":"<p>The recent success of deep learning models in solving complex problems and in\ndifferent domains has increased interest in understanding what they learn.\nTherefore, different approaches have been employed to explain these models, one\nof which uses human-understandable concepts as explanations. Two examples of\nmethods that use this approach are Network Dissection and Compositional\nexplanations. The former explains units using atomic concepts, while the latter\nmakes explanations more expressive, replacing atomic concepts with logical\nforms. While intuitively, logical forms are more informative than atomic\nconcepts, it is not clear how to quantify this improvement, and their\nevaluation is often based on the same metric that is optimized during the\nsearch-process and on the usage of hyper-parameters to be tuned. In this paper,\nwe propose to use as evaluation metric the Detection Accuracy, which measures\nunits' consistency of detection of their assigned explanations. We show that\nthis metric (1) evaluates explanations of different lengths effectively, (2)\ncan be used as a stopping criterion for the compositional explanation search,\neliminating the explanation length hyper-parameter, and (3) exposes new\nspecialized units whose length 1 explanations are the perceptual abstractions\nof their longer explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Makinwa_S/0/1/0/all/0/1\">Sayo M. Makinwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosa_B/0/1/0/all/0/1\">Biagio La Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capobianco_R/0/1/0/all/0/1\">Roberto Capobianco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Assignment Distillation for Object Detection. (arXiv:2109.07843v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07843","description":"<p>Knowledge distillation methods are proved to be promising in improving the\nperformance of neural networks and no additional computational expenses are\nrequired during the inference time. For the sake of boosting the accuracy of\nobject detection, a great number of knowledge distillation methods have been\nproposed particularly designed for object detection. However, most of these\nmethods only focus on feature-level distillation and label-level distillation,\nleaving the label assignment step, a unique and paramount procedure for object\ndetection, by the wayside. In this work, we come up with a simple but effective\nknowledge distillation approach focusing on label assignment in object\ndetection, in which the positive and negative samples of student network are\nselected in accordance with the predictions of teacher network. Our method\nshows encouraging results on the MSCOCO2017 benchmark, and can not only be\napplied to both one-stage detectors and two-stage detectors but also be\nutilized orthogonally with other knowledge distillation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Minghao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hailun Zhang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yige Yan</a> (2) ((1) Beijing Institute of Technology, (2) Hohai University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-aware Padding for Semantic Segmentation. (arXiv:2109.07854v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07854","description":"<p>Zero padding is widely used in convolutional neural networks to prevent the\nsize of feature maps diminishing too fast. However, it has been claimed to\ndisturb the statistics at the border. As an alternative, we propose a\ncontext-aware (CA) padding approach to extend the image. We reformulate the\npadding problem as an image extrapolation problem and illustrate the effects on\nthe semantic segmentation task. Using context-aware padding, the ResNet-based\nsegmentation model achieves higher mean Intersection-Over-Union than the\ntraditional zero padding on the Cityscapes and the dataset of DeepGlobe\nsatellite imaging challenge. Furthermore, our padding does not bring noticeable\noverhead during training and testing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu-Hui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proesmans_M/0/1/0/all/0/1\">Marc Proesmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Continual Learning Algorithms by Generating 3D Virtual Environments. (arXiv:2109.07855v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07855","description":"<p>Continual learning refers to the ability of humans and animals to\nincrementally learn over time in a given environment. Trying to simulate this\nlearning process in machines is a challenging task, also due to the inherent\ndifficulty in creating conditions for designing continuously evolving dynamics\nthat are typical of the real-world. Many existing research works usually\ninvolve training and testing of virtual agents on datasets of static images or\nshort videos, considering sequences of distinct learning tasks. However, in\norder to devise continual learning algorithms that operate in more realistic\nconditions, it is fundamental to gain access to rich, fully customizable and\ncontrolled experimental playgrounds. Focussing on the specific case of vision,\nwe thus propose to leverage recent advances in 3D virtual environments in order\nto approach the automatic generation of potentially life-long dynamic scenes\nwith photo-realistic appearance. Scenes are composed of objects that move along\nvariable routes with different and fully customizable timings, and randomness\ncan also be included in their evolution. A novel element of this paper is that\nscenes are described in a parametric way, thus allowing the user to fully\ncontrol the visual complexity of the input stream the agent perceives. These\ngeneral principles are concretely implemented exploiting a recently published\n3D virtual environment. The user can generate scenes without the need of having\nstrong skills in computer graphics, since all the generation facilities are\nexposed through a simple high-level Python interface. We publicly share the\nproposed generator.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meloni_E/0/1/0/all/0/1\">Enrico Meloni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Betti_A/0/1/0/all/0/1\">Alessandro Betti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faggi_L/0/1/0/all/0/1\">Lapo Faggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marullo_S/0/1/0/all/0/1\">Simone Marullo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiezzi_M/0/1/0/all/0/1\">Matteo Tiezzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melacci_S/0/1/0/all/0/1\">Stefano Melacci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Humanly Certifying Superhuman Classifiers. (arXiv:2109.07867v1 [cs.LG])","link":"http://arxiv.org/abs/2109.07867","description":"<p>Estimating the performance of a machine learning system is a longstanding\nchallenge in artificial intelligence research. Today, this challenge is\nespecially relevant given the emergence of systems which appear to increasingly\noutperform human beings. In some cases, this \"superhuman\" performance is\nreadily demonstrated; for example by defeating legendary human players in\ntraditional two player games. On the other hand, it can be challenging to\nevaluate classification models that potentially surpass human performance.\nIndeed, human annotations are often treated as a ground truth, which implicitly\nassumes the superiority of the human over any models trained on human\nannotations. In reality, human annotators can make mistakes and be subjective.\nEvaluating the performance with respect to a genuine oracle may be more\nobjective and reliable, even when querying the oracle is expensive or\nimpossible. In this paper, we first raise the challenge of evaluating the\nperformance of both humans and models with respect to an oracle which is\nunobserved. We develop a theory for estimating the accuracy compared to the\noracle, using only imperfect human annotations for reference. Our analysis\nprovides a simple recipe for detecting and certifying superhuman performance in\nthis setting, which we believe will assist in understanding the stage of\ncurrent research on classification. We validate the convergence of the bounds\nand the assumptions of our theory on carefully designed toy experiments with\nknown oracles. Moreover, we demonstrate the utility of our theory by\nmeta-analyzing large-scale natural language processing tasks, for which an\noracle does not exist, and show that under our assumptions a number of models\nfrom recent years are with high probability superhuman.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiongkai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walder_C/0/1/0/all/0/1\">Christian Walder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenchen Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainability Requires Interactivity. (arXiv:2109.07869v1 [cs.LG])","link":"http://arxiv.org/abs/2109.07869","description":"<p>When explaining the decisions of deep neural networks, simple stories are\ntempting but dangerous. Especially in computer vision, the most popular\nexplanation approaches give a false sense of comprehension to its users and\nprovide an overly simplistic picture. We introduce an interactive framework to\nunderstand the highly complex decision boundaries of modern vision models. It\nallows the user to exhaustively inspect, probe, and test a network's decisions.\nAcross a range of case studies, we compare the power of our interactive\napproach to static explanation methods, showing how these can lead a user\nastray, with potentially severe consequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirchler_M/0/1/0/all/0/1\">Matthias Kirchler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graf_M/0/1/0/all/0/1\">Martin Graf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kloft_M/0/1/0/all/0/1\">Marius Kloft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippert_C/0/1/0/all/0/1\">Christoph Lippert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resolution based Feature Distillation for Cross Resolution Person Re-Identification. (arXiv:2109.07871v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07871","description":"<p>Person re-identification (re-id) aims to retrieve images of same identities\nacross different camera views. Resolution mismatch occurs due to varying\ndistances between person of interest and cameras, this significantly degrades\nthe performance of re-id in real world scenarios. Most of the existing\napproaches resolve the re-id task as low resolution problem in which a low\nresolution query image is searched in a high resolution images gallery. Several\napproaches apply image super resolution techniques to produce high resolution\nimages but ignore the multiple resolutions of gallery images which is a better\nrealistic scenario. In this paper, we introduce channel correlations to improve\nthe learning of features from the degraded data. In addition, to overcome the\nproblem of multiple resolutions we propose a Resolution based Feature\nDistillation (RFD) approach. Such an approach learns resolution invariant\nfeatures by filtering the resolution related features from the final feature\nvectors that are used to compute the distance matrix. We tested the proposed\napproach on two synthetically created datasets and on one original multi\nresolution dataset with real degradation. Our approach improves the performance\nwhen multiple resolutions occur in the gallery and have comparable results in\ncase of single resolution (low resolution re-id).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Munir_A/0/1/0/all/0/1\">Asad Munir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chengjin Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goossens_B/0/1/0/all/0/1\">Bart Goossens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philips_W/0/1/0/all/0/1\">Wilfried Philips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Micheloni_C/0/1/0/all/0/1\">Christian Micheloni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SketchHairSalon: Deep Sketch-based Hair Image Synthesis. (arXiv:2109.07874v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07874","description":"<p>Recent deep generative models allow real-time generation of hair images from\nsketch inputs. Existing solutions often require a user-provided binary mask to\nspecify a target hair shape. This not only costs users extra labor but also\nfails to capture complicated hair boundaries. Those solutions usually encode\nhair structures via orientation maps, which, however, are not very effective to\nencode complex structures. We observe that colored hair sketches already\nimplicitly define target hair shapes as well as hair appearance and are more\nflexible to depict hair structures than orientation maps. Based on these\nobservations, we present SketchHairSalon, a two-stage framework for generating\nrealistic hair images directly from freehand sketches depicting desired hair\nstructure and appearance. At the first stage, we train a network to predict a\nhair matte from an input hair sketch, with an optional set of non-hair strokes.\nAt the second stage, another network is trained to synthesize the structure and\nappearance of hair images from the input sketch and the generated matte. To\nmake the networks in the two stages aware of long-term dependency of strokes,\nwe apply self-attention modules to them. To train these networks, we present a\nnew dataset containing thousands of annotated hair sketch-image pairs and\ncorresponding hair mattes. Two efficient methods for sketch completion are\nproposed to automatically complete repetitive braided parts and hair strokes,\nrespectively, thus reducing the workload of users. Based on the trained\nnetworks and the two sketch completion strategies, we build an intuitive\ninterface to allow even novice users to design visually pleasing hair images\nexhibiting various hair structures and appearance via freehand sketches. The\nqualitative and quantitative evaluations show the advantages of the proposed\nsystem over the existing or alternative solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chufeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Deng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Youyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongbo Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Medical Pre-Diagnosis System for Histopathological Image of Breast Cancer. (arXiv:2109.07878v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07878","description":"<p>This paper constructs a novel intelligent medical diagnosis system, which can\nrealize automatic communication and breast cancer pathological image\nrecognition. This system contains two main parts, including a pre-training\nchatbot called M-Chatbot and an improved neural network model of\nEfficientNetV2-S named EfficientNetV2-SA, in which the activation function in\ntop layers is replaced by ACON-C. Using information retrieval mechanism,\nM-Chatbot instructs patients to send breast pathological image to\nEfficientNetV2-SA network, and then the classifier trained by transfer learning\nwill return the diagnosis results. We verify the performance of our chatbot and\nclassification on the extrinsic metrics and BreaKHis dataset, respectively. The\ntask completion rate of M-Chatbot reached 63.33\\%. For the BreaKHis dataset,\nthe highest accuracy of EfficientNetV2-SA network have achieved 84.71\\%. All\nthese experimental results illustrate that the proposed model can improve the\naccuracy performance of image recognition and our new intelligent medical\ndiagnosis system is successful and efficient in providing automatic diagnosis\nof breast cancer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Shiyu Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runhai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhaohang Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated risk classification of colon biopsies based on semantic segmentation of histopathology images. (arXiv:2109.07892v1 [eess.IV])","link":"http://arxiv.org/abs/2109.07892","description":"<p>Artificial Intelligence (AI) can potentially support histopathologists in the\ndiagnosis of a broad spectrum of cancer types. In colorectal cancer (CRC), AI\ncan alleviate the laborious task of characterization and reporting on resected\nbiopsies, including polyps, the numbers of which are increasing as a result of\nCRC population screening programs, ongoing in many countries all around the\nglobe. Here, we present an approach to address two major challenges in\nautomated assessment of CRC histopathology whole-slide images. First, we\npresent an AI-based method to segment multiple tissue compartments in the\nH\\&amp;E-stained whole-slide image, which provides a different, more perceptible\npicture of tissue morphology and composition. We test and compare a panel of\nstate-of-the-art loss functions available for segmentation models, and provide\nindications about their use in histopathology image segmentation, based on the\nanalysis of a) a multi-centric cohort of CRC cases from five medical centers in\nthe Netherlands and Germany, and b) two publicly available datasets on\nsegmentation in CRC. Second, we use the best performing AI model as the basis\nfor a computer-aided diagnosis system (CAD) that classifies colon biopsies into\nfour main categories that are relevant pathologically. We report the\nperformance of this system on an independent cohort of more than 1,000\npatients. The results show the potential of such an AI-based system to assist\npathologists in diagnosis of CRC in the context of population screening. We\nhave made the segmentation model available for research use on\nhttps://grand-challenge.org/algorithms/colon-tissue-segmentation/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bokhorsta_J/0/1/0/all/0/1\">John-Melle Bokhorsta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nagtegaal_I/0/1/0/all/0/1\">Iris D. Nagtegaal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fraggetta_F/0/1/0/all/0/1\">Filippo Fraggetta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vatrano_S/0/1/0/all/0/1\">Simona Vatrano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mesker_W/0/1/0/all/0/1\">Wilma Mesker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vieth_M/0/1/0/all/0/1\">Michael Vieth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laak_J/0/1/0/all/0/1\">Jeroen van der Laak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ciompi_F/0/1/0/all/0/1\">Francesco Ciompi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heterogeneous Relational Complement for Vehicle Re-identification. (arXiv:2109.07894v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07894","description":"<p>The crucial problem in vehicle re-identification is to find the same vehicle\nidentity when reviewing this object from cross-view cameras, which sets a\nhigher demand for learning viewpoint-invariant representations. In this paper,\nwe propose to solve this problem from two aspects: constructing robust feature\nrepresentations and proposing camera-sensitive evaluations. We first propose a\nnovel Heterogeneous Relational Complement Network (HRCN) by incorporating\nregion-specific features and cross-level features as complements for the\noriginal high-level output. Considering the distributional differences and\nsemantic misalignment, we propose graph-based relation modules to embed these\nheterogeneous features into one unified high-dimensional space. On the other\nhand, considering the deficiencies of cross-camera evaluations in existing\nmeasures (i.e., CMC and AP), we then propose a Cross-camera Generalization\nMeasure (CGM) to improve the evaluations by introducing position-sensitivity\nand cross-camera generalization penalties. We further construct a new benchmark\nof existing models with our proposed CGM and experimental results reveal that\nour proposed HRCN model achieves new state-of-the-art in VeRi-776, VehicleID,\nand VERI-Wild.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiajian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yifan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Ke Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Computer Science Can Aid Forest Restoration. (arXiv:2109.07898v1 [cs.CY])","link":"http://arxiv.org/abs/2109.07898","description":"<p>The world faces two interlinked crises: climate change and loss of\nbiodiversity. Forest restoration on degraded lands and surplus croplands can\nplay a significant role both in sequestering carbon and re-establishing\nbio-diversity. There is a considerable body of research and practice that\naddresses forest restoration. However, there has been little work by computer\nscientists to bring powerful computational techniques to bear on this important\narea of work, perhaps due to a lack of awareness. In an attempt to bridge this\ngap, we present our vision of how techniques from computer science, broadly\nspeaking, can aid current practice in forest restoration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gordon_G/0/1/0/all/0/1\">Gemma Gordon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holcomb_A/0/1/0/all/0/1\">Amelia Holcomb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_T/0/1/0/all/0/1\">Tom Kelly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keshav_S/0/1/0/all/0/1\">Srinivasan Keshav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludlum_J/0/1/0/all/0/1\">Jon Ludlum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhavapeddy_A/0/1/0/all/0/1\">Anil Madhavapeddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FSER: Deep Convolutional Neural Networks for Speech Emotion Recognition. (arXiv:2109.07916v1 [eess.AS])","link":"http://arxiv.org/abs/2109.07916","description":"<p>Using mel-spectrograms over conventional MFCCs features, we assess the\nabilities of convolutional neural networks to accurately recognize and classify\nemotions from speech data. We introduce FSER, a speech emotion recognition\nmodel trained on four valid speech databases, achieving a high-classification\naccuracy of 95,05\\%, over 8 different emotion classes: anger, anxiety, calm,\ndisgust, happiness, neutral, sadness, surprise. On each benchmark dataset, FSER\noutperforms the best models introduced so far, achieving a state-of-the-art\nperformance. We show that FSER stays reliable, independently of the language,\nsex identity, and any other external factor. Additionally, we describe how FSER\ncould potentially be used to improve mental and emotional health care and how\nour analysis and findings serve as guidelines and benchmarks for further works\nin the same direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dossou_B/0/1/0/all/0/1\">Bonaventure F. P. Dossou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gbenou_Y/0/1/0/all/0/1\">Yeno K. S. Gbenou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M2RNet: Multi-modal and Multi-scale Refined Network for RGB-D Salient Object Detection. (arXiv:2109.07922v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07922","description":"<p>Salient object detection is a fundamental topic in computer vision. Previous\nmethods based on RGB-D often suffer from the incompatibility of multi-modal\nfeature fusion and the insufficiency of multi-scale feature aggregation. To\ntackle these two dilemmas, we propose a novel multi-modal and multi-scale\nrefined network (M2RNet). Three essential components are presented in this\nnetwork. The nested dual attention module (NDAM) explicitly exploits the\ncombined features of RGB and depth flows. The adjacent interactive aggregation\nmodule (AIAM) gradually integrates the neighbor features of high, middle and\nlow levels. The joint hybrid optimization loss (JHOL) makes the predictions\nhave a prominent outline. Extensive experiments demonstrate that our method\noutperforms other state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xian Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jinchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruixun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_X/0/1/0/all/0/1\">Xiuli Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongpeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lifting 2D Object Locations to 3D by Discounting LiDAR Outliers across Objects and Views. (arXiv:2109.07945v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07945","description":"<p>We present a system for automatic converting of 2D mask object predictions\nand raw LiDAR point clouds into full 3D bounding boxes of objects. Because the\nLiDAR point clouds are partial, directly fitting bounding boxes to the point\nclouds is meaningless. Instead, we suggest that obtaining good results requires\nsharing information between \\emph{all} objects in the dataset jointly, over\nmultiple frames. We then make three improvements to the baseline. First, we\naddress ambiguities in predicting the object rotations via direct optimization\nin this space while still backpropagating rotation prediction through the\nmodel. Second, we explicitly model outliers and task the network with learning\ntheir typical patterns, thus better discounting them. Third, we enforce\ntemporal consistency when video data is available. With these contributions,\nour method significantly outperforms previous work despite the fact that those\nmethods use significantly more complex pipelines, 3D models and additional\nhuman-annotated external sources of prior information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McCraith_R/0/1/0/all/0/1\">Robert McCraith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Insafudinov_E/0/1/0/all/0/1\">Eldar Insafudinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_L/0/1/0/all/0/1\">Lukas Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learnable Multi-level Frequency Decomposition and Hierarchical Attention Mechanism for Generalized Face Presentation Attack Detection. (arXiv:2109.07950v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07950","description":"<p>With the increased deployment of face recognition systems in our daily lives,\nface presentation attack detection (PAD) is attracting a lot of attention and\nplaying a key role in securing face recognition systems. Despite the great\nperformance achieved by the hand-crafted and deep learning based methods in\nintra-dataset evaluations, the performance drops when dealing with unseen\nscenarios. In this work, we propose a dual-stream convolution neural networks\n(CNNs) framework. One stream adapts four learnable frequency filters to learn\nfeatures in the frequency domain, which are less influenced variations in\nsensors/illuminations. The other stream leverage the RGB images to complement\nthe features of the frequency domain. Moreover, we propose a hierarchical\nattention module integration to join the information from the two streams at\ndifferent stages by considering the nature of deep features in different layers\nof the CNN. The proposed method is evaluated in the intra-dataset and\ncross-dataset setups and the results demonstrates that our proposed approach\nenhances the generalizability in most experimental setups in comparison to\nstate-of-the-art, including the methods designed explicitly for domain\nadaption/shift problem. We successfully prove the design of our proposed PAD\nsolution in a step-wise ablation study that involves our proposed learnable\nfrequency decomposition, our hierarchical attention module design, and the used\nloss function. Training codes and pre-trained models are publicly released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meiling Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of Tencent Multi-modal Ads Video Understanding Challenge. (arXiv:2109.07951v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07951","description":"<p>Multi-modal Ads Video Understanding Challenge is the first grand challenge\naiming to comprehensively understand ads videos. Our challenge includes two\ntasks: video structuring in the temporal dimension and multi-modal video\nclassification. It asks the participants to accurately predict both the scene\nboundaries and the multi-label categories of each scene based on a fine-grained\nand ads-related category hierarchy. Therefore, our task has four distinguishing\nfeatures from previous ones: ads domain, multi-modal information, temporal\nsegmentation, and multi-label classification. It will advance the foundation of\nads video understanding and have a significant impact on many ads applications\nlike video recommendation. This paper presents an overview of our challenge,\nincluding the background of ads videos, an elaborate description of task and\ndataset, evaluation protocol, and our proposed baseline. By ablating the key\ncomponents of our baseline, we would like to reveal the main challenges of this\ntask and provide useful guidance for future research of this area. In this\npaper, we give an extended version of our challenge overview. The dataset will\nbe publicly available at https://algo.qq.com/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhimin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jiangfeng Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qinglin Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quality-aware Cine Cardiac MRI Reconstruction and Analysis from Undersampled k-space Data. (arXiv:2109.07955v1 [eess.IV])","link":"http://arxiv.org/abs/2109.07955","description":"<p>Cine cardiac MRI is routinely acquired for the assessment of cardiac health,\nbut the imaging process is slow and typically requires several breath-holds to\nacquire sufficient k-space profiles to ensure good image quality. Several\nundersampling-based reconstruction techniques have been proposed during the\nlast decades to speed up cine cardiac MRI acquisition. However, the\nundersampling factor is commonly fixed to conservative values before\nacquisition to ensure diagnostic image quality, potentially leading to\nunnecessarily long scan times. In this paper, we propose an end-to-end\nquality-aware cine short-axis cardiac MRI framework that combines image\nacquisition and reconstruction with downstream tasks such as segmentation,\nvolume curve analysis and estimation of cardiac functional parameters. The goal\nis to reduce scan time by acquiring only a fraction of k-space data to enable\nthe reconstruction of images that can pass quality control checks and produce\nreliable estimates of cardiac functional parameters. The framework consists of\na deep learning model for the reconstruction of 2D+t cardiac cine MRI images\nfrom undersampled data, an image quality-control step to detect good quality\nreconstructions, followed by a deep learning model for bi-ventricular\nsegmentation, a quality-control step to detect good quality segmentations and\nautomated calculation of cardiac functional parameters. To demonstrate the\nfeasibility of the proposed approach, we perform simulations using a cohort of\nselected participants from the UK Biobank (n=270), 200 healthy subjects and 70\npatients with cardiomyopathies. Our results show that we can produce\nquality-controlled images in a scan time reduced from 12 to 4 seconds per\nslice, enabling reliable estimates of cardiac functional parameters such as\nejection fraction within 5% mean absolute error.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Machado_I/0/1/0/all/0/1\">Ines Machado</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puyol_Anton_E/0/1/0/all/0/1\">Esther Puyol-Anton</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hammernik_K/0/1/0/all/0/1\">Kerstin Hammernik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cruz_G/0/1/0/all/0/1\">Gastao Cruz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ugurlu_D/0/1/0/all/0/1\">Devran Ugurlu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ruijsink_B/0/1/0/all/0/1\">Bram Ruijsink</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Castelo_Branco_M/0/1/0/all/0/1\">Miguel Castelo-Branco</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Young_A/0/1/0/all/0/1\">Alistair Young</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prieto_C/0/1/0/all/0/1\">Claudia Prieto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schnabel_J/0/1/0/all/0/1\">Julia A. Schnabel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+King_A/0/1/0/all/0/1\">Andrew P. King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real Time Monocular Vehicle Velocity Estimation using Synthetic Data. (arXiv:2109.07957v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07957","description":"<p>Vision is one of the primary sensing modalities in autonomous driving. In\nthis paper we look at the problem of estimating the velocity of road vehicles\nfrom a camera mounted on a moving car. Contrary to prior methods that train\nend-to-end deep networks that estimate the vehicles' velocity from the video\npixels, we propose a two-step approach where first an off-the-shelf tracker is\nused to extract vehicle bounding boxes and then a small neural network is used\nto regress the vehicle velocity from the tracked bounding boxes. Surprisingly,\nwe find that this still achieves state-of-the-art estimation performance with\nthe significant benefit of separating perception from dynamics estimation via a\nclean, interpretable and verifiable interface which allows us distill the\nstatistics which are crucial for velocity estimation. We show that the latter\ncan be used to easily generate synthetic training data in the space of bounding\nboxes and use this to improve the performance of our method further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McCraith_R/0/1/0/all/0/1\">Robert McCraith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_L/0/1/0/all/0/1\">Lukas Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R3LIVE: A Robust, Real-time, RGB-colored, LiDAR-Inertial-Visual tightly-coupled state Estimation and mapping package. (arXiv:2109.07982v1 [cs.RO])","link":"http://arxiv.org/abs/2109.07982","description":"<p>In this letter, we propose a novel LiDAR-Inertial-Visual sensor fusion\nframework termed R3LIVE, which takes advantage of measurement of LiDAR,\ninertial, and visual sensors to achieve robust and accurate state estimation.\nR3LIVE is contained of two subsystems, the LiDAR-inertial odometry (LIO) and\nvisual-inertial odometry (VIO). The LIO subsystem (FAST-LIO) takes advantage of\nthe measurement from LiDAR and inertial sensors and builds the geometry\nstructure of (i.e. the position of 3D points) global maps. The VIO subsystem\nutilizes the data of visual-inertial sensors and renders the map's texture\n(i.e. the color of 3D points). More specifically, the VIO subsystem fuses the\nvisual data directly and effectively by minimizing the frame-to-map photometric\nerror. The developed system R3LIVE is developed based on our previous work\nR2LIVE, with careful architecture design and implementation. Experiment results\nshow that the resultant system achieves more robustness and higher accuracy in\nstate estimation than current counterparts (see our attached video).\n</p>\n<p>R3LIVE is a versatile and well-engineered system toward various possible\napplications, which can not only serve as a SLAM system for real-time robotic\napplications, but can also reconstruct the dense, precise, RGB-colored 3D maps\nfor applications like surveying and mapping. Moreover, to make R3LIVE more\nextensible, we develop a series of offline utilities for reconstructing and\ntexturing meshes, which further minimizes the gap between R3LIVE and various of\n3D applications such as simulators, video games and etc (see our demos video).\nTo share our findings and make contributions to the community, we open source\nR3LIVE on our Github, including all of our codes, software utilities, and the\nmechanical design of our device.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiarong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harnessing Perceptual Adversarial Patches for Crowd Counting. (arXiv:2109.07986v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07986","description":"<p>Crowd counting, which is significantly important for estimating the number of\npeople in safety-critical scenes, has been shown to be vulnerable to\nadversarial examples in the physical world (e.g., adversarial patches). Though\nharmful, adversarial examples are also valuable for assessing and better\nunderstanding model robustness. However, existing adversarial example\ngeneration methods in crowd counting scenarios lack strong transferability\namong different black-box models. Motivated by the fact that transferability is\npositively correlated to the model-invariant characteristics, this paper\nproposes the Perceptual Adversarial Patch (PAP) generation framework to learn\nthe shared perceptual features between models by exploiting both the model\nscale perception and position perception. Specifically, PAP exploits\ndifferentiable interpolation and density attention to help learn the invariance\nbetween models during training, leading to better transferability. In addition,\nwe surprisingly found that our adversarial patches could also be utilized to\nbenefit the performance of vanilla models for alleviating several challenges\nincluding cross datasets and complex backgrounds. Extensive experiments under\nboth digital and physical world scenarios demonstrate the effectiveness of our\nPAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shunchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiakai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aishan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yijie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations. (arXiv:2109.07991v1 [cs.RO])","link":"http://arxiv.org/abs/2109.07991","description":"<p>Multisensory object-centric perception, reasoning, and interaction have been\na key research topic in recent years. However, the progress in these directions\nis limited by the small set of objects available -- synthetic objects are not\nrealistic enough and are mostly centered around geometry, while real object\ndatasets such as YCB are often practically challenging and unstable to acquire\ndue to international shipping, inventory, and financial cost. We present\nObjectFolder, a dataset of 100 virtualized objects that addresses both\nchallenges with two key innovations. First, ObjectFolder encodes the visual,\nauditory, and tactile sensory data for all objects, enabling a number of\nmultisensory object recognition tasks, beyond existing datasets that focus\npurely on object geometry. Second, ObjectFolder employs a uniform,\nobject-centric, and implicit representation for each object's visual textures,\nacoustic simulations, and tactile readings, making the dataset flexible to use\nand easy to share. We demonstrate the usefulness of our dataset as a testbed\nfor multisensory perception and control by evaluating it on a variety of\nbenchmark tasks, including instance recognition, cross-sensory retrieval, 3D\nreconstruction, and robotic grasping.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruohan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yen-Yu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mall_S/0/1/0/all/0/1\">Shivani Mall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Propaganda Techniques in Memes. (arXiv:2109.08013v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08013","description":"<p>Propaganda can be defined as a form of communication that aims to influence\nthe opinions or the actions of people towards a specific goal; this is achieved\nby means of well-defined rhetorical and psychological devices. Propaganda, in\nthe form we know it today, can be dated back to the beginning of the 17th\ncentury. However, it is with the advent of the Internet and the social media\nthat it has started to spread on a much larger scale than before, thus becoming\nmajor societal and political issue. Nowadays, a large fraction of propaganda in\nsocial media is multimodal, mixing textual with visual content. With this in\nmind, here we propose a new multi-label multimodal task: detecting the type of\npropaganda techniques used in memes. We further create and release a new corpus\nof 950 memes, carefully annotated with 22 propaganda techniques, which can\nappear in the text, in the image, or in both. Our analysis of the corpus shows\nthat understanding both modalities together is essential for detecting these\ntechniques. This is further confirmed in our experiments with several\nstate-of-the-art multimodal models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1\">Dimitar Dimitrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_B/0/1/0/all/0/1\">Bishr Bin Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1\">Fabrizio Silvestri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The pitfalls of using open data to develop deep learning solutions for COVID-19 detection in chest X-rays. (arXiv:2109.08020v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08020","description":"<p>Since the emergence of COVID-19, deep learning models have been developed to\nidentify COVID-19 from chest X-rays. With little to no direct access to\nhospital data, the AI community relies heavily on public data comprising\nnumerous data sources. Model performance results have been exceptional when\ntraining and testing on open-source data, surpassing the reported capabilities\nof AI in pneumonia-detection prior to the COVID-19 outbreak. In this study\nimpactful models are trained on a widely used open-source data and tested on an\nexternal test set and a hospital dataset, for the task of classifying chest\nX-rays into one of three classes: COVID-19, non-COVID pneumonia and\nno-pneumonia. Classification performance of the models investigated is\nevaluated through ROC curves, confusion matrices and standard classification\nmetrics. Explainability modules are implemented to explore the image features\nmost important to classification. Data analysis and model evaluations show that\nthe popular open-source dataset COVIDx is not representative of the real\nclinical problem and that results from testing on this are inflated. Dependence\non open-source data can leave models vulnerable to bias and confounding\nvariables, requiring careful analysis to develop clinically useful/viable AI\ntools for COVID-19 detection in chest X-rays.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harkness_R/0/1/0/all/0/1\">Rachael Harkness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_G/0/1/0/all/0/1\">Geoff Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F Frangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zucker_K/0/1/0/all/0/1\">Kieran Zucker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Wide-Angle Portraits Correction by Multi-Scale Transformer. (arXiv:2109.08024v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08024","description":"<p>We propose a semi-supervised network for wide-angle portraits correction.\nWide-angle images often suffer from skew and distortion affected by perspective\ndistortion, especially noticeable at the face regions. Previous deep learning\nbased approaches require the ground-truth correction flow maps for the training\nguidance. However, such labels are expensive, which can only be obtained\nmanually. In this work, we propose a semi-supervised scheme, which can consume\nunlabeled data in addition to the labeled data for improvements. Specifically,\nour semi-supervised scheme takes the advantages of the consistency mechanism,\nwith several novel components such as direction and range consistency (DRC) and\nregression consistency (RC). Furthermore, our network, named as Multi-Scale\nSwin-Unet (MS-Unet), is built upon the multi-scale swin transformer block\n(MSTB), which can learn both local-scale and long-range semantic information\neffectively. In addition, we introduce a high-quality unlabeled dataset with\nrich scenarios for the training. Extensive experiments demonstrate that the\nproposed method is superior over the state-of-the-art methods and other\nrepresentative baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fushun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hua Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Architecture Search in operational context: a remote sensing case-study. (arXiv:2109.08028v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08028","description":"<p>Deep learning has become in recent years a cornerstone tool fueling key\ninnovations in the industry, such as autonomous driving. To attain good\nperformances, the neural network architecture used for a given application must\nbe chosen with care. These architectures are often handcrafted and therefore\nprone to human biases and sub-optimal selection. Neural Architecture Search\n(NAS) is a framework introduced to mitigate such risks by jointly optimizing\nthe network architectures and its weights. Albeit its novelty, it was applied\non complex tasks with significant results - e.g. semantic image segmentation.\nIn this technical paper, we aim to evaluate its ability to tackle a challenging\noperational task: semantic segmentation of objects of interest in satellite\nimagery. Designing a NAS framework is not trivial and has strong dependencies\nto hardware constraints. We therefore motivate our NAS approach selection and\nprovide corresponding implementation details. We also present novel ideas to\ncarry out other such use-case studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cazasnoves_A/0/1/0/all/0/1\">Anthony Cazasnoves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganaye_P/0/1/0/all/0/1\">Pierre-Antoine Ganaye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchis_K/0/1/0/all/0/1\">K&#xe9;vin Sanchis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceillier_T/0/1/0/all/0/1\">Tugdual Ceillier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering. (arXiv:2109.08029v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08029","description":"<p>Integrating outside knowledge for reasoning in visio-linguistic tasks such as\nvisual question answering (VQA) is an open problem. Given that pretrained\nlanguage models have been shown to include world knowledge, we propose to use a\nunimodal (text-only) train and inference procedure based on automatic\noff-the-shelf captioning of images and pretrained language models. Our results\non a visual question answering task which requires external knowledge (OK-VQA)\nshow that our text-only model outperforms pretrained multimodal (image-text)\nmodels of comparable number of parameters. In contrast, our model is less\neffective in a standard VQA task (VQA 2.0) confirming that our text-only method\nis specially effective for tasks requiring external knowledge. In addition, we\nshow that our unimodal model is complementary to multimodal models in both\nOK-VQA and VQA 2.0, and yield the best result to date in OK-VQA among systems\nnot using external knowledge graphs, and comparable to systems that do use\nthem. Our qualitative analysis on OK-VQA reveals that automatic captions often\nfail to capture relevant information in the images, which seems to be balanced\nby the better inference ability of the text-only language models. Our work\nopens up possibilities to further improve inference in visio-linguistic tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salaberria_A/0/1/0/all/0/1\">Ander Salaberria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azkune_G/0/1/0/all/0/1\">Gorka Azkune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacalle_O/0/1/0/all/0/1\">Oier Lopez de Lacalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1\">Aitor Soroa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1\">Eneko Agirre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Temporal Sentence Grounding in Videos. (arXiv:2109.08039v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08039","description":"<p>Temporal sentence grounding in videos~(TSGV), which aims to localize one\ntarget segment from an untrimmed video with respect to a given sentence query,\nhas drawn increasing attentions in the research community over the past few\nyears. Different from the task of temporal action localization, TSGV is more\nflexible since it can locate complicated activities via natural languages,\nwithout restrictions from predefined action categories. Meanwhile, TSGV is more\nchallenging since it requires both textual and visual understanding for\nsemantic alignment between two modalities~(i.e., text and video). In this\nsurvey, we give a comprehensive overview for TSGV, which i) summarizes the\ntaxonomy of existing methods, ii) provides a detailed description of the\nevaluation protocols~(i.e., datasets and metrics) to be used in TSGV, and iii)\nin-depth discusses potential problems of current benchmarking designs and\nresearch directions for further investigations. To the best of our knowledge,\nthis is the first systematic survey on temporal sentence grounding. More\nspecifically, we first discuss existing TSGV approaches by grouping them into\nfour categories, i.e., two-stage methods, end-to-end methods, reinforcement\nlearning-based methods, and weakly supervised methods. Then we present the\nbenchmark datasets and evaluation metrics to assess current research progress.\nFinally, we discuss some limitations in TSGV through pointing out potential\nproblems improperly resolved in the current evaluation protocols, which may\npush forwards more cutting edge research in TSGV. Besides, we also share our\ninsights on several promising directions, including three typical tasks with\nnew and practical settings based on TSGV.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1\">Xiaohan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yitian Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Dataset For Large-scale 3D Facial Emotion Recognition. (arXiv:2109.08043v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08043","description":"<p>The tremendous development in deep learning has led facial expression\nrecognition (FER) to receive much attention in the past few years. Although 3D\nFER has an inherent edge over its 2D counterpart, work on 2D images has\ndominated the field. The main reason for the slow development of 3D FER is the\nunavailability of large training and large test datasets. Recognition\naccuracies have already saturated on existing 3D emotion recognition datasets\ndue to their small gallery sizes. Unlike 2D photographs, 3D facial scans are\nnot easy to collect, causing a bottleneck in the development of deep 3D FER\nnetworks and datasets. In this work, we propose a method for generating a large\ndataset of 3D faces with labeled emotions. We also develop a deep convolutional\nneural network(CNN) for 3D FER trained on 624,000 3D facial scans. The test\ndata comprises 208,000 3D facial scans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Faizan Farooq Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilani_S/0/1/0/all/0/1\">Syed Zulqarnain Gilani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eformer: Edge Enhancement based Transformer for Medical Image Denoising. (arXiv:2109.08044v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08044","description":"<p>In this work, we present Eformer - Edge enhancement based transformer, a\nnovel architecture that builds an encoder-decoder network using transformer\nblocks for medical image denoising. Non-overlapping window-based self-attention\nis used in the transformer block that reduces computational requirements. This\nwork further incorporates learnable Sobel-Feldman operators to enhance edges in\nthe image and propose an effective way to concatenate them in the intermediate\nlayers of our architecture. The experimental analysis is conducted by comparing\ndeterministic learning and residual learning for the task of medical image\ndenoising. To defend the effectiveness of our approach, our model is evaluated\non the AAPM-Mayo Clinic Low-Dose CT Grand Challenge Dataset and achieves\nstate-of-the-art performance, $i.e.$, 43.487 PSNR, 0.0067 RMSE, and 0.9861\nSSIM. We believe that our work will encourage more research in\ntransformer-based architectures for medical image denoising using residual\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Luthra_A/0/1/0/all/0/1\">Achleshwar Luthra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sulakhe_H/0/1/0/all/0/1\">Harsh Sulakhe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mittal_T/0/1/0/all/0/1\">Tanish Mittal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iyer_A/0/1/0/all/0/1\">Abhishek Iyer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yadav_S/0/1/0/all/0/1\">Santosh Yadav</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rotation Averaging in a Split Second: A Primal-Dual Method and a Closed-Form for Cycle Graphs. (arXiv:2109.08046v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08046","description":"<p>A cornerstone of geometric reconstruction, rotation averaging seeks the set\nof absolute rotations that optimally explains a set of measured relative\norientations between them. In spite of being an integral part of bundle\nadjustment and structure-from-motion, averaging rotations is both a non-convex\nand high-dimensional optimization problem. In this paper, we address it from a\nmaximum likelihood estimation standpoint and make a twofold contribution.\nFirstly, we set forth a novel initialization-free primal-dual method which we\nshow empirically to converge to the global optimum. Further, we derive what is\nto our knowledge, the first optimal closed-form solution for rotation averaging\nin cycle graphs and contextualize this result within spectral graph theory. Our\nproposed methods achieve a significant gain both in precision and performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moreira_G/0/1/0/all/0/1\">Gabriel Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marques_M/0/1/0/all/0/1\">Manuel Marques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costeira_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Costeira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Raising context awareness in motion forecasting. (arXiv:2109.08048v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08048","description":"<p>Learning-based trajectory prediction models have encountered great success,\nwith the promise of leveraging contextual information in addition to motion\nhistory. Yet, we find that state-of-the-art forecasting methods tend to overly\nrely on the agent's dynamics, failing to exploit the semantic cues provided at\nits input. To alleviate this issue, we introduce CAB, a motion forecasting\nmodel equipped with a training procedure designed to promote the use of\nsemantic contextual information. We also introduce two novel metrics --\ndispersion and convergence-to-range -- to measure the temporal consistency of\nsuccessive forecasts, which we found missing in standard metrics. Our method is\nevaluated on the widely adopted nuScenes Prediction benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Younes_H/0/1/0/all/0/1\">H&#xe9;di Ben-Younes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zablocki_E/0/1/0/all/0/1\">&#xc9;loi Zablocki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Micka&#xeb;l Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Machine Learning Framework for Automatic Prediction of Human Semen Motility. (arXiv:2109.08049v1 [cs.LG])","link":"http://arxiv.org/abs/2109.08049","description":"<p>In the field of reproductive health, a vital aspect for the detection of male\nfertility issues is the analysis of human semen quality. Two factors of\nimportance are the morphology and motility of the sperm cells. While the former\ndescribes defects in different parts of a spermatozoon, the latter measures the\nefficient movement of cells. For many non-human species, so-called\nComputer-Aided Sperm Analysis systems work well for assessing these\ncharacteristics from microscopic video recordings but struggle with human sperm\nsamples which generally show higher degrees of debris and dead spermatozoa, as\nwell as lower overall sperm motility. Here, machine learning methods that\nharness large amounts of training data to extract salient features could\nsupport physicians with the detection of fertility issues or in vitro\nfertilisation procedures. In this work, the overall motility of given sperm\nsamples is predicted with the help of a machine learning framework integrating\nunsupervised methods for feature extraction with downstream regression models.\nThe models evaluated herein improve on the state-of-the-art for video-based\nsperm-motility prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ottl_S/0/1/0/all/0/1\">Sandra Ottl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerczuk_M/0/1/0/all/0/1\">Maurice Gerczuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amiriparian_S/0/1/0/all/0/1\">Shahin Amiriparian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn Schuller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Visual Representation Learning for Fashion Compatibility. (arXiv:2109.08052v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08052","description":"<p>We consider the problem of complementary fashion prediction. Existing\napproaches focus on learning an embedding space where fashion items from\ndifferent categories that are visually compatible are closer to each other.\nHowever, creating such labeled outfits is intensive and also not feasible to\ngenerate all possible outfit combinations, especially with large fashion\ncatalogs. In this work, we propose a semi-supervised learning approach where we\nleverage large unlabeled fashion corpus to create pseudo-positive and\npseudo-negative outfits on the fly during training. For each labeled outfit in\na training batch, we obtain a pseudo-outfit by matching each item in the\nlabeled outfit with unlabeled items. Additionally, we introduce consistency\nregularization to ensure that representation of the original images and their\ntransformations are consistent to implicitly incorporate colour and other\nimportant attributes through self-supervision. We conduct extensive experiments\non Polyvore, Polyvore-D and our newly created large-scale Fashion Outfits\ndatasets, and show that our approach with only a fraction of labeled examples\nperforms on-par with completely supervised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Revanur_A/0/1/0/all/0/1\">Ambareesh Revanur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vijay Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Deepthi Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Urdu text in natural scene images: a new dataset and preliminary text detection. (arXiv:2109.08060v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08060","description":"<p>Text detection in natural scene images for content analysis is an interesting\ntask. The research community has seen some great developments for\nEnglish/Mandarin text detection. However, Urdu text extraction in natural scene\nimages is a task not well addressed. In this work, firstly, a new dataset is\nintroduced for Urdu text in natural scene images. The dataset comprises of 500\nstandalone images acquired from real scenes. Secondly, the channel enhanced\nMaximally Stable Extremal Region (MSER) method is applied to extract Urdu text\nregions as candidates in an image. Two-stage filtering mechanism is applied to\neliminate non-candidate regions. In the first stage, text and noise are\nclassified based on their geometric properties. In the second stage, a support\nvector machine classifier is trained to discard non-text candidate regions.\nAfter this, text candidate regions are linked using centroid-based vertical and\nhorizontal distances. Text lines are further analyzed by a different classifier\nbased on HOG features to remove non-text regions. Extensive experimentation is\nperformed on the locally developed dataset to evaluate the performance. The\nexperimental results show good performance on test set images. The dataset will\nbe made available for research use. To the best of our knowledge, the work is\nthe first of its kind for the Urdu language and would provide a good dataset\nfor free research use and serve as a baseline performance on the task of Urdu\ntext extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ali_H/0/1/0/all/0/1\">Hazrat Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_K/0/1/0/all/0/1\">Khalid Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mujtaba_G/0/1/0/all/0/1\">Ghulam Mujtaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fayyaz_A/0/1/0/all/0/1\">Ahmad Fayyaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulbul_M/0/1/0/all/0/1\">Mohammad Farhad Bulbul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karam_F/0/1/0/all/0/1\">Fazal Wahab Karam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zahir_A/0/1/0/all/0/1\">Ali Zahir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invertable Frowns: Video-to-Video Facial Emotion Translation. (arXiv:2109.08061v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08061","description":"<p>We present Wav2Lip-Emotion, a video-to-video translation architecture that\nmodifies facial expressions of emotion in videos of speakers. Previous work\nmodifies emotion in images, uses a single image to produce a video with\nanimated emotion, or puppets facial expressions in videos with landmarks from a\nreference video. However, many use cases such as modifying an actor's\nperformance in post-production, coaching individuals to be more animated\nspeakers, or touching up emotion in a teleconference require a video-to-video\ntranslation approach. We explore a method to maintain speakers' lip movements,\nidentity, and pose while translating their expressed emotion. Our approach\nextends an existing multi-modal lip synchronization architecture to modify the\nspeaker's emotion using L1 reconstruction and pre-trained emotion objectives.\nWe also propose a novel automated emotion evaluation approach and corroborate\nit with a user study. These find that we succeed in modifying emotion while\nmaintaining lip synchronization. Visual quality is somewhat diminished, with a\ntrade off between greater emotion modification and visual quality between model\nvariants. Nevertheless, we demonstrate (1) that facial expressions of emotion\ncan be modified with nothing other than L1 reconstruction and pre-trained\nemotion objectives and (2) that our automated emotion evaluation approach\naligns with human judgements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Magnusson_I/0/1/0/all/0/1\">Ian Magnusson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1\">Aruna Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippman_A/0/1/0/all/0/1\">Andrew Lippman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DisUnknown: Distilling Unknown Factors for Disentanglement Learning. (arXiv:2109.08090v1 [cs.LG])","link":"http://arxiv.org/abs/2109.08090","description":"<p>Disentangling data into interpretable and independent factors is critical for\ncontrollable generation tasks. With the availability of labeled data,\nsupervision can help enforce the separation of specific factors as expected.\nHowever, it is often expensive or even impossible to label every single factor\nto achieve fully-supervised disentanglement. In this paper, we adopt a general\nsetting where all factors that are hard to label or identify are encapsulated\nas a single unknown factor. Under this setting, we propose a flexible\nweakly-supervised multi-factor disentanglement framework DisUnknown, which\nDistills Unknown factors for enabling multi-conditional generation regarding\nboth labeled and unknown factors. Specifically, a two-stage training approach\nis adopted to first disentangle the unknown factor with an effective and robust\ntraining method, and then train the final generator with the proper\ndisentanglement of all labeled factors utilizing the unknown distillation. To\ndemonstrate the generalization capacity and scalability of our method, we\nevaluate it on multiple benchmark datasets qualitatively and quantitatively and\nfurther apply it to various real-world applications on complicated datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1\">Sitao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuming Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_P/0/1/0/all/0/1\">Pengda Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_M/0/1/0/all/0/1\">Menglei Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yajie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingming He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aesthetics and neural network image representations. (arXiv:2109.08103v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08103","description":"<p>We analyze the spaces of images encoded by generative networks of the BigGAN\narchitecture. We find that generic multiplicative perturbations away from the\nphoto-realistic point often lead to images which appear as \"artistic\nrenditions\" of the corresponding objects. This demonstrates an emergence of\naesthetic properties directly from the structure of the photo-realistic\nenvironment coupled with its neural network parametrization. Moreover,\nmodifying a deep semantic part of the neural network encoding leads to the\nappearance of symbolic visual representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Janik_R/0/1/0/all/0/1\">Romuald A. Janik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural \\'{E}tendue Expander for Ultra-Wide-Angle High-Fidelity Holographic Display. (arXiv:2109.08123v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08123","description":"<p>Holographic displays can generate light fields by dynamically modulating the\nwavefront of a coherent beam of light using a spatial light modulator,\npromising rich virtual and augmented reality applications. However, the limited\nspatial resolution of existing dynamic spatial light modulators imposes a tight\nbound on the diffraction angle. As a result, today's holographic displays\npossess low \\'{e}tendue, which is the product of the display area and the\nmaximum solid angle of diffracted light. The low \\'{e}tendue forces a sacrifice\nof either the field of view (FOV) or the display size. In this work, we lift\nthis limitation by presenting neural \\'{e}tendue expanders. This new breed of\noptical elements, which is learned from a natural image dataset, enables higher\ndiffraction angles for ultra-wide FOV while maintaining both a compact form\nfactor and the fidelity of displayed contents to human viewers. With neural\n\\'{e}tendue expanders, we achieve 64$\\times$ \\'{e}tendue expansion of natural\nimages with reconstruction quality (measured in PSNR) over 29dB on simulated\nretinal-resolution images. As a result, the proposed approach with expansion\nfactor 64$\\times$ enables high-fidelity ultra-wide-angle holographic projection\nof natural images using an 8K-pixel SLM, resulting in a 18.5 mm eyebox size and\n2.18 steradians FOV, covering 85\\% of the human stereo FOV.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baek_S/0/1/0/all/0/1\">Seung-Hwan Baek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tseng_E/0/1/0/all/0/1\">Ethan Tseng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maimone_A/0/1/0/all/0/1\">Andrew Maimone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matsuda_N/0/1/0/all/0/1\">Nathan Matsuda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuo_G/0/1/0/all/0/1\">Grace Kuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heidrich_W/0/1/0/all/0/1\">Wolfgang Heidrich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lanman_D/0/1/0/all/0/1\">Douglas Lanman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heide_F/0/1/0/all/0/1\">Felix Heide</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An End-to-End Transformer Model for 3D Object Detection. (arXiv:2109.08141v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08141","description":"<p>We propose 3DETR, an end-to-end Transformer based object detection model for\n3D point clouds. Compared to existing detection methods that employ a number of\n3D-specific inductive biases, 3DETR requires minimal modifications to the\nvanilla Transformer block. Specifically, we find that a standard Transformer\nwith non-parametric queries and Fourier positional embeddings is competitive\nwith specialized architectures that employ libraries of 3D-specific operators\nwith hand-tuned hyperparameters. Nevertheless, 3DETR is conceptually simple and\neasy to implement, enabling further improvements by incorporating 3D domain\nknowledge. Through extensive experiments, we show 3DETR outperforms the\nwell-established and highly optimized VoteNet baselines on the challenging\nScanNetV2 dataset by 9.5%. Furthermore, we show 3DETR is applicable to 3D tasks\nbeyond detection, and can serve as a building block for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1\">Rohit Girdhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Fishyscapes Benchmark: Measuring Blind Spots in Semantic Segmentation. (arXiv:1904.03215v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1904.03215","description":"<p>Deep learning has enabled impressive progress in the accuracy of semantic\nsegmentation. Yet, the ability to estimate uncertainty and detect failure is\nkey for safety-critical applications like autonomous driving. Existing\nuncertainty estimates have mostly been evaluated on simple tasks, and it is\nunclear whether these methods generalize to more complex scenarios. We present\nFishyscapes, the first public benchmark for uncertainty estimation in a\nreal-world task of semantic segmentation for urban driving. It evaluates\npixel-wise uncertainty estimates towards the detection of anomalous objects in\nfront of the vehicle. We~adapt state-of-the-art methods to recent semantic\nsegmentation models and compare approaches based on softmax confidence,\nBayesian learning, and embedding density. Our results show that anomaly\ndetection is far from solved even for ordinary situations, while our benchmark\nallows measuring advancements beyond the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blum_H/0/1/0/all/0/1\">Hermann Blum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarlin_P/0/1/0/all/0/1\">Paul-Edouard Sarlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieto_J/0/1/0/all/0/1\">Juan Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siegwart_R/0/1/0/all/0/1\">Roland Siegwart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadena_C/0/1/0/all/0/1\">Cesar Cadena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Out-of-Distribution Detection with Divergence Guarantee in Deep Generative Models. (arXiv:2002.03328v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2002.03328","description":"<p>Recent research has revealed that deep generative models including flow-based\nmodels and Variational autoencoders may assign higher likelihood to\nout-of-distribution (OOD) data than in-distribution (ID) data. However, we\ncannot sample out OOD data from the model. This counterintuitive phenomenon has\nnot been satisfactorily explained. In this paper, we prove theorems to\ninvestigate the divergences in flow-based model and give two explanations to\nthe above phenomenon from divergence and geometric perspectives, respectively.\nBased on our analysis, we propose two group anomaly detection methods.\nFurthermore, we decompose the KL divergence and propose a point-wise anomaly\ndetection method. We have conducted extensive experiments on prevalent\nbenchmarks to evaluate our methods. For group anomaly detection (GAD), our\nmethod can achieve near 100\\% AUROC on all problems and has robustness against\ndata manipulations. On the contrary, the state-of-the-art (SOTA) GAD method\nperforms not better than random guessing for challenging problems and can be\nattacked by data manipulation in almost all cases. For point-wise anomaly\ndetection (PAD), our method is comparable to the SOTA PAD method on one\ncategory of problems and outperforms the baseline significantly on another\ncategory of problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yufeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wanwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenbang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Ji Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kenli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hongmei Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monocular, One-stage, Regression of Multiple 3D People. (arXiv:2008.12272v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.12272","description":"<p>This paper focuses on the regression of multiple 3D people from a single RGB\nimage. Existing approaches predominantly follow a multi-stage pipeline that\nfirst detects people in bounding boxes and then independently regresses their\n3D body meshes. In contrast, we propose to Regress all meshes in a One-stage\nfashion for Multiple 3D People (termed ROMP). The approach is conceptually\nsimple, bounding box-free, and able to learn a per-pixel representation in an\nend-to-end manner. Our method simultaneously predicts a Body Center heatmap and\na Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel\nlevel. Through a body-center-guided sampling process, the body mesh parameters\nof all people in the image are easily extracted from the Mesh Parameter map.\nEquipped with such a fine-grained representation, our one-stage framework is\nfree of the complex multi-stage process and more robust to occlusion. Compared\nwith state-of-the-art methods, ROMP achieves superior performance on the\nchallenging multi-person benchmarks, including 3DPW and CMU Panoptic.\nExperiments on crowded/occluded datasets demonstrate the robustness under\nvarious types of occlusion. The released code is the first real-time\nimplementation of monocular multi-person 3D mesh regression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Q/0/1/0/all/0/1\">Qian Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yili Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring the Biases and Effectiveness of Content-Style Disentanglement. (arXiv:2008.12378v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.12378","description":"<p>A recent spate of state-of-the-art semi- and un-supervised solutions\ndisentangle and encode image \"content\" into a spatial tensor and image\nappearance or \"style\" into a vector, to achieve good performance in spatially\nequivariant tasks (e.g. image-to-image translation). To achieve this, they\nemploy different model design, learning objective, and data biases. While\nconsiderable effort has been made to measure disentanglement in vector\nrepresentations, and assess its impact on task performance, such analysis for\n(spatial) content - style disentanglement is lacking. In this paper, we conduct\nan empirical study to investigate the role of different biases in content-style\ndisentanglement settings and unveil the relationship between the degree of\ndisentanglement and task performance. In particular, we consider the setting\nwhere we: (i) identify key design choices and learning constraints for three\npopular content-style disentanglement models; (ii) relax or remove such\nconstraints in an ablation fashion; and (iii) use two metrics to measure the\ndegree of disentanglement and assess its effect on each task performance. Our\nexperiments reveal that there is a \"sweet spot\" between disentanglement, task\nperformance and - surprisingly - content interpretability, suggesting that\nblindly forcing for higher disentanglement can hurt model performance and\ncontent factors semanticness. Our findings, as well as the used\ntask-independent metrics, can be used to guide the design and selection of new\nmodels for tasks where content-style representations are useful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thermos_S/0/1/0/all/0/1\">Spyridon Thermos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valvano_G/0/1/0/all/0/1\">Gabriele Valvano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chartsias_A/0/1/0/all/0/1\">Agisilaos Chartsias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1\">Alison O&#x27;Neil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pedestrian Trajectory Prediction with Convolutional Neural Networks. (arXiv:2010.05796v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.05796","description":"<p>Predicting the future trajectories of pedestrians is a challenging problem\nthat has a range of application, from crowd surveillance to autonomous driving.\nIn literature, methods to approach pedestrian trajectory prediction have\nevolved, transitioning from physics-based models to data-driven models based on\nrecurrent neural networks. In this work, we propose a new approach to\npedestrian trajectory prediction, with the introduction of a novel 2D\nconvolutional model. This new model outperforms recurrent models, and it\nachieves state-of-the-art results on the ETH and TrajNet datasets. We also\npresent an effective system to represent pedestrian positions and powerful data\naugmentation techniques, such as the addition of Gaussian noise and the use of\nrandom rotations, which can be applied to any model. As an additional\nexploratory analysis, we present experimental results on the inclusion of\noccupancy methods to model social information, which empirically show that\nthese methods are ineffective in capturing social interaction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zamboni_S/0/1/0/all/0/1\">Simone Zamboni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kefato_Z/0/1/0/all/0/1\">Zekarias Tilahun Kefato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girdzijauskas_S/0/1/0/all/0/1\">Sarunas Girdzijauskas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christoffer_N/0/1/0/all/0/1\">Noren Christoffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Col_L/0/1/0/all/0/1\">Laura Dal Col</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeFlow: Learning Complex Image Degradations from Unpaired Data with Conditional Flows. (arXiv:2101.05796v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.05796","description":"<p>The difficulty of obtaining paired data remains a major bottleneck for\nlearning image restoration and enhancement models for real-world applications.\nCurrent strategies aim to synthesize realistic training data by modeling noise\nand degradations that appear in real-world settings. We propose DeFlow, a\nmethod for learning stochastic image degradations from unpaired data. Our\napproach is based on a novel unpaired learning formulation for conditional\nnormalizing flows. We model the degradation process in the latent space of a\nshared flow encoder-decoder network. This allows us to learn the conditional\ndistribution of a noisy image given the clean input by solely minimizing the\nnegative log-likelihood of the marginal distributions. We validate our DeFlow\nformulation on the task of joint image restoration and super-resolution. The\nmodels trained with the synthetic data generated by DeFlow outperform previous\nlearnable approaches on three recent datasets. Code and trained models are\navailable at: https://github.com/volflow/DeFlow\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolf_V/0/1/0/all/0/1\">Valentin Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lugmayr_A/0/1/0/all/0/1\">Andreas Lugmayr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Awareness Attention for Few-Shot Object Detection. (arXiv:2102.12152v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.12152","description":"<p>While recent progress has significantly boosted few-shot classification (FSC)\nperformance, few-shot object detection (FSOD) remains challenging for modern\nlearning systems. Existing FSOD systems follow FSC approaches, ignoring\ncritical issues such as spatial variability and uncertain representations, and\nconsequently result in low performance. Observing this, we propose a novel\n\\textbf{Dual-Awareness Attention (DAnA)} mechanism that enables networks to\nadaptively interpret the given support images. DAnA transforms support images\ninto \\textbf{query-position-aware} (QPA) features, guiding detection networks\nprecisely by assigning customized support information to each local region of\nthe query. In addition, the proposed DAnA component is flexible and adaptable\nto multiple existing object detection frameworks. By adopting DAnA,\nconventional object detection networks, Faster R-CNN and RetinaNet, which are\nnot designed explicitly for few-shot learning, reach state-of-the-art\nperformance in FSOD tasks. In comparison with previous methods, our model\nsignificantly increases the performance by 47\\% (+6.9 AP), showing remarkable\nability under various evaluation settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tung-I Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueh-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yu-Cheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu-Hsiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_J/0/1/0/all/0/1\">Jia-Fong Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wen-Chin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Field Convolutions for Surface CNNs. (arXiv:2104.03916v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03916","description":"<p>We present a novel surface convolution operator acting on vector fields that\nis based on a simple observation: instead of combining neighboring features\nwith respect to a single coordinate parameterization defined at a given point,\nwe have every neighbor describe the position of the point within its own\ncoordinate frame. This formulation combines intrinsic spatial convolution with\nparallel transport in a scattering operation while placing no constraints on\nthe filters themselves, providing a definition of convolution that commutes\nwith the action of isometries, has increased descriptive potential, and is\nrobust to noise and other nuisance factors. The result is a rich notion of\nconvolution which we call field convolution, well-suited for CNNs on surfaces.\nField convolutions are flexible, straight-forward to incorporate into surface\nlearning frameworks, and their highly discriminating nature has cascading\neffects throughout the learning pipeline. Using simple networks constructed\nfrom residual field convolution blocks, we achieve state-of-the-art results on\nstandard benchmarks in fundamental geometry processing tasks, such as shape\nclassification, segmentation, correspondence, and sparse matching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitchel_T/0/1/0/all/0/1\">Thomas W. Mitchel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1\">Vladimir G. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazhdan_M/0/1/0/all/0/1\">Michael Kazhdan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NURBS-Diff: A differentiable programming module for NURBS. (arXiv:2104.14547v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.14547","description":"<p>Boundary representations (B-reps) using Non-Uniform Rational B-splines\n(NURBS) are the de facto standard used in CAD, but their utility in deep\nlearning-based approaches is not well researched. We propose a differentiable\nNURBS module to integrate the NURBS representation of CAD models with deep\nlearning methods. We mathematically define the derivatives of the NURBS curves\nor surfaces with respect to the input parameters. These derivatives are used to\ndefine an approximate Jacobian that can be used to perform the \"backward\"\nevaluation used while training deep learning models. We have implemented our\nNURBS module using GPU-accelerated algorithms and integrated it with PyTorch, a\npopular deep learning framework. We demonstrate the efficacy of our NURBS\nmodule in performing CAD operations such as curve or surface fitting and\nsurface offsetting. Further, we show its utility in deep learning for\nunsupervised point cloud reconstruction. These examples show that our module\nperforms better for certain deep learning frameworks and can be directly\nintegrated with any deep-learning framework requiring NURBS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Anjana Deva Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balu_A/0/1/0/all/0/1\">Aditya Balu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1\">Harshil Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Soumik Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1\">Chinmay Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1\">Adarsh Krishnamurthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Pursuit of Knowledge: Discovering and Localizing Novel Categories using Dual Memory. (arXiv:2105.01652v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.01652","description":"<p>We tackle object category discovery, which is the problem of discovering and\nlocalizing novel objects in a large unlabeled dataset. While existing methods\nshow results on datasets with less cluttered scenes and fewer object instances\nper image, we present our results on the challenging COCO dataset. Moreover, we\nargue that, rather than discovering new categories from scratch, discovery\nalgorithms can benefit from identifying what is already known and focusing\ntheir attention on the unknown. We propose a method that exploits prior\nknowledge about certain object types to discover new categories by leveraging\ntwo memory modules, namely Working and Semantic memory. We show the performance\nof our detector on the COCO minival dataset to demonstrate its in-the-wild\ncapabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rambhatla_S/0/1/0/all/0/1\">Sai Saketh Rambhatla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EchoCP: An Echocardiography Dataset in Contrast Transthoracic Echocardiography for Patent Foramen Ovale Diagnosis. (arXiv:2105.08267v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.08267","description":"<p>Patent foramen ovale (PFO) is a potential separation between the septum,\nprimum and septum secundum located in the anterosuperior portion of the atrial\nseptum. PFO is one of the main factors causing cryptogenic stroke which is the\nfifth leading cause of death in the United States. For PFO diagnosis, contrast\ntransthoracic echocardiography (cTTE) is preferred as being a more robust\nmethod compared with others. However, the current PFO diagnosis through cTTE is\nextremely slow as it is proceeded manually by sonographers on echocardiography\nvideos. Currently there is no publicly available dataset for this important\ntopic in the community. In this paper, we present EchoCP, as the first\nechocardiography dataset in cTTE targeting PFO diagnosis.\n</p>\n<p>EchoCP consists of 30 patients with both rest and Valsalva maneuver videos\nwhich covers various PFO grades. We further establish an automated baseline\nmethod for PFO diagnosis based on the state-of-the-art cardiac chamber\nsegmentation technique, which achieves 0.89 average mean Dice score, but only\n0.60/0.67 mean accuracies for PFO diagnosis, leaving large room for\nimprovement. We hope that the challenging EchoCP dataset can stimulate further\nresearch and lead to innovative and generic solutions that would have an impact\nin multiple domains. Our dataset is released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1\">Tianchen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhihe Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_M/0/1/0/all/0/1\">Meiping Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_J/0/1/0/all/0/1\">Jian Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bi_S/0/1/0/all/0/1\">Shanshan Bi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fei_H/0/1/0/all/0/1\">Hongwen Fei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Semantic Hallucination for Domain Generalized Semantic Segmentation. (arXiv:2106.04144v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04144","description":"<p>Convolutional neural networks may perform poorly when the test and train data\nare from different domains. While this problem can be mitigated by using the\ntarget domain data to align the source and target domain feature\nrepresentations, the target domain data may be unavailable due to privacy\nconcerns. Consequently, there is a need for methods that generalize well\nwithout access to target domain data during training. In this work, we propose\nan adversarial hallucination approach, which combines a class-wise\nhallucination module and a semantic segmentation module. Since the segmentation\nperformance varies across different classes, we design a semantic-conditioned\nstyle hallucination layer to adaptively stylize each class. The classwise\nstylization parameters are generated from the semantic knowledge in the\nsegmentation probability maps of the source domain image. Both modules compete\nadversarially, with the hallucination module generating increasingly\n'difficult' style images to challenge the segmentation module. In response, the\nsegmentation module improves its performance as it is trained with generated\nsamples at an appropriate class-wise difficulty level. Experiments on state of\nthe art domain adaptation work demonstrate the efficacy of our proposed method\nwhen no target domain data are available for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tjio_G/0/1/0/all/0/1\">Gabriel Tjio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1\">Rick Siow Mong Goh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Separating Boundary Points via Structural Regularization for Very Compact Clusters. (arXiv:2106.05430v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05430","description":"<p>Clustering algorithms have significantly improved along with Deep Neural\nNetworks which provide effective representation of data. Existing methods are\nbuilt upon deep autoencoder and self-training process that leverages the\ndistribution of cluster assignments of samples. However, as the fundamental\nobjective of the autoencoder is focused on efficient data reconstruction, the\nlearnt space may be sub-optimal for clustering. Moreover, it requires highly\neffective codes (i.e., representation) of data, otherwise the initial cluster\ncenters often cause stability issues during self-training. Many\nstate-of-the-art clustering algorithms use convolution operation to extract\nefficient codes but their applications are limited to image data. In this\nregard, we propose an end-to-end deep clustering algorithm, i.e., Very Compact\nClusters (VCC). VCC takes advantage of distributions of local relationships of\nsamples near the boundary of clusters, so that they can be properly separated\nand pulled to cluster centers to form compact clusters. Experimental results on\nvarious datasets illustrate that our proposed approach achieves competitive\nclustering performance against most of the state-of-the-art clustering methods\nfor both image and non-image data, and its results can be easily qualitatively\nseen in the learnt low-dimensional space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Won Hwa Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Meta-learning with Disentanglement for Domain-generalised Medical Image Segmentation. (arXiv:2106.13292v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13292","description":"<p>Generalising deep models to new data from new centres (termed here domains)\nremains a challenge. This is largely attributed to shifts in data statistics\n(domain shifts) between source and unseen domains. Recently, gradient-based\nmeta-learning approaches where the training data are split into meta-train and\nmeta-test sets to simulate and handle the domain shifts during training have\nshown improved generalisation performance. However, the current fully\nsupervised meta-learning approaches are not scalable for medical image\nsegmentation, where large effort is required to create pixel-wise annotations.\nMeanwhile, in a low data regime, the simulated domain shifts may not\napproximate the true domain shifts well across source and unseen domains. To\naddress this problem, we propose a novel semi-supervised meta-learning\nframework with disentanglement. We explicitly model the representations related\nto domain shifts. Disentangling the representations and combining them to\nreconstruct the input image allows unlabeled data to be used to better\napproximate the true domain shifts for meta-learning. Hence, the model can\nachieve better generalisation performance, especially when there is a limited\namount of labeled data. Experiments show that the proposed method is robust on\ndifferent segmentation tasks and achieves state-of-the-art generalisation\nperformance on two public benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thermos_S/0/1/0/all/0/1\">Spyridon Thermos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1\">Alison O&#x27;Neil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Trajectory Prediction Conditioned on Lane-Graph Traversals. (arXiv:2106.15004v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15004","description":"<p>Accurately predicting the future motion of surrounding vehicles requires\nreasoning about the inherent uncertainty in driving behavior. This uncertainty\ncan be loosely decoupled into lateral (e.g., keeping lane, turning) and\nlongitudinal (e.g., accelerating, braking). We present a novel method that\ncombines learned discrete policy rollouts with a focused decoder on subsets of\nthe lane graph. The policy rollouts explore different goals given current\nobservations, ensuring that the model captures lateral variability.\nLongitudinal variability is captured by our latent variable model decoder that\nis conditioned on various subsets of the lane graph. Our model achieves\nstate-of-the-art performance on the nuScenes motion prediction dataset, and\nqualitatively demonstrates excellent scene compliance. Detailed ablations\nhighlight the importance of the policy rollouts and the decoder architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deo_N/0/1/0/all/0/1\">Nachiket Deo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolff_E/0/1/0/all/0/1\">Eric M. Wolff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Micro-expression Recognition: A Survey. (arXiv:2107.02823v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02823","description":"<p>Micro-expressions (MEs) are involuntary facial movements revealing people's\nhidden feelings in high-stake situations and have practical importance in\nmedical treatment, national security, interrogations and many human-computer\ninteraction systems. Early methods for MER mainly based on traditional\nappearance and geometry features. Recently, with the success of deep learning\n(DL) in various fields, neural networks have received increasing interests in\nMER. Different from macro-expressions, MEs are spontaneous, subtle, and rapid\nfacial movements, leading to difficult data collection, thus have small-scale\ndatasets. DL based MER becomes challenging due to above ME characters. To date,\nvarious DL approaches have been proposed to solve the ME issues and improve MER\nperformance. In this survey, we provide a comprehensive review of deep\nmicro-expression recognition (MER), including datasets, deep MER pipeline, and\nthe bench-marking of most influential methods. This survey defines a new\ntaxonomy for the field, encompassing all aspects of MER based on DL. For each\naspect, the basic approaches and advanced developments are summarized and\ndiscussed. In addition, we conclude the remaining challenges and and potential\ndirections for the design of robust deep MER systems. To the best of our\nknowledge, this is the first survey of deep MER methods, and this survey can\nserve as a reference point for future MER research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yante Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jinsheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Transfer and Interference in Multi-Domain Learning. (arXiv:2107.05445v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05445","description":"<p>Humans are incredibly good at transferring knowledge from one domain to\nanother, enabling rapid learning of new tasks. Likewise, transfer learning has\nenabled enormous success in many computer vision problems using pretraining.\nHowever, the benefits of transfer in multi-domain learning, where a network\nlearns multiple tasks defined by different datasets, has not been adequately\nstudied. Learning multiple domains could be beneficial or these domains could\ninterfere with each other given limited network capacity. In this work, we\ndecipher the conditions where interference and knowledge transfer occur in\nmulti-domain learning. We propose new metrics disentangling interference and\ntransfer and set up experimental protocols. We further examine the roles of\nnetwork capacity, task grouping, and dynamic loss weighting in reducing\ninterference and facilitating transfer. We demonstrate our findings on the\nCIFAR-100, MiniPlaces, and Tiny-ImageNet datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1\">Tyler L. Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Coarse-to-Fine Approach in Single Image Deblurring. (arXiv:2108.05054v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05054","description":"<p>Coarse-to-fine strategies have been extensively used for the architecture\ndesign of single image deblurring networks. Conventional methods typically\nstack sub-networks with multi-scale input images and gradually improve\nsharpness of images from the bottom sub-network to the top sub-network,\nyielding inevitably high computational costs. Toward a fast and accurate\ndeblurring network design, we revisit the coarse-to-fine strategy and present a\nmulti-input multi-output U-net (MIMO-UNet). The MIMO-UNet has three distinct\nfeatures. First, the single encoder of the MIMO-UNet takes multi-scale input\nimages to ease the difficulty of training. Second, the single decoder of the\nMIMO-UNet outputs multiple deblurred images with different scales to mimic\nmulti-cascaded U-nets using a single U-shaped network. Last, asymmetric feature\nfusion is introduced to merge multi-scale features in an efficient manner.\nExtensive experiments on the GoPro and RealBlur datasets demonstrate that the\nproposed network outperforms the state-of-the-art methods in terms of both\naccuracy and computational complexity. Source code is available for research\npurposes at https://github.com/chosj95/MIMO-UNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sung-Jin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Seo-Won Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jun-Pyo Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Seung-Won Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1\">Sung-Jea Ko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voxel-wise Cross-Volume Representation Learning for 3D Neuron Reconstruction. (arXiv:2108.06522v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.06522","description":"<p>Automatic 3D neuron reconstruction is critical for analysing the morphology\nand functionality of neurons in brain circuit activities. However, the\nperformance of existing tracing algorithms is hinged by the low image quality.\nRecently, a series of deep learning based segmentation methods have been\nproposed to improve the quality of raw 3D optical image stacks by removing\nnoises and restoring neuronal structures from low-contrast background. Due to\nthe variety of neuron morphology and the lack of large neuron datasets, most of\ncurrent neuron segmentation models rely on introducing complex and\nspecially-designed submodules to a base architecture with the aim of encoding\nbetter feature representations. Though successful, extra burden would be put on\ncomputation during inference. Therefore, rather than modifying the base\nnetwork, we shift our focus to the dataset itself. The encoder-decoder backbone\nused in most neuron segmentation models attends only intra-volume voxel points\nto learn structural features of neurons but neglect the shared intrinsic\nsemantic features of voxels belonging to the same category among different\nvolumes, which is also important for expressive representation learning. Hence,\nto better utilise the scarce dataset, we propose to explicitly exploit such\nintrinsic features of voxels through a novel voxel-level cross-volume\nrepresentation learning paradigm on the basis of an encoder-decoder\nsegmentation model. Our method introduces no extra cost during inference.\nEvaluated on 42 3D neuron images from BigNeuron project, our proposed method is\ndemonstrated to improve the learning ability of the original segmentation model\nand further enhancing the reconstruction performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Heng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jianhui Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Siqi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chrzanowski_W/0/1/0/all/0/1\">Wojciech Chrzanowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Tutorial on Learning Disentangled Representations in the Imaging Domain. (arXiv:2108.12043v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12043","description":"<p>Disentangled representation learning has been proposed as an approach to\nlearning general representations. This can be done in the absence of, or with\nlimited, annotations. A good general representation can be readily fine-tuned\nfor new target tasks using modest amounts of data, or even be used directly in\nunseen domains achieving remarkable performance in the corresponding task. This\nalleviation of the data and annotation requirements offers tantalising\nprospects for tractable and affordable applications in computer vision and\nhealthcare. Finally, disentangled representations can offer model\nexplainability and can help us understand the underlying causal relations of\nthe factors of variation, increasing their suitability for real-world\ndeployment. In this tutorial paper, we will offer an overview of the\ndisentangled representation learning, its building blocks and criteria, and\ndiscuss applications in computer vision and medical imaging. We conclude our\ntutorial by presenting the identified opportunities for the integration of\nrecent machine learning advances into disentanglement, as well as the remaining\nchallenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1\">Pedro Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thermos_S/0/1/0/all/0/1\">Spyridon Thermos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1\">Alison Q. O&#x27;Neil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point Cloud Pre-training by Mixing and Disentangling. (arXiv:2109.00452v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00452","description":"<p>The annotation for large-scale point clouds is still time-consuming and\nunavailable for many real-world tasks. Point cloud pre-training is one\npotential solution for obtaining a scalable model for fast adaptation.\nTherefore, in this paper, we investigate a new self-supervised learning\napproach, called Mixing and Disentangling (MD), for point cloud pre-training.\nAs the name implies, we explore how to separate the original point cloud from\nthe mixed point cloud, and leverage this challenging task as a pretext\noptimization objective for model training. Considering the limited training\ndata in the original dataset, which is much less than prevailing ImageNet, the\nmixing process can efficiently generate more high-quality samples. We build one\nbaseline network to verify our intuition, which simply contains two modules,\nencoder and decoder. Given a mixed point cloud, the encoder is first\npre-trained to extract the semantic embedding. Then an instance-adaptive\ndecoder is harnessed to disentangle the point clouds according to the\nembedding. Albeit simple, the encoder is inherently able to capture the point\ncloud keypoints after training and can be fast adapted to downstream tasks\nincluding classification and segmentation by the pre-training and fine-tuning\nparadigm. Extensive experiments on two datasets show that the encoder + ours\n(MD) significantly surpasses that of the encoder trained from scratch and\nconverges quickly. In ablation studies, we further study the effect of each\ncomponent and discuss the advantages of the proposed self-supervised learning\nstrategy. We hope this self-supervised learning attempt on point clouds can\npave the way for reducing the deeply-learned model dependence on large-scale\nlabeled data and saving a lot of annotation costs in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhedong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ErfAct: Non-monotonic smooth trainable Activation Functions. (arXiv:2109.04386v2 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2109.04386","description":"<p>An activation function is a crucial component of a neural network that\nintroduces non-linearity in the network. The state-of-the-art performance of a\nneural network depends on the perfect choice of an activation function. We\npropose two novel non-monotonic smooth trainable activation functions, called\nErfAct-1 and ErfAct-2. Experiments suggest that the proposed functions improve\nthe network performance significantly compared to the widely used activations\nlike ReLU, Swish, and Mish. Replacing ReLU by ErfAct-1 and ErfAct-2, we have\n5.21% and 5.04% improvement for top-1 accuracy on PreactResNet-34 network in\nCIFAR100 dataset, 2.58% and 2.76% improvement for top-1 accuracy on\nPreactResNet-34 network in CIFAR10 dataset, 1.0%, and 1.0% improvement on mean\naverage precision (mAP) on SSD300 model in Pascal VOC dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_K/0/1/0/all/0/1\">Koushik Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sandeep Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Shilpak Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Ashish Kumar Pandey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiresolution Deep Implicit Functions for 3D Shape Representation. (arXiv:2109.05591v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05591","description":"<p>We introduce Multiresolution Deep Implicit Functions (MDIF), a hierarchical\nrepresentation that can recover fine geometry detail, while being able to\nperform global operations such as shape completion. Our model represents a\ncomplex 3D shape with a hierarchy of latent grids, which can be decoded into\ndifferent levels of detail and also achieve better accuracy. For shape\ncompletion, we propose latent grid dropout to simulate partial data in the\nlatent space and therefore defer the completing functionality to the decoder\nside. This along with our multires design significantly improves the shape\ncompletion quality under decoder-only latent optimization. To the best of our\nknowledge, MDIF is the first deep implicit function model that can at the same\ntime (1) represent different levels of detail and allow progressive decoding;\n(2) support both encoder-decoder inference and decoder-only latent\noptimization, and fulfill multiple applications; (3) perform detailed\ndecoder-only shape completion. Experiments demonstrate its superior performance\nagainst prior art in various 3D reconstruction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinda Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genova_K/0/1/0/all/0/1\">Kyle Genova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fanello_S/0/1/0/all/0/1\">Sean Fanello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouaziz_S/0/1/0/all/0/1\">Sofien Bouaziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haene_C/0/1/0/all/0/1\">Christian Haene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1\">Ruofei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keskin_C/0/1/0/all/0/1\">Cem Keskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1\">Thomas Funkhouser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Danhang Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PnP-DETR: Towards Efficient Visual Analysis with Transformers. (arXiv:2109.07036v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07036","description":"<p>Recently, DETR pioneered the solution of vision tasks with transformers, it\ndirectly translates the image feature map into the object detection result.\nThough effective, translating the full feature map can be costly due to\nredundant computation on some area like the background. In this work, we\nencapsulate the idea of reducing spatial redundancy into a novel poll and pool\n(PnP) sampling module, with which we build an end-to-end PnP-DETR architecture\nthat adaptively allocates its computation spatially to be more efficient.\nConcretely, the PnP module abstracts the image feature map into fine foreground\nobject feature vectors and a small number of coarse background contextual\nfeature vectors. The transformer models information interaction within the\nfine-coarse feature space and translates the features into the detection\nresult. Moreover, the PnP-augmented model can instantly achieve various desired\ntrade-offs between performance and computation with a single model by varying\nthe sampled feature length, without requiring to train multiple models as\nexisting methods. Thus it offers greater flexibility for deployment in diverse\nscenarios with varying computation constraint. We further validate the\ngeneralizability of the PnP module on panoptic segmentation and the recent\ntransformer-based image recognition model ViT and show consistent efficiency\ngain. We believe our method makes a step for efficient visual analysis with\ntransformers, wherein spatial redundancy is commonly observed. Code will be\navailable at \\url{https://github.com/twangnh/pnp-detr}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AGRNet: Adaptive Graph Representation Learning and Reasoning for Face Parsing. (arXiv:2101.07034v2 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2101.07034","description":"<p>Face parsing infers a pixel-wise label to each facial component, which has\ndrawn much attention recently.Previous methods have shown their success in face\nparsing, which however overlook the correlation among facial components.As a\nmatter of fact, the component-wise relationship is a critical clue in\ndiscriminating ambiguous pixels in facial area.To address this issue, we\npropose adaptive graph representation learning and reasoning over facial\ncomponents, aiming to learn representative vertices that describe each\ncomponent, exploit the component-wise relationship and thereby produce accurate\nparsing results against ambiguity. In particular, we devise an adaptive and\ndifferentiable graph abstraction method to represent the components on a graph\nvia pixel-to-vertex projection under the initial condition of a predicted\nparsing map, where pixel features within a certain facial region are aggregated\nonto a vertex. Further, we explicitly incorporate the image edge as a prior in\nthe model, which helps to discriminate edge and non-edge pixels during the\nprojection, thus leading to refined parsing results along the edges.Then, our\nmodel learns and reasons over the relations among components by propagating\ninformation across vertices on the graph. Finally, the refined vertex features\nare projected back to pixel grids for the prediction of the final parsing\nmap.To train our model, we propose a discriminative loss to penalize small\ndistances between vertices in the feature space, which leads to distinct\nvertices with strong semantics. Experimental results show the superior\nperformance of the proposed model on multiple face parsing datasets, along with\nthe validation on the human parsing task to demonstrate the generalizability of\nour model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Te_G/0/1/0/all/0/1\">Gusi Te</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinglu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hailin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-16T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/"}}]}]}