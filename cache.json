{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-07T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Combining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Translation. (arXiv:2204.02470v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02470","description":"<p>Self-Supervised Learning (SSL) models have been successfully applied in\nvarious deep learning-based speech tasks, particularly those with a limited\namount of data. However, the quality of SSL representations depends highly on\nthe relatedness between the SSL training domain(s) and the target data domain.\nOn the contrary, spectral feature (SF) extractors such as log Mel-filterbanks\nare hand-crafted non-learnable components, and could be more robust to domain\nshifts. The present work examines the assumption that combining non-learnable\nSF extractors to SSL models is an effective approach to low resource speech\ntasks. We propose a learnable and interpretable framework to combine SF and SSL\nrepresentations. The proposed framework outperforms significantly both baseline\nand SSL models on Automatic Speech Recognition (ASR) and Speech Translation\n(ST) tasks on three low resource datasets. We additionally design a mixture of\nexperts based combination model. This last model reveals that the relative\ncontribution of SSL models over conventional SF extractors is very small in\ncase of domain mismatch between SSL training set and the target language data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berrebbi_D/0/1/0/all/0/1\">Dan Berrebbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Francisco_O/0/1/0/all/0/1\">Osbel Lopez-Francisco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amith_J/0/1/0/all/0/1\">Jonathan D. Amith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Considerations for Multilingual Wikipedia Research. (arXiv:2204.02483v1 [cs.CY])","link":"http://arxiv.org/abs/2204.02483","description":"<p>English Wikipedia has long been an important data source for much research\nand natural language machine learning modeling. The growth of non-English\nlanguage editions of Wikipedia, greater computational resources, and calls for\nequity in the performance of language and multimodal models have led to the\ninclusion of many more language editions of Wikipedia in datasets and models.\nBuilding better multilingual and multimodal models requires more than just\naccess to expanded datasets; it also requires a better understanding of what is\nin the data and how this content was generated. This paper seeks to provide\nsome background to help researchers think about what differences might arise\nbetween different language editions of Wikipedia and how that might affect\ntheir models. It details three major ways in which content differences between\nlanguage editions arise (local context, community and governance, and\ntechnology) and recommendations for good practices when using multilingual and\nmultimodal data for research and modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Johnson_I/0/1/0/all/0/1\">Isaac Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lescak_E/0/1/0/all/0/1\">Emily Lescak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards End-to-end Unsupervised Speech Recognition. (arXiv:2204.02492v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02492","description":"<p>Unsupervised speech recognition has shown great potential to make Automatic\nSpeech Recognition (ASR) systems accessible to every language. However,\nexisting methods still heavily rely on hand-crafted pre-processing. Similar to\nthe trend of making supervised speech recognition end-to-end, we introduce\n\\wvu~which does away with all audio-side pre-processing and improves accuracy\nthrough better architecture. In addition, we introduce an auxiliary\nself-supervised objective that ties model predictions back to the input.\nExperiments show that \\wvu~improves unsupervised recognition results across\ndifferent languages while being conceptually simpler.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alexander H. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inferring Rewards from Language in Context. (arXiv:2204.02515v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02515","description":"<p>In classic instruction following, language like \"I'd like the JetBlue flight\"\nmaps to actions (e.g., selecting that flight). However, language also conveys\ninformation about a user's underlying reward function (e.g., a general\npreference for JetBlue), which can allow a model to carry out desirable actions\nin new contexts. We present a model that infers rewards from language\npragmatically: reasoning about how speakers choose utterances not only to\nelicit desired actions, but also to reveal information about their preferences.\nOn a new interactive flight-booking task with natural language, our model more\naccurately infers rewards and predicts optimal actions in unseen environments,\nin comparison to past work that first maps language to actions (instruction\nfollowing) and then maps actions to rewards (inverse reinforcement learning).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jessy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1\">Anca Dragan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple and Effective Unsupervised Speech Synthesis. (arXiv:2204.02524v1 [cs.SD])","link":"http://arxiv.org/abs/2204.02524","description":"<p>We introduce the first unsupervised speech synthesis system based on a\nsimple, yet effective recipe. The framework leverages recent work in\nunsupervised speech recognition as well as existing neural-based speech\nsynthesis. Using only unlabeled speech audio and unlabeled text as well as a\nlexicon, our method enables speech synthesis without the need for a\nhuman-labeled corpus. Experiments demonstrate the unsupervised system can\nsynthesize speech similar to a supervised counterpart in terms of naturalness\nand intelligibility measured by human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alexander H. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Cheng-I Jeff Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevskiv_A/0/1/0/all/0/1\">Alexei Baevskiv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prosodic Alignment for off-screen automatic dubbing. (arXiv:2204.02530v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02530","description":"<p>The goal of automatic dubbing is to perform speech-to-speech translation\nwhile achieving audiovisual coherence. This entails isochrony, i.e.,\ntranslating the original speech by also matching its prosodic structure into\nphrases and pauses, especially when the speaker's mouth is visible. In previous\nwork, we introduced a prosodic alignment model to address isochrone or\non-screen dubbing. In this work, we extend the prosodic alignment model to also\naddress off-screen dubbing that requires less stringent synchronization\nconstraints. We conduct experiments on four dubbing directions - English to\nFrench, Italian, German and Spanish - on a publicly available collection of TED\nTalks and on publicly available YouTube videos. Empirical results show that\ncompared to our previous work the extended prosodic alignment model provides\nsignificantly better subjective viewing experience on videos in which on-screen\nand off-screen automatic dubbing is applied for sentences with speakers mouth\nvisible and not visible, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Virkar_Y/0/1/0/all/0/1\">Yogesh Virkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federico_M/0/1/0/all/0/1\">Marcello Federico</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Enyedi_R/0/1/0/all/0/1\">Robert Enyedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barra_Chicote_R/0/1/0/all/0/1\">Roberto Barra-Chicote</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Zero-Shot Event Extraction via Sentence Simplification. (arXiv:2204.02531v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02531","description":"<p>The success of sites such as ACLED and Our World in Data have demonstrated\nthe massive utility of extracting events in structured formats from large\nvolumes of textual data in the form of news, social media, blogs and discussion\nforums. Event extraction can provide a window into ongoing geopolitical crises\nand yield actionable intelligence. With the proliferation of large pretrained\nlanguage models, Machine Reading Comprehension (MRC) has emerged as a new\nparadigm for event extraction in recent times. In this approach, event argument\nextraction is framed as an extractive question-answering task. One of the key\nadvantages of the MRC-based approach is its ability to perform zero-shot\nextraction. However, the problem of long-range dependencies, i.e., large\nlexical distance between trigger and argument words and the difficulty of\nprocessing syntactically complex sentences plague MRC-based approaches. In this\npaper, we present a general approach to improve the performance of MRC-based\nevent extraction by performing unsupervised sentence simplification guided by\nthe MRC model itself. We evaluate our approach on the ICEWS geopolitical event\nextraction dataset, with specific attention to `Actor' and `Target' argument\nroles. We show how such context simplification can improve the performance of\nMRC-based event extraction by more than 5% for actor extraction and more than\n10% for target extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sneha Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangwala_H/0/1/0/all/0/1\">Huzefa Rangwala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_N/0/1/0/all/0/1\">Naren Ramakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quick Starting Dialog Systems with Paraphrase Generation. (arXiv:2204.02546v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02546","description":"<p>Acquiring training data to improve the robustness of dialog systems can be a\npainstakingly long process. In this work, we propose a method to reduce the\ncost and effort of creating new conversational agents by artificially\ngenerating more data from existing examples, using paraphrase generation. Our\nproposed approach can kick-start a dialog system with little human effort, and\nbrings its performance to a level satisfactory enough for allowing actual\ninteractions with real end-users. We experimented with two neural paraphrasing\napproaches, namely Neural Machine Translation and a Transformer-based seq2seq\nmodel. We present the results obtained with two datasets in English and in\nFrench:~a crowd-sourced public intent classification dataset and our own\ncorporate dialog system dataset. We show that our proposed approach increased\nthe generalization capabilities of the intent classification model on both\ndatasets, reducing the effort required to initialize a new dialog system and\nhelping to deploy this technology at scale within an organization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marceau_L/0/1/0/all/0/1\">Louis Marceau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belbahar_R/0/1/0/all/0/1\">Raouf Belbahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Queudot_M/0/1/0/all/0/1\">Marc Queudot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charton_E/0/1/0/all/0/1\">Eric Charton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meurs_M/0/1/0/all/0/1\">Marie-Jean Meurs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Motion with Multi-Modal Features for Text-Based Video Segmentation. (arXiv:2204.02547v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02547","description":"<p>Text-based video segmentation aims to segment the target object in a video\nbased on a describing sentence. Incorporating motion information from optical\nflow maps with appearance and linguistic modalities is crucial yet has been\nlargely ignored by previous work. In this paper, we design a method to fuse and\nalign appearance, motion, and linguistic features to achieve accurate\nsegmentation. Specifically, we propose a multi-modal video transformer, which\ncan fuse and aggregate multi-modal and temporal features between frames.\nFurthermore, we design a language-guided feature fusion module to progressively\nfuse appearance and motion features in each feature level with guidance from\nlinguistic features. Finally, a multi-modal alignment loss is proposed to\nalleviate the semantic gap between features from different modalities.\nExtensive experiments on A2D Sentences and J-HMDB Sentences verify the\nperformance and the generalization ability of our method compared to the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wangbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C3KG: A Chinese Commonsense Conversation Knowledge Graph. (arXiv:2204.02549v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02549","description":"<p>Existing commonsense knowledge bases often organize tuples in an isolated\nmanner, which is deficient for commonsense conversational models to plan the\nnext steps. To fill the gap, we curate a large-scale multi-turn human-written\nconversation corpus, and create the first Chinese commonsense conversation\nknowledge graph which incorporates both social commonsense knowledge and dialog\nflow information. To show the potential of our graph, we develop a\ngraph-conversation matching approach, and benchmark two graph-grounded\nconversational tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dawei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiayi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jianwei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Temporal-Modal Entity Graph for Procedural Multimodal Machine Comprehension. (arXiv:2204.02566v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02566","description":"<p>Procedural Multimodal Documents (PMDs) organize textual instructions and\ncorresponding images step by step. Comprehending PMDs and inducing their\nrepresentations for the downstream reasoning tasks is designated as Procedural\nMultiModal Machine Comprehension (M3C). In this study, we approach Procedural\nM3C at a fine-grained level (compared with existing explorations at a document\nor sentence level), that is, entity. With delicate consideration, we model\nentity both in its temporal and cross-modal relation and propose a novel\nTemporal-Modal Entity Graph (TMEG). Specifically, graph structure is formulated\nto capture textual and visual entities and trace their temporal-modal\nevolution. In addition, a graph aggregation module is introduced to conduct\ngraph encoding and reasoning. Comprehensive experiments across three Procedural\nM3C tasks are conducted on a traditional dataset RecipeQA and our new dataset\nCraftQA, which can better evaluate the generalization of TMEG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huibin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengkun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+jiang_N/0/1/0/all/0/1\">Ning jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+wei_X/0/1/0/all/0/1\">Xin wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenglu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Structured Pruning on Multilingual Pre-trained Models: Settings, Algorithms, and Efficiency. (arXiv:2204.02601v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02601","description":"<p>Structured pruning has been extensively studied on monolingual pre-trained\nlanguage models and is yet to be fully evaluated on their multilingual\ncounterparts. This work investigates three aspects of structured pruning on\nmultilingual pre-trained language models: settings, algorithms, and efficiency.\nExperiments on nine downstream tasks show several counter-intuitive phenomena:\nfor settings, individually pruning for each language does not induce a better\nresult; for algorithms, the simplest method performs the best; for efficiency,\na fast model does not imply that it is also small. To facilitate the comparison\non all sparsity levels, we present Dynamic Sparsification, a simple approach\nthat allows training the model once and adapting to different model sizes at\ninference. We hope this work fills the gap in the study of structured pruning\non multilingual pre-trained models and sheds light on future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Fuli Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distributed Transition Systems with Tags for Privacy Analysis. (arXiv:2204.02602v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02602","description":"<p>We present a logical framework that formally models how a given private\ninformation P stored on a given database D, can get captured progressively, by\nan agent/adversary querying the database repeatedly.Named DLTTS (Distributed\nLabeled Tagged Transition System), the frame-work borrows ideas from several\ndomains: Probabilistic Automata of Segala, Probabilistic Concurrent Systems,\nand Probabilistic labelled transition systems. To every node on a DLTTS is\nattached a tag that represents the 'current' knowledge of the adversary,\nacquired from the responses of the answering mechanism of the DBMS to his/her\nqueries, at the nodes traversed earlier, along any given run; this knowledge is\ncompleted at the same node, with further relational deductions, possibly in\ncombination with 'public' information from other databases given in advance. A\n'blackbox' mechanism is also part of a DLTTS, and it is meant as an oracle; its\nrole is to tell if the private information has been deduced by the adversary at\nthe current node, and if so terminate the run. An additional special feature is\nthat the blackbox also gives information on how 'close',or how 'far', the\nknowledge of the adversary is, from the private information P , at the current\nnode. A metric is defined for that purpose, on the set of all 'type compatible'\ntuples from the given database, the data themselves being typed with the\nheaders of the base. Despite the transition systems flavor of our framework,\nthis metric is not 'behavioral' in the sense presented in some other works. It\nis exclusively database oriented,and allows to define new notions of adjacency\nand of -indistinguishabilty between databases, more generally than those\nusually based on the Hamming metric (and a restricted notion of adjacency).\nExamples are given all along to illustrate how our framework works.\nKeywords:Database, Privacy, Transition System, Probability, Distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anantharaman_S/0/1/0/all/0/1\">Siva Anantharaman</a> (LMV), <a href=\"http://arxiv.org/find/cs/1/au:+Frittella_S/0/1/0/all/0/1\">Sabine Frittella</a> (SDS), <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Benjamin Nguyen</a> (SDS)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"There Are a Thousand Hamlets in a Thousand People's Eyes: Enhancing Knowledge-grounded Dialogue with Personal Memory. (arXiv:2204.02624v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02624","description":"<p>Knowledge-grounded conversation (KGC) shows great potential in building an\nengaging and knowledgeable chatbot, and knowledge selection is a key ingredient\nin it. However, previous methods for knowledge selection only concentrate on\nthe relevance between knowledge and dialogue context, ignoring the fact that\nage, hobby, education and life experience of an interlocutor have a major\neffect on his or her personal preference over external knowledge. Without\ntaking the personalization issue into account, it is difficult to select the\nproper knowledge and generate persona-consistent responses. In this work, we\nintroduce personal memory into knowledge selection in KGC to address the\npersonalization issue. We propose a variational method to model the underlying\nrelationship between one's personal memory and his or her selection of\nknowledge, and devise a learning scheme in which the forward mapping from\npersonal memory to knowledge and its inverse mapping is included in a closed\nloop so that they could teach each other. Experiment results show that our\nmethod outperforms existing KGC methods significantly on both automatic\nevaluation and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1\">Tingchen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xueliang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Weakly Supervised Propagation Model for Rumor Verification and Stance Detection with Multiple Instance Learning. (arXiv:2204.02626v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02626","description":"<p>The diffusion of rumors on microblogs generally follows a propagation tree\nstructure, that provides valuable clues on how an original message is\ntransmitted and responded by users over time. Recent studies reveal that rumor\ndetection and stance detection are two different but relevant tasks which can\njointly enhance each other, e.g., rumors can be debunked by cross-checking the\nstances conveyed by their relevant microblog posts, and stances are also\nconditioned on the nature of the rumor. However, most stance detection methods\nrequire enormous post-level stance labels for training, which are\nlabor-intensive given a large number of posts. Enlightened by Multiple Instance\nLearning (MIL) scheme, we first represent the diffusion of claims with\nbottom-up and top-down trees, then propose two tree-structured weakly\nsupervised frameworks to jointly classify rumors and stances, where only the\nbag-level labels concerning claim's veracity are needed. Specifically, we\nconvert the multi-class problem into a multiple MIL-based binary classification\nproblem where each binary model focuses on differentiating a target stance or\nrumor type and other types. Finally, we propose a hierarchical attention\nmechanism to aggregate the binary predictions, including (1) a bottom-up or\ntop-down tree attention layer to aggregate binary stances into binary veracity;\nand (2) a discriminative attention layer to aggregate the binary class into\nfiner-grained classes. Extensive experiments conducted on three Twitter-based\ndatasets demonstrate promising performance of our model on both claim-level\nrumor detection and post-level stance classification compared with\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongzhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAGAM: Data Augmentation with Generation And Modification. (arXiv:2204.02633v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02633","description":"<p>Text classification is a representative downstream task of natural language\nprocessing, and has exhibited excellent performance since the advent of\npre-trained language models based on Transformer architecture. However, in\npre-trained language models, under-fitting often occurs due to the size of the\nmodel being very large compared to the amount of available training data. Along\nwith significant importance of data collection in modern machine learning\nparadigm, studies have been actively conducted for natural language data\naugmentation. In light of this, we introduce three data augmentation schemes\nthat help reduce underfitting problems of large-scale language models.\nPrimarily we use a generation model for data augmentation, which is defined as\nData Augmentation with Generation (DAG). Next, we augment data using text\nmodification techniques such as corruption and word order change (Data\nAugmentation with Modification, DAM). Finally, we propose Data Augmentation\nwith Generation And Modification (DAGAM), which combines DAG and DAM techniques\nfor a boosted performance. We conduct data augmentation for six benchmark\ndatasets of text classification task, and verify the usefulness of DAG, DAM,\nand DAGAM through BERT-based fine-tuning and evaluation, deriving better\nresults compared to the performance with original datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jo_B/0/1/0/all/0/1\">Byeong-Cheol Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_T/0/1/0/all/0/1\">Tak-Sung Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">Yeongjoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Yongmin Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_W/0/1/0/all/0/1\">Won Ik Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyungsun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Synthetic Data for Conversational Response Generation in Low-resource Settings. (arXiv:2204.02653v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02653","description":"<p>Response generation is a task in natural language processing (NLP) where a\nmodel is trained to respond to human statements. Conversational response\ngenerators take this one step further with the ability to respond within the\ncontext of previous responses. While there are existing techniques for training\nsuch models, they all require an abundance of conversational data which are not\nalways available for low-resource languages. In this research, we make three\ncontributions. First, we released the first Filipino conversational dataset\ncollected from a popular Philippine online forum, which we named the PEx\nConversations Dataset. Second, we introduce a data augmentation (DA)\nmethodology for Filipino data by employing a Tagalog RoBERTa model to increase\nthe size of the existing corpora. Lastly, we published the first Filipino\nconversational response generator capable of generating responses related to\nthe previous 3 responses. With the supplementary synthetic data, we were able\nto improve the performance of the response generator by up to 12.2% in\nBERTScore, 10.7% in perplexity, and 11.7% in content word usage as compared to\ntraining with zero synthetic data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_G/0/1/0/all/0/1\">Gabriel Louis Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ty_A/0/1/0/all/0/1\">Adrian Paule Ty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1\">Schuyler Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Co_D/0/1/0/all/0/1\">Denzel Adrian Co</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1\">Jan Christian Blaise Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Charibeth Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Yunshan Cup 2020: Overview of the Part-of-Speech Tagging Task for Low-resourced Languages. (arXiv:2204.02658v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02658","description":"<p>The Yunshan Cup 2020 track focused on creating a framework for evaluating\ndifferent methods of part-of-speech (POS). There were two tasks for this track:\n(1) POS tagging for the Indonesian language, and (2) POS tagging for the Lao\ntagging. The Indonesian dataset is comprised of 10000 sentences from Indonesian\nnews within 29 tags. And the Lao dataset consists of 8000 sentences within 27\ntags. 25 teams registered for the task. The methods of participants ranged from\nfeature-based to neural networks using either classical machine learning\ntechniques or ensemble methods. The best performing results achieve an accuracy\nof 95.82% for Indonesian and 93.03%, showing that neural sequence labeling\nmodels significantly outperform classic feature-based methods and rule-based\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingwen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xixuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xinying Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model for Text Analytic in Cybersecurity. (arXiv:2204.02685v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02685","description":"<p>NLP is a form of artificial intelligence and machine learning concerned with\na computer or machine's ability to understand and interpret human language.\nLanguage models are crucial in text analytics and NLP since they allow\ncomputers to interpret qualitative input and convert it to quantitative data\nthat they can use in other tasks. In essence, in the context of transfer\nlearning, language models are typically trained on a large generic corpus,\nreferred to as the pre-training stage, and then fine-tuned to a specific\nunderlying task. As a result, pre-trained language models are mostly used as a\nbaseline model that incorporates a broad grasp of the context and may be\nfurther customized to be used in a new NLP task.\n</p>\n<p>The majority of pre-trained models are trained on corpora from general\ndomains, such as Twitter, newswire, Wikipedia, and Web. Such off-the-shelf NLP\nmodels trained on general text may be inefficient and inaccurate in specialized\nfields. In this paper, we propose a cybersecurity language model called\nSecureBERT, which is able to capture the text connotations in the cybersecurity\ndomain, and therefore could further be used in automation for many important\ncybersecurity tasks that would otherwise rely on human expertise and tedious\nmanual efforts. SecureBERT is trained on a large corpus of cybersecurity text\ncollected and preprocessed by us from a variety of sources in cybersecurity and\nthe general computing domain. Using our proposed methods for tokenization and\nmodel weights adjustment, SecureBERT is not only able to preserve the\nunderstanding of general English as most pre-trained language models can do,\nbut also effective when applied to text that has cybersecurity implications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aghaei_E/0/1/0/all/0/1\">Ehsan Aghaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xi Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shadid_W/0/1/0/all/0/1\">Waseem Shadid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Shaer_E/0/1/0/all/0/1\">Ehab Al-Shaer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mix-and-Match: Scalable Dialog Response Retrieval using Gaussian Mixture Embeddings. (arXiv:2204.02710v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02710","description":"<p>Embedding-based approaches for dialog response retrieval embed the\ncontext-response pairs as points in the embedding space. These approaches are\nscalable, but fail to account for the complex, many-to-many relationships that\nexist between context-response pairs. On the other end of the spectrum, there\nare approaches that feed the context-response pairs jointly through multiple\nlayers of neural networks. These approaches can model the complex relationships\nbetween context-response pairs, but fail to scale when the set of responses is\nmoderately large (&gt;100). In this paper, we combine the best of both worlds by\nproposing a scalable model that can learn complex relationships between\ncontext-response pairs. Specifically, the model maps the contexts as well as\nresponses to probability distributions over the embedding space. We train the\nmodels by optimizing the Kullback-Leibler divergence between the distributions\ninduced by context-response pairs in the training data. We show that the\nresultant model achieves better performance as compared to other\nembedding-based approaches on publicly available conversation data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_G/0/1/0/all/0/1\">Gaurav Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Contractor_D/0/1/0/all/0/1\">Danish Contractor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sachindra Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Dataset for Topic-Based Paragraph Classification in Genocide-Related Court Transcripts. (arXiv:2204.02712v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02712","description":"<p>Recent progress in natural language processing has been impressive in many\ndifferent areas with transformer-based approaches setting new benchmarks for a\nwide range of applications. This development has also lowered the barriers for\npeople outside the NLP community to tap into the tools and resources applied to\na variety of domain-specific applications. The bottleneck however still remains\nthe lack of annotated gold-standard collections as soon as one's research or\nprofessional interest falls outside the scope of what is readily available. One\nsuch area is genocide-related research (also including the work of experts who\nhave a professional interest in accessing, exploring and searching large-scale\ndocument collections on the topic, such as lawyers). We present GTC (Genocide\nTranscript Corpus), the first annotated corpus of genocide-related court\ntranscripts which serves three purposes: (1) to provide a first reference\ncorpus for the community, (2) to establish benchmark performances (using\nstate-of-the-art transformer-based approaches) for the new classification task\nof paragraph identification of violence-related witness statements, (3) to\nexplore first steps towards transfer learning within the domain. We consider\nour contribution to be addressing in particular this year's hot topic on\nLanguage Technology for All.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schirmer_M/0/1/0/all/0/1\">Miriam Schirmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruschwitz_U/0/1/0/all/0/1\">Udo Kruschwitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donabauer_G/0/1/0/all/0/1\">Gregor Donabauer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotation-Scheme Reconstruction for \"Fake News\" and Japanese Fake News Dataset. (arXiv:2204.02718v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02718","description":"<p>Fake news provokes many societal problems; therefore, there has been\nextensive research on fake news detection tasks to counter it. Many fake news\ndatasets were constructed as resources to facilitate this task. Contemporary\nresearch focuses almost exclusively on the factuality aspect of the news.\nHowever, this aspect alone is insufficient to explain \"fake news,\" which is a\ncomplex phenomenon that involves a wide range of issues. To fully understand\nthe nature of each instance of fake news, it is important to observe it from\nvarious perspectives, such as the intention of the false news disseminator, the\nharmfulness of the news to our society, and the target of the news. We propose\na novel annotation scheme with fine-grained labeling based on detailed\ninvestigations of existing fake news datasets to capture these various aspects\nof fake news. Using the annotation scheme, we construct and publish the first\nJapanese fake news dataset. The annotation scheme is expected to provide an\nin-depth understanding of fake news. We plan to build datasets for both\nJapanese and other languages using our scheme. Our Japanese dataset is\npublished at https://hkefka385.github.io/dataset/fakenews-japanese/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murayama_T/0/1/0/all/0/1\">Taichi Murayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hisada_S/0/1/0/all/0/1\">Shohei Hisada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uehara_M/0/1/0/all/0/1\">Makoto Uehara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wakamiya_S/0/1/0/all/0/1\">Shoko Wakamiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aramaki_E/0/1/0/all/0/1\">Eiji Aramaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Multi-task Generalization Ability for Neural Text Matching via Prompt Learning. (arXiv:2204.02725v1 [cs.IR])","link":"http://arxiv.org/abs/2204.02725","description":"<p>Text matching is a fundamental technique in both information retrieval and\nnatural language processing. Text matching tasks share the same paradigm that\ndetermines the relationship between two given texts. Evidently, the\nrelationships vary from task to task, e.g. relevance in document retrieval,\nsemantic alignment in paraphrase identification and answerable judgment in\nquestion answering. However, the essential signals for text matching remain in\na finite scope, i.e. exact matching, semantic matching, and inference matching.\nRecent state-of-the-art neural text matching models, e.g. pre-trained language\nmodels (PLMs), are hard to generalize to different tasks. It is because the\nend-to-end supervised learning on task-specific dataset makes model\noveremphasize the data sample bias and task-specific signals instead of the\nessential matching signals, which ruins the generalization of model to\ndifferent tasks. To overcome this problem, we adopt a\nspecialization-generalization training strategy and refer to it as\nMatch-Prompt. In specialization stage, descriptions of different matching tasks\nare mapped to only a few prompt tokens. In generalization stage, text matching\nmodel explores the essential matching signals by being trained on diverse\nmultiple matching tasks. High diverse matching tasks avoid model fitting the\ndata sample bias on a specific task, so that model can focus on learning the\nessential matching signals. Meanwhile, the prompt tokens obtained in the first\nstep are added to the corresponding tasks to help the model distinguish\ndifferent task-specific matching signals. Experimental results on eighteen\npublic datasets show that Match-Prompt can significantly improve multi-task\ngeneralization capability of PLMs in text matching, and yield better in-domain\nmulti-task, out-of-domain multi-task and new task adaptation performance than\ntask-specific model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shicheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Readiness of Language Technology for Healthcare: What would it Take to Combat the Next Pandemic?. (arXiv:2204.02790v1 [cs.CY])","link":"http://arxiv.org/abs/2204.02790","description":"<p>The COVID-19 pandemic has brought out both the best and worst of language\ntechnology (LT). On one hand, conversational agents for information\ndissemination and basic diagnosis have seen widespread use, and arguably, had\nan important role in combating the pandemic. On the other hand, it has also\nbecome clear that such technologies are readily available for a handful of\nlanguages, and the vast majority of the global south is completely bereft of\nthese benefits. What is the state of LT, especially conversational agents, for\nhealthcare across the world's languages? And, what would it take to ensure\nglobal readiness of LT before the next pandemic? In this paper, we try to\nanswer these questions through survey of existing literature and resources, as\nwell as through a rapid chatbot building exercise for 15 Asian and African\nlanguages with varying amount of resource-availability. The study confirms the\npitiful state of LT even for languages with large speaker bases, such as\nSinhala and Hausa, and identifies the gaps that could help us prioritize\nresearch and investment strategies in LT for healthcare.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mondal_I/0/1/0/all/0/1\">Ishani Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1\">Kabir Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1\">Mohit Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neil_J/0/1/0/all/0/1\">Jacki O Neil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bali_K/0/1/0/all/0/1\">Kalika Bali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transformer-Based Contrastive Learning Approach for Few-Shot Sign Language Recognition. (arXiv:2204.02803v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02803","description":"<p>Sign language recognition from sequences of monocular images or 2D poses is a\nchallenging field, not only due to the difficulty to infer 3D information from\n2D data, but also due to the temporal relationship between the sequences of\ninformation. Additionally, the wide variety of signs and the constant need to\nadd new ones on production environments makes it infeasible to use traditional\nclassification techniques. We propose a novel Contrastive Transformer-based\nmodel, which demonstrate to learn rich representations from body key points\nsequences, allowing better comparison between vector embedding. This allows us\nto apply these techniques to perform one-shot or few-shot tasks, such as\nclassification and translation. The experiments showed that the model could\ngeneralize well and achieved competitive results for sign classes never seen in\nthe training process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_S/0/1/0/all/0/1\">Silvan Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_E/0/1/0/all/0/1\">Esdras Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahia_M/0/1/0/all/0/1\">M&#xe1;rcio Dahia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_J/0/1/0/all/0/1\">Jampierre Rocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Algebraic Approach to Learning and Grounding. (arXiv:2204.02813v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02813","description":"<p>We consider the problem of learning the semantics of composite algebraic\nexpressions from examples. The outcome is a versatile framework for studying\nlearning tasks that can be put into the following abstract form: The input is a\npartial algebra A and a finite set of samples ({\\phi}1, O1), ({\\phi}2, O2),\n..., each consisting of an algebraic term {\\phi}i and a set of objects Oi. The\nobjective is to simultaneously fill in the missing algebraic operations in A\nand ground the variables of every {\\phi}i in Oi, so that the combined value of\nthe terms is optimised. We demonstrate the applicability of this framework\nthrough case studies in grammatical inference, picture-language learning, and\nthe grounding of logic scene descriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bjorklund_J/0/1/0/all/0/1\">Johanna Bj&#xf6;rklund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindstrom_A/0/1/0/all/0/1\">Adam Dahlgren Lindstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drewes_F/0/1/0/all/0/1\">Frank Drewes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aggression in Hindi and English Speech: Acoustic Correlates and Automatic Identification. (arXiv:2204.02814v1 [cs.SD])","link":"http://arxiv.org/abs/2204.02814","description":"<p>In the present paper, we will present the results of an acoustic analysis of\npolitical discourse in Hindi and discuss some of the conventionalised acoustic\nfeatures of aggressive speech regularly employed by the speakers of Hindi and\nEnglish. The study is based on a corpus of slightly over 10 hours of political\ndiscourse and includes debates on news channel and political speeches. Using\nthis study, we develop two automatic classification systems for identifying\naggression in English and Hindi speech, based solely on an acoustic model. The\nHindi classifier, trained using 50 hours of annotated speech, and English\nclassifier, trained using 40 hours of annotated speech, achieve a respectable\naccuracy of over 73% and 66% respectively. In this paper, we discuss the\ndevelopment of this annotated dataset, the experiments for developing the\nclassifier and discuss the errors that it makes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Ritesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojha_A/0/1/0/all/0/1\">Atul Kr. Ojha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahiri_B/0/1/0/all/0/1\">Bornini Lahiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungleng_C/0/1/0/all/0/1\">Chingrimnng Lungleng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"drsphelps at SemEval-2022 Task 2: Learning idiom representations using BERTRAM. (arXiv:2204.02821v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02821","description":"<p>This paper describes our system for SemEval-2022 Task 2 Multilingual\nIdiomaticity Detection and Sentence Embedding sub-task B. We modify a standard\nBERT sentence transformer by adding embeddings for each idioms, which are\ncreated using BERTRAM and a small number of contexts. We show that this\ntechnique increases the quality of idiom representations and leads to better\nperformance on the task. We also perform analysis on our final results and show\nthat the quality of the produced idiom embeddings is highly sensitive to the\nquality of the input contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phelps_D/0/1/0/all/0/1\">Dylan Phelps</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Resources and Technologies for Non-Scheduled and Endangered Indian Languages. (arXiv:2204.02822v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02822","description":"<p>In the present paper, we will present a survey of the language resources and\ntechnologies available for the non-scheduled and endangered languages of India.\nWhile there have been different estimates from different sources about the\nnumber of languages in India, it could be assumed that there are more than\n1,000 languages currently being spoken in India. However barring some of the 22\nlanguages included in the 8th Schedule of the Indian Constitution (called the\nscheduled languages), there is hardly any substantial resource or technology\navailable for the rest of the languages. Nonetheless there have been some\nindividual attempts at developing resources and technologies for the different\nlanguages across the country. Of late, some financial support has also become\navailable for the endangered languages. In this paper, we give a summary of the\nresources and technologies for those Indian languages which are not included in\nthe 8th schedule of the Indian Constitution and/or which are endangered.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Ritesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahiri_B/0/1/0/all/0/1\">Bornini Lahiri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KNN-Diffusion: Image Generation via Large-Scale Retrieval. (arXiv:2204.02849v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02849","description":"<p>While the availability of massive Text-Image datasets is shown to be\nextremely useful in training large-scale generative models (e.g. DDPMs,\nTransformers), their output typically depends on the quality of both the input\ntext, as well as the training dataset. In this work, we show how large-scale\nretrieval methods, in particular efficient K-Nearest-Neighbors (KNN) search,\ncan be used in order to train a model to adapt to new samples. Learning to\nadapt enables several new capabilities. Sifting through billions of records at\ninference time is extremely efficient and can alleviate the need to train or\nmemorize an adequately large generative model. Additionally, fine-tuning\ntrained models to new samples can be achieved by simply adding them to the\ntable. Rare concepts, even without any presence in the training set, can be\nthen leveraged during test time without any modification to the generative\nmodel. Our diffusion-based model trains on images only, by leveraging a joint\nText-Image multi-modal metric. Compared to baseline methods, our generations\nachieve state of the art results both in human evaluations as well as with\nperceptual scores when tested on a public multimodal dataset of natural images,\nas well as on a collected dataset of 400 million Stickers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ashual_O/0/1/0/all/0/1\">Oron Ashual</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheynin_S/0/1/0/all/0/1\">Shelly Sheynin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singer_U/0/1/0/all/0/1\">Uriel Singer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gafni_O/0/1/0/all/0/1\">Oran Gafni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachmani_E/0/1/0/all/0/1\">Eliya Nachmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taigman_Y/0/1/0/all/0/1\">Yaniv Taigman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound. (arXiv:2204.02874v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02874","description":"<p>We introduce an audiovisual method for long-range text-to-video retrieval.\nUnlike previous approaches designed for short video retrieval (e.g., 5-15\nseconds in duration), our approach aims to retrieve minute-long videos that\ncapture complex human actions. One challenge of standard video-only approaches\nis the large computational cost associated with processing hundreds of densely\nextracted frames from such long videos. To address this issue, we propose to\nreplace parts of the video with compact audio cues that succinctly summarize\ndynamic audio events and are cheap to process. Our method, named ECLIPSE\n(Efficient CLIP with Sound Encoding), adapts the popular CLIP model to an\naudiovisual video setting, by adding a unified audiovisual transformer block\nthat captures complementary cues from the video and audio streams. In addition\nto being 2.92x faster and 2.34x memory-efficient than long-range video-only\napproaches, our method also achieves better text-to-video retrieval accuracy on\nseveral diverse long-range video datasets such as ActivityNet, QVHighlights,\nYouCook2, DiDeMo and Charades.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yan-Bo Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks. (arXiv:2204.02892v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02892","description":"<p>The field of Natural Language Processing (NLP) has experienced a dramatic\nleap in capabilities with the recent introduction of huge Language Models\n(LMs). Despite this success, natural language problems that involve several\ncompounded steps are still practically unlearnable, even by the largest LMs.\nThis complies with experimental failures for end-to-end learning of composite\nproblems that were demonstrated in a variety of domains. A known mitigation is\nto introduce intermediate supervision for solving sub-tasks of the compounded\nproblem. Recently, several works have demonstrated high gains by taking a\nstraightforward approach for incorporating intermediate supervision in\ncompounded natural language problems: the sequence-to-sequence LM is fed with\nan augmented input, in which the decomposed tasks' labels are simply\nconcatenated to the original input. In this paper, we prove a positive learning\nresult that motivates these recent efforts. We show that when concatenating\nintermediate supervision to the input and training a sequence-to-sequence model\non this modified input, an unlearnable composite problem becomes learnable. We\nprove this for the notoriously unlearnable composite task of bit-subset parity,\nwith the intermediate supervision being parity results of increasingly large\nbit-subsets. Beyond motivating contemporary empirical efforts for incorporating\nintermediate supervision in sequence-to-sequence language models, our positive\ntheoretical result is the first of its kind in the landscape of results on the\nbenefits of intermediate supervision: Until now, all theoretical results on the\nsubject are negative, i.e., show cases where learning is impossible without\nintermediate supervision, while our result is positive, showing a case where\nlearning is facilitated in the presence of intermediate supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wies_N/0/1/0/all/0/1\">Noam Wies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1\">Yoav Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EMMT: A simultaneous eye-tracking, 4-electrode EEG and audio corpus for multi-modal reading and translation scenarios. (arXiv:2204.02905v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02905","description":"<p>We present the Eyetracked Multi-Modal Translation (EMMT) corpus, a dataset\ncontaining monocular eye movement recordings, audio and 4-electrode\nelectroencephalogram (EEG) data of 43 participants. The objective was to\ncollect cognitive signals as responses of participants engaged in a number of\nlanguage intensive tasks involving different text-image stimuli settings when\ntranslating from English to Czech.\n</p>\n<p>Each participant was exposed to 32 text-image stimuli pairs and asked to (1)\nread the English sentence, (2) translate it into Czech, (3) consult the image,\n(4) translate again, either updating or repeating the previous translation. The\ntext stimuli consisted of 200 unique sentences with 616 unique words coupled\nwith 200 unique images as the visual stimuli.\n</p>\n<p>The recordings were collected over a two week period and all the participants\nincluded in the study were Czech natives with strong English skills. Due to the\nnature of the tasks involved in the study and the relatively large number of\nparticipants involved, the corpus is well suited for research in Translation\nProcess Studies, Cognitive Sciences among other disciplines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Sunit Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kloudova_V/0/1/0/all/0/1\">V&#x11b;ra Kloudov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zouhar_V/0/1/0/all/0/1\">Vil&#xe9;m Zouhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Base Index Compression via Dimensionality and Precision Reduction. (arXiv:2204.02906v1 [cs.IR])","link":"http://arxiv.org/abs/2204.02906","description":"<p>Recently neural network based approaches to knowledge-intensive NLP tasks,\nsuch as question answering, started to rely heavily on the combination of\nneural retrievers and readers. Retrieval is typically performed over a large\ntextual knowledge base (KB) which requires significant memory and compute\nresources, especially when scaled up. On HotpotQA we systematically investigate\nreducing the size of the KB index by means of dimensionality (sparse random\nprojections, PCA, autoencoders) and numerical precision reduction.\n</p>\n<p>Our results show that PCA is an easy solution that requires very little data\nand is only slightly worse than autoencoders, which are less stable. All\nmethods are sensitive to pre- and post-processing and data should always be\ncentered and normalized both before and after dimension reduction. Finally, we\nshow that it is possible to combine PCA with using 1bit per dimension. Overall\nwe achieve (1) 100$\\times$ compression with 75%, and (2) 24$\\times$ compression\nwith 92% original retrieval performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zouhar_V/0/1/0/all/0/1\">Vil&#xe9;m Zouhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosbach_M/0/1/0/all/0/1\">Marius Mosbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miaoran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask. (arXiv:2204.02908v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02908","description":"<p>Reading is integral to everyday life, and yet learning to read is a struggle\nfor many young learners. During lessons, teachers can use comprehension\nquestions to increase engagement, test reading skills, and improve retention.\nHistorically such questions were written by skilled teachers, but recently\nlanguage models have been used to generate comprehension questions. However,\nmany existing Question Generation (QG) systems focus on generating literal\nquestions from the text, and have no way to control the type of the generated\nquestion. In this paper, we study QG for reading comprehension where\ninferential questions are critical and extractive techniques cannot be used. We\npropose a two-step model (HTA-WTA) that takes advantage of previous datasets,\nand can generate questions for a specific targeted comprehension skill. We\npropose a new reading comprehension dataset that contains questions annotated\nwith story-based reading comprehension skills (SBRCS), allowing for a more\ncomplete reader assessment. Across several experiments, our results show that\nHTA-WTA outperforms multiple strong baselines on this new dataset. We show that\nthe HTA-WTA model tests for strong SCRS by asking deep inferential questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bilal Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coleman_L/0/1/0/all/0/1\">Lauren Lutz Coleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dexter_J/0/1/0/all/0/1\">Julia Rivard Dexter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohe_S/0/1/0/all/0/1\">Spencer McIntosh von der Ohe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fyshe_A/0/1/0/all/0/1\">Alona Fyshe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paying More Attention to Self-attention: Improving Pre-trained Language Models via Attention Guiding. (arXiv:2204.02922v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02922","description":"<p>Pre-trained language models (PLM) have demonstrated their effectiveness for a\nbroad range of information retrieval and natural language processing tasks. As\nthe core part of PLM, multi-head self-attention is appealing for its ability to\njointly attend to information from different positions. However, researchers\nhave found that PLM always exhibits fixed attention patterns regardless of the\ninput (e.g., excessively paying attention to [CLS] or [SEP]), which we argue\nmight neglect important information in the other positions. In this work, we\npropose a simple yet effective attention guiding mechanism to improve the\nperformance of PLM by encouraging attention towards the established goals.\nSpecifically, we propose two kinds of attention guiding methods, i.e., map\ndiscrimination guiding (MDG) and attention pattern decorrelation guiding (PDG).\nThe former definitely encourages the diversity among multiple self-attention\nheads to jointly attend to information from different representation subspaces,\nwhile the latter encourages self-attention to attend to as many different\npositions of the input as possible. We conduct experiments with multiple\ngeneral pre-trained models (i.e., BERT, ALBERT, and Roberta) and\ndomain-specific pre-trained models (i.e., BioBERT, ClinicalBERT, BlueBert, and\nSciBERT) on three benchmark datasets (i.e., MultiNLI, MedNLI, and\nCross-genre-IR). Extensive experimental results demonstrate that our proposed\nMDG and PDG bring stable performance improvements on all datasets with high\nefficiency and low cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Huasheng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inducing Positive Perspectives with Text Reframing. (arXiv:2204.02952v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02952","description":"<p>Sentiment transfer is one popular example of a text style transfer task,\nwhere the goal is to reverse the sentiment polarity of a text. With a sentiment\nreversal comes also a reversal in meaning. We introduce a different but related\ntask called positive reframing in which we neutralize a negative point of view\nand generate a more positive perspective for the author without contradicting\nthe original meaning. Our insistence on meaning preservation makes positive\nreframing a challenging and semantically rich task. To facilitate rapid\nprogress, we introduce a large-scale benchmark, Positive Psychology Frames,\nwith 8,349 sentence pairs and 12,755 structured annotations to explain positive\nreframing in terms of six theoretically-motivated reframing strategies. Then we\nevaluate a set of state-of-the-art text style transfer models, and conclude by\ndiscussing key challenges and directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ziems_C/0/1/0/all/0/1\">Caleb Ziems</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Anthony Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation. (arXiv:2204.02967v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02967","description":"<p>Direct speech-to-speech translation (S2ST) models suffer from data scarcity\nissues as there exists little parallel S2ST data, compared to the amount of\ndata available for conventional cascaded systems that consist of automatic\nspeech recognition (ASR), machine translation (MT), and text-to-speech (TTS)\nsynthesis. In this work, we explore self-supervised pre-training with unlabeled\nspeech data and data augmentation to tackle this issue. We take advantage of a\nrecently proposed speech-to-unit translation (S2UT) framework that encodes\ntarget speech into discrete representations, and transfer pre-training and\nefficient partial finetuning techniques that work well for speech-to-text\ntranslation (S2T) to the S2UT domain by studying both speech encoder and\ndiscrete unit decoder pre-training. Our experiments show that self-supervised\npre-training consistently improves model performance compared with multitask\nlearning with a BLEU gain of 4.3-12.0 under various data setups, and it can be\nfurther combined with data augmentation techniques that apply MT to create\nweakly supervised training data. Audio samples are available at:\nhttps://facebookresearch.github.io/speech_translation/enhanced_direct_s2st_units/index.html .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Popuri_S/0/1/0/all/0/1\">Sravya Popuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng-Jen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiatao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction. (arXiv:1911.09419v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1911.09419","description":"<p>Knowledge graph embedding, which aims to represent entities and relations as\nlow dimensional vectors (or matrices, tensors, etc.), has been shown to be a\npowerful technique for predicting missing links in knowledge graphs. Existing\nknowledge graph embedding models mainly focus on modeling relation patterns\nsuch as symmetry/antisymmetry, inversion, and composition. However, many\nexisting approaches fail to model semantic hierarchies, which are common in\nreal-world applications. To address this challenge, we propose a novel\nknowledge graph embedding model -- namely, Hierarchy-Aware Knowledge Graph\nEmbedding (HAKE) -- which maps entities into the polar coordinate system. HAKE\nis inspired by the fact that concentric circles in the polar coordinate system\ncan naturally reflect the hierarchy. Specifically, the radial coordinate aims\nto model entities at different levels of the hierarchy, and entities with\nsmaller radii are expected to be at higher levels; the angular coordinate aims\nto distinguish entities at the same level of the hierarchy, and these entities\nare expected to have roughly the same radii but different angles. Experiments\ndemonstrate that HAKE can effectively model the semantic hierarchies in\nknowledge graphs, and significantly outperforms existing state-of-the-art\nmethods on benchmark datasets for the link prediction task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhanqiu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ktrain: A Low-Code Library for Augmented Machine Learning. (arXiv:2004.10703v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2004.10703","description":"<p>We present ktrain, a low-code Python library that makes machine learning more\naccessible and easier to apply. As a wrapper to TensorFlow and many other\nlibraries (e.g., transformers, scikit-learn, stellargraph), it is designed to\nmake sophisticated, state-of-the-art machine learning models simple to build,\ntrain, inspect, and apply by both beginners and experienced practitioners.\nFeaturing modules that support text data (e.g., text classification, sequence\ntagging, open-domain question-answering), vision data (e.g., image\nclassification), graph data (e.g., node classification, link prediction), and\ntabular data, ktrain presents a simple unified interface enabling one to\nquickly solve a wide range of tasks in as little as three or four \"commands\" or\nlines of code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1\">Arun S. Maiya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Learning for Personalized Humor Recognition. (arXiv:2012.01675v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.01675","description":"<p>Computational understanding of humor is an important topic under creative\nlanguage understanding and modeling. It can play a key role in complex human-AI\ninteractions. The challenge here is that human perception of humorous content\nis highly subjective. The same joke may receive different funniness ratings\nfrom different readers. This makes it highly challenging for humor recognition\nmodels to achieve personalization in practical scenarios. Existing approaches\nare generally designed based on the assumption that users have a consensus on\nwhether a given text is humorous or not. Thus, they cannot handle diverse humor\npreferences well. In this paper, we propose the FedHumor approach for the\nrecognition of humorous content in a personalized manner through Federated\nLearning (FL). Extending a pre-trained language model, FedHumor guides the\nfine-tuning process by considering diverse distributions of humor preferences\nfrom individuals. It incorporates a diversity adaptation strategy into the FL\nparadigm to train a personalized humor recognition model. To the best of our\nknowledge, FedHumor is the first text-based personalized humor recognition\nmodel through federated learning. Extensive experiments demonstrate the\nadvantage of FedHumor in recognizing humorous texts compared to nine\nstate-of-the-art humor recognition approaches with superior capability for\nhandling the diversity in humor labels produced by users with diverse\npreferences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Han Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_P/0/1/0/all/0/1\">Pengwei Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Siwei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Z/0/1/0/all/0/1\">Zaiqing Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linguistic Characterization of Divisive Topics Online: Case Studies on Contentiousness in Abortion, Climate Change, and Gun Control. (arXiv:2108.13556v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13556","description":"<p>As public discourse continues to move and grow online, conversations about\ndivisive topics on social media platforms have also increased. These divisive\ntopics prompt both contentious and non-contentious conversations. Although what\ndistinguishes these conversations, often framed as what makes these\nconversations contentious, is known in broad strokes, much less is known about\nthe linguistic signature of these conversations. Prior work has shown that\ncontentious content and structure can be a predictor for this task, however,\nmost of them have been focused on conversation in general, very specific\nevents, or complex structural analysis. Additionally, many models used in prior\nwork have lacked interpret-ability, a key factor in online moderation. Our work\nfills these gaps by focusing on conversations from highly divisive topics\n(abortion, climate change, and gun control), operationalizing a set of novel\nlinguistic and conversational characteristics and user factors, and\nincorporating them to build interpretable models. We demonstrate that such\ncharacteristics can largely improve the performance of prediction on this task,\nand also enable nuanced interpretability. Our case studies on these three\ncontentious topics suggest that certain generic linguistic characteristics are\nhighly correlated with contentiousness in conversations while others\ndemonstrate significant contextual influences on specific divisive topics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beel_J/0/1/0/all/0/1\">Jacob Beel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soni_S/0/1/0/all/0/1\">Sandeep Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Grammar-Learning Trajectories of Neural Language Models. (arXiv:2109.06096v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06096","description":"<p>The learning trajectories of linguistic phenomena in humans provide insight\ninto linguistic representation, beyond what can be gleaned from inspecting the\nbehavior of an adult speaker. To apply a similar approach to analyze neural\nlanguage models (NLM), it is first necessary to establish that different models\nare similar enough in the generalizations they make. In this paper, we show\nthat NLMs with different initialization, architecture, and training data\nacquire linguistic phenomena in a similar order, despite their different end\nperformance. These findings suggest that there is some mutual inductive bias\nthat underlies these models' learning of linguistic phenomena. Taking\ninspiration from psycholinguistics, we argue that studying this inductive bias\nis an opportunity to study the linguistic representation implicit in NLMs.\n</p>\n<p>Leveraging these findings, we compare the relative performance on different\nphenomena at varying learning stages with simpler reference models. Results\nsuggest that NLMs exhibit consistent \"developmental\" stages. Moreover, we find\nthe learning trajectory to be approximately one-dimensional: given an NLM with\na certain overall performance, it is possible to predict what linguistic\ngeneralizations it has already acquired. Initial analysis of these stages\npresents phenomena clusters (notably morphological ones), whose performance\nprogresses in unison, suggesting a potential link between the generalizations\nbehind them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hacohen_G/0/1/0/all/0/1\">Guy Hacohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinshall_D/0/1/0/all/0/1\">Daphna Weinshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting TTS models For New Speakers using Transfer Learning. (arXiv:2110.05798v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2110.05798","description":"<p>Training neural text-to-speech (TTS) models for a new speaker typically\nrequires several hours of high quality speech data. Prior works on voice\ncloning attempt to address this challenge by adapting pre-trained multi-speaker\nTTS models for a new voice, using a few minutes of speech data of the new\nspeaker. However, publicly available large multi-speaker datasets are often\nnoisy, thereby resulting in TTS models that are not suitable for use in\nproducts. We address this challenge by proposing transfer-learning guidelines\nfor adapting high quality single-speaker TTS models for a new speaker, using\nonly a few minutes of speech data. We conduct an extensive study using\ndifferent amounts of data for a new speaker and evaluate the synthesized speech\nin terms of naturalness and voice/style similarity to the target speaker. We\nfind that fine-tuning a single-speaker TTS model on just 30 minutes of data,\ncan yield comparable performance to a model trained from scratch on more than\n27 hours of data for both male and female target speakers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neekhara_P/0/1/0/all/0/1\">Paarth Neekhara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jason Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding. (arXiv:2110.14170v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.14170","description":"<p>Knowledge graphs (KGs) which consist of a large number of triples have become\nwidespread recently, and many knowledge graph embedding (KGE) methods are\nproposed to embed entities and relations of a KG into continuous vector spaces.\nSuch embedding methods aim at simplifying the operations of conducting various\nin-KG tasks (e.g., link prediction) and out-of-KG tasks (e.g., question\nanswering), and can be viewed as general solutions for representing KGs.\nHowever, existing KGE methods are not applicable to inductive settings, where a\nmodel trained on source KGs will be tested on target KGs with entities unseen\nduring model training. Existing works focusing on KGs in inductive settings can\nonly solve the inductive relation prediction task and can not handle other\nout-of-KG tasks as general as KGE methods, since they don't produce embeddings\nfor entities. In this paper, to achieve inductive knowledge graph embedding, we\npropose a model MorsE, which doesn't learn embeddings for entities, while\nlearning transferable meta-knowledge that can be used to produce entity\nembeddings. Such meta-knowledge is modeled by entity-independent modules and\nlearned by meta-learning. Experimental results show that our model\nsignificantly outperforms corresponding baselines for in-KG and out-of-KG tasks\nin inductive settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yushan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hongting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zonggang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v11 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11133","description":"<p>Far beyond learning long-range interactions of natural language, transformers\nare becoming the de-facto standard for many vision tasks with their power and\nscalability. Especially with cross-modal tasks between image and text, vector\nquantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB\nimage into a sequence of feature vectors. To better leverage the correlation\nbetween image and text, we propose L-Verse, a novel architecture consisting of\nfeature-augmented variational autoencoder (AugVAE) and bidirectional\nauto-regressive transformer (BiART) for image-to-text and text-to-image\ngeneration. Our AugVAE shows the state-of-the-art reconstruction performance on\nImageNet1K validation set, along with the robustness to unseen images in the\nwild. Unlike other models, BiART can distinguish between image (or text) as a\nconditional reference and a generation target. L-Verse can be directly used for\nimage-to-text or text-to-image generation without any finetuning or extra\nobject detection framework. In quantitative and qualitative experiments,\nL-Verse shows impressive results against previous methods in both image-to-text\nand text-to-image generation on MS-COCO Captions. We furthermore assess the\nscalability of L-Verse architecture on Conceptual Captions and present the\ninitial result of bidirectional vision-language representation learning on\ngeneral domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Gwangmo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sihaeng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1\">Yewon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Soonyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seung Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_K/0/1/0/all/0/1\">Kyunghoon Bae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAVT: Language-Aware Vision Transformer for Referring Image Segmentation. (arXiv:2112.02244v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02244","description":"<p>Referring image segmentation is a fundamental vision-language task that aims\nto segment out an object referred to by a natural language expression from an\nimage. One of the key challenges behind this task is leveraging the referring\nexpression for highlighting relevant positions in the image. A paradigm for\ntackling this problem is to leverage a powerful vision-language (\"cross-modal\")\ndecoder to fuse features independently extracted from a vision encoder and a\nlanguage encoder. Recent methods have made remarkable advancements in this\nparadigm by exploiting Transformers as cross-modal decoders, concurrent to the\nTransformer's overwhelming success in many other vision-language tasks.\nAdopting a different approach in this work, we show that significantly better\ncross-modal alignments can be achieved through the early fusion of linguistic\nand visual features in intermediate layers of a vision Transformer encoder\nnetwork. By conducting cross-modal feature fusion in the visual feature\nencoding stage, we can leverage the well-proven correlation modeling power of a\nTransformer encoder for excavating helpful multi-modal context. This way,\naccurate segmentation results are readily harvested with a light-weight mask\npredictor. Without bells and whistles, our method surpasses the previous\nstate-of-the-art methods on RefCOCO, RefCOCO+, and G-Ref by large margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yansong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable Natural Language Processing with Matrix Product States. (arXiv:2112.08628v2 [cond-mat.dis-nn] UPDATED)","link":"http://arxiv.org/abs/2112.08628","description":"<p>Despite empirical successes of recurrent neural networks (RNNs) in natural\nlanguage processing (NLP), theoretical understanding of RNNs is still limited\ndue to intrinsically complex non-linear computations. We systematically analyze\nRNNs' behaviors in a ubiquitous NLP task, the sentiment analysis of movie\nreviews, via the mapping between a class of RNNs called recurrent arithmetic\ncircuits (RACs) and a matrix product state (MPS). Using the von-Neumann\nentanglement entropy (EE) as a proxy for information propagation, we show that\nsingle-layer RACs possess a maximum information propagation capacity, reflected\nby the saturation of the EE. Enlarging the bond dimension beyond the EE\nsaturation threshold does not increase model prediction accuracies, so a\nminimal model that best estimates the data statistics can be inferred. Although\nthe saturated EE is smaller than the maximum EE allowed by the area law, our\nminimal model still achieves ~99% training accuracies in realistic sentiment\nanalysis data sets. Thus, low EE is not a warrant against the adoption of\nsingle-layer RACs for NLP. Contrary to a common belief that long-range\ninformation propagation is the main source of RNNs' successes, we show that\nsingle-layer RACs harness high expressiveness from the subtle interplay between\nthe information propagation and the word vector embeddings. Our work sheds\nlight on the phenomenology of learning in RACs, and more generally on the\nexplainability of RNNs for NLP, using tools from many-body quantum physics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Tangpanitanon_J/0/1/0/all/0/1\">Jirawat Tangpanitanon</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Mangkang_C/0/1/0/all/0/1\">Chanatip Mangkang</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Bhadola_P/0/1/0/all/0/1\">Pradeep Bhadola</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Minato_Y/0/1/0/all/0/1\">Yuichiro Minato</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Angelakis_D/0/1/0/all/0/1\">Dimitris G. Angelakis</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Chotibut_T/0/1/0/all/0/1\">Thiparat Chotibut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal data matters: language model pre-training over structured and unstructured electronic health records. (arXiv:2201.10113v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.10113","description":"<p>The massive amount of electronic health records (EHRs) has created enormous\npotential for improving healthcare, among which clinical codes (structured\ndata) and clinical narratives (unstructured data) are two important textual\nmodalities. Most existing EHR-oriented studies, however, either only focus on a\nparticular modality or integrate data from different modalities in a shallow\nmanner, which ignores the intrinsic interactions between them. To address these\nissues, we proposed a Medical Multimodal Pre-trained Language Model, named\nMedM-PLM, to learn enhanced EHR representations over structured and\nunstructured data. In MedM-PLM, two Transformer-based neural networks\ncomponents are firstly adopted to learn representative characteristics from\neach modality. A cross-modal module is then introduced to model their\ninteractions. We pre-trained MedM-PLM on the MIMIC-III dataset and verified the\neffectiveness of the model on three downstream clinical tasks, i.e., medication\nrecommendation, 30-day readmission, and ICD coding. Extensive experiments\ndemonstrate the power of MedM-PLM compared with state-of-the-art methods.\nFurther analyses and visualizations show the robustness of our model which\ncould potentially provide more comprehensive interpretations for clinical\ndecision-making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sicen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yongshuai Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Buzhou Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models. (arXiv:2201.11903v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11903","description":"<p>Although scaling up language model size has reliably improved performance on\na range of NLP tasks, even the largest models currently struggle with certain\nreasoning tasks such as math word problems, symbolic manipulation, and\ncommonsense reasoning. This paper explores the ability of language models to\ngenerate a coherent chain of thought -- a series of short sentences that mimic\nthe reasoning process a person might have when responding to a question.\nExperiments show that inducing a chain of thought via prompting can enable\nsufficiently large language models to better perform reasoning tasks that\notherwise have flat scaling curves. When combined with the 540B parameter PaLM\nmodel, chain of thought prompting achieves new state of the art of 58.1\\% on\nthe GSM8K benchmark of math word problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Legal Argument Mining with Domain Pre-training and Neural Networks. (arXiv:2202.13457v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13457","description":"<p>The contextual word embedding model, BERT, has proved its ability on\ndownstream tasks with limited quantities of annotated data. BERT and its\nvariants help to reduce the burden of complex annotation work in many\ninterdisciplinary research areas, for example, legal argument mining in digital\nhumanities. Argument mining aims to develop text analysis tools that can\nautomatically retrieve arguments and identify relationships between\nargumentation clauses. Since argumentation is one of the key aspects of case\nlaw, argument mining tools for legal texts are applicable to both academic and\nnon-academic legal research. Domain-specific BERT variants (pre-trained with\ncorpora from a particular background) have also achieved strong performance in\nmany tasks. To our knowledge, previous machine learning studies of argument\nmining on judicial case law still heavily rely on statistical models. In this\npaper, we provide a broad study of both classic and contextual embedding models\nand their performance on practical case law from the European Court of Human\nRights (ECHR). During our study, we also explore a number of neural networks\nwhen being combined with different embeddings. Our experiments provide a\ncomprehensive overview of a variety of approaches to the legal argument mining\ntask. We conclude that domain pre-trained transformer models have great\npotential in this area, although traditional embeddings can also achieve strong\nperformance when combined with additional neural network layers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gechuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nulty_P/0/1/0/all/0/1\">Paul Nulty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lillis_D/0/1/0/all/0/1\">David Lillis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERNIE-GeoL: A Geography-and-Language Pre-trained Model and its Applications in Baidu Maps. (arXiv:2203.09127v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09127","description":"<p>Pre-trained models (PTMs) have become a fundamental backbone for downstream\ntasks in natural language processing and computer vision. Despite initial gains\nthat were obtained by applying generic PTMs to geo-related tasks at Baidu Maps,\na clear performance plateau over time was observed. One of the main reasons for\nthis plateau is the lack of readily available geographic knowledge in generic\nPTMs. To address this problem, in this paper, we present ERNIE-GeoL, which is a\ngeography-and-language pre-trained model designed and developed for improving\nthe geo-related tasks at Baidu Maps. ERNIE-GeoL is elaborately designed to\nlearn a universal representation of geography-language by pre-training on\nlarge-scale data generated from a heterogeneous graph that contains abundant\ngeographic knowledge. Extensive quantitative and qualitative experiments\nconducted on large-scale real-world datasets demonstrate the superiority and\neffectiveness of ERNIE-GeoL. ERNIE-GeoL has already been deployed in production\nat Baidu Maps since April 2021, which significantly benefits the performance of\na wide range of downstream tasks. This demonstrates that ERNIE-GeoL can serve\nas a fundamental backbone for geo-related tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jizhou Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yibo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yunsheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhengjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_A/0/1/0/all/0/1\">An Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shikun Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation. (arXiv:2203.09435v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09435","description":"<p>The performance of multilingual pretrained models is highly dependent on the\navailability of monolingual or parallel text present in a target language.\nThus, the majority of the world's languages cannot benefit from recent progress\nin NLP as they have no or limited textual data. To expand possibilities of\nusing NLP technology in these under-represented languages, we systematically\nstudy strategies that relax the reliance on conventional language resources\nthrough the use of bilingual lexicons, an alternative resource with much better\nlanguage coverage. We analyze different strategies to synthesize textual or\nlabeled data using lexicons, and how this data can be combined with monolingual\nor parallel text when available. For 19 under-represented languages across 3\ntasks, our methods lead to consistent improvements of up to 5 and 15 points\nwith and without extra monolingual text respectively. Overall, our study\nhighlights how NLP methods can be adapted to thousands more languages that are\nunder-served by current technology\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models. (arXiv:2203.11171v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.11171","description":"<p>We explore a simple ensemble strategy, self-consistency, that significantly\nimproves the reasoning accuracy of large language models. The idea is to sample\na diverse set of reasoning paths from a language model via chain of thought\nprompting then return the most consistent final answer in the set. We evaluate\nself-consistency on a range of arithmetic and commonsense reasoning benchmarks,\nand find that it robustly improves accuracy across a variety of language models\nand model scales without the need for additional training or auxiliary models.\nWhen combined with a recent large language model, PaLM-540B, self-consistency\nincreases performance to state-of-the-art levels across several benchmark\nreasoning tasks, including GSM8K (56.5% -&gt; 74.4%), SVAMP (79.0% -&gt; 86.6%), AQuA\n(35.8% -&gt; 48.3%), StrategyQA (75.3% -&gt; 81.6%) and ARC-challenge (85.2% -&gt;\n88.7%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1\">Aakanksha Chowdhery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Expressive Speaking Style Modelling with Hierarchical Context Information for Mandarin Speech Synthesis. (arXiv:2203.12201v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.12201","description":"<p>Previous works on expressive speech synthesis mainly focus on current\nsentence. The context in adjacent sentences is neglected, resulting in\ninflexible speaking style for the same text, which lacks speech variations. In\nthis paper, we propose a hierarchical framework to model speaking style from\ncontext. A hierarchical context encoder is proposed to explore a wider range of\ncontextual information considering structural relationship in context,\nincluding inter-phrase and inter-sentence relations. Moreover, to encourage\nthis encoder to learn style representation better, we introduce a novel\ntraining strategy with knowledge distillation, which provides the target for\nencoder training. Both objective and subjective evaluations on a Mandarin\nlecture dataset demonstrate that the proposed method can significantly improve\nthe naturalness and expressiveness of the synthesized speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shun Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yixuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Shiyin Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Duality-Induced Regularizer for Semantic Matching Knowledge Graph Embeddings. (arXiv:2203.12949v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12949","description":"<p>Semantic matching models -- which assume that entities with similar semantics\nhave similar embeddings -- have shown great power in knowledge graph embeddings\n(KGE). Many existing semantic matching models use inner products in embedding\nspaces to measure the plausibility of triples and quadruples in static and\ntemporal knowledge graphs. However, vectors that have the same inner products\nwith another vector can still be orthogonal to each other, which implies that\nentities with similar semantics may have dissimilar embeddings. This property\nof inner products significantly limits the performance of semantic matching\nmodels. To address this challenge, we propose a novel regularizer -- namely,\nDUality-induced RegulArizer (DURA) -- which effectively encourages the entities\nwith similar semantics to have similar embeddings. The major novelty of DURA is\nbased on the observation that, for an existing semantic matching KGE model\n(primal), there is often another distance based KGE model (dual) closely\nassociated with it, which can be used as effective constraints for entity\nembeddings. Experiments demonstrate that DURA consistently and significantly\nimproves the performance of state-of-the-art semantic matching models on both\nstatic and temporal knowledge graph benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhanqiu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhihao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shuiwang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Baseline Readability Model for Cebuano. (arXiv:2203.17225v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.17225","description":"<p>In this study, we developed the first baseline readability model for the\nCebuano language. Cebuano is the second most-used native language in the\nPhilippines with about 27.5 million speakers. As the baseline, we extracted\ntraditional or surface-based features, syllable patterns based from Cebuano's\ndocumented orthography, and neural embeddings from the multilingual BERT model.\nResults show that the use of the first two handcrafted linguistic features\nobtained the best performance trained on an optimized Random Forest model with\napproximately 87% across all metrics. The feature sets and algorithm used also\nis similar to previous results in readability assessment for the Filipino\nlanguage showing potential of crosslingual application. To encourage more work\nfor readability assessment in Philippine languages such as Cebuano, we\nopen-sourced both code and data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reyes_L/0/1/0/all/0/1\">Lloyd Lois Antonie Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibanez_M/0/1/0/all/0/1\">Michael Antonio Iba&#xf1;ez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapinit_R/0/1/0/all/0/1\">Ranz Sapinit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussien_M/0/1/0/all/0/1\">Mohammed Hussien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1\">Joseph Marvin Imperial</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metaphorical User Simulators for Evaluating Task-oriented Dialogue Systems. (arXiv:2204.00763v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.00763","description":"<p>Task-oriented dialogue systems (TDSs) are assessed mainly in an offline\nsetting or through human evaluation. The evaluation is often limited to\nsingle-turn or very time-intensive. As an alternative, user simulators that\nmimic user behavior allow us to consider a broad set of user goals to generate\nhuman-like conversations for simulated evaluation. Employing existing user\nsimulators to evaluate TDSs is challenging as user simulators are primarily\ndesigned to optimize dialogue policies for TDSs and have limited evaluation\ncapability. Moreover, the evaluation of user simulators is an open challenge.\nIn this work, we proposes a metaphorical user simulator for endto-end TDS\nevaluation. We also propose a tester-based evaluation framework to generate\nvariants, i.e., dialogue systems with different capabilities. Our user\nsimulator constructs a metaphorical user model that assists the simulator in\nreasoning by referring to prior knowledge when encountering new items. We\nestimate the quality of simulators by checking the simulated interactions\nbetween simulators and variants. Our experiments are conducted using three TDS\ndatasets. The metaphorical user simulator demonstrates better consistency with\nmanual evaluation than Agenda-based simulator and Seq2seq model on three\ndatasets; our tester framework demonstrates efficiency, and our approach\ndemonstrates better generalization and scalability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shuyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Multi-Modal Hypergraph Diffusion Network with Dual Prior for Alzheimer Classification. (arXiv:2204.02399v1 [cs.LG])","link":"http://arxiv.org/abs/2204.02399","description":"<p>The automatic early diagnosis of prodromal stages of Alzheimer's disease is\nof great relevance for patient treatment to improve quality of life. We address\nthis problem as a multi-modal classification task. Multi-modal data provides\nricher and complementary information. However, existing techniques only\nconsider either lower order relations between the data and single/multi-modal\nimaging data. In this work, we introduce a novel semi-supervised hypergraph\nlearning framework for Alzheimer's disease diagnosis. Our framework allows for\nhigher-order relations among multi-modal imaging and non-imaging data whilst\nrequiring a tiny labelled set. Firstly, we introduce a dual embedding strategy\nfor constructing a robust hypergraph that preserves the data semantics. We\nachieve this by enforcing perturbation invariance at the image and graph levels\nusing a contrastive based mechanism. Secondly, we present a dynamically\nadjusted hypergraph diffusion model, via a semi-explicit flow, to improve the\npredictive uncertainty. We demonstrate, through our experiments, that our\nframework is able to outperform current techniques for Alzheimer's disease\ndiagnosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1\">Angelica I. Aviles-Rivero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Runkel_C/0/1/0/all/0/1\">Christina Runkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadakis_N/0/1/0/all/0/1\">Nicolas Papadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kourtzi_Z/0/1/0/all/0/1\">Zoe Kourtzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable Deep Learning Algorithm for Distinguishing Incomplete Kawasaki Disease by Coronary Artery Lesions on Echocardiographic Imaging. (arXiv:2204.02403v1 [eess.IV])","link":"http://arxiv.org/abs/2204.02403","description":"<p>Background and Objective: Incomplete Kawasaki disease (KD) has often been\nmisdiagnosed due to a lack of the clinical manifestations of classic KD.\nHowever, it is associated with a markedly higher prevalence of coronary artery\nlesions. Identifying coronary artery lesions by echocardiography is important\nfor the timely diagnosis of and favorable outcomes in KD. Moreover, similar to\nKD, coronavirus disease 2019, currently causing a worldwide pandemic, also\nmanifests with fever; therefore, it is crucial at this moment that KD should be\ndistinguished clearly among the febrile diseases in children. In this study, we\naimed to validate a deep learning algorithm for classification of KD and other\nacute febrile diseases.\n</p>\n<p>Methods: We obtained coronary artery images by echocardiography of children\n(n = 88 for KD; n = 65 for pneumonia). We trained six deep learning networks\n(VGG19, Xception, ResNet50, ResNext50, SE-ResNet50, and SE-ResNext50) using the\ncollected data.\n</p>\n<p>Results: SE-ResNext50 showed the best performance in terms of accuracy,\nspecificity, and precision in the classification. SE-ResNext50 offered a\nprecision of 76.35%, a sensitivity of 82.64%, and a specificity of 58.12%.\n</p>\n<p>Conclusions: The results of our study suggested that deep learning algorithms\nhave similar performance to an experienced cardiologist in detecting coronary\nartery lesions to facilitate the diagnosis of KD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Haeyun Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eun_Y/0/1/0/all/0/1\">Yongsoon Eun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hwang_J/0/1/0/all/0/1\">Jae Youn Hwang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eun_L/0/1/0/all/0/1\">Lucy Youngmin Eun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hospital-Agnostic Image Representation Learning in Digital Pathology. (arXiv:2204.02404v1 [eess.IV])","link":"http://arxiv.org/abs/2204.02404","description":"<p>Whole Slide Images (WSIs) in digital pathology are used to diagnose cancer\nsubtypes. The difference in procedures to acquire WSIs at various trial sites\ngives rise to variability in the histopathology images, thus making consistent\ndiagnosis challenging. These differences may stem from variability in image\nacquisition through multi-vendor scanners, variable acquisition parameters, and\ndifferences in staining procedure; as well, patient demographics may bias the\nglass slide batches before image acquisition. These variabilities are assumed\nto cause a domain shift in the images of different hospitals. It is crucial to\novercome this domain shift because an ideal machine-learning model must be able\nto work on the diverse sources of images, independent of the acquisition\ncenter. A domain generalization technique is leveraged in this study to improve\nthe generalization capability of a Deep Neural Network (DNN), to an unseen\nhistopathology image set (i.e., from an unseen hospital/trial site) in the\npresence of domain shift. According to experimental results, the conventional\nsupervised-learning regime generalizes poorly to data collected from different\nhospitals. However, the proposed hospital-agnostic learning can improve the\ngeneralization considering the low-dimensional latent space representation\nvisualization, and classification accuracy results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sikaroudi_M/0/1/0/all/0/1\">Milad Sikaroudi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahnamayan_S/0/1/0/all/0/1\">Shahryar Rahnamayan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tizhoosh_H/0/1/0/all/0/1\">H.R. Tizhoosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Blind Image Denoising via Implicit Neural Representations. (arXiv:2204.02405v1 [eess.IV])","link":"http://arxiv.org/abs/2204.02405","description":"<p>Recent denoising algorithms based on the \"blind-spot\" strategy show\nimpressive blind image denoising performances, without utilizing any external\ndataset. While the methods excel in recovering highly contaminated images, we\nobserve that such algorithms are often less effective under a low-noise or real\nnoise regime. To address this gap, we propose an alternative denoising strategy\nthat leverages the architectural inductive bias of implicit neural\nrepresentations (INRs), based on our two findings: (1) INR tends to fit the\nlow-frequency clean image signal faster than the high-frequency noise, and (2)\nINR layers that are closer to the output play more critical roles in fitting\nhigher-frequency parts. Building on these observations, we propose a denoising\nalgorithm that maximizes the innate denoising capability of INRs by penalizing\nthe growth of deeper layer weights. We show that our method outperforms\nexisting zero-shot denoising methods under an extensive set of low-noise or\nreal-noise scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kim_C/0/1/0/all/0/1\">Chaewon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Jaeho Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A deep learning framework for the detection and quantification of drusen and reticular pseudodrusen on optical coherence tomography. (arXiv:2204.02406v1 [eess.IV])","link":"http://arxiv.org/abs/2204.02406","description":"<p>Purpose - To develop and validate a deep learning (DL) framework for the\ndetection and quantification of drusen and reticular pseudodrusen (RPD) on\noptical coherence tomography scans.\n</p>\n<p>Design - Development and validation of deep learning models for\nclassification and feature segmentation.\n</p>\n<p>Methods - A DL framework was developed consisting of a classification model\nand an out-of-distribution (OOD) detection model for the identification of\nungradable scans; a classification model to identify scans with drusen or RPD;\nand an image segmentation model to independently segment lesions as RPD or\ndrusen. Data were obtained from 1284 participants in the UK Biobank (UKBB) with\na self-reported diagnosis of age-related macular degeneration (AMD) and 250\nUKBB controls. Drusen and RPD were manually delineated by five retina\nspecialists. The main outcome measures were sensitivity, specificity, area\nunder the ROC curve (AUC), kappa, accuracy and intraclass correlation\ncoefficient (ICC).\n</p>\n<p>Results - The classification models performed strongly at their respective\ntasks (0.95, 0.93, and 0.99 AUC, respectively, for the ungradable scans\nclassifier, the OOD model, and the drusen and RPD classification model). The\nmean ICC for drusen and RPD area vs. graders was 0.74 and 0.61, respectively,\ncompared with 0.69 and 0.68 for intergrader agreement. FROC curves showed that\nthe model's sensitivity was close to human performance.\n</p>\n<p>Conclusions - The models achieved high classification and segmentation\nperformance, similar to human performance. Application of this robust framework\nwill further our understanding of RPD as a separate entity from drusen in both\nresearch and clinical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khalid_H/0/1/0/all/0/1\">Hagar Khalid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liakopoulos_S/0/1/0/all/0/1\">Sandra Liakopoulos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ouyang_Y/0/1/0/all/0/1\">Yanling Ouyang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vente_C/0/1/0/all/0/1\">Coen de Vente</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_Gonzalo_C/0/1/0/all/0/1\">Cristina Gonz&#xe1;lez-Gonzalo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_A/0/1/0/all/0/1\">Aaron Y. Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guymer_R/0/1/0/all/0/1\">Robyn Guymer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chew_E/0/1/0/all/0/1\">Emily Y. Chew</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Egan_C/0/1/0/all/0/1\">Catherine Egan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1\">Zhichao Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_H/0/1/0/all/0/1\">Himeesh Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Farrington_J/0/1/0/all/0/1\">Joseph Farrington</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sanchez_C/0/1/0/all/0/1\">Clara I. S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tufail_A/0/1/0/all/0/1\">Adnan Tufail</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Texturify: Generating Textures on 3D Shape Surfaces. (arXiv:2204.02411v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02411","description":"<p>Texture cues on 3D objects are key to compelling visual representations, with\nthe possibility to create high visual fidelity with inherent spatial\nconsistency across different views. Since the availability of textured 3D\nshapes remains very limited, learning a 3D-supervised data-driven method that\npredicts a texture based on the 3D input is very challenging. We thus propose\nTexturify, a GAN-based method that leverages a 3D shape dataset of an object\nclass and learns to reproduce the distribution of appearances observed in real\nimages by generating high-quality textures. In particular, our method does not\nrequire any 3D color supervision or correspondence between shape geometry and\nimages to learn the texturing of 3D objects. Texturify operates directly on the\nsurface of the 3D objects by introducing face convolutional operators on a\nhierarchical 4-RoSy parametrization to generate plausible object-specific\ntextures. Employing differentiable rendering and adversarial losses that\ncritique individual views and consistency across views, we effectively learn\nthe high-quality surface texturing distribution from real-world images.\nExperiments on car and chair shape collections show that our approach\noutperforms state of the art by an average of 22% in FID score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_Y/0/1/0/all/0/1\">Yawar Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1\">Justus Thies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fangchang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Q/0/1/0/all/0/1\">Qi Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CHORE: Contact, Human and Object REconstruction from a single RGB image. (arXiv:2204.02445v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02445","description":"<p>While most works in computer vision and learning have focused on perceiving\n3D humans from single images in isolation, in this work we focus on capturing\n3D humans interacting with objects. The problem is extremely challenging due to\nheavy occlusions between human and object, diverse interaction types and depth\nambiguity. In this paper, we introduce CHORE, a novel method that learns to\njointly reconstruct human and object from a single image. CHORE takes\ninspiration from recent advances in implicit surface learning and classical\nmodel-based fitting. We compute a neural reconstruction of human and object\nrepresented implicitly with two unsigned distance fields, and additionally\npredict a correspondence field to a parametric body as well as an object pose\nfield. This allows us to robustly fit a parametric body model and a 3D object\ntemplate, while reasoning about interactions. Furthermore, prior pixel-aligned\nimplicit learning methods use synthetic data and make assumptions that are not\nmet in real data. We propose a simple yet effective depth-aware scaling that\nallows more efficient shape learning on real data. Our experiments show that\nour joint reconstruction learned with the proposed strategy significantly\noutperforms the SOTA. Our code and models will be released to foster future\nresearch in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xianghui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatnagar_B/0/1/0/all/0/1\">Bharat Lal Bhatnagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1\">Gerard Pons-Moll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Cloud-Based Phishing Attacks by Combining Deep Learning Models. (arXiv:2204.02446v1 [cs.CR])","link":"http://arxiv.org/abs/2204.02446","description":"<p>Web-based phishing attacks nowadays exploit popular cloud web hosting\nservices and apps such as Google Sites and Typeform for hosting their attacks.\nSince these attacks originate from reputable domains and IP addresses of the\ncloud services, traditional phishing detection methods such as IP reputation\nmonitoring and blacklisting are not very effective. Here we investigate the\neffectiveness of deep learning models in detecting this class of cloud-based\nphishing attacks. Specifically, we evaluate deep learning models for three\nphishing detection methods--LSTM model for URL analysis, YOLOv2 model for logo\nanalysis, and triplet network model for visual similarity analysis. We train\nthe models using well-known datasets and test their performance on phishing\nattacks in the wild. Our results qualitatively explain why the models succeed\nor fail. Furthermore, our results highlight how combining results from the\nindividual models can improve the effectiveness of detecting cloud-based\nphishing attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atre_M/0/1/0/all/0/1\">Medha Atre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_B/0/1/0/all/0/1\">Birendra Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Ashwini Rao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting and Explaining Mobile UI Tappability with Vision Modeling and Saliency Analysis. (arXiv:2204.02448v1 [cs.HC])","link":"http://arxiv.org/abs/2204.02448","description":"<p>We use a deep learning based approach to predict whether a selected element\nin a mobile UI screenshot will be perceived by users as tappable, based on\npixels only instead of view hierarchies required by previous work. To help\ndesigners better understand model predictions and to provide more actionable\ndesign feedback than predictions alone, we additionally use ML interpretability\ntechniques to help explain the output of our model. We use XRAI to highlight\nareas in the input screenshot that most strongly influence the tappability\nprediction for the selected region, and use k-Nearest Neighbors to present the\nmost similar mobile UIs from the dataset with opposing influences on\ntappability perception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schoop_E/0/1/0/all/0/1\">Eldon Schoop</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhourong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_B/0/1/0/all/0/1\">Bj&#xf6;rn Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Cross Learning for Medical Image Segmentation. (arXiv:2204.02450v1 [eess.IV])","link":"http://arxiv.org/abs/2204.02450","description":"<p>Federated learning (FL) can collaboratively train deep learning models using\nisolated patient data owned by different hospitals for various clinical\napplications, including medical image segmentation. However, a major problem of\nFL is its performance degradation when dealing with the data that are not\nindependently and identically distributed (non-iid), which is often the case in\nmedical images. In this paper, we first conduct a theoretical analysis on the\nFL algorithm to reveal the problem of model aggregation during training on\nnon-iid data. With the insights gained through the analysis, we propose a\nsimple and yet effective method, federated cross learning (FedCross), to tackle\nthis challenging problem. Unlike the conventional FL methods that combine\nmultiple individually trained local models on a server node, our FedCross\nsequentially trains the global model across different clients in a round-robin\nmanner, and thus the entire training procedure does not involve any model\naggregation steps. To further improve its performance to be comparable with the\ncentralized learning method, we combine the FedCross with an ensemble learning\nmechanism to compose a federated cross ensemble learning (FedCrossEns) method.\nFinally, we conduct extensive experiments using a set of public datasets. The\nexperimental results show that the proposed FedCross training strategy\noutperforms the mainstream FL methods on non-iid data. In addition to improving\nthe segmentation performance, our FedCrossEns can further provide a\nquantitative estimation of the model uncertainty, demonstrating the\neffectiveness and clinical significance of our designs. Source code will be\nmade publicly available after paper publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xuanang Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1\">Tianyi Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_H/0/1/0/all/0/1\">Han Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuang_T/0/1/0/all/0/1\">Tianshu Kuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barber_J/0/1/0/all/0/1\">Joshua C. Barber</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_D/0/1/0/all/0/1\">Daeseung Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gateno_J/0/1/0/all/0/1\">Jaime Gateno</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_P/0/1/0/all/0/1\">Pingkun Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_J/0/1/0/all/0/1\">James J. Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Optimal K-space Acquisition and Reconstruction using Physics-Informed Neural Networks. (arXiv:2204.02480v1 [eess.IV])","link":"http://arxiv.org/abs/2204.02480","description":"<p>The inherent slow imaging speed of Magnetic Resonance Image (MRI) has spurred\nthe development of various acceleration methods, typically through\nheuristically undersampling the MRI measurement domain known as k-space.\nRecently, deep neural networks have been applied to reconstruct undersampled\nk-space data and have shown improved reconstruction performance. While most of\nthese methods focus on designing novel reconstruction networks or new training\nstrategies for a given undersampling pattern, \\textit{e.g.}, Cartesian\nundersampling or Non-Cartesian sampling, to date, there is limited research\naiming to learn and optimize k-space sampling strategies using deep neural\nnetworks. This work proposes a novel optimization framework to learn k-space\nsampling trajectories by considering it as an Ordinary Differential Equation\n(ODE) problem that can be solved using neural ODE. In particular, the sampling\nof k-space data is framed as a dynamic system, in which neural ODE is\nformulated to approximate the system with additional constraints on MRI\nphysics. In addition, we have also demonstrated that trajectory optimization\nand image reconstruction can be learned collaboratively for improved imaging\nefficiency and reconstruction performance. Experiments were conducted on\ndifferent in-vivo datasets (\\textit{e.g.}, brain and knee images) acquired with\ndifferent sequences. Initial results have shown that our proposed method can\ngenerate better image quality in accelerated MRI than conventional\nundersampling schemes in Cartesian and Non-Cartesian acquisitions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_L/0/1/0/all/0/1\">Li Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1\">Fang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Robustness through the Lens of Convolutional Filters. (arXiv:2204.02481v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02481","description":"<p>Deep learning models are intrinsically sensitive to distribution shifts in\nthe input data. In particular, small, barely perceivable perturbations to the\ninput data can force models to make wrong predictions with high confidence. An\ncommon defense mechanism is regularization through adversarial training which\ninjects worst-case perturbations back into training to strengthen the decision\nboundaries, and to reduce overfitting. In this context, we perform an\ninvestigation of 3x3 convolution filters that form in adversarially-trained\nmodels. Filters are extracted from 71 public models of the linf-RobustBench\nCIFAR-10/100 and ImageNet1k leaderboard and compared to filters extracted from\nmodels built on the same architectures but trained without robust\nregularization. We observe that adversarially-robust models appear to form more\ndiverse, less sparse, and more orthogonal convolution filters than their normal\ncounterparts. The largest differences between robust and normal models are\nfound in the deepest layers, and the very first convolution layer, which\nconsistently and predominantly forms filters that can partially eliminate\nperturbations, irrespective of the architecture. Data &amp; Project website:\nhttps://github.com/paulgavrikov/cvpr22w_RobustnessThroughTheLens\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gavrikov_P/0/1/0/all/0/1\">Paul Gavrikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1\">Janis Keuper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training-Free Robust Multimodal Learning via Sample-Wise Jacobian Regularization. (arXiv:2204.02485v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02485","description":"<p>Multimodal fusion emerges as an appealing technique to improve model\nperformances on many tasks. Nevertheless, the robustness of such fusion methods\nis rarely involved in the present literature. In this paper, we propose a\ntraining-free robust late-fusion method by exploiting conditional independence\nassumption and Jacobian regularization. Our key is to minimize the Frobenius\nnorm of a Jacobian matrix, where the resulting optimization problem is relaxed\nto a tractable Sylvester equation. Furthermore, we provide a theoretical error\nbound of our method and some insights about the function of the extra modality.\nSeveral numerical experiments on AV-MNIST, RAVDESS, and VGGsound demonstrate\nthe efficacy of our method under both adversarial attacks and random\ncorruptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhengqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zihui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text2LIVE: Text-Driven Layered Image and Video Editing. (arXiv:2204.02491v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02491","description":"<p>We present a method for zero-shot, text-driven appearance manipulation in\nnatural images and videos. Given an input image or video and a target text\nprompt, our goal is to edit the appearance of existing objects (e.g., object's\ntexture) or augment the scene with visual effects (e.g., smoke, fire) in a\nsemantically meaningful manner. We train a generator using an internal dataset\nof training examples, extracted from a single input (image or video and target\ntext prompt), while leveraging an external pre-trained CLIP model to establish\nour losses. Rather than directly generating the edited output, our key idea is\nto generate an edit layer (color+opacity) that is composited over the original\ninput. This allows us to constrain the generation process and maintain high\nfidelity to the original input via novel text-driven losses that are applied\ndirectly to the edit layer. Our method neither relies on a pre-trained\ngenerator nor requires user-provided edit masks. We demonstrate localized,\nsemantic edits on high-resolution natural images and videos across a variety of\nobjects and scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bar_Tal_O/0/1/0/all/0/1\">Omer Bar-Tal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofri_Amar_D/0/1/0/all/0/1\">Dolev Ofri-Amar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fridman_R/0/1/0/all/0/1\">Rafail Fridman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasten_Y/0/1/0/all/0/1\">Yoni Kasten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dekel_T/0/1/0/all/0/1\">Tali Dekel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Disentangled Representations to Improve Vision-Based Keystroke Inference Attacks Under Low Data. (arXiv:2204.02494v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02494","description":"<p>Keystroke inference attacks are a form of side-channel attacks in which an\nattacker leverages various techniques to recover a user's keystrokes as she\ninputs information into some display (e.g., while sending a text message or\nentering her pin). Typically, these attacks leverage machine learning\napproaches, but assessing the realism of the threat space has lagged behind the\npace of machine learning advancements, due in-part, to the challenges in\ncurating large real-life datasets. We aim to overcome the challenge of having\nlimited number of real data by introducing a video domain adaptation technique\nthat is able to leverage synthetic data through supervised disentangled\nlearning. Specifically, for a given domain, we decompose the observed data into\ntwo factors of variation: Style and Content. Doing so provides four learned\nrepresentations: real-life style, synthetic style, real-life content and\nsynthetic content. Then, we combine them into feature representations from all\ncombinations of style-content pairings across domains, and train a model on\nthese combined representations to classify the content (i.e., labels) of a\ngiven datapoint in the style of another domain. We evaluate our method on\nreal-life data using a variety of metrics to quantify the amount of information\nan attacker is able to recover. We show that our method prevents our model from\noverfitting to a small real-life training set, indicating that our method is an\neffective form of data augmentation, thereby making keystroke inference attacks\nmore practical.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">John Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frahm_J/0/1/0/all/0/1\">Jan-Michael Frahm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monrose_F/0/1/0/all/0/1\">Fabian Monrose</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth-Guided Sparse Structure-from-Motion for Movies and TV Shows. (arXiv:2204.02509v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02509","description":"<p>Existing approaches for Structure from Motion (SfM) produce impressive 3-D\nreconstruction results especially when using imagery captured with large\nparallax. However, to create engaging video-content in movies and TV shows, the\namount by which a camera can be moved while filming a particular shot is often\nlimited. The resulting small-motion parallax between video frames makes\nstandard geometry-based SfM approaches not as effective for movies and TV\nshows. To address this challenge, we propose a simple yet effective approach\nthat uses single-frame depth-prior obtained from a pretrained network to\nsignificantly improve geometry-based SfM for our small-parallax setting. To\nthis end, we first use the depth-estimates of the detected keypoints to\nreconstruct the point cloud and camera-pose for initial two-view\nreconstruction. We then perform depth-regularized optimization to register new\nimages and triangulate the new points during incremental reconstruction. To\ncomprehensively evaluate our approach, we introduce a new dataset (StudioSfM)\nconsisting of 130 shots with 21K frames from 15 studio-produced videos that are\nmanually annotated by a professional CG studio. We demonstrate that our\napproach: (a) significantly improves the quality of 3-D reconstruction for our\nsmall-parallax setting, (b) does not cause any degradation for data with\nlarge-parallax, and (c) maintains the generalizability and scalability of\ngeometry-based sparse SfM. Our dataset can be obtained at\nhttps://github.com/amazon-research/small-baseline-camera-tracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_X/0/1/0/all/0/1\">Xiaohan Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamid_R/0/1/0/all/0/1\">Raffay Hamid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emphasis on the Minimization of False Negatives or False Positives in Binary Classification. (arXiv:2204.02526v1 [cs.LG])","link":"http://arxiv.org/abs/2204.02526","description":"<p>The minimization of specific cases in binary classification, such as false\nnegatives or false positives, grows increasingly important as humans begin to\nimplement more machine learning into current products. While there are a few\nmethods to put a bias towards the reduction of specific cases, these methods\naren't very effective, hence their minimal use in models. To this end, a new\nmethod is introduced to reduce the False Negatives or False positives without\ndrastically changing the overall performance or F1 score of the model. This\nmethod involving the careful change to the real value of the input after\npre-training the model. Presenting the results of this method being applied on\nvarious datasets, some being more complex than others. Through experimentation\non multiple model architectures on these datasets, the best model was found. In\nall the models, an increase in the recall or precision, minimization of False\nNegatives or False Positives respectively, was shown without a large drop in F1\nscore.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sanskriti Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Motion with Multi-Modal Features for Text-Based Video Segmentation. (arXiv:2204.02547v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02547","description":"<p>Text-based video segmentation aims to segment the target object in a video\nbased on a describing sentence. Incorporating motion information from optical\nflow maps with appearance and linguistic modalities is crucial yet has been\nlargely ignored by previous work. In this paper, we design a method to fuse and\nalign appearance, motion, and linguistic features to achieve accurate\nsegmentation. Specifically, we propose a multi-modal video transformer, which\ncan fuse and aggregate multi-modal and temporal features between frames.\nFurthermore, we design a language-guided feature fusion module to progressively\nfuse appearance and motion features in each feature level with guidance from\nlinguistic features. Finally, a multi-modal alignment loss is proposed to\nalleviate the semantic gap between features from different modalities.\nExtensive experiments on A2D Sentences and J-HMDB Sentences verify the\nperformance and the generalization ability of our method compared to the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wangbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style-Hallucinated Dual Consistency Learning for Domain Generalized Semantic Segmentation. (arXiv:2204.02548v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02548","description":"<p>In this paper, we study the task of synthetic-to-real domain generalized\nsemantic segmentation, which aims to learn a model that is robust to unseen\nreal-world scenes using only synthetic data. The large domain shift between\nsynthetic and real-world data, including the limited source environmental\nvariations and the large distribution gap between synthetic and real-world\ndata, significantly hinders the model performance on unseen real-world scenes.\nIn this work, we propose the Style-HAllucinated Dual consistEncy learning\n(SHADE) framework to handle such domain shift. Specifically, SHADE is\nconstructed based on two consistency constraints, Style Consistency (SC) and\nRetrospection Consistency (RC). SC enriches the source situations and\nencourages the model to learn consistent representation across\nstyle-diversified samples. RC leverages real-world knowledge to prevent the\nmodel from overfitting to synthetic data and thus largely keeps the\nrepresentation consistent between the synthetic and real-world models.\nFurthermore, we present a novel style hallucination module (SHM) to generate\nstyle-diversified samples that are essential to consistency learning. SHM\nselects basis styles from the source distribution, enabling the model to\ndynamically generate diverse and realistic samples during training. Experiments\nshow that our SHADE yields significant improvement and outperforms\nstate-of-the-art methods by 5.07% and 8.35% on the average mIoU of three\nreal-world datasets on single- and multi-source settings respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1\">Na Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RODD: A Self-Supervised Approach for Robust Out-of-Distribution Detection. (arXiv:2204.02553v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02553","description":"<p>Recent studies have addressed the concern of detecting and rejecting the\nout-of-distribution (OOD) samples as a major challenge in the safe deployment\nof deep learning (DL) models. It is desired that the DL model should only be\nconfident about the in-distribution (ID) data which reinforces the driving\nprinciple of the OOD detection. In this paper, we propose a simple yet\neffective generalized OOD detection method independent of out-of-distribution\ndatasets. Our approach relies on self-supervised feature learning of the\ntraining samples, where the embeddings lie on a compact low-dimensional space.\nMotivated by the recent studies that show self-supervised adversarial\ncontrastive learning helps robustify the model, we empirically show that a\npre-trained model with self-supervised contrastive learning yields a better\nmodel for uni-dimensional feature learning in the latent space. The method\nproposed in this work referred to as \\texttt{RODD}, outperforms SOTA detection\nperformance on an extensive suite of benchmark datasets on OOD detection tasks.\nOn the CIFAR-100 benchmarks, \\texttt{RODD} achieves a 26.97 $\\%$ lower\nfalse-positive rate (FPR@95) compared to SOTA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalid_U/0/1/0/all/0/1\">Umar Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esmaeili_A/0/1/0/all/0/1\">Ashkan Esmaeili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1\">Nazmul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahnavard_N/0/1/0/all/0/1\">Nazanin Rahnavard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixFormer: Mixing Features across Windows and Dimensions. (arXiv:2204.02557v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02557","description":"<p>While local-window self-attention performs notably in vision tasks, it\nsuffers from limited receptive field and weak modeling capability issues. This\nis mainly because it performs self-attention within non-overlapped windows and\nshares weights on the channel dimension. We propose MixFormer to find a\nsolution. First, we combine local-window self-attention with depth-wise\nconvolution in a parallel design, modeling cross-window connections to enlarge\nthe receptive fields. Second, we propose bi-directional interactions across\nbranches to provide complementary clues in the channel and spatial dimensions.\nThese two designs are integrated to achieve efficient feature mixing among\nwindows and dimensions. Our MixFormer provides competitive results on image\nclassification with EfficientNet and shows better results than RegNet and Swin\nTransformer. Performance in downstream tasks outperforms its alternatives by\nsignificant margins with less computational costs in 5 dense prediction tasks\non MS COCO, ADE20k, and LVIS. Code is available at\n\\url{https://github.com/PaddlePaddle/PaddleClas}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiman Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jian Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gait Recognition in the Wild with Dense 3D Representations and A Benchmark. (arXiv:2204.02569v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02569","description":"<p>Existing studies for gait recognition are dominated by 2D representations\nlike the silhouette or skeleton of the human body in constrained scenes.\nHowever, humans live and walk in the unconstrained 3D space, so projecting the\n3D human body onto the 2D plane will discard a lot of crucial information like\nthe viewpoint, shape, and dynamics for gait recognition. Therefore, this paper\naims to explore dense 3D representations for gait recognition in the wild,\nwhich is a practical yet neglected problem. In particular, we propose a novel\nframework to explore the 3D Skinned Multi-Person Linear (SMPL) model of the\nhuman body for gait recognition, named SMPLGait. Our framework has two\nelaborately-designed branches of which one extracts appearance features from\nsilhouettes, the other learns knowledge of 3D viewpoints and shapes from the 3D\nSMPL model. In addition, due to the lack of suitable datasets, we build the\nfirst large-scale 3D representation-based gait recognition dataset, named\nGait3D. It contains 4,000 subjects and over 25,000 sequences extracted from 39\ncameras in an unconstrained indoor scene. More importantly, it provides 3D SMPL\nmodels recovered from video frames which can provide dense 3D information of\nbody shape, viewpoint, and dynamics. Based on Gait3D, we comprehensively\ncompare our method with existing gait recognition approaches, which reflects\nthe superior performance of our framework and the potential of 3D\nrepresentations for gait recognition in the wild. The code and dataset are\navailable at https://gait3d.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jinkai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lingxiao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chenggang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting key Soccer match events to create highlights using Computer Vision. (arXiv:2204.02573v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02573","description":"<p>The research and data science community has been fascinated with the\ndevelopment of automatic systems for the detection of key events in a video.\nSpecial attention in this field is given to sports video analytics which could\nhelp in identifying key events during a match and help in preparing a strategy\nfor the games going forward. For this paper, we have chosen Football (soccer)\nas a sport where we would want to create highlights for a given match video,\nthrough a computer vision model that aims to identify important events in a\nSoccer match to create highlights of the match. We built the models based on\nFaster RCNN and YoloV5 architectures and noticed that for the amount of data we\nused for training Faster RCNN did better than YoloV5 in detecting the events in\nthe match though it was much slower. Within Faster RCNN using ResNet50 as a\nbase model gave a better class accuracy of 95.5% as compared to 92% with VGG16\nas base model completely outperforming YoloV5 for our training dataset. We\ntested with an original video of size 23 minutes and our model could reduce it\nto 4:50 minutes of highlights capturing almost all important events in the\nmatch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Darapaneni_N/0/1/0/all/0/1\">Narayana Darapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Prashant Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malhotra_N/0/1/0/all/0/1\">Nikhil Malhotra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaramurthy_V/0/1/0/all/0/1\">Vigneswaran Sundaramurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Abhaya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_S/0/1/0/all/0/1\">Shivam Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thangeda_K/0/1/0/all/0/1\">Krishna Chaitanya Thangeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paduri_A/0/1/0/all/0/1\">Anwesh Reddy Paduri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FocalClick: Towards Practical Interactive Image Segmentation. (arXiv:2204.02574v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02574","description":"<p>Interactive segmentation allows users to extract target masks by making\npositive/negative clicks. Although explored by many previous works, there is\nstill a gap between academic approaches and industrial needs: first, existing\nmodels are not efficient enough to work on low power devices; second, they\nperform poorly when used to refine preexisting masks as they could not avoid\ndestroying the correct part. FocalClick solves both issues at once by\npredicting and updating the mask in localized areas. For higher efficiency, we\ndecompose the slow prediction on the entire image into two fast inferences on\nsmall crops: a coarse segmentation on the Target Crop, and a local refinement\non the Focus Crop. To make the model work with preexisting masks, we formulate\na sub-task termed Interactive Mask Correction, and propose Progressive Merge as\nthe solution. Progressive Merge exploits morphological information to decide\nwhere to preserve and where to update, enabling users to refine any preexisting\nmask effectively. FocalClick achieves competitive results against SOTA methods\nwith significantly smaller FLOPs. It also shows significant superiority when\nmaking corrections on preexisting masks. Code and data will be released at\ngithub.com/XavierCHEN34/ClickSEG\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhiyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yilei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_M/0/1/0/all/0/1\">Manni Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_D/0/1/0/all/0/1\">Donglian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Banana Sub-Family Classification and Quality Prediction using Computer Vision. (arXiv:2204.02581v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02581","description":"<p>India is the second largest producer of fruits and vegetables in the world,\nand one of the largest consumers of fruits like Banana, Papaya and Mangoes\nthrough retail and ecommerce giants like BigBasket, Grofers and Amazon Fresh.\nHowever, adoption of technology in supply chain and retail stores is still low\nand there is a great potential to adopt computer-vision based technology for\nidentification and classification of fruits. We have chosen banana fruit to\nbuild a computer vision based model to carry out the following three use-cases\n(a) Identify Banana from a given image (b) Determine sub-family or variety of\nBanana (c) Determine the quality of Banana. Successful execution of these\nuse-cases using computer-vision model would greatly help with overall inventory\nmanagement automation, quality control, quick and efficient weighing and\nbilling which all are manual labor intensive currently. In this work, we\nsuggest a machine learning pipeline that combines the ideas of CNNs, transfer\nlearning, and data augmentation towards improving Banana fruit sub family and\nquality image classification. We have built a basic CNN and then went on to\ntune a MobileNet Banana classification model using a combination of\nself-curated and publicly-available dataset of 3064 images. The results show an\noverall 93.4% and 100% accuracy for sub-family/variety and for quality test\nclassifications respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Darapaneni_N/0/1/0/all/0/1\">Narayana Darapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanndalam_A/0/1/0/all/0/1\">Arjun Tanndalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Mohit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taneja_N/0/1/0/all/0/1\">Neeta Taneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purushothaman_P/0/1/0/all/0/1\">Prabu Purushothaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eswar_S/0/1/0/all/0/1\">Swati Eswar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paduri_A/0/1/0/all/0/1\">Anwesh Reddy Paduri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arichandrapandian_T/0/1/0/all/0/1\">Thangaselvi Arichandrapandian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SqueezeNeRF: Further factorized FastNeRF for memory-efficient inference. (arXiv:2204.02585v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02585","description":"<p>Neural Radiance Fields (NeRF) has emerged as the state-of-the-art method for\nnovel view generation of complex scenes, but is very slow during inference.\nRecently, there have been multiple works on speeding up NeRF inference, but the\nstate of the art methods for real-time NeRF inference rely on caching the\nneural network output, which occupies several giga-bytes of disk space that\nlimits their real-world applicability. As caching the neural network of\noriginal NeRF network is not feasible, Garbin et.al. proposed \"FastNeRF\" which\nfactorizes the problem into 2 sub-networks - one which depends only on the 3D\ncoordinate of a sample point and one which depends only on the 2D camera\nviewing direction. Although this factorization enables them to reduce the cache\nsize and perform inference at over 200 frames per second, the memory overhead\nis still substantial. In this work, we propose SqueezeNeRF, which is more than\n60 times memory-efficient than the sparse cache of FastNeRF and is still able\nto render at more than 190 frames per second on a high spec GPU during\ninference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wadhwani_K/0/1/0/all/0/1\">Krishna Wadhwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kojima_T/0/1/0/all/0/1\">Tamaki Kojima</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Anticipate Future with Dynamic Context Removal. (arXiv:2204.02587v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02587","description":"<p>Anticipating future events is an essential feature for intelligent systems\nand embodied AI. However, compared to the traditional recognition task, the\nuncertainty of future and reasoning ability requirement make the anticipation\ntask very challenging and far beyond solved. In this filed, previous methods\nusually care more about the model architecture design or but few attention has\nbeen put on how to train an anticipation model with a proper learning policy.\nTo this end, in this work, we propose a novel training scheme called Dynamic\nContext Removal (DCR), which dynamically schedules the visibility of observed\nfuture in the learning procedure. It follows the human-like curriculum learning\nprocess, i.e., gradually removing the event context to increase the\nanticipation difficulty till satisfying the final anticipation target. Our\nlearning scheme is plug-and-play and easy to integrate any reasoning model\nincluding transformer and LSTM, with advantages in both effectiveness and\nefficiency. In extensive experiments, the proposed method achieves\nstate-of-the-art on four widely-used benchmarks. Our code and models are\npublicly released at https://github.com/AllenXuuu/DCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong-Lu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Attention Mechanism, SRGAN Based Inpainting System for Eliminating Interruptions from Images. (arXiv:2204.02591v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02591","description":"<p>The new alternative is to use deep learning to inpaint any image by utilizing\nimage classification and computer vision techniques. In general, image\ninpainting is a task of recreating or reconstructing any broken image which\ncould be a photograph or oil/acrylic painting. With the advancement in the\nfield of Artificial Intelligence, this topic has become popular among AI\nenthusiasts. With our approach, we propose an initial end-to-end pipeline for\ninpainting images using a complete Machine Learning approach instead of a\nconventional application-based approach. We first use the YOLO model to\nautomatically identify and localize the object we wish to remove from the\nimage. Using the result obtained from the model we can generate a mask for the\nsame. After this, we provide the masked image and original image to the GAN\nmodel which uses the Contextual Attention method to fill in the region. It\nconsists of two generator networks and two discriminator networks and is also\ncalled a coarse-to-fine network structure. The two generators use fully\nconvolutional networks while the global discriminator gets hold of the entire\nimage as input while the local discriminator gets the grip of the filled region\nas input. The contextual Attention mechanism is proposed to effectively borrow\nthe neighbor information from distant spatial locations for reconstructing the\nmissing pixels. The third part of our implementation uses SRGAN to resolve the\ninpainted image back to its original size. Our work is inspired by the paper\nFree-Form Image Inpainting with Gated Convolution and Generative Image\nInpainting with Contextual Attention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Darapaneni_N/0/1/0/all/0/1\">Narayana Darapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kherde_V/0/1/0/all/0/1\">Vaibhav Kherde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1\">Kameswara Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikam_D/0/1/0/all/0/1\">Deepali Nikam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katdare_S/0/1/0/all/0/1\">Swanand Katdare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_A/0/1/0/all/0/1\">Anima Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomate_A/0/1/0/all/0/1\">Anagha Lomate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paduri_A/0/1/0/all/0/1\">Anwesh Reddy Paduri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Predicates Learning for Scene Graph Generation. (arXiv:2204.02597v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02597","description":"<p>The performance of current Scene Graph Generation models is severely hampered\nby some hard-to-distinguish predicates, e.g., \"woman-on/standing on/walking\non-beach\" or \"woman-near/looking at/in front of-child\". While general SGG\nmodels are prone to predict head predicates and existing re-balancing\nstrategies prefer tail categories, none of them can appropriately handle these\nhard-to-distinguish predicates. To tackle this issue, inspired by fine-grained\nimage classification, which focuses on differentiating among\nhard-to-distinguish object classes, we propose a method named Fine-Grained\nPredicates Learning (FGPL) which aims at differentiating among\nhard-to-distinguish predicates for Scene Graph Generation task. Specifically,\nwe first introduce a Predicate Lattice that helps SGG models to figure out\nfine-grained predicate pairs. Then, utilizing the Predicate Lattice, we propose\na Category Discriminating Loss and an Entity Discriminating Loss, which both\ncontribute to distinguishing fine-grained predicates while maintaining learned\ndiscriminatory power over recognizable ones. The proposed model-agnostic\nstrategy significantly boosts the performances of three benchmark models\n(Transformer, VCTree, and Motif) by 22.8\\%, 24.1\\% and 21.7\\% of Mean Recall\n(mR@100) on the Predicate Classification sub-task, respectively. Our model also\noutperforms state-of-the-art methods by a large margin (i.e., 6.1\\%, 4.6\\%, and\n3.2\\% of Mean Recall (mR@100)) on the Visual Genome dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1\">Xinyu Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face recognition in a transformed domain. (arXiv:2204.02608v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02608","description":"<p>This paper proposes the use of a discrete cosine transform (DCT) instead of\nthe eigenfaces method (Karhunen-Loeve Transform) for biometric identification\nbased on frontal face images. Experimental results show better recognition\naccuracies and reduced computational burden. This paper includes results with\ndifferent classifiers and a combination of them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cloning Outfits from Real-World Images to 3D Characters for Generalizable Person Re-Identification. (arXiv:2204.02611v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02611","description":"<p>Recently, large-scale synthetic datasets are shown to be very useful for\ngeneralizable person re-identification. However, synthesized persons in\nexisting datasets are mostly cartoon-like and in random dress collocation,\nwhich limits their performance. To address this, in this work, an automatic\napproach is proposed to directly clone the whole outfits from real-world person\nimages to virtual 3D characters, such that any virtual person thus created will\nappear very similar to its real-world counterpart. Specifically, based on UV\ntexture mapping, two cloning methods are designed, namely registered clothes\nmapping and homogeneous cloth expansion. Given clothes keypoints detected on\nperson images and labeled on regular UV maps with clear clothes structures,\nregistered mapping applies perspective homography to warp real-world clothes to\nthe counterparts on the UV map. As for invisible clothes parts and irregular UV\nmaps, homogeneous expansion segments a homogeneous area on clothes as a\nrealistic cloth pattern or cell, and expand the cell to fill the UV map.\nFurthermore, a similarity-diversity expansion strategy is proposed, by\nclustering person images, sampling images per cluster, and cloning outfits for\n3D character generation. This way, virtual persons can be scaled up densely in\nvisual similarity to challenge model learning, and diversely in population to\nenrich sample distribution. Finally, by rendering the cloned characters in\nUnity3D scenes, a more realistic virtual dataset called ClonedPerson is\ncreated, with 5,621 identities and 887,766 images. Experimental results show\nthat the model trained on ClonedPerson has a better generalization performance,\nsuperior to that trained on other popular real-world and synthetic person\nre-identification datasets. The ClonedPerson project is available at\nhttps://github.com/Yanan-Wang-cs/ClonedPerson.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xuezhi Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Adaptive Object Detection under Noisy Annotations. (arXiv:2204.02620v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02620","description":"<p>Domain Adaptive Object Detection (DAOD) models a joint distribution of images\nand labels from an annotated source domain and learns a domain-invariant\ntransformation to estimate the target labels with the given target domain\nimages. Existing methods assume that the source domain labels are completely\nclean, yet large-scale datasets often contain error-prone annotations due to\ninstance ambiguity, which may lead to a biased source distribution and severely\ndegrade the performance of the domain adaptive detector de facto. In this\npaper, we represent the first effort to formulate noisy DAOD and propose a\nNoise Latent Transferability Exploration (NLTE) framework to address this\nissue. It is featured with 1) Potential Instance Mining (PIM), which leverages\neligible proposals to recapture the miss-annotated instances from the\nbackground; 2) Morphable Graph Relation Module (MGRM), which models the\nadaptation feasibility and transition probability of noisy samples with\nrelation matrices; 3) Entropy-Aware Gradient Reconcilement (EAGR), which\nincorporates the semantic information into the discrimination process and\nenforces the gradients provided by noisy and clean samples to be consistent\ntowards learning domain-invariant representations. A thorough evaluation on\nbenchmark DAOD datasets with noisy source annotations validates the\neffectiveness of NLTE. In particular, NLTE improves the mAP by 8.4\\% under 60\\%\ncorrupted annotations and even approaches the ideal upper bound of training on\na clean source dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wuyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiushi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baopu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yixuan Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IterVM: Iterative Vision Modeling Module for Scene Text Recognition. (arXiv:2204.02630v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02630","description":"<p>Scene text recognition (STR) is a challenging problem due to the imperfect\nimagery conditions in natural images. State-of-the-art methods utilize both\nvisual cues and linguistic knowledge to tackle this challenging problem.\nSpecifically, they propose iterative language modeling module (IterLM) to\nrepeatedly refine the output sequence from the visual modeling module (VM).\nThough achieving promising results, the vision modeling module has become the\nperformance bottleneck of these methods. In this paper, we newly propose\niterative vision modeling module (IterVM) to further improve the STR accuracy.\nSpecifically, the first VM directly extracts multi-level features from the\ninput image, and the following VMs re-extract multi-level features from the\ninput image and fuse them with the high-level (i.e., the most semantic one)\nfeature extracted by the previous VM. By combining the proposed IterVM with\niterative language modeling module, we further propose a powerful scene text\nrecognizer called IterNet. Extensive experiments demonstrate that the proposed\nIterVM can significantly improve the scene text recognition accuracy,\nespecially on low-quality scene text images. Moreover, the proposed scene text\nrecognizer IterNet achieves new state-of-the-art results on several public\nbenchmarks. Codes will be available at https://github.com/VDIGPKU/IterNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaojie Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongtao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Super-resolved multi-temporal segmentation with deep permutation-invariant networks. (arXiv:2204.02631v1 [eess.IV])","link":"http://arxiv.org/abs/2204.02631","description":"<p>Multi-image super-resolution from multi-temporal satellite acquisitions of a\nscene has recently enjoyed great success thanks to new deep learning models. In\nthis paper, we go beyond classic image reconstruction at a higher resolution by\nstudying a super-resolved inference problem, namely semantic segmentation at a\nspatial resolution higher than the one of sensing platform. We expand upon\nrecently proposed models exploiting temporal permutation invariance with a\nmulti-resolution fusion module able to infer the rich semantic information\nneeded by the segmentation task. The model presented in this paper has recently\nwon the AI4EO challenge on Enhanced Sentinel 2 Agriculture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Valsesia_D/0/1/0/all/0/1\">Diego Valsesia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Magli_E/0/1/0/all/0/1\">Enrico Magli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Swiss Army Knife for Image-to-Image Translation: Multi-Task Diffusion Models. (arXiv:2204.02641v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02641","description":"<p>Recently, diffusion models were applied to a wide range of image analysis\ntasks. We build on a method for image-to-image translation using denoising\ndiffusion implicit models and include a regression problem and a segmentation\nproblem for guiding the image generation to the desired output. The main\nadvantage of our approach is that the guidance during the denoising process is\ndone by an external gradient. Consequently, the diffusion model does not need\nto be retrained for the different tasks on the same dataset. We apply our\nmethod to simulate the aging process on facial photos using a regression task,\nas well as on a brain magnetic resonance (MR) imaging dataset for the\nsimulation of brain tumor growth. Furthermore, we use a segmentation model to\ninpaint tumors at the desired location in healthy slices of brain MR images. We\nachieve convincing results for all problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolleb_J/0/1/0/all/0/1\">Julia Wolleb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandkuhler_R/0/1/0/all/0/1\">Robin Sandk&#xfc;hler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bieder_F/0/1/0/all/0/1\">Florentin Bieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cattin_P/0/1/0/all/0/1\">Philippe C. Cattin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAIPI in Practice: Towards Explainable Interactive Medical Image Classification. (arXiv:2204.02661v1 [cs.LG])","link":"http://arxiv.org/abs/2204.02661","description":"<p>Would you trust physicians if they cannot explain their decisions to you?\nMedical diagnostics using machine learning gained enormously in importance\nwithin the last decade. However, without further enhancements many\nstate-of-the-art machine learning methods are not suitable for medical\napplication. The most important reasons are insufficient data set quality and\nthe black-box behavior of machine learning algorithms such as Deep Learning\nmodels. Consequently, end-users cannot correct the model's decisions and the\ncorresponding explanations. The latter is crucial for the trustworthiness of\nmachine learning in the medical domain. The research field explainable\ninteractive machine learning searches for methods that address both\nshortcomings. This paper extends the explainable and interactive CAIPI\nalgorithm and provides an interface to simplify human-in-the-loop approaches\nfor image classification. The interface enables the end-user (1) to investigate\nand (2) to correct the model's prediction and explanation, and (3) to influence\nthe data set quality. After CAIPI optimization with only a single\ncounterexample per iteration, the model achieves an accuracy of $97.48\\%$ on\nthe Medical MNIST and $95.02\\%$ on the Fashion MNIST. This accuracy is\napproximately equal to state-of-the-art Deep Learning optimization procedures.\nBesides, CAIPI reduces the labeling effort by approximately $80\\%$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Slany_E/0/1/0/all/0/1\">Emanuel Slany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_Y/0/1/0/all/0/1\">Yannik Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheele_S/0/1/0/all/0/1\">Stephan Scheele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulus_J/0/1/0/all/0/1\">Jan Paulus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_U/0/1/0/all/0/1\">Ute Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards An End-to-End Framework for Flow-Guided Video Inpainting. (arXiv:2204.02663v1 [eess.IV])","link":"http://arxiv.org/abs/2204.02663","description":"<p>Optical flow, which captures motion information across frames, is exploited\nin recent video inpainting methods through propagating pixels along its\ntrajectories. However, the hand-crafted flow-based processes in these methods\nare applied separately to form the whole inpainting pipeline. Thus, these\nmethods are less efficient and rely heavily on the intermediate results from\nearlier stages. In this paper, we propose an End-to-End framework for\nFlow-Guided Video Inpainting (E$^2$FGVI) through elaborately designed three\ntrainable modules, namely, flow completion, feature propagation, and content\nhallucination modules. The three modules correspond with the three stages of\nprevious flow-based methods but can be jointly optimized, leading to a more\nefficient and effective inpainting process. Experimental results demonstrate\nthat the proposed method outperforms state-of-the-art methods both\nqualitatively and quantitatively and shows promising efficiency. The code is\navailable at https://github.com/MCG-NKU/E2FGVI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_C/0/1/0/all/0/1\">Cheng-Ze Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_J/0/1/0/all/0/1\">Jianhua Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_C/0/1/0/all/0/1\">Chun-Le Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faster-TAD: Towards Temporal Action Detection with Proposal Generation and Classification in a Unified Network. (arXiv:2204.02674v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02674","description":"<p>Temporal action detection (TAD) aims to detect the semantic labels and\nboundaries of action instances in untrimmed videos. Current mainstream\napproaches are multi-step solutions, which fall short in efficiency and\nflexibility. In this paper, we propose a unified network for TAD, termed\nFaster-TAD, by re-purposing a Faster-RCNN like architecture. To tackle the\nunique difficulty in TAD, we make important improvements over the original\nframework. We propose a new Context-Adaptive Proposal Module and an innovative\nFake-Proposal Generation Block. What's more, we use atomic action features to\nimprove the performance. Faster-TAD simplifies the pipeline of TAD and gets\nremarkable performance on lots of benchmarks, i.e., ActivityNet-1.3 (40.01%\nmAP), HACS Segments (38.39% mAP), SoccerNet-Action Spotting (54.09% mAP). It\noutperforms existing single-network detector by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1\">Xunqiang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rolling Colors: Adversarial Laser Exploits against Traffic Light Recognition. (arXiv:2204.02675v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02675","description":"<p>Traffic light recognition is essential for fully autonomous driving in urban\nareas. In this paper, we investigate the feasibility of fooling traffic light\nrecognition mechanisms by shedding laser interference on the camera. By\nexploiting the rolling shutter of CMOS sensors, we manage to inject a color\nstripe overlapped on the traffic light in the image, which can cause a red\nlight to be recognized as a green light or vice versa. To increase the success\nrate, we design an optimization method to search for effective laser parameters\nbased on empirical models of laser interference. Our evaluation in emulated and\nreal-world setups on 2 state-of-the-art recognition systems and 5 cameras\nreports a maximum success rate of 30% and 86.25% for Red-to-Green and\nGreen-to-Red attacks. We observe that the attack is effective in continuous\nframes from more than 40 meters away against a moving vehicle, which may cause\nend-to-end impacts on self-driving such as running a red light or emergency\nstop. To mitigate the threat, we propose redesigning the rolling shutter\nmechanism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhijian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhanyuan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiaoyu Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenyuan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PP-LiteSeg: A Superior Real-Time Semantic Segmentation Model. (arXiv:2204.02681v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02681","description":"<p>Real-world applications have high demands for semantic segmentation methods.\nAlthough semantic segmentation has made remarkable leap-forwards with deep\nlearning, the performance of real-time methods is not satisfactory. In this\nwork, we propose PP-LiteSeg, a novel lightweight model for the real-time\nsemantic segmentation task. Specifically, we present a Flexible and Lightweight\nDecoder (FLD) to reduce computation overhead of previous decoder. To strengthen\nfeature representations, we propose a Unified Attention Fusion Module (UAFM),\nwhich takes advantage of spatial and channel attention to produce a weight and\nthen fuses the input features with the weight. Moreover, a Simple Pyramid\nPooling Module (SPPM) is proposed to aggregate global context with low\ncomputation cost. Extensive evaluations demonstrate that PP-LiteSeg achieves a\nsuperior trade-off between accuracy and speed compared to other methods. On the\nCityscapes test set, PP-LiteSeg achieves 72.0% mIoU/273.6 FPS and 77.5%\nmIoU/102.6 FPS on NVIDIA GTX 1080Ti. Source code and models are available at\nPaddleSeg: https://github.com/PaddlePaddle/PaddleSeg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Juncai Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shiyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yuying Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1\">Lutao Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guowei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zewu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiliang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuning Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_Q/0/1/0/all/0/1\">Qingqing Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_B/0/1/0/all/0/1\">Baohua Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoguang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dianhai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yanjun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Agnostic Prior for Transfer Semantic Segmentation. (arXiv:2204.02684v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02684","description":"<p>Unsupervised domain adaptation (UDA) is an important topic in the computer\nvision community. The key difficulty lies in defining a common property between\nthe source and target domains so that the source-domain features can align with\nthe target-domain semantics. In this paper, we present a simple and effective\nmechanism that regularizes cross-domain representation learning with a\ndomain-agnostic prior (DAP) that constrains the features extracted from source\nand target domains to align with a domain-agnostic space. In practice, this is\neasily implemented as an extra loss term that requires a little extra costs. In\nthe standard evaluation protocol of transferring synthesized data to real data,\nwe validate the effectiveness of different types of DAP, especially that\nborrowed from a text embedding model that shows favorable performance beyond\nthe state-of-the-art UDA approaches in terms of segmentation accuracy. Our\nresearch reveals that UDA benefits much from better proxies, possibly from\nother data modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huo_X/0/1/0/all/0/1\">Xinyue Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hengtong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SEAL: A Large-scale Video Dataset of Multi-grained Spatio-temporally Action Localization. (arXiv:2204.02688v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02688","description":"<p>In spite of many dataset efforts for human action recognition, current\ncomputer vision algorithms are still limited to coarse-grained spatial and\ntemporal annotations among human daily life. In this paper, we introduce a\nnovel large-scale video dataset dubbed SEAL for multi-grained Spatio-tEmporal\nAction Localization. SEAL consists of two kinds of annotations, SEAL Tubes and\nSEAL Clips. We observe that atomic actions can be combined into many complex\nactivities. SEAL Tubes provide both atomic action and complex activity\nannotations in tubelet level, producing 49.6k atomic actions spanning 172\naction categories and 17.7k complex activities spanning 200 activity\ncategories. SEAL Clips localizes atomic actions in space during two-second\nclips, producing 510.4k action labels with multiple labels per person.\nExtensive experimental results show that SEAL significantly helps to advance\nvideo understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jianyang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_J/0/1/0/all/0/1\">Jiaming Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1\">Xunqiang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aesthetic Text Logo Synthesis via Content-aware Layout Inferring. (arXiv:2204.02701v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02701","description":"<p>Text logo design heavily relies on the creativity and expertise of\nprofessional designers, in which arranging element layouts is one of the most\nimportant procedures. However, few attention has been paid to this task which\nneeds to take many factors (e.g., fonts, linguistics, topics, etc.) into\nconsideration. In this paper, we propose a content-aware layout generation\nnetwork which takes glyph images and their corresponding text as input and\nsynthesizes aesthetic layouts for them automatically. Specifically, we develop\na dual-discriminator module, including a sequence discriminator and an image\ndiscriminator, to evaluate both the character placing trajectories and rendered\nshapes of synthesized text logos, respectively. Furthermore, we fuse the\ninformation of linguistics from texts and visual semantics from glyphs to guide\nlayout prediction, which both play important roles in professional layout\ndesign. To train and evaluate our approach, we construct a dataset named as\nTextLogo3K, consisting of about 3,500 text logo images and their pixel-level\nannotations. Experimental studies on this dataset demonstrate the effectiveness\nof our approach for synthesizing visually-pleasing text logos and verify its\nsuperiority against the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1\">Guo Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenhan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_P/0/1/0/all/0/1\">Pengfei Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hongwen Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zhouhui Lian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Georeferencing of Photovoltaic Modules from Aerial Infrared Videos using Structure-from-Motion. (arXiv:2204.02733v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02733","description":"<p>To identify abnormal photovoltaic (PV) modules in large-scale PV plants\neconomically, drone-mounted infrared (IR) cameras and automated video\nprocessing algorithms are frequently used. While most related works focus on\nthe detection of abnormal modules, little has been done to automatically\nlocalize those modules within the plant. In this work, we use incremental\nstructure-from-motion to automatically obtain geocoordinates of all PV modules\nin a plant based on visual cues and the measured GPS trajectory of the drone.\nIn addition, we extract multiple IR images of each PV module. Using our method,\nwe successfully map 99.3 % of the 35084 modules in four large-scale and one\nrooftop plant and extract over 2.2 million module images. As compared to our\nprevious work, extraction misses 18 times less modules (one in 140 modules as\ncompared to one in eight). Furthermore, two or three plant rows can be\nprocessed simultaneously, increasing module throughput and reducing flight\nduration by a factor of 2.1 and 3.7, respectively. Comparison with an accurate\northophoto of one of the large-scale plants yields a root mean square error of\nthe estimated module geocoordinates of 5.87 m and a relative error within each\nplant row of 0.22 m to 0.82 m. Finally, we use the module geocoordinates and\nextracted IR images to visualize distributions of module temperatures and\nanomaly predictions of a deep learning classifier on a map. While the\ntemperature distribution helps to identify disconnected strings, we also find\nthat its detection accuracy for module anomalies reaches, or even exceeds, that\nof a deep learning classifier for seven out of ten common anomaly types. The\nsoftware is published at https://github.com/LukasBommes/PV-Hawk.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bommes_L/0/1/0/all/0/1\">Lukas Bommes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buerhop_Lutz_C/0/1/0/all/0/1\">Claudia Buerhop-Lutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pickel_T/0/1/0/all/0/1\">Tobias Pickel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauch_J/0/1/0/all/0/1\">Jens Hauch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brabec_C/0/1/0/all/0/1\">Christoph Brabec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_I/0/1/0/all/0/1\">Ian Marius Peters</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masking Adversarial Damage: Finding Adversarial Saliency for Robust and Sparse Network. (arXiv:2204.02738v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02738","description":"<p>Adversarial examples provoke weak reliability and potential security issues\nin deep neural networks. Although adversarial training has been widely studied\nto improve adversarial robustness, it works in an over-parameterized regime and\nrequires high computations and large memory budgets. To bridge adversarial\nrobustness and model compression, we propose a novel adversarial pruning\nmethod, Masking Adversarial Damage (MAD) that employs second-order information\nof adversarial loss. By using it, we can accurately estimate adversarial\nsaliency for model parameters and determine which parameters can be pruned\nwithout weakening adversarial robustness. Furthermore, we reveal that model\nparameters of initial layer are highly sensitive to the adversarial examples\nand show that compressed feature representation retains semantic information\nfor the target objects. Through extensive experiments on three public datasets,\nwe demonstrate that MAD effectively prunes adversarially trained networks\nwithout loosing adversarial robustness and shows better performance than\nprevious adversarial pruning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Byung-Kwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1\">Yong Man Ro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Representations: A Unified Look at Multiple Task and Domain Learning. (arXiv:2204.02744v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02744","description":"<p>We propose a unified look at jointly learning multiple vision tasks and\nvisual domains through universal representations, a single deep neural network.\nLearning multiple problems simultaneously involves minimizing a weighted sum of\nmultiple loss functions with different magnitudes and characteristics and thus\nresults in unbalanced state of one loss dominating the optimization and poor\nresults compared to learning a separate model for each problem. To this end, we\npropose distilling knowledge of multiple task/domain-specific networks into a\nsingle deep neural network after aligning its representations with the\ntask/domain-specific ones through small capacity adapters. We rigorously show\nthat universal representations achieve state-of-the-art performances in\nlearning of multiple dense prediction problems in NYU-v2 and Cityscapes,\nmultiple image classification problems from diverse domains in Visual Decathlon\nDataset and cross-domain few-shot learning in MetaDataset. Finally we also\nconduct multiple analysis through ablation and qualitative studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei-Hong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xialei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1\">Hakan Bilen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BFRnet: A deep learning-based MR background field removal method for QSM of the brain containing significant pathological susceptibility sources. (arXiv:2204.02760v1 [q-bio.QM])","link":"http://arxiv.org/abs/2204.02760","description":"<p>Introduction: Background field removal (BFR) is a critical step required for\nsuccessful quantitative susceptibility mapping (QSM). However, eliminating the\nbackground field in brains containing significant susceptibility sources, such\nas intracranial hemorrhages, is challenging due to the relatively large scale\nof the field induced by these pathological susceptibility sources. Method: This\nstudy proposes a new deep learning-based method, BFRnet, to remove background\nfield in healthy and hemorrhagic subjects. The network is built with the\ndual-frequency octave convolutions on the U-net architecture, trained with\nsynthetic field maps containing significant susceptibility sources. The BFRnet\nmethod is compared with three conventional BFR methods and one previous deep\nlearning method using simulated and in vivo brains from 4 healthy and 2\nhemorrhagic subjects. Robustness against acquisition field-of-view (FOV)\norientation and brain masking are also investigated. Results: For both\nsimulation and in vivo experiments, BFRnet led to the best visually appealing\nresults in the local field and QSM results with the minimum contrast loss and\nthe most accurate hemorrhage susceptibility measurements among all five\nmethods. In addition, BFRnet produced the most consistent local field and\nsusceptibility maps between different sizes of brain masks, while conventional\nmethods depend drastically on precise brain extraction and further brain edge\nerosions. It is also observed that BFRnet performed the best among all BFR\nmethods for acquisition FOVs oblique to the main magnetic field. Conclusion:\nThe proposed BFRnet improved the accuracy of local field reconstruction in the\nhemorrhagic subjects compared with conventional BFR algorithms. The BFRnet\nmethod was effective for acquisitions of titled orientations and retained whole\nbrains without edge erosion as often required by traditional BFR methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Zhu_X/0/1/0/all/0/1\">Xuanyu Zhu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Crozier_S/0/1/0/all/0/1\">Stuart Crozier</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sun_H/0/1/0/all/0/1\">Hongfu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-DRDNet Semi-supervised Detail-recovery Image Deraining Network via Unpaired Contrastive Learning. (arXiv:2204.02772v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02772","description":"<p>The intricacy of rainy image contents often leads cutting-edge deraining\nmodels to image degradation including remnant rain, wrongly-removed details,\nand distorted appearance. Such degradation is further exacerbated when applying\nthe models trained on synthetic data to real-world rainy images. We raise an\nintriguing question -- if leveraging both accessible unpaired clean/rainy yet\nreal-world images and additional detail repair guidance, can improve the\ngeneralization ability of a deraining model? To answer it, we propose a\nsemi-supervised detail-recovery image deraining network (termed as\nSemi-DRDNet). Semi-DRDNet consists of three branches: 1) for removing rain\nstreaks without remnants, we present a \\textit{squeeze-and-excitation}\n(SE)-based rain residual network; 2) for encouraging the lost details to\nreturn, we construct a \\textit{structure detail context aggregation}\n(SDCAB)-based detail repair network; to our knowledge, this is the first time;\nand 3) for bridging the domain gap, we develop a novel contrastive\nregularization network to learn from unpaired positive (clean) and negative\n(rainy) yet real-world images. As a semi-supervised learning paradigm,\nSemi-DRDNet operates smoothly on both synthetic and real-world rainy data in\nterms of deraining robustness and detail accuracy. Comparisons on four datasets\nshow clear visual and numerical improvements of our Semi-DRDNet over thirteen\nstate-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yiyang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Sen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenhan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mingqiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haoran Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">XiaoPing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D face reconstruction with dense landmarks. (arXiv:2204.02776v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02776","description":"<p>Landmarks often play a key role in face analysis, but many aspects of\nidentity or expression cannot be represented by sparse landmarks alone. Thus,\nin order to reconstruct faces more accurately, landmarks are often combined\nwith additional signals like depth images or techniques like differentiable\nrendering. Can we keep things simple by just using more landmarks? In answer,\nwe present the first method that accurately predicts 10x as many landmarks as\nusual, covering the whole head, including the eyes and teeth. This is\naccomplished using synthetic training data, which guarantees perfect landmark\nannotations. By fitting a morphable model to these dense landmarks, we achieve\nstate-of-the-art results for monocular 3D face reconstruction in the wild. We\nshow that dense landmarks are an ideal signal for integrating face shape\ninformation across frames by demonstrating accurate and expressive facial\nperformance capture in both monocular and multi-view scenarios. This approach\nis also highly efficient: we can predict dense landmarks and fit our 3D face\nmodel at over 150FPS on a single CPU thread.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wood_E/0/1/0/all/0/1\">Erroll Wood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baltrusaitis_T/0/1/0/all/0/1\">Tadas Baltrusaitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hewitt_C/0/1/0/all/0/1\">Charlie Hewitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Matthew Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jingjing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milosavljevic_N/0/1/0/all/0/1\">Nikola Milosavljevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilde_D/0/1/0/all/0/1\">Daniel Wilde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garbin_S/0/1/0/all/0/1\">Stephan Garbin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharp_T/0/1/0/all/0/1\">Toby Sharp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stojiljkovic_I/0/1/0/all/0/1\">Ivan Stojiljkovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cashman_T/0/1/0/all/0/1\">Tom Cashman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentin_J/0/1/0/all/0/1\">Julien Valentin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dempster-Shafer approach to trustworthy AI with application to fetal brain MRI segmentation. (arXiv:2204.02779v1 [eess.IV])","link":"http://arxiv.org/abs/2204.02779","description":"<p>Deep learning models for medical image segmentation can fail unexpectedly and\nspectacularly for pathological cases and for images acquired at different\ncenters than those used for training, with labeling errors that violate expert\nknowledge about the anatomy and the intensity distribution of the regions to be\nsegmented. Such errors undermine the trustworthiness of deep learning models\ndeveloped for medical image segmentation. Mechanisms with a fallback method for\ndetecting and correcting such failures are essential for safely translating\nthis technology into clinics and are likely to be a requirement of future\nregulations on artificial intelligence (AI). Here, we propose a principled\ntrustworthy AI theoretical framework and a practical system that can augment\nany backbone AI system using a fallback method and a fail-safe mechanism based\non Dempster-Shafer theory. Our approach relies on an actionable definition of\ntrustworthy AI. Our method automatically discards the voxel-level labeling\npredicted by the backbone AI that are likely to violate expert knowledge and\nrelies on a fallback atlas-based segmentation method for those voxels. We\ndemonstrate the effectiveness of the proposed trustworthy AI approach on the\nlargest reported annotated dataset of fetal T2w MRI consisting of 540 manually\nannotated fetal brain 3D MRIs with neurotypical or abnormal brain development\nand acquired from 13 sources of data across 6 countries. We show that our\ntrustworthy AI method improves the robustness of a state-of-the-art backbone AI\nfor fetal brain MRI segmentation on MRIs acquired across various centers and\nfor fetuses with various brain abnormalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aertsen_M/0/1/0/all/0/1\">Michael Aertsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kofler_F/0/1/0/all/0/1\">Florian Kofler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bink_A/0/1/0/all/0/1\">Andrea Bink</a>, <a href=\"http://arxiv.org/find/eess/1/au:+David_A/0/1/0/all/0/1\">Anna L. David</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deprest_T/0/1/0/all/0/1\">Thomas Deprest</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Emam_D/0/1/0/all/0/1\">Doaa Emam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guffens_F/0/1/0/all/0/1\">Fr/&#x27;ed/&#x27;eric Guffens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jakab_A/0/1/0/all/0/1\">Andr&#xe1;s Jakab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kasprian_G/0/1/0/all/0/1\">Gregor Kasprian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kienast_P/0/1/0/all/0/1\">Patric Kienast</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Melbourne_A/0/1/0/all/0/1\">Andrew Melbourne</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern Menze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mufti_N/0/1/0/all/0/1\">Nada Mufti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pogledic_I/0/1/0/all/0/1\">Ivana Pogledic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prayer_D/0/1/0/all/0/1\">Daniela Prayer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stuempflen_M/0/1/0/all/0/1\">Marlene Stuempflen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elslander_E/0/1/0/all/0/1\">Esther Van Elslander</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">S&#xe9;bastien Ourselin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deprest_J/0/1/0/all/0/1\">Jan Deprest</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Motion-Compensated Network for Unsupervised Video Object Segmentation. (arXiv:2204.02791v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02791","description":"<p>Unsupervised video object segmentation (UVOS) aims at automatically\nseparating the primary foreground object(s) from the background in a video\nsequence. Existing UVOS methods either lack robustness when there are visually\nsimilar surroundings (appearance-based) or suffer from deterioration in the\nquality of their predictions because of dynamic background and inaccurate flow\n(flow-based). To overcome the limitations, we propose an implicit\nmotion-compensated network (IMCNet) combining complementary cues\n($\\textit{i.e.}$, appearance and motion) with aligned motion information from\nthe adjacent frames to the current frame at the feature level without\nestimating optical flows. The proposed IMCNet consists of an affinity computing\nmodule (ACM), an attention propagation module (APM), and a motion compensation\nmodule (MCM). The light-weight ACM extracts commonality between neighboring\ninput frames based on appearance features. The APM then transmits global\ncorrelation in a top-down manner. Through coarse-to-fine iterative inspiring,\nthe APM will refine object regions from multiple resolutions so as to\nefficiently avoid losing details. Finally, the MCM aligns motion information\nfrom temporally adjacent frames to the current frame which achieves implicit\nmotion compensation at the feature level. We perform extensive experiments on\n$\\textit{DAVIS}_{\\textit{16}}$ and $\\textit{YouTube-Objects}$. Our network\nachieves favorable performance while running at a faster speed compared to the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xi_L/0/1/0/all/0/1\">Lin Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xingming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengguo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transformer-Based Contrastive Learning Approach for Few-Shot Sign Language Recognition. (arXiv:2204.02803v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02803","description":"<p>Sign language recognition from sequences of monocular images or 2D poses is a\nchallenging field, not only due to the difficulty to infer 3D information from\n2D data, but also due to the temporal relationship between the sequences of\ninformation. Additionally, the wide variety of signs and the constant need to\nadd new ones on production environments makes it infeasible to use traditional\nclassification techniques. We propose a novel Contrastive Transformer-based\nmodel, which demonstrate to learn rich representations from body key points\nsequences, allowing better comparison between vector embedding. This allows us\nto apply these techniques to perform one-shot or few-shot tasks, such as\nclassification and translation. The experiments showed that the model could\ngeneralize well and achieved competitive results for sign classes never seen in\nthe training process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_S/0/1/0/all/0/1\">Silvan Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_E/0/1/0/all/0/1\">Esdras Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahia_M/0/1/0/all/0/1\">M&#xe1;rcio Dahia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_J/0/1/0/all/0/1\">Jampierre Rocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expression-preserving face frontalization improves visually assisted speech processing. (arXiv:2204.02810v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02810","description":"<p>Face frontalization consists of synthesizing a frontally-viewed face from an\narbitrarily-viewed one. The main contribution of this paper is a frontalization\nmethodology that preserves non-rigid facial deformations in order to boost the\nperformance of visually assisted speech communication. The method alternates\nbetween the estimation of (i)~the rigid transformation (scale, rotation, and\ntranslation) and (ii)~the non-rigid deformation between an arbitrarily-viewed\nface and a face model. The method has two important merits: it can deal with\nnon-Gaussian errors in the data and it incorporates a dynamical face\ndeformation model. For that purpose, we use the generalized Student\nt-distribution in combination with a linear dynamic system in order to account\nfor both rigid head motions and time-varying facial deformations caused by\nspeech production. We propose to use the zero-mean normalized cross-correlation\n(ZNCC) score to evaluate the ability of the method to preserve facial\nexpressions. The method is thoroughly evaluated and compared with several state\nof the art methods, either based on traditional geometric models or on deep\nlearning. Moreover, we show that the method, when incorporated into deep\nlearning pipelines, namely lip reading and speech enhancement, improves word\nrecognition and speech intelligibilty scores by a considerable margin.\nSupplemental material is accessible at\nhttps://team.inria.fr/robotlearn/research/facefrontalization-benchmark/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhiqi Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghi_M/0/1/0/all/0/1\">Mostafa Sadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horaud_R/0/1/0/all/0/1\">Radu Horaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donley_J/0/1/0/all/0/1\">Jacob Donley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Anurag Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BMD: A General Class-balanced Multicentric Dynamic Prototype Strategy for Source-free Domain Adaptation. (arXiv:2204.02811v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02811","description":"<p>Source-free Domain Adaptation (SFDA) aims to adapt a pre-trained source model\nto the unlabeled target domain without accessing the well-labeled source data,\nwhich is a much more practical setting due to the data privacy, security, and\ntransmission issues. To make up for the absence of source data, most existing\nmethods introduced feature prototype based pseudo-labeling strategies to\nrealize self-training model adaptation. However, feature prototypes are\nobtained by instance-level predictions based feature clustering, which is\ncategory-biased and tends to result in noisy labels since the visual domain\ngaps between source and target are usually different between categories. In\naddition, we found that a monocentric feature prototype may be ineffective to\nrepresent each category and introduce negative transfer, especially for those\nhard-transfer data. To address these issues, we propose a general\nclass-Balanced Multicentric Dynamic prototype (BMD) strategy for the SFDA task.\nSpecifically, for each target category, we first introduce a global inter-class\nbalanced sampling strategy to aggregate potential representative target\nsamples. Then, we design an intra-class multicentric clustering strategy to\nachieve more robust and representative prototypes generation. In contrast to\nexisting strategies that update the pseudo label at a fixed training period, we\nfurther introduce a dynamic pseudo labeling strategy to incorporate network\nupdate information during model adaptation. Extensive experiments show that the\nproposed model-agnostic BMD strategy significantly improves representative SFDA\nmethods to yield new state-of-the-art results, e.g., improving SHOT from 82.9\\%\nto 85.8\\% on VisDA-C and NRC from 52.6\\% to 57.0\\% on PointDA. The code is\navailable at https://github.com/ispc-lab/BMD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_S/0/1/0/all/0/1\">Sanqing Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhijun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ShowFace: Coordinated Face Inpainting with Memory-Disentangled Refinement Networks. (arXiv:2204.02824v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02824","description":"<p>Face inpainting aims to complete the corrupted regions of the face images,\nwhich requires coordination between the completed areas and the non-corrupted\nareas. Recently, memory-oriented methods illustrate great prospects in the\ngeneration related tasks by introducing an external memory module to improve\nimage coordination. However, such methods still have limitations in restoring\nthe consistency and continuity for specificfacial semantic parts. In this\npaper, we propose the coarse-to-fine Memory-Disentangled Refinement Networks\n(MDRNets) for coordinated face inpainting, in which two collaborative modules\nare integrated, Disentangled Memory Module (DMM) and Mask-Region Enhanced\nModule (MREM). Specifically, the DMM establishes a group of disentangled memory\nblocks to store the semantic-decoupled face representations, which could\nprovide the most relevant information to refine the semantic-level\ncoordination. The MREM involves a masked correlation mining mechanism to\nenhance the feature relationships into the corrupted regions, which could also\nmake up for the correlation loss caused by memory disentanglement. Furthermore,\nto better improve the inter-coordination between the corrupted and\nnon-corrupted regions and enhance the intra-coordination in corrupted regions,\nwe design InCo2 Loss, a pair of similarity based losses to constrain the\nfeature consistency. Eventually, extensive experiments conducted on CelebA-HQ\nand FFHQ datasets demonstrate the superiority of our MDRNets compared with\nprevious State-Of-The-Art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhuojie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xingqun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wanting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Muyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of Remote Sensing Pretraining. (arXiv:2204.02825v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02825","description":"<p>Deep learning has largely reshaped remote sensing research for aerial image\nunderstanding. Nevertheless, most of existing deep models are initialized with\nImageNet pretrained weights, where the natural images inevitably presents a\nlarge domain gap relative to the aerial images, probably limiting the\nfinetuning performance on downstream aerial scene tasks. This issue motivates\nus to conduct an empirical study of remote sensing pretraining (RSP). To this\nend, we train different networks from scratch with the help of the largest\nremote sensing scene recognition dataset up to now-MillionAID, to obtain the\nremote sensing pretrained backbones, including both convolutional neural\nnetworks (CNN) and vision transformers such as Swin and ViTAE, which have shown\npromising performance on computer vision tasks. Then, we investigate the impact\nof ImageNet pretraining (IMP) and RSP on a series of downstream tasks including\nscene recognition, semantic segmentation, object detection, and change\ndetection using the CNN and vision transformers backbones. We have some\nempirical findings as follows. First, vision transformers generally outperforms\nCNN backbones, where ViTAE achieves the best performance, owing to its strong\nrepresentation capacity by introducing intrinsic inductive bias from\nconvolutions to transformers. Second, both IMP and RSP help deliver better\nperformance, where IMP enjoys a versatility by learning more universal\nrepresentations from diverse images belonging to much more categories while RSP\nis distinctive in perceiving remote sensing related semantics. Third, RSP\nmitigates the data discrepancy of IMP for remote sensing but may still suffer\nfrom the task discrepancy, where downstream tasks require different\nrepresentations from the scene recognition task. These findings call for\nfurther research efforts on both large-scale pretraining datasets and effective\npretraining methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CCAT-NET: A Novel Transformer Based Semi-supervised Framework for Covid-19 Lung Lesion Segmentation. (arXiv:2204.02839v1 [eess.IV])","link":"http://arxiv.org/abs/2204.02839","description":"<p>The spread of the novel coronavirus disease 2019 (COVID-19) has claimed\nmillions of lives. Automatic segmentation of lesions from CT images can assist\ndoctors with screening, treatment, and monitoring. However, accurate\nsegmentation of lesions from CT images can be very challenging due to data and\nmodel limitations. Recently, Transformer-based networks have attracted a lot of\nattention in the area of computer vision, as Transformer outperforms CNN at a\nbunch of tasks. In this work, we propose a novel network structure that\ncombines CNN and Transformer for the segmentation of COVID-19 lesions. We\nfurther propose an efficient semi-supervised learning framework to address the\nshortage of labeled data. Extensive experiments showed that our proposed\nnetwork outperforms most existing networks and the semi-supervised learning\nframework can outperform the base network by 3.0% and 8.2% in terms of Dice\ncoefficient and sensitivity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_M/0/1/0/all/0/1\">Mingyang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_L/0/1/0/all/0/1\">Li Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_H/0/1/0/all/0/1\">Huiqin Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Source Tools for Behavioral Video Analysis: Setup, Methods, and Development. (arXiv:2204.02842v1 [q-bio.QM])","link":"http://arxiv.org/abs/2204.02842","description":"<p>Recently developed methods for video analysis, especially models for pose\nestimation and behavior classification, are transforming behavioral\nquantification to be more precise, scalable, and reproducible in fields such as\nneuroscience and ethology. These tools overcome long-standing limitations of\nmanual scoring of video frames and traditional \"center of mass\" tracking\nalgorithms to enable video analysis at scale. The expansion of open-source\ntools for video acquisition and analysis has led to new experimental approaches\nto understand behavior. Here, we review currently available open source tools\nfor video analysis, how to set them up in a lab that is new to video recording\nmethods, and some issues that should be addressed by developers and advanced\nusers, including the need to openly share datasets and code, how to compare\nalgorithms and their parameters, and the need for documentation and\ncommunity-wide standards. We hope to encourage more widespread use and\ncontinued development of the tools. They have tremendous potential for\naccelerating scientific progress for understanding the brain and behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Luxem_K/0/1/0/all/0/1\">Kevin Luxem</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sun_J/0/1/0/all/0/1\">Jennifer J. Sun</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bradley_S/0/1/0/all/0/1\">Sean P. Bradley</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Krishnan_K/0/1/0/all/0/1\">Keerthi Krishnan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pereira_T/0/1/0/all/0/1\">Talmo D. Pereira</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yttri_E/0/1/0/all/0/1\">Eric A. Yttri</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zimmermann_J/0/1/0/all/0/1\">Jan Zimmermann</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Laubach_M/0/1/0/all/0/1\">Mark Laubach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Generate Realistic Noisy Images via Pixel-level Noise-aware Adversarial Training. (arXiv:2204.02844v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02844","description":"<p>Existing deep learning real denoising methods require a large amount of\nnoisy-clean image pairs for supervision. Nonetheless, capturing a real\nnoisy-clean dataset is an unacceptable expensive and cumbersome procedure. To\nalleviate this problem, this work investigates how to generate realistic noisy\nimages. Firstly, we formulate a simple yet reasonable noise model that treats\neach real noisy pixel as a random variable. This model splits the noisy image\ngeneration problem into two sub-problems: image domain alignment and noise\ndomain alignment. Subsequently, we propose a novel framework, namely\nPixel-level Noise-aware Generative Adversarial Network (PNGAN). PNGAN employs a\npre-trained real denoiser to map the fake and real noisy images into a nearly\nnoise-free solution space to perform image domain alignment. Simultaneously,\nPNGAN establishes a pixel-level adversarial training to conduct noise domain\nalignment. Additionally, for better noise fitting, we present an efficient\narchitecture Simple Multi-scale Network (SMNet) as the generator. Qualitative\nvalidation shows that noise generated by PNGAN is highly similar to real noise\nin terms of intensity and distribution. Quantitative experiments demonstrate\nthat a series of denoisers trained with the generated noisy images achieve\nstate-of-the-art (SOTA) results on four real denoising benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanhao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Donglai Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KNN-Diffusion: Image Generation via Large-Scale Retrieval. (arXiv:2204.02849v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02849","description":"<p>While the availability of massive Text-Image datasets is shown to be\nextremely useful in training large-scale generative models (e.g. DDPMs,\nTransformers), their output typically depends on the quality of both the input\ntext, as well as the training dataset. In this work, we show how large-scale\nretrieval methods, in particular efficient K-Nearest-Neighbors (KNN) search,\ncan be used in order to train a model to adapt to new samples. Learning to\nadapt enables several new capabilities. Sifting through billions of records at\ninference time is extremely efficient and can alleviate the need to train or\nmemorize an adequately large generative model. Additionally, fine-tuning\ntrained models to new samples can be achieved by simply adding them to the\ntable. Rare concepts, even without any presence in the training set, can be\nthen leveraged during test time without any modification to the generative\nmodel. Our diffusion-based model trains on images only, by leveraging a joint\nText-Image multi-modal metric. Compared to baseline methods, our generations\nachieve state of the art results both in human evaluations as well as with\nperceptual scores when tested on a public multimodal dataset of natural images,\nas well as on a collected dataset of 400 million Stickers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ashual_O/0/1/0/all/0/1\">Oron Ashual</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheynin_S/0/1/0/all/0/1\">Shelly Sheynin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singer_U/0/1/0/all/0/1\">Uriel Singer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gafni_O/0/1/0/all/0/1\">Oran Gafni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachmani_E/0/1/0/all/0/1\">Eliya Nachmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taigman_Y/0/1/0/all/0/1\">Yaniv Taigman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Influence of Color Spaces for Deep Learning Image Colorization. (arXiv:2204.02850v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02850","description":"<p>Colorization is a process that converts a grayscale image into a color one\nthat looks as natural as possible. Over the years this task has received a lot\nof attention. Existing colorization methods rely on different color spaces:\nRGB, YUV, Lab, etc. In this chapter, we aim to study their influence on the\nresults obtained by training a deep neural network, to answer the question: \"Is\nit crucial to correctly choose the right color space in deep-learning based\ncolorization?\". First, we briefly summarize the literature and, in particular,\ndeep learning-based methods. We then compare the results obtained with the same\ndeep neural network architecture with RGB, YUV and Lab color spaces.\nQualitative and quantitative analysis do not conclude similarly on which color\nspace is better. We then show the importance of carefully designing the\narchitecture and evaluation protocols depending on the types of images that are\nbeing processed and their specificities: strong/small contours, few/many\nobjects, recent/archive images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ballester_C/0/1/0/all/0/1\">Coloma Ballester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugeau_A/0/1/0/all/0/1\">Aur&#xe9;lie Bugeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrillo_H/0/1/0/all/0/1\">Hernan Carrillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clement_M/0/1/0/all/0/1\">Micha&#xeb;l Cl&#xe9;ment</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giraud_R/0/1/0/all/0/1\">R&#xe9;mi Giraud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raad_L/0/1/0/all/0/1\">Lara Raad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitoria_P/0/1/0/all/0/1\">Patricia Vitoria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-based Spatially Adaptive Normalization for Semantic Image Synthesis. (arXiv:2204.02854v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02854","description":"<p>Semantic image synthesis is a challenging task with many practical\napplications. Albeit remarkable progress has been made in semantic image\nsynthesis with spatially-adaptive normalization and existing methods normalize\nthe feature activations under the coarse-level guidance (e.g., semantic class).\nHowever, different parts of a semantic object (e.g., wheel and window of car)\nare quite different in structures and textures, making blurry synthesis results\nusually inevitable due to the missing of fine-grained guidance. In this paper,\nwe propose a novel normalization module, termed as REtrieval-based Spatially\nAdaptIve normaLization (RESAIL), for introducing pixel level fine-grained\nguidance to the normalization architecture. Specifically, we first present a\nretrieval paradigm by finding a content patch of the same semantic class from\ntraining set with the most similar shape to each test semantic mask. Then,\nRESAIL is presented to use the retrieved patch for guiding the feature\nnormalization of corresponding region, and can provide pixel level fine-grained\nguidance, thereby greatly mitigating blurry synthesis results. Moreover,\ndistorted ground-truth images are also utilized as alternatives of\nretrieval-based guidance for feature normalization, further benefiting model\ntraining and improving visual quality of generated images. Experiments on\nseveral challenging datasets show that our RESAIL performs favorably against\nstate-of-the-arts in terms of quantitative metrics, visual quality, and\nsubjective evaluation. The source code and pre-trained models will be publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yupeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yuxiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Demonstrate Once, Imitate Immediately (DOME): Learning Visual Servoing for One-Shot Imitation Learning. (arXiv:2204.02863v1 [cs.RO])","link":"http://arxiv.org/abs/2204.02863","description":"<p>We present DOME, a novel method for one-shot imitation learning, where a task\ncan be learned from just a single demonstration and then be deployed\nimmediately, without any further data collection or training. DOME does not\nrequire prior task or object knowledge, and can perform the task in novel\nobject configurations and with distractors. At its core, DOME uses an\nimage-conditioned object segmentation network followed by a learned visual\nservoing network, to move the robot's end-effector to the same relative pose to\nthe object as during the demonstration, after which the task can be completed\nby replaying the demonstration's end-effector velocities. We show that DOME\nachieves near 100% success rate on 7 real-world everyday tasks, and we perform\nseveral studies to thoroughly understand each individual component of DOME.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valassakis_E/0/1/0/all/0/1\">Eugene Valassakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papagiannis_G/0/1/0/all/0/1\">Georgios Papagiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palo_N/0/1/0/all/0/1\">Norman Di Palo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1\">Edward Johns</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound. (arXiv:2204.02874v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02874","description":"<p>We introduce an audiovisual method for long-range text-to-video retrieval.\nUnlike previous approaches designed for short video retrieval (e.g., 5-15\nseconds in duration), our approach aims to retrieve minute-long videos that\ncapture complex human actions. One challenge of standard video-only approaches\nis the large computational cost associated with processing hundreds of densely\nextracted frames from such long videos. To address this issue, we propose to\nreplace parts of the video with compact audio cues that succinctly summarize\ndynamic audio events and are cheap to process. Our method, named ECLIPSE\n(Efficient CLIP with Sound Encoding), adapts the popular CLIP model to an\naudiovisual video setting, by adding a unified audiovisual transformer block\nthat captures complementary cues from the video and audio streams. In addition\nto being 2.92x faster and 2.34x memory-efficient than long-range video-only\napproaches, our method also achieves better text-to-video retrieval accuracy on\nseveral diverse long-range video datasets such as ActivityNet, QVHighlights,\nYouCook2, DiDeMo and Charades.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yan-Bo Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial Attacks. (arXiv:2204.02887v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02887","description":"<p>Deep neural networks have shown to be very vulnerable to adversarial examples\ncrafted by adding human-imperceptible perturbations to benign inputs. After\nachieving impressive attack success rates in the white-box setting, more focus\nis shifted to black-box attacks. In either case, the common gradient-based\napproaches generally use the $sign$ function to generate perturbations at the\nend of the process. However, only a few works pay attention to the limitation\nof the $sign$ function. Deviation between the original gradient and the\ngenerated noises may lead to inaccurate gradient update estimation and\nsuboptimal solutions for adversarial transferability, which is crucial for\nblack-box attacks. To address this issue, we propose a Sampling-based Fast\nGradient Rescaling Method (S-FGRM) to improve the transferability of the\ncrafted adversarial examples. Specifically, we use data rescaling to substitute\nthe inefficient $sign$ function in gradient-based attacks without extra\ncomputational cost. We also propose a Depth First Sampling method to eliminate\nthe fluctuation of rescaling and stabilize the gradient update. Our method can\nbe used in any gradient-based optimizations and is extensible to be integrated\nwith various input transformation or ensemble methods for further improving the\nadversarial transferability. Extensive experiments on the standard ImageNet\ndataset show that our S-FGRM could significantly boost the transferability of\ngradient-based attacks and outperform the state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anmin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yifeng Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yanbo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DBF: Dynamic Belief Fusion for Combining Multiple Object Detectors. (arXiv:2204.02890v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02890","description":"<p>In this paper, we propose a novel and highly practical score-level fusion\napproach called dynamic belief fusion (DBF) that directly integrates inference\nscores of individual detections from multiple object detection methods. To\neffectively integrate the individual outputs of multiple detectors, the level\nof ambiguity in each detection score is estimated using a confidence model\nbuilt on a precision-recall relationship of the corresponding detector. For\neach detector output, DBF then calculates the probabilities of three hypotheses\n(target, non-target, and intermediate state (target or non-target)) based on\nthe confidence level of the detection score conditioned on the prior confidence\nmodel of individual detectors, which is referred to as basic probability\nassignment. The probability distributions over three hypotheses of all the\ndetectors are optimally fused via the Dempster's combination rule. Experiments\non the ARL, PASCAL VOC 07, and 12 datasets show that the detection accuracy of\nthe DBF is significantly higher than any of the baseline fusion approaches as\nwell as individual detectors used for the fusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyungtae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1\">Heesung Kwon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Instance Edge Detection. (arXiv:2204.02898v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02898","description":"<p>Edge detection has long been an important problem in the field of computer\nvision. Previous works have explored category-agnostic or category-aware edge\ndetection. In this paper, we explore edge detection in the context of object\ninstances. Although object boundaries could be easily derived from segmentation\nmasks, in practice, instance segmentation models are trained to maximize IoU to\nthe ground-truth mask, which means that segmentation boundaries are not\nenforced to precisely align with ground-truth edge boundaries. Thus, the task\nof instance edge detection itself is different and critical. Since precise edge\ndetection requires high resolution feature maps, we design a novel transformer\narchitecture that efficiently combines a FPN and a transformer decoder to\nenable cross attention on multi-scale high resolution feature maps within a\nreasonable computation budget. Further, we propose a light weight dense\nprediction head that is applicable to both instance edge and mask detection.\nFinally, we use a penalty reduced focal loss to effectively train the model\nwith point supervision on instance edges, which can reduce annotation costs. We\ndemonstrate highly competitive instance edge detection performance compared to\nstate-of-the-art baselines, and also show that the proposed task and loss are\ncomplementary to instance segmentation and object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1\">Xueyan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haotian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Jae Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of End-to-End Temporal Action Detection. (arXiv:2204.02932v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02932","description":"<p>Temporal action detection (TAD) is an important yet challenging task in video\nunderstanding. It aims to simultaneously predict the semantic label and the\ntemporal interval of every action instance in an untrimmed video. Rather than\nend-to-end learning, most existing methods adopt a head-only learning paradigm,\nwhere the video encoder is pre-trained for action classification, and only the\ndetection head upon the encoder is optimized for TAD. The effect of end-to-end\nlearning is not systematically evaluated. Besides, there lacks an in-depth\nstudy on the efficiency-accuracy trade-off in end-to-end TAD. In this paper, we\npresent an empirical study of end-to-end temporal action detection. We validate\nthe advantage of end-to-end learning over head-only learning and observe up to\n11\\% performance improvement. Besides, we study the effects of multiple design\nchoices that affect the TAD performance and speed, including detection head,\nvideo encoder, and resolution of input videos. Based on the findings, we build\na mid-resolution baseline detector, which achieves the state-of-the-art\nperformance of end-to-end methods while running more than 4$\\times$ faster. We\nhope that this paper can serve as a guide for end-to-end learning and inspire\nfuture research in this field. Code and models are available at\n\\url{https://github.com/xlliu7/E2E-TAD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaolong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations. (arXiv:2204.02937v1 [cs.LG])","link":"http://arxiv.org/abs/2204.02937","description":"<p>Neural network classifiers can largely rely on simple spurious features, such\nas backgrounds, to make predictions. However, even in these cases, we show that\nthey still often learn core features associated with the desired attributes of\nthe data, contrary to recent findings. Inspired by this insight, we demonstrate\nthat simple last layer retraining can match or outperform state-of-the-art\napproaches on spurious correlation benchmarks, but with profoundly lower\ncomplexity and computational expenses. Moreover, we show that last layer\nretraining on large ImageNet-trained models can also significantly reduce\nreliance on background and texture information, improving robustness to\ncovariate shift, after only minutes of training on a single GPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirichenko_P/0/1/0/all/0/1\">Polina Kirichenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izmailov_P/0/1/0/all/0/1\">Pavel Izmailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1\">Andrew Gordon Wilson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S-R2F2U-Net: A single-stage model for teeth segmentation. (arXiv:2204.02939v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02939","description":"<p>Precision tooth segmentation is crucial in the oral sector because it\nprovides location information for orthodontic therapy, clinical diagnosis, and\nsurgical treatments. In this paper, we investigate residual, recurrent, and\nattention networks to segment teeth from panoramic dental images. Based on our\nfindings, we suggest three single-stage models: Single Recurrent R2U-Net\n(S-R2U-Net), Single Recurrent Filter Double R2U-Net (S-R2F2U-Net), and Single\nRecurrent Attention Enabled Filter Double (S-R2F2-Attn-U-Net). Particularly,\nS-R2F2U-Net outperforms state-of-the-art models in terms of accuracy and dice\nscore. A hybrid loss function combining the cross-entropy loss and dice loss is\nused to train the model. In addition, it reduces around 45% of model parameters\ncompared to the R2U-Net model. Models are trained and evaluated on a benchmark\ndataset containing 1500 dental panoramic X-ray images. S-R2F2U-Net achieves\n97.31% of accuracy and 93.26% of dice score, showing superiority over the\nstate-of-the-art methods. Codes are available at\nhttps://github.com/mrinal054/teethSeg_sr2f2u-net.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhar_M/0/1/0/all/0/1\">Mrinal Kanti Dhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zeyun Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intervertebral Disc Labeling With Learning Shape Information, A Look Once Approach. (arXiv:2204.02943v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02943","description":"<p>Accurate and automatic segmentation of intervertebral discs from medical\nimages is a critical task for the assessment of spine-related diseases such as\nosteoporosis, vertebral fractures, and intervertebral disc herniation. To date,\nvarious approaches have been developed in the literature which routinely relies\non detecting the discs as the primary step. A disadvantage of many cohort\nstudies is that the localization algorithm also yields false-positive\ndetections. In this study, we aim to alleviate this problem by proposing a\nnovel U-Net-based structure to predict a set of candidates for intervertebral\ndisc locations. In our design, we integrate the image shape information (image\ngradients) to encourage the model to learn rich and generic geometrical\ninformation. This additional signal guides the model to selectively emphasize\nthe contextual representation and suppress the less discriminative features. On\nthe post-processing side, to further decrease the false positive rate, we\npropose a permutation invariant 'look once' model, which accelerates the\ncandidate recovery procedure. In comparison with previous studies, our proposed\napproach does not need to perform the selection in an iterative fashion. The\nproposed method was evaluated on the spine generic public multi-center dataset\nand demonstrated superior performance compared to previous work. We have\nprovided the implementation code in\nhttps://github.com/rezazad68/intervertebral-lookonce\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azad_R/0/1/0/all/0/1\">Reza Azad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidari_M/0/1/0/all/0/1\">Moein Heidari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Adad_J/0/1/0/all/0/1\">Julien Cohen-Adad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merhof_D/0/1/0/all/0/1\">Dorit Merhof</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"The Pedestrian next to the Lamppost\" Adaptive Object Graphs for Better Instantaneous Mapping. (arXiv:2204.02944v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02944","description":"<p>Estimating a semantically segmented bird's-eye-view (BEV) map from a single\nimage has become a popular technique for autonomous control and navigation.\nHowever, they show an increase in localization error with distance from the\ncamera. While such an increase in error is entirely expected - localization is\nharder at distance - much of the drop in performance can be attributed to the\ncues used by current texture-based models, in particular, they make heavy use\nof object-ground intersections (such as shadows), which become increasingly\nsparse and uncertain for distant objects. In this work, we address these\nshortcomings in BEV-mapping by learning the spatial relationship between\nobjects in a scene. We propose a graph neural network which predicts BEV\nobjects from a monocular image by spatially reasoning about an object within\nthe context of other objects. Our approach sets a new state-of-the-art in BEV\nestimation from monocular images across three large-scale datasets, including a\n50% relative improvement for objects on nuScenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Avishkar Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_O/0/1/0/all/0/1\">Oscar Mendez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1\">Chris Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Demoireing with Relation-Based Temporal Consistency. (arXiv:2204.02957v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02957","description":"<p>Moire patterns, appearing as color distortions, severely degrade image and\nvideo qualities when filming a screen with digital cameras. Considering the\nincreasing demands for capturing videos, we study how to remove such\nundesirable moire patterns in videos, namely video demoireing. To this end, we\nintroduce the first hand-held video demoireing dataset with a dedicated data\ncollection pipeline to ensure spatial and temporal alignments of captured data.\nFurther, a baseline video demoireing model with implicit feature space\nalignment and selective feature aggregation is developed to leverage\ncomplementary information from nearby frames to improve frame-level video\ndemoireing. More importantly, we propose a relation-based temporal consistency\nloss to encourage the model to learn temporal consistency priors directly from\nground-truth reference videos, which facilitates producing temporally\nconsistent predictions and effectively maintains frame-level qualities.\nExtensive experiments manifest the superiority of our model. Code is available\nat \\url{https://daipengwa.github.io/VDmoire_ProjectPage/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1\">Peng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baoheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiajun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEAD: Self-Supervised Landmark Estimation by Aligning Distributions of Feature Similarity. (arXiv:2204.02958v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02958","description":"<p>In this work, we introduce LEAD, an approach to discover landmarks from an\nunannotated collection of category-specific images. Existing works in\nself-supervised landmark detection are based on learning dense (pixel-level)\nfeature representations from an image, which are further used to learn\nlandmarks in a semi-supervised manner. While there have been advances in\nself-supervised learning of image features for instance-level tasks like\nclassification, these methods do not ensure dense equivariant representations.\nThe property of equivariance is of interest for dense prediction tasks like\nlandmark estimation. In this work, we introduce an approach to enhance the\nlearning of dense equivariant representations in a self-supervised fashion. We\nfollow a two-stage training approach: first, we train a network using the BYOL\nobjective which operates at an instance level. The correspondences obtained\nthrough this network are further used to train a dense and compact\nrepresentation of the image using a lightweight network. We show that having\nsuch a prior in the feature extractor helps in landmark detection, even under\ndrastically limited number of annotations while also improving generalization\nacross scale variations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karmali_T/0/1/0/all/0/1\">Tejan Karmali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atrishi_A/0/1/0/all/0/1\">Abhinav Atrishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harsha_S/0/1/0/all/0/1\">Sai Sree Harsha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Susmit Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1\">R. Venkatesh Babu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple and Effective Synthesis of Indoor 3D Scenes. (arXiv:2204.02960v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02960","description":"<p>We study the problem of synthesizing immersive 3D indoor scenes from one or\nmore images. Our aim is to generate high-resolution images and videos from\nnovel viewpoints, including viewpoints that extrapolate far beyond the input\nimages while maintaining 3D consistency. Existing approaches are highly\ncomplex, with many separately trained stages and components. We propose a\nsimple alternative: an image-to-image GAN that maps directly from reprojections\nof incomplete point clouds to full high-resolution RGB-D images. On the\nMatterport3D and RealEstate10K datasets, our approach significantly outperforms\nprior work when evaluated by humans, as well as on FID scores. Further, we show\nthat our model is useful for generative data augmentation. A\nvision-and-language navigation (VLN) agent trained with trajectories\nspatially-perturbed by our model improves success rate by up to 1.5% over a\nstate of the art baseline on the R2R benchmark. Our code will be made available\nto facilitate generative data augmentation and applications to downstream\nrobotics and embodied AI tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1\">Jing Yu Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_H/0/1/0/all/0/1\">Harsh Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_R/0/1/0/all/0/1\">Richard Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waters_A/0/1/0/all/0/1\">Austin Waters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1\">Peter Anderson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMU-Net: Style matching U-Net for brain tumor segmentation with missing modalities. (arXiv:2204.02961v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02961","description":"<p>Gliomas are one of the most prevalent types of primary brain tumours,\naccounting for more than 30\\% of all cases and they develop from the glial stem\nor progenitor cells. In theory, the majority of brain tumours could well be\nidentified exclusively by the use of Magnetic Resonance Imaging (MRI). Each MRI\nmodality delivers distinct information on the soft tissue of the human brain\nand integrating all of them would provide comprehensive data for the accurate\nsegmentation of the glioma, which is crucial for the patient's prognosis,\ndiagnosis, and determining the best follow-up treatment. Unfortunately, MRI is\nprone to artifacts for a variety of reasons, which might result in missing one\nor more MRI modalities. Various strategies have been proposed over the years to\nsynthesize the missing modality or compensate for the influence it has on\nautomated segmentation models. However, these methods usually fail to model the\nunderlying missing information. In this paper, we propose a style matching\nU-Net (SMU-Net) for brain tumour segmentation on MRI images. Our co-training\napproach utilizes a content and style-matching mechanism to distill the\ninformative features from the full-modality network into a missing modality\nnetwork. To do so, we encode both full-modality and missing-modality data into\na latent space, then we decompose the representation space into a style and\ncontent representation. Our style matching module adaptively recalibrates the\nrepresentation space by learning a matching function to transfer the\ninformative and textural features from a full-modality path into a\nmissing-modality path. Moreover, by modelling the mutual information, our\ncontent module surpasses the less informative features and re-calibrates the\nrepresentation space based on discriminative semantic features. The evaluation\nprocess on the BraTS 2018 dataset shows a significant results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azad_R/0/1/0/all/0/1\">Reza Azad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_N/0/1/0/all/0/1\">Nika Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merhof_D/0/1/0/all/0/1\">Dorit Merhof</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unleashing Vanilla Vision Transformer with Masked Image Modeling for Object Detection. (arXiv:2204.02964v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02964","description":"<p>We present an approach to efficiently and effectively adapt a masked image\nmodeling (MIM) pre-trained vanilla Vision Transformer (ViT) for object\ndetection, which is based on our two novel observations: (i) A MIM pre-trained\nvanilla ViT can work surprisingly well in the challenging object-level\nrecognition scenario even with random sampled partial observations, e.g., only\n25% ~ 50% of the input sequence. (ii) In order to construct multi-scale\nrepresentations for object detection, a random initialized compact\nconvolutional stem supplants the pre-trained large kernel patchify stem, and\nits intermediate features can naturally serve as the higher resolution inputs\nof a feature pyramid without upsampling. While the pre-trained ViT is only\nregarded as the third-stage of our detector's backbone instead of the whole\nfeature extractor, resulting in a ConvNet-ViT hybrid architecture. The proposed\ndetector, named MIMDet, enables a MIM pre-trained vanilla ViT to outperform\nhierarchical Swin Transformer by 2.3 box AP and 2.5 mask AP on COCO, and\nachieve even better results compared with other adapted vanilla ViT using a\nmore modest fine-tuning recipe while converging 2.8x faster. Code and\npre-trained models are available at \\url{https://github.com/hustvl/MIMDet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuxin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shusheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LilNetX: Lightweight Networks with EXtreme Model Compression and Structured Sparsification. (arXiv:2204.02965v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02965","description":"<p>We introduce LilNetX, an end-to-end trainable technique for neural networks\nthat enables learning models with specified accuracy-rate-computation\ntrade-off. Prior works approach these problems one at a time and often require\npost-processing or multistage training which become less practical and do not\nscale very well for large datasets or architectures. Our method constructs a\njoint training objective that penalizes the self-information of network\nparameters in a reparameterized latent space to encourage small model size\nwhile also introducing priors to increase structured sparsity in the parameter\nspace to reduce computation. We achieve up to 50% smaller model size and 98%\nmodel sparsity on ResNet-20 while retaining the same accuracy on the CIFAR-10\ndataset as well as 35% smaller model size and 42% structured sparsity on\nResNet-50 trained on ImageNet, when compared to existing state-of-the-art model\ncompression methods. Code is available at\nhttps://github.com/Sharath-girish/LilNetX.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Girish_S/0/1/0/all/0/1\">Sharath Girish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kamal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Saurabh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Alignment Networks for Long-term Video. (arXiv:2204.02968v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02968","description":"<p>The objective of this paper is a temporal alignment network that ingests long\nterm video sequences, and associated text sentences, in order to: (1) determine\nif a sentence is alignable with the video; and (2) if it is alignable, then\ndetermine its alignment. The challenge is to train such networks from\nlarge-scale datasets, such as HowTo100M, where the associated text sentences\nhave significant noise, and are only weakly aligned when relevant. Apart from\nproposing the alignment network, we also make four contributions: (i) we\ndescribe a novel co-training method that enables to denoise and train on raw\ninstructional videos without using manual annotation, despite the considerable\nnoise; (ii) to benchmark the alignment performance, we manually curate a\n10-hour subset of HowTo100M, totalling 80 videos, with sparse temporal\ndescriptions. Our proposed model, trained on HowTo100M, outperforms strong\nbaselines (CLIP, MIL-NCE) on this alignment dataset by a significant margin;\n(iii) we apply the trained model in the zero-shot settings to multiple\ndownstream video understanding tasks and achieve state-of-the-art results,\nincluding text-video retrieval on YouCook2, and weakly supervised video action\nsegmentation on Breakfast-Action; (iv) we use the automatically aligned\nHowTo100M annotations for end-to-end finetuning of the backbone model, and\nobtain improved performance on downstream action recognition tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tengda Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ME R-CNN: Multi-Expert R-CNN for Object Detection. (arXiv:1704.01069v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1704.01069","description":"<p>We introduce Multi-Expert Region-based Convolutional Neural Network (ME\nR-CNN) which is equipped with multiple experts (ME) where each expert is\nlearned to process a certain type of regions of interest (RoIs). This\narchitecture better captures the appearance variations of the RoIs caused by\ndifferent shapes, poses, and viewing angles. In order to direct each RoI to the\nappropriate expert, we devise a novel \"learnable\" network, which we call,\nexpert assignment network (EAN). EAN automatically learns the optimal\nRoI-expert relationship even without any supervision of expert assignment. As\nthe major components of ME R-CNN, ME and EAN, are mutually affecting each other\nwhile tied to a shared network, neither an alternating nor a naive end-to-end\noptimization is likely to fail. To address this problem, we introduce a\npractical training strategy which is tailored to optimize ME, EAN, and the\nshared network in an end-to-end fashion. We show that both of the architectures\nprovide considerable performance increase over the baselines on PASCAL VOC 07,\n12, and MS COCO datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyungtae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eum_S/0/1/0/all/0/1\">Sungmin Eum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1\">Heesung Kwon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning V1 Simple Cells with Vector Representation of Local Content and Matrix Representation of Local Motion. (arXiv:1902.03871v5 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/1902.03871","description":"<p>This paper proposes a representational model for image pairs such as\nconsecutive video frames that are related by local pixel displacements, in the\nhope that the model may shed light on motion perception in primary visual\ncortex (V1). The model couples the following two components: (1) the vector\nrepresentations of local contents of images and (2) the matrix representations\nof local pixel displacements caused by the relative motions between the agent\nand the objects in the 3D scene. When the image frame undergoes changes due to\nlocal pixel displacements, the vectors are multiplied by the matrices that\nrepresent the local displacements. Thus the vector representation is\nequivariant as it varies according to the local displacements. Our experiments\nshow that our model can learn Gabor-like filter pairs of quadrature phases. The\nprofiles of the learned filters match those of simple cells in Macaque V1.\nMoreover, we demonstrate that the model can learn to infer local motions in\neither a supervised or unsupervised manner. With such a simple model, we\nachieve competitive results on optical flow estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruiqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jianwen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yufan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ktrain: A Low-Code Library for Augmented Machine Learning. (arXiv:2004.10703v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2004.10703","description":"<p>We present ktrain, a low-code Python library that makes machine learning more\naccessible and easier to apply. As a wrapper to TensorFlow and many other\nlibraries (e.g., transformers, scikit-learn, stellargraph), it is designed to\nmake sophisticated, state-of-the-art machine learning models simple to build,\ntrain, inspect, and apply by both beginners and experienced practitioners.\nFeaturing modules that support text data (e.g., text classification, sequence\ntagging, open-domain question-answering), vision data (e.g., image\nclassification), graph data (e.g., node classification, link prediction), and\ntabular data, ktrain presents a simple unified interface enabling one to\nquickly solve a wide range of tasks in as little as three or four \"commands\" or\nlines of code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1\">Arun S. Maiya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Adversarial Learnable Filters. (arXiv:2008.06069v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.06069","description":"<p>We present an adversarial framework to craft perturbations that mislead\nclassifiers by accounting for the image content and the semantics of the\nlabels. The proposed framework combines a structure loss and a semantic\nadversarial loss in a multi-task objective function to train a fully\nconvolutional neural network. The structure loss helps generate perturbations\nwhose type and magnitude are defined by a target image processing filter. The\nsemantic adversarial loss considers groups of (semantic) labels to craft\nperturbations that prevent the filtered image {from} being classified with a\nlabel in the same group. We validate our framework with three different target\nfilters, namely detail enhancement, log transformation and gamma correction\nfilters; and evaluate the adversarially filtered images against three\nclassifiers, ResNet50, ResNet18 and AlexNet, pre-trained on ImageNet. We show\nthat the proposed framework generates filtered images with a high success rate,\nrobustness, and transferability to unseen classifiers. We also discuss\nobjective and subjective evaluations of the adversarial perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shamsabadi_A/0/1/0/all/0/1\">Ali Shahin Shamsabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1\">Changjae Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1\">Andrea Cavallaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video action recognition for lane-change classification and prediction of surrounding vehicles. (arXiv:2101.05043v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.05043","description":"<p>In highway scenarios, an alert human driver will typically anticipate early\ncut-in/cut-out maneuvers of surrounding vehicles using visual cues mainly.\nAutonomous vehicles must anticipate these situations at an early stage too, to\nincrease their safety and efficiency. In this work, lane-change recognition and\nprediction tasks are posed as video action recognition problems. Up to four\ndifferent two-stream-based approaches, that have been successfully applied to\naddress human action recognition, are adapted here by stacking visual cues from\nforward-looking video cameras to recognize and anticipate lane-changes of\ntarget vehicles. We study the influence of context and observation horizons on\nperformance, and different prediction horizons are analyzed. The different\nmodels are trained and evaluated using the PREVENTION dataset. The obtained\nresults clearly demonstrate the potential of these methodologies to serve as\nrobust predictors of future lane-changes of surrounding vehicles proving an\naccuracy higher than 90% in time horizons of between 1-2 seconds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biparva_M/0/1/0/all/0/1\">Mahdi Biparva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Llorca_D/0/1/0/all/0/1\">David Fern&#xe1;ndez-Llorca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izquierdo_Gonzalo_R/0/1/0/all/0/1\">Rub&#xe9;n Izquierdo-Gonzalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsotsos_J/0/1/0/all/0/1\">John K. Tsotsos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeRF--: Neural Radiance Fields Without Known Camera Parameters. (arXiv:2102.07064v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.07064","description":"<p>Considering the problem of novel view synthesis (NVS) from only a set of 2D\nimages, we simplify the training process of Neural Radiance Field (NeRF) on\nforward-facing scenes by removing the requirement of known or pre-computed\ncamera parameters, including both intrinsics and 6DoF poses. To this end, we\npropose NeRF$--$, with three contributions: First, we show that the camera\nparameters can be jointly optimised as learnable parameters with NeRF training,\nthrough a photometric reconstruction; Second, to benchmark the camera parameter\nestimation and the quality of novel view renderings, we introduce a new dataset\nof path-traced synthetic scenes, termed as Blender Forward-Facing Dataset\n(BLEFF); Third, we conduct extensive analyses to understand the training\nbehaviours under various camera motions, and show that in most scenarios, the\njoint optimisation pipeline can recover accurate camera parameters and achieve\ncomparable novel view synthesis quality as those trained with COLMAP\npre-computed camera parameters. Our code and data are available at\nhttps://nerfmm.active.vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shangzhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Min Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prisacariu_V/0/1/0/all/0/1\">Victor Adrian Prisacariu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Calibrated-Guidance for Object Detection in Aerial Images. (arXiv:2103.11399v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11399","description":"<p>Object detection is one of the most fundamental yet challenging research\ntopics in the domain of computer vision. Recently, the study on this topic in\naerial images has made tremendous progress. However, complex background and\nworse imaging quality are obvious problems in aerial object detection. Most\nstate-of-the-art approaches tend to develop elaborate attention mechanisms for\nthe space-time feature calibrations with arduous computational complexity,\nwhile surprisingly ignoring the importance of feature calibrations in\nchannel-wise. In this work, we propose a simple yet effective\nCalibrated-Guidance (CG) scheme to enhance channel communications in a feature\ntransformer fashion, which can adaptively determine the calibration weights for\neach channel based on the global feature affinity correlations. Specifically,\nfor a given set of feature maps, CG first computes the feature similarity\nbetween each channel and the remaining channels as the intermediary calibration\nguidance. Then, re-representing each channel by aggregating all the channels\nweighted together via the guidance operation. Our CG is a general module that\ncan be plugged into any deep neural networks, which is named as CG-Net. To\ndemonstrate its effectiveness and efficiency, extensive experiments are carried\nout on both oriented object detection task and horizontal object detection task\nin aerial images. Experimental results on two challenging benchmarks (DOTA and\nHRSC2016) demonstrate that our CG-Net can achieve the new state-of-the-art\nperformance in accuracy with a fair computational overhead. The source code has\nbeen open sourced at https://github.com/WeiZongqi/CG-Net\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zongqi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Dong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Q/0/1/0/all/0/1\">Qixiang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mingqiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Sampling Based Deep Metric Learning for Generalizable Person Re-Identification. (arXiv:2104.01546v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.01546","description":"<p>Recent studies show that, both explicit deep feature matching as well as\nlarge-scale and diverse training data can significantly improve the\ngeneralization of person re-identification. However, the efficiency of learning\ndeep matchers on large-scale data has not yet been adequately studied. Though\nlearning with classification parameters or class memory is a popular way, it\nincurs large memory and computational costs. In contrast, pairwise deep metric\nlearning within mini batches would be a better choice. However, the most\npopular random sampling method, the well-known PK sampler, is not informative\nand efficient for deep metric learning. Though online hard example mining has\nimproved the learning efficiency to some extent, the mining in mini batches\nafter random sampling is still limited. This inspires us to explore the use of\nhard example mining earlier, in the data sampling stage. To do so, in this\npaper, we propose an efficient mini-batch sampling method, called graph\nsampling (GS), for large-scale deep metric learning. The basic idea is to build\na nearest neighbor relationship graph for all classes at the beginning of each\nepoch. Then, each mini batch is composed of a randomly selected class and its\nnearest neighboring classes so as to provide informative and challenging\nexamples for learning. Together with an adapted competitive baseline, we\nimprove the state of the art in generalizable person re-identification\nsignificantly, by 25.1% in Rank-1 on MSMT17 when trained on RandPerson.\nBesides, the proposed method also outperforms the competitive baseline, by 6.8%\nin Rank-1 on CUHK03-NP when trained on MSMT17. Meanwhile, the training time is\nsignificantly reduced, from 25.4 hours to 2 hours when trained on RandPerson\nwith 8,000 identities. Code is available at\nhttps://github.com/ShengcaiLiao/QAConv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Common Limitations of Image Processing Metrics: A Picture Story. (arXiv:2104.05642v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.05642","description":"<p>While the importance of automatic image analysis is continuously increasing,\nrecent meta-research revealed major flaws with respect to algorithm validation.\nPerformance metrics are particularly key for meaningful, objective, and\ntransparent performance assessment and validation of the used automatic\nalgorithms, but relatively little attention has been given to the practical\npitfalls when using specific metrics for a given image analysis task. These are\ntypically related to (1) the disregard of inherent metric properties, such as\nthe behaviour in the presence of class imbalance or small target structures,\n(2) the disregard of inherent data set properties, such as the non-independence\nof the test cases, and (3) the disregard of the actual biomedical domain\ninterest that the metrics should reflect. This living dynamically document has\nthe purpose to illustrate important limitations of performance metrics commonly\napplied in the field of image analysis. In this context, it focuses on\nbiomedical image analysis problems that can be phrased as image-level\nclassification, semantic segmentation, instance segmentation, or object\ndetection task. The current version is based on a Delphi process on metrics\nconducted by an international consortium of image analysis experts from more\nthan 60 institutions worldwide.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Reinke_A/0/1/0/all/0/1\">Annika Reinke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tizabi_M/0/1/0/all/0/1\">Minu D. Tizabi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sudre_C/0/1/0/all/0/1\">Carole H. Sudre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eisenmann_M/0/1/0/all/0/1\">Matthias Eisenmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Radsch_T/0/1/0/all/0/1\">Tim R&#xe4;dsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baumgartner_M/0/1/0/all/0/1\">Michael Baumgartner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Acion_L/0/1/0/all/0/1\">Laura Acion</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Antonelli_M/0/1/0/all/0/1\">Michela Antonelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakas_S/0/1/0/all/0/1\">Spyridon Bakas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bankhead_P/0/1/0/all/0/1\">Peter Bankhead</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benis_A/0/1/0/all/0/1\">Arriel Benis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardoso_M/0/1/0/all/0/1\">M. Jorge Cardoso</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cimini_B/0/1/0/all/0/1\">Beth Cimini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Collins_G/0/1/0/all/0/1\">Gary S. Collins</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Farahani_K/0/1/0/all/0/1\">Keyvan Farahani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Godau_P/0/1/0/all/0/1\">Patrick Godau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamprecht_F/0/1/0/all/0/1\">Fred Hamprecht</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hashimoto_D/0/1/0/all/0/1\">Daniel A. Hashimoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heckmann_Notzel_D/0/1/0/all/0/1\">Doreen Heckmann-N&#xf6;tzel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_M/0/1/0/all/0/1\">Michael M. Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huisman_M/0/1/0/all/0/1\">Merel Huisman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Isensee_F/0/1/0/all/0/1\">Fabian Isensee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jannin_P/0/1/0/all/0/1\">Pierre Jannin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kahn_C/0/1/0/all/0/1\">Charles E. Kahn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karargyris_A/0/1/0/all/0/1\">Alexandros Karargyris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karthikesalingam_A/0/1/0/all/0/1\">Alan Karthikesalingam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kavur_E/0/1/0/all/0/1\">Emre Kavur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kenngott_H/0/1/0/all/0/1\">Hannes Kenngott</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kleesiek_J/0/1/0/all/0/1\">Jens Kleesiek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kooi_T/0/1/0/all/0/1\">Thijs Kooi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kozubek_M/0/1/0/all/0/1\">Michal Kozubek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kreshuk_A/0/1/0/all/0/1\">Anna Kreshuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurc_T/0/1/0/all/0/1\">Tahsin Kurc</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Litjens_G/0/1/0/all/0/1\">Geert Litjens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Madani_A/0/1/0/all/0/1\">Amin Madani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_Hein_K/0/1/0/all/0/1\">Klaus Maier-Hein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martel_A/0/1/0/all/0/1\">Anne L. Martel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mattson_P/0/1/0/all/0/1\">Peter Mattson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meijering_E/0/1/0/all/0/1\">Erik Meijering</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern Menze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moher_D/0/1/0/all/0/1\">David Moher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moons_K/0/1/0/all/0/1\">Karel G.M. Moons</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muller_H/0/1/0/all/0/1\">Henning M&#xfc;ller</a>, et al. (24 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FINet: Dual Branches Feature Interaction for Partial-to-Partial Point Cloud Registration. (arXiv:2106.03479v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03479","description":"<p>Data association is important in the point cloud registration. In this work,\nwe propose to solve the partial-to-partial registration from a new perspective,\nby introducing multi-level feature interactions between the source and the\nreference clouds at the feature extraction stage, such that the registration\ncan be realized without the attentions or explicit mask estimation for the\noverlapping detection as adopted previously. Specifically, we present FINet, a\nfeature interaction-based structure with the capability to enable and\nstrengthen the information associating between the inputs at multiple stages.\nTo achieve this, we first split the features into two components, one for\nrotation and one for translation, based on the fact that they belong to\ndifferent solution spaces, yielding a dual branches structure. Second, we\ninsert several interaction modules at the feature extractor for the data\nassociation. Third, we propose a transformation sensitivity loss to obtain\nrotation-attentive and translation-attentive features. Experiments demonstrate\nthat our method performs higher precision and robustness compared to the\nstate-of-the-art traditional and learning-based methods. Code is available at\nhttps://github.com/megvii-research/FINet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_N/0/1/0/all/0/1\">Nianjin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guanghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Bing Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Connection between Local Attention and Dynamic Depth-wise Convolution. (arXiv:2106.04263v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04263","description":"<p>Vision Transformer (ViT) attains state-of-the-art performance in visual\nrecognition, and the variant, Local Vision Transformer, makes further\nimprovements. The major component in Local Vision Transformer, local attention,\nperforms the attention separately over small local windows. We rephrase local\nattention as a channel-wise locally-connected layer and analyze it from two\nnetwork regularization manners, sparse connectivity and weight sharing, as well\nas weight computation. Sparse connectivity: there is no connection across\nchannels, and each position is connected to the positions within a small local\nwindow. Weight sharing: the connection weights for one position are shared\nacross channels or within each group of channels. Dynamic weight: the\nconnection weights are dynamically predicted according to each image instance.\nWe point out that local attention resembles depth-wise convolution and its\ndynamic version in sparse connectivity. The main difference lies in weight\nsharing - depth-wise convolution shares connection weights (kernel weights)\nacross spatial positions. We empirically observe that the models based on\ndepth-wise convolution and the dynamic variant with lower computation\ncomplexity perform on-par with or sometimes slightly better than Swin\nTransformer, an instance of Local Vision Transformer, for ImageNet\nclassification, COCO object detection and ADE semantic segmentation. These\nobservations suggest that Local Vision Transformer takes advantage of two\nregularization forms and dynamic weight to increase the network capacity. Code\nis available at https://github.com/Atten4Vis/DemystifyLocalViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1\">Qi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zejia Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Dynamics of Nonlinear Representation Learning and Its Application. (arXiv:2106.14836v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.14836","description":"<p>Representations of the world environment play a crucial role in artificial\nintelligence. It is often inefficient to conduct reasoning and inference\ndirectly in the space of raw sensory representations, such as pixel values of\nimages. Representation learning allows us to automatically discover suitable\nrepresentations from raw sensory data. For example, given raw sensory data, a\ndeep neural network learns nonlinear representations at its hidden layers,\nwhich are subsequently used for classification at its output layer. This\nhappens implicitly during training through minimizing a supervised or\nunsupervised loss. In this paper, we study the dynamics of such implicit\nnonlinear representation learning. We identify a pair of a new assumption and a\nnovel condition, called the common model structure assumption and the\ndata-architecture alignment condition. Under the common model structure\nassumption, the data-architecture alignment condition is shown to be sufficient\nfor the global convergence and necessary for the global optimality. Moreover,\nour theory explains how and when increasing the network size does and does not\nimprove the training behaviors in the practical regime. Our results provide\npractical guidance for designing a model structure: e.g., the common model\nstructure assumption can be used as a justification for using a particular\nmodel structure instead of others. We also derive a new training framework,\nwhich satisfies the data-architecture alignment condition by automatically\nmodifying any given training algorithm. Given a standard training algorithm,\nthe framework running its modified version is empirically shown to maintain\ncompetitive test performances while providing global convergence guarantees for\ndeep residual neural networks with convolutions, skip connections, and batch\nnormalization with datasets, including MNIST, CIFAR-10, CIFAR-100, Semeion,\nKMNIST and SVHN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhun Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-Short Ensemble Network for Bipolar Manic-Euthymic State Recognition Based on Wrist-worn Sensors. (arXiv:2107.00710v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.00710","description":"<p>Manic episodes of bipolar disorder can lead to uncritical behaviour and\ndelusional psychosis, often with destructive consequences for those affected\nand their surroundings. Early detection and intervention of a manic episode are\ncrucial to prevent escalation, hospital admission and premature death. However,\npeople with bipolar disorder may not recognize that they are experiencing a\nmanic episode and symptoms such as euphoria and increased productivity can also\ndeter affected individuals from seeking help. This work proposes to perform\nuser-independent, automatic mood-state detection based on actigraphy and\nelectrodermal activity acquired from a wrist-worn device during mania and after\nrecovery (euthymia). This paper proposes a new deep learning-based ensemble\nmethod leveraging long (20h) and short (5 minutes) time-intervals to\ndiscriminate between the mood-states. When tested on 47 bipolar patients, the\nproposed classification scheme achieves an average accuracy of 91.59% in\neuthymic/manic mood-state recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cote_Allard_U/0/1/0/all/0/1\">Ulysse C&#xf4;t&#xe9;-Allard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakobsen_P/0/1/0/all/0/1\">Petter Jakobsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stautland_A/0/1/0/all/0/1\">Andrea Stautland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nordgreen_T/0/1/0/all/0/1\">Tine Nordgreen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fasmer_O/0/1/0/all/0/1\">Ole Bernt Fasmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oedegaard_K/0/1/0/all/0/1\">Ketil Joachim Oedegaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torresen_J/0/1/0/all/0/1\">Jim Torresen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NTIRE 2021 Multi-modal Aerial View Object Classification Challenge. (arXiv:2107.01189v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.01189","description":"<p>In this paper, we introduce the first Challenge on Multi-modal Aerial View\nObject Classification (MAVOC) in conjunction with the NTIRE 2021 workshop at\nCVPR. This challenge is composed of two different tracks using EO andSAR\nimagery. Both EO and SAR sensors possess different advantages and drawbacks.\nThe purpose of this competition is to analyze how to use both sets of sensory\ninformation in complementary ways. We discuss the top methods submitted for\nthis competition and evaluate their results on our blind test set. Our\nchallenge results show significant improvement of more than 15% accuracy from\nour current baselines for each track of the competition\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jerrick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inkawhich_N/0/1/0/all/0/1\">Nathan Inkawhich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nina_O/0/1/0/all/0/1\">Oliver Nina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bob Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yuru Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Songzheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiaqi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xueli Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mengru Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gongzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xueli Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Huanqia Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chengxue Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cummings_S/0/1/0/all/0/1\">Sol Cummings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miron_C/0/1/0/all/0/1\">Casian Miron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasarica_A/0/1/0/all/0/1\">Alexandru Pasarica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng-Yen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_H/0/1/0/all/0/1\">Hung-Min Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jiarui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jie Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Chia-Ying Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_M/0/1/0/all/0/1\">Michael Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_Z/0/1/0/all/0/1\">Zhongkai Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zihe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yifei_X/0/1/0/all/0/1\">Xu Yifei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lehan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Min Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detect and Locate: Exposing Face Manipulation by Semantic- and Noise-level Telltales. (arXiv:2107.05821v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05821","description":"<p>The technological advancements of deep learning have enabled sophisticated\nface manipulation schemes, raising severe trust issues and security concerns in\nmodern society. Generally speaking, detecting manipulated faces and locating\nthe potentially altered regions are challenging tasks. Herein, we propose a\nconceptually simple but effective method to efficiently detect forged faces in\nan image while simultaneously locating the manipulated regions. The proposed\nscheme relies on a segmentation map that delivers meaningful high-level\nsemantic information clues about the image. Furthermore, a noise map is\nestimated, playing a complementary role in capturing low-level clues and\nsubsequently empowering decision-making. Finally, the features from these two\nmodules are combined to distinguish fake faces. Extensive experiments show that\nthe proposed model achieves state-of-the-art detection accuracy and remarkable\nlocalization performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Chenqi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_A/0/1/0/all/0/1\">Anderson Rocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Video Object Segmentation with Compressed Video. (arXiv:2107.12192v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12192","description":"<p>We propose an efficient plug-and-play acceleration framework for\nsemi-supervised video object segmentation by exploiting the temporal\nredundancies in videos presented by the compressed bitstream. Specifically, we\npropose a motion vector-based warping method for propagating segmentation masks\nfrom keyframes to other frames in a bi-directional and multi-hop manner.\nAdditionally, we introduce a residual-based correction module that can fix\nwrongly propagated segmentation masks from noisy or erroneous motion vectors.\nOur approach is flexible and can be added on top of several existing video\nobject segmentation algorithms. We achieved highly competitive results on\nDAVIS17 and YouTube-VOS on various base models with substantial speed-ups of up\nto 3.5X with minor drops in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WORD: A large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from CT image. (arXiv:2111.02403v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.02403","description":"<p>Whole abdominal organ segmentation plays an important role in diagnosing\nabdomen lesions, radiotherapy, and follow-up. However, oncologists delineating\nall abdominal organs is time-consuming and very expensive. Recently, deep\nlearning-based medical image segmentation has shown the potential to reduce\nmanual delineation efforts, but it still requires a large-scale fine annotated\ndataset for training. Although many efforts in this task, there are still few\nlarge image datasets covering the whole abdomen region with accurate and\ndetailed annotations for the whole abdominal organ segmentation. In this work,\nwe establish a large-scale \\textit{W}hole abdominal \\textit{OR}gan\n\\textit{D}ataset (\\textit{WORD}) for algorithms research and clinical\napplications development. This dataset contains 150 abdominal CT volumes (30495\nslices). Each volume has 16 organs with fine pixel-level annotations and\nscribble-based sparse annotation, which may be the largest dataset with whole\nabdominal organ annotation. Several state-of-the-art segmentation methods are\nevaluated on this dataset. And we also invited three clinical oncologists to\nrevise the model predictions to measure the gap between the deep learning\nmethod and three oncologists. Afterwards, we investigate the\ninference-efficiently learning on the WORD dataset, as the high-resolution\nimage requires large GPU memory and inference time in the test stage. We\nfurther evaluate the scribble-based annotation-efficient learning on this\ndataset, as the pixel-wise manual annotation is time-consuming and expensive.\nThe work provided a new benchmark for the abdominal multi-organ segmentation\ntask, and these experiments can serve as the baseline for future research and\nclinical application development. The codebase and dataset is released at:\n\\url{https://github.com/HiLab-git/WORD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1\">Xiangde Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_W/0/1/0/all/0/1\">Wenjun Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jianghong Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_T/0/1/0/all/0/1\">Tao Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaofan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_K/0/1/0/all/0/1\">Kang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Guotai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It's About Time: Analog Clock Reading in the Wild. (arXiv:2111.09162v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09162","description":"<p>In this paper, we present a framework for reading analog clocks in natural\nimages or videos. Specifically, we make the following contributions: First, we\ncreate a scalable pipeline for generating synthetic clocks, significantly\nreducing the requirements for the labour-intensive annotations; Second, we\nintroduce a clock recognition architecture based on spatial transformer\nnetworks (STN), which is trained end-to-end for clock alignment and\nrecognition. We show that the model trained on the proposed synthetic dataset\ngeneralises towards real clocks with good accuracy, advocating a Sim2Real\ntraining regime; Third, to further reduce the gap between simulation and real\ndata, we leverage the special property of \"time\", i.e.uniformity, to generate\nreliable pseudo-labels on real unlabelled clock videos, and show that training\non these videos offers further improvements while still requiring zero manual\nannotations. Lastly, we introduce three benchmark datasets based on COCO, Open\nImages, and The Clock movie, with full annotations for time, accurate to the\nminute.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Charig Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v11 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11133","description":"<p>Far beyond learning long-range interactions of natural language, transformers\nare becoming the de-facto standard for many vision tasks with their power and\nscalability. Especially with cross-modal tasks between image and text, vector\nquantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB\nimage into a sequence of feature vectors. To better leverage the correlation\nbetween image and text, we propose L-Verse, a novel architecture consisting of\nfeature-augmented variational autoencoder (AugVAE) and bidirectional\nauto-regressive transformer (BiART) for image-to-text and text-to-image\ngeneration. Our AugVAE shows the state-of-the-art reconstruction performance on\nImageNet1K validation set, along with the robustness to unseen images in the\nwild. Unlike other models, BiART can distinguish between image (or text) as a\nconditional reference and a generation target. L-Verse can be directly used for\nimage-to-text or text-to-image generation without any finetuning or extra\nobject detection framework. In quantitative and qualitative experiments,\nL-Verse shows impressive results against previous methods in both image-to-text\nand text-to-image generation on MS-COCO Captions. We furthermore assess the\nscalability of L-Verse architecture on Conceptual Captions and present the\ninitial result of bidirectional vision-language representation learning on\ngeneral domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Gwangmo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sihaeng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1\">Yewon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Soonyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seung Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_K/0/1/0/all/0/1\">Kyunghoon Bae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Fields in Visual Computing and Beyond. (arXiv:2111.11426v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11426","description":"<p>Recent advances in machine learning have created increasing interest in\nsolving visual computing problems using a class of coordinate-based neural\nnetworks that parametrize physical properties of scenes or objects across space\nand time. These methods, which we call neural fields, have seen successful\napplication in the synthesis of 3D shapes and image, animation of human bodies,\n3D reconstruction, and pose estimation. However, due to rapid progress in a\nshort time, many papers exist but a comprehensive review and formulation of the\nproblem has not yet emerged. In this report, we address this limitation by\nproviding context, mathematical grounding, and an extensive review of\nliterature on neural fields. This report covers research along two dimensions.\nIn Part I, we focus on techniques in neural fields by identifying common\ncomponents of neural field methods, including different representations,\narchitectures, forward mapping, and generalization methods. In Part II, we\nfocus on applications of neural fields to different problems in visual\ncomputing, and beyond (e.g., robotics, audio). Our review shows the breadth of\ntopics already covered in visual computing, both historically and in current\nincarnations, demonstrating the improved quality, flexibility, and capability\nbrought by neural fields methods. Finally, we present a companion website that\ncontributes a living version of this review that can be continually updated by\nthe community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yiheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takikawa_T/0/1/0/all/0/1\">Towaki Takikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1\">Shunsuke Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litany_O/0/1/0/all/0/1\">Or Litany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shiqin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Numair Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tompkin_J/0/1/0/all/0/1\">James Tompkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1\">Vincent Sitzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1\">Srinath Sridhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-invasive hemodynamic analysis for aortic regurgitation using computational fluid dynamics and deep learning. (arXiv:2111.11660v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11660","description":"<p>Changes in cardiovascular hemodynamics are closely related to the development\nof aortic regurgitation, a type of valvular heart disease. Metrics derived from\nblood flows are used to indicate aortic regurgitation onset and evaluate its\nseverity. These metrics can be non-invasively obtained using four-dimensional\n(4D) flow magnetic resonance imaging (MRI), where accuracy is primarily\ndependent on spatial resolution. However, insufficient resolution often results\nfrom limitations in 4D flow MRI and complex aortic regurgitation hemodynamics.\nTo address this, computational fluid dynamics simulations were transformed into\nsynthetic 4D flow MRI data and used to train a variety of neural networks.\nThese networks generated super resolution, full-field phase images with an\nupsample factor of 4. Results showed decreased velocity error, high structural\nsimilarity scores, and improved learning capabilities from previous work.\nFurther validation was performed on two sets of in-vivo 4D flow MRI data and\ndemonstrated success in de-noising flow images. This approach presents an\nopportunity to comprehensively analyse aortic regurgitation hemodynamics in a\nnon-invasive manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_D/0/1/0/all/0/1\">Derek Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMurdo_C/0/1/0/all/0/1\">Cameron McMurdo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferdian_E/0/1/0/all/0/1\">Edward Ferdian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mauger_C/0/1/0/all/0/1\">Charlene A. Mauger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marlevi_D/0/1/0/all/0/1\">David Marlevi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_A/0/1/0/all/0/1\">Alistair A. Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nash_M/0/1/0/all/0/1\">Martyn P. Nash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Image Patch is a Wave: Phase-Aware Vision MLP. (arXiv:2111.12294v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12294","description":"<p>In the field of computer vision, recent works show that a pure MLP\narchitecture mainly stacked by fully-connected layers can achieve competing\nperformance with CNN and transformer. An input image of vision MLP is usually\nsplit into multiple tokens (patches), while the existing MLP models directly\naggregate them with fixed weights, neglecting the varying semantic information\nof tokens from different images. To dynamically aggregate tokens, we propose to\nrepresent each token as a wave function with two parts, amplitude and phase.\nAmplitude is the original feature and the phase term is a complex value\nchanging according to the semantic contents of input images. Introducing the\nphase term can dynamically modulate the relationship between tokens and fixed\nweights in MLP. Based on the wave-like token representation, we establish a\nnovel Wave-MLP architecture for vision tasks. Extensive experiments demonstrate\nthat the proposed Wave-MLP is superior to the state-of-the-art MLP\narchitectures on various vision tasks such as image classification, object\ndetection and semantic segmentation. The source code is available at\nhttps://github.com/huawei-noah/CV-Backbones/tree/master/wavemlp_pytorch and\nhttps://gitee.com/mindspore/models/tree/master/research/cv/wave_mlp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UDA-COPE: Unsupervised Domain Adaptation for Category-level Object Pose Estimation. (arXiv:2111.12580v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12580","description":"<p>Learning to estimate object pose often requires ground-truth (GT) labels,\nsuch as CAD model and absolute-scale object pose, which is expensive and\nlaborious to obtain in the real world. To tackle this problem, we propose an\nunsupervised domain adaptation (UDA) for category-level object pose estimation,\ncalled UDA-COPE. Inspired by recent multi-modal UDA techniques, the proposed\nmethod exploits a teacher-student self-supervised learning scheme to train a\npose estimation network without using target domain pose labels. We also\nintroduce a bidirectional filtering method between the predicted normalized\nobject coordinate space (NOCS) map and observed point cloud, to not only make\nour teacher network more robust to the target domain but also to provide more\nreliable pseudo labels for the student network training. Extensive experimental\nresults demonstrate the effectiveness of our proposed method both\nquantitatively and qualitatively. Notably, without leveraging target-domain GT\nlabels, our proposed method achieved comparable or sometimes superior\nperformance to existing methods that depend on the GT labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Taeyeop Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Byeong-Uk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_I/0/1/0/all/0/1\">Inkyu Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1\">Jaesung Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_U/0/1/0/all/0/1\">Ukcheol Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1\">Kuk-Jin Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeRFReN: Neural Radiance Fields with Reflections. (arXiv:2111.15234v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15234","description":"<p>Neural Radiance Fields (NeRF) has achieved unprecedented view synthesis\nquality using coordinate-based neural scene representations. However, NeRF's\nview dependency can only handle simple reflections like highlights but cannot\ndeal with complex reflections such as those from glass and mirrors. In these\nscenarios, NeRF models the virtual image as real geometries which leads to\ninaccurate depth estimation, and produces blurry renderings when the multi-view\nconsistency is violated as the reflected objects may only be seen under some of\nthe viewpoints. To overcome these issues, we introduce NeRFReN, which is built\nupon NeRF to model scenes with reflections. Specifically, we propose to split a\nscene into transmitted and reflected components, and model the two components\nwith separate neural radiance fields. Considering that this decomposition is\nhighly under-constrained, we exploit geometric priors and apply\ncarefully-designed training strategies to achieve reasonable decomposition\nresults. Experiments on various self-captured scenes show that our method\nachieves high-quality novel view synthesis and physically sound depth\nestimation results while enabling scene editing applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuan-Chen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Di Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Linchao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Song-Hai Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PolyWorld: Polygonal Building Extraction with Graph Neural Networks in Satellite Images. (arXiv:2111.15491v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15491","description":"<p>While most state-of-the-art instance segmentation methods produce binary\nsegmentation masks, geographic and cartographic applications typically require\nprecise vector polygons of extracted objects instead of rasterized output. This\npaper introduces PolyWorld, a neural network that directly extracts building\nvertices from an image and connects them correctly to create precise polygons.\nThe model predicts the connection strength between each pair of vertices using\na graph neural network and estimates the assignments by solving a\ndifferentiable optimal transport problem. Moreover, the vertex positions are\noptimized by minimizing a combined segmentation and polygonal angle difference\nloss. PolyWorld significantly outperforms the state of the art in building\npolygonization and achieves not only notable quantitative results, but also\nproduces visually pleasing building polygons. Code and trained weights are\npublicly available at https://github.com/zorzi-s/PolyWorldPretrainedNetwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zorzi_S/0/1/0/all/0/1\">Stefano Zorzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazrafkan_S/0/1/0/all/0/1\">Shabab Bazrafkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habenschuss_S/0/1/0/all/0/1\">Stefan Habenschuss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraundorfer_F/0/1/0/all/0/1\">Friedrich Fraundorfer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAVT: Language-Aware Vision Transformer for Referring Image Segmentation. (arXiv:2112.02244v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02244","description":"<p>Referring image segmentation is a fundamental vision-language task that aims\nto segment out an object referred to by a natural language expression from an\nimage. One of the key challenges behind this task is leveraging the referring\nexpression for highlighting relevant positions in the image. A paradigm for\ntackling this problem is to leverage a powerful vision-language (\"cross-modal\")\ndecoder to fuse features independently extracted from a vision encoder and a\nlanguage encoder. Recent methods have made remarkable advancements in this\nparadigm by exploiting Transformers as cross-modal decoders, concurrent to the\nTransformer's overwhelming success in many other vision-language tasks.\nAdopting a different approach in this work, we show that significantly better\ncross-modal alignments can be achieved through the early fusion of linguistic\nand visual features in intermediate layers of a vision Transformer encoder\nnetwork. By conducting cross-modal feature fusion in the visual feature\nencoding stage, we can leverage the well-proven correlation modeling power of a\nTransformer encoder for excavating helpful multi-modal context. This way,\naccurate segmentation results are readily harvested with a light-weight mask\npredictor. Without bells and whistles, our method surpasses the previous\nstate-of-the-art methods on RefCOCO, RefCOCO+, and G-Ref by large margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yansong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Watch It Move: Unsupervised Discovery of 3D Joints for Re-Posing of Articulated Objects. (arXiv:2112.11347v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11347","description":"<p>Rendering articulated objects while controlling their poses is critical to\napplications such as virtual reality or animation for movies. Manipulating the\npose of an object, however, requires the understanding of its underlying\nstructure, that is, its joints and how they interact with each other.\nUnfortunately, assuming the structure to be known, as existing methods do,\nprecludes the ability to work on new object categories. We propose to learn\nboth the appearance and the structure of previously unseen articulated objects\nby observing them move from multiple views, with no joints annotation\nsupervision, or information about the structure. We observe that 3D points that\nare static relative to one another should belong to the same part, and that\nadjacent parts that move relative to each other must be connected by a joint.\nTo leverage this insight, we model the object parts in 3D as ellipsoids, which\nallows us to identify joints. We combine this explicit representation with an\nimplicit one that compensates for the approximation introduced. We show that\nour method works for different structures, from quadrupeds, to single-arm\nrobots, to humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noguchi_A/0/1/0/all/0/1\">Atsuhiro Noguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_U/0/1/0/all/0/1\">Umar Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tremblay_J/0/1/0/all/0/1\">Jonathan Tremblay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallo_O/0/1/0/all/0/1\">Orazio Gallo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross Modal Retrieval with Querybank Normalisation. (arXiv:2112.12777v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12777","description":"<p>Profiting from large-scale training datasets, advances in neural architecture\ndesign and efficient inference, joint embeddings have become the dominant\napproach for tackling cross-modal retrieval. In this work we first show that,\ndespite their effectiveness, state-of-the-art joint embeddings suffer\nsignificantly from the longstanding \"hubness problem\" in which a small number\nof gallery embeddings form the nearest neighbours of many queries. Drawing\ninspiration from the NLP literature, we formulate a simple but effective\nframework called Querybank Normalisation (QB-Norm) that re-normalises query\nsimilarities to account for hubs in the embedding space. QB-Norm improves\nretrieval performance without requiring retraining. Differently from prior\nwork, we show that QB-Norm works effectively without concurrent access to any\ntest set queries. Within the QB-Norm framework, we also propose a novel\nsimilarity normalisation method, the Dynamic Inverted Softmax, that is\nsignificantly more robust than existing approaches. We showcase QB-Norm across\na range of cross modal retrieval models and benchmarks where it consistently\nenhances strong baselines beyond the state of the art. Code is available at\nhttps://vladbogo.github.io/QB-Norm/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bogolin_S/0/1/0/all/0/1\">Simion-Vlad Bogolin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Croitoru_I/0/1/0/all/0/1\">Ioana Croitoru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1\">Samuel Albanie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Deep Image Matting via Local Smoothness Assumption. (arXiv:2112.13809v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13809","description":"<p>Natural image matting is a fundamental and challenging computer vision task.\nConventionally, the problem is formulated as an underconstrained problem. Since\nthe problem is ill-posed, further assumptions on the data distribution are\nrequired to make the problem well-posed. For classical matting methods, a\ncommonly adopted assumption is the local smoothness assumption on foreground\nand background colors. However, the use of such assumptions was not\nsystematically considered for deep learning based matting methods. In this\nwork, we consider two local smoothness assumptions which can help improving\ndeep image matting models. Based on the local smoothness assumptions, we\npropose three techniques, i.e., training set refinement, color augmentation and\nbackpropagating refinement, which can improve the performance of the deep image\nmatting model significantly. We conduct experiments to examine the\neffectiveness of the proposed algorithm. The experimental results show that the\nproposed method has favorable performance compared with existing matting\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiacheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_D/0/1/0/all/0/1\">Dezhen Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal registration of FISH and nanoSIMS images using convolutional neural network models. (arXiv:2201.05545v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.05545","description":"<p>Nanoscale secondary ion mass spectrometry (nanoSIMS) and fluorescence in situ\nhybridization (FISH) microscopy provide high-resolution, multimodal image\nrepresentations of the identity and cell activity respectively of targeted\nmicrobial communities in microbiological research. Despite its importance to\nmicrobiologists, multimodal registration of FISH and nanoSIMS images is\nchallenging given the morphological distortion and background noise in both\nimages. In this study, we use convolutional neural networks (CNNs) for\nmultiscale feature extraction, shape context for computation of the minimum\ntransformation cost feature matching and the thin-plate spline (TPS) model for\nmultimodal registration of the FISH and nanoSIMS images. Registration accuracy\nwas quantitatively assessed against manually registered images, at both, the\npixel and structural levels using standard metrics. Although all six tested CNN\nmodels performed well, ResNet18 was observed to outperform VGG16, VGG19,\nGoogLeNet and ShuffleNet and ResNet101 based on most metrics. This study\ndemonstrates the utility of CNNs in the registration of multimodal images with\nsignificant background noise and morphology distortion. We also show aggregate\nshape, preserved by binarization, to be a robust feature for registering\nmultimodal microbiology-related images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaojia He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meile_C/0/1/0/all/0/1\">Christof Meile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandarkar_S/0/1/0/all/0/1\">Suchendra M. Bhandarkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"vCLIMB: A Novel Video Class Incremental Learning Benchmark. (arXiv:2201.09381v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09381","description":"<p>Continual learning (CL) is under-explored in the video domain. The few\nexisting works contain splits with imbalanced class distributions over the\ntasks, or study the problem in unsuitable datasets. We introduce vCLIMB, a\nnovel video continual learning benchmark. vCLIMB is a standardized test-bed to\nanalyze catastrophic forgetting of deep models in video continual learning. In\ncontrast to previous work, we focus on class incremental continual learning\nwith models trained on a sequence of disjoint tasks, and distribute the number\nof classes uniformly across the tasks. We perform in-depth evaluations of\nexisting CL methods in vCLIMB, and observe two unique challenges in video data.\nThe selection of instances to store in episodic memory is performed at the\nframe level. Second, untrimmed training data influences the effectiveness of\nframe sampling strategies. We address these two challenges by proposing a\ntemporal consistency regularization that can be applied on top of memory-based\ncontinual learning methods. Our approach significantly improves the baseline,\nby up to 24% on the untrimmed continual learning task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Villa_A/0/1/0/all/0/1\">Andr&#xe9;s Villa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhamoud_K/0/1/0/all/0/1\">Kumail Alhamoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1\">Juan Le&#xf3;n Alc&#xe1;zar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escorcia_V/0/1/0/all/0/1\">Victor Escorcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Recognition and Digital Documentation of Cultural Heritage Hemispherical Domes using Images. (arXiv:2201.10015v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10015","description":"<p>Advancements in optical metrology has enabled documentation of dense 3D point\nclouds of cultural heritage sites. For large scale and continuous digital\ndocumentation, processing of dense 3D point clouds becomes computationally\ncumbersome, and often requires additional hardware for data management,\nincreasing the time cost, and complexity of projects. To this end, this\nmanuscript presents an original approach to generate fast and reliable semantic\ndigital models of heritage hemispherical domes using only two images. New\nclosed formulations were derived to establish the relationships between spheres\nand their projected ellipses onto images, which fostered the development of a\nnew automatic framework for as-built generation of spheres. The effectiveness\nof the proposed method was evaluated under both laboratory and real-world\ndatasets. The results revealed that the proposed method achieved as-built\nmodeling accuracy of around 6mm, while improving the computation time by a\nfactor of 7, when compared to established point cloud processing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maalek_R/0/1/0/all/0/1\">Reza Maalek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maalek_S/0/1/0/all/0/1\">Shahrokh Maalek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Online Video Super-Resolution with Deformable Attention Pyramid. (arXiv:2202.01731v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.01731","description":"<p>Video super-resolution (VSR) has many applications that pose strict causal,\nreal-time, and latency constraints, including video streaming and TV. We\naddress the VSR problem under these settings, which poses additional important\nchallenges since information from future frames is unavailable. Importantly,\ndesigning efficient, yet effective frame alignment and fusion modules remain\ncentral problems. In this work, we propose a recurrent VSR architecture based\non a deformable attention pyramid (DAP). Our DAP aligns and integrates\ninformation from the recurrent state into the current frame prediction. To\ncircumvent the computational cost of traditional attention-based methods, we\nonly attend to a limited number of spatial locations, which are dynamically\npredicted by the DAP. Comprehensive experiments and analysis of the proposed\nkey innovations show the effectiveness of our approach. We significantly reduce\nprocessing time and computational complexity in comparison to state-of-the-art\nmethods, while maintaining a high performance. We surpass state-of-the-art\nmethod EDVR-M on two standard benchmarks with a speed-up of over $3\\times$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fuoli_D/0/1/0/all/0/1\">Dario Fuoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-direction and Multi-scale Pyramid in Transformer for Video-based Pedestrian Retrieval. (arXiv:2202.06014v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06014","description":"<p>In video surveillance, pedestrian retrieval (also called person\nre-identification) is a critical task. This task aims to retrieve the\npedestrian of interest from non-overlapping cameras. Recently,\ntransformer-based models have achieved significant progress for this task.\nHowever, these models still suffer from ignoring fine-grained, part-informed\ninformation. This paper proposes a multi-direction and multi-scale Pyramid in\nTransformer (PiT) to solve this problem. In transformer-based architecture,\neach pedestrian image is split into many patches. Then, these patches are fed\nto transformer layers to obtain the feature representation of this image. To\nexplore the fine-grained information, this paper proposes to apply vertical\ndivision and horizontal division on these patches to generate\ndifferent-direction human parts. These parts provide more fine-grained\ninformation. To fuse multi-scale feature representation, this paper presents a\npyramid structure containing global-level information and many pieces of\nlocal-level information from different scales. The feature pyramids of all the\npedestrian images from the same video are fused to form the final\nmulti-direction and multi-scale feature representation. Experimental results on\ntwo challenging video-based benchmarks, MARS and iLIDS-VID, show the proposed\nPiT achieves state-of-the-art performance. Extensive ablation studies\ndemonstrate the superiority of the proposed pyramid structure. The code is\navailable at https://git.openi.org.cn/zangxh/PiT.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1\">Xianghao Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X-Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning. (arXiv:2203.00843v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00843","description":"<p>3D dense captioning aims to describe individual objects by natural language\nin 3D scenes, where 3D scenes are usually represented as RGB-D scans or point\nclouds. However, only exploiting single modal information, e.g., point cloud,\nprevious approaches fail to produce faithful descriptions. Though aggregating\n2D features into point clouds may be beneficial, it introduces an extra\ncomputational burden, especially in inference phases. In this study, we\ninvestigate a cross-modal knowledge transfer using Transformer for 3D dense\ncaptioning, X-Trans2Cap, to effectively boost the performance of single-modal\n3D caption through knowledge distillation using a teacher-student framework. In\npractice, during the training phase, the teacher network exploits auxiliary 2D\nmodality and guides the student network that only takes point clouds as input\nthrough the feature consistency constraints. Owing to the well-designed\ncross-modal feature fusion module and the feature alignment in the training\nphase, X-Trans2Cap acquires rich appearance information embedded in 2D images\nwith ease. Thus, a more faithful caption can be generated only using point\nclouds during the inference. Qualitative and quantitative results confirm that\nX-Trans2Cap outperforms previous state-of-the-art by a large margin, i.e.,\nabout +21 and about +16 absolute CIDEr score on ScanRefer and Nr3D datasets,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhihao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yinghong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SkinningNet: Two-Stream Graph Convolutional Neural Network for Skinning Prediction of Synthetic Characters. (arXiv:2203.04746v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04746","description":"<p>This work presents SkinningNet, an end-to-end Two-Stream Graph Neural Network\narchitecture that computes skinning weights from an input mesh and its\nassociated skeleton, without making any assumptions on shape class and\nstructure of the provided mesh. Whereas previous methods pre-compute\nhandcrafted features that relate the mesh and the skeleton or assume a fixed\ntopology of the skeleton, the proposed method extracts this information in an\nend-to-end learnable fashion by jointly learning the best relationship between\nmesh vertices and skeleton joints. The proposed method exploits the benefits of\nthe novel Multi-Aggregator Graph Convolution that combines the results of\ndifferent aggregators during the summarizing step of the Message-Passing\nscheme, helping the operation to generalize for unseen topologies. Experimental\nresults demonstrate the effectiveness of the contributions of our novel\narchitecture, with SkinningNet outperforming current state-of-the-art\nalternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mosella_Montoro_A/0/1/0/all/0/1\">Albert Mosella-Montoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Hidalgo_J/0/1/0/all/0/1\">Javier Ruiz-Hidalgo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations?. (arXiv:2203.08392v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08392","description":"<p>Vision transformers (ViTs) have recently set off a new wave in neural\narchitecture design thanks to their record-breaking performance in various\nvision tasks. In parallel, to fulfill the goal of deploying ViTs into\nreal-world vision applications, their robustness against potential malicious\nattacks has gained increasing attention. In particular, recent works show that\nViTs are more robust against adversarial attacks as compared with convolutional\nneural networks (CNNs), and conjecture that this is because ViTs focus more on\ncapturing global interactions among different input/feature patches, leading to\ntheir improved robustness to local perturbations imposed by adversarial\nattacks. In this work, we ask an intriguing question: \"Under what kinds of\nperturbations do ViTs become more vulnerable learners compared to CNNs?\" Driven\nby this question, we first conduct a comprehensive experiment regarding the\nrobustness of both ViTs and CNNs under various existing adversarial attacks to\nunderstand the underlying reason favoring their robustness. Based on the drawn\ninsights, we then propose a dedicated attack framework, dubbed Patch-Fool, that\nfools the self-attention mechanism by attacking its basic component (i.e., a\nsingle patch) with a series of attention-aware optimization techniques.\nInterestingly, our Patch-Fool framework shows for the first time that ViTs are\nnot necessarily more robust than CNNs against adversarial perturbations. In\nparticular, we find that ViTs are more vulnerable learners compared with CNNs\nagainst our Patch-Fool attack which is consistent across extensive experiments,\nand the observations from Sparse/Mild Patch-Fool, two variants of Patch-Fool,\nindicate an intriguing insight that the perturbation density and strength on\neach patch seem to be the key factors that influence the robustness ranking\nbetween ViTs and CNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yonggan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shunyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1\">Cheng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingyan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medium Transmission Map Matters for Learning to Restore Real-World Underwater Images. (arXiv:2203.09414v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09414","description":"<p>Underwater visual perception is essentially important for underwater\nexploration, archeology, ecosystem and so on. The low illumination, light\nreflections, scattering, absorption and suspended particles inevitably lead to\nthe critically degraded underwater image quality, which causes great challenges\non recognizing the objects from the underwater images. The existing underwater\nenhancement methods that aim to promote the underwater visibility, heavily\nsuffer from the poor image restoration performance and generalization ability.\nTo reduce the difficulty of underwater image enhancement, we introduce the\nmedia transmission map as guidance to assist in image enhancement. We formulate\nthe interaction between the underwater visual images and the transmission map\nto obtain better enhancement results. Even with simple and lightweight network\nconfiguration, the proposed method can achieve advanced results of 22.6 dB on\nthe challenging Test-R90 with an impressive 30 times faster than the existing\nmodels. Comprehensive experimental results have demonstrated the superiority\nand potential on underwater perception. Paper's code is offered on:\nhttps://github.com/GroupG-yk/MTUR-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kai_Y/0/1/0/all/0/1\">Yan Kai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanyue_L/0/1/0/all/0/1\">Liang Lanyue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziqiang_Z/0/1/0/all/0/1\">Zheng Ziqiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guoqing_W/0/1/0/all/0/1\">Wang Guoqing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expression Classification using Concatenation of Deep Neural Network for the 3rd ABAW3 Competition. (arXiv:2203.12899v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12899","description":"<p>For computers to recognize human emotions, expression classification is an\nequally important problem in the human-computer interaction area. In the 3rd\nAffective Behavior Analysis In-The-Wild competition, the task of expression\nclassification includes eight classes with six basic expressions of human faces\nfrom videos. In this paper, we employ a transformer mechanism to encode the\nrobust representation from the backbone. Fusion of the robust representations\nplays an important role in the expression classification task. Our approach\nachieves 30.35\\% and 28.60\\% for the $F_1$ score on the validation set and the\ntest set, respectively. This result shows the effectiveness of the proposed\narchitecture based on the Aff-Wild2 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phan_K/0/1/0/all/0/1\">Kim Ngan Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hong-Hai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_V/0/1/0/all/0/1\">Van-Thong Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soo-Hyung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CenterLoc3D: Monocular 3D Vehicle Localization Network for Roadside Surveillance Cameras. (arXiv:2203.14550v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14550","description":"<p>Monocular 3D vehicle localization is an important task in Intelligent\nTransportation System (ITS) and Cooperative Vehicle Infrastructure System\n(CVIS), which is usually achieved by monocular 3D vehicle detection. However,\ndepth information cannot be obtained directly by monocular cameras due to the\ninherent imaging mechanism, resulting in more challenging monocular 3D tasks.\nMost of the current monocular 3D vehicle detection methods leverage 2D\ndetectors and additional geometric modules, which reduces the efficiency. In\nthis paper, we propose a 3D vehicle localization network CenterLoc3D for\nroadside monocular cameras, which directly predicts centroid and eight vertexes\nin image space, and the dimension of 3D bounding boxes without 2D detectors. To\nimprove the precision of 3D vehicle localization, we propose a weighted-fusion\nmodule and a loss with spatial constraints embedded in CenterLoc3D. Firstly,\nthe transformation matrix between 2D image space and 3D world space is solved\nby camera calibration. Secondly, vehicle type, centroid, eight vertexes, and\nthe dimension of 3D vehicle bounding boxes are obtained by CenterLoc3D.\nFinally, centroid in 3D world space can be obtained by camera calibration and\nCenterLoc3D for 3D vehicle localization. To the best of our knowledge, this is\nthe first application of 3D vehicle localization for roadside monocular\ncameras. Hence, we also propose a benchmark for this application including a\ndataset (SVLD-3D), an annotation tool (LabelImg-3D), and evaluation metrics.\nThrough experimental validation, the proposed method achieves high accuracy and\nreal-time performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xinyao_T/0/1/0/all/0/1\">Tang Xinyao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huansheng_S/0/1/0/all/0/1\">Song Huansheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chunhui_Z/0/1/0/all/0/1\">Zhao Chunhui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Mechanisms Inspired Efficient Transformers for Image and Video Quality Assessment. (arXiv:2203.14557v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14557","description":"<p>Visual (image, video) quality assessments can be modelled by visual features\nin different domains, e.g., spatial, frequency, and temporal domains.\nPerceptual mechanisms in the human visual system (HVS) play a crucial role in\ngeneration of quality perception. This paper proposes a general framework for\nno-reference visual quality assessment using efficient windowed transformer\narchitectures. A lightweight module for multi-stage channel attention is\nintegrated into Swin (shifted window) Transformer. Such module can represent\nappropriate perceptual mechanisms in image quality assessment (IQA) to build an\naccurate IQA model. Meanwhile, representative features for image quality\nperception in the spatial and frequency domains can also be derived from the\nIQA model, which are then fed into another windowed transformer architecture\nfor video quality assessment (VQA). The VQA model efficiently reuses attention\ninformation across local windows to tackle the issue of expensive time and\nmemory complexities of original transformer. Experimental results on both\nlarge-scale IQA and VQA databases demonstrate that the proposed quality\nassessment models outperform other state-of-the-art models by large margins.\nThe complete source code will be published on Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Junyong You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Hyperspectral Images Using SVM with Shape-adaptive Reconstruction and Smoothed Total Variation. (arXiv:2203.15619v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15619","description":"<p>In this work, a novel algorithm called SVM with Shape-adaptive Reconstruction\nand Smoothed Total Variation (SaR-SVM-STV) is introduced to classify\nhyperspectral images, which makes full use of spatial and spectral information.\nThe Shape-adaptive Reconstruction (SaR) is introduced to preprocess each pixel\nbased on the Pearson Correlation between pixels in its shape-adaptive (SA)\nregion. Support Vector Machines (SVMs) are trained to estimate the pixel-wise\nprobability maps of each class. Then the Smoothed Total Variation (STV) model\nis applied to denoise and generate the final classification map. Experiments\nshow that SaR-SVM-STV outperforms the SVM-STV method with a few training\nlabels, demonstrating the significance of reconstructing hyperspectral images\nbefore classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruoning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1\">Kangning Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1\">Raymond H. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plemmons_R/0/1/0/all/0/1\">Robert J. Plemmons</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Framework for Domain Adaptive Pose Estimation. (arXiv:2204.00172v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00172","description":"<p>While pose estimation is an important computer vision task, it requires\nexpensive annotation and suffers from domain shift. In this paper, we\ninvestigate the problem of domain adaptive 2D pose estimation that transfers\nknowledge learned on a synthetic source domain to a target domain without\nsupervision. While several domain adaptive pose estimation models have been\nproposed recently, they are not generic but only focus on either human pose or\nanimal pose estimation, and thus their effectiveness is somewhat limited to\nspecific scenarios. In this work, we propose a unified framework that\ngeneralizes well on various domain adaptive pose estimation problems. We\npropose to align representations using both input-level and output-level cues\n(pixels and pose labels, respectively), which facilitates the knowledge\ntransfer from the source domain to the unlabeled target domain. Our experiments\nshow that our method achieves state-of-the-art performance under various domain\nshifts. Our method outperforms existing baselines on human pose estimation by\nup to 4.5 percent points (pp), hand pose estimation by up to 7.4 pp, and animal\npose estimation by up to 4.8 pp for dogs and 3.3 pp for sheep. These results\nsuggest that our method is able to mitigate domain shift on diverse tasks and\neven unseen domains and objects (e.g., trained on horse and tested on dog).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Betke_M/0/1/0/all/0/1\">Margrit Betke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1\">Stan Sclaroff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MRI-based Multi-task Decoupling Learning for Alzheimer's Disease Detection and MMSE Score Prediction: A Multi-site Validation. (arXiv:2204.01708v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.01708","description":"<p>Accurately detecting Alzheimer's disease (AD) and predicting mini-mental\nstate examination (MMSE) score are important tasks in elderly health by\nmagnetic resonance imaging (MRI). Most of the previous methods on these two\ntasks are based on single-task learning and rarely consider the correlation\nbetween them. Since the MMSE score, which is an important basis for AD\ndiagnosis, can also reflect the progress of cognitive impairment, some studies\nhave begun to apply multi-task learning methods to these two tasks. However,\nhow to exploit feature correlation remains a challenging problem for these\nmethods. To comprehensively address this challenge, we propose a MRI-based\nmulti-task decoupled learning method for AD detection and MMSE score\nprediction. First, a multi-task learning network is proposed to implement AD\ndetection and MMSE score prediction, which exploits feature correlation by\nadding three multi-task interaction layers between the backbones of the two\ntasks. Each multi-task interaction layer contains two feature decoupling\nmodules and one feature interaction module. Furthermore, to enhance the\ngeneralization between tasks of the features selected by the feature decoupling\nmodule, we propose the feature consistency loss constrained feature decoupling\nmodule. Finally, in order to exploit the specific distribution information of\nMMSE score in different groups, a distribution loss is proposed to further\nenhance the model performance. We evaluate our proposed method on multi-site\ndatasets. Experimental results show that our proposed multi-task decoupled\nrepresentation learning method achieves good performance, outperforming\nsingle-task learning and other existing state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tian_X/0/1/0/all/0/1\">Xu Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuang_H/0/1/0/all/0/1\">Hulin Kuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheng_Y/0/1/0/all/0/1\">Yu Sheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jianxin Wang</a>, The <a href=\"http://arxiv.org/find/eess/1/au:+Initiative_A/0/1/0/all/0/1\">Alzheimer&#x27;s Disease Neuroimaging Initiative</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Explaining Multimodal Hateful Meme Detection Models. (arXiv:2204.01734v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01734","description":"<p>Hateful meme detection is a new multimodal task that has gained significant\ntraction in academic and industry research communities. Recently, researchers\nhave applied pre-trained visual-linguistic models to perform the multimodal\nclassification task, and some of these solutions have yielded promising\nresults. However, what these visual-linguistic models learn for the hateful\nmeme classification task remains unclear. For instance, it is unclear if these\nmodels are able to capture the derogatory or slurs references in multimodality\n(i.e., image and text) of the hateful memes. To fill this research gap, this\npaper propose three research questions to improve our understanding of these\nvisual-linguistic models performing the hateful meme classification task. We\nfound that the image modality contributes more to the hateful meme\nclassification task, and the visual-linguistic models are able to perform\nvisual-text slurs grounding to a certain extent. Our error analysis also shows\nthat the visual-linguistic models have acquired biases, which resulted in\nfalse-positive predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hee_M/0/1/0/all/0/1\">Ming Shan Hee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_W/0/1/0/all/0/1\">Wen-Haw Chong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Implicit Neural Stylization. (arXiv:2204.01943v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01943","description":"<p>Representing visual signals by implicit representation (e.g., a coordinate\nbased deep network) has prevailed among many vision tasks. This work explores a\nnew intriguing direction: training a stylized implicit representation, using a\ngeneralized approach that can apply to various 2D and 3D scenarios. We conduct\na pilot study on a variety of implicit functions, including 2D coordinate-based\nrepresentation, neural radiance field, and signed distance function. Our\nsolution is a Unified Implicit Neural Stylization framework, dubbed INS. In\ncontrary to vanilla implicit representation, INS decouples the ordinary\nimplicit function into a style implicit module and a content implicit module,\nin order to separately encode the representations from the style image and\ninput scenes. An amalgamation module is then applied to aggregate these\ninformation and synthesize the stylized output. To regularize the geometry in\n3D scenes, we propose a novel self-distillation geometry consistency loss which\npreserves the geometry fidelity of the stylized scenes. Comprehensive\nexperiments are conducted on multiple task settings, including novel view\nsynthesis of complex scenes, stylization for implicit surfaces, and fitting\nimages using MLPs. We further demonstrate that the learned representation is\ncontinuous not only spatially but also style-wise, leading to effortlessly\ninterpolating between different styles and generating images with new mixed\nstyles. Please refer to the video on our project page for more view synthesis\nresults: https://zhiwenfan.github.io/INS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhiwen Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xinyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dejia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Local Latent Relation Distillation for Self-Adaptive 3D Human Pose Estimation. (arXiv:2204.01971v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01971","description":"<p>Available 3D human pose estimation approaches leverage different forms of\nstrong (2D/3D pose) or weak (multi-view or depth) paired supervision. Barring\nsynthetic or in-studio domains, acquiring such supervision for each new target\nenvironment is highly inconvenient. To this end, we cast 3D pose learning as a\nself-supervised adaptation problem that aims to transfer the task knowledge\nfrom a labeled source domain to a completely unpaired target. We propose to\ninfer image-to-pose via two explicit mappings viz. image-to-latent and\nlatent-to-pose where the latter is a pre-learned decoder obtained from a\nprior-enforcing generative adversarial auto-encoder. Next, we introduce\nrelation distillation as a means to align the unpaired cross-modal samples i.e.\nthe unpaired target videos and unpaired 3D pose sequences. To this end, we\npropose a new set of non-local relations in order to characterize long-range\nlatent pose interactions unlike general contrastive relations where positive\ncouplings are limited to a local neighborhood structure. Further, we provide an\nobjective way to quantify non-localness in order to select the most effective\nrelation set. We evaluate different self-adaptation settings and demonstrate\nstate-of-the-art 3D human pose estimation performance on standard benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kundu_J/0/1/0/all/0/1\">Jogendra Nath Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seth_S/0/1/0/all/0/1\">Siddharth Seth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamkhandi_A/0/1/0/all/0/1\">Anirudh Jamkhandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+YM_P/0/1/0/all/0/1\">Pradyumna YM</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Anirban Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1\">R. Venkatesh Babu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-AI: Dual-path Actor Interaction Learning for Group Activity Recognition. (arXiv:2204.02148v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02148","description":"<p>Learning spatial-temporal relation among multiple actors is crucial for group\nactivity recognition. Different group activities often show the diversified\ninteractions between actors in the video. Hence, it is often difficult to model\ncomplex group activities from a single view of spatial-temporal actor\nevolution. To tackle this problem, we propose a distinct Dual-path Actor\nInteraction (DualAI) framework, which flexibly arranges spatial and temporal\ntransformers in two complementary orders, enhancing actor relations by\nintegrating merits from different spatiotemporal paths. Moreover, we introduce\na novel Multi-scale Actor Contrastive Loss (MAC-Loss) between two interactive\npaths of Dual-AI. Via self-supervised actor consistency in both frame and video\nlevels, MAC-Loss can effectively distinguish individual actor representations\nto reduce action confusion among different actors. Consequently, our Dual-AI\ncan boost group activity recognition by fusing such discriminative features of\ndifferent actors. To evaluate the proposed approach, we conduct extensive\nexperiments on the widely used benchmarks, including Volleyball, Collective\nActivity, and NBA datasets. The proposed Dual-AI achieves state-of-the-art\nperformance on all these datasets. It is worth noting the proposed Dual-AI with\n50% training data outperforms a number of recent approaches with 100% training\ndata. This confirms the generalization power of Dual-AI for group activity\nrecognition, even under the challenging scenarios of limited supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Mingfei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Junhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lina Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNN Filter DB: An Empirical Investigation of Trained Convolutional Filters. (arXiv:2203.15331v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2203.15331","description":"<p>Currently, many theoretical as well as practically relevant questions towards\nthe transferability and robustness of Convolutional Neural Networks (CNNs)\nremain unsolved. While ongoing research efforts are engaging these problems\nfrom various angles, in most computer vision related cases these approaches can\nbe generalized to investigations of the effects of distribution shifts in image\ndata. In this context, we propose to study the shifts in the learned weights of\ntrained CNN models. Here we focus on the properties of the distributions of\ndominantly used 3x3 convolution filter kernels. We collected and publicly\nprovide a dataset with over 1.4 billion filters from hundreds of trained CNNs,\nusing a wide range of datasets, architectures, and vision tasks. In a first use\ncase of the proposed dataset, we can show highly relevant properties of many\npublicly available pre-trained models for practical applications: I) We analyze\ndistribution shifts (or the lack thereof) between trained filters along\ndifferent axes of meta-parameters, like visual category of the dataset, task,\narchitecture, or layer depth. Based on these results, we conclude that model\npre-training can succeed on arbitrary datasets if they meet size and variance\nconditions. II) We show that many pre-trained models contain degenerated\nfilters which make them less robust and less suitable for fine-tuning on target\napplications.\n</p>\n<p>Data &amp; Project website: https://github.com/paulgavrikov/cnn-filter-db\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gavrikov_P/0/1/0/all/0/1\">Paul Gavrikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1\">Janis Keuper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}