{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-11T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision. (arXiv:2204.03685v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03685","description":"<p>Revision is an essential part of the human writing process. It tends to be\nstrategic, adaptive, and, more importantly, iterative in nature. Despite the\nsuccess of large language models on text revision tasks, they are limited to\nnon-iterative, one-shot revisions. Examining and evaluating the capability of\nlarge language models for making continuous revisions and collaborating with\nhuman writers is a critical step towards building effective writing assistants.\nIn this work, we present a human-in-the-loop iterative text revision system,\nRead, Revise, Repeat (R3), which aims at achieving high quality text revisions\nwith minimal human efforts by reading model-generated revisions and user\nfeedbacks, revising documents, and repeating human-machine interactions. In R3,\na text revision model provides text editing suggestions for human writers, who\ncan accept or reject the suggested edits. The accepted edits are then\nincorporated into the model for the next iteration of document revision.\nWriters can therefore revise documents iteratively by interacting with the\nsystem and simply accepting/rejecting its suggested edits until the text\nrevision model stops making further revisions or reaches a predefined maximum\nnumber of revisions. Empirical experiments show that R3 can generate revisions\nwith comparable acceptance rate to human writers at early revision depths, and\nthe human-machine interaction can get higher quality revisions with fewer\niterations and edits. The collected human-model interaction dataset and system\ncode are available at \\url{https://github.com/vipulraheja/IteraTeR}. Our system\ndemonstration is available at \\url{https://youtu.be/lK08tIpEoaE}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wanyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Z/0/1/0/all/0/1\">Zae Myung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raheja_V/0/1/0/all/0/1\">Vipul Raheja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Dhruv Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MHMS: Multimodal Hierarchical Multimedia Summarization. (arXiv:2204.03734v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03734","description":"<p>Multimedia summarization with multimodal output can play an essential role in\nreal-world applications, i.e., automatically generating cover images and titles\nfor news articles or providing introductions to online videos. In this work, we\npropose a multimodal hierarchical multimedia summarization (MHMS) framework by\ninteracting visual and language domains to generate both video and textual\nsummaries. Our MHMS method contains video and textual segmentation and\nsummarization module, respectively. It formulates a cross-domain alignment\nobjective with optimal transport distance which leverages cross-domain\ninteraction to generate the representative keyframe and textual summary. We\nevaluated MHMS on three recent multimodal datasets and demonstrated the\neffectiveness of our method in producing high-quality multimodal summaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jielin Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiacheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengdi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaowen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Simultaneous Speech Translation need Simultaneous Models?. (arXiv:2204.03783v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03783","description":"<p>In simultaneous speech translation (SimulST), finding the best trade-off\nbetween high translation quality and low latency is a challenging task. To meet\nthe latency constraints posed by different application scenarios, multiple\ndedicated SimulST models are usually trained and maintained, causing high\ncomputational costs and increased environmental impact. In this paper, we show\nthat a single model trained offline can effectively serve not only offline but\nalso simultaneous tasks at different latency regimes, bypassing any\ntraining/adaptation procedures. This single-model solution does not only\nfacilitate the adoption of well-established offline techniques and\narchitectures without affecting latency but also yields similar or even better\ntranslation quality compared to the same model trained in the simultaneous\nsetting. Experiments on En$\\rightarrow$\\{De, Es\\} indicate the effectiveness of\nour approach, showing competitive results with the SimulST state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PharmMT: A Neural Machine Translation Approach to Simplify Prescription Directions. (arXiv:2204.03830v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03830","description":"<p>The language used by physicians and health professionals in prescription\ndirections includes medical jargon and implicit directives and causes much\nconfusion among patients. Human intervention to simplify the language at the\npharmacies may introduce additional errors that can lead to potentially severe\nhealth outcomes. We propose a novel machine translation-based approach,\nPharmMT, to automatically and reliably simplify prescription directions into\npatient-friendly language, thereby significantly reducing pharmacist workload.\nWe evaluate the proposed approach over a dataset consisting of over 530K\nprescriptions obtained from a large mail-order pharmacy. The end-to-end system\nachieves a BLEU score of 60.27 against the reference directions generated by\npharmacists, a 39.6% relative improvement over the rule-based normalization.\nPharmacists judged 94.3% of the simplified directions as usable as-is or with\nminimal changes. This work demonstrates the feasibility of a machine\ntranslation-based tool for simplifying prescription directions in real-life.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiazhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lester_C/0/1/0/all/0/1\">Corey Lester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xinyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuting Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vydiswaran_V/0/1/0/all/0/1\">V.G.Vinod Vydiswaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Marvelous Agglutinative Language Effect on Cross Lingual Transfer Learning. (arXiv:2204.03831v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03831","description":"<p>As for multilingual language models, it is important to select languages for\ntraining because of the curse of multilinguality. (Conneau et al., 2020). It is\nknown that using languages with similar language structures is effective for\ncross lingual transfer learning (Pires et al., 2019). However, we demonstrate\nthat using agglutinative languages such as Korean is more effective in cross\nlingual transfer learning. This is a great discovery that will change the\ntraining strategy of cross lingual transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Wooyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_C/0/1/0/all/0/1\">Chaerin Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minjung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Wooju Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Infusing Knowledge from Wikipedia to Enhance Stance Detection. (arXiv:2204.03839v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03839","description":"<p>Stance detection infers a text author's attitude towards a target. This is\nchallenging when the model lacks background knowledge about the target. Here,\nwe show how background knowledge from Wikipedia can help enhance the\nperformance on stance detection. We introduce Wikipedia Stance Detection BERT\n(WS-BERT) that infuses the knowledge into stance encoding. Extensive results on\nthree benchmark datasets covering social media discussions and online debates\nindicate that our model significantly outperforms the state-of-the-art methods\non target-specific stance detection, cross-target stance detection, and\nzero/few-shot stance detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokhberian_N/0/1/0/all/0/1\">Negar Mokhberian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1\">Kristina Lerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Softmax for End-to-End Low-resource Multilingual Speech Recognition. (arXiv:2204.03855v1 [eess.AS])","link":"http://arxiv.org/abs/2204.03855","description":"<p>Low resource speech recognition has been long-suffering from insufficient\ntraining data. While neighbour languages are often used as assistant training\ndata, it would be difficult for the model to induct similar units (character,\nsubword, etc.) across the languages. In this paper, we assume similar units in\nneighbour language share similar term frequency and form a Huffman tree to\nperform multi-lingual hierarchical Softmax decoding. During decoding, the\nhierarchical structure can benefit the training of low-resource languages.\nExperimental results show the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qianying Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yuhang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gong_Z/0/1/0/all/0/1\">Zhuo Gong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_C/0/1/0/all/0/1\">Chenchen Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Minematsu_N/0/1/0/all/0/1\">Nobuaki Minematsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Hao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_F/0/1/0/all/0/1\">Fei Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Pronunciation Assessment using Self-Supervised Speech Representation Learning. (arXiv:2204.03863v1 [eess.AS])","link":"http://arxiv.org/abs/2204.03863","description":"<p>Self-supervised learning (SSL) approaches such as wav2vec 2.0 and HuBERT\nmodels have shown promising results in various downstream tasks in the speech\ncommunity. In particular, speech representations learned by SSL models have\nbeen shown to be effective for encoding various speech-related characteristics.\nIn this context, we propose a novel automatic pronunciation assessment method\nbased on SSL models. First, the proposed method fine-tunes the pre-trained SSL\nmodels with connectionist temporal classification to adapt the English\npronunciation of English-as-a-second-language (ESL) learners in a data\nenvironment. Then, the layer-wise contextual representations are extracted from\nall across the transformer layers of the SSL models. Finally, the automatic\npronunciation score is estimated using bidirectional long short-term memory\nwith the layer-wise contextual representations and the corresponding text. We\nshow that the proposed SSL model-based methods outperform the baselines, in\nterms of the Pearson correlation coefficient, on datasets of Korean ESL learner\nchildren and Speechocean762. Furthermore, we analyze how different\nrepresentations of transformer layers in the SSL model affect the performance\nof the pronunciation assessment task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kim_E/0/1/0/all/0/1\">Eesung Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jeon_J/0/1/0/all/0/1\">Jae-Jin Jeon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seo_H/0/1/0/all/0/1\">Hyeji Seo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Hoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrudeOilNews: An Annotated Crude Oil News Corpus for Event Extraction. (arXiv:2204.03871v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03871","description":"<p>In this paper, we present CrudeOilNews, a corpus of English Crude Oil news\nfor event extraction. It is the first of its kind for Commodity News and serve\nto contribute towards resource building for economic and financial text mining.\nThis paper describes the data collection process, the annotation methodology\nand the event typology used in producing the corpus. Firstly, a seed set of 175\nnews articles were manually annotated, of which a subset of 25 news were used\nas the adjudicated reference test set for inter-annotator and system\nevaluation. Agreement was generally substantial and annotator performance was\nadequate, indicating that the annotation scheme produces consistent event\nannotations of high quality. Subsequently the dataset is expanded through (1)\ndata augmentation and (2) Human-in-the-loop active learning. The resulting\ncorpus has 425 news articles with approximately 11k events annotated. As part\nof active learning process, the corpus was used to train basic event extraction\nmodels for machine labeling, the resulting models also serve as a validation or\nas a pilot study demonstrating the use of the corpus in machine learning\npurposes. The annotated corpus is made available for academic research purpose\nat https://github.com/meisin/CrudeOilNews-Corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Meisin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soon_L/0/1/0/all/0/1\">Lay-Ki Soon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siew_E/0/1/0/all/0/1\">Eu-Gene Siew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugianto_L/0/1/0/all/0/1\">Ly Fie Sugianto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study of Different Ways to Use The Conformer Model For Spoken Language Understanding. (arXiv:2204.03879v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03879","description":"<p>SLU combines ASR and NLU capabilities to accomplish speech-to-intent\nunderstanding. In this paper, we compare different ways to combine ASR and NLU,\nin particular using a single Conformer model with different ways to use its\ncomponents, to better understand the strengths and weaknesses of each approach.\nWe find that it is not necessarily a choice between two-stage decoding and\nend-to-end systems which determines the best system for research or\napplication. System optimization still entails carefully improving the\nperformance of each component. It is difficult to prove that one direction is\nconclusively better than the other. In this paper, we also propose a novel\nconnectionist temporal summarization (CTS) method to reduce the length of\nacoustic encoding sequences while improving the accuracy and processing speed\nof end-to-end models. This method achieves the same intent accuracy as the best\ntwo-stage SLU recognition with complicated and time-consuming decoding but does\nso at lower computational cost. This stacked end-to-end SLU system yields an\nintent accuracy of 93.97% for the SmartLights far-field set, 95.18% for the\nclose-field set, and 99.71% for FluentSpeech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nick J.C. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaojun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transducer-based language embedding for spoken language identification. (arXiv:2204.03888v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03888","description":"<p>The acoustic and linguistic features are important cues for the spoken\nlanguage identification (LID) task. Recent advanced LID systems mainly use\nacoustic features that lack the usage of explicit linguistic feature encoding.\nIn this paper, we propose a novel transducer-based language embedding approach\nfor LID tasks by integrating an RNN transducer model into a language embedding\nframework. Benefiting from the advantages of the RNN transducer's linguistic\nrepresentation capability, the proposed method can exploit both\nphonetically-aware acoustic features and explicit linguistic features for LID\ntasks. Experiments were carried out on the large-scale multilingual LibriSpeech\nand VoxLingua107 datasets. Experimental results showed the proposed method\nsignificantly improves the performance on LID tasks with 12% to 59% and 16% to\n24% relative improvement on in-domain and cross-domain datasets, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1\">Peng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xugang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawai_H/0/1/0/all/0/1\">Hisashi Kawai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adding Connectionist Temporal Summarization into Conformer to Improve Its Decoder Efficiency For Speech Recognition. (arXiv:2204.03889v1 [cs.SD])","link":"http://arxiv.org/abs/2204.03889","description":"<p>The Conformer model is an excellent architecture for speech recognition\nmodeling that effectively utilizes the hybrid losses of connectionist temporal\nclassification (CTC) and attention to train model parameters. To improve the\ndecoding efficiency of Conformer, we propose a novel connectionist temporal\nsummarization (CTS) method that reduces the number of frames required for the\nattention decoder fed from the acoustic sequences generated by the encoder,\nthus reducing operations. However, to achieve such decoding improvements, we\nmust fine-tune model parameters, as cross-attention observations are changed\nand thus require corresponding refinements. Our final experiments show that,\nwith a beamwidth of 4, the LibriSpeech's decoding budget can be reduced by up\nto 20% and for FluentSpeech data it can be reduced by 11%, without losing ASR\naccuracy. An improvement in accuracy is even found for the LibriSpeech\n\"test-other\" set. The word error rate (WER) is reduced by 6\\% relative at the\nbeam width of 1 and by 3% relative at the beam width of 4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nick J.C. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_Z/0/1/0/all/0/1\">Zongfeng Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaojun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Semi-Supervised Learning of Automatic Post-Editing: Data-Synthesis by Infilling Mask with Erroneous Tokens. (arXiv:2204.03896v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03896","description":"<p>Semi-supervised learning that leverages synthetic training data has been\nwidely adopted in the field of Automatic post-editing (APE) to overcome the\nlack of human-annotated training data. In that context, data-synthesis methods\nto create high-quality synthetic data have also received much attention.\nConsidering that APE takes machine-translation outputs containing translation\nerrors as input, we propose a noising-based data-synthesis method that uses a\nmask language model to create noisy texts through substituting masked tokens\nwith erroneous tokens, yet following the error-quantity statistics appearing in\ngenuine APE data. In addition, we propose corpus interleaving, which is to\ncombine two separate synthetic data by taking only advantageous samples, to\nfurther enhance the quality of the synthetic data created with our noising\nmethod. Experimental results reveal that using the synthetic data created with\nour approach results in significant improvements in APE performance upon using\nother synthetic data created with different existing data-synthesis methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">WonKee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_S/0/1/0/all/0/1\">Seong-Hwan Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_B/0/1/0/all/0/1\">Baikjin Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jong-Hyeok Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model. (arXiv:2204.03905v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03905","description":"<p>Pretrained language models have served as important backbones for natural\nlanguage processing. Recently, in-domain pretraining has been shown to benefit\nvarious domain-specific downstream tasks. In the biomedical domain, natural\nlanguage generation (NLG) tasks are of critical importance, while understudied.\nApproaching natural language understanding (NLU) tasks as NLG achieves\nsatisfying performance in the general domain through constrained language\ngeneration or language prompting. We emphasize the lack of in-domain generative\nlanguage models and the unsystematic generative downstream benchmarks in the\nbiomedical domain, hindering the development of the research community. In this\nwork, we introduce the generative language model BioBART that adapts BART to\nthe biomedical domain. We collate various biomedical language generation tasks\nincluding dialogue, summarization, entity linking, and named entity\nrecognition. BioBART pretrained on PubMed abstracts has enhanced performance\ncompared to BART and set strong baselines on several tasks. Furthermore, we\nconduct ablation studies on the pretraining tasks for BioBART and find that\nsentence permutation has negative effects on downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_R/0/1/0/all/0/1\">Ruyi Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yutao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Rewriting to Remembering: Common Ground for Conversational QA Models. (arXiv:2204.03930v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03930","description":"<p>In conversational QA, models have to leverage information in previous turns\nto answer upcoming questions. Current approaches, such as Question Rewriting,\nstruggle to extract relevant information as the conversation unwinds. We\nintroduce the Common Ground (CG), an approach to accumulate conversational\ninformation as it emerges and select the relevant information at every turn. We\nshow that CG offers a more efficient and human-like way to exploit\nconversational information compared to existing approaches, leading to\nimprovements on Open Domain Conversational QA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tredici_M/0/1/0/all/0/1\">Marco Del Tredici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barlacchi_G/0/1/0/all/0/1\">Gianni Barlacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byrne_B/0/1/0/all/0/1\">Bill Byrne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gispert_A/0/1/0/all/0/1\">Adri&#xe0; de Gispert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GigaST: A 10,000-hour Pseudo Speech Translation Corpus. (arXiv:2204.03939v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03939","description":"<p>This paper introduces GigaST, a large-scale pseudo speech translation (ST)\ncorpus. We create the corpus by translating the text in GigaSpeech, an English\nASR corpus, into German and Chinese. The training set is translated by a strong\nmachine translation system and the test set is translated by human. ST models\ntrained with an addition of our corpus obtain new state-of-the-art results on\nthe MuST-C English-German benchmark test set. We provide a detailed description\nof the translation process and verify its quality. We make the translated text\ndata public and hope to facilitate research in speech translation.\nAdditionally, we also release the training scripts on NeurST to make it easy to\nreplicate our systems. GigaST dataset is available at\nhttps://st-benchmark.github.io/resources/GigaST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chengqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1\">Chutong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jun Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RuBioRoBERTa: a pre-trained biomedical language model for Russian language biomedical text mining. (arXiv:2204.03951v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03951","description":"<p>This paper presents several BERT-based models for Russian language biomedical\ntext mining (RuBioBERT, RuBioRoBERTa). The models are pre-trained on a corpus\nof freely available texts in the Russian biomedical domain. With this\npre-training, our models demonstrate state-of-the-art results on RuMedBench -\nRussian medical language understanding benchmark that covers a diverse set of\ntasks, including text classification, question answering, natural language\ninference, and named entity recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yalunin_A/0/1/0/all/0/1\">Alexander Yalunin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nesterov_A/0/1/0/all/0/1\">Alexander Nesterov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umerenkov_D/0/1/0/all/0/1\">Dmitriy Umerenkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RubCSG at SemEval-2022 Task 5: Ensemble learning for identifying misogynous MEMEs. (arXiv:2204.03953v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03953","description":"<p>This work presents an ensemble system based on various uni-modal and bi-modal\nmodel architectures developed for the SemEval 2022 Task 5: MAMI-Multimedia\nAutomatic Misogyny Identification. The challenge organizers provide an English\nmeme dataset to develop and train systems for identifying and classifying\nmisogynous memes. More precisely, the competition is separated into two\nsub-tasks: sub-task A asks for a binary decision as to whether a meme expresses\nmisogyny, while sub-task B is to classify misogynous memes into the potentially\noverlapping sub-categories of stereotype, shaming, objectification, and\nviolence. For our submission, we implement a new model fusion network and\nemploy an ensemble learning approach for better performance. With this\nstructure, we achieve a 0.755 macroaverage F1-score (11th) in sub-task A and a\n0.709 weighted-average F1-score (10th) in sub-task B.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wentao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boenninghoff_B/0/1/0/all/0/1\">Benedikt Boenninghoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roehrig_J/0/1/0/all/0/1\">Jonas Roehrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolossa_D/0/1/0/all/0/1\">Dorothea Kolossa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bag-of-Words vs. Sequence vs. Graph vs. Hierarchy for Single- and Multi-Label Text Classification. (arXiv:2204.03954v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03954","description":"<p>Graph neural networks have triggered a resurgence of graph-based text\nclassification methods, defining today's state of the art. We show that a\nsimple multi-layer perceptron (MLP) using a Bag of Words (BoW) outperforms the\nrecent graph-based models TextGCN and HeteGCN in an inductive text\nclassification setting and is comparable with HyperGAT in single-label\nclassification. We also run our own experiments on multi-label classification,\nwhere the simple MLP outperforms the recent sequential-based gMLP and aMLP\nmodels. Moreover, we fine-tune a sequence-based BERT and a lightweight\nDistilBERT model, which both outperform all models on both single-label and\nmulti-label settings in most datasets. These results question the importance of\nsynthetic graphs used in modern text classifiers. In terms of parameters,\nDistilBERT is still twice as large as our BoW-based wide MLP, while graph-based\nmodels like TextGCN require setting up an $\\mathcal{O}(N^2)$ graph, where $N$\nis the vocabulary plus corpus size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diera_A/0/1/0/all/0/1\">Andor Diera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bao Xin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khera_B/0/1/0/all/0/1\">Bhakti Khera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuser_T/0/1/0/all/0/1\">Tim Meuser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_T/0/1/0/all/0/1\">Tushar Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1\">Lukas Galke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhance Incomplete Utterance Restoration by Joint Learning Token Extraction and Text Generation. (arXiv:2204.03958v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03958","description":"<p>This paper introduces a model for incomplete utterance restoration (IUR).\nDifferent from prior studies that only work on extraction or abstraction\ndatasets, we design a simple but effective model, working for both scenarios of\nIUR. Our design simulates the nature of IUR, where omitted tokens from the\ncontext contribute to restoration. From this, we construct a Picker that\nidentifies the omitted tokens. To support the picker, we design two label\ncreation methods (soft and hard labels), which can work in cases of no\nannotation of the omitted tokens. The restoration is done by using a Generator\nwith the help of the Picker on joint learning. Promising results on four\nbenchmark datasets in extraction and abstraction scenarios show that our model\nis better than the pretrained T5 and non-generative language model methods in\nboth rich and limited training data settings. The code will be also available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inoue_S/0/1/0/all/0/1\">Shumpei Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tsungwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_N/0/1/0/all/0/1\">Nguyen Hong Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh-Tien Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FashionCLIP: Connecting Language and Images for Product Representations. (arXiv:2204.03972v1 [cs.IR])","link":"http://arxiv.org/abs/2204.03972","description":"<p>The steady rise of online shopping goes hand in hand with the development of\nincreasingly complex ML and NLP models. While most use cases are cast as\nspecialized supervised learning problems, we argue that practitioners would\ngreatly benefit from more transferable representations of products. In this\nwork, we build on recent developments in contrastive learning to train\nFashionCLIP, a CLIP-like model for the fashion industry. We showcase its\ncapabilities for retrieval, classification and grounding, and release our model\nand code to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chia_P/0/1/0/all/0/1\">Patrick John Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attanasio_G/0/1/0/all/0/1\">Giuseppe Attanasio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terragni_S/0/1/0/all/0/1\">Silvia Terragni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magalhaes_A/0/1/0/all/0/1\">Ana Rita Magalh&#xe3;es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_D/0/1/0/all/0/1\">Diogo Goncalves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greco_C/0/1/0/all/0/1\">Ciro Greco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1\">Jacopo Tagliabue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KGI: An Integrated Framework for Knowledge Intensive Language Tasks. (arXiv:2204.03985v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03985","description":"<p>In a recent work, we presented a novel state-of-the-art approach to zero-shot\nslot filling that extends dense passage retrieval with hard negatives and\nrobust training procedures for retrieval augmented generation models. In this\npaper, we propose a system based on an enhanced version of this approach where\nwe train task specific models for other knowledge intensive language tasks,\nsuch as open domain question answering (QA), dialogue and fact checking. Our\nsystem achieves results comparable to the best models in the KILT leaderboards.\nMoreover, given a user query, we show how the output from these different\nmodels can be combined to cross-examine each other. Particularly, we show how\naccuracy in dialogue can be improved using the QA model. A short video\ndemonstrating the system is available here -\n\\url{https://ibm.box.com/v/kgi-interactive-demo} .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1\">Md Faisal Mahbub Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_M/0/1/0/all/0/1\">Michael Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled Latent Speech Representation for Automatic Pathological Intelligibility Assessment. (arXiv:2204.04016v1 [eess.AS])","link":"http://arxiv.org/abs/2204.04016","description":"<p>Speech intelligibility assessment plays an important role in the therapy of\npatients suffering from pathological speech disorders. Automatic and objective\nmeasures are desirable to assist therapists in their traditionally subjective\nand labor-intensive assessments. In this work, we investigate a novel approach\nfor obtaining such a measure using the divergence in disentangled latent speech\nrepresentations of a parallel utterance pair, obtained from a healthy reference\nand a pathological speaker. Experiments on an English database of Cerebral\nPalsy patients, using all available utterances per speaker, show high and\nsignificant correlation values (R = -0.9) with subjective intelligibility\nmeasures, while having only minimal deviation (+-0.01) across four different\nreference speaker pairs. We also demonstrate the robustness of the proposed\nmethod (R = -0.89 deviating +-0.02 over 1000 iterations) by considering a\nsignificantly smaller amount of utterances per speaker. Our results are among\nthe first to show that disentangled speech representations can be used for\nautomatic pathological speech intelligibility assessment, resulting in a\nreference speaker pair invariant method, applicable in scenarios with only few\nutterances available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Weise_T/0/1/0/all/0/1\">Tobias Weise</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klumpp_P/0/1/0/all/0/1\">Philipp Klumpp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Noeth_E/0/1/0/all/0/1\">Elmar Noeth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heismann_B/0/1/0/all/0/1\">Bjoern Heismann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schuster_M/0/1/0/all/0/1\">Maria Schuster</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Seung Hee Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fair and Argumentative Language Modeling for Computational Argumentation. (arXiv:2204.04026v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04026","description":"<p>Although much work in NLP has focused on measuring and mitigating\nstereotypical bias in semantic spaces, research addressing bias in\ncomputational argumentation is still in its infancy. In this paper, we address\nthis research gap and conduct a thorough investigation of bias in argumentative\nlanguage models. To this end, we introduce ABBA, a novel resource for bias\nmeasurement specifically tailored to argumentation. We employ our resource to\nassess the effect of argumentative fine-tuning and debiasing on the intrinsic\nbias found in transformer-based language models using a lightweight\nadapter-based approach that is more sustainable and parameter-efficient than\nfull fine-tuning. Finally, we analyze the potential impact of language model\ndebiasing on the performance in argument quality prediction, a downstream task\nof computational argumentation. Our results show that we are able to\nsuccessfully and sustainably remove bias in general and argumentative language\nmodels while preserving (and sometimes improving) model performance in\ndownstream tasks. We make all experimental code and data available at\nhttps://github.com/umanlp/FairArgumentativeLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holtermann_C/0/1/0/all/0/1\">Carolin Holtermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponzetto_S/0/1/0/all/0/1\">Simone Paolo Ponzetto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Checking HateCheck: a cross-functional analysis of behaviour-aware learning for hate speech detection. (arXiv:2204.04042v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04042","description":"<p>Behavioural testing -- verifying system capabilities by validating\nhuman-designed input-output pairs -- is an alternative evaluation method of\nnatural language processing systems proposed to address the shortcomings of the\nstandard approach: computing metrics on held-out data. While behavioural tests\ncapture human prior knowledge and insights, there has been little exploration\non how to leverage them for model training and development. With this in mind,\nwe explore behaviour-aware learning by examining several fine-tuning schemes\nusing HateCheck, a suite of functional tests for hate speech detection systems.\nTo address potential pitfalls of training on data originally intended for\nevaluation, we train and evaluate models on different configurations of\nHateCheck by holding out categories of test cases, which enables us to estimate\nperformance on potentially overlooked system properties. The fine-tuning\nprocedure led to improvements in the classification accuracy of held-out\nfunctionalities and identity groups, suggesting that models can potentially\ngeneralise to overlooked functionalities. However, performance on held-out\nfunctionality classes and i.i.d. hate speech detection data decreased, which\nindicates that generalisation occurs mostly across functionalities from the\nsame class and that the procedure led to overfitting to the HateCheck data\ndistribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Araujo_P/0/1/0/all/0/1\">Pedro Henrique Luz de Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C-NMT: A Collaborative Inference Framework for Neural Machine Translation. (arXiv:2204.04043v1 [cs.LG])","link":"http://arxiv.org/abs/2204.04043","description":"<p>Collaborative Inference (CI) optimizes the latency and energy consumption of\ndeep learning inference through the inter-operation of edge and cloud devices.\nAlbeit beneficial for other tasks, CI has never been applied to the sequence-\nto-sequence mapping problem at the heart of Neural Machine Translation (NMT).\nIn this work, we address the specific issues of collaborative NMT, such as\nestimating the latency required to generate the (unknown) output sequence, and\nshow how existing CI methods can be adapted to these applications. Our\nexperiments show that CI can reduce the latency of NMT by up to 44% compared to\na non-collaborative approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yukai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiaro_R/0/1/0/all/0/1\">Roberta Chiaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macii_E/0/1/0/all/0/1\">Enrico Macii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poncino_M/0/1/0/all/0/1\">Massimo Poncino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagliari_D/0/1/0/all/0/1\">Daniele Jahier Pagliari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Tokenisation by Alternative Treatment of Spaces. (arXiv:2204.04058v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04058","description":"<p>Tokenisation is the first step in almost all NLP tasks, and state-of-the-art\ntransformer-based language models all use subword tokenisation algorithms to\nprocess input text. Existing algorithms have problems, often producing\ntokenisations of limited linguistic validity, and representing equivalent\nstrings differently depending on their position within a word. We hypothesise\nthat these problems hinder the ability of transformer-based models to handle\ncomplex words, and suggest that these problems are a result of allowing tokens\nto include spaces. We thus experiment with an alternative tokenisation approach\nwhere spaces are always treated as individual tokens. Specifically, we apply\nthis modification to the BPE and Unigram algorithms. We find that our modified\nalgorithms lead to improved performance on downstream NLP tasks that involve\nhandling complex words, whilst having no detrimental effect on performance in\ngeneral natural language understanding tasks. Intrinsically, we find our\nmodified algorithms give more morphologically correct tokenisations, in\nparticular when handling prefixes. Given the results of our experiments, we\nadvocate for always treating spaces as individual tokens as an improved\ntokenisation method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gow_Smith_E/0/1/0/all/0/1\">Edward Gow-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madabushi_H/0/1/0/all/0/1\">Harish Tayyar Madabushi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villavicencio_A/0/1/0/all/0/1\">Aline Villavicencio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning the Ordering of Coordinate Compounds and Elaborate Expressions in Hmong, Lahu, and Chinese. (arXiv:2204.04080v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04080","description":"<p>Coordinate compounds (CCs) and elaborate expressions (EEs) are coordinate\nconstructions common in languages of East and Southeast Asia. Mortensen (2006)\nclaims that (1) the linear ordering of EEs and CCs in Hmong, Lahu, and Chinese\ncan be predicted via phonological hierarchies and (2) these phonological\nhierarchies lack a clear phonetic rationale. These claims are significant\nbecause morphosyntax has often been seen as in a feed-forward relationship with\nphonology, and phonological generalizations have often been assumed to be\nphonetically \"natural\". We investigate whether the ordering of CCs and EEs can\nbe learned empirically and whether computational models (classifiers and\nsequence labeling models) learn unnatural hierarchies similar to those posited\nby Mortensen (2006). We find that decision trees and SVMs learn to predict the\norder of CCs/EEs on the basis of phonology, with DTs learning hierarchies\nstrikingly similar to those proposed by Mortensen. However, we also find that a\nneural sequence labeling model is able to learn the ordering of elaborate\nexpressions in Hmong very effectively without using any phonological\ninformation. We argue that EE ordering can be learned through two independent\nroutes: phonology and lexical distribution, presenting a more nuanced picture\nthan previous work. [ISO 639-3:hmn, lhu, cmn]\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Chenxuan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Katherine J. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1\">David R. Mortensen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Representation Learning beyond Masked Language Modeling. (arXiv:2204.04163v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04163","description":"<p>How do masked language models (MLMs) such as BERT learn contextual\nrepresentations? In this work, we analyze the learning dynamics of MLMs. We\nfind that MLMs adopt sampled embeddings as anchors to estimate and inject\ncontextual semantics to representations, which limits the efficiency and\neffectiveness of MLMs. To address these issues, we propose TACO, a simple yet\neffective representation learning approach to directly model global semantics.\nTACO extracts and aligns contextual semantics hidden in contextualized\nrepresentations to encourage models to attend global semantics when generating\ncontextualized representations. Experiments on the GLUE benchmark show that\nTACO achieves up to 5x speedup and up to 1.2 points average improvement over\nexisting MLMs. The code is available at https://github.com/FUZHIYI/TACO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhiyi Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRAM: Fast Fine-tuning of Pre-trained Language Models for Content-based Collaborative Filtering. (arXiv:2204.04179v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04179","description":"<p>Content-based collaborative filtering (CCF) provides personalized item\nrecommendations based on both users' interaction history and items' content\ninformation. Recently, pre-trained language models (PLM) have been used to\nextract high-quality item encodings for CCF. However, it is resource-intensive\nto finetune PLM in an end-to-end (E2E) manner in CCF due to its multi-modal\nnature: optimization involves redundant content encoding for interactions from\nusers. For this, we propose GRAM (GRadient Accumulation for Multi-modality):\n(1) Single-step GRAM which aggregates gradients for each item while maintaining\ntheoretical equivalence with E2E, and (2) Multi-step GRAM which further\naccumulates gradients across multiple training steps, with less than 40\\% GPU\nmemory footprint of E2E. We empirically confirm that GRAM achieves a remarkable\nboost in training efficiency based on five datasets from two task domains of\nKnowledge Tracing and News Recommendation, where single-step and multi-step\nGRAM achieve 4x and 45x training speedup on average, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yoonseok Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyu Seok Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsam Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Juneyoung Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey on Publicly Available Sinhala Natural Language Processing Tools and Research. (arXiv:1906.02358v12 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1906.02358","description":"<p>Sinhala is the native language of the Sinhalese people who make up the\nlargest ethnic group of Sri Lanka. The language belongs to the globe-spanning\nlanguage tree, Indo-European. However, due to poverty in both linguistic and\neconomic capital, Sinhala, in the perspective of Natural Language Processing\ntools and research, remains a resource-poor language which has neither the\neconomic drive its cousin English has nor the sheer push of the law of numbers\na language such as Chinese has. A number of research groups from Sri Lanka have\nnoticed this dearth and the resultant dire need for proper tools and research\nfor Sinhala natural language processing. However, due to various reasons, these\nattempts seem to lack coordination and awareness of each other. The objective\nof this paper is to fill that gap of a comprehensive literature survey of the\npublicly available Sinhala natural language tools and research so that the\nresearchers working in this field can better utilize contributions of their\npeers. As such, we shall be uploading this paper to arXiv and perpetually\nupdate it periodically to reflect the advances made in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Few-Shot Semantic Parser for Wizard-of-Oz Dialogues with the Precise ThingTalk Representation. (arXiv:2009.07968v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.07968","description":"<p>Previous attempts to build effective semantic parsers for Wizard-of-Oz (WOZ)\nconversations suffer from the difficulty in acquiring a high-quality, manually\nannotated training set. Approaches based only on dialogue synthesis are\ninsufficient, as dialogues generated from state-machine based models are poor\napproximations of real-life conversations. Furthermore, previously proposed\ndialogue state representations are ambiguous and lack the precision necessary\nfor building an effective agent. This paper proposes a new dialogue\nrepresentation and a sample-efficient methodology that can predict precise\ndialogue states in WOZ conversations. We extended the ThingTalk representation\nto capture all information an agent needs to respond properly. Our training\nstrategy is sample-efficient: we combine (1) fewshot data sparsely sampling the\nfull dialogue space and (2) synthesized data covering a subset space of\ndialogues generated by a succinct state-based dialogue model. The completeness\nof the extended ThingTalk language is demonstrated with a fully operational\nagent, which is also used in training data synthesis. We demonstrate the\neffectiveness of our methodology on MultiWOZ 3.0, a reannotation of the\nMultiWOZ 2.1 dataset in ThingTalk. ThingTalk can represent 98% of the test\nturns, while the simulator can emulate 85% of the validation set. We train a\ncontextual semantic parser using our strategy, and obtain 79% turn-by-turn\nexact match accuracy on the reannotated test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campagna_G/0/1/0/all/0/1\">Giovanni Campagna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semnani_S/0/1/0/all/0/1\">Sina J. Semnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kearns_R/0/1/0/all/0/1\">Ryan Kearns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_L/0/1/0/all/0/1\">Lucas Jun Koba Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Silei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1\">Monica S. Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Math Word Problems using Pretrained Multilingual Language Models. (arXiv:2105.08928v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.08928","description":"<p>In this paper, we revisit math word problems~(MWPs) from the cross-lingual\nand multilingual perspective. We construct our MWP solvers over pretrained\nmultilingual language models using sequence-to-sequence model with copy\nmechanism. We compare how the MWP solvers perform in cross-lingual and\nmultilingual scenarios. To facilitate the comparison of cross-lingual\nperformance, we first adapt the large-scale English dataset MathQA as a\ncounterpart of the Chinese dataset Math23K. Then we extend several English\ndatasets to bilingual datasets through machine translation plus human\nannotation. Our experiments show that the MWP solvers may not be transferred to\na different language even if the target expressions have the same operator set\nand constants. But for both cross-lingual and multilingual cases, it can be\nbetter generalized if problem types exist on both source language and target\nlanguage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Minghuan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lingxiao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight Cross-Lingual Sentence Representation Learning. (arXiv:2105.13856v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.13856","description":"<p>Large-scale models for learning fixed-dimensional cross-lingual sentence\nrepresentations like LASER (Artetxe and Schwenk, 2019b) lead to significant\nimprovement in performance on downstream tasks. However, further increases and\nmodifications based on such large-scale models are usually impractical due to\nmemory limitations. In this work, we introduce a lightweight dual-transformer\narchitecture with just 2 layers for generating memory-efficient cross-lingual\nsentence representations. We explore different training tasks and observe that\ncurrent cross-lingual training tasks leave a lot to be desired for this shallow\narchitecture. To ameliorate this, we propose a novel cross-lingual language\nmodel, which combines the existing single-word masked language model with the\nnewly proposed cross-lingual token-level reconstruction task. We further\naugment the training task by the introduction of two computationally-lite\nsentence-level contrastive learning tasks to enhance the alignment of\ncross-lingual sentence representation space, which compensates for the learning\nbottleneck of the lightweight transformer for generative tasks. Our comparisons\nwith competing models on cross-lingual sentence retrieval and multilingual\ndocument classification confirm the effectiveness of the newly proposed\ntraining tasks for a shallow model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhuoyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Prakhar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Chenhui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation. (arXiv:2106.15078v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.15078","description":"<p>Neural text generation models are typically trained by maximizing\nlog-likelihood with the sequence cross entropy (CE) loss, which encourages an\n\\emph{exact} token-by-token match between a target sequence with a generated\nsequence. Such training objective is sub-optimal when the target sequence is\nnot perfect, e.g., when the target sequence is corrupted with noises, or when\nonly weak sequence supervision is available. To address the challenge, we\npropose a novel Edit-Invariant Sequence Loss (EISL), which computes the\nmatching loss of a target n-gram with all n-grams in the generated sequence.\nEISL is designed to be robust to various noises and edits in the target\nsequences. Moreover, the EISL computation is essentially an approximate\nconvolution operation with target n-grams as kernels, which is easy to\nimplement and efficient to compute with existing libraries. To demonstrate the\neffectiveness of EISL, we conduct experiments on a wide range of tasks,\nincluding machine translation with noisy target sequences, unsupervised text\nstyle transfer with only weak training signals, and non-autoregressive\ngeneration with non-predefined generation order. Experimental results show our\nmethod significantly outperforms the common CE loss and other strong baselines\non all the tasks. EISL has a simple API which can be used as a drop-in\nreplacement of the CE loss: https://anonymous.4open.science/r/EISLLoss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1\">Tianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Resource Adaptation of Open-Domain Generative Chatbots. (arXiv:2108.06329v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.06329","description":"<p>Recent work building open-domain chatbots has demonstrated that increasing\nmodel size improves performance. On the other hand, latency and connectivity\nconsiderations dictate the move of digital assistants on the device. Giving a\ndigital assistant like Siri, Alexa, or Google Assistant the ability to discuss\njust about anything leads to the need for reducing the chatbot model size such\nthat it fits on the user's device. We demonstrate that low parameter models can\nsimultaneously retain their general knowledge conversational abilities while\nimproving in a specific domain. Additionally, we propose a generic framework\nthat accounts for variety in question types, tracks reference throughout\nmulti-turn conversations, and removes inconsistent and potentially toxic\nresponses. Our framework seamlessly transitions between chatting and performing\ntransactional tasks, which will ultimately make interactions with digital\nassistants more human-like. We evaluate our framework on 1 internal and 4\npublic benchmark datasets using both automatic (Perplexity) and human (SSA -\nSensibleness and Specificity Average) evaluation metrics and establish\ncomparable performance while reducing model parameters by 90%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gerhard_Young_G/0/1/0/all/0/1\">Greyson Gerhard-Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anantha_R/0/1/0/all/0/1\">Raviteja Anantha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chappidi_S/0/1/0/all/0/1\">Srinivas Chappidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmeister_B/0/1/0/all/0/1\">Bj&#xf6;rn Hoffmeister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Group-based Distinctive Image Captioning with Memory Attention. (arXiv:2108.09151v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09151","description":"<p>Describing images using natural language is widely known as image captioning,\nwhich has made consistent progress due to the development of computer vision\nand natural language generation techniques. Though conventional captioning\nmodels achieve high accuracy based on popular metrics, i.e., BLEU, CIDEr, and\nSPICE, the ability of captions to distinguish the target image from other\nsimilar images is under-explored. To generate distinctive captions, a few\npioneers employ contrastive learning or re-weighted the ground-truth captions,\nwhich focuses on one single input image. However, the relationships between\nobjects in a similar image group (e.g., items or properties within the same\nalbum or fine-grained events) are neglected. In this paper, we improve the\ndistinctiveness of image captions using a Group-based Distinctive Captioning\nModel (GdisCap), which compares each image with other images in one similar\ngroup and highlights the uniqueness of each image. In particular, we propose a\ngroup-based memory attention (GMA) module, which stores object features that\nare unique among the image group (i.e., with low similarity to objects in other\nimages). These unique object features are highlighted when generating captions,\nresulting in more distinctive captions. Furthermore, the distinctive words in\nthe ground-truth captions are selected to supervise the language decoder and\nGMA. Finally, we propose a new evaluation metric, distinctive word rate\n(DisWordRate) to measure the distinctiveness of captions. Quantitative results\nindicate that the proposed method significantly improves the distinctiveness of\nseveral baseline models, and achieves the state-of-the-art performance on both\naccuracy and distinctiveness. Results of a user study agree with the\nquantitative evaluation and demonstrate the rationality of the new metric\nDisWordRate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiuniu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenjia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Antoni B. Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep learning. (arXiv:2110.13005v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.13005","description":"<p>In the last few years, the memory requirements to train state-of-the-art\nneural networks have far exceeded the DRAM capacities of modern hardware\naccelerators. This has necessitated the development of efficient algorithms to\ntrain these neural networks in parallel on large-scale GPU-based clusters.\nSince computation is relatively inexpensive on modern GPUs, designing and\nimplementing extremely efficient communication in these parallel training\nalgorithms is critical for extracting the maximum performance. This paper\npresents AxoNN, a parallel deep learning framework that exploits asynchrony and\nmessage-driven execution to schedule neural network operations on each GPU,\nthereby reducing GPU idle time and maximizing hardware efficiency. By using the\nCPU memory as a scratch space for offloading data periodically during training,\nAxoNN is able to reduce GPU memory consumption by four times. This allows us to\nincrease the number of parameters per GPU by four times, thus reducing the\namount of communication and increasing performance by over 13%. When tested\nagainst large transformer models with 12-100 billion parameters on 48-384\nNVIDIA Tesla V100 GPUs, AxoNN achieves a per-GPU throughput of 49.4-54.78% of\ntheoretical peak and reduces the training time by 22-37 days (15-25% speedup)\nas compared to the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Siddharth Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatele_A/0/1/0/all/0/1\">Abhinav Bhatele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures. (arXiv:2112.05224v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2112.05224","description":"<p>We investigate a new threat to neural sequence-to-sequence (seq2seq) models:\ntraining-time attacks that cause models to \"spin\" their outputs so as to\nsupport an adversary-chosen sentiment or point of view -- but only when the\ninput contains adversary-chosen trigger words. For example, a spinned\nsummarization model outputs positive summaries of any text that mentions the\nname of some individual or organization.\n</p>\n<p>Model spinning introduces a \"meta-backdoor\" into a model. Whereas\nconventional backdoors cause models to produce incorrect outputs on inputs with\nthe trigger, outputs of spinned models preserve context and maintain standard\naccuracy metrics, yet also satisfy a meta-task chosen by the adversary.\n</p>\n<p>Model spinning enables propaganda-as-a-service, where propaganda is defined\nas biased speech. An adversary can create customized language models that\nproduce desired spins for chosen triggers, then deploy these models to generate\ndisinformation (a platform attack), or else inject them into ML training\npipelines (a supply-chain attack), transferring malicious functionality to\ndownstream models trained by victims.\n</p>\n<p>To demonstrate the feasibility of model spinning, we develop a new\nbackdooring technique. It stacks an adversarial meta-task onto a seq2seq model,\nbackpropagates the desired meta-task output to points in the word-embedding\nspace we call \"pseudo-words,\" and uses pseudo-words to shift the entire output\ndistribution of the seq2seq model. We evaluate this attack on language\ngeneration, summarization, and translation models with different triggers and\nmeta-tasks such as sentiment, toxicity, and entailment. Spinned models largely\nmaintain their accuracy metrics (ROUGE and BLEU) while shifting their outputs\nto satisfy the adversary's meta-task. We also show that, in the case of a\nsupply-chain attack, the spin functionality transfers to downstream models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bagdasaryan_E/0/1/0/all/0/1\">Eugene Bagdasaryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmatikov_V/0/1/0/all/0/1\">Vitaly Shmatikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Homepage2Vec: Language-Agnostic Website Embedding and Classification. (arXiv:2201.03677v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03677","description":"<p>Currently, publicly available models for website classification do not offer\nan embedding method and have limited support for languages beyond English. We\nrelease a dataset of more than two million category-labeled websites in 92\nlanguages collected from Curlie, the largest multilingual human-edited Web\ndirectory. The dataset contains 14 website categories aligned across languages.\nAlongside it, we introduce Homepage2Vec, a machine-learned pre-trained model\nfor classifying and embedding websites based on their homepage in a\nlanguage-agnostic way. Homepage2Vec, thanks to its feature set (textual\ncontent, metadata tags, and visual attributes) and recent progress in natural\nlanguage representation, is language-independent by design and generates\nembedding-based representations. We show that Homepage2Vec correctly classifies\nwebsites with a macro-averaged F1-score of 0.90, with stable performance across\nlow- as well as high-resource languages. Feature analysis shows that a small\nsubset of efficiently computable features suffices to achieve high performance\neven with limited computational resources. We make publicly available the\ncurated Curlie dataset aligned across languages, the pre-trained Homepage2Vec\nmodel, and libraries\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lugeon_S/0/1/0/all/0/1\">Sylvain Lugeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccardi_T/0/1/0/all/0/1\">Tiziano Piccardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Many Ways to be Lonely: Fine-grained Characterization of Loneliness and its Potential Changes in COVID-19. (arXiv:2201.07423v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.07423","description":"<p>Loneliness has been associated with negative outcomes for physical and mental\nhealth. Understanding how people express and cope with various forms of\nloneliness is critical for early screening and targeted interventions to reduce\nloneliness, particularly among vulnerable groups such as young adults. To\nexamine how different forms of loneliness and coping strategies manifest in\nloneliness self-disclosure, we built a dataset, FIG-Loneliness (FIne-Grained\nLoneliness) by using Reddit posts in two young adult-focused forums and two\nloneliness related forums consisting of a diverse age group. We provided\nannotations by trained human annotators for binary and fine-grained loneliness\nclassifications of the posts. Trained on FIG-Loneliness, two BERT-based models\nwere used to understand loneliness forms and authors' coping strategies in\nthese forums. Our binary loneliness classification achieved an accuracy above\n97%, and fine-grained loneliness category classification reached an average\naccuracy of 77% across all labeled categories. With FIG-Loneliness and model\npredictions, we found that loneliness expressions in the young adults related\nforums were distinct from other forums. Those in young adult-focused forums\nwere more likely to express concerns pertaining to peer relationship, and were\npotentially more sensitive to geographical isolation impacted by the COVID-19\npandemic lockdown. Also, we showed that different forms of loneliness have\ndifferential use in coping strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yueyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yunfan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leqi_L/0/1/0/all/0/1\">Liu Leqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winkielman_P/0/1/0/all/0/1\">Piotr Winkielman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embarrassingly Simple Performance Prediction for Abductive Natural Language Inference. (arXiv:2202.10408v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.10408","description":"<p>The task of abductive natural language inference (\\alpha{}nli), to decide\nwhich hypothesis is the more likely explanation for a set of observations, is a\nparticularly difficult type of NLI. Instead of just determining a causal\nrelationship, it requires common sense to also evaluate how reasonable an\nexplanation is. All recent competitive systems build on top of contextualized\nrepresentations and make use of transformer architectures for learning an NLI\nmodel. When somebody is faced with a particular NLI task, they need to select\nthe best model that is available. This is a time-consuming and resource-intense\nendeavour. To solve this practical problem, we propose a simple method for\npredicting the performance without actually fine-tuning the model. We do this\nby testing how well the pre-trained models perform on the \\alpha{}nli task when\njust comparing sentence embeddings with cosine similarity to what the\nperformance that is achieved when training a classifier on top of these\nembeddings. We show that the accuracy of the cosine similarity approach\ncorrelates strongly with the accuracy of the classification approach with a\nPearson correlation coefficient of 0.65. Since the similarity computation is\norders of magnitude faster to compute on a given dataset (less than a minute\nvs. hours), our method can lead to significant time savings in the process of\nmodel selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kadikis_E/0/1/0/all/0/1\">Em&#x12b;ls Kadi&#x137;is</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastav_V/0/1/0/all/0/1\">Vaibhav Srivastav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Items from Psychometric Tests as Training Data for Personality Profiling Models of Twitter Users. (arXiv:2202.10415v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.10415","description":"<p>Machine-learned models for author profiling in social media often rely on\ndata acquired via self-reporting-based psychometric tests (questionnaires)\nfilled out by social media users. This is an expensive but accurate data\ncollection strategy. Another, less costly alternative, which leads to\npotentially more noisy and biased data, is to rely on labels inferred from\npublicly available information in the profiles of the users, for instance\nself-reported diagnoses or test results. In this paper, we explore a third\nstrategy, namely to directly use a corpus of items from validated psychometric\ntests as training data. Items from psychometric tests often consist of\nsentences from an I-perspective (e.g., \"I make friends easily.\"). Such corpora\nof test items constitute 'small data', but their availability for many concepts\nis a rich resource. We investigate this approach for personality profiling, and\nevaluate BERT classifiers fine-tuned on such psychometric test items for the\nbig five personality traits (openness, conscientiousness, extraversion,\nagreeableness, neuroticism) and analyze various augmentation strategies\nregarding their potential to address the challenges coming with such a small\ncorpus. Our evaluation on a publicly available Twitter corpus shows a\ncomparable performance to in-domain training for 4/5 personality traits with\nT5-based data augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kreuter_A/0/1/0/all/0/1\">Anne Kreuter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sassenberg_K/0/1/0/all/0/1\">Kai Sassenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Attention through Gradient-Based Learned Runtime Pruning. (arXiv:2204.03227v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03227","description":"<p>Self-attention is a key enabler of state-of-art accuracy for various\ntransformer-based Natural Language Processing models. This attention mechanism\ncalculates a correlation score for each word with respect to the other words in\na sentence. Commonly, only a small subset of words highly correlates with the\nword under attention, which is only determined at runtime. As such, a\nsignificant amount of computation is inconsequential due to low attention\nscores and can potentially be pruned. The main challenge is finding the\nthreshold for the scores below which subsequent computation will be\ninconsequential. Although such a threshold is discrete, this paper formulates\nits search through a soft differentiable regularizer integrated into the loss\nfunction of the training. This formulation piggy backs on the back-propagation\ntraining to analytically co-optimize the threshold and the weights\nsimultaneously, striking a formally optimal balance between accuracy and\ncomputation pruning. To best utilize this mathematical innovation, we devise a\nbit-serial architecture, dubbed LeOPArd, for transformer language models with\nbit-level early termination microarchitectural mechanism. We evaluate our\ndesign across 43 back-end tasks for MemN2N, BERT, ALBERT, GPT-2, and Vision\ntransformer models. Post-layout results show that, on average, LeOPArd yields\n1.9x and 3.9x speedup and energy reduction, respectively, while keeping the\naverage accuracy virtually intact (&lt;0.2% degradation)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodrati_S/0/1/0/all/0/1\">Soroush Ghodrati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdanbakhsh_A/0/1/0/all/0/1\">Amir Yazdanbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esmaeilzadeh_H/0/1/0/all/0/1\">Hadi Esmaeilzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Mingu Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Korean Online Hate Speech Dataset for Multilabel Classification: How Can Social Science Improve Dataset on Hate Speech?. (arXiv:2204.03262v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03262","description":"<p>We suggest a multilabel Korean online hate speech dataset that covers seven\ncategories of hate speech: (1) Race and Nationality, (2) Religion, (3)\nRegionalism, (4) Ageism, (5) Misogyny, (6) Sexual Minorities, and (7) Male. Our\n35K dataset consists of 24K online comments with Krippendorff's Alpha label\naccordance of .713, 2.2K neutral sentences from Wikipedia, 1.7K additionally\nlabeled sentences generated by the Human-in-the-Loop procedure and\nrule-generated 7.1K neutral sentences. The base model with 24K initial dataset\nachieved the accuracy of LRAP .892, but improved to .919 after being combined\nwith 11K additional data. Unlike the conventional binary hate and non-hate\ndichotomy approach, we designed a dataset considering both the cultural and\nlinguistic context to overcome the limitations of western culture-based English\ntexts. Thus, this paper is not only limited to presenting a local hate speech\ndataset but extends as a manual for building a more generalized hate speech\ndataset with diverse cultural backgrounds based on social science perspectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_T/0/1/0/all/0/1\">TaeYoung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_E/0/1/0/all/0/1\">Eunrang Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junbum Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_Y/0/1/0/all/0/1\">Youngeun Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Junmo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suh_J/0/1/0/all/0/1\">JeongKyu Suh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Review of Sign Language Recognition: Different Types, Modalities, and Datasets. (arXiv:2204.03328v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2204.03328","description":"<p>A machine can understand human activities, and the meaning of signs can help\novercome the communication barriers between the inaudible and ordinary people.\nSign Language Recognition (SLR) is a fascinating research area and a crucial\ntask concerning computer vision and pattern recognition. Recently, SLR usage\nhas increased in many applications, but the environment, background image\nresolution, modalities, and datasets affect the performance a lot. Many\nresearchers have been striving to carry out generic real-time SLR models. This\nreview paper facilitates a comprehensive overview of SLR and discusses the\nneeds, challenges, and problems associated with SLR. We study related works\nabout manual and non-manual, various modalities, and datasets. Research\nprogress and existing state-of-the-art SLR models over the past decade have\nbeen reviewed. Finally, we find the research gap and limitations in this domain\nand suggest future directions. This review paper will be helpful for readers\nand researchers to get complete guidance about SLR and the progressive design\nof the state-of-the-art SLR model\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madhiarasan_D/0/1/0/all/0/1\">Dr. M. Madhiarasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_P/0/1/0/all/0/1\">Prof. Partha Pratim Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"PlutoNet: An Efficient Polyp Segmentation Network. (arXiv:2204.03652v1 [eess.IV])","link":"http://arxiv.org/abs/2204.03652","description":"<p>Polyps in the colon can turn into cancerous cells if not removed with early\nintervention. Deep learning models are used to minimize the number of polyps\nthat goes unnoticed by the experts, and to accurately segment the detected\npolyps during these interventions. Although these models perform well on these\ntasks, they require too many parameters, which can pose a problem with\nreal-time applications. To address this problem, we propose a novel\nsegmentation model called PlutoNet which requires only 2,626,337 parameters\nwhile outperforming state-of-the-art models on multiple medical image\nsegmentation tasks. We use EfficientNetB0 architecture as a backbone and\npropose the novel modified partial decoder, which is a combination of partial\ndecoder and full scale connections, which further reduces the number of\nparameters required, as well as captures semantic details. We use asymmetric\nconvolutions to handle varying polyp sizes. Finally, we weight each feature map\nto improve segmentation by using a squeeze and excitation block. In addition to\npolyp segmentation in colonoscopy, we tested our model on segmentation of\nnuclei and surgical instruments to demonstrate its generalizability to\ndifferent medical image segmentation tasks. Our model outperformed the\nstate-of-the-art models with a Dice score of %92.3 in CVC-ClinicDB dataset and\n%89.3 in EndoScene dataset, a Dice score of %91.93 on the 2018 Data Science\nBowl Challenge dataset, and a Dice score of %94.8 on Kvasir-Instrument dataset.\nOur experiments and ablation studies show that our model is superior in terms\nof accuracy, and it is able generalize well to multiple medical segmentation\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Erol_T/0/1/0/all/0/1\">Tugberk Erol</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarikaya_D/0/1/0/all/0/1\">Duygu Sarikaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identification of Autism spectrum disorder based on a novel feature selection method and Variational Autoencoder. (arXiv:2204.03654v1 [eess.IV])","link":"http://arxiv.org/abs/2204.03654","description":"<p>The development of noninvasive brain imaging such as resting-state functional\nmagnetic resonance imaging (rs-fMRI) and its combination with AI algorithm\nprovides a promising solution for the early diagnosis of Autism spectrum\ndisorder (ASD). However, the performance of the current ASD classification\nbased on rs-fMRI still needs to be improved. This paper introduces a\nclassification framework to aid ASD diagnosis based on rs-fMRI. In the\nframework, we proposed a novel filter feature selection method based on the\ndifference between step distribution curves (DSDC) to select remarkable\nfunctional connectivities (FCs) and utilized a multilayer perceptron (MLP)\nwhich was pretrained by a simplified Variational Autoencoder (VAE) for\nclassification. We also designed a pipeline consisting of a normalization\nprocedure and a modified hyperbolic tangent (tanh) activation function to\nreplace the original tanh function, further improving the model accuracy. Our\nmodel was evaluated by 10 times 10-fold cross-validation and achieved an\naverage accuracy of 78.12%, outperforming the state-of-the-art methods reported\non the same dataset. Given the importance of sensitivity and specificity in\ndisease diagnosis, two constraints were designed in our model which can improve\nthe model's sensitivity and specificity by up to 9.32% and 10.21%,\nrespectively. The added constraints allow our model to handle different\napplication scenarios and can be used broadly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fangyu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_Y/0/1/0/all/0/1\">Yanjie Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yanlin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xi_W/0/1/0/all/0/1\">Wenhui Xi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1\">Yi Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TemporalUV: Capturing Loose Clothing with Temporally Coherent UV Coordinates. (arXiv:2204.03671v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03671","description":"<p>We propose a novel approach to generate temporally coherent UV coordinates\nfor loose clothing. Our method is not constrained by human body outlines and\ncan capture loose garments and hair. We implemented a differentiable pipeline\nto learn UV mapping between a sequence of RGB inputs and textures via UV\ncoordinates. Instead of treating the UV coordinates of each frame separately,\nour data generation approach connects all UV coordinates via feature matching\nfor temporal stability. Subsequently, a generative model is trained to balance\nthe spatial quality and temporal stability. It is driven by supervised and\nunsupervised losses in both UV and image spaces. Our experiments show that the\ntrained models output high-quality UV coordinates and generalize to new poses.\nOnce a sequence of UV coordinates has been inferred by our model, it can be\nused to flexibly synthesize new looks and modified visual styles. Compared to\nexisting methods, our approach reduces the computational workload to animate\nnew outfits by several orders of magnitude.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">You Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1\">Huiqi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thuerey_N/0/1/0/all/0/1\">Nils Thuerey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAD-3DHeads: A Large-scale Dense, Accurate and Diverse Dataset for 3D Head Alignment from a Single Image. (arXiv:2204.03688v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03688","description":"<p>We present DAD-3DHeads, a dense and diverse large-scale dataset, and a robust\nmodel for 3D Dense Head Alignment in the wild. It contains annotations of over\n3.5K landmarks that accurately represent 3D head shape compared to the\nground-truth scans. The data-driven model, DAD-3DNet, trained on our dataset,\nlearns shape, expression, and pose parameters, and performs 3D reconstruction\nof a FLAME mesh. The model also incorporates a landmark prediction branch to\ntake advantage of rich supervision and co-training of multiple related tasks.\nExperimentally, DAD-3DNet outperforms or is comparable to the state-of-the-art\nmodels in (i) 3D Head Pose Estimation on AFLW2000-3D and BIWI, (ii) 3D Face\nShape Reconstruction on NoW and Feng, and (iii) 3D Dense Head Alignment and 3D\nLandmarks Estimation on DAD-3DHeads dataset. Finally, the diversity of\nDAD-3DHeads in camera angles, facial expressions, and occlusions enables a\nbenchmark to study in-the-wild generalization and robustness to distribution\nshifts. The dataset webpage is https://p.farm/research/dad-3dheads.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martyniuk_T/0/1/0/all/0/1\">Tetiana Martyniuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kupyn_O/0/1/0/all/0/1\">Orest Kupyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurlyak_Y/0/1/0/all/0/1\">Yana Kurlyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krashenyi_I/0/1/0/all/0/1\">Igor Krashenyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji%5Cv%7Br%7Di/0/1/0/all/0/1\">Ji&#x159;i</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharmanska_V/0/1/0/all/0/1\">Viktoriia Sharmanska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive-Gravity: A Defense Against Adversarial Samples. (arXiv:2204.03694v1 [cs.LG])","link":"http://arxiv.org/abs/2204.03694","description":"<p>This paper presents a novel model training solution, denoted as\nAdaptive-Gravity, for enhancing the robustness of deep neural network\nclassifiers against adversarial examples. We conceptualize the model\nparameters/features associated with each class as a mass characterized by its\ncentroid location and the spread (standard deviation of the distance) of\nfeatures around the centroid. We use the centroid associated with each cluster\nto derive an anti-gravity force that pushes the centroids of different classes\naway from one another during network training. Then we customized an objective\nfunction that aims to concentrate each class's features toward their\ncorresponding new centroid, which has been obtained by anti-gravity force. This\nmethodology results in a larger separation between different masses and reduces\nthe spread of features around each centroid. As a result, the samples are\npushed away from the space that adversarial examples could be mapped to,\neffectively increasing the degree of perturbation needed for making an\nadversarial example. We have implemented this training solution as an iterative\nmethod consisting of four steps at each iteration: 1) centroid extraction, 2)\nanti-gravity force calculation, 3) centroid relocation, and 4) gravity\ntraining. Gravity's efficiency is evaluated by measuring the corresponding\nfooling rates against various attack models, including FGSM, MIM, BIM, and PGD\nusing LeNet and ResNet110 networks, benchmarked against MNIST and CIFAR10\nclassification problems. Test results show that Gravity not only functions as a\npowerful instrument to robustify a model against state-of-the-art adversarial\nattacks but also effectively improves the model training accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirzaeian_A/0/1/0/all/0/1\">Ali Mirzaeian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+D_S/0/1/0/all/0/1\">Sai Manoj P D</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latibari_B/0/1/0/all/0/1\">Banafsheh S. Latibari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savidis_I/0/1/0/all/0/1\">Ioannis Savidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homayoun_H/0/1/0/all/0/1\">Houman Homayoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasan_A/0/1/0/all/0/1\">Avesta Sasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Solar Flares Using CNN and LSTM on Two Solar Cycles of Active Region Data. (arXiv:2204.03710v1 [astro-ph.SR])","link":"http://arxiv.org/abs/2204.03710","description":"<p>We consider the flare prediction problem that distinguishes flare-imminent\nactive regions that produce an M- or X-class flare in the future 24 hours, from\nquiet active regions that do not produce any flare within $\\pm 24$ hours. Using\nline-of-sight magnetograms and parameters of active regions in two data\nproducts covering Solar Cycle 23 and 24, we train and evaluate two deep\nlearning algorithms -- CNN and LSTM -- and their stacking ensembles. The\ndecisions of CNN are explained using visual attribution methods. We have the\nfollowing three main findings. (1) LSTM trained on data from two solar cycles\nachieves significantly higher True Skill Scores (TSS) than that trained on data\nfrom a single solar cycle with a confidence level of at least 0.95. (2) On data\nfrom Solar Cycle 23, a stacking ensemble that combines predictions from LSTM\nand CNN using the TSS criterion achieves significantly higher TSS than the\n\"select-best\" strategy with a confidence level of at least 0.95. (3) A visual\nattribution method called Integrated Gradients is able to attribute the CNN's\npredictions of flares to the emerging magnetic flux in the active region. It\nalso reveals a limitation of CNN as a flare prediction method using\nline-of-sight magnetograms: it treats the polarity artifact of line-of-sight\nmagnetograms as positive evidence of flares.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Sun_Z/0/1/0/all/0/1\">Zeyu Sun</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Bobra_M/0/1/0/all/0/1\">Monica G. Bobra</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wang_X/0/1/0/all/0/1\">Xiantong Wang</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Sun_H/0/1/0/all/0/1\">Hu Sun</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Gombosi_T/0/1/0/all/0/1\">Tamas Gombosi</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Hero_A/0/1/0/all/0/1\">Alfred Hero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Multiple Self-Supervised Tasks Improves Model Robustness. (arXiv:2204.03714v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03714","description":"<p>Deep networks achieve state-of-the-art performance on computer vision tasks,\nyet they fail under adversarial attacks that are imperceptible to humans. In\nthis paper, we propose a novel defense that can dynamically adapt the input\nusing the intrinsic structure from multiple self-supervised tasks. By\nsimultaneously using many self-supervised tasks, our defense avoids\nover-fitting the adapted image to one specific self-supervised task and\nrestores more intrinsic structure in the image compared to a single\nself-supervised task approach. Our approach further improves robustness and\nclean accuracy significantly compared to the state-of-the-art single task\nself-supervised defense. Our work is the first to connect multiple\nself-supervised tasks to robustness, and suggests that we can achieve better\nrobustness with more intrinsic signal from visual data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lawhon_M/0/1/0/all/0/1\">Matthew Lawhon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1\">Chengzhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Junfeng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gravitationally Lensed Black Hole Emission Tomography. (arXiv:2204.03715v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03715","description":"<p>Measurements from the Event Horizon Telescope enabled the visualization of\nlight emission around a black hole for the first time. So far, these\nmeasurements have been used to recover a 2D image under the assumption that the\nemission field is static over the period of acquisition. In this work, we\npropose BH-NeRF, a novel tomography approach that leverages gravitational\nlensing to recover the continuous 3D emission field near a black hole. Compared\nto other 3D reconstruction or tomography settings, this task poses two\nsignificant challenges: first, rays near black holes follow curved paths\ndictated by general relativity, and second, we only observe measurements from a\nsingle viewpoint. Our method captures the unknown emission field using a\ncontinuous volumetric function parameterized by a coordinate-based neural\nnetwork, and uses knowledge of Keplerian orbital dynamics to establish\ncorrespondence between 3D points over time. Together, these enable BH-NeRF to\nrecover accurate 3D emission fields, even in challenging situations with sparse\nmeasurements and uncertain orbital dynamics. This work takes the first steps in\nshowing how future measurements from the Event Horizon Telescope could be used\nto recover evolving 3D emission around the supermassive black hole in our\nGalactic center.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levis_A/0/1/0/all/0/1\">Aviad Levis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1\">Pratul P. Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chael_A/0/1/0/all/0/1\">Andrew A. Chael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_R/0/1/0/all/0/1\">Ren Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouman_K/0/1/0/all/0/1\">Katherine L. Bouman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Design of Salient Object Detection Algorithms with Brain Programming. (arXiv:2204.03722v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03722","description":"<p>Despite recent improvements in computer vision, artificial visual systems'\ndesign is still daunting since an explanation of visual computing algorithms\nremains elusive. Salient object detection is one problem that is still open due\nto the difficulty of understanding the brain's inner workings. Progress on this\nresearch area follows the traditional path of hand-made designs using\nneuroscience knowledge. In recent years two different approaches based on\ngenetic programming appear to enhance their technique. One follows the idea of\ncombining previous hand-made methods through genetic programming and fuzzy\nlogic. The other approach consists of improving the inner computational\nstructures of basic hand-made models through artificial evolution. This\nresearch work proposes expanding the artificial dorsal stream using a recent\nproposal to solve salient object detection problems. This approach uses the\nbenefits of the two main aspects of this research area: fixation prediction and\ndetection of salient objects. We decided to apply the fusion of visual saliency\nand image segmentation algorithms as a template. The proposed methodology\ndiscovers several critical structures in the template through artificial\nevolution. We present results on a benchmark designed by experts with\noutstanding results in comparison with the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Olague_G/0/1/0/all/0/1\">Gustavo Olague</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menendez_Clavijo_J/0/1/0/all/0/1\">Jose Armando Menendez-Clavijo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olague_M/0/1/0/all/0/1\">Matthieu Olague</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ocampo_A/0/1/0/all/0/1\">Arturo Ocampo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibarra_Vazquez_G/0/1/0/all/0/1\">Gerardo Ibarra-Vazquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_R/0/1/0/all/0/1\">Rocio Ochoa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineda_R/0/1/0/all/0/1\">Roberto Pineda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MHMS: Multimodal Hierarchical Multimedia Summarization. (arXiv:2204.03734v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03734","description":"<p>Multimedia summarization with multimodal output can play an essential role in\nreal-world applications, i.e., automatically generating cover images and titles\nfor news articles or providing introductions to online videos. In this work, we\npropose a multimodal hierarchical multimedia summarization (MHMS) framework by\ninteracting visual and language domains to generate both video and textual\nsummaries. Our MHMS method contains video and textual segmentation and\nsummarization module, respectively. It formulates a cross-domain alignment\nobjective with optimal transport distance which leverages cross-domain\ninteraction to generate the representative keyframe and textual summary. We\nevaluated MHMS on three recent multimodal datasets and demonstrated the\neffectiveness of our method in producing high-quality multimodal summaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jielin Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiacheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengdi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaowen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BankNote-Net: Open dataset for assistive universal currency recognition. (arXiv:2204.03738v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03738","description":"<p>Millions of people around the world have low or no vision. Assistive software\napplications have been developed for a variety of day-to-day tasks, including\noptical character recognition, scene identification, person recognition, and\ncurrency recognition. This last task, the recognition of banknotes from\ndifferent denominations, has been addressed by the use of computer vision\nmodels for image recognition. However, the datasets and models available for\nthis task are limited, both in terms of dataset size and in variety of\ncurrencies covered. In this work, we collect a total of 24,826 images of\nbanknotes in variety of assistive settings, spanning 17 currencies and 112\ndenominations. Using supervised contrastive learning, we develop a machine\nlearning model for universal currency recognition. This model learns compliant\nembeddings of banknote images in a variety of contexts, which can be shared\npublicly (as a compressed vector representation), and can be used to train and\ntest specialized downstream models for any currency, including those not\ncovered by our dataset or for which only a few real images per denomination are\navailable (few-shot learning). We deploy a variation of this model for public\nuse in the last version of the Seeing AI app developed by Microsoft. We share\nour encoder model and the embeddings as an open dataset in our BankNote-Net\nrepository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oviedo_F/0/1/0/all/0/1\">Felipe Oviedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinnakota_S/0/1/0/all/0/1\">Srinivas Vinnakota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seleznev_E/0/1/0/all/0/1\">Eugene Seleznev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malhotra_H/0/1/0/all/0/1\">Hemant Malhotra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_S/0/1/0/all/0/1\">Saqib Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1\">Juan Lavista Ferres</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Drivers' attention detection: a systematic literature review. (arXiv:2204.03741v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03741","description":"<p>Countless traffic accidents often occur because of the inattention of the\ndrivers. Many factors can contribute to distractions while driving, since\nobjects or events to physiological conditions, as drowsiness and fatigue, do\nnot allow the driver to stay attentive. The technological progress allowed the\ndevelopment and application of many solutions to detect the attention in real\nsituations, promoting the interest of the scientific community in these last\nyears. Commonly, these solutions identify the lack of attention and alert the\ndriver, in order to help her/him to recover the attention, avoiding serious\naccidents and preserving lives. Our work presents a Systematic Literature\nReview (SLR) of the methods and criteria used to detect attention of drivers at\nthe wheel, focusing on those methods based on images. As results, 50 studies\nwere selected from the literature on drivers' attention detection, in which 22\ncontain solutions in the desired context. The results of SLR can be used as a\nresource in the preparation of new research projects in drivers' attention\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Veras_L/0/1/0/all/0/1\">Luiz G. V&#xe9;ras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomes_A/0/1/0/all/0/1\">Anna K. F. Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dominguez_G/0/1/0/all/0/1\">Guilherme A. R. Dominguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1\">Alexandre T. Oliveira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitosis domain generalization in histopathology images -- The MIDOG challenge. (arXiv:2204.03742v1 [eess.IV])","link":"http://arxiv.org/abs/2204.03742","description":"<p>The density of mitotic figures within tumor tissue is known to be highly\ncorrelated with tumor proliferation and thus is an important marker in tumor\ngrading. Recognition of mitotic figures by pathologists is known to be subject\nto a strong inter-rater bias, which limits the prognostic value.\nState-of-the-art deep learning methods can support the expert in this\nassessment but are known to strongly deteriorate when applied in a different\nclinical environment than was used for training. One decisive component in the\nunderlying domain shift has been identified as the variability caused by using\ndifferent whole slide scanners. The goal of the MICCAI MIDOG 2021 challenge has\nbeen to propose and evaluate methods that counter this domain shift and derive\nscanner-agnostic mitosis detection algorithms. The challenge used a training\nset of 200 cases, split across four scanning systems. As a test set, an\nadditional 100 cases split across four scanning systems, including two\npreviously unseen scanners, were given. The best approaches performed on an\nexpert level, with the winning algorithm yielding an F_1 score of 0.748 (CI95:\n0.704-0.781). In this paper, we evaluate and compare the approaches that were\nsubmitted to the challenge and identify methodological factors contributing to\nbetter performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Aubreville_M/0/1/0/all/0/1\">Marc Aubreville</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stathonikos_N/0/1/0/all/0/1\">Nikolas Stathonikos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bertram_C/0/1/0/all/0/1\">Christof A. Bertram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klopleisch_R/0/1/0/all/0/1\">Robert Klopleisch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoeve_N/0/1/0/all/0/1\">Natalie ter Hoeve</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ciompi_F/0/1/0/all/0/1\">Francesco Ciompi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wilm_F/0/1/0/all/0/1\">Frauke Wilm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marzahl_C/0/1/0/all/0/1\">Christian Marzahl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Donovan_T/0/1/0/all/0/1\">Taryn A. Donovan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breen_J/0/1/0/all/0/1\">Jack Breen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_Y/0/1/0/all/0/1\">Youjin Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_J/0/1/0/all/0/1\">Jinah Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nateghi_R/0/1/0/all/0/1\">Ramin Nateghi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pourakpour_F/0/1/0/all/0/1\">Fattaneh Pourakpour</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fick_R/0/1/0/all/0/1\">Rutger H.J. Fick</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hadj_S/0/1/0/all/0/1\">Saima Ben Hadj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jahanifar_M/0/1/0/all/0/1\">Mostafa Jahanifar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dexl_J/0/1/0/all/0/1\">Jakob Dexl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wittenberg_T/0/1/0/all/0/1\">Thomas Wittenberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kondo_S/0/1/0/all/0/1\">Satoshi Kondo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lafarge_M/0/1/0/all/0/1\">Maxime W. Lafarge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koelzer_V/0/1/0/all/0/1\">Viktor H. Koelzer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_J/0/1/0/all/0/1\">Jingtang Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yubo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Long_X/0/1/0/all/0/1\">Xi Long</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jingxin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Razavi_S/0/1/0/all/0/1\">Salar Razavi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khademi_A/0/1/0/all/0/1\">April Khademi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiyue Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veta_M/0/1/0/all/0/1\">Mitko Veta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Powering Finetuning in Few-shot Learning: Domain-Agnostic Feature Adaptation with Rectified Class Prototypes. (arXiv:2204.03749v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03749","description":"<p>In recent works, utilizing a deep network trained on meta-training set serves\nas a strong baseline in few-shot learning. In this paper, we move forward to\nrefine novel-class features by finetuning a trained deep network. Finetuning is\ndesigned to focus on reducing biases in novel-class feature distributions,\nwhich we define as two aspects: class-agnostic and class-specific biases.\nClass-agnostic bias is defined as the distribution shifting introduced by\ndomain difference, which we propose Distribution Calibration Module(DCM) to\nreduce. DCM owes good property of eliminating domain difference and fast\nfeature adaptation during optimization. Class-specific bias is defined as the\nbiased estimation using a few samples in novel classes, which we propose\nSelected Sampling(SS) to reduce. Without inferring the actual class\ndistribution, SS is designed by running sampling using proposal distributions\naround support-set samples. By powering finetuning with DCM and SS, we achieve\nstate-of-the-art results on Meta-Dataset with consistent performance boosts\nover ten datasets from different domains. We believe our simple yet effective\nmethod demonstrates its possibility to be applied on practical few-shot\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Ran Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yutong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savvides_M/0/1/0/all/0/1\">Marios Savvides</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-objective optimization determines when, which and how to fuse deep networks: an application to predict COVID-19 outcomes. (arXiv:2204.03772v1 [eess.IV])","link":"http://arxiv.org/abs/2204.03772","description":"<p>The COVID-19 pandemic has caused millions of cases and deaths and the\nAI-related scientific community, after being involved with detecting COVID-19\nsigns in medical images, has been now directing the efforts towards the\ndevelopment of methods that can predict the progression of the disease. This\ntask is multimodal by its very nature and, recently, baseline results achieved\non the publicly available AIforCOVID dataset have shown that chest X-ray scans\nand clinical information are useful to identify patients at risk of severe\noutcomes. While deep learning has shown superior performance in several medical\nfields, in most of the cases it considers unimodal data only. In this respect,\nwhen, which and how to fuse the different modalities is an open challenge in\nmultimodal deep learning. To cope with these three questions here we present a\nnovel approach optimizing the setup of a multimodal end-to-end model. It\nexploits Pareto multi-objective optimization working with a performance metric\nand the diversity score of multiple candidate unimodal neural networks to be\nfused. We test our method on the AIforCOVID dataset, attaining state-of-the-art\nresults, not only outperforming the baseline performance but also being robust\nto external validation. Moreover, exploiting XAI algorithms we figure out a\nhierarchy among the modalities and we extract the features' intra-modality\nimportance, enriching the trust on the predictions made by the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guarrasi_V/0/1/0/all/0/1\">Valerio Guarrasi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soda_P/0/1/0/all/0/1\">Paolo Soda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TorMentor: Deterministic dynamic-path, data augmentations with fractals. (arXiv:2204.03776v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03776","description":"<p>We propose the use of fractals as a means of efficient data augmentation.\nSpecifically, we employ plasma fractals for adapting global image augmentation\ntransformations into continuous local transforms. We formulate the diamond\nsquare algorithm as a cascade of simple convolution operations allowing\nefficient computation of plasma fractals on the GPU. We present the TorMentor\nimage augmentation framework that is totally modular and deterministic across\nimages and point-clouds. All image augmentation operations can be combined\nthrough pipelining and random branching to form flow networks of arbitrary\nwidth and depth. We demonstrate the efficiency of the proposed approach with\nexperiments on document image segmentation (binarization) with the DIBCO\ndatasets. The proposed approach demonstrates superior performance to\ntraditional image augmentation techniques. Finally, we use extended synthetic\nbinary text images in a self-supervision regiment and outperform the same model\nwhen trained with limited data and simple extensions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nicolaou_A/0/1/0/all/0/1\">Anguelos Nicolaou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christlein_V/0/1/0/all/0/1\">Vincent Christlein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riba_E/0/1/0/all/0/1\">Edgar Riba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogeler_G/0/1/0/all/0/1\">Georg Vogeler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seuret_M/0/1/0/all/0/1\">Mathias Seuret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Representation and Dependency Learning for Multi-Label Image Recognition. (arXiv:2204.03795v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03795","description":"<p>Recently many multi-label image recognition (MLR) works have made significant\nprogress by introducing pre-trained object detection models to generate lots of\nproposals or utilizing statistical label co-occurrence enhance the correlation\namong different categories. However, these works have some limitations: (1) the\neffectiveness of the network significantly depends on pre-trained object\ndetection models that bring expensive and unaffordable computation; (2) the\nnetwork performance degrades when there exist occasional co-occurrence objects\nin images, especially for the rare categories. To address these problems, we\npropose a novel and effective semantic representation and dependency learning\n(SRDL) framework to learn category-specific semantic representation for each\ncategory and capture semantic dependency among all categories. Specifically, we\ndesign a category-specific attentional regions (CAR) module to generate\nchannel/spatial-wise attention matrices to guide model to focus on\nsemantic-aware regions. We also design an object erasing (OE) module to\nimplicitly learn semantic dependency among categories by erasing semantic-aware\nregions to regularize the network training. Extensive experiments and\ncomparisons on two popular MLR benchmark datasets (i.e., MS-COCO and Pascal VOC\n2007) demonstrate the effectiveness of the proposed framework over current\nstate-of-the-art algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pu_T/0/1/0/all/0/1\">Tao Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lixian Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hefeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianshui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Ling Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Learnable Variational Model for Joint Multimodal MRI Reconstruction and Synthesis. (arXiv:2204.03804v1 [eess.IV])","link":"http://arxiv.org/abs/2204.03804","description":"<p>Generating multi-contrasts/modal MRI of the same anatomy enriches diagnostic\ninformation but is limited in practice due to excessive data acquisition time.\nIn this paper, we propose a novel deep-learning model for joint reconstruction\nand synthesis of multi-modal MRI using incomplete k-space data of several\nsource modalities as inputs. The output of our model includes reconstructed\nimages of the source modalities and high-quality image synthesized in the\ntarget modality. Our proposed model is formulated as a variational problem that\nleverages several learnable modality-specific feature extractors and a\nmultimodal synthesis module. We propose a learnable optimization algorithm to\nsolve this model, which induces a multi-phase network whose parameters can be\ntrained using multi-modal MRI data. Moreover, a bilevel-optimization framework\nis employed for robust parameter training. We demonstrate the effectiveness of\nour approach using extensive numerical experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bian_W/0/1/0/all/0/1\">Wanyu Bian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingchao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_X/0/1/0/all/0/1\">Xiaojing Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yunmei Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Canonical Mean Filter for Almost Zero-Shot Multi-Task classification. (arXiv:2204.03815v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03815","description":"<p>The support set is a key to providing conditional prior for fast adaption of\nthe model in few-shot tasks. But the strict form of support set makes its\nconstruction actually difficult in practical application. Motivated by ANIL, we\nrethink the role of adaption in the feature extractor of CNAPs, which is a\nstate-of-the-art representative few-shot method. To investigate the role,\nAlmost Zero-Shot (AZS) task is designed by fixing the support set to replace\nthe common scheme, which provides corresponding support sets for the different\nconditional prior of different tasks. The AZS experiment results infer that the\nadaptation works little in the feature extractor. However, CNAPs cannot be\nrobust to randomly selected support sets and perform poorly on some datasets of\nMeta-Dataset because of its scattered mean embeddings responded by the simple\nmean operator. To enhance the robustness of CNAPs, Canonical Mean Filter (CMF)\nmodule is proposed to make the mean embeddings intensive and stable in feature\nspace by mapping the support sets into a canonical form. CMFs make CNAPs robust\nto any fixed support sets even if they are random matrices. This attribution\nmakes CNAPs be able to remove the mean encoder and the parameter adaptation\nnetwork at the test stage, while CNAP-CMF on AZS tasks keeps the performance\nwith one-shot tasks. It leads to a big parameter reduction. Precisely, 40.48\\%\nparameters are dropped at the test stage. Also, CNAP-CMF outperforms CNAPs in\none-shot tasks because it addresses inner-task unstable performance problems.\nClassification performance, visualized and clustering results verify that CMFs\nmake CNAPs better and simpler.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Heng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiang Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reusing the Task-specific Classifier as a Discriminator: Discriminator-free Adversarial Domain Adaptation. (arXiv:2204.03838v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03838","description":"<p>Adversarial learning has achieved remarkable performances for unsupervised\ndomain adaptation (UDA). Existing adversarial UDA methods typically adopt an\nadditional discriminator to play the min-max game with a feature extractor.\nHowever, most of these methods failed to effectively leverage the predicted\ndiscriminative information, and thus cause mode collapse for generator. In this\nwork, we address this problem from a different perspective and design a simple\nyet effective adversarial paradigm in the form of a discriminator-free\nadversarial learning network (DALN), wherein the category classifier is reused\nas a discriminator, which achieves explicit domain alignment and category\ndistinguishment through a unified objective, enabling the DALN to leverage the\npredicted discriminative information for sufficient feature alignment.\nBasically, we introduce a Nuclear-norm Wasserstein discrepancy (NWD) that has\ndefinite guidance meaning for performing discrimination. Such NWD can be\ncoupled with the classifier to serve as a discriminator satisfying the\nK-Lipschitz constraint without the requirements of additional weight clipping\nor gradient penalty strategy. Without bells and whistles, DALN compares\nfavorably against the existing state-of-the-art (SOTA) methods on a variety of\npublic datasets. Moreover, as a plug-and-play technique, NWD can be directly\nused as a generic regularizer to benefit existing UDA algorithms. Code is\navailable at https://github.com/xiaoachen98/DALN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huaian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhixiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From 2D Images to 3D Model:Weakly Supervised Multi-View Face Reconstruction with Deep Fusion. (arXiv:2204.03842v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03842","description":"<p>We consider the problem of Multi-view 3D Face Reconstruction (MVR) with\nweakly supervised learning that leverages a limited number of 2D face images\n(e.g. 3) to generate a high-quality 3D face model with very light annotation.\nDespite their encouraging performance, present MVR methods simply concatenate\nmulti-view image features and pay less attention to critical areas (e.g. eye,\nbrow, nose and mouth). To this end, we propose a novel model called Deep Fusion\nMVR (DF-MVR) and design a multi-view encoding to a single decoding framework\nwith skip connections, able to extract, integrate, and compensate deep features\nwith attention from multi-view images. In addition, we develop a multi-view\nface parse network to learn, identify, and emphasize the critical common face\narea. Finally, though our model is trained with a few 2D images, it can\nreconstruct an accurate 3D model even if one single 2D image is input. We\nconduct extensive experiments to evaluate various multi-view 3D face\nreconstruction methods. Our proposed model attains superior performance,\nleading to 11.4% RMSE improvement over the existing best weakly supervised\nMVRs. Source codes are available in the supplementary materials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weiguang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chaolong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jianan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yuyao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prediction of COVID-19 using chest X-ray images. (arXiv:2204.03849v1 [eess.IV])","link":"http://arxiv.org/abs/2204.03849","description":"<p>COVID-19, also known as Novel Coronavirus Disease, is a highly contagious\ndisease that first surfaced in China in late 2019. SARS-CoV-2 is a coronavirus\nthat belongs to the vast family of coronaviruses that causes this disease. The\nsickness originally appeared in Wuhan, China in December 2019 and quickly\nspread to over 213 nations, becoming a global pandemic. Fever, dry cough, and\ntiredness are the most typical COVID-19 symptoms. Aches, pains, and difficulty\nbreathing are some of the other symptoms that patients may face. The majority\nof these symptoms are indicators of respiratory infections and lung\nabnormalities, which radiologists can identify. Chest x-rays of COVID-19\npatients seem similar, with patchy and hazy lungs rather than clear and healthy\nlungs. On x-rays, however, pneumonia and other chronic lung disorders can\nresemble COVID-19. Trained radiologists must be able to distinguish between\nCOVID-19 and an illness that is less contagious. Our AI algorithm seeks to give\ndoctors a quantitative estimate of the risk of deterioration. So that patients\nat high risk of deterioration can be triaged and treated efficiently. The\nmethod could be particularly useful in pandemic hotspots when screening upon\nadmission is important for allocating limited resources like hospital beds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Darapaneni_N/0/1/0/all/0/1\">Narayana Darapaneni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maram_S/0/1/0/all/0/1\">Suma Maram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_H/0/1/0/all/0/1\">Harpreet Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Subhani_S/0/1/0/all/0/1\">Syed Subhani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kour_M/0/1/0/all/0/1\">Mandeep Kour</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nagam_S/0/1/0/all/0/1\">Sathish Nagam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paduri_A/0/1/0/all/0/1\">Anwesh Reddy Paduri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-scale temporal network for continuous sign language recognition. (arXiv:2204.03864v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03864","description":"<p>Continuous Sign Language Recognition (CSLR) is a challenging research task\ndue to the lack of accurate annotation on the temporal sequence of sign\nlanguage data. The recent popular usage is a hybrid model based on \"CNN + RNN\"\nfor CSLR. However, when extracting temporal features in these works, most of\nthe methods using a fixed temporal receptive field and cannot extract the\ntemporal features well for each sign language word. In order to obtain more\naccurate temporal features, this paper proposes a multi-scale temporal network\n(MSTNet). The network mainly consists of three parts. The Resnet and two fully\nconnected (FC) layers constitute the frame-wise feature extraction part. The\ntime-wise feature extraction part performs temporal feature learning by first\nextracting temporal receptive field features of different scales using the\nproposed multi-scale temporal block (MST-block) to improve the temporal\nmodeling capability, and then further encoding the temporal features of\ndifferent scales by the transformers module to obtain more accurate temporal\nfeatures. Finally, the proposed multi-level Connectionist Temporal\nClassification (CTC) loss part is used for training to obtain recognition\nresults. The multi-level CTC loss enables better learning and updating of the\nshallow network parameters in CNN, and the method has no parameter increase and\ncan be flexibly embedded in other models. Experimental results on two publicly\navailable datasets demonstrate that our method can effectively extract sign\nlanguage features in an end-to-end manner without any prior knowledge,\nimproving the accuracy of CSLR and reaching the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qidan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1\">Fei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Q/0/1/0/all/0/1\">Quan Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatiotemporal Augmentation on Selective Frequencies for Video Representation Learning. (arXiv:2204.03865v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03865","description":"<p>Recent self-supervised video representation learning methods focus on\nmaximizing the similarity between multiple augmented views from the same video\nand largely rely on the quality of generated views. In this paper, we propose\nfrequency augmentation (FreqAug), a spatio-temporal data augmentation method in\nthe frequency domain for video representation learning. FreqAug stochastically\nremoves undesirable information from the video by filtering out specific\nfrequency components so that learned representation captures essential features\nof the video for various downstream tasks. Specifically, FreqAug pushes the\nmodel to focus more on dynamic features rather than static features in the\nvideo via dropping spatial or temporal low-frequency components. In other\nwords, learning invariance between remaining frequency components results in\nhigh-frequency enhanced representation with less static bias. To verify the\ngenerality of the proposed method, we experiment with FreqAug on multiple\nself-supervised learning frameworks along with standard augmentations.\nTransferring the improved representation to five video action recognition and\ntwo temporal action localization downstream tasks shows consistent improvements\nover baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinhyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taeoh Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_M/0/1/0/all/0/1\">Minho Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dongyoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wee_D/0/1/0/all/0/1\">Dongyoon Wee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junmo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Missingness from Uncontrollable Missingness: Joint Learning Measurement Policy and Imputation. (arXiv:2204.03872v1 [cs.LG])","link":"http://arxiv.org/abs/2204.03872","description":"<p>Due to the cost or interference of measurement, we need to control\nmeasurement system. Assuming that each variable can be measured sequentially,\nthere exists optimal policy choosing next measurement for the former\nobservations. Though optimal measurement policy is actually dependent on the\ngoal of measurement, we mainly focus on retrieving complete data, so called as\nimputation. Also, we adapt the imputation method to missingness varying with\nmeasurement policy. However, learning measurement policy and imputation\nrequires complete data which is impossible to be observed, unfortunately. To\ntackle this problem, we propose a data generation method and joint learning\nalgorithm. The main idea is that 1) the data generation method is inherited by\nimputation method, and 2) the adaptation of imputation encourages measurement\npolicy to learn more than individual learning. We implemented some variations\nof proposed algorithm for two different datasets and various missing rates.\nFrom the experimental results, we demonstrate that our algorithm is generally\napplicable and outperforms baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seongwook Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaehyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heejeong Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sull_S/0/1/0/all/0/1\">Sanghoon Sull</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Transformer Network on Skeleton-based Gait Recognition. (arXiv:2204.03873v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03873","description":"<p>Skeleton-based gait recognition models usually suffer from the robustness\nproblem, as the Rank-1 accuracy varies from 90\\% in normal walking cases to\n70\\% in walking with coats cases. In this work, we propose a state-of-the-art\nrobust skeleton-based gait recognition model called Gait-TR, which is based on\nthe combination of spatial transformer frameworks and temporal convolutional\nnetworks. Gait-TR achieves substantial improvements over other skeleton-based\ngait models with higher accuracy and better robustness on the well-known gait\ndataset CASIA-B. Particularly in walking with coats cases, Gait-TR get a 90\\%\nRank-1 gait recognition accuracy rate, which is higher than the best result of\nsilhouette-based models, which usually have higher accuracy than the\nsilhouette-based gait recognition models. Moreover, our experiment on CASIA-B\nshows that the spatial transformer can extract gait features from the human\nskeleton better than the widely used graph convolutional network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xing-Peng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Guo-Qiang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiang-Jie Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CD$^2$-pFed: Cyclic Distillation-guided Channel Decoupling for Model Personalization in Federated Learning. (arXiv:2204.03880v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03880","description":"<p>Federated learning (FL) is a distributed learning paradigm that enables\nmultiple clients to collaboratively learn a shared global model. Despite the\nrecent progress, it remains challenging to deal with heterogeneous data\nclients, as the discrepant data distributions usually prevent the global model\nfrom delivering good generalization ability on each participating client. In\nthis paper, we propose CD^2-pFed, a novel Cyclic Distillation-guided Channel\nDecoupling framework, to personalize the global model in FL, under various\nsettings of data heterogeneity. Different from previous works which establish\nlayer-wise personalization to overcome the non-IID data across different\nclients, we make the first attempt at channel-wise assignment for model\npersonalization, referred to as channel decoupling. To further facilitate the\ncollaboration between private and shared weights, we propose a novel cyclic\ndistillation scheme to impose a consistent regularization between the local and\nglobal model representations during the federation. Guided by the cyclical\ndistillation, our channel decoupling framework can deliver more accurate and\ngeneralized results for different kinds of heterogeneity, such as feature skew,\nlabel distribution skew, and concept shift. Comprehensive experiments on four\nbenchmarks, including natural image and medical image analysis tasks,\ndemonstrate the consistent effectiveness of our method on both local and\nexternal validations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yiqing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuyin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lequan Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformers for Single Image Dehazing. (arXiv:2204.03883v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03883","description":"<p>Image dehazing is a representative low-level vision task that estimates\nlatent haze-free images from hazy images. In recent years, convolutional neural\nnetwork-based methods have dominated image dehazing. However, vision\nTransformers, which has recently made a breakthrough in high-level vision\ntasks, has not brought new dimensions to image dehazing. We start with the\npopular Swin Transformer and find that several of its key designs are\nunsuitable for image dehazing. To this end, we propose DehazeFormer, which\nconsists of various improvements, such as the modified normalization layer,\nactivation function, and spatial information aggregation scheme. We train\nmultiple variants of DehazeFormer on various datasets to demonstrate its\neffectiveness. Specifically, on the most frequently used SOTS indoor set, our\nsmall model outperforms FFA-Net with only 25% #Param and 5% computational cost.\nTo the best of our knowledge, our large model is the first method with the PSNR\nover 40 dB on the SOTS indoor set, dramatically outperforming the previous\nstate-of-the-art methods. We also collect a large-scale realistic remote\nsensing dehazing dataset for evaluating the method's capability to remove\nhighly non-homogeneous haze.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuda Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhuqing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xin Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SuperNet in Neural Architecture Search: A Taxonomic Survey. (arXiv:2204.03916v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03916","description":"<p>Deep Neural Networks (DNN) have made significant progress in a wide range of\nvisual recognition tasks such as image classification, object detection, and\nsemantic segmentation. The evolution of convolutional architectures has led to\nbetter performance by incurring expensive computational costs. In addition,\nnetwork design has become a difficult task, which is labor-intensive and\nrequires a high level of domain knowledge. To mitigate such issues, there have\nbeen studies for a variety of neural architecture search methods that\nautomatically search for optimal architectures, achieving models with\nimpressive performance that outperform human-designed counterparts. This survey\naims to provide an overview of existing works in this field of research and\nspecifically focus on the supernet optimization that builds a neural network\nthat assembles all the architectures as its sub models by using weight sharing.\nWe aim to accomplish that by categorizing supernet optimization by proposing\nthem as solutions to the common challenges found in the literature: data-side\noptimization, poor rank correlation alleviation, and transferable NAS for a\nnumber of deployment scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Stephen Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biometric identification by means of hand geometry and a neural net classifier. (arXiv:2204.03925v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03925","description":"<p>This Paper describes a hand geometry biometric identification system. We have\nacquired a database of 22 people using a conventional document scanner. The\nexperimental section consists of a study about the discrimination capability of\ndifferent extracted features, and the identification rate using different\nclassifiers based on neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merida_G/0/1/0/all/0/1\">Guillermo Mar Navarro M&#xe9;rida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Hyperspectral-Depth Reconstruction Using Single Color-Dot Projection. (arXiv:2204.03929v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03929","description":"<p>Depth reconstruction and hyperspectral reflectance reconstruction are two\nactive research topics in computer vision and image processing. Conventionally,\nthese two topics have been studied separately using independent imaging setups\nand there is no existing method which can acquire depth and spectral\nreflectance simultaneously in one shot without using special hardware. In this\npaper, we propose a novel single-shot hyperspectral-depth reconstruction method\nusing an off-the-shelf RGB camera and projector. Our method is based on a\nsingle color-dot projection, which simultaneously acts as structured light for\ndepth reconstruction and spatially-varying color illuminations for\nhyperspectral reflectance reconstruction. To jointly reconstruct the depth and\nthe hyperspectral reflectance from a single color-dot image, we propose a novel\nend-to-end network architecture that effectively incorporates a geometric\ncolor-dot pattern loss and a photometric hyperspectral reflectance loss.\nThrough the experiments, we demonstrate that our hyperspectral-depth\nreconstruction method outperforms the combination of an existing\nstate-of-the-art single-shot hyperspectral reflectance reconstruction method\nand depth reconstruction method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monno_Y/0/1/0/all/0/1\">Yusuke Monno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okutomi_M/0/1/0/all/0/1\">Masatoshi Okutomi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Robustness on ImageNet Transfer to Downstream Tasks?. (arXiv:2204.03934v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03934","description":"<p>As clean ImageNet accuracy nears its ceiling, the research community is\nincreasingly more concerned about robust accuracy under distributional shifts.\nWhile a variety of methods have been proposed to robustify neural networks,\nthese techniques often target models trained on ImageNet classification. At the\nsame time, it is a common practice to use ImageNet pretrained backbones for\ndownstream tasks such as object detection, semantic segmentation, and image\nclassification from different domains. This raises a question: Can these robust\nimage classifiers transfer robustness to downstream tasks? For object detection\nand semantic segmentation, we find that a vanilla Swin Transformer, a variant\nof Vision Transformer tailored for dense prediction tasks, transfers robustness\nbetter than Convolutional Neural Networks that are trained to be robust to the\ncorrupted version of ImageNet. For CIFAR10 classification, we find that models\nthat are robustified for ImageNet do not retain robustness when fully\nfine-tuned. These findings suggest that current robustification techniques tend\nto emphasize ImageNet evaluations. Moreover, network architecture is a strong\nsource of robustness when we consider transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamada_Y/0/1/0/all/0/1\">Yutaro Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otani_M/0/1/0/all/0/1\">Mayu Otani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Study of a committee of neural networks for biometric hand-geometry recognition. (arXiv:2204.03935v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03935","description":"<p>This Paper studies different committees of neural networks for biometric\npattern recognition. We use the neural nets as classifiers for identification\nand verification purposes. We show that a committee of nets can improve the\nrecognition rates when compared with a multi-start initialization algo-rithm\nthat just picks up the neural net which offers the best performance. On the\nother hand, we found that there is no strong correlation between\nidentifi-cation and verification applications using the same classifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Distinctive Image Captioning via Comparing and Reweighting. (arXiv:2204.03938v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03938","description":"<p>Recent image captioning models are achieving impressive results based on\npopular metrics, i.e., BLEU, CIDEr, and SPICE. However, focusing on the most\npopular metrics that only consider the overlap between the generated captions\nand human annotation could result in using common words and phrases, which\nlacks distinctiveness, i.e., many similar images have the same caption. In this\npaper, we aim to improve the distinctiveness of image captions via comparing\nand reweighting with a set of similar images. First, we propose a\ndistinctiveness metric -- between-set CIDEr (CIDErBtw) to evaluate the\ndistinctiveness of a caption with respect to those of similar images. Our\nmetric reveals that the human annotations of each image in the MSCOCO dataset\nare not equivalent based on distinctiveness; however, previous works normally\ntreat the human annotations equally during training, which could be a reason\nfor generating less distinctive captions. In contrast, we reweight each\nground-truth caption according to its distinctiveness during training. We\nfurther integrate a long-tailed weight strategy to highlight the rare words\nthat contain more information, and captions from the similar image set are\nsampled as negative examples to encourage the generated sentence to be unique.\nFinally, extensive experiments are conducted, showing that our proposed\napproach significantly improves both distinctiveness (as measured by CIDErBtw\nand retrieval metrics) and accuracy (e.g., as measured by CIDEr) for a wide\nvariety of image captioning baselines. These results are further confirmed\nthrough a user study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiuniu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenjia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Antoni B. Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic Representations for Video Contrastive Learning. (arXiv:2204.03946v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03946","description":"<p>This paper presents Probabilistic Video Contrastive Learning, a\nself-supervised representation learning method that bridges contrastive\nlearning with probabilistic representation. We hypothesize that the clips\ncomposing the video have different distributions in short-term duration, but\ncan represent the complicated and sophisticated video distribution through\ncombination in a common embedding space. Thus, the proposed method represents\nvideo clips as normal distributions and combines them into a Mixture of\nGaussians to model the whole video distribution. By sampling embeddings from\nthe whole video distribution, we can circumvent the careful sampling strategy\nor transformations to generate augmented views of the clips, unlike previous\ndeterministic methods that have mainly focused on such sample generation\nstrategies for contrastive learning. We further propose a stochastic\ncontrastive loss to learn proper video distributions and handle the inherent\nuncertainty from the nature of the raw video. Experimental results verify that\nour probabilistic embedding stands as a state-of-the-art video representation\nlearning for action recognition and video retrieval on the most popular\nbenchmarks, including UCF101 and HMDB51.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jungin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1\">Ig-Jae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kwanghoon Sohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Points to Patches: Enabling the Use of Self-Attention for 3D Shape Recognition. (arXiv:2204.03957v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03957","description":"<p>While the Transformer architecture has become ubiquitous in the machine\nlearning field, its adaptation to 3D shape recognition is non-trivial. Due to\nits quadratic computational complexity, the self-attention operator quickly\nbecomes inefficient as the set of input points grows larger. Furthermore, we\nfind that the attention mechanism struggles to find useful connections between\nindividual points on a global scale. In order to alleviate these problems, we\npropose a two-stage Point Transformer-in-Transformer (Point-TnT) approach which\ncombines local and global attention mechanisms, enabling both individual points\nand patches of points to attend to each other effectively. Experiments on shape\nclassification show that such an approach provides more useful features for\ndownstream tasks than the baseline Transformer, while also being more\ncomputationally efficient. In addition, we also extend our method to feature\nmatching for scene reconstruction, showing that it can be used in conjunction\nwith existing scene reconstruction pipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berg_A/0/1/0/all/0/1\">Axel Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oskarsson_M/0/1/0/all/0/1\">Magnus Oskarsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_M/0/1/0/all/0/1\">Mark O&#x27;Connor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SnapMode: An Intelligent and Distributed Large-Scale Fashion Image Retrieval Platform Based On Big Data and Deep Generative Adversarial Network Technologies. (arXiv:2204.03998v1 [cs.IR])","link":"http://arxiv.org/abs/2204.03998","description":"<p>Fashion is now among the largest industries worldwide, for it represents\nhuman history and helps tell the worlds story. As a result of the Fourth\nIndustrial Revolution, the Internet has become an increasingly important source\nof fashion information. However, with a growing number of web pages and social\ndata, it is nearly impossible for humans to manually catch up with the ongoing\nevolution and the continuously variable content in this domain. The proper\nmanagement and exploitation of big data can pave the way for the substantial\ngrowth of the global economy as well as citizen satisfaction. Therefore,\ncomputer scientists have found it challenging to handle e-commerce fashion\nwebsites by using big data and machine learning technologies. This paper first\nproposes a scalable focused Web Crawler engine based on the distributed\ncomputing platforms to extract and process fashion data on e-commerce websites.\nThe role of the proposed platform is then described in developing a\ndisentangled feature extraction method by employing deep convolutional\ngenerative adversarial networks (DCGANs) for content-based image indexing and\nretrieval. Finally, the state-of-the-art solutions are compared, and the\nresults of the proposed approach are analyzed on a standard dataset. For the\nreal-life implementation of the proposed solution, a Web-based application is\ndeveloped on Apache Storm, Kafka, Solr, and Milvus platforms to create a\nfashion search engine called SnapMode.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_N/0/1/0/all/0/1\">Narges Norouzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azmi_R/0/1/0/all/0/1\">Reza Azmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moghadam_S/0/1/0/all/0/1\">Sara Saberi Tehrani Moghadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarvani_M/0/1/0/all/0/1\">Maral Zarvani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Quasi-AutoRegression: Forecasting the visual popularity of new fashion products. (arXiv:2204.04014v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04014","description":"<p>Estimating the preferences of consumers is of utmost importance for the\nfashion industry as appropriately leveraging this information can be beneficial\nin terms of profit. Trend detection in fashion is a challenging task due to the\nfast pace of change in the fashion industry. Moreover, forecasting the visual\npopularity of new garment designs is even more demanding due to lack of\nhistorical data. To this end, we propose MuQAR, a Multimodal\nQuasi-AutoRegressive deep learning architecture that combines two modules: (1)\na multi-modal multi-layer perceptron processing categorical and visual features\nextracted by computer vision networks and (2) a quasi-autoregressive neural\nnetwork modelling the time series of the product's attributes, which are used\nas a proxy of temporal popularity patterns mitigating the lack of historical\ndata. We perform an extensive ablation analysis on two large scale image\nfashion datasets, Mallzee-popularity and SHIFT15m to assess the adequacy of\nMuQAR and also use the Amazon Reviews: Home and Kitchen dataset to assess\ngeneralisability to other domains. A comparative study on the VISUELLE dataset,\nshows that MuQAR is capable of competing and surpassing the domain's current\nstate of the art by 2.88% in terms of WAPE and 3.04% in terms of MAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Stefanos I. Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutlis_C/0/1/0/all/0/1\">Christos Koutlis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Symeon Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1\">Ioannis Kompatsiaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Engagement Detection with Multi-Task Training in E-Learning Environments. (arXiv:2204.04020v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04020","description":"<p>Recognition of user interaction, in particular engagement detection, became\nhighly crucial for online working and learning environments, especially during\nthe COVID-19 outbreak. Such recognition and detection systems significantly\nimprove the user experience and efficiency by providing valuable feedback. In\nthis paper, we propose a novel Engagement Detection with Multi-Task Training\n(ED-MTT) system which minimizes mean squared error and triplet loss together to\ndetermine the engagement level of students in an e-learning environment. The\nperformance of this system is evaluated and compared against the\nstate-of-the-art on a publicly available dataset as well as videos collected\nfrom real-life scenarios. The results show that ED-MTT achieves 6% lower MSE\nthan the best state-of-the-art performance with highly acceptable training time\nand lightweight feature extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Copur_O/0/1/0/all/0/1\">Onur Copur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakip_M/0/1/0/all/0/1\">Mert Nak&#x131;p</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scardapane_S/0/1/0/all/0/1\">Simone Scardapane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slowack_J/0/1/0/all/0/1\">J&#xfc;rgen Slowack</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Generic Image Retrieval Method for Date Estimation of Historical Document Collections. (arXiv:2204.04028v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04028","description":"<p>Date estimation of historical document images is a challenging problem, with\nseveral contributions in the literature that lack of the ability to generalize\nfrom one dataset to others. This paper presents a robust date estimation system\nbased in a retrieval approach that generalizes well in front of heterogeneous\ncollections. we use a ranking loss function named smooth-nDCG to train a\nConvolutional Neural Network that learns an ordination of documents for each\nproblem. One of the main usages of the presented approach is as a tool for\nhistorical contextual retrieval. It means that scholars could perform\ncomparative analysis of historical images from big datasets in terms of the\nperiod where they were produced. We provide experimental evaluation on\ndifferent types of documents from real datasets of manuscript and newspaper\nimages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Molina_A/0/1/0/all/0/1\">Adri&#xe0; Molina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_L/0/1/0/all/0/1\">Lluis Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terrades_O/0/1/0/all/0/1\">Oriol Ramos Terrades</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1\">Josep Llad&#xf3;s</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confidence Score for Unsupervised Foreground Background Separation of Document Images. (arXiv:2204.04044v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04044","description":"<p>Foreground-background separation is an important problem in document image\nanalysis. Popular unsupervised binarization methods (such as the Sauvola's\nalgorithm) employ adaptive thresholding to classify pixels as foreground or\nbackground. In this work, we propose a novel approach for computing confidence\nscores of the classification in such algorithms. This score provides an insight\nof the confidence level of the prediction. The computational complexity of the\nproposed approach is the same as the underlying binarization algorithm. Our\nexperiments illustrate the utility of the proposed scores in various\napplications like document binarization, document image cleanup, and texture\naddition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Soumyadeep Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawanpuria_P/0/1/0/all/0/1\">Pratik Jawanpuria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient tracking of team sport players with few game-specific annotations. (arXiv:2204.04049v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04049","description":"<p>One of the requirements for team sports analysis is to track and recognize\nplayers. Many tracking and reidentification methods have been proposed in the\ncontext of video surveillance. They show very convincing results when tested on\npublic datasets such as the MOT challenge. However, the performance of these\nmethods are not as satisfactory when applied to player tracking. Indeed, in\naddition to moving very quickly and often being occluded, the players wear the\nsame jersey, which makes the task of reidentification very complex. Some recent\ntracking methods have been developed more specifically for the team sport\ncontext. Due to the lack of public data, these methods use private datasets\nthat make impossible a comparison with them. In this paper, we propose a new\ngeneric method to track team sport players during a full game thanks to few\nhuman annotations collected via a semi-interactive system. Non-ambiguous\ntracklets and their appearance features are automatically generated with a\ndetection and a reidentification network both pre-trained on public datasets.\nThen an incremental learning mechanism trains a Transformer to classify\nidentities using few game-specific human annotations. Finally, tracklets are\nlinked by an association algorithm. We demonstrate the efficiency of our\napproach on a challenging rugby sevens dataset. To overcome the lack of public\nsports tracking dataset, we publicly release this dataset at\nhttps://kalisteo.cea.fr/index.php/free-resources/. We also show that our method\nis able to track rugby sevens players during a full match, if they are\nobservable at a minimal resolution, with the annotation of only 6 few seconds\nlength tracklets per player.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maglo_A/0/1/0/all/0/1\">Adrien Maglo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orcesi_A/0/1/0/all/0/1\">Astrid Orcesi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1\">Quoc-Cuong Pham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Ambiguous Similarity Conditions via Semantic Matching. (arXiv:2204.04053v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04053","description":"<p>Rich semantics inside an image result in its ambiguous relationship with\nothers, i.e., two images could be similar in one condition but dissimilar in\nanother. Given triplets like \"aircraft\" is similar to \"bird\" than \"train\",\nWeakly Supervised Conditional Similarity Learning (WS-CSL) learns multiple\nembeddings to match semantic conditions without explicit condition labels such\nas \"can fly\". However, similarity relationships in a triplet are uncertain\nexcept providing a condition. For example, the previous comparison becomes\ninvalid once the conditional label changes to \"is vehicle\". To this end, we\nintroduce a novel evaluation criterion by predicting the comparison's\ncorrectness after assigning the learned embeddings to their optimal conditions,\nwhich measures how much WS-CSL could cover latent semantics as the supervised\nmodel. Furthermore, we propose the Distance Induced Semantic COndition\nVERification Network (DiscoverNet), which characterizes the instance-instance\nand triplets-condition relations in a \"decompose-and-fuse\" manner. To make the\nlearned embeddings cover all semantics, DiscoverNet utilizes a set module or an\nadditional regularizer over the correspondence between a triplet and a\ncondition. DiscoverNet achieves state-of-the-art performance on benchmarks like\nUT-Zappos-50k and Celeb-A w.r.t. different criteria.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning-Based Intra Mode Derivation for Versatile Video Coding. (arXiv:2204.04059v1 [eess.IV])","link":"http://arxiv.org/abs/2204.04059","description":"<p>In intra coding, Rate Distortion Optimization (RDO) is performed to achieve\nthe optimal intra mode from a pre-defined candidate list. The optimal intra\nmode is also required to be encoded and transmitted to the decoder side besides\nthe residual signal, where lots of coding bits are consumed. To further improve\nthe performance of intra coding in Versatile Video Coding (VVC), an intelligent\nintra mode derivation method is proposed in this paper, termed as Deep Learning\nbased Intra Mode Derivation (DLIMD). In specific, the process of intra mode\nderivation is formulated as a multi-class classification task, which aims to\nskip the module of intra mode signaling for coding bits reduction. The\narchitecture of DLIMD is developed to adapt to different quantization parameter\nsettings and variable coding blocks including non-square ones, which are\nhandled by one single trained model. Different from the existing deep learning\nbased classification problems, the hand-crafted features are also fed into the\nintra mode derivation network besides the learned features from feature\nlearning network. To compete with traditional method, one additional binary\nflag is utilized in the video codec to indicate the selected scheme with RDO.\nExtensive experimental results reveal that the proposed method can achieve\n2.28%, 1.74%, and 2.18% bit rate reduction on average for Y, U, and V\ncomponents on the platform of VVC test model, which outperforms the\nstate-of-the-art works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1\">Linwei Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_N/0/1/0/all/0/1\">Na Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_G/0/1/0/all/0/1\">Gangyi Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Attacks Revisited: A Large-Scale Empirical Study in Real Computer Vision Settings. (arXiv:2204.04063v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04063","description":"<p>One intriguing property of adversarial attacks is their \"transferability\" --\nan adversarial example crafted with respect to one deep neural network (DNN)\nmodel is often found effective against other DNNs as well. Intensive research\nhas been conducted on this phenomenon under simplistic controlled conditions.\nYet, thus far, there is still a lack of comprehensive understanding about\ntransferability-based attacks (\"transfer attacks\") in real-world environments.\n</p>\n<p>To bridge this critical gap, we conduct the first large-scale systematic\nempirical study of transfer attacks against major cloud-based MLaaS platforms,\ntaking the components of a real transfer attack into account. The study leads\nto a number of interesting findings which are inconsistent to the existing\nones, including: (1) Simple surrogates do not necessarily improve real transfer\nattacks. (2) No dominant surrogate architecture is found in real transfer\nattacks. (3) It is the gap between posterior (output of the softmax layer)\nrather than the gap between logit (so-called $\\kappa$ value) that increases\ntransferability. Moreover, by comparing with prior works, we demonstrate that\ntransfer attacks possess many previously unknown properties in real-world\nenvironments, such as (1) Model similarity is not a well-defined concept. (2)\n$L_2$ norm of perturbation can generate high transferability without usage of\ngradient and is a more powerful source than $L_\\infty$ norm. We believe this\nwork sheds light on the vulnerabilities of popular MLaaS platforms and points\nto a few promising research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuhao Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Saizhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenguang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alex X. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyah_R/0/1/0/all/0/1\">Raheem Beyah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Ting Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invariant Descriptors for Intrinsic Reflectance Optimization. (arXiv:2204.04076v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04076","description":"<p>Intrinsic image decomposition aims to factorize an image into albedo\n(reflectance) and shading (illumination) sub-components. Being ill-posed and\nunder-constrained, it is a very challenging computer vision problem. There are\ninfinite pairs of reflectance and shading images that can reconstruct the same\ninput. To address the problem, Intrinsic Images in the Wild provides an\noptimization framework based on a dense conditional random field (CRF)\nformulation that considers long-range material relations. We improve upon their\nmodel by introducing illumination invariant image descriptors: color ratios.\nThe color ratios and the reflectance intrinsic are both invariant to\nillumination and thus are highly correlated. Through detailed experiments, we\nprovide ways to inject the color ratios into the dense CRF optimization. Our\napproach is physics-based, learning-free and leads to more accurate and robust\nreflectance decompositions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baslamisli_A/0/1/0/all/0/1\">Anil S. Baslamisli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gevers_T/0/1/0/all/0/1\">Theo Gevers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"General Incremental Learning with Domain-aware Categorical Representations. (arXiv:2204.04078v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04078","description":"<p>Continual learning is an important problem for achieving human-level\nintelligence in real-world applications as an agent must continuously\naccumulate knowledge in response to streaming data/tasks. In this work, we\nconsider a general and yet under-explored incremental learning problem in which\nboth the class distribution and class-specific domain distribution change over\ntime. In addition to the typical challenges in class incremental learning, this\nsetting also faces the intra-class stability-plasticity dilemma and intra-class\ndomain imbalance problems. To address above issues, we develop a novel\ndomain-aware continual learning method based on the EM framework. Specifically,\nwe introduce a flexible class representation based on the von Mises-Fisher\nmixture model to capture the intra-class structure, using an\nexpansion-and-reduction strategy to dynamically increase the number of\ncomponents according to the class complexity. Moreover, we design a bi-level\nbalanced memory to cope with data imbalances within and across classes, which\ncombines with a distillation loss to achieve better inter- and intra-class\nstability-plasticity trade-off. We conduct exhaustive experiments on three\nbenchmarks: iDigits, iDomainNet and iCIFAR-20. The results show that our\napproach consistently outperforms previous methods by a significant margin,\ndemonstrating its superiority.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiangwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shipeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POSTER: A Pyramid Cross-Fusion Transformer Network for Facial Expression Recognition. (arXiv:2204.04083v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04083","description":"<p>Facial Expression Recognition (FER) has received increasing interest in the\ncomputer vision community. As a challenging task, there are three key issues\nespecially prevalent in FER: inter-class similarity, intra-class discrepancy,\nand scale sensitivity. Existing methods typically address some of these issues,\nbut do not tackle them all in a unified framework. Therefore, in this paper, we\npropose a two-stream Pyramid crOss-fuSion TransformER network (POSTER) that\naims to holistically solve these issues. Specifically, we design a\ntransformer-based cross-fusion paradigm that enables effective collaboration of\nfacial landmark and direct image features to maximize proper attention to\nsalient facial regions. Furthermore, POSTER employs a pyramid structure to\npromote scale invariance. Extensive experimental results demonstrate that our\nPOSTER outperforms SOTA methods on RAF-DB with 92.05%, FERPlus with 91.62%,\nAffectNet (7 cls) with 67.31%, and AffectNet (8 cls) with 63.34%, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Ce Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendieta_M/0/1/0/all/0/1\">Matias Mendieta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic super-resolution in particle tracking problems. (arXiv:2204.04092v1 [eess.SP])","link":"http://arxiv.org/abs/2204.04092","description":"<p>Particle tracking in biological imaging is concerned with reconstructing the\ntrajectories, locations, or velocities of the targeting particles. The standard\napproach of particle tracking consists of two steps: first reconstructing\nstatically the source locations in each time step, and second applying tracking\ntechniques to obtain the trajectories and velocities. In contrast, the dynamic\nreconstruction seeks to simultaneously recover the source locations and\nvelocities from all frames, which enjoys certain advantages. In this paper, we\nprovide a rigorous mathematical analysis for the resolution limit of\nreconstructing source number, locations, and velocities by general dynamical\nreconstruction in particle tracking problems, by which we demonstrate the\npossibility of achieving super-resolution for the dynamic reconstruction. We\nshow that when the location-velocity pairs of the particles are separated\nbeyond certain distances (the resolution limits), the number of particles and\nthe location-velocity pair can be stably recovered. The resolution limits are\nrelated to the cut-off frequency of the imaging system, signal-to-noise ratio,\nand the sparsity of the source. By these estimates, we also derive a stability\nresult for a sparsity-promoting dynamic reconstruction. In addition, we further\nshow that the reconstruction of velocities has a better resolution limit which\nimproves constantly as the particles moving. This result is derived by an\nobservation that the inherent cut-off frequency for the velocity recovery can\nbe viewed as the total observation time multiplies the cut-off frequency of the\nimaging system, which may lead to a better resolution limit as compared to the\none for each diffraction-limited frame. It is anticipated that this observation\ncan inspire new reconstruction algorithms that improve the resolution of\nparticle tracking in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ammari_H/0/1/0/all/0/1\">Habib Ammari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visible-Thermal UAV Tracking: A Large-Scale Benchmark and New Baseline. (arXiv:2204.04120v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04120","description":"<p>With the popularity of multi-modal sensors, visible-thermal (RGB-T) object\ntracking is to achieve robust performance and wider application scenarios with\nthe guidance of objects' temperature information. However, the lack of paired\ntraining samples is the main bottleneck for unlocking the power of RGB-T\ntracking. Since it is laborious to collect high-quality RGB-T sequences, recent\nbenchmarks only provide test sequences. In this paper, we construct a\nlarge-scale benchmark with high diversity for visible-thermal UAV tracking\n(VTUAV), including 500 sequences with 1.7 million high-resolution (1920\n$\\times$ 1080 pixels) frame pairs. In addition, comprehensive applications\n(short-term tracking, long-term tracking and segmentation mask prediction) with\ndiverse categories and scenes are considered for exhaustive evaluation.\nMoreover, we provide a coarse-to-fine attribute annotation, where frame-level\nattributes are provided to exploit the potential of challenge-specific\ntrackers. In addition, we design a new RGB-T baseline, named Hierarchical\nMulti-modal Fusion Tracker (HMFT), which fuses RGB-T data in various levels.\nNumerous experiments on several datasets are conducted to reveal the\neffectiveness of HMFT and the complement of different fusion types. The project\nis available at here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_X/0/1/0/all/0/1\">Xiang Ruan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sat2lod2: A Software For Automated Lod-2 Modeling From Satellite-Derived Orthophoto And Digital Surface Model. (arXiv:2204.04139v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04139","description":"<p>Deriving LoD2 models from orthophoto and digital surface models (DSM)\nreconstructed from satellite images is a challenging task. Existing solutions\nare mostly system approaches that require complicated step-wise processes,\nincluding not only heuristic geometric operations, but also high-level steps\nsuch as machine learning-based semantic segmentation and building detection.\nHere in this paper, we describe an open-source tool, called SAT2LOD2, built\nbased on a minorly modified version of our recently published work. SAT2LoD2 is\na fully open-source and GUI (Graphics User Interface) based software, coded in\nPython, which takes an orthophoto and DSM as inputs, and outputs individual\nbuilding models, and it can additionally take road network shapefiles, and\ncustomized classification maps to further improve the reconstruction results.\nWe further improve the robustness of the method by 1) intergrading building\nsegmentation based on HRNetV2 into our software; and 2) having implemented a\ndecision strategy to identify complex buildings and directly generate mesh to\navoid erroneous LoD2 reconstruction from a system point of view. The software\ncan process a moderate level of data (around 5000*5000 size of orthophoto and\nDSM) using a PC with a graphics card supporting CUDA. Furthermore, the GUI is\nself-contained and stores the intermediate processing results facilitating\nresearchers to learn the process easily and reuse intermediate files as needed.\nThe updated codes and software are available under this GitHub page:\nhttps://github.com/GDAOSU/LOD2BuildingModel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gui_S/0/1/0/all/0/1\">Shengxi Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yang Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Spherical Epipolar Rectification for Multi-View Stereo 3D Reconstruction. (arXiv:2204.04141v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04141","description":"<p>Multi-view stereo (MVS) reconstruction is essential for creating 3D models.\nThe approach involves applying epipolar rectification followed by dense\nmatching for disparity estimation. However, existing approaches face challenges\nin applying dense matching for images with different viewpoints primarily due\nto large differences in object scale. In this paper, we propose a spherical\nmodel for epipolar rectification to minimize distortions caused by differences\nin principal rays. We evaluate the proposed approach using two aerial-based\ndatasets consisting of multi-camera head systems. We show through qualitative\nand quantitative evaluation that the proposed approach performs better than\nframe-based epipolar correction by enhancing the completeness of point clouds\nby up to 4.05% while improving the accuracy by up to 10.23% using LiDAR data as\nground truth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elhashash_M/0/1/0/all/0/1\">Mostafa Elhashash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Intrinsic Image Decomposition Method to Recover Albedo for Aerial Images in Photogrammetry Processing. (arXiv:2204.04142v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04142","description":"<p>Recovering surface albedos from photogrammetric images for realistic\nrendering and synthetic environments can greatly facilitate its downstream\napplications in VR/AR/MR and digital twins. The textured 3D models from\nstandard photogrammetric pipelines are suboptimal to these applications because\nthese textures are directly derived from images, which intrinsically embedded\nthe spatially and temporally variant environmental lighting information, such\nas the sun illumination, direction, causing different looks of the surface,\nmaking such models less realistic when used in 3D rendering under synthetic\nlightings. On the other hand, since albedo images are less variable by\nenvironmental lighting, it can, in turn, benefit basic photogrammetric\nprocessing. In this paper, we attack the problem of albedo recovery for aerial\nimages for the photogrammetric process and demonstrate the benefit of albedo\nrecovery for photogrammetry data processing through enhanced feature matching\nand dense matching. To this end, we proposed an image formation model with\nrespect to outdoor aerial imagery under natural illumination conditions; we\nthen, derived the inverse model to estimate the albedo by utilizing the typical\nphotogrammetric products as an initial approximation of the geometry. The\nestimated albedo images are tested in intrinsic image decomposition,\nrelighting, feature matching, and dense matching/point cloud generation\nresults. Both synthetic and real-world experiments have demonstrated that our\nmethod outperforms existing methods and can enhance photogrammetric processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optical tracking in team sports. (arXiv:2204.04143v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04143","description":"<p>Sports analysis has gained paramount importance for coaches, scouts, and\nfans. Recently, computer vision researchers have taken on the challenge of\ncollecting the necessary data by proposing several methods of automatic player\nand ball tracking. Building on the gathered tracking data, data miners are able\nto perform quantitative analysis on the performance of players and teams. With\nthis survey, our goal is to provide a basic understanding for quantitative data\nanalysts about the process of creating the input data and the characteristics\nthereof. Thus, we summarize the recent methods of optical tracking by providing\na comprehensive taxonomy of conventional and deep learning methods, separately.\nMoreover, we discuss the preprocessing steps of tracking, the most common\nchallenges in this domain, and the application of tracking data to sports\nteams. Finally, we compare the methods by their cost and limitations, and\nconclude the work by highlighting potential future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahimian_P/0/1/0/all/0/1\">Pegah Rahimian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toka_L/0/1/0/all/0/1\">Laszlo Toka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constrained Bundle Adjustment for Structure From Motion Using Uncalibrated Multi-Camera Systems. (arXiv:2204.04145v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04145","description":"<p>Structure from motion using uncalibrated multi-camera systems is a\nchallenging task. This paper proposes a bundle adjustment solution that\nimplements a baseline constraint respecting that these cameras are static to\neach other. We assume these cameras are mounted on a mobile platform,\nuncalibrated, and coarsely synchronized. To this end, we propose the baseline\nconstraint that is formulated for the scenario in which the cameras have\noverlapping views. The constraint is incorporated in the bundle adjustment\nsolution to keep the relative motion of different cameras static. Experiments\nwere conducted using video frames of two collocated GoPro cameras mounted on a\nvehicle with no system calibration. These two cameras were placed capturing\noverlapping contents. We performed our bundle adjustment using the proposed\nconstraint and then produced 3D dense point clouds. Evaluations were performed\nby comparing these dense point clouds against LiDAR reference data. We showed\nthat, as compared to traditional bundle adjustment, our proposed method\nachieved an improvement of 29.38%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Debao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhashash_M/0/1/0/all/0/1\">Mostafa Elhashash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Video Anomaly Detection Framework based on Appearance-Motion Semantics Representation Consistency. (arXiv:2204.04151v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04151","description":"<p>Video anomaly detection refers to the identification of events that deviate\nfrom the expected behavior. Due to the lack of anomalous samples in training,\nvideo anomaly detection becomes a very challenging task. Existing methods\nalmost follow a reconstruction or future frame prediction mode. However, these\nmethods ignore the consistency between appearance and motion information of\nsamples, which limits their anomaly detection performance. Anomalies only occur\nin the moving foreground of surveillance videos, so the semantics expressed by\nvideo frame sequences and optical flow without background information in\nanomaly detection should be highly consistent and significant for anomaly\ndetection. Based on this idea, we propose Appearance-Motion Semantics\nRepresentation Consistency (AMSRC), a framework that uses normal data's\nappearance and motion semantic representation consistency to handle anomaly\ndetection. Firstly, we design a two-stream encoder to encode the appearance and\nmotion information representations of normal samples and introduce constraints\nto further enhance the consistency of the feature semantics between appearance\nand motion information of normal samples so that abnormal samples with low\nconsistency appearance and motion feature representation can be identified.\nMoreover, the lower consistency of appearance and motion features of anomalous\nsamples can be used to generate predicted frames with larger reconstruction\nerror, which makes anomalies easier to spot. Experimental results demonstrate\nthe effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiangyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Caidan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiqiang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Particle Videos Revisited: Tracking Through Occlusions Using Point Trajectories. (arXiv:2204.04153v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04153","description":"<p>Tracking pixels in videos is typically studied as an optical flow estimation\nproblem, where every pixel is described with a displacement vector that locates\nit in the next frame. Even though wider temporal context is freely available,\nprior efforts to take this into account have yielded only small gains over\n2-frame methods. In this paper, we revisit Sand and Teller's \"particle video\"\napproach, and study pixel tracking as a long-range motion estimation problem,\nwhere every pixel is described with a trajectory that locates it in multiple\nfuture frames. We re-build this classic approach using components that drive\nthe current state-of-the-art in flow and object tracking, such as dense cost\nmaps, iterative optimization, and learned appearance updates. We train our\nmodels using long-range amodal point trajectories mined from existing optical\nflow datasets that we synthetically augment with occlusions. We test our\napproach in trajectory estimation benchmarks and in keypoint label propagation\ntasks, and compare favorably against state-of-the-art optical flow and feature\ntracking methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1\">Adam W. Harley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhaoyuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1\">Katerina Fragkiadaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Underwater Image Enhancement Using Pre-trained Transformer. (arXiv:2204.04199v1 [eess.IV])","link":"http://arxiv.org/abs/2204.04199","description":"<p>The goal of this work is to apply a denoising image transformer to remove the\ndistortion from underwater images and compare it with other similar approaches.\nAutomatic restoration of underwater images plays an important role since it\nallows to increase the quality of the images, without the need for more\nexpensive equipment. This is a critical example of the important role of the\nmachine learning algorithms to support marine exploration and monitoring,\nreducing the need for human intervention like the manual processing of the\nimages, thus saving time, effort, and cost. This paper is the first application\nof the image transformer-based approach called \"Pre-Trained Image Processing\nTransformer\" to underwater images. This approach is tested on the UFO-120\ndataset, containing 1500 images with the corresponding clean images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Boudiaf_A/0/1/0/all/0/1\">Abderrahmene Boudiaf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1\">Yuhang Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghimire_A/0/1/0/all/0/1\">Adarsh Ghimire</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Werghi_N/0/1/0/all/0/1\">Naoufel Werghi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masi_G/0/1/0/all/0/1\">Giulia De Masi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Javed_S/0/1/0/all/0/1\">Sajid Javed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dias_J/0/1/0/all/0/1\">Jorge Dias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dancing under the stars: video denoising in starlight. (arXiv:2204.04210v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04210","description":"<p>Imaging in low light is extremely challenging due to low photon counts. Using\nsensitive CMOS cameras, it is currently possible to take videos at night under\nmoonlight (0.05-0.3 lux illumination). In this paper, we demonstrate\nphotorealistic video under starlight (no moon present, $&lt;$0.001 lux) for the\nfirst time. To enable this, we develop a GAN-tuned physics-based noise model to\nmore accurately represent camera noise at the lowest light levels. Using this\nnoise model, we train a video denoiser using a combination of simulated noisy\nvideo clips and real noisy still images. We capture a 5-10 fps video dataset\nwith significant motion at approximately 0.6-0.7 millilux with no active\nillumination. Comparing against alternative methods, we achieve improved video\nquality at the lowest light levels, demonstrating photorealistic video\ndenoising in starlight for the first time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Monakhova_K/0/1/0/all/0/1\">Kristina Monakhova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richter_S/0/1/0/all/0/1\">Stephan R. Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waller_L/0/1/0/all/0/1\">Laura Waller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for Apparel Products from Images in the Wild. (arXiv:1907.02244v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1907.02244","description":"<p>In this age of social media, people often look at what others are wearing. In\nparticular, Instagram and Twitter influencers often provide images of\nthemselves wearing different outfits and their followers are often inspired to\nbuy similar clothes.We propose a system to automatically find the closest\nvisually similar clothes in the online Catalog (street-to-shop searching). The\nproblem is challenging since the original images are taken under different pose\nand lighting conditions. The system initially localizes high-level descriptive\nregions (top, bottom, wristwear. . . ) using multiple CNN detectors such as\nYOLO and SSD that are trained specifically for apparel domain. It then\nclassifies these regions into more specific regions such as t-shirts, tunic or\ndresses. Finally, a feature embedding learned using a multi-task function is\nrecovered for every item and then compared with corresponding items in the\nonline Catalog database and ranked according to distance. We validate our\napproach component-wise using benchmark datasets and end-to-end using human\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Son Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1\">Ming Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1\">Sampath Chanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1\">R. Manmatha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_C/0/1/0/all/0/1\">Cj Taylor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gaining Scale Invariance in UAV Bird's Eye View Object Detection by Adaptive Resizing. (arXiv:2101.12694v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.12694","description":"<p>This work introduces a new preprocessing step for object detection applicable\nto UAV bird's eye view imagery, which we call Adaptive Resizing. By design, it\nhelps alleviate the challenges coming with the vast variances in objects'\nscales, naturally inherent to UAV data sets. Furthermore, it improves inference\nspeed by two to three times on average. We test this extensively on UAVDT,\nVisDrone, and on a new data set we captured ourselves and achieve consistent\nimprovements while being considerably faster. Moreover, we show how to apply\nthis method to generic UAV object detection tasks. Additionally, we\nsuccessfully test our approach on a height transfer task where we train on some\ninterval of altitudes and test on a different one. Furthermore, we introduce a\nsmall, fast detector meant for deployment to an embedded GPU. Code will be made\npublicly available on our website.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Messmer_M/0/1/0/all/0/1\">Martin Messmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiefer_B/0/1/0/all/0/1\">Benjamin Kiefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1\">Andreas Zell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing to Unseen Domains: A Survey on Domain Generalization. (arXiv:2103.03097v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.03097","description":"<p>Machine learning systems generally assume that the training and testing\ndistributions are the same. To this end, a key requirement is to develop models\nthat can generalize to unseen distributions. Domain generalization (DG), i.e.,\nout-of-distribution generalization, has attracted increasing interests in\nrecent years. Domain generalization deals with a challenging setting where one\nor several different but related domain(s) are given, and the goal is to learn\na model that can generalize to an unseen test domain. Great progress has been\nmade in the area of domain generalization for years. This paper presents the\nfirst review of recent advances in this area. First, we provide a formal\ndefinition of domain generalization and discuss several related fields. We then\nthoroughly review the theories related to domain generalization and carefully\nanalyze the theory behind generalization. We categorize recent algorithms into\nthree classes: data manipulation, representation learning, and learning\nstrategy, and present several popular algorithms in detail for each category.\nThird, we introduce the commonly used datasets, applications, and our\nopen-sourced codebase for fair evaluation. Finally, we summarize existing\nliterature and present some potential research topics for the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1\">Yidong Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiqiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Graph Embeddings for Open World Compositional Zero-Shot Learning. (arXiv:2105.01017v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.01017","description":"<p>Compositional Zero-Shot learning (CZSL) aims to recognize unseen compositions\nof state and object visual primitives seen during training. A problem with\nstandard CZSL is the assumption of knowing which unseen compositions will be\navailable at test time. In this work, we overcome this assumption operating on\nthe open world setting, where no limit is imposed on the compositional space at\ntest time, and the search space contains a large number of unseen compositions.\nTo address this problem, we propose a new approach, Compositional Cosine Graph\nEmbeddings (Co-CGE), based on two principles. First, Co-CGE models the\ndependency between states, objects and their compositions through a graph\nconvolutional neural network. The graph propagates information from seen to\nunseen concepts, improving their representations. Second, since not all unseen\ncompositions are equally feasible, and less feasible ones may damage the\nlearned representations, Co-CGE estimates a feasibility score for each unseen\ncomposition, using the scores as margins in a cosine similarity-based loss and\nas weights in the adjacency matrix of the graphs. Experiments show that our\napproach achieves state-of-the-art performances in standard CZSL while\noutperforming previous methods in the open world scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1\">Massimiliano Mancini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naeem_M/0/1/0/all/0/1\">Muhammad Ferjad Naeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1\">Yongqin Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Voxel Graph CNN for Object Classification with Event Cameras. (arXiv:2106.00216v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00216","description":"<p>Event cameras attract researchers' attention due to their low power\nconsumption, high dynamic range, and extremely high temporal resolution.\nLearning models on event-based object classification have recently achieved\nmassive success by accumulating sparse events into dense frames to apply\ntraditional 2D learning methods. Yet, these approaches necessitate heavy-weight\nmodels and are with high computational complexity due to the redundant\ninformation introduced by the sparse-to-dense conversion, limiting the\npotential of event cameras on real-life applications. This study aims to\naddress the core problem of balancing accuracy and model complexity for\nevent-based classification models. To this end, we introduce a novel graph\nrepresentation for event data to exploit their sparsity better and customize a\nlightweight voxel graph convolutional neural network (\\textit{EV-VGCNN}) for\nevent-based classification. Specifically, (1) using voxel-wise vertices rather\nthan previous point-wise inputs to explicitly exploit regional 2D semantics of\nevent streams while keeping the sparsity;(2) proposing a multi-scale feature\nrelational layer (\\textit{MFRL}) to extract spatial and motion cues from each\nvertex discriminatively concerning its distances to neighbors. Comprehensive\nexperiments show that our model can advance state-of-the-art classification\naccuracy with extremely low model complexity (merely 0.84M parameters).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yongjian Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Youfu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Provident Vehicle Detection at Night for Advanced Driver Assistance Systems. (arXiv:2107.11302v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11302","description":"<p>In recent years, computer vision algorithms have become more powerful.\nHowever, current algorithms mainly share one limitation: They rely on directly\nvisible objects. This is a significant drawback compared to human behavior,\nwhere visual cues caused by objects (e.g., shadows) are already used\nintuitively to retrieve information or anticipate occurring objects. While\ndriving at night, this performance deficit becomes even more obvious: Humans\nalready process the light artifacts caused by the headlamps of oncoming\nvehicles to estimate where they appear, whereas current object detection\nsystems require that the oncoming vehicle is directly visible before it can be\ndetected. Based on previous work on this subject, in this paper, we present a\ncomplete system that can detect light artifacts caused by the headlights of\noncoming vehicles so that it detects that a vehicle is approaching providently.\nFor that, an entire algorithm architecture is investigated, including the\ndetection in the image space, the three-dimensional localization, and the\ntracking of light artifacts. To demonstrate the usefulness of such an\nalgorithm, the proposed algorithm is deployed in a test vehicle to use the\ndetected light artifacts to control the glare-free high beam system\nproactively. Using this experimental setting, the provident vehicle detection\nsystem's time benefit compared to an in-production computer vision system is\nquantified. Additionally, the glare-free high beam use case provides a\nreal-time and real-world visualization interface of the detection results by\nconsidering the adaptive headlamps as projectors. With this investigation of\nprovident vehicle detection, we want to put awareness on the unconventional\nsensing task of detecting objects providently and further close the performance\ngap between human behavior and computer vision algorithms to bring autonomous\nand automated driving a step forward.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ewecker_L/0/1/0/all/0/1\">Lukas Ewecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asan_E/0/1/0/all/0/1\">Ebubekir Asan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohnemus_L/0/1/0/all/0/1\">Lars Ohnemus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saralajew_S/0/1/0/all/0/1\">Sascha Saralajew</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CONet: Channel Optimization for Convolutional Neural Networks. (arXiv:2108.06822v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06822","description":"<p>Neural Architecture Search (NAS) has shifted network design from using human\nintuition to leveraging search algorithms guided by evaluation metrics. We\nstudy channel size optimization in convolutional neural networks (CNN) and\nidentify the role it plays in model accuracy and complexity. Current channel\nsize selection methods are generally limited by discrete sample spaces while\nsuffering from manual iteration and simple heuristics. To solve this, we\nintroduce an efficient dynamic scaling algorithm -- CONet -- that automatically\noptimizes channel sizes across network layers for a given CNN. Two metrics --\n\"\\textit{Rank}\" and \"\\textit{Rank Average Slope}\" -- are introduced to identify\nthe information accumulated in training. The algorithm dynamically scales\nchannel sizes up or down over a fixed searching phase. We conduct experiments\non CIFAR10/100 and ImageNet datasets and show that CONet can find efficient and\naccurate architectures searched in ResNet, DARTS, and DARTS+ spaces that\noutperform their baseline models.\n</p>\n<p>This document supersedes previously published paper in ICCV2021-NeurArch\nworkshop. An additional section is included on manual scaling of channel size\nin CNNs to numerically validate of the metrics used in searching optimum\nchannel configurations in CNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1\">Mahdi S. Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jia Shu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_A/0/1/0/all/0/1\">Andre Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jingxuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuli_M/0/1/0/all/0/1\">Mathieu Tuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1\">Sepehr Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadakia_A/0/1/0/all/0/1\">Arsh Kadakia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Group-based Distinctive Image Captioning with Memory Attention. (arXiv:2108.09151v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09151","description":"<p>Describing images using natural language is widely known as image captioning,\nwhich has made consistent progress due to the development of computer vision\nand natural language generation techniques. Though conventional captioning\nmodels achieve high accuracy based on popular metrics, i.e., BLEU, CIDEr, and\nSPICE, the ability of captions to distinguish the target image from other\nsimilar images is under-explored. To generate distinctive captions, a few\npioneers employ contrastive learning or re-weighted the ground-truth captions,\nwhich focuses on one single input image. However, the relationships between\nobjects in a similar image group (e.g., items or properties within the same\nalbum or fine-grained events) are neglected. In this paper, we improve the\ndistinctiveness of image captions using a Group-based Distinctive Captioning\nModel (GdisCap), which compares each image with other images in one similar\ngroup and highlights the uniqueness of each image. In particular, we propose a\ngroup-based memory attention (GMA) module, which stores object features that\nare unique among the image group (i.e., with low similarity to objects in other\nimages). These unique object features are highlighted when generating captions,\nresulting in more distinctive captions. Furthermore, the distinctive words in\nthe ground-truth captions are selected to supervise the language decoder and\nGMA. Finally, we propose a new evaluation metric, distinctive word rate\n(DisWordRate) to measure the distinctiveness of captions. Quantitative results\nindicate that the proposed method significantly improves the distinctiveness of\nseveral baseline models, and achieves the state-of-the-art performance on both\naccuracy and distinctiveness. Results of a user study agree with the\nquantitative evaluation and demonstrate the rationality of the new metric\nDisWordRate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiuniu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenjia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Antoni B. Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Misalignment Problem in Dense Object Detection. (arXiv:2108.12176v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12176","description":"<p>Object detection aims to localize and classify the objects in a given image,\nand these two tasks are sensitive to different object regions. Therefore, some\nlocations predict high-quality bounding boxes but low classification scores,\nand some locations are quite the opposite. A misalignment exists between the\ntwo tasks, and their features are spatially entangled. In order to solve the\nmisalignment problem, we propose a plug-in Spatial-disentangled and\nTask-aligned operator (SALT). By predicting two task-aware point sets that are\nlocated in each task's sensitive regions, SALT can reassign features from those\nregions and align them to the corresponding anchor point. Therefore, features\nfor the two tasks are spatially aligned and disentangled. To minimize the\ndifference between the two regression stages, we propose a Self-distillation\nregression (SDR) loss that can transfer knowledge from the refined regression\nresults to the coarse regression results. On the basis of SALT and SDR loss, we\npropose SALT-Net, which explicitly exploits task-aligned point-set features for\naccurate detection results. Extensive experiments on the MS-COCO dataset show\nthat our proposed methods can consistently boost different state-of-the-art\ndense detectors by $\\sim$2 AP. Notably, SALT-Net with Res2Net-101-DCN backbone\nachieves 53.8 AP on the MS-COCO test-dev.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Min Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_B/0/1/0/all/0/1\">Bo Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Junxing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Degang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zihao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CardiSort: a convolutional neural network for cross vendor automated sorting of cardiac MR images. (arXiv:2109.08479v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.08479","description":"<p>Objectives: To develop an image-based automatic deep learning method to\nclassify cardiac MR images by sequence type and imaging plane for improved\nclinical post-processing efficiency. Methods: Multi-vendor cardiac MRI studies\nwere retrospectively collected from 4 centres and 3 vendors. A two-head\nconvolutional neural network ('CardiSort') was trained to classify 35 sequences\nby imaging sequence (n=17) and plane (n=10). Single vendor training (SVT) on\nsingle centre images (n=234 patients) and multi-vendor training (MVT) with\nmulticentre images (n = 479 patients, 3 centres) was performed. Model accuracy\nwas compared to manual ground truth labels by an expert radiologist on a\nhold-out test set for both SVT and MVT. External validation of MVT\n(MVTexternal) was performed on data from 3 previously unseen magnet systems\nfrom 2 vendors (n=80 patients). Results: High sequence and plane accuracies\nwere observed for SVT (85.2% and 93.2% respectively), and MVT (96.5% and 98.1%\nrespectively) on the hold-out test set. MVTexternal yielded sequence accuracy\nof 92.7% and plane accuracy of 93.0%. There was high accuracy for common\nsequences and conventional cardiac planes. Poor accuracy was observed for\nunderrepresented classes and sequences where there was greater variability in\nacquisition parameters across centres, such as perfusion imaging. Conclusions:\nA deep learning network was developed on multivendor data to classify MRI\nstudies into component sequences and planes, with external validation. With\nrefinement, it has potential to improve workflow by enabling automated sequence\nselection, an important first step in completely automated post-processing\npipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lim_R/0/1/0/all/0/1\">Ruth P Lim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kachel_S/0/1/0/all/0/1\">Stefan Kachel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Villa_A/0/1/0/all/0/1\">Adriana DM Villa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kearney_L/0/1/0/all/0/1\">Leighton Kearney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bettencourt_N/0/1/0/all/0/1\">Nuno Bettencourt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Young_A/0/1/0/all/0/1\">Alistair A Young</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiribiri_A/0/1/0/all/0/1\">Amedeo Chiribiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Scannell_C/0/1/0/all/0/1\">Cian M Scannell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Explanations by Contrastive Learning. (arXiv:2110.00527v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00527","description":"<p>Post-hoc explanation methods, e.g., Grad-CAM, enable humans to inspect the\nspatial regions responsible for a particular network decision. However, it is\nshown that such explanations are not always consistent with human priors, such\nas consistency across image transformations. Given an interpretation algorithm,\ne.g., Grad-CAM, we introduce a novel training method to train the model to\nproduce more consistent explanations. Since obtaining the ground truth for a\ndesired model interpretation is not a well-defined task, we adopt ideas from\ncontrastive self-supervised learning, and apply them to the interpretations of\nthe model rather than its embeddings. We show that our method, Contrastive\nGrad-CAM Consistency (CGC), results in Grad-CAM interpretation heatmaps that\nare more consistent with human annotations while still achieving comparable\nclassification accuracy. Moreover, our method acts as a regularizer and\nimproves the accuracy on limited-data, fine-grained classification settings. In\naddition, because our method does not rely on annotations, it allows for the\nincorporation of unlabeled data into training, which enables better\ngeneralization of the model. Our code is available here:\nhttps://github.com/UCDvision/CGC\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pillai_V/0/1/0/all/0/1\">Vipin Pillai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koohpayegani_S/0/1/0/all/0/1\">Soroush Abbasi Koohpayegani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouligian_A/0/1/0/all/0/1\">Ashley Ouligian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fong_D/0/1/0/all/0/1\">Dennis Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image prediction of disease progression by style-based manifold extrapolation. (arXiv:2111.11439v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.11439","description":"<p>Disease-modifying management aims to prevent deterioration and progression of\nthe disease, not just relieve symptoms. Unfortunately, the development of\nnecessary therapies is often hampered by the failure to recognize the\npresymptomatic disease and limited understanding of disease development. We\npresent a generic solution for this problem by a methodology that allows the\nprediction of progression risk and morphology in individuals using a latent\nextrapolation optimization approach. To this end, we combined a regularized\ngenerative adversarial network (GAN) and a latent nearest neighbor algorithm\nfor joint optimization to generate plausible images of future time points. We\nevaluated our method on osteoarthritis (OA) data from a multi-center\nlongitudinal study (the Osteoarthritis Initiative, OAI). With presymptomatic\nbaseline data, our model is generative and significantly outperforms the\nend-to-end learning model in discriminating the progressive cohort. Two\nexperiments were performed with seven experienced radiologists. When no\nsynthetic follow-up radiographs were provided, our model performed better than\nall seven radiologists. In cases where the synthetic follow-ups generated by\nour model were available, the specificity and sensitivity of all readers in\ndiscriminating progressors increased from $72.3\\%$ to $88.6\\%$ and from\n$42.1\\%$ to $51.6\\%$, respectively. Our results open up a new possibility of\nusing model-based morphology and risk prediction to make predictions about\nfuture disease occurrence, as demonstrated in the example of OA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Han_T/0/1/0/all/0/1\">Tianyu Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kather_J/0/1/0/all/0/1\">Jakob Nikolas Kather</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pedersoli_F/0/1/0/all/0/1\">Federico Pedersoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zimmermann_M/0/1/0/all/0/1\">Markus Zimmermann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keil_S/0/1/0/all/0/1\">Sebastian Keil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schulze_Hagen_M/0/1/0/all/0/1\">Maximilian Schulze-Hagen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Terwoelbeck_M/0/1/0/all/0/1\">Marc Terwoelbeck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Isfort_P/0/1/0/all/0/1\">Peter Isfort</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haarburger_C/0/1/0/all/0/1\">Christoph Haarburger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiessling_F/0/1/0/all/0/1\">Fabian Kiessling</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schulz_V/0/1/0/all/0/1\">Volkmar Schulz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuhl_C/0/1/0/all/0/1\">Christiane Kuhl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nebelung_S/0/1/0/all/0/1\">Sven Nebelung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Truhn_D/0/1/0/all/0/1\">Daniel Truhn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A War Beyond Deepfake: Benchmarking Facial Counterfeits and Countermeasures. (arXiv:2111.12912v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12912","description":"<p>In recent years, visual forgery has reached a level of sophistication that\nhumans cannot identify fraud, which poses a significant threat to information\nsecurity. A wide range of malicious applications have emerged, such as fake\nnews, defamation or blackmailing of celebrities, impersonation of politicians\nin political warfare, and the spreading of rumours to attract views. As a\nresult, a rich body of visual forensic techniques has been proposed in an\nattempt to stop this dangerous trend. In this paper, we present a benchmark\nthat provides in-depth insights into visual forgery and visual forensics, using\na comprehensive and empirical approach. More specifically, we develop an\nindependent framework that integrates state-of-the-arts counterfeit generators\nand detectors, and measure the performance of these techniques using various\ncriteria. We also perform an exhaustive analysis of the benchmarking results,\nto determine the characteristics of the methods that serve as a comparative\nreference in this never-ending war between measures and countermeasures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1\">Minh Tam Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1\">Thanh Trung Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_V/0/1/0/all/0/1\">Van Vinh Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh Tam Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh Thi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongzhi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1\">Quoc Viet Hung Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPIN: Simplifying Polar Invariance for Neural networks Application to vision-based irradiance forecasting. (arXiv:2111.14507v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14507","description":"<p>Translational invariance induced by pooling operations is an inherent\nproperty of convolutional neural networks, which facilitates numerous computer\nvision tasks such as classification. Yet to leverage rotational invariant\ntasks, convolutional architectures require specific rotational invariant layers\nor extensive data augmentation to learn from diverse rotated versions of a\ngiven spatial configuration. Unwrapping the image into its polar coordinates\nprovides a more explicit representation to train a convolutional architecture\nas the rotational invariance becomes translational, hence the visually distinct\nbut otherwise equivalent rotated versions of a given scene can be learnt from a\nsingle image. We show with two common vision-based solar irradiance forecasting\nchallenges (i.e. using ground-taken sky images or satellite images), that this\npreprocessing step significantly improves prediction results by standardising\nthe scene representation, while decreasing training time by a factor of 4\ncompared to augmenting data with rotations. In addition, this transformation\nmagnifies the area surrounding the centre of the rotation, leading to more\naccurate short-term irradiance predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paletta_Q/0/1/0/all/0/1\">Quentin Paletta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anthony Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbod_G/0/1/0/all/0/1\">Guillaume Arbod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanc_P/0/1/0/all/0/1\">Philippe Blanc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1\">Joan Lasenby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Partial-to-Partial Point Cloud Registration in a Full Range. (arXiv:2111.15606v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15606","description":"<p>Point cloud registration for 3D objects is a challenging task due to sparse\nand noisy measurements, incomplete observations and large transformations. In\nthis work, we propose \\textbf{G}raph \\textbf{M}atching \\textbf{C}onsensus\n\\textbf{Net}work (\\textbf{GMCNet}), which estimates pose-invariant\ncorrespondences for full-range Partial-to-Partial point cloud Registration\n(PPR) in the object-level registration scenario. To encode robust point\ndescriptors, \\textbf{1)} we first comprehensively investigate\ntransformation-robustness and noise-resilience of various geometric features.\n\\textbf{2)} Then, we employ a novel {T}ransformation-robust {P}oint\n{T}ransformer (\\textbf{TPT}) module to adaptively aggregate local features\nregarding the structural relations, which takes advantage from both handcrafted\nrotation-invariant ({\\textit{RI}}) features and noise-resilient spatial\ncoordinates. \\textbf{3)} Based on a synergy of hierarchical graph networks and\ngraphical modeling, we propose the {H}ierarchical {G}raphical {M}odeling\n(\\textbf{HGM}) architecture to encode robust descriptors consisting of i) a\nunary term learned from {\\textit{RI}} features; and ii) multiple smoothness\nterms encoded from neighboring point relations at different scales through our\nTPT modules. Moreover, we construct a challenging PPR dataset (\\textbf{MVP-RG})\nbased on the recent MVP dataset that features high-quality scans. Extensive\nexperiments show that GMCNet outperforms previous state-of-the-art methods for\nPPR. Notably, GMCNet encodes point descriptors for each point cloud\nindividually without using cross-contextual information, or ground truth\ncorrespondences for training. Our code and datasets are available at:\nhttps://github.com/paul007pl/GMCNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event-Based Fusion for Motion Deblurring with Cross-modal Attention. (arXiv:2112.00167v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00167","description":"<p>Traditional frame-based cameras inevitably suffer from motion blur due to\nlong exposure times. As a kind of bio-inspired camera, the event camera records\nthe intensity changes in an asynchronous way with high temporal resolution,\nproviding valid image degradation information within the exposure time. In this\npaper, we rethink the eventbased image deblurring problem and unfold it into an\nend-to-end two-stage image restoration network. To effectively fuse event and\nimage features, we design an event-image cross-modal attention module applied\nat multiple levels of our network, which allows to focus on relevant features\nfrom the event branch and filter out noise. We also introduce a novel symmetric\ncumulative event representation specifically for image deblurring as well as an\nevent mask gated connection between the two stages of our network which helps\navoid information loss. At the dataset level, to foster event-based motion\ndeblurring and to facilitate evaluation on challenging real-world images, we\nintroduce the Real Event Blur (REBlur) dataset, captured with an event camera\nin an illumination controlled optical laboratory. Our Event Fusion Network\n(EFNet) sets the new state of the art in motion deblurring, surpassing both the\nprior best-performing image-based method and all event-based methods with\npublic implementations on the GoPro dataset (by up to 2.47dB) and on our REBlur\ndataset, even in extreme blurry conditions. The code and our REBlur dataset\nwill be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1\">Christos Sakaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jingyun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yaozu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaiwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Learning with Adaptive Batchnorm for Personalized Healthcare. (arXiv:2112.00734v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.00734","description":"<p>There is a growing interest in applying machine learning techniques for\nhealthcare. Recently, federated machine learning (FL) is gaining popularity\nsince it allows researchers to train powerful models without compromising data\nprivacy and security. However, the performance of existing FL approaches often\ndeteriorates when encountering non-iid situations where there exist\ndistribution gaps among clients, and few previous efforts focus on\npersonalization in healthcare. In this article, we propose AdaFed to tackle\ndomain shifts and obtain personalized models for local clients. AdaFed learns\nthe similarity between clients via the statistics of the batch normalization\nlayers while preserving the specificity of each client with different local\nbatch normalization. Comprehensive experiments on five healthcare benchmarks\ndemonstrate that AdaFed achieves better accuracy compared to state-of-the-art\nmethods (e.g., \\textbf{10}\\%+ accuracy improvement for PAMAP2) with faster\nconvergence speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiqiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Putting 3D Spatially Sparse Networks on a Diet. (arXiv:2112.01316v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01316","description":"<p>3D neural networks have become prevalent for many 3D vision tasks including\nobject detection, segmentation, registration, and various perception tasks for\n3D inputs. However, due to the sparsity and irregularity of 3D data, custom 3D\noperators or network designs have been the primary focus of research, while the\nsize of networks or efficacy of parameters has been overlooked. In this work,\nwe perform the first comprehensive study on the weight sparsity of spatially\nsparse 3D convolutional networks and propose a compact weight-sparse and\nspatially sparse 3D convnet (WS^3-Convnet) for semantic and instance\nsegmentation on the real-world indoor and outdoor datasets. We employ various\nnetwork pruning strategies to find compact networks and show our WS^3-Convnet\nachieves minimal loss in performance (2.15\\% drop) with orders-of-magnitude\nsmaller number of parameters (99\\% compression rate) and computational cost\n(95\\% reduction). Finally, we systematically analyze the compression patterns\nof WS^3-Convnet and show interesting emerging sparsity patterns common in our\ncompressed networks to further speed up inference (45\\% faster).\n\\keywords{Efficient network architecture, Network pruning, 3D scene\nsegmentation, Spatially sparse convolution}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junha Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choy_C/0/1/0/all/0/1\">Christopher Choy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GCA-Net : Utilizing Gated Context Attention for Improving Image Forgery Localization and Detection. (arXiv:2112.04298v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04298","description":"<p>Forensic analysis of manipulated pixels requires the identification of\nvarious hidden and subtle features from images. Conventional image recognition\nmodels generally fail at this task because they are biased and more attentive\ntoward the dominant local and spatial features. In this paper, we propose a\nnovel Gated Context Attention Network (GCA-Net) that utilizes non-local\nattention in conjunction with a gating mechanism in order to capture the finer\nimage discrepancies and better identify forged regions. The proposed framework\nuses high dimensional embeddings to filter and aggregate the relevant context\nfrom coarse feature maps at various stages of the decoding process. This\nimproves the network's understanding of global differences and reduces\nfalse-positive localizations. Our evaluation on standard image forensic\nbenchmarks shows that GCA-Net can both compete against and improve over\nstate-of-the-art networks by an average of 4.7% AUC. Additional ablation\nstudies also demonstrate the method's robustness against attributions and\nresilience to false-positive predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sowmen Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md. Saiful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1\">Md. Ruhul Amin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Hands as Probes for Interactive Object Understanding. (arXiv:2112.09120v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09120","description":"<p>Interactive object understanding, or what we can do to objects and how is a\nlong-standing goal of computer vision. In this paper, we tackle this problem\nthrough observation of human hands in in-the-wild egocentric videos. We\ndemonstrate that observation of what human hands interact with and how can\nprovide both the relevant data and the necessary supervision. Attending to\nhands, readily localizes and stabilizes active objects for learning and reveals\nplaces where interactions with objects occur. Analyzing the hands shows what we\ncan do to objects and how. We apply these basic principles on the EPIC-KITCHENS\ndataset, and successfully learn state-sensitive features, and object\naffordances (regions of interaction and afforded grasps), purely by observing\nhands in egocentric videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_M/0/1/0/all/0/1\">Mohit Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_S/0/1/0/all/0/1\">Sahil Modi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_R/0/1/0/all/0/1\">Rishabh Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Saurabh Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An effective coaxiality measurement for twist drill based on line structured light sensor. (arXiv:2112.09873v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09873","description":"<p>Aiming at the accurate and effective coaxiality measurement for twist drill\nwith irregular surface, an optical measurement mechanism is proposed in this\npaper. First, A high-precision rotation instrument based on four core units is\ndesigned, which can obtain the 3-D point cloud data of full angle for the twist\ndrill. Second, in the data processing stage, an improved robust Gaussian\nmixture model is established for accurate and rapid blade back segmentation. To\nimprove measurement efficiency, a rapid reconstruction method of the twist\ndrill axis based on orthogonal synthesis is provided to locate the axial\nposition of the maximum deviation from the benchmark by utilizing the extracted\nblade back data. Finally, by calculating the maximum radial Euclidean distance\nfrom the benchmark, the coaxiality error of the twist drill is obtained.\nComparing with other measurement methods, experimental results show that our\nproposed method is effective with high precision of 3 um and high efficiency of\nless than 3 s/pc. The result demonstrate that the proposed method is effective,\nrobust and automatic, it can be applied in many actual industrial scene.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1\">Ailing Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiaojiao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shufang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Fei Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An analysis of over-sampling labeled data in semi-supervised learning with FixMatch. (arXiv:2201.00604v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.00604","description":"<p>Most semi-supervised learning methods over-sample labeled data when\nconstructing training mini-batches. This paper studies whether this common\npractice improves learning and how. We compare it to an alternative setting\nwhere each mini-batch is uniformly sampled from all the training data, labeled\nor not, which greatly reduces direct supervision from true labels in typical\nlow-label regimes. However, this simpler setting can also be seen as more\ngeneral and even necessary in multi-task problems where over-sampling labeled\ndata would become intractable. Our experiments on semi-supervised CIFAR-10\nimage classification using FixMatch show a performance drop when using the\nuniform sampling approach which diminishes when the amount of labeled data or\nthe training time increases. Further, we analyse the training dynamics to\nunderstand how over-sampling of labeled data compares to uniform sampling. Our\nmain finding is that over-sampling is especially beneficial early in training\nbut gets less important in the later stages when more pseudo-labels become\ncorrect. Nevertheless, we also find that keeping some true labels remains\nimportant to avoid the accumulation of confirmation errors from incorrect\npseudo-labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rabadan_M/0/1/0/all/0/1\">Miquel Mart&#xed; i Rabad&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bujwid_S/0/1/0/all/0/1\">Sebastian Bujwid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pieropan_A/0/1/0/all/0/1\">Alessandro Pieropan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizpour_H/0/1/0/all/0/1\">Hossein Azizpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maki_A/0/1/0/all/0/1\">Atsuto Maki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalised Image Outpainting with U-Transformer. (arXiv:2201.11403v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.11403","description":"<p>While most present image outpainting conducts horizontal extrapolation, we\nstudy the generalised image outpainting problem that extrapolates visual\ncontext all-side around a given image. To this end, we develop a novel\ntransformer-based generative adversarial network called U-Transformer able to\nextend image borders with plausible structure and details even for complicated\nscenery images. Specifically, we design a generator as an encoder-to-decoder\nstructure embedded with the popular Swin Transformer blocks. As such, our novel\nframework can better cope with image long-range dependencies which are\ncrucially important for generalised image outpainting. We propose additionally\na U-shaped structure and multi-view Temporal Spatial Predictor network to\nreinforce image self-reconstruction as well as unknown-part prediction smoothly\nand realistically. We experimentally demonstrate that our proposed method could\nproduce visually appealing results for generalized image outpainting against\nthe state-of-the-art image outpainting approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Penglei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1\">Yujie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yuyao Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Should I take a walk? Estimating Energy Expenditure from Video Data. (arXiv:2202.00712v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.00712","description":"<p>We explore the problem of automatically inferring the amount of kilocalories\nused by human during physical activity from his/her video observation. To study\nthis underresearched task, we introduce Vid2Burn -- an omni-source benchmark\nfor estimating caloric expenditure from video data featuring both, high- and\nlow-intensity activities for which we derive energy expenditure annotations\nbased on models established in medical literature. In practice, a training set\nwould only cover a certain amount of activity types, and it is important to\nvalidate, if the model indeed captures the essence of energy expenditure,\n(e.g., how many and which muscles are involved and how intense they work)\ninstead of memorizing fixed values of specific activity categories seen during\ntraining. Ideally, the models should look beyond such category-specific biases\nand regress the caloric cost in videos depicting activity categories not\nexplicitly present during training. With this property in mind, Vid2Burn is\naccompanied with a cross-category benchmark, where the task is to regress\ncaloric expenditure for types of physical activities not present during\ntraining. An extensive evaluation of state-of-the-art approaches for video\nrecognition modified for the energy expenditure estimation task demonstrates\nthe difficulty of this problem, especially for new activity types at test-time,\nmarking a new research direction. Dataset and code are available at\nhttps://github.com/KPeng9510/Vid2Burn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1\">Alina Roitberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray. (arXiv:2202.01020v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.01020","description":"<p>Computed tomography (CT) is an effective medical imaging modality, widely\nused in the field of clinical medicine for the diagnosis of various\npathologies. Advances in Multidetector CT imaging technology have enabled\nadditional functionalities, including generation of thin slice multiplanar\ncross-sectional body imaging and 3D reconstructions. However, this involves\npatients being exposed to a considerable dose of ionising radiation. Excessive\nionising radiation can lead to deterministic and harmful effects on the body.\nThis paper proposes a Deep Learning model that learns to reconstruct CT\nprojections from a few or even a single-view X-ray. This is based on a novel\narchitecture that builds from neural radiance fields, which learns a continuous\nrepresentation of CT scans by disentangling the shape and volumetric depth of\nsurface and internal anatomical structures from 2D images. Our model is trained\non chest and knee datasets, and we demonstrate qualitative and quantitative\nhigh-fidelity renderings and compare our approach to other recent radiance\nfield-based methods. Our code and link to our datasets are available at\nhttps://github.com/abrilcf/mednerf\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Corona_Figueroa_A/0/1/0/all/0/1\">Abril Corona-Figueroa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frawley_J/0/1/0/all/0/1\">Jonathan Frawley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bond_Taylor_S/0/1/0/all/0/1\">Sam Bond-Taylor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bethapudi_S/0/1/0/all/0/1\">Sarath Bethapudi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shum_H/0/1/0/all/0/1\">Hubert P. H. Shum</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Willcocks_C/0/1/0/all/0/1\">Chris G. Willcocks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cyber Mobility Mirror: A Deep Learning-based Real-World Object Perception Platform Using Roadside LiDAR. (arXiv:2202.13505v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13505","description":"<p>Object perception plays a fundamental role in Cooperative Driving Automation\n(CDA) which is regarded as a revolutionary promoter for the next-generation\ntransportation systems. However, the vehicle-based perception may suffer from\nthe limited sensing range and occlusion as well as low penetration rates in\nconnectivity. In this paper, we propose Cyber Mobility Mirror (CMM), a\nnext-generation real-time traffic surveillance system for 3D object perception\nand reconstruction, to explore the potential of roadside sensors for enabling\nCDA in the real world. The CMM system consists of six main components: 1) the\ndata pre-processor to retrieve and preprocess the raw data; 2) the roadside 3D\nobject detector to generate 3D detection results; 3) the multi-object tracker\nto identify detected objects; 4) the global locator to map positioning\ninformation from the LiDAR coordinate to geographic coordinate using coordinate\ntransformation; 5) the cloud-based communicator to transmit perception\ninformation from roadside sensors to equipped vehicles, and 6) the onboard\nadvisor to reconstruct and display the real-time traffic conditions via\nGraphical User Interface (GUI). In this study, a field-operational system is\ndeployed at a real-world intersection, University Avenue and Iowa Avenue in\nRiverside, California to assess the feasibility and performance of our CMM\nsystem. Results from field tests demonstrate that our CMM prototype system can\nprovide satisfactory perception performance with 96.99% precision and 83.62%\nrecall. High-fidelity real-time traffic conditions (at the object level) can be\ngeo-localized with an average error of 0.14m and displayed on the GUI of the\nequipped vehicle with a frequency of 3-4 Hz.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1\">Zhengwei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1\">Saswat Priyadarshi Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xuanpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barth_M/0/1/0/all/0/1\">Matthew J. Barth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xuewei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongkang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sisbot_E/0/1/0/all/0/1\">Emrah Akin Sisbot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguchi_K/0/1/0/all/0/1\">Kentaro Oguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Point Cloud Based Place Recognition with Ranking-based Loss and Large Batch Training. (arXiv:2203.00972v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00972","description":"<p>The paper presents a simple and effective learning-based method for computing\na discriminative 3D point cloud descriptor for place recognition purposes.\nRecent state-of-the-art methods have relatively complex architectures such as\nmulti-scale oyramid of point Transformers combined with a pyramid of feature\naggregation modules. Our method uses a simple and efficient 3D convolutional\nfeature extraction, based on a sparse voxelized representation, enhanced with\nchannel attention blocks. We employ recent advances in image retrieval and\npropose a modified version of a loss function based on a differentiable average\nprecision approximation. Such loss function requires training with very large\nbatches for the best results. This is enabled by using multistaged\nbackpropagation. Experimental evaluation on the popular benchmarks proves the\neffectiveness of our approach, with a consistent improvement over the state of\nthe art\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Komorowski_J/0/1/0/all/0/1\">Jacek Komorowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OVE6D: Object Viewpoint Encoding for Depth-based 6D Object Pose Estimation. (arXiv:2203.01072v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01072","description":"<p>This paper proposes a universal framework, called OVE6D, for model-based 6D\nobject pose estimation from a single depth image and a target object mask. Our\nmodel is trained using purely synthetic data rendered from ShapeNet, and,\nunlike most of the existing methods, it generalizes well on new real-world\nobjects without any fine-tuning. We achieve this by decomposing the 6D pose\ninto viewpoint, in-plane rotation around the camera optical axis and\ntranslation, and introducing novel lightweight modules for estimating each\ncomponent in a cascaded manner. The resulting network contains less than 4M\nparameters while demonstrating excellent performance on the challenging T-LESS\nand Occluded LINEMOD datasets without any dataset-specific training. We show\nthat OVE6D outperforms some contemporary deep learning-based pose estimation\nmethods specifically trained for individual objects or datasets with real-world\ntraining data.\n</p>\n<p>The implementation and the pre-trained model will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Dingding Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heikkila_J/0/1/0/all/0/1\">Janne Heikkil&#xe4;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction. (arXiv:2203.01577v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01577","description":"<p>We present HOI4D, a large-scale 4D egocentric dataset with rich annotations,\nto catalyze the research of category-level human-object interaction. HOI4D\nconsists of 2.4M RGB-D egocentric video frames over 4000 sequences collected by\n4 participants interacting with 800 different object instances from 16\ncategories over 610 different indoor rooms. Frame-wise annotations for panoptic\nsegmentation, motion segmentation, 3D hand pose, category-level object pose and\nhand action have also been provided, together with reconstructed object meshes\nand scene point clouds. With HOI4D, we establish three benchmarking tasks to\npromote category-level HOI from 4D visual signals including semantic\nsegmentation of 4D dynamic point cloud sequences, category-level object pose\ntracking, and egocentric action segmentation with diverse interaction targets.\nIn-depth analysis shows HOI4D poses great challenges to existing methods and\nproduces great research opportunities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Che Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_K/0/1/0/all/0/1\">Kangbo Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1\">Weikang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1\">Boqiang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhoujie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PINA: Learning a Personalized Implicit Neural Avatar from a Single RGB-D Video Sequence. (arXiv:2203.01754v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01754","description":"<p>We present a novel method to learn Personalized Implicit Neural Avatars\n(PINA) from a short RGB-D sequence. This allows non-expert users to create a\ndetailed and personalized virtual copy of themselves, which can be animated\nwith realistic clothing deformations. PINA does not require complete scans, nor\ndoes it require a prior learned from large datasets of clothed humans. Learning\na complete avatar in this setting is challenging, since only few depth\nobservations are available, which are noisy and incomplete (i.e. only partial\nvisibility of the body per frame). We propose a method to learn the shape and\nnon-rigid deformations via a pose-conditioned implicit surface and a\ndeformation field, defined in canonical space. This allows us to fuse all\npartial observations into a single consistent canonical representation. Fusion\nis formulated as a global optimization problem over the pose, shape and\nskinning parameters. The method can learn neural avatars from real noisy RGB-D\nsequences for a diverse set of people and clothing styles and these avatars can\nbe animated given unseen motion sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zijian Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voice-Face Homogeneity Tells Deepfake. (arXiv:2203.02195v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02195","description":"<p>Detecting forgery videos is highly desired due to the abuse of deepfake.\nExisting detection approaches contribute to exploring the specific artifacts in\ndeepfake videos and fit well on certain data. However, the growing technique on\nthese artifacts keeps challenging the robustness of traditional deepfake\ndetectors. As a result, the development of generalizability of these approaches\nhas reached a blockage. To address this issue, given the empirical results that\nthe identities behind voices and faces are often mismatched in deepfake videos,\nand the voices and faces have homogeneity to some extent, in this paper, we\npropose to perform the deepfake detection from an unexplored voice-face\nmatching view. To this end, a voice-face matching detection model is devised to\nmeasure the matching degree of these two on a generic audio-visual dataset.\nThereafter, this model can be smoothly transferred to deepfake datasets without\nany fine-tuning, and the generalization across datasets is accordingly\nenhanced. We conduct extensive experiments over two widely exploited datasets -\nDFDC and FakeAVCeleb. Our model obtains significantly improved performance as\ncompared to other state-of-the-art competitors and maintains favorable\ngeneralizability. The code has been released at\nhttps://github.com/xaCheng1996/VFD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Harry Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yangyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Tao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor Space Using Wearable IMUs and LiDAR. (arXiv:2203.09215v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09215","description":"<p>We propose Human-centered 4D Scene Capture (HSC4D) to accurately and\nefficiently create a dynamic digital world, containing large-scale\nindoor-outdoor scenes, diverse human motions, and rich interactions between\nhumans and environments. Using only body-mounted IMUs and LiDAR, HSC4D is\nspace-free without any external devices' constraints and map-free without\npre-built maps. Considering that IMUs can capture human poses but always drift\nfor long-period use, while LiDAR is stable for global localization but rough\nfor local positions and orientations, HSC4D makes both sensors complement each\nother by a joint optimization and achieves promising results for long-term\ncapture. Relationships between humans and environments are also explored to\nmake their interaction more realistic. To facilitate many down-stream tasks,\nlike AR, VR, robots, autonomous driving, etc., we propose a dataset containing\nthree large scenes (1k-5k $m^2$) with accurate dynamic human motions and\nlocations. Diverse scenarios (climbing gym, multi-story building, slope, etc.)\nand challenging human activities (exercising, walking up/down stairs, climbing,\netc.) demonstrate the effectiveness and the generalization ability of HSC4D.\nThe dataset and code are available at <a href=\"http://www.lidarhumanmotion.net/hsc4d/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yudi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yitai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1\">Chenglu Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Siqi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Fixation: Dynamic Window Visual Transformer. (arXiv:2203.12856v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12856","description":"<p>Recently, a surge of interest in visual transformers is to reduce the\ncomputational cost by limiting the calculation of self-attention to a local\nwindow. Most current work uses a fixed single-scale window for modeling by\ndefault, ignoring the impact of window size on model performance. However, this\nmay limit the modeling potential of these window-based models for multi-scale\ninformation. In this paper, we propose a novel method, named Dynamic Window\nVision Transformer (DW-ViT). The dynamic window strategy proposed by DW-ViT\ngoes beyond the model that employs a fixed single window setting. To the best\nof our knowledge, we are the first to use dynamic multi-scale windows to\nexplore the upper limit of the effect of window settings on model performance.\nIn DW-ViT, multi-scale information is obtained by assigning windows of\ndifferent sizes to different head groups of window multi-head self-attention.\nThen, the information is dynamically fused by assigning different weights to\nthe multi-scale window branches. We conducted a detailed performance evaluation\non three datasets, ImageNet-1K, ADE20K, and COCO. Compared with related\nstate-of-the-art (SoTA) methods, DW-ViT obtains the best performance.\nSpecifically, compared with the current SoTA Swin Transformers\n\\cite{liu2021swin}, DW-ViT has achieved consistent and substantial improvements\non all three datasets with similar parameters and computational costs. In\naddition, DW-ViT exhibits good scalability and can be easily inserted into any\nwindow-based visual transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengzhen Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangrun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qing Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facial Expression Classification using Fusion of Deep Neural Network in Video for the 3rd ABAW3 Competition. (arXiv:2203.12899v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12899","description":"<p>For computers to recognize human emotions, expression classification is an\nequally important problem in the human-computer interaction area. In the 3rd\nAffective Behavior Analysis In-The-Wild competition, the task of expression\nclassification includes eight classes with six basic expressions of human faces\nfrom videos. In this paper, we employ a transformer mechanism to encode the\nrobust representation from the backbone. Fusion of the robust representations\nplays an important role in the expression classification task. Our approach\nachieves 30.35\\% and 28.60\\% for the $F_1$ score on the validation set and the\ntest set, respectively. This result shows the effectiveness of the proposed\narchitecture based on the Aff-Wild2 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phan_K/0/1/0/all/0/1\">Kim Ngan Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hong-Hai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_V/0/1/0/all/0/1\">Van-Thong Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soo-Hyung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MGRR-Net: Multi-level Graph Relational Reasoning Network for Facial Action Units Detection. (arXiv:2204.01349v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01349","description":"<p>The Facial Action Coding System (FACS) encodes the action units (AUs) in\nfacial images, which has attracted extensive research attention due to its wide\nuse in facial expression analysis. Many methods that perform well on automatic\nfacial action unit (AU) detection primarily focus on modeling various types of\nAU relations between corresponding local muscle areas, or simply mining global\nattention-aware facial features, however, neglect the dynamic interactions\namong local-global features. We argue that encoding AU features just from one\nperspective may not capture the rich contextual information between regional\nand global face features, as well as the detailed variability across AUs,\nbecause of the diversity in expression and individual characteristics. In this\npaper, we propose a novel Multi-level Graph Relational Reasoning Network\n(termed MGRR-Net) for facial AU detection. Each layer of MGRR-Net performs a\nmulti-level (i.e., region-level, pixel-wise and channel-wise level) feature\nlearning. While the region-level feature learning from local face patches\nfeatures via graph neural network can encode the correlation across different\nAUs, the pixel-wise and channel-wise feature learning via graph attention\nnetwork can enhance the discrimination ability of AU features from global face\nfeatures. The fused features from the three levels lead to improved AU\ndiscriminative ability. Extensive experiments on DISFA and BP4D AU datasets\nshow that the proposed approach achieves superior performance than the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xuri Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1\">Joemon M. Jose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Songpei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Hu Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAD: Data-free Adversarial Defense at Test Time. (arXiv:2204.01568v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.01568","description":"<p>Deep models are highly susceptible to adversarial attacks. Such attacks are\ncarefully crafted imperceptible noises that can fool the network and can cause\nsevere consequences when deployed. To encounter them, the model requires\ntraining data for adversarial training or explicit regularization-based\ntechniques. However, privacy has become an important concern, restricting\naccess to only trained models but not the training data (e.g. biometric data).\nAlso, data curation is expensive and companies may have proprietary rights over\nit. To handle such situations, we propose a completely novel problem of\n'test-time adversarial defense in absence of training data and even their\nstatistics'. We solve it in two stages: a) detection and b) correction of\nadversarial samples. Our adversarial sample detection framework is initially\ntrained on arbitrary data and is subsequently adapted to the unlabelled test\ndata through unsupervised domain adaptation. We further correct the predictions\non detected adversarial samples by transforming them in Fourier domain and\nobtaining their low frequency component at our proposed suitable radius for\nmodel prediction. We demonstrate the efficacy of our proposed technique via\nextensive experiments against several adversarial attacks and for different\nmodel architectures and datasets. For a non-robust Resnet-18 model pre-trained\non CIFAR-10, our detection method correctly identifies 91.42% adversaries.\nAlso, we significantly improve the adversarial accuracy from 0% to 37.37% with\na minimal drop of 0.02% in clean accuracy on state-of-the-art 'Auto Attack'\nwithout having to retrain the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1\">Gaurav Kumar Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawal_R/0/1/0/all/0/1\">Ruchit Rawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Anirban Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-visual multi-channel speech separation, dereverberation and recognition. (arXiv:2204.01977v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2204.01977","description":"<p>Despite the rapid advance of automatic speech recognition (ASR) technologies,\naccurate recognition of cocktail party speech characterised by the interference\nfrom overlapping speakers, background noise and room reverberation remains a\nhighly challenging task to date. Motivated by the invariance of visual modality\nto acoustic signal corruption, audio-visual speech enhancement techniques have\nbeen developed, although predominantly targeting overlapping speech separation\nand recognition tasks. In this paper, an audio-visual multi-channel speech\nseparation, dereverberation and recognition approach featuring a full\nincorporation of visual information into all three stages of the system is\nproposed. The advantage of the additional visual modality over using audio only\nis demonstrated on two neural dereverberation approaches based on DNN-WPE and\nspectral mapping respectively. The learning cost function mismatch between the\nseparation and dereverberation models and their integration with the back-end\nrecognition system is minimised using fine-tuning on the MSE and LF-MMI\ncriteria. Experiments conducted on the LRS2 dataset suggest that the proposed\naudio-visual multi-channel speech separation, dereverberation and recognition\nsystem outperforms the baseline audio-visual multi-channel speech separation\nand recognition system containing no dereverberation module by a statistically\nsignificant word error rate (WER) reduction of 2.06% absolute (8.77% relative).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guinan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiajun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xunying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Predicates Learning for Scene Graph Generation. (arXiv:2204.02597v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02597","description":"<p>The performance of current Scene Graph Generation models is severely hampered\nby some hard-to-distinguish predicates, e.g., \"woman-on/standing on/walking\non-beach\" or \"woman-near/looking at/in front of-child\". While general SGG\nmodels are prone to predict head predicates and existing re-balancing\nstrategies prefer tail categories, none of them can appropriately handle these\nhard-to-distinguish predicates. To tackle this issue, inspired by fine-grained\nimage classification, which focuses on differentiating among\nhard-to-distinguish object classes, we propose a method named Fine-Grained\nPredicates Learning (FGPL) which aims at differentiating among\nhard-to-distinguish predicates for Scene Graph Generation task. Specifically,\nwe first introduce a Predicate Lattice that helps SGG models to figure out\nfine-grained predicate pairs. Then, utilizing the Predicate Lattice, we propose\na Category Discriminating Loss and an Entity Discriminating Loss, which both\ncontribute to distinguishing fine-grained predicates while maintaining learned\ndiscriminatory power over recognizable ones. The proposed model-agnostic\nstrategy significantly boosts the performances of three benchmark models\n(Transformer, VCTree, and Motif) by 22.8\\%, 24.1\\% and 21.7\\% of Mean Recall\n(mR@100) on the Predicate Classification sub-task, respectively. Our model also\noutperforms state-of-the-art methods by a large margin (i.e., 6.1\\%, 4.6\\%, and\n3.2\\% of Mean Recall (mR@100)) on the Visual Genome dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1\">Xinyu Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDA GAN: Adversarial-Learning-based 3-D Seismic Data Interpolation and Reconstruction for Complex Missing. (arXiv:2204.03197v2 [physics.geo-ph] UPDATED)","link":"http://arxiv.org/abs/2204.03197","description":"<p>The interpolation and reconstruction of missing traces is a crucial step in\nseismic data processing, moreover it is also a highly ill-posed problem,\nespecially for complex cases such as high-ratio random discrete missing,\ncontinuous missing and missing in fault-rich or salt body surveys. These\ncomplex cases are rarely mentioned in current sparse or low-rank priorbased and\ndeep learning-based approaches. To cope with complex missing cases, we propose\nMulti-Dimensional Adversarial GAN (MDA GAN), a novel 3-D GAN framework. It\nemploys three discriminators to ensure the consistency of the reconstructed\ndata with the original data distribution in each dimension. The feature\nsplicing module (FSM) is designed and embedded into the generator of this\nframework, which automatically splices the features of the unmissing part with\nthose of the reconstructed part (missing part), thus fully preserving the\ninformation of the unmissing part. To prevent pixel distortion in the seismic\ndata caused by the adversarial learning process, we propose a new\nreconstruction loss Tanh Cross Entropy (TCE) loss to provide smoother\ngradients. We experimentally verified the effectiveness of the individual\ncomponents of the study and then tested the method on multiple publicly\navailable data. The method achieves reasonable reconstructions for up to 95% of\nrandom discrete missing, 100 traces of continuous missing and more complex\nhybrid missing. In surveys of fault-rich and salt bodies, the method can\nachieve promising reconstructions with up to 75% missing in each of the three\ndirections (98.2% in total).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Dou_Y/0/1/0/all/0/1\">Yimin Dou</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_K/0/1/0/all/0/1\">Kewen Li</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Duan_H/0/1/0/all/0/1\">Hongjie Duan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_T/0/1/0/all/0/1\">Timing Li</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dong_L/0/1/0/all/0/1\">Lin Dong</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Huang_Z/0/1/0/all/0/1\">Zongchao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Sensitive Temporal Feature Learning for Gait Recognition. (arXiv:2204.03270v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03270","description":"<p>Although gait recognition has drawn increasing research attention recently,\nit remains challenging to learn discriminative temporal representation, since\nthe silhouette differences are quite subtle in spatial domain. Inspired by the\nobservation that human can distinguish gaits of different subjects by\nadaptively focusing on temporal clips with different time scales, we propose a\ncontext-sensitive temporal feature learning (CSTL) network for gait\nrecognition. CSTL produces temporal features in three scales, and adaptively\naggregates them according to the contextual information from local and global\nperspectives. Specifically, CSTL contains an adaptive temporal aggregation\nmodule that subsequently performs local relation modeling and global relation\nmodeling to fuse the multi-scale features. Besides, in order to remedy the\nspatial feature corruption caused by temporal operations, CSTL incorporates a\nsalient spatial feature learning (SSFL) module to select groups of\ndiscriminative spatial features. Particularly, we utilize transformers to\nimplement the global relation modeling and the SSFL module. To the best of our\nknowledge, this is the first work that adopts transformer in gait recognition.\nExtensive experiments conducted on three datasets demonstrate the\nstate-of-the-art performance. Concretely, we achieve rank-1 accuracies of\n98.7%, 96.2% and 88.7% under normal-walking, bag-carrying and coat-wearing\nconditions on CASIA-B, 97.5% on OU-MVLP and 50.6% on GREW.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaohu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Duowang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Botao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Bin Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale. (arXiv:2204.03514v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2204.03514","description":"<p>We present a large-scale study of imitating human demonstrations on tasks\nthat require a virtual robot to search for objects in new environments -- (1)\nObjectGoal Navigation (e.g. 'find &amp; go to a chair') and (2) Pick&amp;Place (e.g.\n'find mug, pick mug, find counter, place mug on counter'). First, we develop a\nvirtual teleoperation data-collection infrastructure -- connecting Habitat\nsimulator running in a web browser to Amazon Mechanical Turk, allowing remote\nusers to teleoperate virtual robots, safely and at scale. We collect 80k\ndemonstrations for ObjectNav and 12k demonstrations for Pick&amp;Place, which is an\norder of magnitude larger than existing human demonstration datasets in\nsimulation or on real robots.\n</p>\n<p>Second, we attempt to answer the question -- how does large-scale imitation\nlearning (IL) (which hasn't been hitherto possible) compare to reinforcement\nlearning (RL) (which is the status quo)? On ObjectNav, we find that IL (with no\nbells or whistles) using 70k human demonstrations outperforms RL using 240k\nagent-gathered trajectories. The IL-trained agent demonstrates efficient\nobject-search behavior -- it peeks into rooms, checks corners for small\nobjects, turns in place to get a panoramic view -- none of these are exhibited\nas prominently by the RL agent, and to induce these behaviors via RL would\nrequire tedious reward engineering. Finally, accuracy vs. training data size\nplots show promising scaling behavior, suggesting that simply collecting more\ndemonstrations is likely to advance the state of art further. On Pick&amp;Place,\nthe comparison is starker -- IL agents achieve ${\\sim}$18% success on episodes\nwith new object-receptacle locations when trained with 9.5k human\ndemonstrations, while RL agents fail to get beyond 0%. Overall, our work\nprovides compelling evidence for investing in large-scale imitation learning.\n</p>\n<p>Project page: https://ram81.github.io/projects/habitat-web.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramrakhya_R/0/1/0/all/0/1\">Ram Ramrakhya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Undersander_E/0/1/0/all/0/1\">Eric Undersander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Abhishek Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Review of Sign Language Recognition: Different Types, Modalities, and Datasets. (arXiv:2204.03328v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2204.03328","description":"<p>A machine can understand human activities, and the meaning of signs can help\novercome the communication barriers between the inaudible and ordinary people.\nSign Language Recognition (SLR) is a fascinating research area and a crucial\ntask concerning computer vision and pattern recognition. Recently, SLR usage\nhas increased in many applications, but the environment, background image\nresolution, modalities, and datasets affect the performance a lot. Many\nresearchers have been striving to carry out generic real-time SLR models. This\nreview paper facilitates a comprehensive overview of SLR and discusses the\nneeds, challenges, and problems associated with SLR. We study related works\nabout manual and non-manual, various modalities, and datasets. Research\nprogress and existing state-of-the-art SLR models over the past decade have\nbeen reviewed. Finally, we find the research gap and limitations in this domain\nand suggest future directions. This review paper will be helpful for readers\nand researchers to get complete guidance about SLR and the progressive design\nof the state-of-the-art SLR model\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madhiarasan_D/0/1/0/all/0/1\">Dr. M. Madhiarasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_P/0/1/0/all/0/1\">Prof. Partha Pratim Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}