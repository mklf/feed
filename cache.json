{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-13T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Overview of the Shared Task on Fake News Detection in Urdu at FIRE 2021. (arXiv:2207.05133v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05133","description":"<p>Automatic detection of fake news is a highly important task in the\ncontemporary world. This study reports the 2nd shared task called\nUrduFake@FIRE2021 on identifying fake news detection in Urdu. The goal of the\nshared task is to motivate the community to come up with efficient methods for\nsolving this vital problem, particularly for the Urdu language. The task is\nposed as a binary classification problem to label a given news article as a\nreal or a fake news article. The organizers provide a dataset comprising news\nin five domains: (i) Health, (ii) Sports, (iii) Showbiz, (iv) Technology, and\n(v) Business, split into training and testing sets. The training set contains\n1300 annotated news articles -- 750 real news, 550 fake news, while the testing\nset contains 300 news articles -- 200 real, 100 fake news. 34 teams from 7\ndifferent countries (China, Egypt, Israel, India, Mexico, Pakistan, and UAE)\nregistered to participate in the UrduFake@FIRE2021 shared task. Out of those,\n18 teams submitted their experimental results, and 11 of those submitted their\ntechnical reports, which is substantially higher compared to the UrduFake\nshared task in 2020 when only 6 teams submitted their technical reports. The\ntechnical reports submitted by the participants demonstrated different data\nrepresentation techniques ranging from count-based BoW features to word vector\nembeddings as well as the use of numerous machine learning algorithms ranging\nfrom traditional SVM to various neural network architectures including\nTransformers such as BERT and RoBERTa. In this year's competition, the best\nperforming system obtained an F1-macro score of 0.679, which is lower than the\npast year's best result of 0.907 F1-macro. Admittedly, while training sets from\nthe past and the current years overlap to a large extent, the testing set\nprovided this year is completely different.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amjad_M/0/1/0/all/0/1\">Maaz Amjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butt_S/0/1/0/all/0/1\">Sabur Butt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amjad_H/0/1/0/all/0/1\">Hamza Imam Amjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhila_A/0/1/0/all/0/1\">Alisa Zhila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidorov_G/0/1/0/all/0/1\">Grigori Sidorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UrduFake@FIRE2021: Shared Track on Fake News Identification in Urdu. (arXiv:2207.05144v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05144","description":"<p>This study reports the second shared task named as UrduFake@FIRE2021 on\nidentifying fake news detection in Urdu language. This is a binary\nclassification problem in which the task is to classify a given news article\ninto two classes: (i) real news, or (ii) fake news. In this shared task, 34\nteams from 7 different countries (China, Egypt, Israel, India, Mexico,\nPakistan, and UAE) registered to participate in the shared task, 18 teams\nsubmitted their experimental results and 11 teams submitted their technical\nreports. The proposed systems were based on various count-based features and\nused different classifiers as well as neural network architectures. The\nstochastic gradient descent (SGD) algorithm outperformed other classifiers and\nachieved 0.679 F-score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amjad_M/0/1/0/all/0/1\">Maaz Amjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butt_S/0/1/0/all/0/1\">Sabur Butt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amjad_H/0/1/0/all/0/1\">Hamza Imam Amjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidorov_G/0/1/0/all/0/1\">Grigori Sidorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhila_A/0/1/0/all/0/1\">Alisa Zhila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Language Models perform Abductive Commonsense Reasoning?. (arXiv:2207.05155v1 [cs.AI])","link":"http://arxiv.org/abs/2207.05155","description":"<p>Abductive Reasoning is a task of inferring the most plausible hypothesis\ngiven a set of observations. In literature, the community has approached to\nsolve this challenge by classifying/generating a likely hypothesis that does\nnot contradict with a past observation and future observation. Some of the most\nwell-known benchmarks that tackle this problem are aNLI and aNLG (pronounced as\nalpha-NLI and alpha-NLG). In this report, I review over some of the\nmethodologies that were attempted to solve this challenge, re-implement the\nbaseline models, and analyze some of the weaknesses that current approaches\nhave. The code and the re-implemented results are available at this link.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungone Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Induction enabling Recommending and Trend Analysis: A Corporate Research Community Use Case. (arXiv:2207.05188v1 [cs.AI])","link":"http://arxiv.org/abs/2207.05188","description":"<p>A research division plays an important role of driving innovation in an\norganization. Drawing insights, following trends, keeping abreast of new\nresearch, and formulating strategies are increasingly becoming more challenging\nfor both researchers and executives as the amount of information grows in both\nvelocity and volume. In this paper we present a use case of how a corporate\nresearch community, IBM Research, utilizes Semantic Web technologies to induce\na unified Knowledge Graph from both structured and textual data obtained by\nintegrating various applications used by the community related to research\nprojects, academic papers, datasets, achievements and recognition. In order to\nmake the Knowledge Graph more accessible to application developers, we\nidentified a set of common patterns for exploiting the induced knowledge and\nexposed them as APIs. Those patterns were born out of user research which\nidentified the most valuable use cases or user pain points to be alleviated. We\noutline two distinct scenarios: recommendation and analytics for business use.\nWe will discuss these scenarios in detail and provide an empirical evaluation\non entity recommendation specifically. The methodology used and the lessons\nlearned from this work can be applied to other organizations facing similar\nchallenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sava_M/0/1/0/all/0/1\">Mike Sava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1\">Md Faisal Mahbub Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yachbes_I/0/1/0/all/0/1\">Irene Yachbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gidh_A/0/1/0/all/0/1\">Aditya Gidh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duckwitz_J/0/1/0/all/0/1\">Jillian Duckwitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nisar_K/0/1/0/all/0/1\">Kovit Nisar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1\">Michael Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Neural Numeric-To-Text Generation From Temporal Personal Health Data. (arXiv:2207.05194v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05194","description":"<p>With an increased interest in the production of personal health technologies\ndesigned to track user data (e.g., nutrient intake, step counts), there is now\nmore opportunity than ever to surface meaningful behavioral insights to\neveryday users in the form of natural language. This knowledge can increase\ntheir behavioral awareness and allow them to take action to meet their health\ngoals. It can also bridge the gap between the vast collection of personal\nhealth data and the summary generation required to describe an individual's\nbehavioral tendencies. Previous work has focused on rule-based time-series data\nsummarization methods designed to generate natural language summaries of\ninteresting patterns found within temporal personal health data. We examine\nrecurrent, convolutional, and Transformer-based encoder-decoder models to\nautomatically generate natural language summaries from numeric temporal\npersonal health data. We showcase the effectiveness of our models on real user\nhealth data logged in MyFitnessPal and show that we can automatically generate\nhigh-quality natural language summaries. Our work serves as a first step\ntowards the ambitious goal of automatically generating novel and meaningful\ntemporal summaries from personal health data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harris_J/0/1/0/all/0/1\">Jonathan Harris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1\">Mohammed J. Zaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models (Mostly) Know What They Know. (arXiv:2207.05221v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05221","description":"<p>We study whether language models can evaluate the validity of their own\nclaims and predict which questions they will be able to answer correctly. We\nfirst show that larger models are well-calibrated on diverse multiple choice\nand true/false questions when they are provided in the right format. Thus we\ncan approach self-evaluation on open-ended sampling tasks by asking models to\nfirst propose answers, and then to evaluate the probability \"P(True)\" that\ntheir answers are correct. We find encouraging performance, calibration, and\nscaling for P(True) on a diverse array of tasks. Performance at self-evaluation\nfurther improves when we allow models to consider many of their own samples\nbefore predicting the validity of one specific possibility. Next, we\ninvestigate whether models can be trained to predict \"P(IK)\", the probability\nthat \"I know\" the answer to a question, without reference to any particular\nproposed answer. Models perform well at predicting P(IK) and partially\ngeneralize across tasks, though they struggle with calibration of P(IK) on new\ntasks. The predicted P(IK) probabilities also increase appropriately in the\npresence of relevant source materials in the context, and to the presence of\nhints towards the solution of mathematical word problems. We hope these\nobservations lay the groundwork for training more honest models, and for\ninvestigating how honesty generalizes to cases where models are trained on\nobjectives other than the imitation of human writing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1\">Saurav Kadavath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conerly_T/0/1/0/all/0/1\">Tom Conerly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1\">Amanda Askell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henighan_T/0/1/0/all/0/1\">Tom Henighan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1\">Nicholas Schiefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodds_Z/0/1/0/all/0/1\">Zac Hatfield Dodds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DasSarma_N/0/1/0/all/0/1\">Nova DasSarma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Johnson_E/0/1/0/all/0/1\">Eli Tran-Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnston_S/0/1/0/all/0/1\">Scott Johnston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Showk_S/0/1/0/all/0/1\">Sheer El-Showk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_A/0/1/0/all/0/1\">Andy Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhage_N/0/1/0/all/0/1\">Nelson Elhage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hume_T/0/1/0/all/0/1\">Tristan Hume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anna Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yuntao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Sam Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_D/0/1/0/all/0/1\">Deep Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_D/0/1/0/all/0/1\">Danny Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobson_J/0/1/0/all/0/1\">Josh Jacobson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kernion_J/0/1/0/all/0/1\">Jackson Kernion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kravec_S/0/1/0/all/0/1\">Shauna Kravec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovitt_L/0/1/0/all/0/1\">Liane Lovitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1\">Kamal Ndousse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsson_C/0/1/0/all/0/1\">Catherine Olsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ringer_S/0/1/0/all/0/1\">Sam Ringer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amodei_D/0/1/0/all/0/1\">Dario Amodei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_T/0/1/0/all/0/1\">Tom Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jack Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1\">Nicholas Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mann_B/0/1/0/all/0/1\">Ben Mann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCandlish_S/0/1/0/all/0/1\">Sam McCandlish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olah_C/0/1/0/all/0/1\">Chris Olah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1\">Jared Kaplan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bootstrapping a User-Centered Task-Oriented Dialogue System. (arXiv:2207.05223v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05223","description":"<p>We present TacoBot, a task-oriented dialogue system built for the inaugural\nAlexa Prize TaskBot Challenge, which assists users in completing multi-step\ncooking and home improvement tasks. TacoBot is designed with a user-centered\nprinciple and aspires to deliver a collaborative and accessible dialogue\nexperience. Towards that end, it is equipped with accurate language\nunderstanding, flexible dialogue management, and engaging response generation.\nFurthermore, TacoBot is backed by a strong search engine and an automated\nend-to-end test suite. In bootstrapping the development of TacoBot, we explore\na series of data augmentation strategies to train advanced neural language\nprocessing models and continuously improve the dialogue experience with\ncollected real conversations. At the end of the semifinals, TacoBot achieved an\naverage rating of 3.55/5.0.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shijie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziru Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_A/0/1/0/all/0/1\">Ashley Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_L/0/1/0/all/0/1\">Lingbo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevens_S/0/1/0/all/0/1\">Samuel Stevens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianshu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Korean Sign Language Augmentation (KoSLA) Corpus with Data Augmentation Technique. (arXiv:2207.05261v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05261","description":"<p>We present an efficient framework of corpus for sign language translation.\nAided with a simple but dramatic data augmentation technique, our method\nconverts text into annotated forms with minimum information loss. Sign\nlanguages are composed of manual signals, non-manual signals, and iconic\nfeatures. According to professional sign language interpreters, non-manual\nsignals such as facial expressions and gestures play an important role in\nconveying exact meaning. By considering the linguistic features of sign\nlanguage, our proposed framework is a first and unique attempt to build a\nmultimodal sign language augmentation corpus (hereinafter referred to as the\nKoSLA corpus) containing both manual and non-manual modalities. The corpus we\nbuilt demonstrates confident results in the hospital context, showing improved\nperformance with augmented datasets. To overcome data scarcity, we resorted to\ndata augmentation techniques such as synonym replacement to boost the\nefficiency of our translation model and available data, while maintaining\ngrammatical and semantic structures of sign language. For the experimental\nsupport, we verify the effectiveness of data augmentation technique and\nusefulness of our corpus by performing a translation task between normal\nsentences and sign language annotations on two tokenizers. The result was\nconvincing, proving that the BLEU scores with the KoSLA corpus were\nsignificant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_C/0/1/0/all/0/1\">Changnam An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_E/0/1/0/all/0/1\">Eunkyung Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_D/0/1/0/all/0/1\">Dongmyeong Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_O/0/1/0/all/0/1\">Ohkyoon Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sumi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Hyunshim Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Table Question Answering: Recent Advances. (arXiv:2207.05270v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05270","description":"<p>Table Question Answering (Table QA) refers to providing precise answers from\ntables to answer a user's question. In recent years, there have been a lot of\nworks on table QA, but there is a lack of comprehensive surveys on this\nresearch topic. Hence, we aim to provide an overview of available datasets and\nrepresentative methods in table QA. We classify existing methods for table QA\ninto five categories according to their techniques, which include\nsemantic-parsing-based, generative, extractive, matching-based, and\nretriever-reader-based methods. Moreover, as table QA is still a challenging\ntask for existing methods, we also identify and outline several key challenges\nand discuss the potential future directions of table QA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_N/0/1/0/all/0/1\">Nengzheng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siebert_J/0/1/0/all/0/1\">Joanna Siebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongfang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Few-Shot Named Entity Linking by Meta-Learning. (arXiv:2207.05280v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05280","description":"<p>Entity linking aims to link ambiguous mentions to their corresponding\nentities in a knowledge base, which is significant and fundamental for various\ndownstream applications, e.g., knowledge base completion, question answering,\nand information extraction. While great efforts have been devoted to this task,\nmost of these studies follow the assumption that large-scale labeled data is\navailable. However, when the labeled data is insufficient for specific domains\ndue to labor-intensive annotation work, the performance of existing algorithms\nwill suffer an intolerable decline. In this paper, we endeavor to solve the\nproblem of few-shot entity linking, which only requires a minimal amount of\nin-domain labeled data and is more practical in real situations. Specifically,\nwe firstly propose a novel weak supervision strategy to generate non-trivial\nsynthetic entity-mention pairs based on mention rewriting. Since the quality of\nthe synthetic data has a critical impact on effective model training, we\nfurther design a meta-learning mechanism to assign different weights to each\nsynthetic entity-mention pair automatically. Through this way, we can\nprofoundly exploit rich and precious semantic information to derive a\nwell-trained entity linking model under the few-shot setting. The experiments\non real-world datasets show that the proposed method can extensively improve\nthe state-of-the-art few-shot entity linking model and achieve impressive\nperformance when only a small amount of labeled data is available. Moreover, we\nalso demonstrate the outstanding ability of the model's transferability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiuxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Haitao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLM-ICD: Automatic ICD Coding with Pretrained Language Models. (arXiv:2207.05289v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05289","description":"<p>Automatically classifying electronic health records (EHRs) into diagnostic\ncodes has been challenging to the NLP community. State-of-the-art methods\ntreated this problem as a multilabel classification problem and proposed\nvarious architectures to model this problem. However, these systems did not\nleverage the superb performance of pretrained language models, which achieved\nsuperb performance on natural language understanding tasks. Prior work has\nshown that pretrained language models underperformed on this task with the\nregular finetuning scheme. Therefore, this paper aims at analyzing the causes\nof the underperformance and developing a framework for automatic ICD coding\nwith pretrained language models. We spotted three main issues through the\nexperiments: 1) large label space, 2) long input sequences, and 3) domain\nmismatch between pretraining and fine-tuning. We propose PLMICD, a framework\nthat tackles the challenges with various strategies. The experimental results\nshow that our proposed framework can overcome the challenges and achieves\nstate-of-the-art performance in terms of multiple metrics on the benchmark\nMIMIC data. The source code is available at https://github.com/MiuLab/PLM-ICD\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chao-Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_S/0/1/0/all/0/1\">Shang-Chi Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Nung Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CompoundE: Knowledge Graph Embedding with Translation, Rotation and Scaling Compound Operations. (arXiv:2207.05324v1 [cs.AI])","link":"http://arxiv.org/abs/2207.05324","description":"<p>Translation, rotation, and scaling are three commonly used geometric\nmanipulation operations in image processing. Besides, some of them are\nsuccessfully used in developing effective knowledge graph embedding (KGE)\nmodels such as TransE and RotatE. Inspired by the synergy, we propose a new KGE\nmodel by leveraging all three operations in this work. Since translation,\nrotation, and scaling operations are cascaded to form a compound one, the new\nmodel is named CompoundE. By casting CompoundE in the framework of group\ntheory, we show that quite a few scoring-function-based KGE models are special\ncases of CompoundE. CompoundE extends the simple distance-based relation to\nrelation-dependent compound operations on head and/or tail entities. To\ndemonstrate the effectiveness of CompoundE, we conduct experiments on three\npopular KG completion datasets. Experimental results show that CompoundE\nconsistently achieves the state of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xiou Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yun-Cheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Huqariq: A Multilingual Speech Corpus of Native Languages of Peru for Speech Recognition. (arXiv:2207.05498v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05498","description":"<p>The Huqariq corpus is a multilingual collection of speech from native\nPeruvian languages. The transcribed corpus is intended for the research and\ndevelopment of speech technologies to preserve endangered languages in Peru.\nHuqariq is primarily designed for the development of automatic speech\nrecognition, language identification and text-to-speech tools. In order to\nachieve corpus collection sustainably, we employ the crowdsourcing methodology.\nHuqariq includes four native languages of Peru, and it is expected that by the\nend of the year 2022, it can reach up to 20 native languages out of the 48\nnative languages in Peru. The corpus has 220 hours of transcribed audio\nrecorded by more than 500 volunteers, making it the largest speech corpus for\nnative languages in Peru. In order to verify the quality of the corpus, we\npresent speech recognition experiments using 220 hours of fully transcribed\naudio.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zevallos_R/0/1/0/all/0/1\">Rodolfo Zevallos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_L/0/1/0/all/0/1\">Luis Camacho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melgarejo_N/0/1/0/all/0/1\">Nelsi Melgarejo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoeticTTS -- Controllable Poetry Reading for Literary Studies. (arXiv:2207.05549v1 [eess.AS])","link":"http://arxiv.org/abs/2207.05549","description":"<p>Speech synthesis for poetry is challenging due to specific intonation\npatterns inherent to poetic speech. In this work, we propose an approach to\nsynthesise poems with almost human like naturalness in order to enable literary\nscholars to systematically examine hypotheses on the interplay between text,\nspoken realisation, and the listener's perception of poems. To meet these\nspecial requirements for literary studies, we resynthesise poems by cloning\nprosodic values from a human reference recitation, and afterwards make use of\nfine-grained prosody control to manipulate the synthetic speech in a\nhuman-in-the-loop setting to alter the recitation w.r.t. specific phenomena. We\nfind that finetuning our TTS model on poetry captures poetic intonation\npatterns to a large extent which is beneficial for prosody cloning and\nmanipulation and verify the success of our approach both in an objective\nevaluation as well as in human studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Koch_J/0/1/0/all/0/1\">Julia Koch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lux_F/0/1/0/all/0/1\">Florian Lux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schauffler_N/0/1/0/all/0/1\">Nadja Schauffler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bernhart_T/0/1/0/all/0/1\">Toni Bernhart</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dieterle_F/0/1/0/all/0/1\">Felix Dieterle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuhn_J/0/1/0/all/0/1\">Jonas Kuhn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Richter_S/0/1/0/all/0/1\">Sandra Richter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Viehhauser_G/0/1/0/all/0/1\">Gabriel Viehhauser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Paraphrases to Study Properties of Contextual Embeddings. (arXiv:2207.05553v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05553","description":"<p>We use paraphrases as a unique source of data to analyze contextualized\nembeddings, with a particular focus on BERT. Because paraphrases naturally\nencode consistent word and phrase semantics, they provide a unique lens for\ninvestigating properties of embeddings. Using the Paraphrase Database's\nalignments, we study words within paraphrases as well as phrase\nrepresentations. We find that contextual embeddings effectively handle\npolysemous words, but give synonyms surprisingly different representations in\nmany cases. We confirm previous findings that BERT is sensitive to word order,\nbut find slightly different patterns than prior work in terms of the level of\ncontextualization across BERT's layers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burdick_L/0/1/0/all/0/1\">Laura Burdick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kummerfeld_J/0/1/0/all/0/1\">Jonathan K. Kummerfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear-time calculation of the expected sum of edge lengths in planar linearizations of trees. (arXiv:2207.05564v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05564","description":"<p>Dependency graphs have proven to be a very successful model to represent the\nsyntactic structure of sentences of human languages. In these graphs, widely\naccepted to be trees, vertices are words and arcs connect\nsyntactically-dependent words. The tendency of these dependencies to be short\nhas been demonstrated using random baselines for the sum of the lengths of the\nedges or its variants. A ubiquitous baseline is the expected sum in projective\norderings (wherein edges do not cross and the root word of the sentence is not\ncovered by any edge). It was shown that said expected value can be computed in\n$O(n)$ time. In this article we focus on planar orderings (where the root word\ncan be covered) and present two main results. First, we show the relationship\nbetween the expected sum in planar arrangements and the expected sum in\nprojective arrangements. Second, we also derive a $O(n)$-time algorithm to\ncalculate the expected value of the sum of edge lengths. These two results stem\nfrom another contribution of the present article, namely a characterization of\nplanarity that, given a sentence, yields either the number of planar\npermutations or an efficient algorithm to generate uniformly random planar\npermutations of the words. Our research paves the way for replicating past\nresearch on dependency distance minimization using random planar linearizations\nas random baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1\">Llu&#xed;s Alemany-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inner Monologue: Embodied Reasoning through Planning with Language Models. (arXiv:2207.05608v1 [cs.RO])","link":"http://arxiv.org/abs/2207.05608","description":"<p>Recent works have shown how the reasoning capabilities of Large Language\nModels (LLMs) can be applied to domains beyond natural language processing,\nsuch as planning and interaction for robots. These embodied problems require an\nagent to understand many semantic aspects of the world: the repertoire of\nskills available, how these skills influence the world, and how changes to the\nworld map back to the language. LLMs planning in embodied environments need to\nconsider not just what skills to do, but also how and when to do them - answers\nthat change over time in response to the agent's own choices. In this work, we\ninvestigate to what extent LLMs used in such embodied contexts can reason over\nsources of feedback provided through natural language, without any additional\ntraining. We propose that by leveraging environment feedback, LLMs are able to\nform an inner monologue that allows them to more richly process and plan in\nrobotic control scenarios. We investigate a variety of sources of feedback,\nsuch as success detection, scene description, and human interaction. We find\nthat closed-loop language feedback significantly improves high-level\ninstruction completion on three domains, including simulated and real table top\nrearrangement tasks and long-horizon mobile manipulation tasks in a kitchen\nenvironment in the real world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenlong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Ted Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Harris Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jacky Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1\">Pete Florence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tompson_J/0/1/0/all/0/1\">Jonathan Tompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chebotar_Y/0/1/0/all/0/1\">Yevgen Chebotar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sermanet_P/0/1/0/all/0/1\">Pierre Sermanet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1\">Noah Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jackson_T/0/1/0/all/0/1\">Tomas Jackson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_L/0/1/0/all/0/1\">Linda Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1\">Karol Hausman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1\">Brian Ichter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Cross-lingual Transfer is Under-specified Optimization. (arXiv:2207.05666v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05666","description":"<p>Pretrained multilingual encoders enable zero-shot cross-lingual transfer, but\noften produce unreliable models that exhibit high performance variance on the\ntarget language. We postulate that this high variance results from zero-shot\ncross-lingual transfer solving an under-specified optimization problem. We show\nthat any linear-interpolated model between the source language monolingual\nmodel and source + target bilingual model has equally low source language\ngeneralization error, yet the target language generalization error reduces\nsmoothly and linearly as we move from the monolingual to bilingual model,\nsuggesting that the model struggles to identify good solutions for both source\nand target languages using the source language alone. Additionally, we show\nthat zero-shot solution lies in non-flat region of target language error\ngeneralization surface, causing the high variance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shijie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1\">Mark Dredze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Contribution of Lyrics and Acoustics to Collaborative Understanding of Mood. (arXiv:2207.05680v1 [cs.MM])","link":"http://arxiv.org/abs/2207.05680","description":"<p>In this work, we study the association between song lyrics and mood through a\ndata-driven analysis. Our data set consists of nearly one million songs, with\nsong-mood associations derived from user playlists on the Spotify streaming\nplatform. We take advantage of state-of-the-art natural language processing\nmodels based on transformers to learn the association between the lyrics and\nmoods. We find that a pretrained transformer-based language model in a\nzero-shot setting -- i.e., out of the box with no further training on our data\n-- is powerful for capturing song-mood associations. Moreover, we illustrate\nthat training on song-mood associations results in a highly accurate model that\npredicts these associations for unseen songs. Furthermore, by comparing the\nprediction of a model using lyrics with one using acoustic features, we observe\nthat the relative importance of lyrics for mood prediction in comparison with\nacoustics depends on the specific mood. Finally, we verify if the models are\ncapturing the same information about lyrics and acoustics as humans through an\nannotation task where we obtain human judgments of mood-song relevance based on\nlyrics and acoustics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naseri_S/0/1/0/all/0/1\">Shahrzad Naseri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Sravana Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correia_J/0/1/0/all/0/1\">Joana Correia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlgren_J/0/1/0/all/0/1\">Jussi Karlgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">Rosie Jones</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The MuSe 2022 Multimodal Sentiment Analysis Challenge: Humor, Emotional Reactions, and Stress. (arXiv:2207.05691v1 [cs.LG])","link":"http://arxiv.org/abs/2207.05691","description":"<p>The Multimodal Sentiment Analysis Challenge (MuSe) 2022 is dedicated to\nmultimodal sentiment and emotion recognition. For this year's challenge, we\nfeature three datasets: (i) the Passau Spontaneous Football Coach Humor\n(Passau-SFCH) dataset that contains audio-visual recordings of German football\ncoaches, labelled for the presence of humour; (ii) the Hume-Reaction dataset in\nwhich reactions of individuals to emotional stimuli have been annotated with\nrespect to seven emotional expression intensities, and (iii) the Ulm-Trier\nSocial Stress Test (Ulm-TSST) dataset comprising of audio-visual data labelled\nwith continuous emotion values (arousal and valence) of people in stressful\ndispositions. Using the introduced datasets, MuSe 2022 2022 addresses three\ncontemporary affective computing problems: in the Humor Detection Sub-Challenge\n(MuSe-Humor), spontaneous humour has to be recognised; in the Emotional\nReactions Sub-Challenge (MuSe-Reaction), seven fine-grained `in-the-wild'\nemotions have to be predicted; and in the Emotional Stress Sub-Challenge\n(MuSe-Stress), a continuous prediction of stressed emotion values is featured.\nThe challenge is designed to attract different research communities,\nencouraging a fusion of their disciplines. Mainly, MuSe 2022 targets the\ncommunities of audio-visual emotion recognition, health informatics, and\nsymbolic sentiment analysis. This baseline paper describes the datasets as well\nas the feature sets extracted from them. A recurrent neural network with LSTM\ncells is used to set competitive baseline results on the test partitions for\neach sub-challenge. We report an Area Under the Curve (AUC) of .8480 for\nMuSe-Humor; .2801 mean (from 7-classes) Pearson's Correlations Coefficient for\nMuSe-Reaction, as well as .4931 Concordance Correlation Coefficient (CCC) and\n.4761 for valence and arousal in MuSe-Stress, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christ_L/0/1/0/all/0/1\">Lukas Christ</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amiriparian_S/0/1/0/all/0/1\">Shahin Amiriparian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baird_A/0/1/0/all/0/1\">Alice Baird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzirakis_P/0/1/0/all/0/1\">Panagiotis Tzirakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kathan_A/0/1/0/all/0/1\">Alexander Kathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_N/0/1/0/all/0/1\">Niklas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stappen_L/0/1/0/all/0/1\">Lukas Stappen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messner_E/0/1/0/all/0/1\">Eva-Maria Me&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konig_A/0/1/0/all/0/1\">Andreas K&#xf6;nig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cowen_A/0/1/0/all/0/1\">Alan Cowen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lip-Listening: Mixing Senses to Understand Lips using Cross Modality Knowledge Distillation for Word-Based Models. (arXiv:2207.05692v1 [cs.MM])","link":"http://arxiv.org/abs/2207.05692","description":"<p>In this work, we propose a technique to transfer speech recognition\ncapabilities from audio speech recognition systems to visual speech\nrecognizers, where our goal is to utilize audio data during lipreading model\ntraining. Impressive progress in the domain of speech recognition has been\nexhibited by audio and audio-visual systems. Nevertheless, there is still much\nto be explored with regards to visual speech recognition systems due to the\nvisual ambiguity of some phonemes. To this end, the development of visual\nspeech recognition models is crucial given the instability of audio models. The\nmain contributions of this work are i) building on recent state-of-the-art\nword-based lipreading models by integrating sequence-level and frame-level\nKnowledge Distillation (KD) to their systems; ii) leveraging audio data during\ntraining visual models, a feat which has not been utilized in prior word-based\nwork; iii) proposing the Gaussian-shaped averaging in frame-level KD, as an\nefficient technique that aids the model in distilling knowledge at the sequence\nmodel encoder. This work proposes a novel and competitive architecture for\nlip-reading, as we demonstrate a noticeable improvement in performance, setting\na new benchmark equals to 88.64% on the LRW dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mabrouk_H/0/1/0/all/0/1\">Hadeel Mabrouk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abugabal_O/0/1/0/all/0/1\">Omar Abugabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakr_N/0/1/0/all/0/1\">Nourhan Sakr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eraqi_H/0/1/0/all/0/1\">Hesham M. Eraqi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Do Multilingual Encoders Learn Cross-lingual Representation?. (arXiv:2207.05737v1 [cs.CL])","link":"http://arxiv.org/abs/2207.05737","description":"<p>NLP systems typically require support for more than one language. As\ndifferent languages have different amounts of supervision, cross-lingual\ntransfer benefits languages with little to no training data by transferring\nfrom other languages. From an engineering perspective, multilingual NLP\nbenefits development and maintenance by serving multiple languages with a\nsingle system. Both cross-lingual transfer and multilingual NLP rely on\ncross-lingual representations serving as the foundation. As BERT revolutionized\nrepresentation learning and NLP, it also revolutionized cross-lingual\nrepresentations and cross-lingual transfer. Multilingual BERT was released as a\nreplacement for single-language BERT, trained with Wikipedia data in 104\nlanguages.\n</p>\n<p>Surprisingly, without any explicit cross-lingual signal, multilingual BERT\nlearns cross-lingual representations in addition to representations for\nindividual languages. This thesis first shows such surprising cross-lingual\neffectiveness compared against prior art on various tasks. Naturally, it raises\na set of questions, most notably how do these multilingual encoders learn\ncross-lingual representations. In exploring these questions, this thesis will\nanalyze the behavior of multilingual models in a variety of settings on high\nand low resource languages. We also look at how to inject different\ncross-lingual signals into multilingual encoders, and the optimization behavior\nof cross-lingual transfer with these models. Together, they provide a better\nunderstanding of multilingual encoders on cross-lingual transfer. Our findings\nwill lead us to suggested improvements to multilingual encoders and\ncross-lingual transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shijie Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Can Be Strong Differentially Private Learners. (arXiv:2110.05679v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.05679","description":"<p>Differentially Private (DP) learning has seen limited success for building\nlarge deep learning models of text, and attempts at straightforwardly applying\nDifferentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have\nresulted in large performance drops and high computational overhead. We show\nthat this performance drop can be mitigated with (1) the use of large\npretrained models; (2) hyperparameters that suit DP optimization; and (3)\nfine-tuning objectives aligned with the pretraining procedure. With these\nfactors set right, we obtain private NLP models that outperform\nstate-of-the-art private training approaches and strong non-private baselines\n-- by directly fine-tuning pretrained models with DP optimization on\nmoderately-sized corpora. To address the computational challenge of running\nDP-SGD with large Transformers, we propose a memory saving technique that\nallows clipping in DP-SGD to run without instantiating per-example gradients\nfor any layer in the model. The technique enables privately training\nTransformers with almost the same memory cost as non-private training at a\nmodest run-time overhead. Contrary to conventional wisdom that DP optimization\nfails at learning high-dimensional models (due to noise that scales with\ndimension) empirical results reveal that private learning with pretrained\nmodels tends to not suffer from dimension-dependent performance degradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuechen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1\">Florian Tram&#xe8;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"That's so cute!\": The CARE Dataset for Affective Response Detection. (arXiv:2201.11895v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.11895","description":"<p>Social media plays an increasing role in our communication with friends and\nfamily, and our consumption of information and entertainment. Hence, to design\neffective ranking functions for posts on social media, it would be useful to\npredict the affective response to a post (e.g., whether the user is likely to\nbe humored, inspired, angered, informed). Similar to work on emotion\nrecognition (which focuses on the affect of the publisher of the post), the\ntraditional approach to recognizing affective response would involve an\nexpensive investment in human annotation of training data.\n</p>\n<p>We introduce CARE$_{db}$, a dataset of 230k social media posts annotated\naccording to 7 affective responses using the Common Affective Response\nExpression (CARE) method. The CARE method is a means of leveraging the signal\nthat is present in comments that are posted in response to a post, providing\nhigh-precision evidence about the affective response of the readers to the post\nwithout human annotation. Unlike human annotation, the annotation process we\ndescribe here can be iterated upon to expand the coverage of the method,\nparticularly for new affective responses. We present experiments that\ndemonstrate that the CARE annotations compare favorably with crowd-sourced\nannotations. Finally, we use CARE$_{db}$ to train competitive BERT-based models\nfor predicting affective response as well as emotion detection, demonstrating\nthe utility of the dataset for related tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dwivedi_Yu_J/0/1/0/all/0/1\">Jane Dwivedi-Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halevy_A/0/1/0/all/0/1\">Alon Y. Halevy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ParaNames: A Massively Multilingual Entity Name Corpus. (arXiv:2202.14035v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.14035","description":"<p>We introduce ParaNames, a multilingual parallel name resource consisting of\n118 million names spanning across 400 languages. Names are provided for 13.6\nmillion entities which are mapped to standardized entity types (PER/LOC/ORG).\nUsing Wikidata as a source, we create the largest resource of this type\nto-date. We describe our approach to filtering and standardizing the data to\nprovide the best quality possible. ParaNames is useful for multilingual\nlanguage processing, both in defining tasks for name\ntranslation/transliteration and as supplementary data for tasks such as named\nentity recognition and linking. We demonstrate an application of ParaNames by\ntraining a multilingual model for canonical name translation to and from\nEnglish. Our resource is released under a Creative Commons license (CC BY 4.0)\nat https://github.com/bltlab/paranames.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saleva_J/0/1/0/all/0/1\">Jonne S&#xe4;lev&#xe4;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lignos_C/0/1/0/all/0/1\">Constantine Lignos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Similarity and Content-based Phonetic Self Attention for Speech Recognition. (arXiv:2203.10252v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10252","description":"<p>Transformer-based speech recognition models have achieved great success due\nto the self-attention (SA) mechanism that utilizes every frame in the feature\nextraction process. Especially, SA heads in lower layers capture various\nphonetic characteristics by the query-key dot product, which is designed to\ncompute the pairwise relationship between frames. In this paper, we propose a\nvariant of SA to extract more representative phonetic features. The proposed\nphonetic self-attention (phSA) is composed of two different types of phonetic\nattention; one is similarity-based and the other is content-based. In short,\nsimilarity-based attention captures the correlation between frames while\ncontent-based attention only considers each frame without being affected by\nother frames. We identify which parts of the original dot product equation are\nrelated to two different attention patterns and improve each part with simple\nmodifications. Our experiments on phoneme classification and speech recognition\nshow that replacing SA with phSA for lower layers improves the recognition\nperformance without increasing the latency and the parameter size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shim_K/0/1/0/all/0/1\">Kyuhong Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_W/0/1/0/all/0/1\">Wonyong Sung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment. (arXiv:2203.15937v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.15937","description":"<p>Current leading mispronunciation detection and diagnosis (MDD) systems\nachieve promising performance via end-to-end phoneme recognition. One challenge\nof such end-to-end solutions is the scarcity of human-annotated phonemes on\nnatural L2 speech. In this work, we leverage unlabeled L2 speech via a\npseudo-labeling (PL) procedure and extend the fine-tuning approach based on\npre-trained self-supervised learning (SSL) models. Specifically, we use Wav2vec\n2.0 as our SSL model, and fine-tune it using original labeled L2 speech samples\nplus the created pseudo-labeled L2 speech samples. Our pseudo labels are\ndynamic and are produced by an ensemble of the online model on-the-fly, which\nensures that our model is robust to pseudo label noise. We show that\nfine-tuning with pseudo labels achieves a 5.35% phoneme error rate reduction\nand 2.48% MDD F1 score improvement over a labeled-samples-only fine-tuning\nbaseline. The proposed PL method is also shown to outperform conventional\noffline PL methods. Compared to the state-of-the-art MDD systems, our MDD\nsolution produces a more accurate and consistent phonetic error diagnosis. In\naddition, we conduct an open test on a separate UTD-4Accents dataset, where our\nsystem recognition outputs show a strong correlation with human perception,\nbased on accentedness and intelligibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_M/0/1/0/all/0/1\">Mu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hirschi_K/0/1/0/all/0/1\">Kevin Hirschi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Looney_S/0/1/0/all/0/1\">Stephen D. Looney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_O/0/1/0/all/0/1\">Okim Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hansen_J/0/1/0/all/0/1\">John H. L. Hansen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Role of Task Transferability in Large-Scale Multi-Task Learning. (arXiv:2204.11117v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.11117","description":"<p>Recent work has found that multi-task training with a large number of diverse\ntasks can uniformly improve downstream performance on unseen target tasks. In\ncontrast, literature on task transferability has established that the choice of\nintermediate tasks can heavily affect downstream task performance. In this\nwork, we aim to disentangle the effect of scale and relatedness of tasks in\nmulti-task representation learning. We find that, on average, increasing the\nscale of multi-task learning, in terms of the number of tasks, indeed results\nin better learned representations than smaller multi-task setups. However, if\nthe target tasks are known ahead of time, then training on a smaller set of\nrelated tasks is competitive to the large-scale multi-task training at a\nreduced computational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_V/0/1/0/all/0/1\">Vishakh Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lausen_L/0/1/0/all/0/1\">Leonard Lausen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballesteros_M/0/1/0/all/0/1\">Miguel Ballesteros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_S/0/1/0/all/0/1\">Sheng Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TreeMix: Compositional Constituency-based Data Augmentation for Natural Language Understanding. (arXiv:2205.06153v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.06153","description":"<p>Data augmentation is an effective approach to tackle over-fitting. Many\nprevious works have proposed different data augmentations strategies for NLP,\nsuch as noise injection, word replacement, back-translation etc. Though\neffective, they missed one important characteristic of\nlanguage--compositionality, meaning of a complex expression is built from its\nsub-parts. Motivated by this, we propose a compositional data augmentation\napproach for natural language understanding called TreeMix. Specifically,\nTreeMix leverages constituency parsing tree to decompose sentences into\nconstituent sub-structures and the Mixup data augmentation technique to\nrecombine them to generate new sentences. Compared with previous approaches,\nTreeMix introduces greater diversity to the samples generated and encourages\nmodels to learn compositionality of NLP data. Extensive experiments on text\nclassification and SCAN demonstrate that TreeMix outperforms current\nstate-of-the-art data augmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Clipping: Differentially Private Deep Learning Made Easier and Stronger. (arXiv:2206.07136v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.07136","description":"<p>Per-example gradient clipping is a key algorithmic step that enables\npractical differential private (DP) training for deep learning models. The\nchoice of clipping norm $R$, however, is shown to be vital for achieving high\naccuracy under DP. We propose an easy-to-use replacement, called AutoClipping,\nthat eliminates the need to tune $R$ for any DP optimizers, including DP-SGD,\nDP-Adam, DP-LAMB and many others. The automatic variants are as private and\ncomputationally efficient as existing DP optimizers, but require no DP-specific\nhyperparameters and thus make DP training as amenable as the standard\nnon-private training. We give a rigorous convergence analysis of automatic\nDP-SGD in the non-convex setting, which shows that it enjoys an asymptotic\nconvergence rate that matches the standard SGD. We also demonstrate on various\nlanguage and vision tasks that automatic clipping outperforms or matches the\nstate-of-the-art, and can be easily employed with minimal changes to existing\ncodebases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1\">Zhiqi Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_S/0/1/0/all/0/1\">Sheng Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation of Transformer-based Language Models Revisited. (arXiv:2206.14366v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.14366","description":"<p>In the past few years, transformer-based pre-trained language models have\nachieved astounding success in both industry and academia. However, the large\nmodel size and high run-time latency are serious impediments to applying them\nin practice, especially on mobile phones and Internet of Things (IoT) devices.\nTo compress the model, considerable literature has grown up around the theme of\nknowledge distillation (KD) recently. Nevertheless, how KD works in\ntransformer-based models is still unclear. We tease apart the components of KD\nand propose a unified KD framework. Through the framework, systematic and\nextensive experiments that spent over 23,000 GPU hours render a comprehensive\nanalysis from the perspectives of knowledge types, matching strategies,\nwidth-depth trade-off, initialization, model size, etc. Our empirical results\nshed light on the distillation in the pre-train language model and with\nrelative significant improvement over previous state-of-the-arts(SOTA).\nFinally, we provide a best-practice guideline for the KD in transformer-based\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chengqiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1\">Yunfei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haiqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-specific Characteristic Assistance for Code-switching Speech Recognition. (arXiv:2206.14580v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.14580","description":"<p>Dual-encoder structure successfully utilizes two language-specific encoders\n(LSEs) for code-switching speech recognition. Because LSEs are initialized by\ntwo pre-trained language-specific models (LSMs), the dual-encoder structure can\nexploit sufficient monolingual data and capture the individual language\nattributes. However, most existing methods have no language constraints on LSEs\nand underutilize language-specific knowledge of LSMs. In this paper, we propose\na language-specific characteristic assistance (LSCA) method to mitigate the\nabove problems. Specifically, during training, we introduce two\nlanguage-specific losses as language constraints and generate corresponding\nlanguage-specific targets for them. During decoding, we take the decoding\nabilities of LSMs into account by combining the output probabilities of two\nLSMs and the mixture model to obtain the final predictions. Experiments show\nthat either the training or decoding method of LSCA can improve the model's\nperformance. Furthermore, the best result can obtain up to 15.4% relative error\nreduction on the code-switching test set by combining the training and decoding\nmethods of LSCA. Moreover, the system can process code-switching speech\nrecognition tasks well without extra shared parameters or even retraining based\non two pre-trained LSMs by using our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1\">Tongtong Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_M/0/1/0/all/0/1\">Meng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longbiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yongjie Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuqin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_J/0/1/0/all/0/1\">Jianwu Dang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VEM$^2$L: A Plug-and-play Framework for Fusing Text and Structure Knowledge on Sparse Knowledge Graph Completion. (arXiv:2207.01528v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.01528","description":"<p>Knowledge Graph Completion has been widely studied recently to complete\nmissing elements within triples via mainly modeling graph structural features,\nbut performs sensitive to the sparsity of graph structure. Relevant texts like\nentity names and descriptions, acting as another expression form for Knowledge\nGraphs (KGs), are expected to solve this challenge. Several methods have been\nproposed to utilize both structure and text messages with two encoders, but\nonly achieved limited improvements due to the failure to balance weights\nbetween them. And reserving both structural and textual encoders during\ninference also suffers from heavily overwhelmed parameters. Motivated by\nKnowledge Distillation, we view knowledge as mappings from input to output\nprobabilities and propose a plug-and-play framework VEM2L over sparse KGs to\nfuse knowledge extracted from text and structure messages into a unity.\nSpecifically, we partition knowledge acquired by models into two nonoverlapping\nparts: one part is relevant to the fitting capacity upon training triples,\nwhich could be fused by motivating two encoders to learn from each other on\ntraining sets; the other reflects the generalization ability upon unobserved\nqueries. And correspondingly, we propose a new fusion strategy proved by\nVariational EM algorithm to fuse the generalization ability of models, during\nwhich we also apply graph densification operations to further alleviate the\nsparse graph problem. By combining these two fusion methods, we propose VEM2L\nframework finally. Both detailed theoretical evidence, as well as quantitative\nand qualitative experiments, demonstrates the effectiveness and efficiency of\nour proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tianwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zihao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Haichao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingrun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sendong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-level Fusion of Wav2vec 2.0 and BERT for Multimodal Emotion Recognition. (arXiv:2207.04697v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.04697","description":"<p>The research and applications of multimodal emotion recognition have become\nincreasingly popular recently. However, multimodal emotion recognition faces\nthe challenge of lack of data. To solve this problem, we propose to use\ntransfer learning which leverages state-of-the-art pre-trained models including\nwav2vec 2.0 and BERT for this task. Multi-level fusion approaches including\ncoattention-based early fusion and late fusion with the models trained on both\nembeddings are explored. Also, a multi-granularity framework which extracts not\nonly frame-level speech embeddings but also segment-level embeddings including\nphone, syllable and word-level speech embeddings is proposed to further boost\nthe performance. By combining our coattention-based early fusion model and late\nfusion model with the multi-granularity feature extraction framework, we obtain\nresult that outperforms best baseline approaches by 1.3% unweighted accuracy\n(UA) on the IEMOCAP dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Learning to segment prostate cancer by aggressiveness from scribbles in bi-parametric MRI. (arXiv:2207.05056v1 [eess.IV])","link":"http://arxiv.org/abs/2207.05056","description":"<p>In this work, we propose a deep U-Net based model to tackle the challenging\ntask of prostate cancer segmentation by aggressiveness in MRI based on weak\nscribble annotations. This model extends the size constraint loss proposed by\nKervadec et al. 1 in the context of multiclass detection and segmentation task.\nThis model is of high clinical interest as it allows training on prostate\nbiopsy samples and avoids time-consuming full annotation process. Performance\nis assessed on a private dataset (219 patients) where the full ground truth is\navailable as well as on the ProstateX-2 challenge database, where only biopsy\nresults at different localisations serve as reference. We show that we can\napproach the fully-supervised baseline in grading the lesions by using only\n6.35% of voxels for training. We report a lesion-wise Cohen's kappa score of\n0.29 $\\pm$ 0.07 for the weak model versus 0.32 $\\pm$ 0.05 for the baseline. We\nalso report a kappa score (0.276 $\\pm$ 0.037) on the ProstateX-2 challenge\ndataset with our weak U-Net trained on a combination of ProstateX-2 and our\ndataset, which is the highest reported value on this challenge dataset for a\nsegmentation task to our knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Duran_A/0/1/0/all/0/1\">Audrey Duran</a> (MYRIAD), <a href=\"http://arxiv.org/find/eess/1/au:+Dussert_G/0/1/0/all/0/1\">Gaspard Dussert</a> (MYRIAD), <a href=\"http://arxiv.org/find/eess/1/au:+Lartizien_C/0/1/0/all/0/1\">Carole Lartizien</a> (MYRIAD)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Histopathological Imaging Classification of Breast Tissue for Cancer Diagnosis Support Using Deep Learning Models. (arXiv:2207.05057v1 [eess.IV])","link":"http://arxiv.org/abs/2207.05057","description":"<p>According to some medical imaging techniques, breast histopathology images\ncalled Hematoxylin and Eosin are considered as the gold standard for cancer\ndiagnoses. Based on the idea of dividing the pathologic image (WSI) into\nmultiple patches, we used the window [512,512] sliding from left to right and\nsliding from top to bottom, each sliding step overlapping by 50% to augmented\ndata on a dataset of 400 images which were gathered from the ICIAR 2018 Grand\nChallenge. Then use the EffficientNet model to classify and identify the\nhistopathological images of breast cancer into 4 types: Normal, Benign,\nCarcinoma, Invasive Carcinoma. The EffficientNet model is a recently developed\nmodel that uniformly scales the width, depth, and resolution of the network\nwith a set of fixed scaling factors that are well suited for training images\nwith high resolution. And the results of this model give a rather competitive\nclassification efficiency, achieving 98% accuracy on the training set and 93%\non the evaluation set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1\">Tat-Bao-Thien Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ngo_M/0/1/0/all/0/1\">Minh-Vuong Ngo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_V/0/1/0/all/0/1\">Van-Phong Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Domain Disentanglement for Generalized Multi-source Domain Adaptation. (arXiv:2207.05070v1 [cs.LG])","link":"http://arxiv.org/abs/2207.05070","description":"<p>A typical multi-source domain adaptation (MSDA) approach aims to transfer\nknowledge learned from a set of labeled source domains, to an unlabeled target\ndomain. Nevertheless, prior works strictly assume that each source domain\nshares the identical group of classes with the target domain, which could\nhardly be guaranteed as the target label space is not observable. In this\npaper, we consider a more versatile setting of MSDA, namely Generalized\nMulti-source Domain Adaptation, wherein the source domains are partially\noverlapped, and the target domain is allowed to contain novel categories that\nare not presented in any source domains. This new setting is more elusive than\nany existing domain adaptation protocols due to the coexistence of the domain\nand category shifts across the source and target domains. To address this\nissue, we propose a variational domain disentanglement (VDD) framework, which\ndecomposes the domain representations and semantic features for each instance\nby encouraging dimension-wise independence. To identify the target samples of\nunknown classes, we leverage online pseudo labeling, which assigns the\npseudo-labels to unlabeled target data based on the confidence scores.\nQuantitative and qualitative experiments conducted on two benchmark datasets\ndemonstrate the validity of the proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yadan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng-Fei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RUSH: Robust Contrastive Learning via Randomized Smoothing. (arXiv:2207.05127v1 [cs.LG])","link":"http://arxiv.org/abs/2207.05127","description":"<p>Recently, adversarial training has been incorporated in self-supervised\ncontrastive pre-training to augment label efficiency with exciting adversarial\nrobustness. However, the robustness came at a cost of expensive adversarial\ntraining. In this paper, we show a surprising fact that contrastive\npre-training has an interesting yet implicit connection with robustness, and\nsuch natural robustness in the pre trained representation enables us to design\na powerful robust algorithm against adversarial attacks, RUSH, that combines\nthe standard contrastive pre-training and randomized smoothing. It boosts both\nstandard accuracy and robust accuracy, and significantly reduces training costs\nas compared with adversarial training. We use extensive empirical studies to\nshow that the proposed RUSH outperforms robust classifiers from adversarial\ntraining, by a significant margin on common benchmarks (CIFAR-10, CIFAR-100,\nand STL-10) under first-order attacks. In particular, under\n$\\ell_{\\infty}$-norm perturbations of size 8/255 PGD attack on CIFAR-10, our\nmodel using ResNet-18 as backbone reached 77.8% robust accuracy and 87.9%\nstandard accuracy. Our work has an improvement of over 15% in robust accuracy\nand a slight improvement in standard accuracy, compared to the\nstate-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yijiang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Boyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiayu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FreeREA: Training-Free Evolution-based Architecture Search. (arXiv:2207.05135v1 [cs.NE])","link":"http://arxiv.org/abs/2207.05135","description":"<p>In the last decade, most research in Machine Learning contributed to the\nimprovement of existing models, with the aim of increasing the performance of\nneural networks for the solution of a variety of different tasks. However, such\nadvancements often come at the cost of an increase of model memory and\ncomputational requirements. This represents a significant limitation for the\ndeployability of research output in realistic settings, where the cost, the\nenergy consumption, and the complexity of the framework play a crucial role. To\nsolve this issue, the designer should search for models that maximise the\nperformance while limiting its footprint. Typical approaches to reach this goal\nrely either on manual procedures, which cannot guarantee the optimality of the\nfinal design, or upon Neural Architecture Search algorithms to automatise the\nprocess, at the expenses of extremely high computational time. This paper\nprovides a solution for the fast identification of a neural network that\nmaximises the model accuracy while preserving size and computational\nconstraints typical of tiny devices. Our approach, named FreeREA, is a custom\ncell-based evolution NAS algorithm that exploits an optimised combination of\ntraining-free metrics to rank architectures during the search, thus without\nneed of model training. Our experiments, carried out on the common benchmarks\nNAS-Bench-101 and NATS-Bench, demonstrate that i) FreeREA is the first method\nable to provide very accurate models in minutes of search time; ii) it\noutperforms State of the Art training-based and training-free techniques in all\nthe datasets and benchmarks considered, and iii) it can easily generalise to\nconstrained scenarios, representing a competitive solution for fast Neural\nArchitecture Search in generic constrained applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cavagnero_N/0/1/0/all/0/1\">Niccol&#xf2; Cavagnero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robbiano_L/0/1/0/all/0/1\">Luca Robbiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Averta_G/0/1/0/all/0/1\">Giuseppe Averta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Effective Multi-Label Recognition Attacks via Knowledge Graph Consistency. (arXiv:2207.05137v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05137","description":"<p>Many real-world applications of image recognition require multi-label\nlearning, whose goal is to find all labels in an image. Thus, robustness of\nsuch systems to adversarial image perturbations is extremely important.\nHowever, despite a large body of recent research on adversarial attacks, the\nscope of the existing works is mainly limited to the multi-class setting, where\neach image contains a single label. We show that the naive extensions of\nmulti-class attacks to the multi-label setting lead to violating label\nrelationships, modeled by a knowledge graph, and can be detected using a\nconsistency verification scheme. Therefore, we propose a graph-consistent\nmulti-label attack framework, which searches for small image perturbations that\nlead to misclassifying a desired target set while respecting label hierarchies.\nBy extensive experiments on two datasets and using several multi-label\nrecognition models, we show that our method generates extremely successful\nattacks that, unlike naive multi-label perturbations, can produce model\npredictions consistent with the knowledge graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_H/0/1/0/all/0/1\">Hassan Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhamifar_E/0/1/0/all/0/1\">Ehsan Elhamifar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerated Deep Lossless Image Coding with Unified Paralleleized GPU Coding Architecture. (arXiv:2207.05152v1 [eess.IV])","link":"http://arxiv.org/abs/2207.05152","description":"<p>We propose Deep Lossless Image Coding (DLIC), a full resolution learned\nlossless image compression algorithm. Our algorithm is based on a neural\nnetwork combined with an entropy encoder. The neural network performs a density\nestimation on each pixel of the source image. The density estimation is then\nused to code the target pixel, beating FLIF in terms of compression rate.\nSimilar approaches have been attempted. However, long run times make them\nunfeasible for real world applications. We introduce a parallelized GPU based\nimplementation, allowing for encoding and decoding of grayscale, 8-bit images\nin less than one second. Because DLIC uses a neural network to estimate the\nprobabilities used for the entropy coder, DLIC can be trained on domain\nspecific image data. We demonstrate this capability by adapting and training\nDLIC with Magnet Resonance Imaging (MRI) images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Barzen_B/0/1/0/all/0/1\">Benjamin Lukas Cajus Barzen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glazov_F/0/1/0/all/0/1\">Fedor Glazov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Geistert_J/0/1/0/all/0/1\">Jonas Geistert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sikora_T/0/1/0/all/0/1\">Thomas Sikora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denoising single images by feature ensemble revisited. (arXiv:2207.05176v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05176","description":"<p>Image denoising is still a challenging issue in many computer vision\nsub-domains. Recent studies show that significant improvements are made\npossible in a supervised setting. However, few challenges, such as spatial\nfidelity and cartoon-like smoothing remain unresolved or decisively overlooked.\nOur study proposes a simple yet efficient architecture for the denoising\nproblem that addresses the aforementioned issues. The proposed architecture\nrevisits the concept of modular concatenation instead of long and deeper\ncascaded connections, to recover a cleaner approximation of the given image. We\nfind that different modules can capture versatile representations, and\nconcatenated representation creates a richer subspace for low-level image\nrestoration. The proposed architecture's number of parameters remains smaller\nthan the number for most of the previous networks and still achieves\nsignificant improvements over the current state-of-the-art networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fahim_M/0/1/0/all/0/1\">Masud An Nur Islam Fahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saqib_N/0/1/0/all/0/1\">Nazmus Saqib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siam_S/0/1/0/all/0/1\">Shafkat Khan Siam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Ho Yub Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Activities of People Worldwide. (arXiv:2207.05182v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05182","description":"<p>Every day, humans perform many closely related activities that involve subtle\ndiscriminative motions, such as putting on a shirt vs. putting on a jacket, or\nshaking hands vs. giving a high five. Activity recognition by ethical visual AI\ncould provide insights into our patterns of daily life, however existing\nactivity recognition datasets do not capture the massive diversity of these\nhuman activities around the world. To address this limitation, we introduce\nCollector, a free mobile app to record video while simultaneously annotating\nobjects and activities of consented subjects. This new data collection platform\nwas used to curate the Consented Activities of People (CAP) dataset, the first\nlarge-scale, fine-grained activity dataset of people worldwide. The CAP dataset\ncontains 1.45M video clips of 512 fine grained activity labels of daily life,\ncollected by 780 subjects in 33 countries. We provide activity classification\nand activity detection benchmarks for this dataset, and analyze baseline\nresults to gain insight into how people around with world perform common\nactivities. The dataset, benchmarks, evaluation tools, public leaderboards and\nmobile apps are available for use at visym.github.io/cap.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Byrne_J/0/1/0/all/0/1\">Jeffrey Byrne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castanon_G/0/1/0/all/0/1\">Greg Castanon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ettinger_G/0/1/0/all/0/1\">Gil Ettinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch-level instance-group discrimination with pretext-invariant learning for colitis scoring. (arXiv:2207.05192v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05192","description":"<p>Inflammatory bowel disease (IBD), in particular ulcerative colitis (UC), is\ngraded by endoscopists and this assessment is the basis for risk stratification\nand therapy monitoring. Presently, endoscopic characterisation is largely\noperator dependant leading to sometimes undesirable clinical outcomes for\npatients with IBD. We focus on the Mayo Endoscopic Scoring (MES) system which\nis widely used but requires the reliable identification of subtle changes in\nmucosal inflammation. Most existing deep learning classification methods cannot\ndetect these fine-grained changes which make UC grading such a challenging\ntask. In this work, we introduce a novel patch-level instance-group\ndiscrimination with pretext-invariant representation learning (PLD-PIRL) for\nself-supervised learning (SSL). Our experiments demonstrate both improved\naccuracy and robustness compared to the baseline supervised network and several\nstate-of-the-art SSL methods. Compared to the baseline (ResNet50) supervised\nclassification our proposed PLD-PIRL obtained an improvement of 4.75% on\nhold-out test data and 6.64% on unseen center test data for top-1 accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Ziang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1\">Sharib Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Soumya Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leedham_S/0/1/0/all/0/1\">Simon Leedham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+East_J/0/1/0/all/0/1\">James E East</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rittscher_J/0/1/0/all/0/1\">Jens Rittscher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collaborative Uncertainty Benefits Multi-Agent Multi-Modal Trajectory Forecasting. (arXiv:2207.05195v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05195","description":"<p>In multi-modal multi-agent trajectory forecasting, two major challenges have\nnot been fully tackled: 1) how to measure the uncertainty brought by the\ninteraction module that causes correlations among the predicted trajectories of\nmultiple agents; 2) how to rank the multiple predictions and select the optimal\npredicted trajectory. In order to handle these challenges, this work first\nproposes a novel concept, collaborative uncertainty (CU), which models the\nuncertainty resulting from interaction modules. Then we build a general\nCU-aware regression framework with an original permutation-equivariant\nuncertainty estimator to do both tasks of regression and uncertainty\nestimation. Further, we apply the proposed framework to current SOTA\nmulti-agent multi-modal forecasting systems as a plugin module, which enables\nthe SOTA systems to 1) estimate the uncertainty in the multi-agent multi-modal\ntrajectory forecasting task; 2) rank the multiple predictions and select the\noptimal one based on the estimated uncertainty. We conduct extensive\nexperiments on a synthetic dataset and two public large-scale multi-agent\ntrajectory forecasting benchmarks. Experiments show that: 1) on the synthetic\ndataset, the CU-aware regression framework allows the model to appropriately\napproximate the ground-truth Laplace distribution; 2) on the multi-agent\ntrajectory forecasting benchmarks, the CU-aware regression framework steadily\nhelps SOTA systems improve their performances. Specially, the proposed\nframework helps VectorNet improve by 262 cm regarding the Final Displacement\nError of the chosen optimal prediction on the nuScenes dataset; 3) for\nmulti-agent multi-modal trajectory forecasting systems, prediction uncertainty\nis positively correlated with future stochasticity; and 4) the estimated CU\nvalues are highly related to the interactive information among agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Bohan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei-Tao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_U/0/1/0/all/0/1\">Ulrich Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time And Robust 3D Object Detection with Roadside LiDARs. (arXiv:2207.05200v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05200","description":"<p>This work aims to address the challenges in autonomous driving by focusing on\nthe 3D perception of the environment using roadside LiDARs. We design a 3D\nobject detection model that can detect traffic participants in roadside LiDARs\nin real-time. Our model uses an existing 3D detector as a baseline and improves\nits accuracy. To prove the effectiveness of our proposed modules, we train and\nevaluate the model on three different vehicle and infrastructure datasets. To\nshow the domain adaptation ability of our detector, we train it on an\ninfrastructure dataset from China and perform transfer learning on a different\ndataset recorded in Germany. We do several sets of experiments and ablation\nstudies for each module in the detector that show that our model outperforms\nthe baseline by a significant margin, while the inference speed is at 45 Hz (22\nms). We make a significant contribution with our LiDAR-based 3D detector that\ncan be used for smart city applications to provide connected and automated\nvehicles with a far-reaching view. Vehicles that are connected to the roadside\nsensors can get information about other vehicles around the corner to improve\ntheir path and maneuver planning and to increase road traffic safety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_W/0/1/0/all/0/1\">Walter Zimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jialong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xingcheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois C. Knoll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Novel Object Detection with Weakly Supervised Detection Transformers. (arXiv:2207.05205v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05205","description":"<p>Weakly supervised object detection (WSOD) enables object detectors to be\ntrained using image-level class labels. However, the practical application of\ncurrent WSOD models is limited, as they operate at small scales and require\nextensive training and refinement. We propose the Weakly Supervised Detection\nTransformer, which enables efficient knowledge transfer from a large-scale\npretraining dataset to WSOD finetuning on hundreds of novel objects. We\nleverage pretrained knowledge to improve the multiple instance learning\nframework used in WSOD, and experiments show our approach outperforms the\nstate-of-the-art on datasets with twice the novel classes than previously\nshown.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+LaBonte_T/0/1/0/all/0/1\">Tyler LaBonte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yale Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1\">Vibhav Vineet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Neel Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Susceptibility of Continual Learning Against Adversarial Attacks. (arXiv:2207.05225v1 [cs.LG])","link":"http://arxiv.org/abs/2207.05225","description":"<p>The recent advances in continual (incremental or lifelong) learning have\nconcentrated on the prevention of forgetting that can lead to catastrophic\nconsequences, but there are two outstanding challenges that must be addressed.\nThe first is the evaluation of the robustness of the proposed methods. The\nsecond is ensuring the security of learned tasks remains largely unexplored.\nThis paper presents a comprehensive study of the susceptibility of the\ncontinually learned tasks (including both current and previously learned tasks)\nthat are vulnerable to forgetting. Such vulnerability of tasks against\nadversarial attacks raises profound issues in data integrity and privacy. We\nconsider the task incremental learning (Task-IL) scenario and explore three\nregularization-based experiments, three replay-based experiments, and one\nhybrid technique based on the reply and exemplar approach. We examine the\nrobustness of these methods. In particular, we consider cases where we\ndemonstrate that any class belonging to the current or previously learned tasks\nis prone to misclassification. Our observations highlight the potential\nlimitations of existing Task-IL approaches. Our empirical study recommends that\nthe research community consider the robustness of the proposed continual\nlearning approaches and invest extensive efforts in mitigating catastrophic\nforgetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_H/0/1/0/all/0/1\">Hikmat Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Pir Masoom Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_S/0/1/0/all/0/1\">Syed Farhan Alam Zaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1\">Saif ul Islam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regression Metric Loss: Learning a Semantic Representation Space for Medical Images. (arXiv:2207.05231v1 [eess.IV])","link":"http://arxiv.org/abs/2207.05231","description":"<p>Regression plays an essential role in many medical imaging applications for\nestimating various clinical risk or measurement scores. While training\nstrategies and loss functions have been studied for the deep neural networks in\nmedical image classification tasks, options for regression tasks are very\nlimited. One of the key challenges is that the high-dimensional feature\nrepresentation learned by existing popular loss functions like Mean Squared\nError or L1 loss is hard to interpret. In this paper, we propose a novel\nRegression Metric Loss (RM-Loss), which endows the representation space with\nthe semantic meaning of the label space by finding a representation manifold\nthat is isometric to the label space. Experiments on two regression tasks, i.e.\ncoronary artery calcium score estimation and bone age assessment, show that\nRM-Loss is superior to the existing popular regression losses on both\nperformance and interpretability. Code is available at\nhttps://github.com/DIAL-RPI/Regression-Metric-Loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chao_H/0/1/0/all/0/1\">Hanqing Chao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajin Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_P/0/1/0/all/0/1\">Pingkun Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Human Vision Inspired Action Recognition using Adaptive Spatiotemporal Sampling. (arXiv:2207.05249v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05249","description":"<p>Adaptive sampling that exploits the spatiotemporal redundancy in videos is\ncritical for always-on action recognition on wearable devices with limited\ncomputing and battery resources. The commonly used fixed sampling strategy is\nnot context-aware and may under-sample the visual content, and thus adversely\nimpacts both computation efficiency and accuracy. Inspired by the concepts of\nfoveal vision and pre-attentive processing from the human visual perception\nmechanism, we introduce a novel adaptive spatiotemporal sampling scheme for\nefficient action recognition. Our system pre-scans the global scene context at\nlow-resolution and decides to skip or request high-resolution features at\nsalient regions for further processing. We validate the system on EPIC-KITCHENS\nand UCF-101 datasets for action recognition, and show that our proposed\napproach can greatly speed up inference with a tolerable loss of accuracy\ncompared with those from state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mac_K/0/1/0/all/0/1\">Khoi-Nguyen C. Mac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1\">Minh N. Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_M/0/1/0/all/0/1\">Minh P. Vo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Proposals for Efficient Object Detection. (arXiv:2207.05252v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05252","description":"<p>Object detection is a basic computer vision task to loccalize and categorize\nobjects in a given image. Most state-of-the-art detection methods utilize a\nfixed number of proposals as an intermediate representation of object\ncandidates, which is unable to adapt to different computational constraints\nduring inference. In this paper, we propose a simple yet effective method which\nis adaptive to different computational resources by generating dynamic\nproposals for object detection. We first design a module to make a single\nquery-based model to be able to inference with different numbers of proposals.\nFurther, we extend it to a dynamic model to choose the number of proposals\naccording to the input image, greatly reducing computational costs. Our method\nachieves significant speed-up across a wide range of detection models including\ntwo-stage and query-based models while obtaining similar or even better\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linjie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Ding Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hunting Group Clues with Transformers for Social Group Activity Recognition. (arXiv:2207.05254v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05254","description":"<p>This paper presents a novel framework for social group activity recognition.\nAs an expanded task of group activity recognition, social group activity\nrecognition requires recognizing multiple sub-group activities and identifying\ngroup members. Most existing methods tackle both tasks by refining region\nfeatures and then summarizing them into activity features. Such heuristic\nfeature design renders the effectiveness of features susceptible to incomplete\nperson localization and disregards the importance of scene contexts.\nFurthermore, region features are sub-optimal to identify group members because\nthe features may be dominated by those of people in the regions and have\ndifferent semantics. To overcome these drawbacks, we propose to leverage\nattention modules in transformers to generate effective social group features.\nOur method is designed in such a way that the attention modules identify and\nthen aggregate features relevant to social group activities, generating an\neffective feature for each social group. Group member information is embedded\ninto the features and thus accessed by feed-forward networks. The outputs of\nfeed-forward networks represent groups so concisely that group members can be\nidentified with simple Hungarian matching between groups and individuals.\nExperimental results show that our method outperforms state-of-the-art methods\non the Volleyball and Collective Activity datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tamura_M/0/1/0/all/0/1\">Masato Tamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishwakarma_R/0/1/0/all/0/1\">Rahul Vishwakarma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vennelakanti_R/0/1/0/all/0/1\">Ravigopal Vennelakanti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Normalized Feature Distillation for Semantic Segmentation. (arXiv:2207.05256v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05256","description":"<p>As a promising approach in model compression, knowledge distillation improves\nthe performance of a compact model by transferring the knowledge from a\ncumbersome one. The kind of knowledge used to guide the training of the student\nis important. Previous distillation methods in semantic segmentation strive to\nextract various forms of knowledge from the features, which involve elaborate\nmanual design relying on prior information and have limited performance gains.\nIn this paper, we propose a simple yet effective feature distillation method\ncalled normalized feature distillation (NFD), aiming to enable effective\ndistillation with the original features without the need to manually design new\nforms of knowledge. The key idea is to prevent the student from focusing on\nimitating the magnitude of the teacher's feature response by normalization. Our\nmethod achieves state-of-the-art distillation results for semantic segmentation\non Cityscapes, VOC 2012, and ADE20K datasets. Code will be available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chenshu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Certifiable Estimation with Preconditioned Eigensolvers. (arXiv:2207.05257v1 [cs.RO])","link":"http://arxiv.org/abs/2207.05257","description":"<p>Convex (specifically semidefinite) relaxation provides a powerful approach to\nconstructing robust machine perception systems, enabling the recovery of\ncertifiably globally optimal solutions of challenging estimation problems in\nmany practical settings. However, solving the large-scale semidefinite\nrelaxations underpinning this approach remains a formidable computational\nchallenge. A dominant cost in many state-of-the-art (Burer-Monteiro\nfactorization-based) certifiable estimation methods is solution verification\n(testing the global optimality of a given candidate solution), which entails\ncomputing a minimum eigenpair of a certain symmetric certificate matrix. In\nthis paper, we show how to significantly accelerate this verification step, and\nthereby the overall speed of certifiable estimation methods. First, we show\nthat the certificate matrices arising in the Burer-Monteiro approach\ngenerically possess spectra that make the verification problem expensive to\nsolve using standard iterative eigenvalue methods. We then show how to address\nthis challenge using preconditioned eigensolvers; specifically, we design a\nspecialized solution verification algorithm based upon the locally optimal\nblock preconditioned conjugate gradient (LOBPCG) method together with a simple\nyet highly effective algebraic preconditioner. Experimental evaluation on a\nvariety of simulated and real-world examples shows that our proposed\nverification scheme is very effective in practice, accelerating solution\nverification by up to 280x, and the overall Burer-Monteiro method by up to 16x,\nversus the standard Lanczos method when applied to relaxations derived from\nlarge-scale SLAM benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosen_D/0/1/0/all/0/1\">David M. Rosen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Architecture Knowledge Distillation. (arXiv:2207.05273v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05273","description":"<p>Transformer attracts much attention because of its ability to learn global\nrelations and superior performance. In order to achieve higher performance, it\nis natural to distill complementary knowledge from Transformer to convolutional\nneural network (CNN). However, most existing knowledge distillation methods\nonly consider homologous-architecture distillation, such as distilling\nknowledge from CNN to CNN. They may not be suitable when applying to\ncross-architecture scenarios, such as from Transformer to CNN. To deal with\nthis problem, a novel cross-architecture knowledge distillation method is\nproposed. Specifically, instead of directly mimicking output/intermediate\nfeatures of the teacher, a partially cross attention projector and a group-wise\nlinear projector are introduced to align the student features with the\nteacher's in two projected feature spaces. And a multi-view robust training\nscheme is further presented to improve the robustness and stability of the\nframework. Extensive experiments show that the proposed method outperforms 14\nstate-of-the-arts on both small-scale and large-scale datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yufan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiajiong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jingting Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Photonic Reconfigurable Accelerators for Efficient Inference of CNNs with Mixed-Sized Tensors. (arXiv:2207.05278v1 [cs.AR])","link":"http://arxiv.org/abs/2207.05278","description":"<p>Photonic Microring Resonator (MRR) based hardware accelerators have been\nshown to provide disruptive speedup and energy-efficiency improvements for\nprocessing deep Convolutional Neural Networks (CNNs). However, previous\nMRR-based CNN accelerators fail to provide efficient adaptability for CNNs with\nmixed-sized tensors. One example of such CNNs is depthwise separable CNNs.\nPerforming inferences of CNNs with mixed-sized tensors on such inflexible\naccelerators often leads to low hardware utilization, which diminishes the\nachievable performance and energy efficiency from the accelerators. In this\npaper, we present a novel way of introducing reconfigurability in the MRR-based\nCNN accelerators, to enable dynamic maximization of the size compatibility\nbetween the accelerator hardware components and the CNN tensors that are\nprocessed using the hardware components. We classify the state-of-the-art\nMRR-based CNN accelerators from prior works into two categories, based on the\nlayout and relative placements of the utilized hardware components in the\naccelerators. We then use our method to introduce reconfigurability in\naccelerators from these two classes, to consequently improve their parallelism,\nthe flexibility of efficiently mapping tensors of different sizes, speed, and\noverall energy efficiency. We evaluate our reconfigurable accelerators against\nthree prior works for the area proportionate outlook (equal hardware area for\nall accelerators). Our evaluation for the inference of four modern CNNs\nindicates that our designed reconfigurable CNN accelerators provide\nimprovements of up to 1.8x in Frames-Per-Second (FPS) and up to 1.5x in FPS/W,\ncompared to an MRR-based accelerator from prior work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vatsavai_S/0/1/0/all/0/1\">Sairam Sri Vatsavai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_I/0/1/0/all/0/1\">Ishan G Thakkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PseudoClick: Interactive Image Segmentation with Click Imitation. (arXiv:2207.05282v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05282","description":"<p>The goal of click-based interactive image segmentation is to obtain precise\nobject segmentation masks with limited user interaction, i.e., by a minimal\nnumber of user clicks. Existing methods require users to provide all the\nclicks: by first inspecting the segmentation mask and then providing points on\nmislabeled regions, iteratively. We ask the question: can our model directly\npredict where to click, so as to further reduce the user interaction cost? To\nthis end, we propose {\\PseudoClick}, a generic framework that enables existing\nsegmentation networks to propose candidate next clicks. These automatically\ngenerated clicks, termed pseudo clicks in this work, serve as an imitation of\nhuman clicks to refine the segmentation mask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Meng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Planche_B/0/1/0/all/0/1\">Benjamin Planche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1\">Srikrishna Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niethammer_M/0/1/0/all/0/1\">Marc Niethammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Inlier and Outlier Specification for Improved Out-of-Distribution Detection. (arXiv:2207.05286v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05286","description":"<p>Accurately detecting out-of-distribution (OOD) data with varying levels of\nsemantic and covariate shifts with respect to the in-distribution (ID) data is\ncritical for deployment of safe and reliable models. This is particularly the\ncase when dealing with highly consequential applications (e.g. medical imaging,\nself-driving cars, etc). The goal is to design a detector that can accept\nmeaningful variations of the ID data, while also rejecting examples from OOD\nregimes. In practice, this dual objective can be realized by enforcing\nconsistency using an appropriate scoring function (e.g., energy) and\ncalibrating the detector to reject a curated set of OOD data (referred to as\noutlier exposure or shortly OE). While OE methods are widely adopted,\nassembling representative OOD datasets is both costly and challenging due to\nthe unpredictability of real-world scenarios, hence the recent trend of\ndesigning OE-free detectors. In this paper, we make a surprising finding that\ncontrolled generalization to ID variations and exposure to diverse (synthetic)\noutlier examples are essential to simultaneously improving semantic and\nmodality shift detection. In contrast to existing methods, our approach samples\ninliers in the latent space, and constructs outlier examples via negative data\naugmentation. Through a rigorous empirical study on medical imaging benchmarks\n(MedMNIST, ISIC2019 and NCT), we demonstrate significant performance gains\n($15\\% - 35\\%$ in AUROC) over existing OE-free, OOD detection approaches under\nboth semantic and modality shifts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Narayanaswamy_V/0/1/0/all/0/1\">Vivek Narayanaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mubarka_Y/0/1/0/all/0/1\">Yamen Mubarka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anirudh_R/0/1/0/all/0/1\">Rushil Anirudh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_D/0/1/0/all/0/1\">Deepta Rajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spanias_A/0/1/0/all/0/1\">Andreas Spanias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1\">Jayaraman J. Thiagarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaAge: Meta-Learning Personalized Age Estimators. (arXiv:2207.05288v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05288","description":"<p>Different people age in different ways. Learning a personalized age estimator\nfor each person is a promising direction for age estimation given that it\nbetter models the personalization of aging processes. However, most existing\npersonalized methods suffer from the lack of large-scale datasets due to the\nhigh-level requirements: identity labels and enough samples for each person to\nform a long-term aging pattern. In this paper, we aim to learn personalized age\nestimators without the above requirements and propose a meta-learning method\nnamed MetaAge for age estimation. Unlike most existing personalized methods\nthat learn the parameters of a personalized estimator for each person in the\ntraining set, our method learns the mapping from identity information to age\nestimator parameters. Specifically, we introduce a personalized estimator\nmeta-learner, which takes identity features as the input and outputs the\nparameters of customized estimators. In this way, our method learns the meta\nknowledge without the above requirements and seamlessly transfers the learned\nmeta knowledge to the test set, which enables us to leverage the existing\nlarge-scale age datasets without any additional annotations. Extensive\nexperimental results on three benchmark datasets including MORPH II, ChaLearn\nLAP 2015 and ChaLearn LAP 2016 databases demonstrate that our MetaAge\nsignificantly boosts the performance of existing personalized methods and\noutperforms the state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wanhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wuerkaixi_A/0/1/0/all/0/1\">Abudukelimu Wuerkaixi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianjiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trusted Multi-Scale Classification Framework for Whole Slide Image. (arXiv:2207.05290v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05290","description":"<p>Despite remarkable efforts been made, the classification of gigapixels\nwhole-slide image (WSI) is severely restrained from either the constrained\ncomputing resources for the whole slides, or limited utilizing of the knowledge\nfrom different scales. Moreover, most of the previous attempts lacked of the\nability of uncertainty estimation. Generally, the pathologists often jointly\nanalyze WSI from the different magnifications. If the pathologists are\nuncertain by using single magnification, then they will change the\nmagnification repeatedly to discover various features of the tissues. Motivated\nby the diagnose process of the pathologists, in this paper, we propose a\ntrusted multi-scale classification framework for the WSI. Leveraging the Vision\nTransformer as the backbone for multi branches, our framework can jointly\nclassification modeling, estimating the uncertainty of each magnification of a\nmicroscope and integrate the evidence from different magnification. Moreover,\nto exploit discriminative patches from WSIs and reduce the requirement for\ncomputation resources, we propose a novel patch selection schema using\nattention rollout and non-maximum suppression. To empirically investigate the\neffectiveness of our approach, empirical experiments are conducted on our WSI\nclassification tasks, using two benchmark databases. The obtained results\nsuggest that the trusted framework can significantly improve the WSI\nclassification performance compared with the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Ming Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1\">Nanhui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weiquan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changjian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huaimin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Hard-Positive Query Mining for DETR-based Human-Object Interaction Detection. (arXiv:2207.05293v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05293","description":"<p>Human-Object Interaction (HOI) detection is a core task for high-level image\nunderstanding. Recently, Detection Transformer (DETR)-based HOI detectors have\nbecome popular due to their superior performance and efficient structure.\nHowever, these approaches typically adopt fixed HOI queries for all testing\nimages, which is vulnerable to the location change of objects in one specific\nimage. Accordingly, in this paper, we propose to enhance DETR's robustness by\nmining hard-positive queries, which are forced to make correct predictions\nusing partial visual cues. First, we explicitly compose hard-positive queries\naccording to the ground-truth (GT) position of labeled human-object pairs for\neach training image. Specifically, we shift the GT bounding boxes of each\nlabeled human-object pair so that the shifted boxes cover only a certain\nportion of the GT ones. We encode the coordinates of the shifted boxes for each\nlabeled human-object pair into an HOI query. Second, we implicitly construct\nanother set of hard-positive queries by masking the top scores in\ncross-attention maps of the decoder layers. The masked attention maps then only\ncover partial important cues for HOI predictions. Finally, an alternate\nstrategy is proposed that efficiently combines both types of hard queries. In\neach iteration, both DETR's learnable queries and one selected type of\nhard-positive queries are adopted for loss computation. Experimental results\nshow that our proposed approach can be widely applied to existing DETR-based\nHOI detectors. Moreover, we consistently achieve state-of-the-art performance\non three benchmarks: HICO-DET, V-COCO, and HOI-A. Code is available at\nhttps://github.com/MuchHair/HQM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1\">Xubin Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zijian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaoli Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SD-GAN: Semantic Decomposition for Face Image Synthesis with Discrete Attribute. (arXiv:2207.05300v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05300","description":"<p>Manipulating latent code in generative adversarial networks (GANs) for facial\nimage synthesis mainly focuses on continuous attribute synthesis (e.g., age,\npose and emotion), while discrete attribute synthesis (like face mask and\neyeglasses) receives less attention. Directly applying existing works to facial\ndiscrete attributes may cause inaccurate results. In this work, we propose an\ninnovative framework to tackle challenging facial discrete attribute synthesis\nvia semantic decomposing, dubbed SD-GAN. To be concrete, we explicitly\ndecompose the discrete attribute representation into two components, i.e. the\nsemantic prior basis and offset latent representation. The semantic prior basis\nshows an initializing direction for manipulating face representation in the\nlatent space. The offset latent presentation obtained by 3D-aware semantic\nfusion network is proposed to adjust prior basis. In addition, the fusion\nnetwork integrates 3D embedding for better identity preservation and discrete\nattribute synthesis. The combination of prior basis and offset latent\nrepresentation enable our method to synthesize photo-realistic face images with\ndiscrete attributes. Notably, we construct a large and valuable dataset MEGN\n(Face Mask and Eyeglasses images crawled from Google and Naver) for completing\nthe lack of discrete attributes in the existing dataset. Extensive qualitative\nand quantitative experiments demonstrate the state-of-the-art performance of\nour method. Our code is available at: https://github.com/MontaEllis/SD-GAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kangneng_Z/0/1/0/all/0/1\">Zhou Kangneng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiaobin_Z/0/1/0/all/0/1\">Zhu Xiaobin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daiheng_G/0/1/0/all/0/1\">Gao Daiheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kai_L/0/1/0/all/0/1\">Lee Kai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xinjie_L/0/1/0/all/0/1\">Li Xinjie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Cheng_Y/0/1/0/all/0/1\">Yin Xu-Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Deep Supervision. (arXiv:2207.05306v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05306","description":"<p>The success of deep learning is usually accompanied by the growth in neural\nnetwork depth. However, the traditional training method only supervises the\nneural network at its last layer and propagates the supervision layer-by-layer,\nwhich leads to hardship in optimizing the intermediate layers. Recently, deep\nsupervision has been proposed to add auxiliary classifiers to the intermediate\nlayers of deep neural networks. By optimizing these auxiliary classifiers with\nthe supervised task loss, the supervision can be applied to the shallow layers\ndirectly. However, deep supervision conflicts with the well-known observation\nthat the shallow layers learn low-level features instead of task-biased\nhigh-level semantic features. To address this issue, this paper proposes a\nnovel training framework named Contrastive Deep Supervision, which supervises\nthe intermediate layers with augmentation-based contrastive learning.\nExperimental results on nine popular datasets with eleven models demonstrate\nits effects on general image classification, fine-grained image classification\nand object detection in supervised learning, semi-supervised learning and\nknowledge distillation. Codes have been released in Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Runpei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaisheng Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Outpainting by Queries. (arXiv:2207.05312v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05312","description":"<p>Image outpainting, which is well studied with Convolution Neural Network\n(CNN) based framework, has recently drawn more attention in computer vision.\nHowever, CNNs rely on inherent inductive biases to achieve effective sample\nlearning, which may degrade the performance ceiling. In this paper, motivated\nby the flexible self-attention mechanism with minimal inductive biases in\ntransformer architecture, we reframe the generalised image outpainting problem\nas a patch-wise sequence-to-sequence autoregression problem, enabling\nquery-based image outpainting. Specifically, we propose a novel hybrid\nvision-transformer-based encoder-decoder framework, named \\textbf{Query}\n\\textbf{O}utpainting \\textbf{TR}ansformer (\\textbf{QueryOTR}), for\nextrapolating visual context all-side around a given image. Patch-wise mode's\nglobal modeling capacity allows us to extrapolate images from the attention\nmechanism's query standpoint. A novel Query Expansion Module (QEM) is designed\nto integrate information from the predicted queries based on the encoder's\noutput, hence accelerating the convergence of the pure transformer even with a\nrelatively small dataset. To further enhance connectivity between each patch,\nthe proposed Patch Smoothing Module (PSM) re-allocates and averages the\noverlapped regions, thus providing seamless predicted images. We experimentally\nshow that QueryOTR could generate visually appealing results smoothly and\nrealistically against the state-of-the-art image outpainting approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1\">Kai Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Penglei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jie Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CANF-VC: Conditional Augmented Normalizing Flows for Video Compression. (arXiv:2207.05315v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05315","description":"<p>This paper presents an end-to-end learning-based video compression system,\ntermed CANF-VC, based on conditional augmented normalizing flows (ANF). Most\nlearned video compression systems adopt the same hybrid-based coding\narchitecture as the traditional codecs. Recent research on conditional coding\nhas shown the sub-optimality of the hybrid-based coding and opens up\nopportunities for deep generative models to take a key role in creating new\ncoding frameworks. CANF-VC represents a new attempt that leverages the\nconditional ANF to learn a video generative model for conditional inter-frame\ncoding. We choose ANF because it is a special type of generative model, which\nincludes variational autoencoder as a special case and is able to achieve\nbetter expressiveness. CANF-VC also extends the idea of conditional coding to\nmotion coding, forming a purely conditional coding framework. Extensive\nexperimental results on commonly used datasets confirm the superiority of\nCANF-VC to the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ho_Y/0/1/0/all/0/1\">Yung-Han Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chih-Peng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gnutti_A/0/1/0/all/0/1\">Alessandro Gnutti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wen-Hsiao Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Twin identification over viewpoint change: A deep convolutional neural network surpasses humans. (arXiv:2207.05316v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05316","description":"<p>Deep convolutional neural networks (DCNNs) have achieved human-level accuracy\nin face identification (Phillips et al., 2018), though it is unclear how\naccurately they discriminate highly-similar faces. Here, humans and a DCNN\nperformed a challenging face-identity matching task that included identical\ntwins. Participants (N=87) viewed pairs of face images of three types:\nsame-identity, general imposter pairs (different identities from similar\ndemographic groups), and twin imposter pairs (identical twin siblings). The\ntask was to determine whether the pairs showed the same person or different\npeople. Identity comparisons were tested in three viewpoint-disparity\nconditions: frontal to frontal, frontal to 45-degree profile, and frontal to\n90-degree profile. Accuracy for discriminating matched-identity pairs from\ntwin-imposters and general imposters was assessed in each viewpoint-disparity\ncondition. Humans were more accurate for general-imposter pairs than\ntwin-imposter pairs, and accuracy declined with increased viewpoint disparity\nbetween the images in a pair. A DCNN trained for face identification (Ranjan et\nal., 2018) was tested on the same image pairs presented to humans. Machine\nperformance mirrored the pattern of human accuracy, but with performance at or\nabove all humans in all but one condition. Human and machine similarity scores\nwere compared across all image-pair types. This item-level analysis showed that\nhuman and machine similarity ratings correlated significantly in six of nine\nimage-pair types [range r=0.38 to r=0.63], suggesting general accord between\nthe perception of face similarity by humans and the DCNN. These findings also\ncontribute to our understanding of DCNN performance for discriminating\nhigh-resemblance faces, demonstrate that the DCNN performs at a level at or\nabove humans, and suggest a degree of parity between the features used by\nhumans and the DCNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parde_C/0/1/0/all/0/1\">Connor J. Parde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strehle_V/0/1/0/all/0/1\">Virginia E. Strehle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_V/0/1/0/all/0/1\">Vivekjyoti Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Ying Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavazos_J/0/1/0/all/0/1\">Jacqueline G. Cavazos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castillo_C/0/1/0/all/0/1\">Carlos D. Castillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OToole_A/0/1/0/all/0/1\">Alice J. O&#x27;Toole</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPO: Change Robust Panorama to Point Cloud Localization. (arXiv:2207.05317v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05317","description":"<p>We present CPO, a fast and robust algorithm that localizes a 2D panorama with\nrespect to a 3D point cloud of a scene possibly containing changes. To robustly\nhandle scene changes, our approach deviates from conventional feature point\nmatching, and focuses on the spatial context provided from panorama images.\nSpecifically, we propose efficient color histogram generation and subsequent\nrobust localization using score maps. By utilizing the unique equivariance of\nspherical projections, we propose very fast color histogram generation for a\nlarge number of camera poses without explicitly rendering images for all\ncandidate poses. We accumulate the regional consistency of the panorama and\npoint cloud as 2D/3D score maps, and use them to weigh the input color values\nto further increase robustness. The weighted color distribution quickly finds\ngood initial poses and achieves stable convergence for gradient-based\noptimization. CPO is lightweight and achieves effective localization in all\ntested scenarios, showing stable performance despite scene changes, repetitive\nstructures, or featureless regions, which are typical challenges for visual\nlocalization with perspective cameras.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1\">Hojun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1\">Changwoon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Min Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Certified Adversarial Robustness via Anisotropic Randomized Smoothing. (arXiv:2207.05327v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05327","description":"<p>Randomized smoothing has achieved great success for certified robustness\nagainst adversarial perturbations. Given any arbitrary classifier, randomized\nsmoothing can guarantee the classifier's prediction over the perturbed input\nwith provable robustness bound by injecting noise into the classifier. However,\nall of the existing methods rely on fixed i.i.d. probability distribution to\ngenerate noise for all dimensions of the data (e.g., all the pixels in an\nimage), which ignores the heterogeneity of inputs and data dimensions. Thus,\nexisting randomized smoothing methods cannot provide optimal protection for all\nthe inputs. To address this limitation, we propose the first anisotropic\nrandomized smoothing method which ensures provable robustness guarantee based\non pixel-wise noise distributions. Also, we design a novel CNN-based noise\ngenerator to efficiently fine-tune the pixel-wise noise distributions for all\nthe pixels in each input. Experimental results demonstrate that our method\nsignificantly outperforms the state-of-the-art randomized smoothing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1\">Hanbin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yuan Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robotic Detection of a Human-Comprehensible Gestural Language for Underwater Multi-Human-Robot Collaboration. (arXiv:2207.05331v1 [cs.RO])","link":"http://arxiv.org/abs/2207.05331","description":"<p>In this paper, we present a motion-based robotic communication framework that\nenables non-verbal communication among autonomous underwater vehicles (AUVs)\nand human divers. We design a gestural language for AUV-to-AUV communication\nwhich can be easily understood by divers observing the conversation unlike\ntypical radio frequency, light, or audio based AUV communication. To allow AUVs\nto visually understand a gesture from another AUV, we propose a deep network\n(RRCommNet) which exploits a self-attention mechanism to learn to recognize\neach message by extracting maximally discriminative spatio-temporal features.\nWe train this network on diverse simulated and real-world data. Our\nexperimental evaluations, both in simulation and in closed-water robot trials,\ndemonstrate that the proposed RRCommNet architecture is able to decipher\ngesture-based messages with an average accuracy of 88-94% on simulated data,\n73-83% on real data (depending on the version of the model used). Further, by\nperforming a message transcription study with human participants, we also show\nthat the proposed language can be understood by humans, with an overall\ntranscription accuracy of 88%. Finally, we discuss the inference runtime of\nRRCommNet on embedded GPU hardware, for real-time use on board AUVs in the\nfield.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Enan_S/0/1/0/all/0/1\">Sadman Sakib Enan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fulton_M/0/1/0/all/0/1\">Michael Fulton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattar_J/0/1/0/all/0/1\">Junaed Sattar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IDEA: Increasing Text Diversity via Online Multi-Label Recognition for Vision-Language Pre-training. (arXiv:2207.05333v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05333","description":"<p>Vision-Language Pre-training (VLP) with large-scale image-text pairs has\ndemonstrated superior performance in various fields. However, the image-text\npairs co-occurrent on the Internet typically lack explicit alignment\ninformation, which is suboptimal for VLP. Existing methods proposed to adopt an\noff-the-shelf object detector to utilize additional image tag information.\nHowever, the object detector is time-consuming and can only identify the\npre-defined object categories, limiting the model capacity. Inspired by the\nobservation that the texts incorporate incomplete fine-grained image\ninformation, we introduce IDEA, which stands for increasing text diversity via\nonline multi-label recognition for VLP. IDEA shows that multi-label learning\nwith image tags extracted from the texts can be jointly optimized during VLP.\nMoreover, IDEA can identify valuable image tags online to provide more explicit\ntextual supervision. Comprehensive experiments demonstrate that IDEA can\nsignificantly boost the performance on multiple downstream datasets with a\nsmall extra computational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Youcai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Ying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_W/0/1/0/all/0/1\">Weiwei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruiwei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Rui Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuejie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaobo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cycle Self-Training for Semi-Supervised Object Detection with Distribution Consistency Reweighting. (arXiv:2207.05334v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05334","description":"<p>Recently, many semi-supervised object detection (SSOD) methods adopt\nteacher-student framework and have achieved state-of-the-art results. However,\nthe teacher network is tightly coupled with the student network since the\nteacher is an exponential moving average (EMA) of the student, which causes a\nperformance bottleneck. To address the coupling problem, we propose a Cycle\nSelf-Training (CST) framework for SSOD, which consists of two teachers T1 and\nT2, two students S1 and S2. Based on these networks, a cycle self-training\nmechanism is built, i.e.,\nS1${\\rightarrow}$T1${\\rightarrow}$S2${\\rightarrow}$T2${\\rightarrow}$S1. For\nS${\\rightarrow}$T, we also utilize the EMA weights of the students to update\nthe teachers. For T${\\rightarrow}$S, instead of providing supervision for its\nown student S1(S2) directly, the teacher T1(T2) generates pseudo-labels for the\nstudent S2(S1), which looses the coupling effect. Moreover, owing to the\nproperty of EMA, the teacher is most likely to accumulate the biases from the\nstudent and make the mistakes irreversible. To mitigate the problem, we also\npropose a distribution consistency reweighting strategy, where pseudo-labels\nare reweighted based on distribution consistency across the teachers T1 and T2.\nWith the strategy, the two students S2 and S1 can be trained robustly with\nnoisy pseudo labels to avoid confirmation biases. Extensive experiments prove\nthe superiority of CST by consistently improving the AP over the baseline and\noutperforming state-of-the-art methods by 2.1% absolute AP improvements with\nscarce labeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chunpeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_F/0/1/0/all/0/1\">Feng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Contrastive Learning for Spatio-temporal Representation. (arXiv:2207.05340v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05340","description":"<p>Contrastive learning has shown promising potential in self-supervised\nspatio-temporal representation learning. Most works naively sample different\nclips to construct positive and negative pairs. However, we observe that this\nformulation inclines the model towards the background scene bias. The\nunderlying reasons are twofold. First, the scene difference is usually more\nnoticeable and easier to discriminate than the motion difference. Second, the\nclips sampled from the same video often share similar backgrounds but have\ndistinct motions. Simply regarding them as positive pairs will draw the model\nto the static background rather than the motion pattern. To tackle this\nchallenge, this paper presents a novel dual contrastive formulation.\nConcretely, we decouple the input RGB video sequence into two complementary\nmodes, static scene and dynamic motion. Then, the original RGB features are\npulled closer to the static features and the aligned dynamic features,\nrespectively. In this way, the static scene and the dynamic motion are\nsimultaneously encoded into the compact RGB representation. We further conduct\nthe feature space decoupling via activation maps to distill static- and\ndynamic-related features. We term our method as \\textbf{D}ual\n\\textbf{C}ontrastive \\textbf{L}earning for spatio-temporal\n\\textbf{R}epresentation (DCLR). Extensive experiments demonstrate that DCLR\nlearns effective spatio-temporal representations and obtains state-of-the-art\nor comparable performance on UCF-101, HMDB-51, and Diving-48 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shuangrui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hongkai Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Graph Transformer for Video Question Answering. (arXiv:2207.05342v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05342","description":"<p>This paper proposes a Video Graph Transformer (VGT) model for Video Quetion\nAnswering (VideoQA). VGT's uniqueness are two-fold: 1) it designs a dynamic\ngraph transformer module which encodes video by explicitly capturing the visual\nobjects, their relations, and dynamics for complex spatio-temporal reasoning;\nand 2) it exploits disentangled video and text Transformers for relevance\ncomparison between the video and text to perform QA, instead of entangled\ncross-modal Transformer for answer classification. Vision-text communication is\ndone by additional cross-modal interaction modules. With more reasonable video\nencoding and QA solution, we show that VGT can achieve much better performances\non VideoQA tasks that challenge dynamic relation reasoning than prior arts in\nthe pretraining-free scenario. Its performances even surpass those models that\nare pretrained with millions of external data. We further show that VGT can\nalso benefit a lot from self-supervised cross-modal pretraining, yet with\norders of magnitude smaller data. These results clearly demonstrate the\neffectiveness and superiority of VGT, and reveal its potential for more\ndata-efficient pretraining. With comprehensive analyses and some heuristic\nobservations, we hope that VGT can promote VQA research beyond coarse\nrecognition/description towards fine-grained relation reasoning in realistic\nvideos. Our code is available at https://github.com/sail-sg/VGT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Junbin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HEAD: HEtero-Assists Distillation for Heterogeneous Object Detectors. (arXiv:2207.05345v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05345","description":"<p>Conventional knowledge distillation (KD) methods for object detection mainly\nconcentrate on homogeneous teacher-student detectors. However, the design of a\nlightweight detector for deployment is often significantly different from a\nhigh-capacity detector. Thus, we investigate KD among heterogeneous\nteacher-student pairs for a wide application. We observe that the core\ndifficulty for heterogeneous KD (hetero-KD) is the significant semantic gap\nbetween the backbone features of heterogeneous detectors due to the different\noptimization manners. Conventional homogeneous KD (homo-KD) methods suffer from\nsuch a gap and are hard to directly obtain satisfactory performance for\nhetero-KD. In this paper, we propose the HEtero-Assists Distillation (HEAD)\nframework, leveraging heterogeneous detection heads as assistants to guide the\noptimization of the student detector to reduce this gap. In HEAD, the assistant\nis an additional detection head with the architecture homogeneous to the\nteacher head attached to the student backbone. Thus, a hetero-KD is transformed\ninto a homo-KD, allowing efficient knowledge transfer from the teacher to the\nstudent. Moreover, we extend HEAD into a Teacher-Free HEAD (TF-HEAD) framework\nwhen a well-trained teacher detector is unavailable. Our method has achieved\nsignificant improvement compared to current detection KD methods. For example,\non the MS-COCO dataset, TF-HEAD helps R18 RetinaNet achieve 33.9 mAP (+2.2),\nwhile HEAD further pushes the limit to 36.2 mAP (+4.5).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Luting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaojie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yue Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zeren Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianlong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"eX-ViT: A Novel eXplainable Vision Transformer for Weakly Supervised Semantic Segmentation. (arXiv:2207.05358v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05358","description":"<p>Recently vision transformer models have become prominent models for a range\nof vision tasks. These models, however, are usually opaque with weak feature\ninterpretability. Moreover, there is no method currently built for an\nintrinsically interpretable transformer, which is able to explain its reasoning\nprocess and provide a faithful explanation. To close these crucial gaps, we\npropose a novel vision transformer dubbed the eXplainable Vision Transformer\n(eX-ViT), an intrinsically interpretable transformer model that is able to\njointly discover robust interpretable features and perform the prediction.\nSpecifically, eX-ViT is composed of the Explainable Multi-Head Attention\n(E-MHA) module, the Attribute-guided Explainer (AttE) module and the\nself-supervised attribute-guided loss. The E-MHA tailors explainable attention\nweights that are able to learn semantically interpretable representations from\nlocal patches in terms of model decisions with noise robustness. Meanwhile,\nAttE is proposed to encode discriminative attribute features for the target\nobject through diverse attribute discovery, which constitutes faithful evidence\nfor the model's predictions. In addition, a self-supervised attribute-guided\nloss is developed for our eX-ViT, which aims at learning enhanced\nrepresentations through the attribute discriminability mechanism and attribute\ndiversity mechanism, to localize diverse and discriminative attributes and\ngenerate more robust explanations. As a result, we can uncover faithful and\nrobust interpretations with diverse attributes through the proposed eX-ViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wei Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Juan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Ping Phoebe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_L/0/1/0/all/0/1\">Lianhua Chi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CP3: Unifying Point Cloud Completion by Pretrain-Prompt-Predict Paradigm. (arXiv:2207.05359v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05359","description":"<p>Point cloud completion aims to predict complete shape from its partial\nobservation. Current approaches mainly consist of generation and refinement\nstages in a coarse-to-fine style. However, the generation stage often lacks\nrobustness to tackle different incomplete variations, while the refinement\nstage blindly recovers point clouds without the semantic awareness. To tackle\nthese challenges, we unify point cloud Completion by a generic\nPretrain-Prompt-Predict paradigm, namely CP3. Inspired by prompting approaches\nfrom NLP, we creatively reinterpret point cloud generation and refinement as\nthe prompting and predicting stages, respectively. Then, we introduce a concise\nself-supervised pretraining stage before prompting. It can effectively increase\nrobustness of point cloud generation, by an Incompletion-Of-Incompletion (IOI)\npretext task. Moreover, we develop a novel Semantic Conditional Refinement\n(SCR) network at the predicting stage. It can discriminatively modulate\nmulti-scale refinement with the guidance of semantics. Finally, extensive\nexperiments demonstrate that our CP3 outperforms the state-of-the-art methods\nwith a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingye Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image and Model Transformation with Secret Key for Vision Transformer. (arXiv:2207.05366v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05366","description":"<p>In this paper, we propose a combined use of transformed images and vision\ntransformer (ViT) models transformed with a secret key. We show for the first\ntime that models trained with plain images can be directly transformed to\nmodels trained with encrypted images on the basis of the ViT architecture, and\nthe performance of the transformed models is the same as models trained with\nplain images when using test images encrypted with the key. In addition, the\nproposed scheme does not require any specially prepared data for training\nmodels or network modification, so it also allows us to easily update the\nsecret key. In an experiment, the effectiveness of the proposed scheme is\nevaluated in terms of performance degradation and model protection performance\nin an image classification task on the CIFAR-10 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iijima_R/0/1/0/all/0/1\">Ryota Iijima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aprilpyone_M/0/1/0/all/0/1\">MaungMaung Aprilpyone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinoshita_Y/0/1/0/all/0/1\">Yuma Kinoshita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking gradient weights' influence over saliency map estimation. (arXiv:2207.05374v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05374","description":"<p>Class activation map (CAM) helps to formulate saliency maps that aid in\ninterpreting the deep neural network's prediction. Gradient-based methods are\ngenerally faster than other branches of vision interpretability and independent\nof human guidance. The performance of CAM-like studies depends on the governing\nmodel's layer response, and the influences of the gradients. Typical\ngradient-oriented CAM studies rely on weighted aggregation for saliency map\nestimation by projecting the gradient maps into single weight values, which may\nlead to over generalized saliency map. To address this issue, we use a global\nguidance map to rectify the weighted aggregation operation during saliency\nestimation, where resultant interpretations are comparatively clean er and\ninstance-specific. We obtain the global guidance map by performing elementwise\nmultiplication between the feature maps and their corresponding gradient maps.\nTo validate our study, we compare the proposed study with eight different\nsaliency visualizers. In addition, we use seven commonly used evaluation\nmetrics for quantitative comparison. The proposed scheme achieves significant\nimprovement over the test images from the ImageNet, MS-COCO 14, and PASCAL VOC\n2012 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fahim_M/0/1/0/all/0/1\">Masud An Nur Islam Fahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saqib_N/0/1/0/all/0/1\">Nazmus Saqib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siam_S/0/1/0/all/0/1\">Shafkat Khan Siam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Ho Yub Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Occluded Human Body Capture with Self-Supervised Spatial-Temporal Motion Prior. (arXiv:2207.05375v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05375","description":"<p>Although significant progress has been achieved on monocular maker-less human\nmotion capture in recent years, it is still hard for state-of-the-art methods\nto obtain satisfactory results in occlusion scenarios. There are two main\nreasons: the one is that the occluded motion capture is inherently ambiguous as\nvarious 3D poses can map to the same 2D observations, which always results in\nan unreliable estimation. The other is that no sufficient occluded human data\ncan be used for training a robust model. To address the obstacles, our key-idea\nis to employ non-occluded human data to learn a joint-level spatial-temporal\nmotion prior for occluded human with a self-supervised strategy. To further\nreduce the gap between synthetic and real occlusion data, we build the first 3D\noccluded motion dataset~(OcMotion), which can be used for both training and\ntesting. We encode the motions in 2D maps and synthesize occlusions on\nnon-occluded data for the self-supervised training. A spatial-temporal layer is\nthen designed to learn joint-level correlations. The learned prior reduces the\nambiguities of occlusions and is robust to diverse occlusion types, which is\nthen adopted to assist the occluded human motion capture. Experimental results\nshow that our method can generate accurate and coherent human motions from\noccluded videos with good generalization ability and runtime efficiency. The\ndataset and code are publicly available at\n\\url{https://github.com/boycehbz/CHOMP}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Buzhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1\">Yuan Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_J/0/1/0/all/0/1\">Jingyi Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yangang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collaborative Neural Rendering using Anime Character Sheets. (arXiv:2207.05378v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05378","description":"<p>Drawing images of characters at desired poses is an essential but laborious\ntask in anime production. In this paper, we present the Collaborative Neural\nRendering~(CoNR) method to create new images from a few arbitrarily posed\nreference images available in character sheets. In general, the high diversity\nof body shapes of anime characters defies the employment of universal body\nmodels for real-world humans, like SMPL. To overcome this difficulty, CoNR uses\na compact and easy-to-obtain landmark encoding to avoid creating a unified UV\nmapping in the pipeline. In addition, CoNR's performance can be significantly\nincreased when having multiple reference images by using feature space\ncross-view dense correspondence and warping in a specially designed neural\nnetwork construct. Moreover, we collect a character sheet dataset containing\nover 700,000 hand-drawn and synthesized images of diverse poses to facilitate\nresearch in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zuzeng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1\">Ailin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhewei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuchang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frequency Domain Model Augmentation for Adversarial Attack. (arXiv:2207.05382v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05382","description":"<p>For black-box attacks, the gap between the substitute model and the victim\nmodel is usually large, which manifests as a weak attack performance. Motivated\nby the observation that the transferability of adversarial examples can be\nimproved by attacking diverse models simultaneously, model augmentation methods\nwhich simulate different models by using transformed images are proposed.\nHowever, existing transformations for spatial domain do not translate to\nsignificantly diverse augmented models. To tackle this issue, we propose a\nnovel spectrum simulation attack to craft more transferable adversarial\nexamples against both normally trained and defense models. Specifically, we\napply a spectrum transformation to the input and thus perform the model\naugmentation in the frequency domain. We theoretically prove that the\ntransformation derived from frequency domain leads to a diverse spectrum\nsaliency map, an indicator we proposed to reflect the diversity of substitute\nmodels. Notably, our method can be generally combined with existing attacks.\nExtensive experiments on the ImageNet dataset demonstrate the effectiveness of\nour method, \\textit{e.g.}, attacking nine state-of-the-art defense models with\nan average success rate of \\textbf{95.4\\%}. Our code is available in\n\\url{https://github.com/yuyang-long/SSA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yuyang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Boheng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Shadow Generation Using Pixel Height Maps. (arXiv:2207.05385v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05385","description":"<p>Shadows are essential for realistic image compositing. Physics-based shadow\nrendering methods require 3D geometries, which are not always available. Deep\nlearning-based shadow synthesis methods learn a mapping from the light\ninformation to an object's shadow without explicitly modeling the shadow\ngeometry. Still, they lack control and are prone to visual artifacts. We\nintroduce pixel heigh, a novel geometry representation that encodes the\ncorrelations between objects, ground, and camera pose. The pixel height can be\ncalculated from 3D geometries, manually annotated on 2D images, and can also be\npredicted from a single-view RGB image by a supervised approach. It can be used\nto calculate hard shadows in a 2D image based on the projective geometry,\nproviding precise control of the shadows' direction and shape. Furthermore, we\npropose a data-driven soft shadow generator to apply softness to a hard shadow\nbased on a softness input parameter. Qualitative and quantitative evaluations\ndemonstrate that the proposed pixel height significantly improves the quality\nof the shadow generation while allowing for controllability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1\">Yichen Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oztireli_A/0/1/0/all/0/1\">A. Cengiz Oztireli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">He Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benes_B/0/1/0/all/0/1\">Bedrich Benes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wound Segmentation with Dynamic Illumination Correction and Dual-view Semantic Fusion. (arXiv:2207.05388v1 [eess.IV])","link":"http://arxiv.org/abs/2207.05388","description":"<p>Wound image segmentation is a critical component for the clinical diagnosis\nand in-time treatment of wounds. Recently, deep learning has become the\nmainstream methodology for wound image segmentation. However, the\npre-processing of the wound image, such as the illumination correction, is\nrequired before the training phase as the performance can be greatly improved.\nThe correction procedure and the training of deep models are independent of\neach other, which leads to sub-optimal segmentation performance as the fixed\nillumination correction may not be suitable for all images. To address\naforementioned issues, an end-to-end dual-view segmentation approach was\nproposed in this paper, by incorporating a learn-able illumination correction\nmodule into the deep segmentation models. The parameters of the module can be\nlearned and updated during the training stage automatically, while the\ndual-view fusion can fully employ the features from both the raw images and the\nenhanced ones. To demonstrate the effectiveness and robustness of the proposed\nframework, the extensive experiments are conducted on the benchmark datasets.\nThe encouraging results suggest that our framework can significantly improve\nthe segmentation performance, compared to the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Honghui Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Changjian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_F/0/1/0/all/0/1\">Fangzhao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_M/0/1/0/all/0/1\">Ming Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_Y/0/1/0/all/0/1\">Yuxing Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_H/0/1/0/all/0/1\">Hongjun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Condensation Distillation. (arXiv:2207.05409v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05409","description":"<p>Knowledge Distillation (KD) transfers the knowledge from a high-capacity\nteacher network to strengthen a smaller student. Existing methods focus on\nexcavating the knowledge hints and transferring the whole knowledge to the\nstudent. However, the knowledge redundancy arises since the knowledge shows\ndifferent values to the student at different learning stages. In this paper, we\npropose Knowledge Condensation Distillation (KCD). Specifically, the knowledge\nvalue on each sample is dynamically estimated, based on which an\nExpectation-Maximization (EM) framework is forged to iteratively condense a\ncompact knowledge set from the teacher to guide the student learning. Our\napproach is easy to build on top of the off-the-shelf KD methods, with no extra\ntraining parameters and negligible computation overhead. Thus, it presents one\nnew perspective for KD, in which the student that actively identifies teacher's\nknowledge in line with its aptitude can learn to learn more effectively and\nefficiently. Experiments on standard benchmarks manifest that the proposed KCD\ncan well boost the performance of student model with even higher distillation\nefficiency. Code is available at https://github.com/dzy3/KCD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhiyuan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yihong Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xinghao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Liujuan Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Baseline for Detecting Out-of-Distribution Examples in Image Captioning. (arXiv:2207.05418v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05418","description":"<p>Image captioning research achieved breakthroughs in recent years by\ndeveloping neural models that can generate diverse and high-quality\ndescriptions for images drawn from the same distribution as training images.\nHowever, when facing out-of-distribution (OOD) images, such as corrupted\nimages, or images containing unknown objects, the models fail in generating\nrelevant captions.\n</p>\n<p>In this paper, we consider the problem of OOD detection in image captioning.\nWe formulate the problem and suggest an evaluation setup for assessing the\nmodel's performance on the task. Then, we analyze and show the effectiveness of\nthe caption's likelihood score at detecting and rejecting OOD images, which\nimplies that the relatedness between the input image and the generated caption\nis encapsulated within the score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shalev_G/0/1/0/all/0/1\">Gabi Shalev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalev_G/0/1/0/all/0/1\">Gal-Lev Shalev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keshet_J/0/1/0/all/0/1\">Joseph Keshet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniNet: Unified Architecture Search with Convolution, Transformer, and MLP. (arXiv:2207.05420v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05420","description":"<p>Recently, transformer and multi-layer perceptron (MLP) architectures have\nachieved impressive results on various vision tasks. However, how to\neffectively combine those operators to form high-performance hybrid visual\narchitectures still remains a challenge. In this work, we study the learnable\ncombination of convolution, transformer, and MLP by proposing a novel unified\narchitecture search approach. Our approach contains two key designs to achieve\nthe search for high-performance networks. First, we model the very different\nsearchable operators in a unified form, and thus enable the operators to be\ncharacterized with the same set of configuration parameters. In this way, the\noverall search space size is significantly reduced, and the total search cost\nbecomes affordable. Second, we propose context-aware downsampling modules\n(DSMs) to mitigate the gap between the different types of operators. Our\nproposed DSMs are able to better adapt features from different types of\noperators, which is important for identifying high-performance hybrid\narchitectures. Finally, we integrate configurable operators and DSMs into a\nunified search space and search with a Reinforcement Learning-based search\nalgorithm to fully explore the optimal combination of the operators. To this\nend, we search a baseline network and scale it up to obtain a family of models,\nnamed UniNets, which achieve much better accuracy and efficiency than previous\nConvNets and Transformers. In particular, our UniNet-B5 achieves 84.9% top-1\naccuracy on ImageNet, outperforming EfficientNet-B7 and BoTNet-T7 with 44% and\n55% fewer FLOPs respectively. By pretraining on the ImageNet-21K, our UniNet-B6\nachieves 87.4%, outperforming Swin-L with 51% fewer FLOPs and 41% fewer\nparameters. Code is available at https://github.com/Sense-X/UniNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Guanglu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Domain Generalization by Learning without Forgetting: Application in Retail Checkout. (arXiv:2207.05422v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05422","description":"<p>Designing an automatic checkout system for retail stores at the human level\naccuracy is challenging due to similar appearance products and their various\nposes. This paper addresses the problem by proposing a method with a two-stage\npipeline. The first stage detects class-agnostic items, and the second one is\ndedicated to classify product categories. We also track the objects across\nvideo frames to avoid duplicated counting. One major challenge is the domain\ngap because the models are trained on synthetic data but tested on the real\nimages. To reduce the error gap, we adopt domain generalization methods for the\nfirst-stage detector. In addition, model ensemble is used to enhance the\nrobustness of the 2nd-stage classifier. The method is evaluated on the AI City\nchallenge 2022 -- Track 4 and gets the F1 score $40\\%$ on the test A set. Code\nis released at the link https://github.com/cybercore-co-ltd/aicity22-track4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thuy C. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_N/0/1/0/all/0/1\">Nam LH. Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1\">Son T. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Diverse Tone Styles for Image Retouching. (arXiv:2207.05430v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05430","description":"<p>Image retouching, aiming to regenerate the visually pleasing renditions of\ngiven images, is a subjective task where the users are with different aesthetic\nsensations. Most existing methods deploy a deterministic model to learn the\nretouching style from a specific expert, making it less flexible to meet\ndiverse subjective preferences. Besides, the intrinsic diversity of an expert\ndue to the targeted processing on different images is also deficiently\ndescribed. To circumvent such issues, we propose to learn diverse image\nretouching with normalizing flow-based architectures. Unlike current flow-based\nmethods which directly generate the output image, we argue that learning in a\nstyle domain could (i) disentangle the retouching styles from the image\ncontent, (ii) lead to a stable style presentation form, and (iii) avoid the\nspatial disharmony effects. For obtaining meaningful image tone style\nrepresentations, a joint-training pipeline is delicately designed, which is\ncomposed of a style encoder, a conditional RetouchNet, and the image tone style\nnormalizing flow (TSFlow) module. In particular, the style encoder predicts the\ntarget style representation of an input image, which serves as the conditional\ninformation in the RetouchNet for retouching, while the TSFlow maps the style\nrepresentation vector into a Gaussian distribution in the forward pass. After\ntraining, the TSFlow can generate diverse image tone style vectors by sampling\nfrom the Gaussian distribution. Extensive experiments on MIT-Adobe FiveK and\nPPR10K datasets show that our proposed method performs favorably against\nstate-of-the-art methods and is effective in generating diverse results to\nsatisfy different human aesthetic preferences. Source code and pre-trained\nmodels are publicly available at https://github.com/SSRHeart/TSFlow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haolin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaohe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synergistic Self-supervised and Quantization Learning. (arXiv:2207.05432v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05432","description":"<p>With the success of self-supervised learning (SSL), it has become a\nmainstream paradigm to fine-tune from self-supervised pretrained models to\nboost the performance on downstream tasks. However, we find that current SSL\nmodels suffer severe accuracy drops when performing low-bit quantization,\nprohibiting their deployment in resource-constrained applications. In this\npaper, we propose a method called synergistic self-supervised and quantization\nlearning (SSQL) to pretrain quantization-friendly self-supervised models\nfacilitating downstream deployment. SSQL contrasts the features of the\nquantized and full precision models in a self-supervised fashion, where the\nbit-width for the quantized model is randomly selected in each step. SSQL not\nonly significantly improves the accuracy when quantized to lower bit-widths,\nbut also boosts the accuracy of full precision models in most cases. By only\ntraining once, SSQL can then benefit various downstream tasks at different\nbit-widths simultaneously. Moreover, the bit-width flexibility is achieved\nwithout additional storage overhead, requiring only one copy of weights during\ntraining and inference. We theoretically analyze the optimization process of\nSSQL, and conduct exhaustive experiments on various benchmarks to further\ndemonstrate the effectiveness of our method. Our code is available at\nhttps://github.com/megvii-research/SSQL-ECCV2022.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yun-Hao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peiqin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yechang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuchang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Category-Level 6D Object Pose and Size Estimation using Self-Supervised Deep Prior Deformation Networks. (arXiv:2207.05444v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05444","description":"<p>It is difficult to precisely annotate object instances and their semantics in\n3D space, and as such, synthetic data are extensively used for these tasks,\ne.g., category-level 6D object pose and size estimation. However, the easy\nannotations in synthetic domains bring the downside effect of synthetic-to-real\n(Sim2Real) domain gap. In this work, we aim to address this issue in the task\nsetting of Sim2Real, unsupervised domain adaptation for category-level 6D\nobject pose and size estimation. We propose a method that is built upon a novel\nDeep Prior Deformation Network, shortened as DPDN. DPDN learns to deform\nfeatures of categorical shape priors to match those of object observations, and\nis thus able to establish deep correspondence in the feature space for direct\nregression of object poses and sizes. To reduce the Sim2Real domain gap, we\nformulate a novel self-supervised objective upon DPDN via consistency learning;\nmore specifically, we apply two rigid transformations to each object\nobservation in parallel, and feed them into DPDN respectively to yield dual\nsets of predictions; on top of the parallel learning, an inter-consistency term\nis employed to keep cross consistency between dual predictions for improving\nthe sensitivity of DPDN to pose changes, while individual intra-consistency\nones are used to enforce self-adaptation within each learning itself. We train\nDPDN on both training sets of the synthetic CAMERA25 and real-world REAL275\ndatasets; our results outperform the existing methods on REAL275 test set under\nboth the unsupervised and supervised settings. Ablation studies also verify the\nefficacy of our designs. Our code is released publicly at\nhttps://github.com/JiehongLin/Self-DPDN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiehong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zewei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Effects of Image Quality Degradation on Minutiae- and Ridge-Based Automatic Fingerprint Recognition. (arXiv:2207.05447v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05447","description":"<p>The effect of image quality degradation on the verification performance of\nautomatic fingerprint recognition is investigated. We study the performance of\ntwo fingerprint matchers based on minutiae and ridge information under varying\nfingerprint image quality. The ridge-based system is found to be more robust to\nimage quality degradation than the minutiae-based system for a number of\ndifferent image quality criteria.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_Aguilar_J/0/1/0/all/0/1\">Julian Fierrez-Aguilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munoz_Serrano_L/0/1/0/all/0/1\">Luis-Miguel Mu&#xf1;oz-Serrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1\">Fernando Alonso-Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1\">Javier Ortega-Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A review of schemes for fingerprint image quality computation. (arXiv:2207.05449v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05449","description":"<p>Fingerprint image quality affects heavily the performance of fingerprint\nrecognition systems. This paper reviews existing approaches for fingerprint\nimage quality computation. We also implement, test and compare a selection of\nthem using the MCYT database including 9000 fingerprint images. Experimental\nresults show that most of the algorithms behave similarly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1\">Fernando Alonso-Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_Aguilar_J/0/1/0/all/0/1\">Julian Fierrez-Aguilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1\">Javier Ortega-Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransFA: Transformer-based Representation for Face Attribute Evaluation. (arXiv:2207.05456v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05456","description":"<p>Face attribute evaluation plays an important role in video surveillance and\nface analysis. Although methods based on convolution neural networks have made\ngreat progress, they inevitably only deal with one local neighborhood with\nconvolutions at a time. Besides, existing methods mostly regard face attribute\nevaluation as the individual multi-label classification task, ignoring the\ninherent relationship between semantic attributes and face identity\ninformation. In this paper, we propose a novel \\textbf{trans}former-based\nrepresentation for \\textbf{f}ace \\textbf{a}ttribute evaluation method\n(\\textbf{TransFA}), which could effectively enhance the attribute\ndiscriminative representation learning in the context of attention mechanism.\nThe multiple branches transformer is employed to explore the inter-correlation\nbetween different attributes in similar semantic regions for attribute feature\nlearning. Specially, the hierarchical identity-constraint attribute loss is\ndesigned to train the end-to-end architecture, which could further integrate\nface identity discriminative information to boost performance. Experimental\nresults on multiple face attribute benchmarks demonstrate that the proposed\nTransFA achieves superior performances compared with state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Decheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Weijie He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Chunlei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the limits of perceptual quality measures for enhanced underwater images. (arXiv:2207.05470v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05470","description":"<p>The appearance of objects in underwater images is degraded by the selective\nattenuation of light, which reduces contrast and causes a colour cast. This\ndegradation depends on the water environment, and increases with depth and with\nthe distance of the object from the camera. Despite an increasing volume of\nworks in underwater image enhancement and restoration, the lack of a commonly\naccepted evaluation measure is hindering the progress as it is difficult to\ncompare methods. In this paper, we review commonly used colour accuracy\nmeasures, such as colour reproduction error and CIEDE2000, and no-reference\nimage quality measures, such as UIQM, UCIQE and CCF, which have not yet been\nsystematically validated. We show that none of the no-reference quality\nmeasures satisfactorily rates the quality of enhanced underwater images and\ndiscuss their main shortcomings. Images and results are available at\nhttps://puiqe.eecs.qmul.ac.uk.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chau Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1\">Andrea Cavallaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A novel conservative chaos driven dynamic DNA coding for image encryption. (arXiv:2207.05475v1 [cs.CR])","link":"http://arxiv.org/abs/2207.05475","description":"<p>In this paper, we propose a novel conservative chaotic standard map-driven\ndynamic DNA coding (encoding, addition, subtraction and decoding) for the image\nencryption. The proposed image encryption algorithm is a dynamic DNA coding\nalgorithm i.e., for the encryption of each pixel different rules for encoding,\naddition/subtraction, decoding etc. are randomly selected based on the\npseudorandom sequences generated with the help of the conservative chaotic\nstandard map. We propose a novel way to generate pseudo-random sequences\nthrough the conservative chaotic standard map and also test them rigorously\nthrough the most stringent test suite of pseudo-randomness, the NIST test\nsuite, before using them in the proposed image encryption algorithm. Our image\nencryption algorithm incorporates a unique feed-forward and feedback mechanisms\nto generate and modify the dynamic one-time pixels that are further used for\nthe encryption of each pixel of the plain image, therefore, bringing in the\ndesired sensitivity on plaintext as well as ciphertext. All the controlling\npseudorandom sequences used in the algorithm are generated for a different\nvalue of the parameter (part of the secret key) with inter-dependency through\nthe iterates of the chaotic map (in the generation process) and therefore\npossess extreme key sensitivity too. The performance and security analysis has\nbeen executed extensively through histogram analysis, correlation analysis,\ninformation entropy analysis, DNA sequence-based analysis, perceptual quality\nanalysis, key sensitivity analysis, plaintext sensitivity analysis, etc., The\nresults are promising and prove the robustness of the algorithm against various\ncommon cryptanalytic attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patidar_V/0/1/0/all/0/1\">Vinod Patidar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaur_G/0/1/0/all/0/1\">Gurpreet Kaur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VertXNet: Automatic Segmentation and Identification of Lumbar and Cervical Vertebrae from Spinal X-ray Images. (arXiv:2207.05476v1 [eess.IV])","link":"http://arxiv.org/abs/2207.05476","description":"<p>Manual annotation of vertebrae on spinal X-ray imaging is costly and\ntime-consuming due to bone shape complexity and image quality variations. In\nthis study, we address this challenge by proposing an ensemble method called\nVertXNet, to automatically segment and label vertebrae in X-ray spinal images.\nVertXNet combines two state-of-the-art segmentation models, namely U-Net and\nMask R-CNN to improve vertebrae segmentation. A main feature of VertXNet is to\nalso infer vertebrae labels thanks to its Mask R-CNN component (trained to\ndetect 'reference' vertebrae) on a given spinal X-ray image. VertXNet was\nevaluated on an in-house dataset of lateral cervical and lumbar X-ray imaging\nfor ankylosing spondylitis (AS) patients. Our results show that VertXNet can\naccurately label spinal X-rays (mean Dice of 0.9). It can be used to circumvent\nthe lack of annotated vertebrae without requiring human expert review. This\nstep is crucial to investigate clinical associations by solving the lack of\nsegmentation, a common bottleneck for most computational imaging projects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mo_Y/0/1/0/all/0/1\">Yuanhan Mo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Readie_A/0/1/0/all/0/1\">Aimee Readie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ligozio_G/0/1/0/all/0/1\">Gregory Ligozio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coroller_T/0/1/0/all/0/1\">Thibaud Coroller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Papiez_B/0/1/0/all/0/1\">Bartlomiej W. Papiez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CorrI2P: Deep Image-to-Point Cloud Registration via Dense Correspondence. (arXiv:2207.05483v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05483","description":"<p>Motivated by the intuition that the critical step of localizing a 2D image in\nthe corresponding 3D point cloud is establishing 2D-3D correspondence between\nthem, we propose the first feature-based dense correspondence framework for\naddressing the image-to-point cloud registration problem, dubbed CorrI2P, which\nconsists of three modules, i.e., feature embedding, symmetric overlapping\nregion detection, and pose estimation through the established correspondence.\nSpecifically, given a pair of a 2D image and a 3D point cloud, we first\ntransform them into high-dimensional feature space and feed the resulting\nfeatures into a symmetric overlapping region detector to determine the region\nwhere the image and point cloud overlap each other. Then we use the features of\nthe overlapping regions to establish the 2D-3D correspondence before running\nEPnP within RANSAC to estimate the camera's pose. Experimental results on KITTI\nand NuScenes datasets show that our CorrI2P outperforms state-of-the-art\nimage-to-point cloud registration methods significantly. We will make the code\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Siyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yiming Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaodong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skeletal Human Action Recognition using Hybrid Attention based Graph Convolutional Network. (arXiv:2207.05493v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05493","description":"<p>In skeleton-based action recognition, Graph Convolutional Networks model\nhuman skeletal joints as vertices and connect them through an adjacency matrix,\nwhich can be seen as a local attention mask. However, in most existing Graph\nConvolutional Networks, the local attention mask is defined based on natural\nconnections of human skeleton joints and ignores the dynamic relations for\nexample between head, hands and feet joints. In addition, the attention\nmechanism has been proven effective in Natural Language Processing and image\ndescription, which is rarely investigated in existing methods. In this work, we\nproposed a new adaptive spatial attention layer that extends local attention\nmap to global based on relative distance and relative angle information.\nMoreover, we design a new initial graph adjacency matrix that connects head,\nhands and feet, which shows visible improvement in terms of action recognition\naccuracy. The proposed model is evaluated on two large-scale and challenging\ndatasets in the field of human activities in daily life: NTU-RGB+D and Kinetics\nskeleton. The results demonstrate that our model has strong performance on both\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_H/0/1/0/all/0/1\">Hao Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burschka_D/0/1/0/all/0/1\">Darius Burschka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paint and Distill: Boosting 3D Object Detection with Semantic Passing Network. (arXiv:2207.05497v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05497","description":"<p>3D object detection task from lidar or camera sensors is essential for\nautonomous driving. Pioneer attempts at multi-modality fusion complement the\nsparse lidar point clouds with rich semantic texture information from images at\nthe cost of extra network designs and overhead. In this work, we propose a\nnovel semantic passing framework, named SPNet, to boost the performance of\nexisting lidar-based 3D detection models with the guidance of rich context\npainting, with no extra computation cost during inference. Our key design is to\nfirst exploit the potential instructive semantic knowledge within the\nground-truth labels by training a semantic-painted teacher model and then guide\nthe pure-lidar network to learn the semantic-painted representation via\nknowledge passing modules at different granularities: class-wise passing,\npixel-wise passing and instance-wise passing. Experimental results show that\nthe proposed SPNet can seamlessly cooperate with most existing 3D detection\nframeworks with 1~5% AP gain and even achieve new state-of-the-art 3D detection\nperformance on the KITTI test benchmark. Code is available at:\nhttps://github.com/jb892/SPNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ju_B/0/1/0/all/0/1\">Bo Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaoqing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Minyue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modality-Aware Contrastive Instance Learning with Self-Distillation for Weakly-Supervised Audio-Visual Violence Detection. (arXiv:2207.05500v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05500","description":"<p>Weakly-supervised audio-visual violence detection aims to distinguish\nsnippets containing multimodal violence events with video-level labels. Many\nprior works perform audio-visual integration and interaction in an early or\nintermediate manner, yet overlooking the modality heterogeneousness over the\nweakly-supervised setting. In this paper, we analyze the modality asynchrony\nand undifferentiated instances phenomena of the multiple instance learning\n(MIL) procedure, and further investigate its negative impact on\nweakly-supervised audio-visual learning. To address these issues, we propose a\nmodality-aware contrastive instance learning with self-distillation (MACIL-SD)\nstrategy. Specifically, we leverage a lightweight two-stream network to\ngenerate audio and visual bags, in which unimodal background, violent, and\nnormal instances are clustered into semi-bags in an unsupervised way. Then\naudio and visual violent semi-bag representations are assembled as positive\npairs, and violent semi-bags are combined with background and normal instances\nin the opposite modality as contrastive negative pairs. Furthermore, a\nself-distillation module is applied to transfer unimodal visual knowledge to\nthe audio-visual model, which alleviates noises and closes the semantic gap\nbetween unimodal and multimodal features. Experiments show that our framework\noutperforms previous methods with lower complexity on the large-scale\nXD-Violence dataset. Results also demonstrate that our proposed approach can be\nused as plug-in modules to enhance other networks. Codes are available at\nhttps://github.com/JustinYuu/MACIL_SD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiashuo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Ying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Rui Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuejie Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios. (arXiv:2207.05501v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05501","description":"<p>Due to the complex attention mechanisms and model design, most existing\nvision Transformers (ViTs) can not perform as efficiently as convolutional\nneural networks (CNNs) in realistic industrial deployment scenarios, e.g.\nTensorRT and CoreML. This poses a distinct challenge: Can a visual neural\nnetwork be designed to infer as fast as CNNs and perform as powerful as ViTs?\nRecent works have tried to design CNN-Transformer hybrid architectures to\naddress this issue, yet the overall performance of these works is far away from\nsatisfactory. To end these, we propose a next generation vision Transformer for\nefficient deployment in realistic industrial scenarios, namely Next-ViT, which\ndominates both CNNs and ViTs from the perspective of latency/accuracy\ntrade-off. In this work, the Next Convolution Block (NCB) and Next Transformer\nBlock (NTB) are respectively developed to capture local and global information\nwith deployment-friendly mechanisms. Then, Next Hybrid Strategy (NHS) is\ndesigned to stack NCB and NTB in an efficient hybrid paradigm, which boosts\nperformance in various downstream tasks. Extensive experiments show that\nNext-ViT significantly outperforms existing CNNs, ViTs and CNN-Transformer\nhybrid architectures with respect to the latency/accuracy trade-off across\nvarious vision tasks. On TensorRT, Next-ViT surpasses ResNet by 5.4 mAP (from\n40.4 to 45.8) on COCO detection and 8.2% mIoU (from 38.8% to 47.0%) on ADE20K\nsegmentation under similar latency. Meanwhile, it achieves comparable\nperformance with CSWin, while the inference speed is accelerated by 3.6x. On\nCoreML, Next-ViT surpasses EfficientFormer by 4.6 mAP (from 42.6 to 47.2) on\nCOCO detection and 3.5% mIoU (from 45.2% to 48.7%) on ADE20K segmentation under\nsimilar latency. Code will be released recently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiashi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huixia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuefeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Min Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xin Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferability-Guided Cross-Domain Cross-Task Transfer Learning. (arXiv:2207.05510v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05510","description":"<p>We propose two novel transferability metrics F-OTCE (Fast Optimal Transport\nbased Conditional Entropy) and JC-OTCE (Joint Correspondence OTCE) to evaluate\nhow much the source model (task) can benefit the learning of the target task\nand to learn more transferable representations for cross-domain cross-task\ntransfer learning. Unlike the existing metric that requires evaluating the\nempirical transferability on auxiliary tasks, our metrics are auxiliary-free\nsuch that they can be computed much more efficiently. Specifically, F-OTCE\nestimates transferability by first solving an Optimal Transport (OT) problem\nbetween source and target distributions, and then uses the optimal coupling to\ncompute the Negative Conditional Entropy between source and target labels. It\ncan also serve as a loss function to maximize the transferability of the source\nmodel before finetuning on the target task. Meanwhile, JC-OTCE improves the\ntransferability robustness of F-OTCE by including label distances in the OT\nproblem, though it may incur additional computation cost. Extensive experiments\ndemonstrate that F-OTCE and JC-OTCE outperform state-of-the-art auxiliary-free\nmetrics by 18.85% and 28.88%, respectively in correlation coefficient with the\nground-truth transfer accuracy. By eliminating the training cost of auxiliary\ntasks, the two metrics reduces the total computation time of the previous\nmethod from 43 minutes to 9.32s and 10.78s, respectively, for a pair of tasks.\nWhen used as a loss function, F-OTCE shows consistent improvements on the\ntransfer accuracy of the source model in few-shot classification experiments,\nwith up to 4.41% accuracy gain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shao-Lun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Ping Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compound Prototype Matching for Few-shot Action Recognition. (arXiv:2207.05515v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05515","description":"<p>Few-shot action recognition aims to recognize novel action classes using only\na small number of labeled training samples. In this work, we propose a novel\napproach that first summarizes each video into compound prototypes consisting\nof a group of global prototypes and a group of focused prototypes, and then\ncompares video similarity based on the prototypes. Each global prototype is\nencouraged to summarize a specific aspect from the entire video, for example,\nthe start/evolution of the action. Since no clear annotation is provided for\nthe global prototypes, we use a group of focused prototypes to focus on certain\ntimestamps in the video. We compare video similarity by matching the compound\nprototypes between the support and query videos. The global prototypes are\ndirectly matched to compare videos from the same perspective, for example, to\ncompare whether two actions start similarly. For the focused prototypes, since\nactions have various temporal variations in the videos, we apply bipartite\nmatching to allow the comparison of actions with different temporal positions\nand shifts. Experiments demonstrate that our proposed method achieves\nstate-of-the-art results on multiple benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lijin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tracking Objects as Pixel-wise Distributions. (arXiv:2207.05518v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05518","description":"<p>Multi-object tracking (MOT) requires detecting and associating objects\nthrough frames. Unlike tracking via detected bounding boxes or tracking objects\nas points, we propose tracking objects as pixel-wise distributions. We\ninstantiate this idea on a transformer-based architecture, P3AFormer, with\npixel-wise propagation, prediction, and association. P3AFormer propagates\npixel-wise features guided by flow information to pass messages between frames.\nFurthermore, P3AFormer adopts a meta-architecture to produce multi-scale object\nfeature maps. During inference, a pixel-wise association procedure is proposed\nto recover object connections through frames based on the pixel-wise\nprediction. P3AFormer yields 81.2\\% in terms of MOTA on the MOT17 benchmark --\nthe first among all transformer networks to reach 80\\% MOTA in literature.\nP3AFormer also outperforms state-of-the-arts on the MOT20 and KITTI benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zelin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ze Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueqing Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boxun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-term Leap Attention, Short-term Periodic Shift for Video Classification. (arXiv:2207.05526v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05526","description":"<p>Video transformer naturally incurs a heavier computation burden than a static\nvision transformer, as the former processes $T$ times longer sequence than the\nlatter under the current attention of quadratic complexity $(T^2N^2)$. The\nexisting works treat the temporal axis as a simple extension of spatial axes,\nfocusing on shortening the spatio-temporal sequence by either generic pooling\nor local windowing without utilizing temporal redundancy.\n</p>\n<p>However, videos naturally contain redundant information between neighboring\nframes; thereby, we could potentially suppress attention on visually similar\nframes in a dilated manner. Based on this hypothesis, we propose the LAPS, a\nlong-term ``\\textbf{\\textit{Leap Attention}}'' (LA), short-term\n``\\textbf{\\textit{Periodic Shift}}'' (\\textit{P}-Shift) module for video\ntransformers, with $(2TN^2)$ complexity. Specifically, the ``LA'' groups\nlong-term frames into pairs, then refactors each discrete pair via attention.\nThe ``\\textit{P}-Shift'' exchanges features between temporal neighbors to\nconfront the loss of short-term dynamics. By replacing a vanilla 2D attention\nwith the LAPS, we could adapt a static transformer into a video one, with zero\nextra parameters and neglectable computation overhead ($\\sim$2.6\\%).\nExperiments on the standard Kinetics-400 benchmark demonstrate that our LAPS\ntransformer could achieve competitive performances in terms of accuracy, FLOPs,\nand Params among CNN and transformer SOTAs. We open-source our project in\n\\sloppy\n\\href{https://github.com/VideoNetworks/LAPS-transformer}{\\textit{\\color{magenta}{https://github.com/VideoNetworks/LAPS-transformer}}} .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lechao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yanbin Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Camera Pose Auto-Encoders for Improving Pose Regression. (arXiv:2207.05530v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05530","description":"<p>Absolute pose regressor (APR) networks are trained to estimate the pose of\nthe camera given a captured image. They compute latent image representations\nfrom which the camera position and orientation are regressed. APRs provide a\ndifferent tradeoff between localization accuracy, runtime, and memory, compared\nto structure-based localization schemes that provide state-of-the-art accuracy.\nIn this work, we introduce Camera Pose Auto-Encoders (PAEs), multilayer\nperceptrons that are trained via a Teacher-Student approach to encode camera\nposes using APRs as their teachers. We show that the resulting latent pose\nrepresentations can closely reproduce APR performance and demonstrate their\neffectiveness for related tasks. Specifically, we propose a light-weight\ntest-time optimization in which the closest train poses are encoded and used to\nrefine camera position estimation. This procedure achieves a new\nstate-of-the-art position accuracy for APRs, on both the CambridgeLandmarks and\n7Scenes benchmarks. We also show that train images can be reconstructed from\nthe learned pose encoding, paving the way for integrating visual information\nfrom the train set at a low memory cost. Our code and pre-trained models are\navailable at https://github.com/yolish/camera-pose-auto-encoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shavit_Y/0/1/0/all/0/1\">Yoli Shavit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_Y/0/1/0/all/0/1\">Yosi Keller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Utilizing Excess Resources in Training Neural Networks. (arXiv:2207.05532v1 [cs.LG])","link":"http://arxiv.org/abs/2207.05532","description":"<p>In this work, we suggest Kernel Filtering Linear Overparameterization (KFLO),\nwhere a linear cascade of filtering layers is used during training to improve\nnetwork performance in test time. We implement this cascade in a kernel\nfiltering fashion, which prevents the trained architecture from becoming\nunnecessarily deeper. This also allows using our approach with almost any\nnetwork architecture and let combining the filtering layers into a single layer\nin test time. Thus, our approach does not add computational complexity during\ninference. We demonstrate the advantage of KFLO on various network models and\ndatasets in supervised learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henig_A/0/1/0/all/0/1\">Amit Henig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DTG-SSOD: Dense Teacher Guidance for Semi-Supervised Object Detection. (arXiv:2207.05536v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05536","description":"<p>The Mean-Teacher (MT) scheme is widely adopted in semi-supervised object\ndetection (SSOD). In MT, the sparse pseudo labels, offered by the final\npredictions of the teacher (e.g., after Non Maximum Suppression (NMS)\npost-processing), are adopted for the dense supervision for the student via\nhand-crafted label assignment. However, the sparse-to-dense paradigm\ncomplicates the pipeline of SSOD, and simultaneously neglects the powerful\ndirect, dense teacher supervision. In this paper, we attempt to directly\nleverage the dense guidance of teacher to supervise student training, i.e., the\ndense-to-dense paradigm. Specifically, we propose the Inverse NMS Clustering\n(INC) and Rank Matching (RM) to instantiate the dense supervision, without the\nwidely used, conventional sparse pseudo labels. INC leads the student to group\ncandidate boxes into clusters in NMS as the teacher does, which is implemented\nby learning grouping information revealed in NMS procedure of the teacher.\nAfter obtaining the same grouping scheme as the teacher via INC, the student\nfurther imitates the rank distribution of the teacher over clustered candidates\nthrough Rank Matching. With the proposed INC and RM, we integrate Dense Teacher\nGuidance into Semi-Supervised Object Detection (termed DTG-SSOD), successfully\nabandoning sparse pseudo labels and enabling more informative learning on\nunlabeled data. On COCO benchmark, our DTG-SSOD achieves state-of-the-art\nperformance under various labelling ratios. For example, under 10% labelling\nratio, DTG-SSOD improves the supervised baseline from 26.9 to 35.9 mAP,\noutperforming the previous best method Soft Teacher by 1.9 points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yichao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanshan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Markovian Gaussian Process Variational Autoencoders. (arXiv:2207.05543v1 [cs.LG])","link":"http://arxiv.org/abs/2207.05543","description":"<p>Deep generative models are widely used for modelling high-dimensional time\nseries, such as video animations, audio and climate data. Sequential\nvariational autoencoders have been successfully considered for many\napplications, with many variant models relying on discrete-time methods and\nrecurrent neural networks (RNNs). On the other hand, continuous-time methods\nhave recently gained attraction, especially in the context of\nirregularly-sampled time series, where they can better handle the data than\ndiscrete-time methods. One such class are Gaussian process variational\nautoencoders (GPVAEs), where the VAE prior is set as a Gaussian process (GPs),\nallowing inductive biases to be explicitly encoded via the kernel function and\ninterpretability of the latent space. However, a major limitation of GPVAEs is\nthat it inherits the same cubic computational cost as GPs. In this work, we\nleverage the equivalent discrete state space representation of Markovian GPs to\nenable a linear-time GP solver via Kalman filtering and smoothing. We show via\ncorrupt and missing frames tasks that our method performs favourably,\nespecially on the latter where it outperforms RNN-based models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Harrison Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodas_C/0/1/0/all/0/1\">Carles Balsells Rodas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingzhen Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LightViT: Towards Light-Weight Convolution-Free Vision Transformers. (arXiv:2207.05557v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05557","description":"<p>Vision transformers (ViTs) are usually considered to be less light-weight\nthan convolutional neural networks (CNNs) due to the lack of inductive bias.\nRecent works thus resort to convolutions as a plug-and-play module and embed\nthem in various ViT counterparts. In this paper, we argue that the\nconvolutional kernels perform information aggregation to connect all tokens;\nhowever, they would be actually unnecessary for light-weight ViTs if this\nexplicit aggregation could function in a more homogeneous way. Inspired by\nthis, we present LightViT as a new family of light-weight ViTs to achieve\nbetter accuracy-efficiency balance upon the pure transformer blocks without\nconvolution. Concretely, we introduce a global yet efficient aggregation scheme\ninto both self-attention and feed-forward network (FFN) of ViTs, where\nadditional learnable tokens are introduced to capture global dependencies; and\nbi-dimensional channel and spatial attentions are imposed over token\nembeddings. Experiments show that our model achieves significant improvements\non image classification, object detection, and semantic segmentation tasks. For\nexample, our LightViT-T achieves 78.7% accuracy on ImageNet with only 0.7G\nFLOPs, outperforming PVTv2-B0 by 8.2% while 11% faster on GPU. Code is\navailable at https://github.com/hunto/LightViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Label Relationships in Human Affect. (arXiv:2207.05577v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05577","description":"<p>Human affect and mental state estimation in an automated manner, face a\nnumber of difficulties, including learning from labels with poor or no temporal\nresolution, learning from few datasets with little data (often due to\nconfidentiality constraints) and, (very) long, in-the-wild videos. For these\nreasons, deep learning methodologies tend to overfit, that is, arrive at latent\nrepresentations with poor generalisation performance on the final regression\ntask. To overcome this, in this work, we introduce two complementary\ncontributions. First, we introduce a novel relational loss for multilabel\nregression and ordinal problems that regularises learning and leads to better\ngeneralisation. The proposed loss uses label vector inter-relational\ninformation to learn better latent representations by aligning batch label\ndistances to the distances in the latent feature space. Second, we utilise a\ntwo-stage attention architecture that estimates a target for each clip by using\nfeatures from the neighbouring clips as temporal context. We evaluate the\nproposed methodology on both continuous affect and schizophrenia severity\nestimation problems, as there are methodological and contextual parallels\nbetween the two. Experimental results demonstrate that the proposed methodology\noutperforms all baselines. In the domain of schizophrenia, the proposed\nmethodology outperforms previous state-of-the-art by a large margin, achieving\na PCC of up to 78% performance close to that of human experts (85%) and much\nhigher than previous works (uplift of up to 40%). In the case of affect\nrecognition, we outperform previous vision-based methods in terms of CCC on\nboth the OMG and the AMIGOS datasets. Specifically for AMIGOS, we outperform\nprevious SoTA CCC for both arousal and valence by 9% and 13% respectively, and\nin the OMG dataset we outperform previous vision works by up to 5% for both\narousal and valence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Foteinopoulou_N/0/1/0/all/0/1\">Niki Maria Foteinopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1\">Ioannis Patras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Video Instance Segmentation via Robust Context Fusion. (arXiv:2207.05580v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05580","description":"<p>Video instance segmentation (VIS) aims at classifying, segmenting and\ntracking object instances in video sequences. Recent transformer-based neural\nnetworks have demonstrated their powerful capability of modeling\nspatio-temporal correlations for the VIS task. Relying on video- or clip-level\ninput, they suffer from high latency and computational cost. We propose a\nrobust context fusion network to tackle VIS in an online fashion, which\npredicts instance segmentation frame-by-frame with a few preceding frames. To\nacquire the precise and temporal-consistent prediction for each frame\nefficiently, the key idea is to fuse effective and compact context from\nreference frames into the target frame. Considering the different effects of\nreference and target frames on the target prediction, we first summarize\ncontextual features through importance-aware compression. A transformer encoder\nis adopted to fuse the compressed context. Then, we leverage an\norder-preserving instance embedding to convey the identity-aware information\nand correspond the identities to predicted instance masks. We demonstrate that\nour robust fusion network achieves the best performance among existing online\nVIS methods and is even better than previously published clip-level methods on\nthe Youtube-VIS 2019 and 2021 benchmarks. In addition, visual objects often\nhave acoustic signatures that are naturally synchronized with them in\naudio-bearing video recordings. By leveraging the flexibility of our context\nfusion network on multi-modal data, we further investigate the influence of\naudios on the video-dense prediction task, which has never been discussed in\nexisting works. We build up an Audio-Visual Instance Segmentation dataset, and\ndemonstrate that acoustic signals in the wild scenarios could benefit the VIS\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinglu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaohao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ego-motion Estimation Based on Fusion of Images and Events. (arXiv:2207.05588v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05588","description":"<p>Event camera is a novel bio-inspired vision sensor that outputs event stream.\nIn this paper, we propose a novel data fusion algorithm called EAS to fuse\nconventional intensity images with the event stream. The fusion result is\napplied to some ego-motion estimation frameworks, and is evaluated on a public\ndataset acquired in dim scenes. In our 3-DoF rotation estimation framework, EAS\nachieves the highest estimation accuracy among intensity images and\nrepresentations of events including event slice, TS and SITS. Compared with\noriginal images, EAS reduces the average APE by 69%, benefiting from the\ninclusion of more features for tracking. The result shows that our algorithm\neffectively leverages the high dynamic range of event cameras to improve the\nperformance of the ego-motion estimation framework based on optical flow\ntracking in difficult illumination conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liren Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Real-time High-Definition Image Snow Removal: Efficient Pyramid Network with Asymmetrical Encoder-decoder Architecture. (arXiv:2207.05605v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05605","description":"<p>In winter scenes, the degradation of images taken under snow can be pretty\ncomplex, where the spatial distribution of snowy degradation is varied from\nimage to image. Recent methods adopt deep neural networks to directly recover\nclean scenes from snowy images. However, due to the paradox caused by the\nvariation of complex snowy degradation, achieving reliable High-Definition\nimage desnowing performance in real time is a considerable challenge. We\ndevelop a novel Efficient Pyramid Network with asymmetrical encoder-decoder\narchitecture for real-time HD image desnowing. The general idea of our proposed\nnetwork is to utilize the multi-scale feature flow fully and implicitly mine\nclean cues from features. Compared with previous state-of-the-art desnowing\nmethods, our approach achieves a better complexity-performance trade-off and\neffectively handles the processing difficulties of HD and Ultra-HD images.\n</p>\n<p>The extensive experiments on three large-scale image desnowing datasets\ndemonstrate that our method surpasses all state-of-the-art approaches by a\nlarge margin both quantitatively and qualitatively, boosting the PSNR metric\nfrom 31.76 dB to 34.10 dB on the CSD test dataset and from 28.29 dB to 30.87 dB\non the SRRS test dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Tian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Erkang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inner Monologue: Embodied Reasoning through Planning with Language Models. (arXiv:2207.05608v1 [cs.RO])","link":"http://arxiv.org/abs/2207.05608","description":"<p>Recent works have shown how the reasoning capabilities of Large Language\nModels (LLMs) can be applied to domains beyond natural language processing,\nsuch as planning and interaction for robots. These embodied problems require an\nagent to understand many semantic aspects of the world: the repertoire of\nskills available, how these skills influence the world, and how changes to the\nworld map back to the language. LLMs planning in embodied environments need to\nconsider not just what skills to do, but also how and when to do them - answers\nthat change over time in response to the agent's own choices. In this work, we\ninvestigate to what extent LLMs used in such embodied contexts can reason over\nsources of feedback provided through natural language, without any additional\ntraining. We propose that by leveraging environment feedback, LLMs are able to\nform an inner monologue that allows them to more richly process and plan in\nrobotic control scenarios. We investigate a variety of sources of feedback,\nsuch as success detection, scene description, and human interaction. We find\nthat closed-loop language feedback significantly improves high-level\ninstruction completion on three domains, including simulated and real table top\nrearrangement tasks and long-horizon mobile manipulation tasks in a kitchen\nenvironment in the real world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenlong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Ted Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Harris Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jacky Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1\">Pete Florence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tompson_J/0/1/0/all/0/1\">Jonathan Tompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chebotar_Y/0/1/0/all/0/1\">Yevgen Chebotar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sermanet_P/0/1/0/all/0/1\">Pierre Sermanet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1\">Noah Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jackson_T/0/1/0/all/0/1\">Tomas Jackson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_L/0/1/0/all/0/1\">Linda Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1\">Karol Hausman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1\">Brian Ichter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning for Online Semi-Supervised General Continual Learning. (arXiv:2207.05615v1 [cs.LG])","link":"http://arxiv.org/abs/2207.05615","description":"<p>We study Online Continual Learning with missing labels and propose SemiCon, a\nnew contrastive loss designed for partly labeled data. We demonstrate its\nefficiency by devising a memory-based method trained on an unlabeled data\nstream, where every data added to memory is labeled using an oracle. Our\napproach outperforms existing semi-supervised methods when few labels are\navailable, and obtain similar results to state-of-the-art supervised methods\nwhile using only 2.6% of labels on Split-CIFAR10 and 10% of labels on\nSplit-CIFAR100.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michel_N/0/1/0/all/0/1\">Nicolas Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negrel_R/0/1/0/all/0/1\">Romain Negrel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chierchia_G/0/1/0/all/0/1\">Giovanni Chierchia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bercher_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Bercher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LudVision -- Remote Detection of Exotic Invasive Aquatic Floral Species using Drone-Mounted Multispectral Data. (arXiv:2207.05620v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05620","description":"<p>Remote sensing is the process of detecting and monitoring the physical\ncharacteristics of an area by measuring its reflected and emitted radiation at\na distance. It is being broadly used to monitor ecosystems, mainly for their\npreservation. Ever-growing reports of invasive species have affected the\nnatural balance of ecosystems. Exotic invasive species have a critical impact\nwhen introduced into new ecosystems and may lead to the extinction of native\nspecies. In this study, we focus on Ludwigia peploides, considered by the\nEuropean Union as an aquatic invasive species. Its presence can negatively\nimpact the surrounding ecosystem and human activities such as agriculture,\nfishing, and navigation. Our goal was to develop a method to identify the\npresence of the species. We used images collected by a drone-mounted\nmultispectral sensor to achieve this, creating our LudVision data set. To\nidentify the targeted species on the collected images, we propose a new method\nfor detecting Ludwigia p. in multispectral images. The method is based on\nexisting state-of-the-art semantic segmentation methods modified to handle\nmultispectral data. The proposed method achieved a producer's accuracy of 0.799\nand a user's accuracy of 0.955.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abreu_A/0/1/0/all/0/1\">Ant&#xf3;nio J. Abreu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexandre_L/0/1/0/all/0/1\">Lu&#xed;s A. Alexandre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jo&#xe3;o A. Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basso_F/0/1/0/all/0/1\">Filippo Basso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSP-Former: Multi-Scale Projection Transformer for Single Image Desnowing. (arXiv:2207.05621v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05621","description":"<p>Image restoration of snow scenes in severe weather is a difficult task. Snow\nimages have complex degradations and are cluttered over clean images, changing\nthe distribution of clean images. The previous methods based on CNNs are\nchallenging to remove perfectly in restoring snow scenes due to their local\ninductive biases' lack of a specific global modeling ability. In this paper, we\napply the vision transformer to the task of snow removal from a single image.\nSpecifically, we propose a parallel network architecture split along the\nchannel, performing local feature refinement and global information modeling\nseparately. We utilize a channel shuffle operation to combine their respective\nstrengths to enhance network performance. Second, we propose the MSP module,\nwhich utilizes multi-scale avgpool to aggregate information of different sizes\nand simultaneously performs multi-scale projection self-attention on multi-head\nself-attention to improve the representation ability of the model under\ndifferent scale degradations. Finally, we design a lightweight and simple local\ncapture module, which can refine the local capture capability of the model.\n</p>\n<p>In the experimental part, we conduct extensive experiments to demonstrate the\nsuperiority of our method. We compared the previous snow removal methods on\nthree snow scene datasets. The experimental results show that our method\nsurpasses the state-of-the-art methods with fewer parameters and computation.\nWe achieve substantial growth by 1.99dB and SSIM 0.03 on the CSD test dataset.\nOn the SRRS and Snow100K datasets, we also increased PSNR by 2.47dB and 1.62dB\ncompared with the Transweather approach and improved by 0.03 in SSIM. In the\nvisual comparison section, our MSP-Former also achieves better visual effects\nthan existing methods, proving the usability of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Tian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_T/0/1/0/all/0/1\">Taodong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Erkang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GANzzle: Reframing jigsaw puzzle solving as a retrieval task using a generative mental image. (arXiv:2207.05634v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05634","description":"<p>Puzzle solving is a combinatorial challenge due to the difficulty of matching\nadjacent pieces. Instead, we infer a mental image from all pieces, which a\ngiven piece can then be matched against avoiding the combinatorial explosion.\nExploiting advancements in Generative Adversarial methods, we learn how to\nreconstruct the image given a set of unordered pieces, allowing the model to\nlearn a joint embedding space to match an encoding of each piece to the cropped\nlayer of the generator. Therefore we frame the problem as a R@1 retrieval task,\nand then solve the linear assignment using differentiable Hungarian attention,\nmaking the process end-to-end. In doing so our model is puzzle size agnostic,\nin contrast to prior deep learning methods which are single size. We evaluate\non two new large-scale datasets, where our model is on par with deep learning\nmethods, while generalizing to multiple puzzle sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Talon_D/0/1/0/all/0/1\">Davide Talon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bue_A/0/1/0/all/0/1\">Alessio Del Bue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stuart James</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backdoor Attacks on Crowd Counting. (arXiv:2207.05641v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05641","description":"<p>Crowd counting is a regression task that estimates the number of people in a\nscene image, which plays a vital role in a range of safety-critical\napplications, such as video surveillance, traffic monitoring and flow control.\nIn this paper, we investigate the vulnerability of deep learning based crowd\ncounting models to backdoor attacks, a major security threat to deep learning.\nA backdoor attack implants a backdoor trigger into a target model via data\npoisoning so as to control the model's predictions at test time. Different from\nimage classification models on which most of existing backdoor attacks have\nbeen developed and tested, crowd counting models are regression models that\noutput multi-dimensional density maps, thus requiring different techniques to\nmanipulate.\n</p>\n<p>In this paper, we propose two novel Density Manipulation Backdoor Attacks\n(DMBA$^{-}$ and DMBA$^{+}$) to attack the model to produce arbitrarily large or\nsmall density estimations. Experimental results demonstrate the effectiveness\nof our DMBA attacks on five classic crowd counting models and four types of\ndatasets. We also provide an in-depth analysis of the unique challenges of\nbackdooring crowd counting models and reveal two key elements of effective\nattacks: 1) full and dense triggers and 2) manipulation of the ground truth\ncounts or density maps. Our work could help evaluate the vulnerability of crowd\ncounting models to potential backdoor attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuhua Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tailai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xingjun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zichuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_X/0/1/0/all/0/1\">Xing Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lichao/0/1/0/all/0/1\">Lichao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Docent: A content-based recommendation system to discover contemporary art. (arXiv:2207.05648v1 [cs.LG])","link":"http://arxiv.org/abs/2207.05648","description":"<p>Recommendation systems have been widely used in various domains such as\nmusic, films, e-shopping etc. After mostly avoiding digitization, the art world\nhas recently reached a technological turning point due to the pandemic, making\nonline sales grow significantly as well as providing quantitative online data\nabout artists and artworks. In this work, we present a content-based\nrecommendation system on contemporary art relying on images of artworks and\ncontextual metadata of artists. We gathered and annotated artworks with\nadvanced and art-specific information to create a completely unique database\nthat was used to train our models. With this information, we built a proximity\ngraph between artworks. Similarly, we used NLP techniques to characterize the\npractices of the artists and we extracted information from exhibitions and\nother event history to create a proximity graph between artists. The power of\ngraph analysis enables us to provide an artwork recommendation system based on\na combination of visual and contextual information from artworks and artists.\nAfter an assessment by a team of art specialists, we get an average final\nrating of 75% of meaningful artworks when compared to their professional\nevaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fosset_A/0/1/0/all/0/1\">Antoine Fosset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Mennaoui_M/0/1/0/all/0/1\">Mohamed El-Mennaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rebei_A/0/1/0/all/0/1\">Amine Rebei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calligaro_P/0/1/0/all/0/1\">Paul Calligaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maria_E/0/1/0/all/0/1\">Elise Farge Di Maria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Ban_H/0/1/0/all/0/1\">H&#xe9;l&#xe8;ne Nguyen-Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rea_F/0/1/0/all/0/1\">Francesca Rea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallade_M/0/1/0/all/0/1\">Marie-Charlotte Vallade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitullo_E/0/1/0/all/0/1\">Elisabetta Vitullo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Christophe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charpiat_G/0/1/0/all/0/1\">Guillaume Charpiat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenbaum_M/0/1/0/all/0/1\">Mathieu Rosenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Gradient Reactivation for Backward Compatible Person Re-identification. (arXiv:2207.05658v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05658","description":"<p>We study the backward compatible problem for person re-identification\n(Re-ID), which aims to constrain the features of an updated new model to be\ncomparable with the existing features from the old model in galleries. Most of\nthe existing works adopt distillation-based methods, which focus on pushing new\nfeatures to imitate the distribution of the old ones. However, the\ndistillation-based methods are intrinsically sub-optimal since it forces the\nnew feature space to imitate the inferior old feature space. To address this\nissue, we propose the Ranking-based Backward Compatible Learning (RBCL), which\ndirectly optimizes the ranking metric between new features and old features.\nDifferent from previous methods, RBCL only pushes the new features to find\nbest-ranking positions in the old feature space instead of strictly alignment,\nand is in line with the ultimate goal of backward retrieval. However, the sharp\nsigmoid function used to make the ranking metric differentiable also incurs the\ngradient vanish issue, therefore stems the ranking refinement during the later\nperiod of training. To address this issue, we propose the Dynamic Gradient\nReactivation (DGR), which can reactivate the suppressed gradients by adding\ndynamic computed constant during forward step. To further help targeting the\nbest-ranking positions, we include the Neighbor Context Agents (NCAs) to\napproximate the entire old feature space during training. Unlike previous works\nwhich only test on the in-domain settings, we make the first attempt to\nintroduce the cross-domain settings (including both supervised and\nunsupervised), which are more meaningful and difficult. The experimental\nresults on all five settings show that the proposed RBCL outperforms previous\nstate-of-the-art methods by large margins under all settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jianyang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peike Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RE-Tagger: A light-weight Real-Estate Image Classifier. (arXiv:2207.05696v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05696","description":"<p>Real-estate image tagging is one of the essential use-cases to save efforts\ninvolved in manual annotation and enhance the user experience. This paper\nproposes an end-to-end pipeline (referred to as RE-Tagger) for the real-estate\nimage classification problem. We present a two-stage transfer learning approach\nusing custom InceptionV3 architecture to classify images into different\ncategories (i.e., bedroom, bathroom, kitchen, balcony, hall, and others).\nFinally, we released the application as REST API hosted as a web application\nrunning on 2 cores machine with 2 GB RAM. The demo video is available here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chhikara_P/0/1/0/all/0/1\">Prateek Chhikara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1\">Anil Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_C/0/1/0/all/0/1\">Chirag Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tell Me the Evidence? Dual Visual-Linguistic Interaction for Answer Grounding. (arXiv:2207.05703v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05703","description":"<p>Answer grounding aims to reveal the visual evidence for visual question\nanswering (VQA), which entails highlighting relevant positions in the image\nwhen answering questions about images. Previous attempts typically tackle this\nproblem using pretrained object detectors, but without the flexibility for\nobjects not in the predefined vocabulary. However, these black-box methods\nsolely concentrate on the linguistic generation, ignoring the visual\ninterpretability. In this paper, we propose Dual Visual-Linguistic Interaction\n(DaVI), a novel unified end-to-end framework with the capability for both\nlinguistic answering and visual grounding. DaVI innovatively introduces two\nvisual-linguistic interaction mechanisms: 1) visual-based linguistic encoder\nthat understands questions incorporated with visual features and produces\nlinguistic-oriented evidence for further answer decoding, and 2)\nlinguistic-based visual decoder that focuses visual features on the\nevidence-related regions for answer grounding. This way, our approach ranked\nthe 1st place in the answer grounding track of 2022 VizWiz Grand Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junwen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanlin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiexiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_C/0/1/0/all/0/1\">Cheng Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Pengfei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhicheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M-FUSE: Multi-frame Fusion for Scene Flow Estimation. (arXiv:2207.05704v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05704","description":"<p>Recently, neural network for scene flow estimation show impressive results on\nautomotive data such as the KITTI benchmark. However, despite of using\nsophisticated rigidity assumptions and parametrizations, such networks are\ntypically limited to only two frame pairs which does not allow them to exploit\ntemporal information. In our paper we address this shortcoming by proposing a\nnovel multi-frame approach that considers an additional preceding stereo pair.\nTo this end, we proceed in two steps: Firstly, building upon the recent RAFT-3D\napproach, we develop an advanced two-frame baseline by incorporating an\nimproved stereo method. Secondly, and even more importantly, exploiting the\nspecific modeling concepts of RAFT-3D, we propose a U-Net like architecture\nthat performs a fusion of forward and backward flow estimates and hence allows\nto integrate temporal information on demand. Experiments on the KITTI benchmark\ndo not only show that the advantages of the improved baseline and the temporal\nfusion approach complement each other, they also demonstrate that the computed\nscene flow is highly accurate. More precisely, our approach ranks second\noverall and first for the even more challenging foreground objects, in total\noutperforming the original RAFT-3D method by more than 16%. Code is available\nat https://github.com/cv-stuttgart/M-FUSE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehl_L/0/1/0/all/0/1\">Lukas Mehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahedi_A/0/1/0/all/0/1\">Azin Jahedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmalfuss_J/0/1/0/all/0/1\">Jenny Schmalfuss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruhn_A/0/1/0/all/0/1\">Andr&#xe9;s Bruhn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Experimental Design for Computed Tomography with the Linearised Deep Image Prior. (arXiv:2207.05714v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05714","description":"<p>We investigate adaptive design based on a single sparse pilot scan for\ngenerating effective scanning strategies for computed tomography\nreconstruction. We propose a novel approach using the linearised deep image\nprior. It allows incorporating information from the pilot measurements into the\nangle selection criteria, while maintaining the tractability of a conjugate\nGaussian-linear model. On a synthetically generated dataset with preferential\ndirections, linearised DIP design allows reducing the number of scans by up to\n30% relative to an equidistant angle baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barbano_R/0/1/0/all/0/1\">Riccardo Barbano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leuschner_J/0/1/0/all/0/1\">Johannes Leuschner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoran_J/0/1/0/all/0/1\">Javier Antor&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1\">Bangti Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1\">Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Fairness of Visual Attribute Predictors. (arXiv:2207.05727v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05727","description":"<p>The performance of deep neural networks for image recognition tasks such as\npredicting a smiling face is known to degrade with under-represented classes of\nsensitive attributes. We address this problem by introducing fairness-aware\nregularization losses based on batch estimates of Demographic Parity, Equalized\nOdds, and a novel Intersection-over-Union measure. The experiments performed on\nfacial and medical images from CelebA, UTKFace, and the SIIM-ISIC melanoma\nclassification challenge show the effectiveness of our proposed fairness losses\nfor bias mitigation as they improve model fairness while maintaining high\nclassification performance. To the best of our knowledge, our work is the first\nattempt to incorporate these types of losses in an end-to-end training scheme\nfor mitigating biases of visual attribute predictors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanel_T/0/1/0/all/0/1\">Tobias H&#xe4;nel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Nishant Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlesinger_D/0/1/0/all/0/1\">Dmitrij Schlesinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_E/0/1/0/all/0/1\">Erdem &#xdc;nal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslami_A/0/1/0/all/0/1\">Abouzar Eslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gumhold_S/0/1/0/all/0/1\">Stefan Gumhold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physical Passive Patch Adversarial Attacks on Visual Odometry Systems. (arXiv:2207.05729v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05729","description":"<p>Deep neural networks are known to be susceptible to adversarial perturbations\n-- small perturbations that alter the output of the network and exist under\nstrict norm limitations. While such perturbations are usually discussed as\ntailored to a specific input, a universal perturbation can be constructed to\nalter the model's output on a set of inputs. Universal perturbations present a\nmore realistic case of adversarial attacks, as awareness of the model's exact\ninput is not required. In addition, the universal attack setting raises the\nsubject of generalization to unseen data, where given a set of inputs, the\nuniversal perturbations aim to alter the model's output on out-of-sample data.\nIn this work, we study physical passive patch adversarial attacks on visual\nodometry-based autonomous navigation systems. A visual odometry system aims to\ninfer the relative camera motion between two corresponding viewpoints, and is\nfrequently used by vision-based autonomous navigation systems to estimate their\nstate. For such navigation systems, a patch adversarial perturbation poses a\nsevere security issue, as it can be used to mislead a system onto some\ncollision course. To the best of our knowledge, we show for the first time that\nthe error margin of a visual odometry model can be significantly increased by\ndeploying patch adversarial attacks in the scene. We provide evaluation on\nsynthetic closed-loop drone navigation data and demonstrate that a comparable\nvulnerability exists in real data. A reference implementation of the proposed\nmethod and the reported experiments is provided at\nhttps://github.com/patchadversarialattacks/patchadversarialattacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nemcovsky_Y/0/1/0/all/0/1\">Yaniv Nemcovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaakoby_M/0/1/0/all/0/1\">Matan Yaakoby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bronstein_A/0/1/0/all/0/1\">Alex M. Bronstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baskin_C/0/1/0/all/0/1\">Chaim Baskin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"1st Place Solution to the EPIC-Kitchens Action Anticipation Challenge 2022. (arXiv:2207.05730v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05730","description":"<p>In this report, we describe the technical details of our submission to the\nEPIC-Kitchens Action Anticipation Challenge 2022. In this competition, we\ndevelop the following two approaches. 1) Anticipation Time Knowledge\nDistillation using the soft labels learned by the teacher model as knowledge to\nguide the student network to learn the information of anticipation time; 2)\nVerb-Noun Relation Module for building the relationship between verbs and\nnouns. Our method achieves state-of-the-art results on the testing set of\nEPIC-Kitchens Action Anticipation Challenge 2022.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zeyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Skeleton-aware Graph Convolutional Network for Human-Object Interaction Detection. (arXiv:2207.05733v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05733","description":"<p>Detecting human-object interactions is essential for comprehensive\nunderstanding of visual scenes. In particular, spatial connections between\nhumans and objects are important cues for reasoning interactions. To this end,\nwe propose a skeleton-aware graph convolutional network for human-object\ninteraction detection, named SGCN4HOI. Our network exploits the spatial\nconnections between human keypoints and object keypoints to capture their\nfine-grained structural interactions via graph convolutions. It fuses such\ngeometric features with visual features and spatial configuration features\nobtained from human-object pairs. Furthermore, to better preserve the object\nstructural information and facilitate human-object interaction detection, we\npropose a novel skeleton-based object keypoints representation. The performance\nof SGCN4HOI is evaluated in the public benchmark V-COCO dataset. Experimental\nresults show that the proposed approach outperforms the state-of-the-art\npose-based models and achieves competitive performance against other models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Manli Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_E/0/1/0/all/0/1\">Edmond S. L. Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Hubert P. H. Shum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer for NeRF-Based View Synthesis from a Single Input Image. (arXiv:2207.05736v1 [cs.CV])","link":"http://arxiv.org/abs/2207.05736","description":"<p>Although neural radiance fields (NeRF) have shown impressive advances for\nnovel view synthesis, most methods typically require multiple input images of\nthe same scene with accurate camera poses. In this work, we seek to\nsubstantially reduce the inputs to a single unposed image. Existing approaches\ncondition on local image features to reconstruct a 3D object, but often render\nblurry predictions at viewpoints that are far away from the source view. To\naddress this issue, we propose to leverage both the global and local features\nto form an expressive 3D representation. The global features are learned from a\nvision transformer, while the local features are extracted from a 2D\nconvolutional network. To synthesize a novel view, we train a multilayer\nperceptron (MLP) network conditioned on the learned 3D representation to\nperform volume rendering. This novel 3D representation allows the network to\nreconstruct unseen regions without enforcing constraints like symmetry or\ncanonical coordinate systems. Our method can render novel views from only a\nsingle input image and generalize across multiple object categories using a\nsingle model. Quantitative and qualitative evaluations demonstrate that the\nproposed method achieves state-of-the-art performance and renders richer\ndetails than existing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kai-En Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_Chen_L/0/1/0/all/0/1\">Lin Yen-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1\">Wei-Sheng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shih_Y/0/1/0/all/0/1\">Yi-Chang Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthi_R/0/1/0/all/0/1\">Ravi Ramamoorthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-structure bone segmentation in pediatric MR images with combined regularization from shape priors and adversarial network. (arXiv:2009.07092v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2009.07092","description":"<p>Morphological and diagnostic evaluation of pediatric musculoskeletal system\nis crucial in clinical practice. However, most segmentation models do not\nperform well on scarce pediatric imaging data. We propose a new pre-trained\nregularized convolutional encoder-decoder network for the challenging task of\nsegmenting heterogeneous pediatric magnetic resonance (MR) images. To this end,\nwe have conceived a novel optimization scheme for the segmentation network\nwhich comprises additional regularization terms to the loss function. In order\nto obtain globally consistent predictions, we incorporate a shape priors based\nregularization, derived from a non-linear shape representation learnt by an\nauto-encoder. Additionally, an adversarial regularization computed by a\ndiscriminator is integrated to encourage precise delineations. The proposed\nmethod is evaluated for the task of multi-bone segmentation on two scarce\npediatric imaging datasets from ankle and shoulder joints, comprising\npathological as well as healthy examinations. The proposed method performed\neither better or at par with previously proposed approaches for Dice,\nsensitivity, specificity, maximum symmetric surface distance, average symmetric\nsurface distance, and relative absolute volume difference metrics. We\nillustrate that the proposed approach can be easily integrated into various\nbone segmentation strategies and can improve the prediction accuracy of models\npre-trained on large non-medical images databases. The obtained results bring\nnew perspectives for the management of pediatric musculoskeletal disorders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Boutillon_A/0/1/0/all/0/1\">Arnaud Boutillon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Borotikar_B/0/1/0/all/0/1\">Bhushan Borotikar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burdin_V/0/1/0/all/0/1\">Val&#xe9;rie Burdin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Conze_P/0/1/0/all/0/1\">Pierre-Henri Conze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biometrics in the Era of COVID-19: Challenges and Opportunities. (arXiv:2102.09258v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2102.09258","description":"<p>Since early 2020 the COVID-19 pandemic has had a considerable impact on many\naspects of daily life. A range of different measures have been implemented\nworldwide to reduce the rate of new infections and to manage the pressure on\nnational health services. A primary strategy has been to reduce gatherings and\nthe potential for transmission through the prioritisation of remote working and\neducation. Enhanced hand hygiene and the use of facial masks have decreased the\nspread of pathogens when gatherings are unavoidable. These particular measures\npresent challenges for reliable biometric recognition, e.g. for facial-, voice-\nand hand-based biometrics. At the same time, new challenges create new\nopportunities and research directions, e.g. renewed interest in non-constrained\niris or periocular recognition, touch-less fingerprint- and vein-based\nauthentication and the use of biometric characteristics for disease detection.\nThis article presents an overview of the research carried out to address those\nchallenges and emerging opportunities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Barrero_M/0/1/0/all/0/1\">Marta Gomez-Barrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdowski_P/0/1/0/all/0/1\">Pawel Drozdowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patino_J/0/1/0/all/0/1\">Jose Patino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todisco_M/0/1/0/all/0/1\">Massimmiliano Todisco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nautsch_A/0/1/0/all/0/1\">Andras Nautsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priesnitz_J/0/1/0/all/0/1\">Jannis Priesnitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_N/0/1/0/all/0/1\">Nicholas Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Classification Network. (arXiv:2103.10994v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.10994","description":"<p>We present Self-Classifier -- a novel self-supervised end-to-end\nclassification learning approach. Self-Classifier learns labels and\nrepresentations simultaneously in a single-stage end-to-end manner by\noptimizing for same-class prediction of two augmented views of the same sample.\nTo guarantee non-degenerate solutions (i.e., solutions where all labels are\nassigned to the same class) we propose a mathematically motivated variant of\nthe cross-entropy loss that has a uniform prior asserted on the predicted\nlabels. In our theoretical analysis, we prove that degenerate solutions are not\nin the set of optimal solutions of our approach. Self-Classifier is simple to\nimplement and scalable. Unlike other popular unsupervised classification and\ncontrastive representation learning approaches, it does not require any form of\npre-training, expectation-maximization, pseudo-labeling, external clustering, a\nsecond network, stop-gradient operation, or negative pairs. Despite its\nsimplicity, our approach sets a new state of the art for unsupervised\nclassification of ImageNet; and even achieves comparable to state-of-the-art\nresults for unsupervised representation learning. Code is available at\nhttps://github.com/elad-amrani/self-classifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amrani_E/0/1/0/all/0/1\">Elad Amrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1\">Leonid Karlinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bronstein_A/0/1/0/all/0/1\">Alex Bronstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-supervised Part-Attention and Mentored Networks for Vehicle Re-Identification. (arXiv:2107.08228v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.08228","description":"<p>Vehicle re-identification (Re-ID) aims to retrieve images with the same\nvehicle ID across different cameras. Current part-level feature learning\nmethods typically detect vehicle parts via uniform division, outside tools, or\nattention modeling. However, such part features often require expensive\nadditional annotations and cause sub-optimal performance in case of unreliable\npart mask predictions. In this paper, we propose a weakly-supervised\nPart-Attention Network (PANet) and Part-Mentored Network (PMNet) for Vehicle\nRe-ID. Firstly, PANet localizes vehicle parts via part-relevant channel\nrecalibration and cluster-based mask generation without vehicle part\nsupervisory information. Secondly, PMNet leverages teacher-student guided\nlearning to distill vehicle part-specific features from PANet and performs\nmulti-scale global-part feature extraction. During inference, PMNet can\nadaptively extract discriminative part features without part localization by\nPANet, preventing unstable part mask predictions. We address this Re-ID issue\nas a multi-task problem and adopt Homoscedastic Uncertainty to learn the\noptimal weighing of ID losses. Experiments are conducted on two public\nbenchmarks, showing that our approach outperforms recent methods, which require\nno extra annotations by an average increase of 3.0% in CMC@5 on VehicleID and\nover 1.4% in mAP on VeRi776. Moreover, our method can extend to the occluded\nvehicle Re-ID task and exhibits good generalization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Lisha Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_L/0/1/0/all/0/1\">Lap-Pui Chau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Video Compression using GANs for Detail Synthesis and Propagation. (arXiv:2107.12038v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.12038","description":"<p>We present the first neural video compression method based on generative\nadversarial networks (GANs). Our approach significantly outperforms previous\nneural and non-neural video compression methods in a user study, setting a new\nstate-of-the-art in visual quality for neural methods. We show that the GAN\nloss is crucial to obtain this high visual quality. Two components make the GAN\nloss effective: we i) synthesize detail by conditioning the generator on a\nlatent extracted from the warped previous reconstruction to then ii) propagate\nthis detail with high-quality flow. We find that user studies are required to\ncompare methods, i.e., none of our quantitative metrics were able to predict\nall studies. We present the network design choices in detail, and ablate them\nwith user studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mentzer_F/0/1/0/all/0/1\">Fabian Mentzer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agustsson_E/0/1/0/all/0/1\">Eirikur Agustsson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Balle_J/0/1/0/all/0/1\">Johannes Ball&#xe9;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Minnen_D/0/1/0/all/0/1\">David Minnen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Johnston_N/0/1/0/all/0/1\">Nick Johnston</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Toderici_G/0/1/0/all/0/1\">George Toderici</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RigNet: Repetitive Image Guided Network for Depth Completion. (arXiv:2107.13802v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.13802","description":"<p>Depth completion deals with the problem of recovering dense depth maps from\nsparse ones, where color images are often used to facilitate this task. Recent\napproaches mainly focus on image guided learning frameworks to predict dense\ndepth. However, blurry guidance in the image and unclear structure in the depth\nstill impede the performance of the image guided frameworks. To tackle these\nproblems, we explore a repetitive design in our image guided network to\ngradually and sufficiently recover depth values. Specifically, the repetition\nis embodied in both the image guidance branch and depth generation branch. In\nthe former branch, we design a repetitive hourglass network to extract\ndiscriminative image features of complex environments, which can provide\npowerful contextual instruction for depth prediction. In the latter branch, we\nintroduce a repetitive guidance module based on dynamic convolution, in which\nan efficient convolution factorization is proposed to simultaneously reduce its\ncomplexity and progressively model high-frequency structures. Extensive\nexperiments show that our method achieves superior or competitive results on\nKITTI benchmark and NYUv2 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Baobei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Similarity Encoder for Deep GAN Inversion. (arXiv:2108.10201v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10201","description":"<p>Current deep generative adversarial networks (GANs) can synthesize\nhigh-quality (HQ) images, so learning representation with GANs is favorable.\nGAN inversion is one of emerging approaches that study how to invert images\ninto latent space. Existing GAN encoders can invert images on StyleGAN, but\ncannot adapt to other deep GANs. We propose a novel approach to address this\nissue. By evaluating diverse similarity in latent vectors and images, we design\nan adaptive encoder, named diverse similarity encoder (DSE), that can be\nexpanded to a variety of state-of-the-art GANs. DSE makes GANs reconstruct\nhigher fidelity images from HQ images, no matter whether they are synthesized\nor real images. DSE has unified convolutional blocks and adapts well to\nmainstream deep GANs, e.g., PGGAN, StyleGAN, and BigGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenmin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cell Multi-Bernoulli (Cell-MB) Sensor Control for Multi-object Search-While-Tracking (SWT). (arXiv:2108.11236v2 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2108.11236","description":"<p>Information-driven control can be used to develop intelligent sensors that\ncan optimize their measurement value based on environmental feedback. In object\ntracking applications, sensor actions are chosen based on the expected\nreduction in uncertainty also known as information gain. Random finite set\n(RFS) theory provides a formalism for quantifying and estimating information\ngain in multi-object tracking problems. However, estimating information gain in\nthese applications remains computationally challenging. This paper presents a\nnew tractable approximation of the RFS expected information gain applicable to\nsensor control for multi-object search and tracking. Unlike existing RFS\napproaches, the information gain approximation presented in this paper\nconsiders the contributions of non-ideal noisy measurements, missed detections,\nfalse alarms, and object appearance/disappearance. The effectiveness of the\ninformation-driven sensor control is demonstrated through two multi-vehicle\nsearch-while-tracking experiments using real video data from remote terrestrial\nand satellite sensors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+LeGrand_K/0/1/0/all/0/1\">Keith A. LeGrand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_P/0/1/0/all/0/1\">Pingping Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferrari_S/0/1/0/all/0/1\">Silvia Ferrari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PC$^2$-PU: Patch Correlation and Point Correlation for Effective Point Cloud Upsampling. (arXiv:2109.09337v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09337","description":"<p>Point cloud upsampling is to densify a sparse point set acquired from 3D\nsensors, providing a denser representation for the underlying surface. Existing\nmethods divide the input points into small patches and upsample each patch\nseparately, however, ignoring the global spatial consistency between patches.\nIn this paper, we present a novel method PC$^2$-PU, which explores\npatch-to-patch and point-to-point correlations for more effective and robust\npoint cloud upsampling. Specifically, our network has two appealing designs:\n(i) We take adjacent patches as supplementary inputs to compensate the loss\nstructure information within a single patch and introduce a Patch Correlation\nModule to capture the difference and similarity between patches. (ii) After\naugmenting each patch's geometry, we further introduce a Point Correlation\nModule to reveal the relationship of points inside each patch to maintain the\nlocal spatial consistency. Extensive experiments on both synthetic and real\nscanned datasets demonstrate that our method surpasses previous upsampling\nmethods, particularly with the noisy inputs. The code and data are at\n\\url{https://github.com/chenlongwhu/PC2-PU.git}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chen Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruihui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bisheng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Propagating State Uncertainty Through Trajectory Forecasting. (arXiv:2110.03267v4 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2110.03267","description":"<p>Uncertainty pervades through the modern robotic autonomy stack, with nearly\nevery component (e.g., sensors, detection, classification, tracking, behavior\nprediction) producing continuous or discrete probabilistic distributions.\nTrajectory forecasting, in particular, is surrounded by uncertainty as its\ninputs are produced by (noisy) upstream perception and its outputs are\npredictions that are often probabilistic for use in downstream planning.\nHowever, most trajectory forecasting methods do not account for upstream\nuncertainty, instead taking only the most-likely values. As a result,\nperceptual uncertainties are not propagated through forecasting and predictions\nare frequently overconfident. To address this, we present a novel method for\nincorporating perceptual state uncertainty in trajectory forecasting, a key\ncomponent of which is a new statistical distance-based loss function which\nencourages predicting uncertainties that better match upstream perception. We\nevaluate our approach both in illustrative simulations and on large-scale,\nreal-world data, demonstrating its efficacy in propagating perceptual state\nuncertainty through prediction and producing more calibrated predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivanovic_B/0/1/0/all/0/1\">Boris Ivanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yifeng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_S/0/1/0/all/0/1\">Shubham Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarty_P/0/1/0/all/0/1\">Punarjay Chakravarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MM-Pyramid: Multimodal Pyramid Attentional Network for Audio-Visual Event Localization and Video Parsing. (arXiv:2111.12374v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12374","description":"<p>Recognizing and localizing events in videos is a fundamental task for video\nunderstanding. Since events may occur in auditory and visual modalities,\nmultimodal detailed perception is essential for complete scene comprehension.\nMost previous works attempted to analyze videos from a holistic perspective.\nHowever, they do not consider semantic information at multiple scales, which\nmakes the model difficult to localize events in different lengths. In this\npaper, we present a Multimodal Pyramid Attentional Network\n(\\textbf{MM-Pyramid}) for event localization. Specifically, we first propose\nthe attentive feature pyramid module. This module captures temporal pyramid\nfeatures via several stacking pyramid units, each of them is composed of a\nfixed-size attention block and dilated convolution block. We also design an\nadaptive semantic fusion module, which leverages a unit-level attention block\nand a selective fusion block to integrate pyramid features interactively.\nExtensive experiments on audio-visual event localization and weakly-supervised\naudio-visual video parsing tasks verify the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiashuo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Ying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui-Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Rui Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuejie Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"diffConv: Analyzing Irregular Point Clouds with an Irregular View. (arXiv:2111.14658v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14658","description":"<p>Standard spatial convolutions assume input data with a regular neighborhood\nstructure. Existing methods typically generalize convolution to the irregular\npoint cloud domain by fixing a regular \"view\" through e.g. a fixed neighborhood\nsize, where the convolution kernel size remains the same for each point.\nHowever, since point clouds are not as structured as images, the fixed neighbor\nnumber gives an unfortunate inductive bias. We present a novel graph\nconvolution named Difference Graph Convolution (diffConv), which does not rely\non a regular view. diffConv operates on spatially-varying and density-dilated\nneighborhoods, which are further adapted by a learned masked attention\nmechanism. Experiments show that our model is very robust to the noise,\nobtaining state-of-the-art performance in 3D shape classification and scene\nunderstanding tasks, along with a faster inference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Manxi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1\">Aasa Feragen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EAGAN: Efficient Two-stage Evolutionary Architecture Search for GANs. (arXiv:2111.15097v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15097","description":"<p>Generative adversarial networks (GANs) have proven successful in image\ngeneration tasks. However, GAN training is inherently unstable. Although many\nworks try to stabilize it by manually modifying GAN architecture, it requires\nmuch expertise. Neural architecture search (NAS) has become an attractive\nsolution to search GANs automatically. The early NAS-GANs search only\ngenerators to reduce search complexity but lead to a sub-optimal GAN. Some\nrecent works try to search both generator (G) and discriminator (D), but they\nsuffer from the instability of GAN training. To alleviate the instability, we\npropose an efficient two-stage evolutionary algorithm-based NAS framework to\nsearch GANs, namely EAGAN. We decouple the search of G and D into two stages,\nwhere stage-1 searches G with a fixed D and adopts the many-to-one training\nstrategy, and stage-2 searches D with the optimal G found in stage-1 and adopts\nthe one-to-one training and weight-resetting strategies to enhance the\nstability of GAN training. Both stages use the non-dominated sorting method to\nproduce Pareto-front architectures under multiple objectives (e.g., model size,\nInception Score (IS), and Fr\\'echet Inception Distance (FID)). EAGAN is applied\nto the unconditional image generation task and can efficiently finish the\nsearch on the CIFAR-10 dataset in 1.2 GPU days. Our searched GANs achieve\ncompetitive results (IS=8.81$\\pm$0.10, FID=9.91) on the CIFAR-10 dataset and\nsurpass prior NAS-GANs on the STL-10 dataset (IS=10.44$\\pm$0.087, FID=22.18).\nSource code: https://github.com/marsggbo/EAGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ying_G/0/1/0/all/0/1\">Guohao Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1\">Bin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaowen Chu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Channel Encoding Transformer for Point Cloud Analysis. (arXiv:2112.02507v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02507","description":"<p>Transformer plays an increasingly important role in various computer vision\nareas and remarkable achievements have also been made in point cloud analysis.\nSince they mainly focus on point-wise transformer, an adaptive channel encoding\ntransformer is proposed in this paper. Specifically, a channel convolution\ncalled Transformer-Conv is designed to encode the channel. It can encode\nfeature channels by capturing the potential relationship between coordinates\nand features. Compared with simply assigning attention weight to each channel,\nour method aims to encode the channel adaptively. In addition, our network\nadopts the neighborhood search method of low-level and high-level dual semantic\nreceptive fields to improve the performance. Extensive experiments show that\nour method is superior to state-of-the-art point cloud classification and\nsegmentation methods on three benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guoquan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hezhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yanxin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jianwei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Image Restoration by Revisiting Global Information Aggregation. (arXiv:2112.04491v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.04491","description":"<p>Global operations, such as global average pooling, are widely used in\ntop-performance image restorers. They aggregate global information from input\nfeatures along entire spatial dimensions but behave differently during training\nand inference in image restoration tasks: they are based on different regions,\nnamely the cropped patches (from images) and the full-resolution images. This\npaper revisits global information aggregation and finds that the image-based\nfeatures during inference have a different distribution than the patch-based\nfeatures during training. This train-test inconsistency negatively impacts the\nperformance of models, which is severely overlooked by previous works. To\nreduce the inconsistency and improve test-time performance, we propose a simple\nmethod called Test-time Local Converter (TLC). Our TLC converts global\noperations to local ones only during inference so that they aggregate features\nwithin local spatial regions rather than the entire large images. The proposed\nmethod can be applied to various global modules (e.g., normalization, channel\nand spatial attention) with negligible costs. Without the need for any\nfine-tuning, TLC improves state-of-the-art results on several image restoration\ntasks, including single-image motion deblurring, video deblurring, defocus\ndeblurring, and image denoising. In particular, with TLC, our Restormer-Local\nimproves the state-of-the-art result in single image deblurring from 32.92 dB\nto 33.57 dB on GoPro dataset. The code is available at\nhttps://github.com/megvii-research/tlc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chu_X/0/1/0/all/0/1\">Xiaojie Chu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Liangyu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chengpeng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_X/0/1/0/all/0/1\">Xin Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PeopleSansPeople: A Synthetic Data Generator for Human-Centric Computer Vision. (arXiv:2112.09290v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09290","description":"<p>In recent years, person detection and human pose estimation have made great\nstrides, helped by large-scale labeled datasets. However, these datasets had no\nguarantees or analysis of human activities, poses, or context diversity.\nAdditionally, privacy, legal, safety, and ethical concerns may limit the\nability to collect more human data. An emerging alternative to real-world data\nthat alleviates some of these issues is synthetic data. However, creation of\nsynthetic data generators is incredibly challenging and prevents researchers\nfrom exploring their usefulness. Therefore, we release a human-centric\nsynthetic data generator PeopleSansPeople which contains simulation-ready 3D\nhuman assets, a parameterized lighting and camera system, and generates 2D and\n3D bounding box, instance and semantic segmentation, and COCO pose labels.\nUsing PeopleSansPeople, we performed benchmark synthetic data training using a\nDetectron2 Keypoint R-CNN variant [1]. We found that pre-training a network\nusing synthetic data and fine-tuning on various sizes of real-world data\nresulted in a keypoint AP increase of $+38.03$ ($44.43 \\pm 0.17$ vs. $6.40$)\nfor few-shot transfer (limited subsets of COCO-person train [2]), and an\nincrease of $+1.47$ ($63.47 \\pm 0.19$ vs. $62.00$) for abundant real data\nregimes, outperforming models trained with the same real data alone. We also\nfound that our models outperformed those pre-trained with ImageNet with a\nkeypoint AP increase of $+22.53$ ($44.43 \\pm 0.17$ vs. $21.90$) for few-shot\ntransfer and $+1.07$ ($63.47 \\pm 0.19$ vs. $62.40$) for abundant real data\nregimes. This freely-available data generator should enable a wide range of\nresearch into the emerging field of simulation to real transfer learning in the\ncritical area of human-centric computer vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ebadi_S/0/1/0/all/0/1\">Salehe Erfanian Ebadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jhang_Y/0/1/0/all/0/1\">You-Cyuan Jhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zook_A/0/1/0/all/0/1\">Alex Zook</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhakad_S/0/1/0/all/0/1\">Saurav Dhakad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crespi_A/0/1/0/all/0/1\">Adam Crespi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parisi_P/0/1/0/all/0/1\">Pete Parisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borkman_S/0/1/0/all/0/1\">Steven Borkman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogins_J/0/1/0/all/0/1\">Jonathan Hogins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_S/0/1/0/all/0/1\">Sujoy Ganguly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Cross-view Mutual Distillation for Self-supervised Medical CT Synthesis. (arXiv:2112.10325v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.10325","description":"<p>Due to the constraints of the imaging device and high cost in operation time,\ncomputer tomography (CT) scans are usually acquired with low intra-slice\nresolution. Improving the intra-slice resolution is beneficial to the disease\ndiagnosis for both human experts and computer-aided systems. To this end, this\npaper builds a novel medical slice synthesis to increase the between-slice\nresolution. Considering that the ground-truth intermediate medical slices are\nalways absent in clinical practice, we introduce the incremental cross-view\nmutual distillation strategy to accomplish this task in the self-supervised\nlearning manner. Specifically, we model this problem from three different\nviews: slice-wise interpolation from axial view and pixel-wise interpolation\nfrom coronal and sagittal views. Under this circumstance, the models learned\nfrom different views can distill valuable knowledge to guide the learning\nprocesses of each other. We can repeat this process to make the models\nsynthesize intermediate slice data with increasing inter-slice resolution. To\ndemonstrate the effectiveness of the proposed approach, we conduct\ncomprehensive experiments on a large-scale CT dataset. Quantitative and\nqualitative comparison results show that our method outperforms\nstate-of-the-art algorithms by clear margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fang_C/0/1/0/all/0/1\">Chaowei Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_Y/0/1/0/all/0/1\">Yixuan Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facial-Sketch Synthesis: A New Challenge. (arXiv:2112.15439v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.15439","description":"<p>This paper aims to conduct a comprehensive study on facial-sketch synthesis\n(FSS). However, due to the high costs of obtaining hand-drawn sketch datasets,\nthere lacks a complete benchmark for assessing the development of FSS\nalgorithms over the last decade. We first introduce a high-quality dataset for\nFSS, named FS2K, which consists of 2,104 image-sketch pairs spanning three\ntypes of sketch styles, image backgrounds, lighting conditions, skin colors,\nand facial attributes. FS2K differs from previous FSS datasets in difficulty,\ndiversity, and scalability and should thus facilitate the progress of FSS\nresearch. Second, we present the largest-scale FSS investigation by reviewing\n89 classical methods, including 25 handcrafted feature-based facial-sketch\nsynthesis approaches, 29 general translation methods, and 35 image-to-sketch\napproaches. Besides, we elaborate comprehensive experiments on the existing 19\ncutting-edge models. Third, we present a simple baseline for FSS, named FSGAN.\nWith only two straightforward components, i.e., facial-aware masking and\nstyle-vector expansion, FSGAN surpasses the performance of all previous\nstate-of-the-art models on the proposed FS2K dataset by a large margin.\nFinally, we conclude with lessons learned over the past years and point out\nseveral unsolved challenges. Our code is available at\nhttps://github.com/DengPingFan/FSGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziling Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_P/0/1/0/all/0/1\">Peng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xuebin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Persistent Homology for Breast Tumor Classification using Mammogram Scans. (arXiv:2201.02295v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.02295","description":"<p>An Important tool in the field topological data analysis is known as\npersistent Homology (PH) which is used to encode abstract representation of the\nhomology of data at different resolutions in the form of persistence diagram\n(PD). In this work we build more than one PD representation of a single image\nbased on a landmark selection method, known as local binary patterns, that\nencode different types of local textures from images. We employed different PD\nvectorizations using persistence landscapes, persistence images, persistence\nbinning (Betti Curve) and statistics. We tested the effectiveness of proposed\nlandmark based PH on two publicly available breast abnormality detection\ndatasets using mammogram scans. Sensitivity of landmark based PH obtained is\nover 90% in both datasets for the detection of abnormal breast scans. Finally,\nexperimental results give new insights on using different types of PD\nvectorizations which help in utilising PH in conjunction with machine learning\nclassifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Asaad_A/0/1/0/all/0/1\">Aras Asaad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_D/0/1/0/all/0/1\">Dashti Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Majeed_T/0/1/0/all/0/1\">Taban Majeed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rashid_R/0/1/0/all/0/1\">Rasber Rashid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Label Assignment for Object Detection by Combining Predicted IoUs and Anchor IoUs. (arXiv:2201.09396v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09396","description":"<p>Label assignment plays a significant role in modern object detection models.\nDetection models may yield totally different performances with different label\nassignment strategies. For anchor-based detection models, the IoU (Intersection\nover Union) threshold between the anchors and their corresponding ground truth\nbounding boxes is the key element since the positive samples and negative\nsamples are divided by the IoU threshold. Early object detectors simply utilize\nthe fixed threshold for all training samples, while recent detection algorithms\nfocus on adaptive thresholds based on the distribution of the IoUs to the\nground truth boxes. In this paper, we introduce a simple while effective\napproach to perform label assignment dynamically based on the training status\nwith predictions. By introducing the predictions in label assignment, more\nhigh-quality samples with higher IoUs to the ground truth objects are selected\nas the positive samples, which could reduce the discrepancy between the\nclassification scores and the IoU scores, and generate more high-quality\nboundary boxes. Our approach shows improvements in the performance of the\ndetection models with the adaptive label assignment algorithm and lower\nbounding box losses for those positive samples, indicating more samples with\nhigher-quality predicted boxes are selected as positives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1\">Bo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharda_A/0/1/0/all/0/1\">Ajay Sharda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Global Ranking-Aware Neural Architecture Ranker for Efficient Image Classifier Search. (arXiv:2201.12725v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12725","description":"<p>Neural Architecture Search (NAS) is a powerful tool for automating effective\nimage processing DNN designing. The ranking has been advocated to design an\nefficient performance predictor for NAS. The previous contrastive method solves\nthe ranking problem by comparing pairs of architectures and predicting their\nrelative performance. However, it only focuses on the rankings between two\ninvolved architectures and neglects the overall quality distributions of the\nsearch space, which may suffer generalization issues. A predictor, namely\nNeural Architecture Ranker (NAR) which concentrates on the global quality tier\nof specific architecture, is proposed to tackle such problems caused by the\nlocal perspective. The NAR explores the quality tiers of the search space\nglobally and classifies each individual to the tier they belong to according to\nits global ranking. Thus, the predictor gains the knowledge of the performance\ndistributions of the search space which helps to generalize its ranking ability\nto the datasets more easily. Meanwhile, the global quality distribution\nfacilitates the search phase by directly sampling candidates according to the\nstatistics of quality tiers, which is free of training a search algorithm,\ne.g., Reinforcement Learning (RL) or Evolutionary Algorithm (EA), thus it\nsimplifies the NAS pipeline and saves the computational overheads. The proposed\nNAR achieves better performance than the state-of-the-art methods on two widely\nused datasets for NAS research. On the vast search space of NAS-Bench-101, the\nNAR easily finds the architecture with top 0.01$\\unicode{x2030}$ performance\nonly by sampling. It also generalizes well to different image datasets of\nNAS-Bench-201, i.e., CIFAR-10, CIFAR-100, and ImageNet-16-120 by identifying\nthe optimal architectures for each of them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Bicheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shibo He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haoyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lilin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1\">Peng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiming Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CGiS-Net: Aggregating Colour, Geometry and Implicit Semantic Features for Indoor Place Recognition. (arXiv:2202.02070v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02070","description":"<p>We describe a novel approach to indoor place recognition from RGB point\nclouds based on aggregating low-level colour and geometry features with\nhigh-level implicit semantic features. It uses a 2-stage deep learning\nframework, in which the first stage is trained for the auxiliary task of\nsemantic segmentation and the second stage uses features from layers in the\nfirst stage to generate discriminate descriptors for place recognition. The\nauxiliary task encourages the features to be semantically meaningful, hence\naggregating the geometry and colour in the RGB point cloud data with implicit\nsemantic information. We use an indoor place recognition dataset derived from\nthe ScanNet dataset for training and evaluation, with a test set comprising\n3,608 point clouds generated from 100 different rooms. Comparison with a\ntraditional feature-based method and four state-of-the-art deep learning\nmethods demonstrate that our approach significantly outperforms all five\nmethods, achieving, for example, a top-3 average recall rate of 75% compared\nwith 41% for the closest rival method. Our code is available at:\nhttps://github.com/YuhangMing/Semantic-Indoor-Place-Recognition\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yuhang Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xingrui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calway_A/0/1/0/all/0/1\">Andrew Calway</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Equivariance versus Augmentation for Spherical Images. (arXiv:2202.03990v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.03990","description":"<p>We analyze the role of rotational equivariance in convolutional neural\nnetworks (CNNs) applied to spherical images. We compare the performance of the\ngroup equivariant networks known as S2CNNs and standard non-equivariant CNNs\ntrained with an increasing amount of data augmentation. The chosen\narchitectures can be considered baseline references for the respective design\nparadigms. Our models are trained and evaluated on single or multiple items\nfrom the MNIST or FashionMNIST dataset projected onto the sphere. For the task\nof image classification, which is inherently rotationally invariant, we find\nthat by considerably increasing the amount of data augmentation and the size of\nthe networks, it is possible for the standard CNNs to reach at least the same\nperformance as the equivariant network. In contrast, for the inherently\nequivariant task of semantic segmentation, the non-equivariant networks are\nconsistently outperformed by the equivariant networks with significantly fewer\nparameters. We also analyze and compare the inference latency and training\ntimes of the different networks, enabling detailed tradeoff considerations\nbetween equivariant architectures and data augmentation for practical problems.\nThe equivariant spherical networks used in the experiments are available at\nhttps://github.com/JanEGerken/sem_seg_s2cnn .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gerken_J/0/1/0/all/0/1\">Jan E. Gerken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlsson_O/0/1/0/all/0/1\">Oscar Carlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linander_H/0/1/0/all/0/1\">Hampus Linander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohlsson_F/0/1/0/all/0/1\">Fredrik Ohlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_C/0/1/0/all/0/1\">Christoffer Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Persson_D/0/1/0/all/0/1\">Daniel Persson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Metric Learning-Based Semi-Supervised Regression With Alternate Learning. (arXiv:2202.11388v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11388","description":"<p>This paper introduces a novel deep metric learning-based semi-supervised\nregression (DML-S2R) method for parameter estimation problems. The proposed\nDML-S2R method aims to mitigate the problems of insufficient amount of labeled\nsamples without collecting any additional sample with a target value. To this\nend, it is made up of two main steps: i) pairwise similarity modeling with\nscarce labeled data; and ii) triplet-based metric learning with abundant\nunlabeled data. The first step aims to model pairwise sample similarities by\nusing a small number of labeled samples. This is achieved by estimating the\ntarget value differences of labeled samples with a Siamese neural network\n(SNN). The second step aims to learn a triplet-based metric space (in which\nsimilar samples are close to each other and dissimilar samples are far apart\nfrom each other) when the number of labeled samples is insufficient. This is\nachieved by employing the SNN of the first step for triplet-based deep metric\nlearning that exploits not only labeled samples but also unlabeled samples. For\nthe end-to-end training of DML-S2R, we investigate an alternate learning\nstrategy for the two steps. Due to this strategy, the encoded information in\neach step becomes a guidance for learning phase of the other step. The\nexperimental results confirm the success of DML-S2R compared to the\nstate-of-the-art semi-supervised regression methods. The code of the proposed\nmethod is publicly available at https://git.tu-berlin.de/rsim/DML-S2R.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1\">Adina Zell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumbul_G/0/1/0/all/0/1\">Gencer Sumbul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1\">Beg&#xfc;m Demir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Self-Supervised Cross-Modal Image Retrieval Method In Remote Sensing. (arXiv:2202.11429v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11429","description":"<p>Due to the availability of multi-modal remote sensing (RS) image archives,\none of the most important research topics is the development of cross-modal RS\nimage retrieval (CM-RSIR) methods that search semantically similar images\nacross different modalities. Existing CM-RSIR methods require the availability\nof a high quality and quantity of annotated training images. The collection of\na sufficient number of reliable labeled images is time consuming, complex and\ncostly in operational scenarios, and can significantly affect the final\naccuracy of CM-RSIR. In this paper, we introduce a novel self-supervised\nCM-RSIR method that aims to: i) model mutual-information between different\nmodalities in a self-supervised manner; ii) retain the distributions of\nmodal-specific feature spaces similar to each other; and iii) define the most\nsimilar images within each modality without requiring any annotated training\nimage. To this end, we propose a novel objective including three loss functions\nthat simultaneously: i) maximize mutual information of different modalities for\ninter-modal similarity preservation; ii) minimize the angular distance of\nmulti-modal image tuples for the elimination of inter-modal discrepancies; and\niii) increase cosine similarity of the most similar images within each modality\nfor the characterization of intra-modal similarities. Experimental results show\nthe effectiveness of the proposed method compared to state-of-the-art methods.\nThe code of the proposed method is publicly available at\nhttps://git.tu-berlin.de/rsim/SS-CM-RSIR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sumbul_G/0/1/0/all/0/1\">Gencer Sumbul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1\">Markus M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1\">Beg&#xfc;m Demir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SWIS: Self-Supervised Representation Learning For Writer Independent Offline Signature Verification. (arXiv:2202.13078v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13078","description":"<p>Writer independent offline signature verification is one of the most\nchallenging tasks in pattern recognition as there is often a scarcity of\ntraining data. To handle such data scarcity problem, in this paper, we propose\na novel self-supervised learning (SSL) framework for writer independent offline\nsignature verification. To our knowledge, this is the first attempt to utilize\nself-supervised setting for the signature verification task. The objective of\nself-supervised representation learning from the signature images is achieved\nby minimizing the cross-covariance between two random variables belonging to\ndifferent feature directions and ensuring a positive cross-covariance between\nthe random variables denoting the same feature direction. This ensures that the\nfeatures are decorrelated linearly and the redundant information is discarded.\nThrough experimental results on different data sets, we obtained encouraging\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manna_S/0/1/0/all/0/1\">Siladittya Manna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1\">Soumitri Chattopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Saumik Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1\">Umapada Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Hard Example Mining Approach for Single Shot Object Detectors. (arXiv:2202.13080v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13080","description":"<p>Hard example mining methods generally improve the performance of the object\ndetectors, which suffer from imbalanced training sets. In this work, two\nexisting hard example mining approaches (LRM and focal loss, FL) are adapted\nand combined in a state-of-the-art real-time object detector, YOLOv5. The\neffectiveness of the proposed approach for improving the performance on hard\nexamples is extensively evaluated. The proposed method increases mAP by 3%\ncompared to using the original loss function and around 1-2% compared to using\nthe hard-mining methods (LRM or FL) individually on 2021 Anti-UAV Challenge\nDataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koksal_A/0/1/0/all/0/1\">Aybora Koksal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuzcuoglu_O/0/1/0/all/0/1\">Onder Tuzcuoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ince_K/0/1/0/all/0/1\">Kutalmis Gokalp Ince</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ataseven_Y/0/1/0/all/0/1\">Yoldas Ataseven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alatan_A/0/1/0/all/0/1\">A. Aydin Alatan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constrained unsupervised anomaly segmentation. (arXiv:2203.01671v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.01671","description":"<p>Current unsupervised anomaly localization approaches rely on generative\nmodels to learn the distribution of normal images, which is later used to\nidentify potential anomalous regions derived from errors on the reconstructed\nimages. However, a main limitation of nearly all prior literature is the need\nof employing anomalous images to set a class-specific threshold to locate the\nanomalies. This limits their usability in realistic scenarios, where only\nnormal data is typically accessible. Despite this major drawback, only a\nhandful of works have addressed this limitation, by integrating supervision on\nattention maps during training. In this work, we propose a novel formulation\nthat does not require accessing images with abnormalities to define the\nthreshold. Furthermore, and in contrast to very recent work, the proposed\nconstraint is formulated in a more principled manner, leveraging well-known\nknowledge in constrained optimization. In particular, the equality constraint\non the attention maps in prior work is replaced by an inequality constraint,\nwhich allows more flexibility. In addition, to address the limitations of\npenalty-based functions we employ an extension of the popular log-barrier\nmethods to handle the constraint. Last, we propose an alternative\nregularization term that maximizes the Shannon entropy of the attention maps,\nreducing the amount of hyperparameters of the proposed model. Comprehensive\nexperiments on two publicly available datasets on brain lesion segmentation\ndemonstrate that the proposed approach substantially outperforms relevant\nliterature, establishing new state-of-the-art results for unsupervised lesion\nsegmentation, and without the need to access anomalous images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Silva_Rodriguez_J/0/1/0/all/0/1\">Julio Silva-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1\">Valery Naranjo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Highly Accurate Dichotomous Image Segmentation. (arXiv:2203.03041v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03041","description":"<p>We present a systematic study on a new task called dichotomous image\nsegmentation (DIS) , which aims to segment highly accurate objects from natural\nimages. To this end, we collected the first large-scale DIS dataset, called\nDIS5K, which contains 5,470 high-resolution (e.g., 2K, 4K or larger) images\ncovering camouflaged, salient, or meticulous objects in various backgrounds.\nDIS is annotated with extremely fine-grained labels. Besides, we introduce a\nsimple intermediate supervision baseline (IS-Net) using both feature-level and\nmask-level guidance for DIS model training. IS-Net outperforms various\ncutting-edge baselines on the proposed DIS5K, making it a general self-learned\nsupervision network that can facilitate future research in DIS. Further, we\ndesign a new metric called human correction efforts (HCE) which approximates\nthe number of mouse clicking operations required to correct the false positives\nand false negatives. HCE is utilized to measure the gap between models and\nreal-world applications and thus can complement existing metrics. Finally, we\nconduct the largest-scale benchmark, evaluating 16 representative segmentation\nmodels, providing a more insightful discussion regarding object complexities,\nand showing several potential applications (e.g., background removal, art\ndesign, 3D reconstruction). Hoping these efforts can open up promising\ndirections for both academic and industries. Project page:\nhttps://xuebinqin.github.io/dis/index.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xuebin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaobin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_a/0/1/0/all/0/1\">and Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CF-ViT: A General Coarse-to-Fine Method for Vision Transformer. (arXiv:2203.03821v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03821","description":"<p>Vision Transformers (ViT) have made many breakthroughs in computer vision\ntasks. However, considerable redundancy arises in the spatial dimension of an\ninput image, leading to massive computational costs. Therefore, We propose a\ncoarse-to-fine vision transformer (CF-ViT) to relieve computational burden\nwhile retaining performance in this paper. Our proposed CF-ViT is motivated by\ntwo important observations in modern ViT models: (1) The coarse-grained patch\nsplitting can locate informative regions of an input image. (2) Most images can\nbe well recognized by a ViT model in a small-length token sequence. Therefore,\nour CF-ViT implements network inference in a two-stage manner. At coarse\ninference stage, an input image is split into a small-length patch sequence for\na computationally economical classification. If not well recognized, the\ninformative patches are identified and further re-split in a fine-grained\ngranularity. Extensive experiments demonstrate the efficacy of our CF-ViT. For\nexample, without any compromise on performance, CF-ViT reduces 53% FLOPs of\nLV-ViT, and also achieves 2.01x throughput.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mengzhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PaCC-Net: Position Aware Circular Convolution with Merits from ConvNets and Transformer. (arXiv:2203.03952v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03952","description":"<p>Recently, vision transformers started to show impressive results which\noutperform large convolution based models significantly. However, in the area\nof small models for mobile or resource constrained devices, ConvNet still has\nits own advantages in both performance and model complexity. We propose\nPaCC-Net, a pure ConvNet based backbone model that further strengthens these\nadvantages by fusing the merits of vision transformers into ConvNets.\nSpecifically, we propose position aware circular convolution (PaCC), a\nlight-weight convolution op which boasts a global receptive field while\nproducing location sensitive features as in local convolutions. We combine the\nPaCCs and squeeze-exictation ops to form a meta-former like model block, which\nfurther has the attention mechanism like transformers. The aforementioned block\ncan be used in plug-and-play manner to replace relevant blocks in ConvNets or\ntransformers. Experiment results show that the proposed PaCC-Net achieves\nbetter performance than popular light-weight ConvNets and vision transformer\nbased models in common vision tasks and datasets, while having fewer parameters\nand faster inference speed. For classification on ImageNet-1k, PaCC-Net\nachieves 78.6% top-1 accuracy with about 5.0 million parameters, saving 11%\nparameters and 13% computational cost but gaining 0.2% higher accuracy and 23%\nfaster inference speed (on ARM based Rockchip RK3288) compared with MobileViT,\nand uses only 0.5 times parameters but gaining 2.7% accuracy compared with\nDeIT. On MS-COCO object detection and PASCAL VOC segmentation tasks, PaCC-Net\nalso shows better performance. Source code is available at\nhttps://github.com/hkzhang91/PaCC-Net\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haokui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenze Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unpaired Deep Image Dehazing Using Contrastive Disentanglement Learning. (arXiv:2203.07677v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.07677","description":"<p>We offer a practical unpaired learning based image dehazing network from an\nunpaired set of clear and hazy images. This paper provides a new perspective to\ntreat image dehazing as a two-class separated factor disentanglement task, i.e,\nthe task-relevant factor of clear image reconstruction and the task-irrelevant\nfactor of haze-relevant distribution. To achieve the disentanglement of these\ntwo-class factors in deep feature space, contrastive learning is introduced\ninto a CycleGAN framework to learn disentangled representations by guiding the\ngenerated images to be associated with latent factors. With such formulation,\nthe proposed contrastive disentangled dehazing method (CDD-GAN) employs\nnegative generators to cooperate with the encoder network to update\nalternately, so as to produce a queue of challenging negative adversaries. Then\nthese negative adversaries are trained end-to-end together with the backbone\nrepresentation network to enhance the discriminative information and promote\nfactor disentanglement performance by maximizing the adversarial contrastive\nloss. During the training, we further show that hard negative examples can\nsuppress the task-irrelevant factors and unpaired clear exemples can enhance\nthe task-relevant factors, in order to better facilitate haze removal and help\nimage restoration. Extensive experiments on both synthetic and real-world\ndatasets demonstrate that our method performs favorably against existing\nunpaired dehazing baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_Z/0/1/0/all/0/1\">Zhentao Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_P/0/1/0/all/0/1\">Pengpeng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_L/0/1/0/all/0/1\">Longgang Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kong_C/0/1/0/all/0/1\">Caihua Kong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhuoran Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yufeng Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yufeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupled Knowledge Distillation. (arXiv:2203.08679v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08679","description":"<p>State-of-the-art distillation methods are mainly based on distilling deep\nfeatures from intermediate layers, while the significance of logit distillation\nis greatly overlooked. To provide a novel viewpoint to study logit\ndistillation, we reformulate the classical KD loss into two parts, i.e., target\nclass knowledge distillation (TCKD) and non-target class knowledge distillation\n(NCKD). We empirically investigate and prove the effects of the two parts: TCKD\ntransfers knowledge concerning the \"difficulty\" of training samples, while NCKD\nis the prominent reason why logit distillation works. More importantly, we\nreveal that the classical KD loss is a coupled formulation, which (1)\nsuppresses the effectiveness of NCKD and (2) limits the flexibility to balance\nthese two parts. To address these issues, we present Decoupled Knowledge\nDistillation (DKD), enabling TCKD and NCKD to play their roles more efficiently\nand flexibly. Compared with complex feature-based methods, our DKD achieves\ncomparable or even better results and has better training efficiency on\nCIFAR-100, ImageNet, and MS-COCO datasets for image classification and object\ndetection tasks. This paper proves the great potential of logit distillation,\nand we hope it will be helpful for future research. The code is available at\nhttps://github.com/megvii-research/mdistiller.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Borui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1\">Quan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Renjie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yiyu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiajun Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PanoFormer: Panorama Transformer for Indoor 360 Depth Estimation. (arXiv:2203.09283v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09283","description":"<p>Existing panoramic depth estimation methods based on convolutional neural\nnetworks (CNNs) focus on removing panoramic distortions, failing to perceive\npanoramic structures efficiently due to the fixed receptive field in CNNs. This\npaper proposes the panorama transformer (named PanoFormer) to estimate the\ndepth in panorama images, with tangent patches from spherical domain, learnable\ntoken flows, and panorama specific metrics. In particular, we divide patches on\nthe spherical tangent domain into tokens to reduce the negative effect of\npanoramic distortions. Since the geometric structures are essential for depth\nestimation, a self-attention module is redesigned with an additional learnable\ntoken flow. In addition, considering the characteristic of the spherical\ndomain, we present two panorama-specific metrics to comprehensively evaluate\nthe panoramic depth estimation models' performance. Extensive experiments\ndemonstrate that our approach significantly outperforms the state-of-the-art\n(SOTA) methods. Furthermore, the proposed method can be effectively extended to\nsolve semantic panorama segmentation, a similar pixel2pixel task. Code will be\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhijie Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Lang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zishuo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion. (arXiv:2203.09855v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09855","description":"<p>In this paper, we formulate a potentially valuable panoramic depth completion\n(PDC) task as panoramic 3D cameras often produce 360{\\deg} depth with missing\ndata in complex scenes. Its goal is to recover dense panoramic depths from raw\nsparse ones and panoramic RGB images. To deal with the PDC task, we train a\ndeep network that takes both depth and image as inputs for the dense panoramic\ndepth recovery. However, it needs to face a challenging optimization problem of\nthe network parameters due to its non-convex objective function. To address\nthis problem, we propose a simple yet effective approach termed M{^3}PT:\nmulti-modal masked pre-training. Specifically, during pre-training, we\nsimultaneously cover up patches of the panoramic RGB image and sparse depth by\nshared random mask, then reconstruct the sparse depth in the masked regions. To\nour best knowledge, it is the first time that we show the effectiveness of\nmasked pre-training in a multi-modal vision task, instead of the single-modal\ntask resolved by masked autoencoders (MAE). Different from MAE where\nfine-tuning completely discards the decoder part of pre-training, there is no\narchitectural difference between the pre-training and fine-tuning stages in our\nM$^{3}$PT as they only differ in the prediction density, which potentially\nmakes the transfer learning more convenient and effective. Extensive\nexperiments verify the effectiveness of M{^3}PT on three panoramic datasets.\nNotably, we improve the state-of-the-art baselines by averagely 26.2% in RMSE,\n51.7% in MRE, 49.7% in MAE, and 37.5% in RMSElog on three benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transform your Smartphone into a DSLR Camera: Learning the ISP in the Wild. (arXiv:2203.10636v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10636","description":"<p>We propose a trainable Image Signal Processing (ISP) framework that produces\nDSLR quality images given RAW images captured by a smartphone. To address the\ncolor misalignments between training image pairs, we employ a color-conditional\nISP network and optimize a novel parametric color mapping between each input\nRAW and reference DSLR image. During inference, we predict the target color\nimage by designing a color prediction network with efficient Global Context\nTransformer modules. The latter effectively leverage global information to\nlearn consistent color and tone mappings. We further propose a robust masked\naligned loss to identify and discard regions with inaccurate motion estimation\nduring training. Lastly, we introduce the ISP in the Wild (ISPW) dataset,\nconsisting of weakly paired phone RAW and DSLR sRGB images. We extensively\nevaluate our method, setting a new state-of-the-art on two datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_A/0/1/0/all/0/1\">Ardhendu Shekhar Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1\">Samarth Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-Supervised Salient Object Detection Using Point Supervision. (arXiv:2203.11652v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11652","description":"<p>Current state-of-the-art saliency detection models rely heavily on large\ndatasets of accurate pixel-wise annotations, but manually labeling pixels is\ntime-consuming and labor-intensive. There are some weakly supervised methods\ndeveloped for alleviating the problem, such as image label, bounding box label,\nand scribble label, while point label still has not been explored in this\nfield. In this paper, we propose a novel weakly-supervised salient object\ndetection method using point supervision. To infer the saliency map, we first\ndesign an adaptive masked flood filling algorithm to generate pseudo labels.\nThen we develop a transformer-based point-supervised saliency detection model\nto produce the first round of saliency maps. However, due to the sparseness of\nthe label, the weakly supervised model tends to degenerate into a general\nforeground detection model. To address this issue, we propose a Non-Salient\nSuppression (NSS) method to optimize the erroneous saliency maps generated in\nthe first round and leverage them for the second round of training. Moreover,\nwe build a new point-supervised dataset (P-DUTS) by relabeling the DUTS\ndataset. In P-DUTS, there is only one labeled point for each salient object.\nComprehensive experiments on five largest benchmark datasets demonstrate our\nmethod outperforms the previous state-of-the-art methods trained with the\nstronger supervision and even surpass several fully supervised state-of-the-art\nmodels. The code is available at: https://github.com/shuyonggao/PSOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shuyong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qianyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenglong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yangji He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Compressed Sensing via Global Image Tokens. (arXiv:2203.12861v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12861","description":"<p>Convolutional neural networks (CNN) have demonstrated outstanding Compressed\nSensing (CS) performance compared to traditional, hand-crafted methods.\nHowever, they are broadly limited in terms of generalisability, inductive bias\nand difficulty to model long distance relationships. Transformer neural\nnetworks (TNN) overcome such issues by implementing an attention mechanism\ndesigned to capture dependencies between inputs. However, high-resolution tasks\ntypically require vision Transformers (ViT) to decompose an image into\npatch-based tokens, limiting inputs to inherently local contexts. We propose a\nnovel image decomposition that naturally embeds images into low-resolution\ninputs. These Kaleidoscope tokens (KD) provide a mechanism for global\nattention, at the same computational cost as a patch-based approach. To\nshowcase this development, we replace CNN components in a well-known CS-MRI\nneural network with TNN blocks and demonstrate the improvements afforded by KD.\nWe also propose an ensemble of image tokens, which enhance overall image\nquality and reduces model size. Supplementary material is available:\nhttps://github.com/uqmarlonbran/TCS.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lorenzana_M/0/1/0/all/0/1\">Marlon Bran Lorenzana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engstrom_C/0/1/0/all/0/1\">Craig Engstrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_S/0/1/0/all/0/1\">Shekhar S. Chandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modality High-Frequency Transformer for MR Image Super-Resolution. (arXiv:2203.15314v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15314","description":"<p>Improving the resolution of magnetic resonance (MR) image data is critical to\ncomputer-aided diagnosis and brain function analysis. Higher resolution helps\nto capture more detailed content, but typically induces to lower\nsignal-to-noise ratio and longer scanning time. To this end, MR image\nsuper-resolution has become a widely-interested topic in recent times. Existing\nworks establish extensive deep models with the conventional architectures based\non convolutional neural networks (CNN). In this work, to further advance this\nresearch field, we make an early effort to build a Transformer-based MR image\nsuper-resolution framework, with careful designs on exploring valuable domain\nprior knowledge. Specifically, we consider two-fold domain priors including the\nhigh-frequency structure prior and the inter-modality context prior, and\nestablish a novel Transformer architecture, called Cross-modality\nhigh-frequency Transformer (Cohf-T), to introduce such priors into\nsuper-resolving the low-resolution (LR) MR images. Experiments on two datasets\nindicate that Cohf-T achieves new state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chaowei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lechao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CycDA: Unsupervised Cycle Domain Adaptation from Image to Video. (arXiv:2203.16244v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16244","description":"<p>Although action recognition has achieved impressive results over recent\nyears, both collection and annotation of video training data are still\ntime-consuming and cost intensive. Therefore, image-to-video adaptation has\nbeen proposed to exploit labeling-free web image source for adapting on\nunlabeled target videos. This poses two major challenges: (1) spatial domain\nshift between web images and video frames; (2) modality gap between image and\nvideo data. To address these challenges, we propose Cycle Domain Adaptation\n(CycDA), a cycle-based approach for unsupervised image-to-video domain\nadaptation by leveraging the joint spatial information in images and videos on\nthe one hand and, on the other hand, training an independent spatio-temporal\nmodel to bridge the modality gap. We alternate between the spatial and\nspatio-temporal learning with knowledge transfer between the two in each cycle.\nWe evaluate our approach on benchmark datasets for image-to-video as well as\nfor mixed-source domain adaptation achieving state-of-the-art results and\ndemonstrating the benefits of our cyclic adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukleva_A/0/1/0/all/0/1\">Anna Kukleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Kunyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Possegger_H/0/1/0/all/0/1\">Horst Possegger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischof_H/0/1/0/all/0/1\">Horst Bischof</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saliency in Augmented Reality. (arXiv:2204.08308v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08308","description":"<p>With the rapid development of multimedia technology, Augmented Reality (AR)\nhas become a promising next-generation mobile platform. The primary theory\nunderlying AR is human visual confusion, which allows users to perceive the\nreal-world scenes and augmented contents (virtual-world scenes) simultaneously\nby superimposing them together. To achieve good Quality of Experience (QoE), it\nis important to understand the interaction between two scenarios, and\nharmoniously display AR contents. However, studies on how this superimposition\nwill influence the human visual attention are lacking. Therefore, in this\npaper, we mainly analyze the interaction effect between background (BG) scenes\nand AR contents, and study the saliency prediction problem in AR. Specifically,\nwe first construct a Saliency in AR Dataset (SARD), which contains 450 BG\nimages, 450 AR images, as well as 1350 superimposed images generated by\nsuperimposing BG and AR images in pair with three mixing levels. A large-scale\neye-tracking experiment among 60 subjects is conducted to collect eye movement\ndata. To better predict the saliency in AR, we propose a vector quantized\nsaliency prediction method and generalize it for AR saliency prediction. For\ncomparison, three benchmark methods are proposed and evaluated together with\nour proposed method on our SARD. Experimental results demonstrate the\nsuperiority of our proposed method on both of the common saliency prediction\nproblem and the AR saliency prediction problem over benchmark methods. Our\ndataset and code are available at: https://github.com/DuanHuiyu/ARSaliency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1\">Huiyu Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_D/0/1/0/all/0/1\">Danyang Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMRotate: A Rotated Object Detection Benchmark using Pytorch. (arXiv:2204.13317v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13317","description":"<p>We present an open-source toolbox, named MMRotate, which provides a coherent\nalgorithm framework of training, inferring, and evaluation for the popular\nrotated object detection algorithm based on deep learning. MMRotate implements\n18 state-of-the-art algorithms and supports the three most frequently used\nangle definition methods. To facilitate future research and industrial\napplications of rotated object detection-related problems, we also provide a\nlarge number of trained models and detailed benchmarks to give insights into\nthe performance of rotated object detection. MMRotate is publicly released at\nhttps://github.com/open-mmlab/mmrotate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gefan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiabao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Liping Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingzhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chengqi Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty estimation for Cross-dataset performance in Trajectory prediction. (arXiv:2205.07310v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.07310","description":"<p>While a lot of work has been carried on developing trajectory prediction\nmethods, and various datasets have been proposed for benchmarking this task,\nlittle study has been done so far on the generalizability and the\ntransferability of these methods across dataset. In this paper, we observe the\nperformance of two of the latest state-of-the-art trajectory prediction methods\nacross four different datasets (Argoverse, NuScenes, Interaction, Shifts). This\nanalysis allows to gain some insights on the generalizability proprieties of\nmost recent trajectory prediction models and to analyze which dataset is more\nrepresentative of real driving scenes and therefore enables better\ntransferability. Furthermore we present a novel method to estimate prediction\nuncertainty and show how it could be used to achieve better performance across\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gilles_T/0/1/0/all/0/1\">Thomas Gilles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabatini_S/0/1/0/all/0/1\">Stefano Sabatini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsishkou_D/0/1/0/all/0/1\">Dzmitry Tsishkou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanciulescu_B/0/1/0/all/0/1\">Bogdan Stanciulescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1\">Fabien Moutarde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MESH2IR: Neural Acoustic Impulse Response Generator for Complex 3D Scenes. (arXiv:2205.09248v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2205.09248","description":"<p>We propose a mesh-based neural network (MESH2IR) to generate acoustic impulse\nresponses (IRs) for indoor 3D scenes represented using a mesh. The IRs are used\nto create a high-quality sound experience in interactive applications and audio\nprocessing. Our method can handle input triangular meshes with arbitrary\ntopologies (2K - 3M triangles). We present a novel training technique to train\nMESH2IR using energy decay relief and highlight its benefits. We also show that\ntraining MESH2IR on IRs preprocessed using our proposed technique significantly\nimproves the accuracy of IR generation. We reduce the non-linearity in the mesh\nspace by transforming 3D scene meshes to latent space using a graph convolution\nnetwork. Our MESH2IR is more than 200 times faster than a geometric acoustic\nalgorithm on a CPU and can generate more than 10,000 IRs per second on an\nNVIDIA GeForce RTX 2080 Ti GPU for a given furnished indoor 3D scene. The\nacoustic metrics are used to characterize the acoustic environment. We show\nthat the acoustic metrics of the IRs predicted from our MESH2IR match the\nground truth with less than 10% error. We also highlight the benefits of\nMESH2IR on audio and speech processing applications such as speech\ndereverberation and speech separation. To the best of our knowledge, ours is\nthe first neural-network-based approach to predict IRs from a given 3D scene\nmesh in real-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ratnarajah_A/0/1/0/all/0/1\">Anton Ratnarajah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhenyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aralikatti_R/0/1/0/all/0/1\">Rohith Chandrashekar Aralikatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TRT-ViT: TensorRT-oriented Vision Transformer. (arXiv:2205.09579v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.09579","description":"<p>We revisit the existing excellent Transformers from the perspective of\npractical application. Most of them are not even as efficient as the basic\nResNets series and deviate from the realistic deployment scenario. It may be\ndue to the current criterion to measure computation efficiency, such as FLOPs\nor parameters is one-sided, sub-optimal, and hardware-insensitive. Thus, this\npaper directly treats the TensorRT latency on the specific hardware as an\nefficiency metric, which provides more comprehensive feedback involving\ncomputational capacity, memory cost, and bandwidth. Based on a series of\ncontrolled experiments, this work derives four practical guidelines for\nTensorRT-oriented and deployment-friendly network design, e.g., early CNN and\nlate Transformer at stage-level, early Transformer and late CNN at block-level.\nAccordingly, a family of TensortRT-oriented Transformers is presented,\nabbreviated as TRT-ViT. Extensive experiments demonstrate that TRT-ViT\nsignificantly outperforms existing ConvNets and vision Transformers with\nrespect to the latency/accuracy trade-off across diverse visual tasks, e.g.,\nimage classification, object detection and semantic segmentation. For example,\nat 82.7% ImageNet-1k top-1 accuracy, TRT-ViT is 2.7$\\times$ faster than CSWin\nand 2.0$\\times$ faster than Twins. On the MS-COCO object detection task,\nTRT-ViT achieves comparable performance with Twins, while the inference speed\nis increased by 2.8$\\times$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiashi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuefeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Min Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporally Precise Action Spotting in Soccer Videos Using Dense Detection Anchors. (arXiv:2205.10450v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10450","description":"<p>We present a model for temporally precise action spotting in videos, which\nuses a dense set of detection anchors, predicting a detection confidence and\ncorresponding fine-grained temporal displacement for each anchor. We experiment\nwith two trunk architectures, both of which are able to incorporate large\ntemporal contexts while preserving the smaller-scale features required for\nprecise localization: a one-dimensional version of a u-net, and a Transformer\nencoder (TE). We also suggest best practices for training models of this kind,\nby applying Sharpness-Aware Minimization (SAM) and mixup data augmentation. We\nachieve a new state-of-the-art on SoccerNet-v2, the largest soccer video\ndataset of its kind, with marked improvements in temporal localization.\nAdditionally, our ablations show: the importance of predicting the temporal\ndisplacements; the trade-offs between the u-net and TE trunks; and the benefits\nof training with SAM and mixup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soares_J/0/1/0/all/0/1\">Jo&#xe3;o V. B. Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Avijit Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_T/0/1/0/all/0/1\">Topojoy Biswas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHREC 2022: pothole and crack detection in the road pavement using images and RGB-D data. (arXiv:2205.13326v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.13326","description":"<p>This paper describes the methods submitted for evaluation to the SHREC 2022\ntrack on pothole and crack detection in the road pavement. A total of 7\ndifferent runs for the semantic segmentation of the road surface are compared,\n6 from the participants plus a baseline method. All methods exploit Deep\nLearning techniques and their performance is tested using the same environment\n(i.e.: a single Jupyter notebook). A training set, composed of 3836 semantic\nsegmentation image/mask pairs and 797 RGB-D video clips collected with the\nlatest depth cameras was made available to the participants. The methods are\nthen evaluated on the 496 image/mask pairs in the validation set, on the 504\npairs in the test set and finally on 8 video clips. The analysis of the results\nis based on quantitative metrics for image segmentation and qualitative\nanalysis of the video clips. The participation and the results show that the\nscenario is of great interest and that the use of RGB-D data is still\nchallenging in this context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thompson_E/0/1/0/all/0/1\">Elia Moscoso Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranieri_A/0/1/0/all/0/1\">Andrea Ranieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biasotti_S/0/1/0/all/0/1\">Silvia Biasotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chicchon_M/0/1/0/all/0/1\">Miguel Chicchon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sipiran_I/0/1/0/all/0/1\">Ivan Sipiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1\">Minh-Khoi Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Ho_T/0/1/0/all/0/1\">Thang-Long Nguyen-Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hai-Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Robustness and Generalization of Deep Neural Network with Confidence Threshold Reduction. (arXiv:2206.00913v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.00913","description":"<p>Deep neural networks are easily attacked by imperceptible perturbation.\nPresently, adversarial training (AT) is the most effective method to enhance\nthe robustness of the model against adversarial examples. However, because\nadversarial training solved a min-max value problem, in comparison with natural\ntraining, the robustness and generalization are contradictory, i.e., the\nrobustness improvement of the model will decrease the generalization of the\nmodel. To address this issue, in this paper, a new concept, namely confidence\nthreshold (CT), is introduced and the reducing of the confidence threshold,\nknown as confidence threshold reduction (CTR), is proven to improve both the\ngeneralization and robustness of the model. Specifically, to reduce the CT for\nnatural training (i.e., for natural training with CTR), we propose a\nmask-guided divergence loss function (MDL) consisting of a cross-entropy loss\nterm and an orthogonal term. The empirical and theoretical analysis\ndemonstrates that the MDL loss improves the robustness and generalization of\nthe model simultaneously for natural training. However, the model robustness\nimprovement of natural training with CTR is not comparable to that of\nadversarial training. Therefore, for adversarial training, we propose a\nstandard deviation loss function (STD), which minimizes the difference in the\nprobabilities of the wrong categories, to reduce the CT by being integrated\ninto the loss function of adversarial training. The empirical and theoretical\nanalysis demonstrates that the STD based loss function can further improve the\nrobustness of the adversarially trained model on basis of guaranteeing the\nchangeless or slight improvement of the natural accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiangyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Ego 3D Representation as Ray Tracing. (arXiv:2206.04042v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.04042","description":"<p>A self-driving perception model aims to extract 3D semantic representations\nfrom multiple cameras collectively into the bird's-eye-view (BEV) coordinate\nframe of the ego car in order to ground downstream planner. Existing perception\nmethods often rely on error-prone depth estimation of the whole scene or\nlearning sparse virtual 3D representations without the target geometry\nstructure, both of which remain limited in performance and/or capability. In\nthis paper, we present a novel end-to-end architecture for ego 3D\nrepresentation learning from an arbitrary number of unconstrained camera views.\nInspired by the ray tracing principle, we design a polarized grid of \"imaginary\neyes\" as the learnable ego 3D representation and formulate the learning process\nwith the adaptive attention mechanism in conjunction with the 3D-to-2D\nprojection. Critically, this formulation allows extracting rich 3D\nrepresentation from 2D images without any depth supervision, and with the\nbuilt-in geometry structure consistent w.r.t. BEV. Despite its simplicity and\nversatility, extensive experiments on standard BEV visual tasks (e.g.,\ncamera-based 3D object detection and BEV segmentation) show that our model\noutperforms all state-of-the-art alternatives significantly, with an extra\nadvantage in computational efficiency from multi-task learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiachen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zheyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantitative Imaging Principles Improves Medical Image Learning. (arXiv:2206.06663v3 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2206.06663","description":"<p>Fundamental differences between natural and medical images have recently\nfavored the use of self-supervised learning (SSL) over ImageNet transfer\nlearning for medical image applications. Differences between image types are\nprimarily due to the imaging modality and medical images utilize a wide range\nof physics based techniques while natural images are captured using only\nvisible light. While many have demonstrated that SSL on medical images has\nresulted in better downstream task performance, our work suggests that more\nperformance can be gained. The scientific principles which are used to acquire\nmedical images are not often considered when constructing learning problems.\nFor this reason, we propose incorporating quantitative imaging principles\nduring generative SSL to improve image quality and quantitative biological\naccuracy. We show that this training schema results in better starting states\nfor downstream supervised training on limited data. Our model also generates\nimages that validate on clinical quantitative analysis software.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Leong_L/0/1/0/all/0/1\">Lambert T. Leong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wong_M/0/1/0/all/0/1\">Michael C. Wong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Glaser_Y/0/1/0/all/0/1\">Yannik Glaser</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wolfgruber_T/0/1/0/all/0/1\">Thomas Wolfgruber</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Heymsfield_S/0/1/0/all/0/1\">Steven B. Heymsfield</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sadowski_P/0/1/0/all/0/1\">Peter Sadowski</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shepherd_J/0/1/0/all/0/1\">John A. Shepherd</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Clipping: Differentially Private Deep Learning Made Easier and Stronger. (arXiv:2206.07136v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.07136","description":"<p>Per-example gradient clipping is a key algorithmic step that enables\npractical differential private (DP) training for deep learning models. The\nchoice of clipping norm $R$, however, is shown to be vital for achieving high\naccuracy under DP. We propose an easy-to-use replacement, called AutoClipping,\nthat eliminates the need to tune $R$ for any DP optimizers, including DP-SGD,\nDP-Adam, DP-LAMB and many others. The automatic variants are as private and\ncomputationally efficient as existing DP optimizers, but require no DP-specific\nhyperparameters and thus make DP training as amenable as the standard\nnon-private training. We give a rigorous convergence analysis of automatic\nDP-SGD in the non-convex setting, which shows that it enjoys an asymptotic\nconvergence rate that matches the standard SGD. We also demonstrate on various\nlanguage and vision tasks that automatic clipping outperforms or matches the\nstate-of-the-art, and can be easily employed with minimal changes to existing\ncodebases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1\">Zhiqi Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_S/0/1/0/all/0/1\">Sheng Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EyeNeRF: A Hybrid Representation for Photorealistic Synthesis, Animation and Relighting of Human Eyes. (arXiv:2206.08428v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.08428","description":"<p>A unique challenge in creating high-quality animatable and relightable 3D\navatars of people is modeling human eyes. The challenge of synthesizing eyes is\nmultifold as it requires 1) appropriate representations for the various\ncomponents of the eye and the periocular region for coherent viewpoint\nsynthesis, capable of representing diffuse, refractive and highly reflective\nsurfaces, 2) disentangling skin and eye appearance from environmental\nillumination such that it may be rendered under novel lighting conditions, and\n3) capturing eyeball motion and the deformation of the surrounding skin to\nenable re-gazing. These challenges have traditionally necessitated the use of\nexpensive and cumbersome capture setups to obtain high-quality results, and\neven then, modeling of the eye region holistically has remained elusive. We\npresent a novel geometry and appearance representation that enables\nhigh-fidelity capture and photorealistic animation, view synthesis and\nrelighting of the eye region using only a sparse set of lights and cameras. Our\nhybrid representation combines an explicit parametric surface model for the\neyeball with implicit deformable volumetric representations for the periocular\nregion and the interior of the eye. This novel hybrid model has been designed\nto address the various parts of that challenging facial area - the explicit\neyeball surface allows modeling refraction and high-frequency specular\nreflection at the cornea, whereas the implicit representation is well suited to\nmodel lower-frequency skin reflection via spherical harmonics and can represent\nnon-surface structures such as hair or diffuse volumetric bodies, both of which\nare a challenge for explicit surface models. We show that for high-resolution\nclose-ups of the eye, our model can synthesize high-fidelity animated gaze from\nnovel views under unseen illumination conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gengyan Li</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1\">Abhimitra Meka</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Muller_F/0/1/0/all/0/1\">Franziska M&#xfc;ller</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Buhler_M/0/1/0/all/0/1\">Marcel C. B&#xfc;hler</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Beeler_T/0/1/0/all/0/1\">Thabo Beeler</a> (1) ((1) Google Inc., (2) ETH Z&#xfc;rich)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GNN-PMB: A Simple but Effective Online 3D Multi-Object Tracker without Bells and Whistles. (arXiv:2206.10255v2 [eess.SY] UPDATED)","link":"http://arxiv.org/abs/2206.10255","description":"<p>Multi-object tracking (MOT) is among crucial applications in modern advanced\ndriver assistance systems (ADAS) and autonomous driving (AD) systems. Global\nnearest neighbor (GNN) filter, as the earliest random vector Bayesian tracking\nframework, has been adopted in most of state-of-the-arts trackers and widely\naccepted in the automotive industry. With the development of random finite set\n(RFS) theory, the RFS Bayesian filters have been applied in MOT tasks recently.\nHowever, their usefulness in the real traffic for ADAS and AD application is\nstill open to doubt. In this paper, we firstly demonstrate the latest RFS\nBayesian tracking framework could be superior to typical random vector Bayesian\ntracking framework like GNN, via a systematic comparative study of both\ntraditional random vector Bayesian filters with rule-based heuristic track\nmaintenance and RFS Bayesian filters on nuScenes validation dataset. Then, we\npropose a RFS-based tracker, namely Poisson multi-Bernoulli filter using the\nglobal nearest neighbor (GNN-PMB), for LiDAR-based MOT tasks. This GNN-PMB\ntracker is simple to use but can achieve competitive results on nuScenes\ndataset. Specifically, the proposed GNN-PMB tracker outperforms most of the\nstate-of-the-art LiDAR-only trackers and LiDAR and camera fusion-based\ntrackers, ranks the 3rd among all LiDAR-only trackers on nuScenes tracking task\nleader board1 at the time of submission. Our code is available at\nhttps://github.com/chisyliu/gnn pmb tracker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jianan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_L/0/1/0/all/0/1\">Liping Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_Y/0/1/0/all/0/1\">Yuxuan Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_T/0/1/0/all/0/1\">Tao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_B/0/1/0/all/0/1\">Bing Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UI Layers Merger: Merging UI layers via Visual Learning and Boundary Prior. (arXiv:2206.13389v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.13389","description":"<p>With the fast-growing GUI development workload in the Internet industry, some\nwork on intelligent methods attempted to generate maintainable front-end code\nfrom UI screenshots. It can be more suitable for utilizing UI design drafts\nthat contain UI metadata. However, fragmented layers inevitably appear in the\nUI design drafts which greatly reduces the quality of code generation. None of\nthe existing GUI automated techniques detects and merges the fragmented layers\nto improve the accessibility of generated code. In this paper, we propose UI\nLayers Merger (UILM), a vision-based method, which can automatically detect and\nmerge fragmented layers into UI components. Our UILM contains Merging Area\nDetector (MAD) and a layers merging algorithm. MAD incorporates the boundary\nprior knowledge to accurately detect the boundaries of UI components. Then, the\nlayers merging algorithm can search out the associated layers within the\ncomponents' boundaries and merge them into a whole part. We present a dynamic\ndata augmentation approach to boost the performance of MAD. We also construct a\nlarge-scale UI dataset for training the MAD and testing the performance of\nUILM. The experiment shows that the proposed method outperforms the best\nbaseline regarding merging area detection and achieves a decent accuracy\nregarding layers merging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-nong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_Y/0/1/0/all/0/1\">Yan-kun Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chu-ning Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia-zhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liu-qing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ze-jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Ling-yun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Ting-ting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yan-fang Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C2FTrans: Coarse-to-Fine Transformers for Medical Image Segmentation. (arXiv:2206.14409v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.14409","description":"<p>Convolutional neural networks (CNN), the most prevailing architecture for\ndeep-learning based medical image analysis, are still functionally limited by\ntheir intrinsic inductive biases and inadequate receptive fields. Transformer,\nborn to address this issue, has drawn explosive attention in natural language\nprocessing and computer vision due to its remarkable ability in capturing\nlong-range dependency. However, most recent transformer-based methods for\nmedical image segmentation directly apply vanilla transformers as an auxiliary\nmodule in CNN-based methods, resulting in severe detail loss due to the rigid\npatch partitioning scheme in transformers. To address this problem, we propose\nC2FTrans, a novel multi-scale architecture that formulates medical image\nsegmentation as a coarse-to-fine procedure. C2FTrans mainly consists of a\ncross-scale global transformer (CGT) which addresses local contextual\nsimilarity in CNN and a boundary-aware local transformer (BLT) which overcomes\nboundary uncertainty brought by rigid patch partitioning in transformers.\nSpecifically, CGT builds global dependency across three different small-scale\nfeature maps to obtain rich global semantic features with an acceptable\ncomputational cost, while BLT captures mid-range dependency by adaptively\ngenerating windows around boundaries under the guidance of entropy to reduce\ncomputational complexity and minimize detail loss based on large-scale feature\nmaps. Extensive experimental results on three public datasets demonstrate the\nsuperior performance of C2FTrans against state-of-the-art CNN-based and\ntransformer-based methods with fewer parameters and lower FLOPs. We believe the\ndesign of C2FTrans would further inspire future work on developing efficient\nand lightweight transformers for medical image segmentation. The source code of\nthis paper is publicly available at https://github.com/xianlin7/C2FTrans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Li Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zengqiang Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Lighter The Better: Rethinking Transformers in Medical Image Segmentation Through Adaptive Pruning. (arXiv:2206.14413v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.14413","description":"<p>Vision transformers have recently set off a new wave in the field of medical\nimage analysis due to their remarkable performance on various computer vision\ntasks. However, recent hybrid-/transformer-based approaches mainly focus on the\nbenefits of transformers in capturing long-range dependency while ignoring the\nissues of their daunting computational complexity, high training costs, and\nredundant dependency. In this paper, we propose to employ adaptive pruning to\ntransformers for medical image segmentation and propose a lightweight and\neffective hybrid network APFormer. To our best knowledge, this is the first\nwork on transformer pruning for medical image analysis tasks. The key features\nof APFormer mainly are self-supervised self-attention (SSA) to improve the\nconvergence of dependency establishment, Gaussian-prior relative position\nembedding (GRPE) to foster the learning of position information, and adaptive\npruning to eliminate redundant computations and perception information.\nSpecifically, SSA and GRPE consider the well-converged dependency distribution\nand the Gaussian heatmap distribution separately as the prior knowledge of\nself-attention and position embedding to ease the training of transformers and\nlay a solid foundation for the following pruning operation. Then, adaptive\ntransformer pruning, both query-wise and dependency-wise, is performed by\nadjusting the gate control parameters for both complexity reduction and\nperformance improvement. Extensive experiments on two widely-used datasets\ndemonstrate the prominent segmentation performance of APFormer against the\nstate-of-the-art methods with much fewer parameters and lower GFLOPs. More\nimportantly, we prove, through ablation studies, that adaptive pruning can work\nas a plug-n-play module for performance improvement on other\nhybrid-/transformer-based methods. Code is available at\nhttps://github.com/xianlin7/APFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Li Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zengqiang Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PolarFormer: Multi-camera 3D Object Detection with Polar Transformers. (arXiv:2206.15398v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.15398","description":"<p>3D object detection in autonomous driving aims to reason \"what\" and \"where\"\nthe objects of interest present in a 3D world. Following the conventional\nwisdom of previous 2D object detection, existing methods often adopt the\ncanonical Cartesian coordinate system with perpendicular axis. However, we\nconjugate that this does not fit the nature of the ego car's perspective, as\neach onboard camera perceives the world in shape of wedge intrinsic to the\nimaging geometry with radical (non-perpendicular) axis. Hence, in this paper we\nadvocate the exploitation of the Polar coordinate system and propose a new\nPolar Transformer (PolarFormer) for more accurate 3D object detection in the\nbird's-eye-view (BEV) taking as input only multi-camera 2D images.\nSpecifically, we design a cross attention based Polar detection head without\nrestriction to the shape of input structure to deal with irregular Polar grids.\nFor tackling the unconstrained object scale variations along Polar's distance\ndimension, we further introduce a multi-scalePolar representation learning\nstrategy. As a result, our model can make best use of the Polar representation\nrasterized via attending to the corresponding image observation in a\nsequence-to-sequence fashion subject to the geometric constraints. Thorough\nexperiments on the nuScenes dataset demonstrate that our PolarFormer\noutperforms significantly state-of-the-art 3D object detection alternatives, as\nwell as yielding competitive performance on BEV semantic segmentation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yanqin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Z/0/1/0/all/0/1\">Zhenwei Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Single-Frame 3D Object Detection by Simulating Multi-Frame Point Clouds. (arXiv:2207.01030v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01030","description":"<p>To boost a detector for single-frame 3D object detection, we present a new\napproach to train it to simulate features and responses following a detector\ntrained on multi-frame point clouds. Our approach needs multi-frame point\nclouds only when training the single-frame detector, and once trained, it can\ndetect objects with only single-frame point clouds as inputs during the\ninference. We design a novel Simulated Multi-Frame Single-Stage object Detector\n(SMF-SSD) framework to realize the approach: multi-view dense object fusion to\ndensify ground-truth objects to generate a multi-frame point cloud;\nself-attention voxel distillation to facilitate one-to-many knowledge transfer\nfrom multi- to single-frame voxels; multi-scale BEV feature distillation to\ntransfer knowledge in low-level spatial and high-level semantic BEV features;\nand adaptive response distillation to activate single-frame responses of high\nconfidence and accurate localization. Experimental results on the Waymo test\nset show that our SMF-SSD consistently outperforms all state-of-the-art\nsingle-frame 3D object detectors for all object classes of difficulty levels 1\nand 2 in terms of both mAP and mAPH.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Li Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Fanbin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yangyang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactually Measuring and Eliminating Social Bias in Vision-Language Pre-training Models. (arXiv:2207.01056v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01056","description":"<p>Vision-Language Pre-training (VLP) models have achieved state-of-the-art\nperformance in numerous cross-modal tasks. Since they are optimized to capture\nthe statistical properties of intra- and inter-modality, there remains risk to\nlearn social biases presented in the data as well. In this work, we (1)\nintroduce a counterfactual-based bias measurement \\emph{CounterBias} to\nquantify the social bias in VLP models by comparing the [MASK]ed prediction\nprobabilities of factual and counterfactual samples; (2) construct a novel\nVL-Bias dataset including 24K image-text pairs for measuring gender bias in VLP\nmodels, from which we observed that significant gender bias is prevalent in VLP\nmodels; and (3) propose a VLP debiasing method \\emph{FairVLP} to minimize the\ndifference in the [MASK]ed prediction probabilities between factual and\ncounterfactual image-text pairs for VLP debiasing. Although CounterBias and\nFairVLP focus on social bias, they are generalizable to serve as tools and\nprovide new insights to probe and regularize more knowledge in VLP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1\">Jitao Sang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of ADHD based on Eye Movements during Natural Viewing. (arXiv:2207.01377v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01377","description":"<p>Attention-deficit/hyperactivity disorder (ADHD) is a neurodevelopmental\ndisorder that is highly prevalent and requires clinical specialists to\ndiagnose. It is known that an individual's viewing behavior, reflected in their\neye movements, is directly related to attentional mechanisms and higher-order\ncognitive processes. We therefore explore whether ADHD can be detected based on\nrecorded eye movements together with information about the video stimulus in a\nfree-viewing task. To this end, we develop an end-to-end deep learning-based\nsequence model which we pre-train on a related task for which more data are\navailable. We find that the method is in fact able to detect ADHD and\noutperforms relevant baselines. We investigate the relevance of the input\nfeatures in an ablation study. Interestingly, we find that the model's\nperformance is closely related to the content of the video, which provides\ninsights for future experimental designs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shuwen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasse_P/0/1/0/all/0/1\">Paul Prasse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reich_D/0/1/0/all/0/1\">David R. Reich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziemian_S/0/1/0/all/0/1\">Sabine Dziemian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stegenwallner_Schutz_M/0/1/0/all/0/1\">Maja Stegenwallner-Sch&#xfc;tz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krakowczyk_D/0/1/0/all/0/1\">Daniel Krakowczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_S/0/1/0/all/0/1\">Silvia Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langer_N/0/1/0/all/0/1\">Nicolas Langer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheffer_T/0/1/0/all/0/1\">Tobias Scheffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jager_L/0/1/0/all/0/1\">Lena A. J&#xe4;ger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AvatarCap: Animatable Avatar Conditioned Monocular Human Volumetric Capture. (arXiv:2207.02031v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02031","description":"<p>To address the ill-posed problem caused by partial observations in monocular\nhuman volumetric capture, we present AvatarCap, a novel framework that\nintroduces animatable avatars into the capture pipeline for high-fidelity\nreconstruction in both visible and invisible regions. Our method firstly\ncreates an animatable avatar for the subject from a small number (~20) of 3D\nscans as a prior. Then given a monocular RGB video of this subject, our method\nintegrates information from both the image observation and the avatar prior,\nand accordingly recon-structs high-fidelity 3D textured models with dynamic\ndetails regardless of the visibility. To learn an effective avatar for\nvolumetric capture from only few samples, we propose GeoTexAvatar, which\nleverages both geometry and texture supervisions to constrain the\npose-dependent dynamics in a decomposed implicit manner. An avatar-conditioned\nvolumetric capture method that involves a canonical normal fusion and a\nreconstruction network is further proposed to integrate both image observations\nand avatar dynamics for high-fidelity reconstruction in both observed and\ninvisible regions. Overall, our method enables monocular human volumetric\ncapture with detailed and pose-dependent dynamics, and the experiments show\nthat our method outperforms state of the art. Code is available at\nhttps://github.com/lizhe00/AvatarCap.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zerong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_C/0/1/0/all/0/1\">Chaonan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Counterfactual Image Manipulation via CLIP. (arXiv:2207.02812v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02812","description":"<p>Leveraging StyleGAN's expressivity and its disentangled latent codes,\nexisting methods can achieve realistic editing of different visual attributes\nsuch as age and gender of facial images. An intriguing yet challenging problem\narises: Can generative models achieve counterfactual editing against their\nlearnt priors? Due to the lack of counterfactual samples in natural datasets,\nwe investigate this problem in a text-driven manner with\nContrastive-Language-Image-Pretraining (CLIP), which can offer rich semantic\nknowledge even for various counterfactual concepts. Different from in-domain\nmanipulation, counterfactual manipulation requires more comprehensive\nexploitation of semantic knowledge encapsulated in CLIP as well as more\ndelicate handling of editing directions for avoiding being stuck in local\nminimum or undesired editing. To this end, we design a novel contrastive loss\nthat exploits predefined CLIP-space directions to guide the editing toward\ndesired directions from different perspectives. In addition, we design a simple\nyet effective scheme that explicitly maps CLIP embeddings (of target text) to\nthe latent space and fuses them with latent codes for effective latent code\noptimization and accurate editing. Extensive experiments show that our design\nachieves accurate and realistic editing while driving by target texts with\nvarious counterfactual concepts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rongliang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1\">Miaomiao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Network Binarization via Contrastive Learning. (arXiv:2207.02970v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02970","description":"<p>Neural network binarization accelerates deep models by quantizing their\nweights and activations into 1-bit. However, there is still a huge performance\ngap between Binary Neural Networks (BNNs) and their full-precision (FP)\ncounterparts. As the quantization error caused by weights binarization has been\nreduced in earlier works, the activations binarization becomes the major\nobstacle for further improvement of the accuracy. BNN characterises a unique\nand interesting structure, where the binary and latent FP activations exist in\nthe same forward pass (i.e., $\\text{Binarize}(\\mathbf{a}_F) = \\mathbf{a}_B$).\nTo mitigate the information degradation caused by the binarization operation\nfrom FP to binary activations, we establish a novel contrastive learning\nframework while training BNNs through the lens of Mutual Information (MI)\nmaximization. MI is introduced as the metric to measure the information shared\nbetween binary and FP activations, which assists binarization with contrastive\nlearning. Specifically, the representation ability of the BNNs is greatly\nstrengthened via pulling the positive pairs with binary and FP activations from\nthe same input samples, as well as pushing negative pairs from different\nsamples (the number of negative pairs can be exponentially large). This\nbenefits the downstream tasks, not only classification but also segmentation\nand depth estimation, etc. The experimental results show that our method can be\nimplemented as a pile-up module on existing state-of-the-art binarization\nmethods and can remarkably improve the performance over them on CIFAR-10/100\nand ImageNet, in addition to the great generalization ability on NYUD-v2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1\">Yuzhang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1\">Ziliang Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Makes for Automatic Reconstruction of Pulmonary Segments. (arXiv:2207.03078v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.03078","description":"<p>3D reconstruction of pulmonary segments plays an important role in surgical\ntreatment planning of lung cancer, which facilitates preservation of pulmonary\nfunction and helps ensure low recurrence rates. However, automatic\nreconstruction of pulmonary segments remains unexplored in the era of deep\nlearning. In this paper, we investigate what makes for automatic reconstruction\nof pulmonary segments. First and foremost, we formulate, clinically and\ngeometrically, the anatomical definitions of pulmonary segments, and propose\nevaluation metrics adhering to these definitions. Second, we propose ImPulSe\n(Implicit Pulmonary Segment), a deep implicit surface model designed for\npulmonary segment reconstruction. The automatic reconstruction of pulmonary\nsegments by ImPulSe is accurate in metrics and visually appealing. Compared\nwith canonical segmentation methods, ImPulSe outputs continuous predictions of\narbitrary resolutions with higher training efficiency and fewer parameters.\nLastly, we experiment with different network inputs to analyze what matters in\nthe task of pulmonary segment reconstruction. Our code is available at\nhttps://github.com/M3DV/ImPulSe.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kuang_K/0/1/0/all/0/1\">Kaiming Kuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jingyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hongwei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jiancheng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complementing Brightness Constancy with Deep Networks for Optical Flow Prediction. (arXiv:2207.03790v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.03790","description":"<p>State-of-the-art methods for optical flow estimation rely on deep learning,\nwhich require complex sequential training schemes to reach optimal performances\non real-world data. In this work, we introduce the COMBO deep network that\nexplicitly exploits the brightness constancy (BC) model used in traditional\nmethods. Since BC is an approximate physical model violated in several\nsituations, we propose to train a physically-constrained network complemented\nwith a data-driven network. We introduce a unique and meaningful flow\ndecomposition between the physical prior and the data-driven complement,\nincluding an uncertainty quantification of the BC model. We derive a joint\ntraining scheme for learning the different components of the decomposition\nensuring an optimal cooperation, in a supervised but also in a semi-supervised\ncontext. Experiments show that COMBO can improve performances over\nstate-of-the-art supervised networks, e.g. RAFT, reaching state-of-the-art\nresults on several benchmarks. We highlight how COMBO can leverage the BC model\nand adapt to its limitations. Finally, we show that our semi-supervised method\ncan significantly simplify the training procedure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guen_V/0/1/0/all/0/1\">Vincent Le Guen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambour_C/0/1/0/all/0/1\">Cl&#xe9;ment Rambour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thome_N/0/1/0/all/0/1\">Nicolas Thome</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Radiomics-Guided Global-Local Transformer for Weakly Supervised Pathology Localization in Chest X-Rays. (arXiv:2207.04394v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04394","description":"<p>Before the recent success of deep learning methods for automated medical\nimage analysis, practitioners used handcrafted radiomic features to\nquantitatively describe local patches of medical images. However, extracting\ndiscriminative radiomic features relies on accurate pathology localization,\nwhich is difficult to acquire in real-world settings. Despite advances in\ndisease classification and localization from chest X-rays, many approaches fail\nto incorporate clinically-informed domain knowledge. For these reasons, we\npropose a Radiomics-Guided Transformer (RGT) that fuses \\textit{global} image\ninformation with \\textit{local} knowledge-guided radiomics information to\nprovide accurate cardiopulmonary pathology localization and classification\n\\textit{without any bounding box annotations}. RGT consists of an image\nTransformer branch, a radiomics Transformer branch, and fusion layers that\naggregate image and radiomic information. Using the learned self-attention of\nits image branch, RGT extracts a bounding box for which to compute radiomic\nfeatures, which are further processed by the radiomics branch; learned image\nand radiomic features are then fused and mutually interact via cross-attention\nlayers. Thus, RGT utilizes a novel end-to-end feedback loop that can bootstrap\naccurate pathology localization only using image-level disease labels.\nExperiments on the NIH ChestXRay dataset demonstrate that RGT outperforms prior\nworks in weakly supervised disease localization (by an average margin of 3.6\\%\nover various intersection-over-union thresholds) and classification (by 1.1\\%\nin average area under the receiver operating characteristic curve). Code and\ntrained models will be released upon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holste_G/0/1/0/all/0/1\">Gregory Holste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Ying Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewfik_A/0/1/0/all/0/1\">Ahmed Tewfik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depthformer : Multiscale Vision Transformer For Monocular Depth Estimation With Local Global Information Fusion. (arXiv:2207.04535v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04535","description":"<p>Attention-based models such as transformers have shown outstanding\nperformance on dense prediction tasks, such as semantic segmentation, owing to\ntheir capability of capturing long-range dependency in an image. However, the\nbenefit of transformers for monocular depth prediction has seldom been explored\nso far. This paper benchmarks various transformer-based models for the depth\nestimation task on an indoor NYUV2 dataset and an outdoor KITTI dataset. We\npropose a novel attention-based architecture, Depthformer for monocular depth\nestimation that uses multi-head self-attention to produce the multiscale\nfeature maps, which are effectively combined by our proposed decoder network.\nWe also propose a Transbins module that divides the depth range into bins whose\ncenter value is estimated adaptively per image. The final depth estimated is a\nlinear combination of bin centers for each pixel. Transbins module takes\nadvantage of the global receptive field using the transformer module in the\nencoding stage. Experimental results on NYUV2 and KITTI depth estimation\nbenchmark demonstrate that our proposed method improves the state-of-the-art by\n3.3%, and 3.3% respectively in terms of Root Mean Squared Error (RMSE). Code is\navailable at https://github.com/ashutosh1807/Depthformer.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Ashutosh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1\">Chetan Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometry-aware Single-image Full-body Human Relighting. (arXiv:2207.04750v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04750","description":"<p>Single-image human relighting aims to relight a target human under new\nlighting conditions by decomposing the input image into albedo, shape and\nlighting. Although plausible relighting results can be achieved, previous\nmethods suffer from both the entanglement between albedo and lighting and the\nlack of hard shadows, which significantly decrease the realism. To tackle these\ntwo problems, we propose a geometry-aware single-image human relighting\nframework that leverages single-image geometry reconstruction for joint\ndeployment of traditional graphics rendering and neural rendering techniques.\nFor the de-lighting, we explore the shortcomings of UNet architecture and\npropose a modified HRNet, achieving better disentanglement between albedo and\nlighting. For the relighting, we introduce a ray tracing-based per-pixel\nlighting representation that explicitly models high-frequency shadows and\npropose a learning-based shading refinement module to restore realistic shadows\n(including hard cast shadows) from the ray-traced shading maps. Our framework\nis able to generate photo-realistic high-frequency shadows such as cast shadows\nunder challenging lighting conditions. Extensive experiments demonstrate that\nour proposed method outperforms previous methods on both synthetic and real\nimages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_C/0/1/0/all/0/1\">Chaonan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Kaiwen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Vision Transformer. (arXiv:2207.04976v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04976","description":"<p>Prior works have proposed several strategies to reduce the computational cost\nof self-attention mechanism. Many of these works consider decomposing the\nself-attention procedure into regional and local feature extraction procedures\nthat each incurs a much smaller computational complexity. However, regional\ninformation is typically only achieved at the expense of undesirable\ninformation lost owing to down-sampling. In this paper, we propose a novel\nTransformer architecture that aims to mitigate the cost issue, named Dual\nVision Transformer (Dual-ViT). The new architecture incorporates a critical\nsemantic pathway that can more efficiently compress token vectors into global\nsemantics with reduced order of complexity. Such compressed global semantics\nthen serve as useful prior information in learning finer pixel level details,\nthrough another constructed pixel pathway. The semantic pathway and pixel\npathway are then integrated together and are jointly trained, spreading the\nenhanced self-attention information in parallel through both of the pathways.\nDual-ViT is henceforth able to reduce the computational complexity without\ncompromising much accuracy. We empirically demonstrate that Dual-ViT provides\nsuperior accuracy than SOTA Transformer architectures with reduced training\ncomplexity. Source code is available at\n\\url{https://github.com/YehLi/ImageNetModel}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yehao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Ping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Continuous Grasping Function with a Dexterous Hand from Human Demonstrations. (arXiv:2207.05053v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2207.05053","description":"<p>We propose to learn to generate grasping motion for manipulation with a\ndexterous hand using implicit functions. With continuous time inputs, the model\ncan generate a continuous and smooth grasping plan. We name the proposed model\nContinuous Grasping Function (CGF). CGF is learned via generative modeling with\na Conditional Variational Autoencoder using 3D human demonstrations. We will\nfirst convert the large-scale human-object interaction trajectories to robot\ndemonstrations via motion retargeting, and then use these demonstrations to\ntrain CGF. During inference, we perform sampling with CGF to generate different\ngrasping plans in the simulator and select the successful ones to transfer to\nthe real robot. By training on diverse human data, our CGF allows\ngeneralization to manipulate multiple objects. Compared to previous planning\nalgorithms, CGF is more efficient and achieves significant improvement on\nsuccess rate when transferred to grasping with the real Allegro Hand. Our\nproject page is at https://jianglongye.com/cgf .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jianglong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiashun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Binghao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yuzhe Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}