{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-02-03T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Visualizing Automatic Speech Recognition -- Means for a Better Understanding?. (arXiv:2202.00673v1 [cs.LG])","link":"http://arxiv.org/abs/2202.00673","description":"<p>Automatic speech recognition (ASR) is improving ever more at mimicking human\nspeech processing. The functioning of ASR, however, remains to a large extent\nobfuscated by the complex structure of the deep neural networks (DNNs) they are\nbased on. In this paper, we show how so-called attribution methods, that we\nimport from image recognition and suitably adapt to handle audio data, can help\nto clarify the working of ASR. Taking DeepSpeech, an end-to-end model for ASR,\nas a case study, we show how these techniques help to visualize which features\nof the input are the most influential in determining the output. We focus on\nthree visualization techniques: Layer-wise Relevance Propagation (LRP),\nSaliency Maps, and Shapley Additive Explanations (SHAP). We compare these\nmethods and discuss potential further applications, such as in the detection of\nadversarial examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Markert_K/0/1/0/all/0/1\">Karla Markert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parracone_R/0/1/0/all/0/1\">Romain Parracone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulakov_M/0/1/0/all/0/1\">Mykhailo Kulakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sperl_P/0/1/0/all/0/1\">Philip Sperl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_C/0/1/0/all/0/1\">Ching-Yu Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bottinger_K/0/1/0/all/0/1\">Konstantin B&#xf6;ttinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to pronounce as measuring cross lingual joint orthography-phonology complexity. (arXiv:2202.00794v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00794","description":"<p>Recent work has demonstrated that machine learning models allow us to compare\nlanguages by showing how hard each language might be to learn under specific\ntasks. Following this line of investigation, we investigate what makes a\nlanguage \"hard to pronounce\" by modelling the task of grapheme-to-phoneme (g2p)\ntransliteration. By training a character-level transformer model on this task\nacross 22 languages and measuring the model's proficiency against its grapheme\nand phoneme inventories, we show that certain characteristics emerge that\nseparate easier and harder languages with respect to learning to pronounce.\nNamely that the complexity of a languages pronunciation from its orthography is\ndue to how expressive or simple its grapheme-to-phoneme mapping is. Further\ndiscussion illustrates how future studies should consider relative data\nsparsity per language in order to design more fair cross lingual comparison\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosati_D/0/1/0/all/0/1\">Domenic Rosati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disaster Tweets Classification using BERT-Based Language Model. (arXiv:2202.00795v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00795","description":"<p>Social networking services have became an important communication channel in\ntime of emergency. The aim of this study is to create a machine learning\nlanguage model that is able to investigate if a person or area was in danger or\nnot. The ubiquitousness of smartphones enables people to announce an emergency\nthey are observing in real-time. Because of this, more agencies are interested\nin programmatically monitoring Twitter (i.e. disaster relief organizations and\nnews agencies). Design a language model that is able to understand and\nacknowledge when a disaster is happening based on the social network posts will\nbecome more and more necessary over time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_A/0/1/0/all/0/1\">Anh Duc Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Semi-Supervised Deep Clustering Pipeline for Mining Intentions From Texts. (arXiv:2202.00802v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00802","description":"<p>Mining the latent intentions from large volumes of natural language inputs is\na key step to help data analysts design and refine Intelligent Virtual\nAssistants (IVAs) for customer service. To aid data analysts in this task we\npresent Verint Intent Manager (VIM), an analysis platform that combines\nunsupervised and semi-supervised approaches to help analysts quickly surface\nand organize relevant user intentions from conversational texts. For the\ninitial exploration of data we make use of a novel unsupervised and\nsemi-supervised pipeline that integrates the fine-tuning of high performing\nlanguage models, a distributed k-NN graph building method and community\ndetection techniques for mining the intentions and topics from texts. The\nfine-tuning step is necessary because pre-trained language models cannot encode\ntexts to efficiently surface particular clustering structures when the target\ntexts are from an unseen domain or the clustering task is not topic detection.\nFor flexibility we deploy two clustering approaches: where the number of\nclusters must be specified and where the number of clusters is detected\nautomatically with comparable clustering quality but at the expense of\nadditional computation time. We describe the application and deployment and\ndemonstrate its performance using BERT on three text mining tasks. Our\nexperiments show that BERT begins to produce better task-aware representations\nusing a labeled subset as small as 0.5% of the task data. The clustering\nquality exceeds the state-of-the-art results when BERT is fine-tuned with\nlabeled subsets of only 2.5% of the task data. As deployed in the VIM\napplication, this flexible clustering pipeline produces high quality results,\nimproving the performance of data analysts and reducing the time it takes to\nsurface intentions from customer service data, thereby reducing the time it\ntakes to build and deploy IVAs in new domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaver_I/0/1/0/all/0/1\">Ian Beaver</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Co-training Improves Prompt-based Learning for Large Language Models. (arXiv:2202.00828v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00828","description":"<p>We demonstrate that co-training (Blum &amp; Mitchell, 1998) can improve the\nperformance of prompt-based learning by using unlabeled data. While prompting\nhas emerged as a promising paradigm for few-shot and zero-shot learning, it is\noften brittle and requires much larger models compared to the standard\nsupervised setup. We find that co-training makes it possible to improve the\noriginal prompt model and at the same time learn a smaller, downstream\ntask-specific model. In the case where we only have partial access to a prompt\nmodel (e.g., output probabilities from GPT-3 (Brown et al., 2020)) we learn a\ncalibration model over the prompt outputs. When we have full access to the\nprompt model's gradients but full finetuning remains prohibitively expensive\n(e.g., T0 (Sanh et al., 2021)), we learn a set of soft prompt continuous\nvectors to iteratively update the prompt model. We find that models trained in\nthis manner can significantly improve performance on challenging datasets where\nthere is currently a large gap between prompt-based learning and\nfully-supervised models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1\">Hunter Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_M/0/1/0/all/0/1\">Monica Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1\">David Sontag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Streaming Multi-Talker ASR with Token-Level Serialized Output Training. (arXiv:2202.00842v1 [eess.AS])","link":"http://arxiv.org/abs/2202.00842","description":"<p>This paper proposes a token-level serialized output training (t-SOT), a novel\nframework for streaming multi-talker automatic speech recognition (ASR). Unlike\nexisting streaming multi-talker ASR models using multiple output layers, the\nt-SOT model has only a single output layer that generates recognition tokens\n(e.g., words, subwords) of multiple speakers in chronological order based on\ntheir emission times. A special token that indicates the change of \"virtual\"\noutput channels is introduced to keep track of the overlapping utterances.\nCompared to the prior streaming multi-talker ASR models, the t-SOT model has\nthe advantages of less inference cost and a simpler model architecture.\nMoreover, in our experiments with LibriSpeechMix and LibriCSS datasets, the\nt-SOT-based transformer transducer model achieves the state-of-the-art word\nerror rates by a significant margin to the prior results. For non-overlapping\nspeech, the t-SOT model is on par with a single-talker ASR model in terms of\nboth accuracy and computational cost, opening the door for deploying one model\nfor both single- and multi-talker scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Some Reflections on Drawing Causal Inference using Textual Data: Parallels Between Human Subjects and Organized Texts. (arXiv:2202.00848v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00848","description":"<p>We examine the role of textual data as study units when conducting causal\ninference by drawing parallels between human subjects and organized texts. %in\nhuman population research. We elaborate on key causal concepts and principles,\nand expose some ambiguity and sometimes fallacies. To facilitate better framing\na causal query, we discuss two strategies: (i) shifting from immutable traits\nto perceptions of them, and (ii) shifting from some abstract concept/property\nto its constituent parts, i.e., adopting a constructivist perspective of an\nabstract concept. We hope this article would raise the awareness of the\nimportance of articulating and clarifying fundamental concepts before delving\ninto developing methodologies when drawing causal inference using textual data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiayao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Longitudinal Dataset of Twitter ISIS Users. (arXiv:2202.00878v1 [cs.SI])","link":"http://arxiv.org/abs/2202.00878","description":"<p>We present a large longitudinal dataset of tweets from two sets of users that\nare suspected to be affiliated with ISIS. These sets of users are identified\nbased on a prior study and a campaign aimed at shutting down ISIS Twitter\naccounts. These users have engaged with known ISIS accounts at least once\nduring 2014-2015 and are still active as of 2021. Some of them have directly\nsupported the ISIS users and their tweets by retweeting them, and some of the\nusers that have quoted tweets of ISIS, have uncertain connections to ISIS seed\naccounts. This study and the dataset represent a unique approach to analyzing\nISIS data. Although much research exists on ISIS online activities, few studies\nhave focused on individual accounts. Our approach to validating accounts as\nwell as developing a framework for differentiating accounts' functionality\n(e.g., propaganda versus operational planning) offers a foundation for future\nresearch. We perform some descriptive statistics and preliminary analyses on\nour collected data to provide deeper insight and highlight the significance and\npracticality of such analyses. We further discuss several cross-disciplinary\npotential use cases and research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karimi_Y/0/1/0/all/0/1\">Younes Karimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Squicciarini_A/0/1/0/all/0/1\">Anna Squicciarini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forster_P/0/1/0/all/0/1\">Peter K. Forster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leavitt_K/0/1/0/all/0/1\">Kira M. Leavitt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Detection of Doxing on Twitter. (arXiv:2202.00879v1 [cs.SI])","link":"http://arxiv.org/abs/2202.00879","description":"<p>Doxing refers to the practice of disclosing sensitive personal information\nabout a person without their consent. This form of cyberbullying is an\nunpleasant and sometimes dangerous phenomenon for online social networks.\nAlthough prior work exists on automated identification of other types of\ncyberbullying, a need exists for methods capable of detecting doxing on Twitter\nspecifically. We propose and evaluate a set of approaches for automatically\ndetecting second- and third-party disclosures on Twitter of sensitive private\ninformation, a subset of which constitutes doxing. We summarize our findings of\ncommon intentions behind doxing episodes and compare nine different approaches\nfor automated detection based on string-matching and one-hot encoded\nheuristics, as well as word and contextualized string embedding representations\nof tweets. We identify an approach providing 96.86% accuracy and 97.37% recall\nusing contextualized string embeddings and conclude by discussing the\npracticality of our proposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karimi_Y/0/1/0/all/0/1\">Younes Karimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Squicciarini_A/0/1/0/all/0/1\">Anna Squicciarini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1\">Shomir Wilson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieve-and-Fill for Scenario-based Task-Oriented Semantic Parsing. (arXiv:2202.00901v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00901","description":"<p>Task-oriented semantic parsing models have achieved strong results in recent\nyears, but unfortunately do not strike an appealing balance between model size,\nruntime latency, and cross-domain generalizability. We tackle this problem by\nintroducing scenario-based semantic parsing: a variant of the original task\nwhich first requires disambiguating an utterance's \"scenario\" (an intent-slot\ntemplate with variable leaf spans) before generating its frame, complete with\nontology and utterance tokens. This formulation enables us to isolate\ncoarse-grained and fine-grained aspects of the task, each of which we solve\nwith off-the-shelf neural modules, also optimizing for the axes outlined above.\nConcretely, we create a Retrieve-and-Fill (RAF) architecture comprised of (1) a\nretrieval module which ranks the best scenario given an utterance and (2) a\nfilling module which imputes spans into the scenario to create the frame. Our\nmodel is modular, differentiable, interpretable, and allows us to garner extra\nsupervision from scenarios. RAF achieves strong results in high-resource,\nlow-resource, and multilingual settings, outperforming recent approaches by\nwide margins despite, using base pre-trained encoders, small sequence lengths,\nand parallel decoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Akshat Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Shrey Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anchit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elkahky_A/0/1/0/all/0/1\">Ali Elkahky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livshits_A/0/1/0/all/0/1\">Aleksandr Livshits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zotov_A/0/1/0/all/0/1\">Alexander Zotov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aly_A/0/1/0/all/0/1\">Ahmed Aly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Knowledge Integration in Language Models with Graph Convolutions. (arXiv:2202.00964v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00964","description":"<p>Pretrained language models (LMs) do not capture factual knowledge very well.\nThis has led to the development of a number of knowledge integration (KI)\nmethods which aim to incorporate external knowledge into pretrained LMs. Even\nthough KI methods show some performance gains over vanilla LMs, the\ninner-workings of these methods are not well-understood. For instance, it is\nunclear how and what kind of knowledge is effectively integrated into these\nmodels and if such integration may lead to catastrophic forgetting of already\nlearned knowledge. This paper revisits the KI process in these models with an\ninformation-theoretic view and shows that KI can be interpreted using a graph\nconvolution operation. We propose a probe model called \\textit{Graph\nConvolution Simulator} (GCS) for interpreting knowledge-enhanced LMs and\nexposing what kind of knowledge is integrated into these models. We conduct\nexperiments to verify that our GCS can indeed be used to correctly interpret\nthe KI process, and we use it to analyze two well-known knowledge-enhanced LMs:\nERNIE and K-Adapter, and find that only a small amount of factual knowledge is\nintegrated in them. We stratify knowledge in terms of various relation types\nand find that ERNIE and K-Adapter integrate different kinds of knowledge to\ndifferent extent. Our analysis also shows that simply increasing the size of\nthe KI corpus may not lead to better KI; fundamental advances may be needed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yifan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Guoji Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RescoreBERT: Discriminative Speech Recognition Rescoring with BERT. (arXiv:2202.01094v1 [eess.AS])","link":"http://arxiv.org/abs/2202.01094","description":"<p>Second-pass rescoring is an important component in automatic speech\nrecognition (ASR) systems that is used to improve the outputs from a first-pass\ndecoder by implementing a lattice rescoring or $n$-best re-ranking. While\npretraining with a masked language model (MLM) objective has received great\nsuccess in various natural language understanding (NLU) tasks, it has not\ngained traction as a rescoring model for ASR. Specifically, training a\nbidirectional model like BERT on a discriminative objective such as minimum WER\n(MWER) has not been explored. Here we where show how to train a BERT-based\nrescoring model with MWER loss, to incorporate the improvements of a\ndiscriminative loss into fine-tuning of deep bidirectional pretrained models\nfor ASR. We propose a fusion strategy that incorporates the MLM into the\ndiscriminative training process to effectively distill the knowledge from a\npretrained model. We further propose an alternative discriminative loss. We\nname this approach RescoreBERT, and evaluate it on the LibriSpeech corpus, and\nit reduces WER by 6.6%/3.4% relative on clean/other test sets over a BERT\nbaseline without discriminative objective. We also evaluate our method on an\ninternal dataset from a conversational agent and find that it reduces both\nlatency and WER (by 3-8% relative) over an LSTM rescoring model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1\">Liyan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1\">Yile Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kolehmainen_J/0/1/0/all/0/1\">Jari Kolehmainen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_H/0/1/0/all/0/1\">Haidar Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gandhe_A/0/1/0/all/0/1\">Ankur Gandhe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rastrow_A/0/1/0/all/0/1\">Ariya Rastrow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bulyko_I/0/1/0/all/0/1\">Ivan Bulyko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keyword localisation in untranscribed speech using visually grounded speech models. (arXiv:2202.01107v1 [cs.CL])","link":"http://arxiv.org/abs/2202.01107","description":"<p>Keyword localisation is the task of finding where in a speech utterance a\ngiven query keyword occurs. We investigate to what extent keyword localisation\nis possible using a visually grounded speech (VGS) model. VGS models are\ntrained on unlabelled images paired with spoken captions. These models are\ntherefore self-supervised -- trained without any explicit textual label or\nlocation information. To obtain training targets, we first tag training images\nwith soft text labels using a pretrained visual classifier with a fixed\nvocabulary. This enables a VGS model to predict the presence of a written\nkeyword in an utterance, but not its location. We consider four ways to equip\nVGS models with localisations capabilities. Two of these -- a saliency approach\nand input masking -- can be applied to an arbitrary prediction model after\ntraining, while the other two -- attention and a score aggregation approach --\nare incorporated directly into the structure of the model. Masked-based\nlocalisation gives some of the best reported localisation scores from a VGS\nmodel, with an accuracy of 57% when the system knows that a keyword occurs in\nan utterance and need to predict its location. In a setting where localisation\nis performed after detection, an $F_1$ of 25% is achieved, and in a setting\nwhere a keyword spotting ranking pass is first performed, we get a localisation\nP@10 of 32%. While these scores are modest compared to the idealised setting\nwith unordered bag-of-word-supervision (from transcriptions), these models do\nnot receive any textual or location supervision. Further analyses show that\nthese models are limited by the first detection or ranking pass. Moreover,\nindividual keyword localisation performance is correlated with the tagging\nperformance from the visual classifier. We also show qualitatively how and\nwhere semantic mistakes occur, e.g. that the model locates surfer when queried\nwith ocean.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Olaleye_K/0/1/0/all/0/1\">Kayode Olaleye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oneata_D/0/1/0/all/0/1\">Dan Oneata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Retrieval-Augmented Text Generation. (arXiv:2202.01110v1 [cs.CL])","link":"http://arxiv.org/abs/2202.01110","description":"<p>Recently, retrieval-augmented text generation attracted increasing attention\nof the computational linguistics community. Compared with conventional\ngeneration models, retrieval-augmented text generation has remarkable\nadvantages and particularly has achieved state-of-the-art performance in many\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable approaches according to different tasks\nincluding dialogue response generation, machine translation, and other\ngeneration tasks. Finally, it points out some important directions on top of\nrecent methods to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huayang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models Explain Word Reading Times Better Than Empirical Predictability. (arXiv:2202.01128v1 [cs.CL])","link":"http://arxiv.org/abs/2202.01128","description":"<p>Though there is a strong consensus that word length and frequency are the\nmost important single-word features determining visual-orthographic access to\nthe mental lexicon, there is less agreement as how to best capture syntactic\nand semantic factors. The traditional approach in cognitive reading research\nassumes that word predictability from sentence context is best captured by\ncloze completion probability (CCP) derived from human performance data. We\nreview recent research suggesting that probabilistic language models provide\ndeeper explanations for syntactic and semantic effects than CCP. Then we\ncompare CCP with (1) Symbolic n-gram models consolidate syntactic and semantic\nshort-range relations by computing the probability of a word to occur, given\ntwo preceding words. (2) Topic models rely on subsymbolic representations to\ncapture long-range semantic similarity by word co-occurrence counts in\ndocuments. (3) In recurrent neural networks (RNNs), the subsymbolic units are\ntrained to predict the next word, given all preceding words in the sentences.\nTo examine lexical retrieval, these models were used to predict single fixation\ndurations and gaze durations to capture rapidly successful and standard lexical\naccess, and total viewing time to capture late semantic integration. The linear\nitem-level analyses showed greater correlations of all language models with all\neye-movement measures than CCP. Then we examined non-linear relations between\nthe different types of predictability and the reading times using generalized\nadditive models. N-gram and RNN probabilities of the present word more\nconsistently predicted reading performance compared with topic models or CCP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_M/0/1/0/all/0/1\">Markus J. Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remus_S/0/1/0/all/0/1\">Steffen Remus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biemann_C/0/1/0/all/0/1\">Chris Biemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radach_R/0/1/0/all/0/1\">Ralph Radach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuchinke_L/0/1/0/all/0/1\">Lars Kuchinke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relative Position Prediction as Pre-training for Text Encoders. (arXiv:2202.01145v1 [cs.CL])","link":"http://arxiv.org/abs/2202.01145","description":"<p>Meaning is defined by the company it keeps. However, company is two-fold:\nIt's based on the identity of tokens and also on their position (topology). We\nargue that a position-centric perspective is more general and useful. The\nclassic MLM and CLM objectives in NLP are easily phrased as position\npredictions over the whole vocabulary. Adapting the relative position encoding\nparadigm in NLP to create relative labels for self-supervised learning, we seek\nto show superior pre-training judged by performance on downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bruel_Gabrielsson_R/0/1/0/all/0/1\">Rickard Br&#xfc;el-Gabrielsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarvelis_C/0/1/0/all/0/1\">Chris Scarvelis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The slurk Interaction Server Framework: Better Data for Better Dialog Models. (arXiv:2202.01155v1 [cs.CL])","link":"http://arxiv.org/abs/2202.01155","description":"<p>This paper presents the slurk software, a lightweight interaction server for\nsetting up dialog data collections and running experiments. Slurk enables a\nmultitude of settings including text-based, speech and video interaction\nbetween two or more humans or humans and bots, and a multimodal display area\nfor presenting shared or private interactive context. The software is\nimplemented in Python with an HTML and JS frontend that can easily be adapted\nto individual needs. It also provides a setup for pairing participants on\ncommon crowdworking platforms such as Amazon Mechanical Turk and some example\nbot scripts for common interaction scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gotze_J/0/1/0/all/0/1\">Jana G&#xf6;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paetzel_Prusmann_M/0/1/0/all/0/1\">Maike Paetzel-Pr&#xfc;smann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liermann_W/0/1/0/all/0/1\">Wencke Liermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diekmann_T/0/1/0/all/0/1\">Tim Diekmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlangen_D/0/1/0/all/0/1\">David Schlangen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Error Correction in ASR using Sequence-to-Sequence Models. (arXiv:2202.01157v1 [cs.CL])","link":"http://arxiv.org/abs/2202.01157","description":"<p>Post-editing in Automatic Speech Recognition (ASR) entails automatically\ncorrecting common and systematic errors produced by the ASR system. The outputs\nof an ASR system are largely prone to phonetic and spelling errors. In this\npaper, we propose to use a powerful pre-trained sequence-to-sequence model,\nBART, further adaptively trained to serve as a denoising model, to correct\nerrors of such types. The adaptive training is performed on an augmented\ndataset obtained by synthetically inducing errors as well as by incorporating\nactual errors from an existing ASR system. We also propose a simple approach to\nrescore the outputs using word level alignments. Experimental results on\naccented speech data demonstrate that our strategy effectively rectifies a\nsignificant number of ASR errors and produces improved WER results when\ncompared against a competitive baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Samrat Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shreyansh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1\">Ayush Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L3Cube-MahaCorpus and MahaBERT: Marathi Monolingual Corpus, Marathi BERT Language Models, and Resources. (arXiv:2202.01159v1 [cs.CL])","link":"http://arxiv.org/abs/2202.01159","description":"<p>We present L3Cube-MahaCorpus a Marathi monolingual data set scraped from\ndifferent internet sources. We expand the existing Marathi monolingual corpus\nwith 24.8M sentences and 289M tokens. We further present, MahaBERT, MahaAlBERT,\nand MahaRoBerta all BERT-based masked language models, and MahaFT, the fast\ntext word embeddings both trained on full Marathi corpus with 752M tokens. We\nshow the effectiveness of these resources on downstream classification and NER\ntasks. Marathi is a popular language in India but still lacks these resources.\nThis work is a step forward in building open resources for the Marathi\nlanguage. The data and models are available at\nhttps://github.com/l3cube-pune/MarathiNLP .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Scaling Laws for Routed Language Models. (arXiv:2202.01169v1 [cs.CL])","link":"http://arxiv.org/abs/2202.01169","description":"<p>The performance of a language model has been shown to be effectively modeled\nas a power-law in its parameter count. Here we study the scaling behaviors of\nRouting Networks: architectures that conditionally use only a subset of their\nparameters while processing an input. For these models, parameter count and\ncomputational requirement form two independent axes along which an increase\nleads to better performance. In this work we derive and justify scaling laws\ndefined on these two variables which generalize those known for standard\nlanguage models and describe the performance of a wide range of routing\narchitectures trained via three different techniques. Afterwards we provide two\napplications of these laws: first deriving an Effective Parameter Count along\nwhich all models scale at the same rate, and then using the scaling\ncoefficients to give a quantitative comparison of the three routing techniques\nconsidered. Our analysis derives from an extensive evaluation of Routing\nNetworks across five orders of magnitude of size, including models with\nhundreds of experts and hundreds of billions of parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clark_A/0/1/0/all/0/1\">Aidan Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casas_D/0/1/0/all/0/1\">Diego de las Casas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guy_A/0/1/0/all/0/1\">Aurelia Guy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensch_A/0/1/0/all/0/1\">Arthur Mensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paganini_M/0/1/0/all/0/1\">Michela Paganini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1\">Jordan Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damoc_B/0/1/0/all/0/1\">Bogdan Damoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hechtman_B/0/1/0/all/0/1\">Blake Hechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Trevor Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driessche_G/0/1/0/all/0/1\">George van den Driessche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rutherford_E/0/1/0/all/0/1\">Eliza Rutherford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigan_T/0/1/0/all/0/1\">Tom Hennigan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Matthew Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millican_K/0/1/0/all/0/1\">Katie Millican</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassirer_A/0/1/0/all/0/1\">Albin Cassirer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_C/0/1/0/all/0/1\">Chris Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buchatskaya_E/0/1/0/all/0/1\">Elena Buchatskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budden_D/0/1/0/all/0/1\">David Budden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifre_L/0/1/0/all/0/1\">Laurent Sifre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osindero_S/0/1/0/all/0/1\">Simon Osindero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rae_J/0/1/0/all/0/1\">Jack Rae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsen_E/0/1/0/all/0/1\">Erich Elsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavukcuoglu_K/0/1/0/all/0/1\">Koray Kavukcuoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonyan_K/0/1/0/all/0/1\">Karen Simonyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Epidemic Dreams: Dreaming about health during the COVID-19 pandemic. (arXiv:2202.01176v1 [cs.SI])","link":"http://arxiv.org/abs/2202.01176","description":"<p>The continuity hypothesis of dreams suggests that the content of dreams is\ncontinuous with the dreamer's waking experiences. Given the unprecedented\nnature of the experiences during COVID-19, we studied the continuity hypothesis\nin the context of the pandemic. We implemented a deep-learning algorithm that\ncan extract mentions of medical conditions from text and applied it to two\ndatasets collected during the pandemic: 2,888 dream reports (dreaming life\nexperiences), and 57M tweets mentioning the pandemic (waking life experiences).\nThe health expressions common to both sets were typical COVID-19 symptoms\n(e.g., cough, fever, and anxiety), suggesting that dreams reflected people's\nreal-world experiences. The health expressions that distinguished the two sets\nreflected differences in thought processes: expressions in waking life\nreflected a linear and logical thought process and, as such, described\nrealistic symptoms or related disorders (e.g., nasal pain, SARS, H1N1); those\nin dreaming life reflected a thought process closer to the visual and emotional\nspheres and, as such, described either conditions unrelated to the virus (e.g.,\nmaggots, deformities, snakebites), or conditions of surreal nature (e.g., teeth\nfalling out, body crumbling into sand). Our results confirm that dream reports\nrepresent an understudied yet valuable source of people's health experiences in\nthe real world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scepanovic_S/0/1/0/all/0/1\">Sanja &#x160;&#x107;epanovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aiello_L/0/1/0/all/0/1\">Luca Maria Aiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrett_D/0/1/0/all/0/1\">Deirdre Barrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quercia_D/0/1/0/all/0/1\">Daniele Quercia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information Extraction through AI techniques: The KIDs use case at CONSOB. (arXiv:2202.01178v1 [cs.CL])","link":"http://arxiv.org/abs/2202.01178","description":"<p>In this paper we report on the initial activities carried out within a\ncollaboration between Consob and Sapienza University. We focus on Information\nExtraction from documents describing financial instruments. We discuss how we\nautomate this task, via both rule-based and machine learning-based methods and\nprovide our first results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lembo_D/0/1/0/all/0/1\">Domenico Lembo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Limosani_A/0/1/0/all/0/1\">Alessandra Limosani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medda_F/0/1/0/all/0/1\">Francesca Medda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monaco_A/0/1/0/all/0/1\">Alessandra Monaco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scafoglieri_F/0/1/0/all/0/1\">Federico Maria Scafoglieri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Comparison of Architectures for Document-Level Sentiment Classification. (arXiv:2002.08131v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2002.08131","description":"<p>Documents are composed of smaller pieces - paragraphs, sentences, and tokens\n- that have complex relationships between one another. Sentiment classification\nmodels that take into account the structure inherent in these documents have a\ntheoretical advantage over those that do not. At the same time, transfer\nlearning models based on language model pretraining have shown promise for\ndocument classification. However, these two paradigms have not been\nsystematically compared and it is not clear under which circumstances one\napproach is better than the other. In this work we empirically compare\nhierarchical models and transfer learning for document-level sentiment\nclassification. We show that non-trivial hierarchical models outperform\nprevious baselines and transfer learning on document-level sentiment\nclassification in five languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barnes_J/0/1/0/all/0/1\">Jeremy Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravishankar_V/0/1/0/all/0/1\">Vinit Ravishankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovrelid_L/0/1/0/all/0/1\">Lilja &#xd8;vrelid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velldal_E/0/1/0/all/0/1\">Erik Velldal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Specifying and Interpreting Reinforcement Learning Policies through Simulatable Machine Learning. (arXiv:2101.07140v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.07140","description":"<p>Human-AI collaborative policy synthesis is a procedure in which (1) a human\ninitializes an autonomous agent's behavior, (2) Reinforcement Learning improves\nthe human specified behavior, and (3) the agent can explain the final optimized\npolicy to the user. This paradigm leverages human expertise and facilitates a\ngreater insight into the learned behaviors of an agent. Existing approaches to\nenabling collaborative policy specification involve black box methods which are\nunintelligible and are not catered towards non-expert end-users. In this paper,\nwe develop a novel collaborative framework to enable humans to initialize and\ninterpret an autonomous agent's behavior, rooted in principles of\nhuman-centered design. Through our framework, we enable humans to specify an\ninitial behavior model in the form of unstructured, natural language, which we\nthen convert to lexical decision trees. Next, we are able to leverage these\nhuman-specified policies, to warm-start reinforcement learning and further\nallow the agent to optimize the policies through reinforcement learning.\nFinally, to close the loop on human-specification, we produce explanations of\nthe final learned policy, in multiple modalities, to provide the user a final\ndepiction about the learned policy of the agent. We validate our approach by\nshowing that our model can produce &gt;80% accuracy, and that human-initialized\npolicies are able to successfully warm-start RL. We then conduct a novel\nhuman-subjects study quantifying the relative subjective and objective benefits\nof varying XAI modalities(e.g., Tree, Language, and Program) for explaining\nlearned policies to end-users, in terms of usability and interpretability and\nidentify the circumstances that influence these measures. Our findings\nemphasize the need for personalized explainable systems that can facilitate\nuser-centric policy explanations for a variety of end-users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tambwekar_P/0/1/0/all/0/1\">Pradyumna Tambwekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1\">Andrew Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalan_N/0/1/0/all/0/1\">Nakul Gopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gombolay_M/0/1/0/all/0/1\">Matthew Gombolay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Question Answering using Graph-Pattern Isomorphism. (arXiv:2103.06752v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2103.06752","description":"<p>Knowledge Graph Question Answering (KGQA) systems are based on machine\nlearning algorithms, requiring thousands of question-answer pairs as training\nexamples or natural language processing pipelines that need module fine-tuning.\nIn this paper, we present a novel QA approach, dubbed TeBaQA. Our approach\nlearns to answer questions based on graph isomorphisms from basic graph\npatterns of SPARQL queries. Learning basic graph patterns is efficient due to\nthe small number of possible patterns. This novel paradigm reduces the amount\nof training data necessary to achieve state-of-the-art performance. TeBaQA also\nspeeds up the domain adaption process by transforming the QA system development\ntask into a much smaller and easier data compilation task. In our evaluation,\nTeBaQA achieves state-of-the-art performance on QALD-8 and delivers comparable\nresults on QALD-9 and LC-QuAD v1. Additionally, we performed a fine-grained\nevaluation on complex queries that deal with aggregation and superlative\nquestions as well as an ablation study, highlighting future research\nchallenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vollmers_D/0/1/0/all/0/1\">Daniel Vollmers</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Jalota_R/0/1/0/all/0/1\">Rricha Jalota</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Moussallem_D/0/1/0/all/0/1\">Diego Moussallem</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Topiwala_H/0/1/0/all/0/1\">Hardik Topiwala</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1\">Axel-Cyrille Ngonga Ngomo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1\">Ricardo Usbeck</a> (2) ((1) Data Science Group, Paderborn University, Germany, (2) Fraunhofer IAIS, Dresden, Germany)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs. (arXiv:2106.12144v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.12144","description":"<p>Conventional representation learning algorithms for knowledge graphs (KG) map\neach entity to a unique embedding vector. Such a shallow lookup results in a\nlinear growth of memory consumption for storing the embedding matrix and incurs\nhigh computational costs when working with real-world KGs. Drawing parallels\nwith subword tokenization commonly used in NLP, we explore the landscape of\nmore parameter-efficient node embedding strategies with possibly sublinear\nmemory requirements. To this end, we propose NodePiece, an anchor-based\napproach to learn a fixed-size entity vocabulary. In NodePiece, a vocabulary of\nsubword/sub-entity units is constructed from anchor nodes in a graph with known\nrelation types. Given such a fixed-size vocabulary, it is possible to bootstrap\nan encoding and embedding for any entity, including those unseen during\ntraining. Experiments show that NodePiece performs competitively in node\nclassification, link prediction, and relation prediction tasks while retaining\nless than 10% of explicit nodes in a graph as anchors and often having 10x\nfewer parameters. To this end, we show that a NodePiece-enabled model\noutperforms existing shallow models on a large OGB WikiKG 2 graph having 70x\nfewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1\">Mikhail Galkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denis_E/0/1/0/all/0/1\">Etienne Denis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiapeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1\">William L. Hamilton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations. (arXiv:2106.12479v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.12479","description":"<p>Knowledge is acquired by humans through experience, and no boundary is set\nbetween the kinds of knowledge or skill levels we can achieve on different\ntasks at the same time. When it comes to Neural Networks, that is not the case.\nThe breakthroughs in the field are extremely task and domain-specific. Vision\nand language are dealt with in separate manners, using separate methods and\ndifferent datasets. Current text classification methods, mostly rely on\nobtaining contextual embeddings for input text samples, then training a\nclassifier on the embedded dataset. Transfer learning in Language-related tasks\nin general, is heavily used in obtaining the contextual text embeddings for the\ninput samples. In this work, we propose to use the knowledge acquired by\nbenchmark Vision Models which are trained on ImageNet to help a much smaller\narchitecture learn to classify text. A data transformation technique is used to\ncreate a new image dataset, where each image represents a sentence embedding\nfrom the last six layers of BERT, projected on a 2D plane using a t-SNE based\nmethod. We trained five models containing early layers sliced from vision\nmodels which are pretrained on ImageNet, on the created image dataset for the\nIMDB dataset embedded with the last six layers of BERT. Despite the challenges\nposed by the very different datasets, experimental results achieved by this\napproach which links large pretrained models on both language and vision, are\nvery promising, without employing compute resources. Specifically, Sentiment\nAnalysis is achieved by five different models on the same image dataset\nobtained after BERT embeddings are transformed into gray scale images.\n</p>\n<p>Index Terms: BERT, Convolutional Neural Networks, Domain Adaptation, image\nclassification, Natural Language Processing, t-SNE, text classification,\nTransfer Learning\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benarab_C/0/1/0/all/0/1\">Charaf Eddine Benarab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Language Model for Efficient Linguistic Steganalysis. (arXiv:2107.12168v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.12168","description":"<p>Recent advances in linguistic steganalysis have successively applied CNN,\nRNN, GNN and other efficient deep models for detecting secret information in\ngenerative texts. These methods tend to seek stronger feature extractors to\nachieve higher steganalysis effects. However, we have found through experiments\nthat there actually exists significant difference between automatically\ngenerated stego texts and carrier texts in terms of the conditional probability\ndistribution of individual words. Such kind of difference can be naturally\ncaptured by the language model used for generating stego texts. Through further\nexperiments, we conclude that this ability can be transplanted to a text\nclassifier by pre-training and fine-tuning to improve the detection\nperformance. Motivated by this insight, we propose two methods for efficient\nlinguistic steganalysis. One is to pre-train a language model based on RNN, and\nthe other is to pre-train a sequence autoencoder. The results indicate that the\ntwo methods have different degrees of performance gain compared to the randomly\ninitialized RNN, and the convergence speed is significantly accelerated.\nMoreover, our methods achieved the best performance compared to related works,\nwhile providing a solution for real-world scenario where there are more cover\ntexts than stego texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_B/0/1/0/all/0/1\">Biao Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanzhou Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1\">Guorui Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient DP-SGD Mechanism for Large Scale NLP Models. (arXiv:2107.14586v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.14586","description":"<p>Recent advances in deep learning have drastically improved performance on\nmany Natural Language Understanding (NLU) tasks. However, the data used to\ntrain NLU models may contain private information such as addresses or phone\nnumbers, particularly when drawn from human subjects. It is desirable that\nunderlying models do not expose private information contained in the training\ndata. Differentially Private Stochastic Gradient Descent (DP-SGD) has been\nproposed as a mechanism to build privacy-preserving models. However, DP-SGD can\nbe prohibitively slow to train. In this work, we propose a more efficient\nDP-SGD for training using a GPU infrastructure and apply it to fine-tuning\nmodels based on LSTM and transformer architectures. We report faster training\ntimes, alongside accuracy, theoretical privacy guarantees and success of\nMembership inference attacks for our models and observe that fine-tuning with\nproposed variant of DP-SGD can yield competitive models without significant\ndegradation in training time and improvement in privacy protection. We also\nmake observations such as looser theoretical $\\epsilon, \\delta$ can translate\ninto significant practical privacy gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dupuy_C/0/1/0/all/0/1\">Christophe Dupuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arava_R/0/1/0/all/0/1\">Radhika Arava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey of Low-Resource Machine Translation. (arXiv:2109.00486v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00486","description":"<p>We present a survey covering the state of the art in low-resource machine\ntranslation research. There are currently around 7000 languages spoken in the\nworld and almost all language pairs lack significant resources for training\nmachine translation models. There has been increasing interest in research\naddressing the challenge of producing useful translation models when very\nlittle translated training data is available. We present a summary of this\ntopical research field and provide a description of the techniques evaluated by\nresearchers in several recent shared tasks in low-resource MT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haddow_B/0/1/0/all/0/1\">Barry Haddow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bawden_R/0/1/0/all/0/1\">Rachel Bawden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barone_A/0/1/0/all/0/1\">Antonio Valerio Miceli Barone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helcl_J/0/1/0/all/0/1\">Jind&#x159;ich Helcl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1\">Alexandra Birch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Unified View of Parameter-Efficient Transfer Learning. (arXiv:2110.04366v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04366","description":"<p>Fine-tuning large pre-trained language models on downstream tasks has become\nthe de-facto learning paradigm in NLP. However, conventional approaches\nfine-tune all the parameters of the pre-trained model, which becomes\nprohibitive as the model size and the number of tasks grow. Recent work has\nproposed a variety of parameter-efficient transfer learning methods that only\nfine-tune a small number of (extra) parameters to attain strong performance.\nWhile effective, the critical ingredients for success and the connections among\nthe various methods are poorly understood. In this paper, we break down the\ndesign of state-of-the-art parameter-efficient transfer learning methods and\npresent a unified framework that establishes connections between them.\nSpecifically, we re-frame them as modifications to specific hidden states in\npre-trained models, and define a set of design dimensions along which different\nmethods vary, such as the function to compute the modification and the position\nto apply the modification. Through comprehensive empirical studies across\nmachine translation, text summarization, language understanding, and text\nclassification benchmarks, we utilize the unified view to identify important\ndesign choices in previous methods. Furthermore, our unified framework enables\nthe transfer of design elements across different approaches, and as a result we\nare able to instantiate new parameter-efficient fine-tuning methods that tune\nless parameters than previous methods while being more effective, achieving\ncomparable results to fine-tuning all parameters on all four tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chunting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuezhe Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Pretrained Language Models Based Text Generation. (arXiv:2201.05273v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05273","description":"<p>Text Generation aims to produce plausible and readable text in human language\nfrom input data. The resurgence of deep learning has greatly advanced this\nfield by neural generation models, especially the paradigm of pretrained\nlanguage models (PLMs). Grounding text generation on PLMs is seen as a\npromising direction in both academia and industry. In this survey, we present\nthe recent advances achieved in the topic of PLMs for text generation. In\ndetail, we begin with introducing three key points of applying PLMs to text\ngeneration: 1) how to encode the input data as representations preserving input\nsemantics which can be fused into PLMs; 2) how to design a universal and\nperformant architecture of PLMs served as generation models; and 3) how to\noptimize PLMs given the reference text and ensure the generated text satisfying\nspecial text properties. Then, we figure out several challenges and future\ndirections within each key point. Next, we present a summary of various useful\nresources and typical text generation applications to work with PLMs. Finally,\nwe conclude and summarize the contribution of this survey.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Dependencies in Adversarial Attacks on Speech Recognition Systems. (arXiv:2202.00399v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00399","description":"<p>Automatic speech recognition (ASR) systems are ubiquitously present in our\ndaily devices. They are vulnerable to adversarial attacks, where manipulated\ninput samples fool the ASR system's recognition. While adversarial examples for\nvarious English ASR systems have already been analyzed, there exists no\ninter-language comparative vulnerability analysis. We compare the attackability\nof a German and an English ASR system, taking Deepspeech as an example. We\ninvestigate if one of the language models is more susceptible to manipulations\nthan the other. The results of our experiments suggest statistically\nsignificant differences between English and German in terms of computational\neffort necessary for the successful generation of adversarial examples. This\nresult encourages further research in language-dependent characteristics in the\nrobustness analysis of ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Markert_K/0/1/0/all/0/1\">Karla Markert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirdita_D/0/1/0/all/0/1\">Donika Mirdita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bottinger_K/0/1/0/all/0/1\">Konstantin B&#xf6;ttinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal effect of racial bias in data and machine learning algorithms on user persuasiveness & discriminatory decision making: An Empirical Study. (arXiv:2202.00471v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00471","description":"<p>Language data and models demonstrate various types of bias, be it ethnic,\nreligious, gender, or socioeconomic. AI/NLP models, when trained on the\nracially biased dataset, AI/NLP models instigate poor model explainability,\ninfluence user experience during decision making and thus further magnifies\nsocietal biases, raising profound ethical implications for society. The\nmotivation of the study is to investigate how AI systems imbibe bias from data\nand produce unexplainable discriminatory outcomes and influence an individual's\narticulateness of system outcome due to the presence of racial bias features in\ndatasets. The design of the experiment involves studying the counterfactual\nimpact of racial bias features present in language datasets and its associated\neffect on the model outcome. A mixed research methodology is adopted to\ninvestigate the cross implication of biased model outcome on user experience,\neffect on decision-making through controlled lab experimentation. The findings\nprovide foundation support for correlating the implication of carry-over an\nartificial intelligence model solving NLP task due to biased concept presented\nin the dataset. Further, the research outcomes justify the negative influence\non users' persuasiveness that leads to alter the decision-making quotient of an\nindividual when trying to rely on the model outcome to act. The paper bridges\nthe gap across the harm caused in establishing poor customer trustworthiness\ndue to an inequitable system design and provides strong support for\nresearchers, policymakers, and data scientists to build responsible AI\nframeworks within organizations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_K/0/1/0/all/0/1\">Kinshuk Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1\">Praveen Ranjan Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining Scaling and Transfer of Language Model Architectures for Machine Translation. (arXiv:2202.00528v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00528","description":"<p>Natural language understanding and generation models follow one of the two\ndominant architectural paradigms: language models (LMs) that process\nconcatenated sequences in a single stack of layers, and encoder-decoder models\n(EncDec) that utilize separate layer stacks for input and output processing. In\nmachine translation, EncDec has long been the favoured approach, but with few\nstudies investigating the performance of LMs. In this work, we thoroughly\nexamine the role of several architectural design choices on the performance of\nLMs on bilingual, (massively) multilingual and zero-shot translation tasks,\nunder systematic variations of data conditions and model sizes. Our results\nshow that: (i) Different LMs have different scaling properties, where\narchitectural differences often have a significant impact on model performance\nat small scales, but the performance gap narrows as the number of parameters\nincreases, (ii) Several design choices, including causal masking and\nlanguage-modeling objectives for the source sequence, have detrimental effects\non translation quality, and (iii) When paired with full-visible masking for\nsource sequences, LMs could perform on par with EncDec on supervised bilingual\nand multilingual translation tasks, and improve greatly on zero-shot directions\nby facilitating the reduction of off-target translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Biao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_B/0/1/0/all/0/1\">Behrooz Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jonathan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A training-free recursive multiresolution framework for diffeomorphic deformable image registration. (arXiv:2202.00675v1 [eess.IV])","link":"http://arxiv.org/abs/2202.00675","description":"<p>Diffeomorphic deformable image registration is one of the crucial tasks in\nmedical image analysis, which aims to find a unique transformation while\npreserving the topology and invertibility of the transformation. Deep\nconvolutional neural networks (CNNs) have yielded well-suited approaches for\nimage registration by learning the transformation priors from a large dataset.\nThe improvement in the performance of these methods is related to their ability\nto learn information from several sample medical images that are difficult to\nobtain and bias the framework to the specific domain of data. In this paper, we\npropose a novel diffeomorphic training-free approach; this is built upon the\nprinciple of an ordinary differential equation.\n</p>\n<p>Our formulation yields an Euler integration type recursive scheme to estimate\nthe changes of spatial transformations between the fixed and the moving image\npyramids at different resolutions. The proposed architecture is simple in\ndesign. The moving image is warped successively at each resolution and finally\naligned to the fixed image; this procedure is recursive in a way that at each\nresolution, a fully convolutional network (FCN) models a progressive change of\ndeformation for the current warped image. The entire system is end-to-end and\noptimized for each pair of images from scratch. In comparison to learning-based\nmethods, the proposed method neither requires a dedicated training set nor\nsuffers from any training bias. We evaluate our method on three cardiac image\ndatasets. The evaluation results demonstrate that the proposed method achieves\nstate-of-the-art registration accuracy while maintaining desirable\ndiffeomorphic properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sheikhjafari_A/0/1/0/all/0/1\">Ameneh Sheikhjafari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Noga_M/0/1/0/all/0/1\">Michelle Noga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Punithakumar_K/0/1/0/all/0/1\">Kumaradevan Punithakumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ray_N/0/1/0/all/0/1\">Nilanjan Ray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A deep residual learning implementation of Metamorphosis. (arXiv:2202.00676v1 [eess.IV])","link":"http://arxiv.org/abs/2202.00676","description":"<p>In medical imaging, most of the image registration methods implicitly assume\na one-to-one correspondence between the source and target images (i.e.,\ndiffeomorphism). However, this is not necessarily the case when dealing with\npathological medical images (e.g., presence of a tumor, lesion, etc.). To cope\nwith this issue, the Metamorphosis model has been proposed. It modifies both\nthe shape and the appearance of an image to deal with the geometrical and\ntopological differences. However, the high computational time and load have\nhampered its applications so far. Here, we propose a deep residual learning\nimplementation of Metamorphosis that drastically reduces the computational time\nat inference. Furthermore, we also show that the proposed framework can easily\nintegrate prior knowledge of the localization of topological changes (e.g.,\nsegmentation masks) that can act as spatial regularization to correctly\ndisentangle appearance and shape changes. We test our method on the BraTS 2021\ndataset, showing that it outperforms current state-of-the-art methods in the\nalignment of images with brain tumors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Maillard_M/0/1/0/all/0/1\">Matthis Maillard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Francois_A/0/1/0/all/0/1\">Anton Fran&#xe7;ois</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glaunes_J/0/1/0/all/0/1\">Joan Glaun&#xe8;s</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bloch_I/0/1/0/all/0/1\">Isabelle Bloch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gori_P/0/1/0/all/0/1\">Pietro Gori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Embarrassingly Simple Consistency Regularization Method for Semi-Supervised Medical Image Segmentation. (arXiv:2202.00677v1 [eess.IV])","link":"http://arxiv.org/abs/2202.00677","description":"<p>The scarcity of pixel-level annotation is a prevalent problem in medical\nimage segmentation tasks. In this paper, we introduce a novel regularization\nstrategy involving interpolation-based mixing for semi-supervised medical image\nsegmentation. The proposed method is a new consistency regularization strategy\nthat encourages segmentation of interpolation of two unlabelled data to be\nconsistent with the interpolation of segmentation maps of those data. This\nmethod represents a specific type of data-adaptive regularization paradigm\nwhich aids to minimize the overfitting of labelled data under high confidence\nvalues. The proposed method is advantageous over adversarial and generative\nmodels as it requires no additional computation. Upon evaluation on two\npublicly available MRI datasets: ACDC and MMWHS, experimental results\ndemonstrate the superiority of the proposed method in comparison to existing\nsemi-supervised models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Basak_H/0/1/0/all/0/1\">Hritam Basak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhattacharya_R/0/1/0/all/0/1\">Rajarshi Bhattacharya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hussain_R/0/1/0/all/0/1\">Rukhshanda Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chatterjee_A/0/1/0/all/0/1\">Agniv Chatterjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Skin Cancer Images using Convolutional Neural Networks. (arXiv:2202.00678v1 [eess.IV])","link":"http://arxiv.org/abs/2202.00678","description":"<p>Skin cancer is the most common human malignancy(American Cancer Society)\nwhich is primarily diagnosed visually, starting with an initial clinical\nscreening and followed potentially by dermoscopic(related to skin) analysis, a\nbiopsy and histopathological examination. Skin cancer occurs when errors\n(mutations) occur in the DNA of skin cells. The mutations cause the cells to\ngrow out of control and form a mass of cancer cells. The aim of this study was\nto try to classify images of skin lesions with the help of convolutional neural\nnetworks. The deep neural networks show humongous potential for image\nclassification while taking into account the large variability exhibited by the\nenvironment. Here we trained images based on the pixel values and classified\nthem on the basis of disease labels. The dataset was acquired from an Open\nSource Kaggle Repository(Kaggle Dataset)which itself was acquired from\nISIC(International Skin Imaging Collaboration) Archive. The training was\nperformed on multiple models accompanied with Transfer Learning. The highest\nmodel accuracy achieved was over 86.65%. The dataset used is publicly available\nto ensure credibility and reproducibility of the aforementioned result.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_K/0/1/0/all/0/1\">Kartikeya Agarwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_T/0/1/0/all/0/1\">Tismeet Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Should I take a walk? Estimating Energy Expenditure from Video Data. (arXiv:2202.00712v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00712","description":"<p>We explore the problem of automatically inferring the amount of kilocalories\nused by human during physical activity from his/her video observation. To study\nthis underresearched task, we introduce Vid2Burn -- an omni-source benchmark\nfor estimating caloric expenditure from video data featuring both, high- and\nlow-intensity activities for which we derive energy expenditure annotations\nbased on models established in medical literature. In practice, a training set\nwould only cover a certain amount of activity types, and it is important to\nvalidate, if the model indeed captures the essence of energy expenditure,\n(e.g., how many and which muscles are involved and how intense they work)\ninstead of memorizing fixed values of specific activity categories seen during\ntraining. Ideally, the models should look beyond such category-specific biases\nand regress the caloric cost in videos depicting activity categories not\nexplicitly present during training. With this property in mind, Vid2Burn is\naccompanied with a cross-category benchmark, where the task is to regress\ncaloric expenditure for types of physical activities not present during\ntraining. An extensive evaluation of state-of-the-art approaches for video\nrecognition modified for the energy expenditure estimation task demonstrates\nthe difficulty of this problem, especially for new activity types at test-time,\nmarking a new research direction. Dataset and code are available at\nhttps://github.com/KPeng9510/Vid2Burn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1\">Alina Roitberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IFOR: Iterative Flow Minimization for Robotic Object Rearrangement. (arXiv:2202.00732v1 [cs.RO])","link":"http://arxiv.org/abs/2202.00732","description":"<p>Accurate object rearrangement from vision is a crucial problem for a wide\nvariety of real-world robotics applications in unstructured environments. We\npropose IFOR, Iterative Flow Minimization for Robotic Object Rearrangement, an\nend-to-end method for the challenging problem of object rearrangement for\nunknown objects given an RGBD image of the original and final scenes. First, we\nlearn an optical flow model based on RAFT to estimate the relative\ntransformation of the objects purely from synthetic data. This flow is then\nused in an iterative minimization algorithm to achieve accurate positioning of\npreviously unseen objects. Crucially, we show that our method applies to\ncluttered scenes, and in the real world, while training only on synthetic data.\nVideos are available at https://imankgoyal.github.io/ifor.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1\">Ankit Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mousavian_A/0/1/0/all/0/1\">Arsalan Mousavian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_Y/0/1/0/all/0/1\">Yu-Wei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okorn_B/0/1/0/all/0/1\">Brian Okorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jia Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Positive Jacobian: Learn to Postprocess Diffeomorphic Image Registration with Matrix Exponential. (arXiv:2202.00749v1 [eess.IV])","link":"http://arxiv.org/abs/2202.00749","description":"<p>We present a postprocessing layer for deformable image registration to make a\nregistration field more diffeomorphic by encouraging Jacobians of the\ntransformation to be positive. Diffeomorphic image registration is important\nfor medical imaging studies because of the properties like invertibility,\nsmoothness of the transformation, and topology preservation/non-folding of the\ngrid. Violation of these properties can lead to destruction of the\nneighbourhood and the connectivity of anatomical structures during image\nregistration. Most of the recent deep learning methods do not explicitly\naddress this folding problem and try to solve it with a smoothness\nregularization on the registration field. In this paper, we propose a\ndifferentiable layer, which takes any registration field as its input, computes\nexponential of the Jacobian matrices of the input and reconstructs a new\nregistration field from the exponentiated Jacobian matrices using Poisson\nreconstruction. Our proposed Poisson reconstruction loss enforces positive\nJacobians for the final registration field. Thus, our method acts as a\npost-processing layer without any learnable parameters of its own and can be\nplaced at the end of any deep learning pipeline to form an end-to-end learnable\nframework. We show the effectiveness of our proposed method for a popular deep\nlearning registration method Voxelmorph and evaluate it with a dataset\ncontaining 3D brain MRI scans. Our results show that our post-processing can\neffectively decrease the number of non-positive Jacobians by a significant\namount without any noticeable deterioration of the registration accuracy, thus\nmaking the registration field more diffeomorphic. Our code is available online\nat\nhttps://github.com/Soumyadeep-Pal/Diffeomorphic-Image-Registration-Postprocess.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pal_S/0/1/0/all/0/1\">Soumyadeep Pal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tennant_M/0/1/0/all/0/1\">Matthew Tennant</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ray_N/0/1/0/all/0/1\">Nilanjan Ray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADG-Pose: Automated Dataset Generation for Real-World Human Pose Estimation. (arXiv:2202.00753v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00753","description":"<p>Recent advancements in computer vision have seen a rise in the prominence of\napplications using neural networks to understand human poses. However, while\naccuracy has been steadily increasing on State-of-the-Art datasets, these\ndatasets often do not address the challenges seen in real-world applications.\nThese challenges are dealing with people distant from the camera, people in\ncrowds, and heavily occluded people. As a result, many real-world applications\nhave trained on data that does not reflect the data present in deployment,\nleading to significant underperformance. This article presents ADG-Pose, a\nmethod for automatically generating datasets for real-world human pose\nestimation. These datasets can be customized to determine person distances,\ncrowdedness, and occlusion distributions. Models trained with our method are\nable to perform in the presence of these challenges where those trained on\nother datasets fail. Using ADG-Pose, end-to-end accuracy for real-world\nskeleton-based action recognition sees a 20% increase on scenes with moderate\ndistance and occlusion levels, and a 4X increase on distant scenes where other\nmodels failed to perform better than random.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noghre_G/0/1/0/all/0/1\">Ghazal Alinezhad Noghre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pazho_A/0/1/0/all/0/1\">Armin Danesh Pazho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_J/0/1/0/all/0/1\">Justin Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hewitt_N/0/1/0/all/0/1\">Nathan Hewitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neff_C/0/1/0/all/0/1\">Christopher Neff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1\">Hamed Tabkhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Model for Multi-View Residual Covariances based on Perspective Deformation. (arXiv:2202.00765v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00765","description":"<p>In this work, we derive a model for the covariance of the visual residuals in\nmulti-view SfM, odometry and SLAM setups. The core of our approach is the\nformulation of the residual covariances as a combination of geometric and\nphotometric noise sources. And our key novel contribution is the derivation of\na term modelling how local 2D patches suffer from perspective deformation when\nimaging 3D surfaces around a point. Together, these add up to an efficient and\ngeneral formulation which not only improves the accuracy of both feature-based\nand direct methods, but can also be used to estimate more accurate measures of\nthe state entropy and hence better founded point visibility thresholds. We\nvalidate our model with synthetic and real data and integrate it into\nphotometric and feature-based Bundle Adjustment, improving their accuracy with\na negligible overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fontan_A/0/1/0/all/0/1\">Alejandro Fontan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_L/0/1/0/all/0/1\">Laura Oliva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Javier Civera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1\">Rudolph Triebel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Feature Matching with Transformers for low-end devices. (arXiv:2202.00770v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00770","description":"<p>LoFTR <a href=\"/abs/2104.00680\">arXiv:2104.00680</a> is an efficient deep learning method for finding\nappropriate local feature matches on image pairs. This paper reports on the\noptimization of this method to work on devices with low computational\nperformance and limited memory. The original LoFTR approach is based on a\nResNet <a href=\"/abs/1512.03385\">arXiv:1512.03385</a> head and two modules based on Linear Transformer\n<a href=\"/abs/2006.04768\">arXiv:2006.04768</a> architecture. In the presented work, only the coarse-matching\nblock was left, the number of parameters was significantly reduced, and the\nnetwork was trained using a knowledge distillation technique. The comparison\nshowed that this approach allows to obtain an appropriate feature detection\naccuracy for the student model compared to the teacher model in the coarse\nmatching block, despite the significant reduction of model size. Also, the\npaper shows additional steps required to make model compatible with NVIDIA\nTensorRT runtime, and shows an approach to optimize training method for low-end\nGPUs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolodiazhnyi_K/0/1/0/all/0/1\">Kyrylo Kolodiazhnyi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating DNN Training with Structured Data Gradient Pruning. (arXiv:2202.00774v1 [cs.LG])","link":"http://arxiv.org/abs/2202.00774","description":"<p>Weight pruning is a technique to make Deep Neural Network (DNN) inference\nmore computationally efficient by reducing the number of model parameters over\nthe course of training. However, most weight pruning techniques generally does\nnot speed up DNN training and can even require more iterations to reach model\nconvergence. In this work, we propose a novel Structured Data Gradient Pruning\n(SDGP) method that can speed up training without impacting model convergence.\nThis approach enforces a specific sparsity structure, where only N out of every\nM elements in a matrix can be nonzero, making it amenable to hardware\nacceleration. Modern accelerators such as the Nvidia A100 GPU support this type\nof structured sparsity for 2 nonzeros per 4 elements in a reduction. Assuming\nhardware support for 2:4 sparsity, our approach can achieve a 15-25\\% reduction\nin total training time without significant impact to performance. Source code\nand pre-trained models are available at\n\\url{https://github.com/BradMcDanel/sdgp}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McDanel_B/0/1/0/all/0/1\">Bradley McDanel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinh_H/0/1/0/all/0/1\">Helia Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magallanes_J/0/1/0/all/0/1\">John Magallanes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Regularizing Coordinate-MLPs. (arXiv:2202.00790v1 [cs.LG])","link":"http://arxiv.org/abs/2202.00790","description":"<p>We show that typical implicit regularization assumptions for deep neural\nnetworks (for regression) do not hold for coordinate-MLPs, a family of MLPs\nthat are now ubiquitous in computer vision for representing high-frequency\nsignals. Lack of such implicit bias disrupts smooth interpolations between\ntraining samples, and hampers generalizing across signal regions with different\nspectra. We investigate this behavior through a Fourier lens and uncover that\nas the bandwidth of a coordinate-MLP is enhanced, lower frequencies tend to get\nsuppressed unless a suitable prior is provided explicitly. Based on these\ninsights, we propose a simple regularization technique that can mitigate the\nabove problem, which can be incorporated into existing networks without any\narchitectural modifications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramasinghe_S/0/1/0/all/0/1\">Sameera Ramasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacDonald_L/0/1/0/all/0/1\">Lachlan MacDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1\">Simon Lucey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mars Terrain Segmentation with Less Labels. (arXiv:2202.00791v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00791","description":"<p>Planetary rover systems need to perform terrain segmentation to identify\ndrivable areas as well as identify specific types of soil for sample\ncollection. The latest Martian terrain segmentation methods rely on supervised\nlearning which is very data hungry and difficult to train where only a small\nnumber of labeled samples are available. Moreover, the semantic classes are\ndefined differently for different applications (e.g., rover traversal vs.\ngeological) and as a result the network has to be trained from scratch each\ntime, which is an inefficient use of resources. This research proposes a\nsemi-supervised learning framework for Mars terrain segmentation where a deep\nsegmentation network trained in an unsupervised manner on unlabeled images is\ntransferred to the task of terrain segmentation trained on few labeled images.\nThe network incorporates a backbone module which is trained using a contrastive\nloss function and an output atrous convolution module which is trained using a\npixel-wise cross-entropy loss function. Evaluation results using the metric of\nsegmentation accuracy show that the proposed method with contrastive\npretraining outperforms plain supervised learning by 2%-10%. Moreover, the\nproposed model is able to achieve a segmentation accuracy of 91.1% using only\n161 training images (1% of the original dataset) compared to 81.9% with plain\nsupervised learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goh_E/0/1/0/all/0/1\">Edwin Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingdao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_B/0/1/0/all/0/1\">Brian Wilson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Graph Based Neural Network Approach to Immune Profiling of Multiplexed Tissue Samples. (arXiv:2202.00813v1 [cs.LG])","link":"http://arxiv.org/abs/2202.00813","description":"<p>Multiplexed immunofluorescence provides an unprecedented opportunity for\nstudying specific cell-to-cell and cell microenvironment interactions. We\nemploy graph neural networks to combine features obtained from tissue\nmorphology with measurements of protein expression to profile the tumour\nmicroenvironment associated with different tumour stages. Our framework\npresents a new approach to analysing and processing these complex\nmulti-dimensional datasets that overcomes some of the key challenges in\nanalysing these data and opens up the opportunity to abstract biologically\nmeaningful interactions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martin_N/0/1/0/all/0/1\">Natalia Garcia Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malacrino_S/0/1/0/all/0/1\">Stefano Malacrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wojciechowska_M/0/1/0/all/0/1\">Marta Wojciechowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campo_L/0/1/0/all/0/1\">Leticia Campo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_H/0/1/0/all/0/1\">Helen Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wedge_D/0/1/0/all/0/1\">David C. Wedge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holmes_C/0/1/0/all/0/1\">Chris Holmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sirinukunwattana_K/0/1/0/all/0/1\">Korsuk Sirinukunwattana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sailem_H/0/1/0/all/0/1\">Heba Sailem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verrill_C/0/1/0/all/0/1\">Clare Verrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rittscher_J/0/1/0/all/0/1\">Jens Rittscher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On-Sensor Binarized Fully Convolutional Neural Network with A Pixel Processor Array. (arXiv:2202.00836v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00836","description":"<p>This work presents a method to implement fully convolutional neural networks\n(FCNs) on Pixel Processor Array (PPA) sensors, and demonstrates coarse\nsegmentation and object localisation tasks. We design and train binarized FCN\nfor both binary weights and activations using batchnorm, group convolution, and\nlearnable threshold for binarization, producing networks small enough to be\nembedded on the focal plane of the PPA, with limited local memory resources,\nand using parallel elementary add/subtract, shifting, and bit operations only.\nWe demonstrate the first implementation of an FCN on a PPA device, performing\nthree convolution layers entirely in the pixel-level processors. We use this\narchitecture to demonstrate inference generating heat maps for object\nsegmentation and localisation at over 280 FPS using the SCAMP-5 PPA vision\nchip.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bose_L/0/1/0/all/0/1\">Laurie Bose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudek_P/0/1/0/all/0/1\">Piotr Dudek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayol_Cuevas_W/0/1/0/all/0/1\">Walterio Mayol-Cuevas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks. (arXiv:2202.00838v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00838","description":"<p>Recent work suggests that representations learned by adversarially robust\nnetworks are more human perceptually-aligned than non-robust networks via image\nmanipulations. Despite appearing closer to human visual perception, it is\nunclear if the constraints in robust DNN representations match biological\nconstraints found in human vision. Human vision seems to rely on\ntexture-based/summary statistic representations in the periphery, which have\nbeen shown to explain phenomena such as crowding and performance on visual\nsearch tasks. To understand how adversarially robust\noptimizations/representations compare to human vision, we performed a\npsychophysics experiment using a set of metameric discrimination tasks where we\nevaluated how well human observers could distinguish between images synthesized\nto match adversarially robust representations compared to non-robust\nrepresentations and a texture synthesis model of peripheral vision (Texforms).\nWe found that the discriminability of robust representation and texture model\nimages decreased to near chance performance as stimuli were presented farther\nin the periphery. Moreover, performance on robust and texture-model images\nshowed similar trends within participants, while performance on non-robust\nrepresentations changed minimally across the visual field. These results\ntogether suggest that (1) adversarially robust representations capture\nperipheral computation better than non-robust representations and (2) robust\nrepresentations capture peripheral computation similar to current\nstate-of-the-art texture peripheral vision models. More broadly, our findings\nsupport the idea that localized texture summary statistic representations may\ndrive human invariance to adversarial perturbations and that the incorporation\nof such representations in DNNs could give rise to useful properties like\nadversarial robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harrington_A/0/1/0/all/0/1\">Anne Harrington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deza_A/0/1/0/all/0/1\">Arturo Deza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pose Guided Image Generation from Misaligned Sources via Residual Flow Based Correction. (arXiv:2202.00843v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00843","description":"<p>Generating new images with desired properties (e.g. new view/poses) from\nsource images has been enthusiastically pursued recently, due to its wide range\nof potential applications. One way to ensure high-quality generation is to use\nmultiple sources with complementary information such as different views of the\nsame object. However, as source images are often misaligned due to the large\ndisparities among the camera settings, strong assumptions have been made in the\npast with respect to the camera(s) or/and the object in interest, limiting the\napplication of such techniques. Therefore, we propose a new general approach\nwhich models multiple types of variations among sources, such as view angles,\nposes, facial expressions, in a unified framework, so that it can be employed\non datasets of vastly different nature. We verify our approach on a variety of\ndata including humans bodies, faces, city scenes and 3D objects. Both the\nqualitative and quantitative results demonstrate the better performance of our\nmethod than the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiawei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_T/0/1/0/all/0/1\">Tianjia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Audio-Visual Separation of Dynamic Sound Sources. (arXiv:2202.00850v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00850","description":"<p>We explore active audio-visual separation for dynamic sound sources, where an\nembodied agent moves intelligently in a 3D environment to continuously isolate\nthe time-varying audio stream being emitted by an object of interest. The agent\nhears a mixed stream of multiple time-varying audio sources (e.g., multiple\npeople conversing and a band playing music at a noisy party). Given a limited\ntime budget, it needs to extract the target sound using egocentric audio-visual\nobservations. We propose a reinforcement learning agent equipped with a novel\ntransformer memory that learns motion policies to control its camera and\nmicrophone to recover the dynamic target audio, improving its own estimates for\npast timesteps via self-attention. Using highly realistic acoustic SoundSpaces\nsimulations in real-world scanned Matterport3D environments, we show that our\nmodel is able to learn efficient behavior to carry out continuous separation of\na time-varying audio target. Project:\nhttps://vision.cs.utexas.edu/projects/active-av-dynamic-separation/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_S/0/1/0/all/0/1\">Sagnik Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Halah_Z/0/1/0/all/0/1\">Ziad Al-Halah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extension -- Adaptive Sampling with Implicit Radiance Field. (arXiv:2202.00855v1 [cs.GR])","link":"http://arxiv.org/abs/2202.00855","description":"<p>This paper aims to explore and summarize the state-of-the-art progress in\nMonte Carlo adaptive light field sampling and reconstruction using deep\nreinforcement learning, with possible extension to it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuchi Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupled IoU Regression for Object Detection. (arXiv:2202.00866v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00866","description":"<p>Non-maximum suppression (NMS) is widely used in object detection pipelines\nfor removing duplicated bounding boxes. The inconsistency between the\nconfidence for NMS and the real localization confidence seriously affects\ndetection performance. Prior works propose to predict Intersection-over-Union\n(IoU) between bounding boxes and corresponding ground-truths to improve NMS,\nwhile accurately predicting IoU is still a challenging problem. We argue that\nthe complex definition of IoU and feature misalignment make it difficult to\npredict IoU accurately. In this paper, we propose a novel Decoupled IoU\nRegression (DIR) model to handle these problems. The proposed DIR decouples the\ntraditional localization confidence metric IoU into two new metrics, Purity and\nIntegrity. Purity reflects the proportion of the object area in the detected\nbounding box, and Integrity refers to the completeness of the detected object\narea. Separately predicting Purity and Integrity can divide the complex mapping\nbetween the bounding box and its IoU into two clearer mappings and model them\nindependently. In addition, a simple but effective feature realignment approach\nis also introduced to make the IoU regressor work in a hindsight manner, which\ncan make the target mapping more stable. The proposed DIR can be conveniently\nintegrated with existing two-stage detectors and significantly improve their\nperformance. Through a simple implementation of DIR with HTC, we obtain 51.3%\nAP on MS COCO benchmark, which outperforms previous methods and achieves\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qimeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haochen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1\">Fei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yao Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automotive Parts Assessment: Applying Real-time Instance-Segmentation Models to Identify Vehicle Parts. (arXiv:2202.00884v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00884","description":"<p>The problem of automated car damage assessment presents a major challenge in\nthe auto repair and damage assessment industry. The domain has several\napplication areas ranging from car assessment companies such as car rentals and\nbody shops to accidental damage assessment for car insurance companies. In\nvehicle assessment, the damage can take any form including scratches, minor and\nmajor dents to missing parts. More often, the assessment area has a significant\nlevel of noise such as dirt, grease, oil or rush that makes an accurate\nidentification challenging. Moreover, the identification of a particular part\nis the first step in the repair industry to have an accurate labour and part\nassessment where the presence of different car models, shapes and sizes makes\nthe task even more challenging for a machine-learning model to perform well. To\naddress these challenges, this research explores and applies various instance\nsegmentation methodologies to evaluate the best performing models.\n</p>\n<p>The scope of this work focusses on two genres of real-time instance\nsegmentation models due to their industrial significance, namely SipMask and\nYolact. These methodologies are evaluated against a previously reported car\nparts dataset (DSMLR) and an internally curated dataset extracted from local\ncar repair workshops. The Yolact-based part localization and segmentation\nmethod performed well when compared to other real-time instance mechanisms with\na mAP of 66.5. For the workshop repair dataset, SipMask++ reported better\naccuracies for object detection with a mAP of 57.0 with outcomes for\nAP_IoU=.50and AP_IoU=.75 reporting 72.0 and 67.0 respectively while Yolact was\nfound to be a better performer for AP_s with 44.0 and 2.6 for object detection\nand segmentation categories respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yusuf_S/0/1/0/all/0/1\">Syed Adnan Yusuf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldawsari_A/0/1/0/all/0/1\">Abdulmalik Ali Aldawsari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souissi_R/0/1/0/all/0/1\">Riad Souissi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate calibration of surround view camera systems from a generalization of the hand eye constraint. (arXiv:2202.00886v1 [cs.RO])","link":"http://arxiv.org/abs/2202.00886","description":"<p>Multi-perspective cameras are quickly gaining importance in many applications\nsuch as smart vehicles and virtual or augmented reality. However, a large\nsystem size or absence of overlap in neighbouring fields-of-view often\ncomplicate their calibration. We present a novel solution which relies on the\navailability of an external motion capture system. Our core contribution\nconsists of an extension to the hand-eye calibration problem which jointly\nsolves multi-eye-to-base problems in closed form. We furthermore demonstrate\nits equivalence to the multi-eye-in-hand problem. The practical validity of our\napproach is supported by our experiments, indicating that the method is highly\nefficient and accurate, and outperforms existing closed-form alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenqing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwertfeger_S/0/1/0/all/0/1\">Soren Schwertfeger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1\">Laurent Kneip</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Video Compression Impact Tracking Accuracy?. (arXiv:2202.00892v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00892","description":"<p>Everyone \"knows\" that compressing a video will degrade the accuracy of object\ntracking. Yet, a literature search on this topic reveals that there is very\nlittle documented evidence for this presumed fact. Part of the reason is that,\nuntil recently, there were no object tracking datasets for uncompressed video,\nwhich made studying the effects of compression on tracking accuracy difficult.\nIn this paper, using a recently published dataset that contains tracking\nannotations for uncompressed videos, we examined the degradation of tracking\naccuracy due to video compression using rigorous statistical methods.\nSpecifically, we examined the impact of quantization parameter (QP) and motion\nsearch range (MSR) on Multiple Object Tracking Accuracy (MOTA). The results\nshow that QP impacts MOTA at the 95% confidence level, while there is\ninsufficient evidence to claim that MSR impacts MOTA. Moreover, regression\nanalysis allows us to derive a quantitative relationship between MOTA and QP\nfor the specific tracker used in the experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1\">Takehiro Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harell_A/0/1/0/all/0/1\">Alon Harell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajic_I/0/1/0/all/0/1\">Ivan V. Baji&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Forgery Detection with Interpretability. (arXiv:2202.00908v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00908","description":"<p>In this work, we present a learning based method focusing on the\nconvolutional neural network (CNN) architecture to detect these forgeries. We\nconsider the detection of both copy-move forgeries and inpainting based\nforgeries. For these, we synthesize our own large dataset. In addition to\nclassification, the focus is also on interpretability of the forgery detection.\nAs the CNN classification yields the image-level label, it is important to\nunderstand if forged region has indeed contributed to the classification. For\nthis purpose, we demonstrate using the Grad-CAM heatmap, that in various\ncorrectly classified examples, that the forged region is indeed the region\ncontributing to the classification. Interestingly, this is also applicable for\nsmall forged regions, as is depicted in our results. Such an analysis can also\nhelp in establishing the reliability of the classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katiyar_A/0/1/0/all/0/1\">Ankit Katiyar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhavsar_A/0/1/0/all/0/1\">Arnav Bhavsar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CSFlow: Learning Optical Flow via Cross Strip Correlation for Autonomous Driving. (arXiv:2202.00909v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00909","description":"<p>Optical flow estimation is an essential task in self-driving systems, which\nhelps autonomous vehicles perceive temporal continuity information of\nsurrounding scenes. The calculation of all-pair correlation plays an important\nrole in many existing state-of-the-art optical flow estimation methods.\nHowever, the reliance on local knowledge often limits the model's accuracy\nunder complex street scenes. In this paper, we propose a new deep network\narchitecture for optical flow estimation in autonomous driving--CSFlow, which\nconsists of two novel modules: Cross Strip Correlation module (CSC) and\nCorrelation Regression Initialization module (CRI). CSC utilizes a striping\noperation across the target image and the attended image to encode global\ncontext into correlation volumes, while maintaining high efficiency. CRI is\nused to maximally exploit the global context for optical flow initialization.\nOur method has achieved state-of-the-art accuracy on the public autonomous\ndriving dataset KITTI-2015. Code is publicly available at\nhttps://github.com/MasterHow/CSFlow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yifan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xiaoting Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaiwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eikonal Fields for Refractive Novel-View Synthesis. (arXiv:2202.00948v1 [cs.GR])","link":"http://arxiv.org/abs/2202.00948","description":"<p>We tackle the problem of generating novel-view images from collections of 2D\nimages showing refractive and reflective objects. Current solutions assume\nopaque or transparent light transport along straight paths following the\nemission-absorption model. Instead, we optimize for a field of 3D-varying Index\nof Refraction (IoR) and trace light through it that bends toward the spatial\ngradients of said IoR according to the laws of eikonal light transport.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bemana_M/0/1/0/all/0/1\">Mojtaba Bemana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myszkowski_K/0/1/0/all/0/1\">Karol Myszkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frisvad_J/0/1/0/all/0/1\">Jeppe Revall Frisvad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seidel_H/0/1/0/all/0/1\">Hans-Peter Seidel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1\">Tobias Ritschel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GANSlider: How Users Control Generative Models for Images using Multiple Sliders with and without Feedforward Information. (arXiv:2202.00965v1 [cs.HC])","link":"http://arxiv.org/abs/2202.00965","description":"<p>We investigate how multiple sliders with and without feedforward\nvisualizations influence users' control of generative models. In an online\nstudy (N=138), we collected a dataset of people interacting with a generative\nadversarial network (StyleGAN2) in an image reconstruction task. We found that\nmore control dimensions (sliders) significantly increase task difficulty and\nuser actions. Visual feedforward partly mitigates this by enabling more\ngoal-directed interaction. However, we found no evidence of faster or more\naccurate task performance. This indicates a tradeoff between feedforward detail\nand implied cognitive costs, such as attention. Moreover, we found that\nvisualizations alone are not always sufficient for users to understand\nindividual control dimensions. Our study quantifies fundamental UI design\nfactors and resulting interaction behavior in this context, revealing\nopportunities for improvement in the UI design for interactive applications of\ngenerative models. We close by discussing design directions and further\naspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dang_H/0/1/0/all/0/1\">Hai Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mecke_L/0/1/0/all/0/1\">Lukas Mecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buschek_D/0/1/0/all/0/1\">Daniel Buschek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DCSAU-Net: A Deeper and More Compact Split-Attention U-Net for Medical Image Segmentation. (arXiv:2202.00972v1 [eess.IV])","link":"http://arxiv.org/abs/2202.00972","description":"<p>Image segmentation is a key step for medical image analysis. Approaches based\non deep neural networks have been introduced and performed more reliable\nresults than traditional image processing methods. However, many models focus\non one medical image application and still show limited abilities to work with\ncomplex images. In this paper, we propose a novel deeper and more compact\nsplit-attention u-shape network (DCSAU-Net) that extracts useful features using\nmulti-scale combined split-attention and deeper depthwise convolution. We\nevaluate the proposed model on CVC-ClinicDB, 2018 Data Science Bowl, ISIC-2018\nand SegPC-2021 datasets. As a result, DCSAU-Net displays better performance\nthan other state-of-the-art (SOTA) methods in terms of the mean Intersection\nover Union (mIoU) and F1-socre. More significantly, the proposed model\ndemonstrate better segmentation performance on challenging images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_Q/0/1/0/all/0/1\">Qing Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_W/0/1/0/all/0/1\">Wenting Duan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_N/0/1/0/all/0/1\">Na He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dictionary learning for clustering on hyperspectral images. (arXiv:2202.00990v1 [eess.IV])","link":"http://arxiv.org/abs/2202.00990","description":"<p>Dictionary learning and sparse coding have been widely studied as mechanisms\nfor unsupervised feature learning. Unsupervised learning could bring enormous\nbenefit to the processing of hyperspectral images and to other remote sensing\ndata analysis because labelled data are often scarce in this field. We propose\na method for clustering the pixels of hyperspectral images using sparse\ncoefficients computed from a representative dictionary as features. We show\nempirically that the proposed method works more effectively than clustering on\nthe original pixels. We also demonstrate that our approach, in certain\ncircumstances, outperforms the clustering results of features extracted using\nprincipal component analysis and non-negative matrix factorisation.\nFurthermore, our method is suitable for applications in repetitively clustering\nan ever-growing amount of high-dimensional data, which is the case when working\nwith hyperspectral satellite imagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bruton_J/0/1/0/all/0/1\">Joshua Bruton</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Hairong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient Variance Loss for Structure-Enhanced Image Super-Resolution. (arXiv:2202.00997v1 [eess.IV])","link":"http://arxiv.org/abs/2202.00997","description":"<p>Recent success in the field of single image super-resolution (SISR) is\nachieved by optimizing deep convolutional neural networks (CNNs) in the image\nspace with the L1 or L2 loss. However, when trained with these loss functions,\nmodels usually fail to recover sharp edges present in the high-resolution (HR)\nimages for the reason that the model tends to give a statistical average of\npotential HR solutions. During our research, we observe that gradient maps of\nimages generated by the models trained with the L1 or L2 loss have\nsignificantly lower variance than the gradient maps of the original\nhigh-resolution images. In this work, we propose to alleviate the above issue\nby introducing a structure-enhancing loss function, coined Gradient Variance\n(GV) loss, and generate textures with perceptual-pleasant details.\nSpecifically, during the training of the model, we extract patches from the\ngradient maps of the target and generated output, calculate the variance of\neach patch and form variance maps for these two images. Further, we minimize\nthe distance between the computed variance maps to enforce the model to produce\nhigh variance gradient maps that will lead to the generation of high-resolution\nimages with sharper edges. Experimental results show that the GV loss can\nsignificantly improve both Structure Similarity (SSIM) and peak signal-to-noise\nratio (PSNR) performance of existing image super-resolution (SR) deep learning\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Abrahamyan_L/0/1/0/all/0/1\">Lusine Abrahamyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Truong_A/0/1/0/all/0/1\">Anh Minh Truong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Philips_W/0/1/0/all/0/1\">Wilfried Philips</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deligiannis_N/0/1/0/all/0/1\">Nikos Deligiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-Transfer: Learning to Route Transferrable Representations. (arXiv:2202.01011v1 [cs.LG])","link":"http://arxiv.org/abs/2202.01011","description":"<p>Knowledge transfer between heterogeneous source and target networks and tasks\nhas received a lot of attention in recent times as large amounts of quality\nlabelled data can be difficult to obtain in many applications. Existing\napproaches typically constrain the target deep neural network (DNN) feature\nrepresentations to be close to the source DNNs feature representations, which\ncan be limiting. We, in this paper, propose a novel adversarial multi-armed\nbandit approach which automatically learns to route source representations to\nappropriate target representations following which they are combined in\nmeaningful ways to produce accurate target models. We see upwards of 5%\naccuracy improvements compared with the state-of-the-art knowledge transfer\nmethods on four benchmark (target) image datasets CUB200, Stanford Dogs, MIT67,\nand Stanford40 where the source dataset is ImageNet. We qualitatively analyze\nthe goodness of our transfer scheme by showing individual examples of the\nimportant features our target network focuses on in different layers compared\nwith the (closest) competitors. We also observe that our improvement over other\nmethods is higher for smaller target datasets making it an effective tool for\nsmall data applications that may benefit from transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murugesan_K/0/1/0/all/0/1\">Keerthiram Murugesan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sadashivaiah_V/0/1/0/all/0/1\">Vijay Sadashivaiah</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Luss_R/0/1/0/all/0/1\">Ronny Luss</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1\">Karthikeyan Shanmugam</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Dhurandhar_A/0/1/0/all/0/1\">Amit Dhurandhar</a> (1) ((1) IBM Research, Yorktown Heights, (2) Rensselaer Polytechnic Institute, New York)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray. (arXiv:2202.01020v1 [eess.IV])","link":"http://arxiv.org/abs/2202.01020","description":"<p>Computed tomography (CT) is an effective medical imaging modality, widely\nused in the field of clinical medicine for the diagnosis of various\npathologies. Advances in Multidetector CT imaging technology have enabled\nadditional functionalities, including generation of thin slice multiplanar\ncross-sectional body imaging and 3D reconstructions. However, this involves\npatients being exposed to a considerable dose of ionising radiation. Excessive\nionising radiation can lead to deterministic and harmful effects on the body.\nThis paper proposes a Deep Learning model that learns to reconstruct CT\nprojections from a few or even a single-view X-ray. This is based on a novel\narchitecture that builds from neural radiance fields, which learns a continuous\nrepresentation of CT scans by disentangling the shape and volumetric depth of\nsurface and internal anatomical structures from 2D images. Our model is trained\non chest and knee datasets, and we demonstrate qualitative and quantitative\nhigh-fidelity renderings and compare our approach to other recent radiance\nfield-based methods. Our code and link to our datasets will be available at our\nGitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Corona_Figueroa_A/0/1/0/all/0/1\">Abril Corona-Figueroa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frawley_J/0/1/0/all/0/1\">Jonathan Frawley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bond_Taylor_S/0/1/0/all/0/1\">Sam Bond-Taylor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bethapudi_S/0/1/0/all/0/1\">Sarath Bethapudi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shum_H/0/1/0/all/0/1\">Hubert P. H. Shum</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Willcocks_C/0/1/0/all/0/1\">Chris G. Willcocks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMSys'22 Grand Challenge on AI-based Video Production for Soccer. (arXiv:2202.01031v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01031","description":"<p>Soccer has a considerable market share of the global sports industry, and the\ninterest in viewing videos from soccer games continues to grow. In this\nrespect, it is important to provide game summaries and highlights of the main\ngame events. However, annotating and producing events and summaries often\nrequire expensive equipment and a lot of tedious, cumbersome, manual labor.\nTherefore, automating the video production pipeline providing fast game\nhighlights at a much lower cost is seen as the \"holy grail\". In this context,\nrecent developments in Artificial Intelligence (AI) technology have shown great\npotential. Still, state-of-the-art approaches are far from being adequate for\npractical scenarios that have demanding real-time requirements, as well as\nstrict performance criteria (where at least the detection of official events\nsuch as goals and cards must be 100% accurate). In addition, event detection\nshould be thoroughly enhanced by annotation and classification, proper\nclipping, generating short descriptions, selecting appropriate thumbnails for\nhighlight clips, and finally, combining the event highlights into an overall\ngame summary, similar to what is commonly aired during sports news. Even though\nthe event tagging operation has by far received the most attention, an\nend-to-end video production pipeline also includes various other operations\nwhich serve the overall purpose of automated soccer analysis. This challenge\naims to assist the automation of such a production pipeline using AI. In\nparticular, we focus on the enhancement operations that take place after an\nevent has been detected, namely event clipping (Task 1), thumbnail selection\n(Task 2), and game summarization (Task 3). Challenge website:\nhttps://mmsys2022.ie/authors/grand-challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Midoglu_C/0/1/0/all/0/1\">Cise Midoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hicks_S/0/1/0/all/0/1\">Steven A. Hicks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thambawita_V/0/1/0/all/0/1\">Vajira Thambawita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kupka_T/0/1/0/all/0/1\">Tomas Kupka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-based Navigation in Real-World Environments via Multiple Mid-level Representations: Fusion Models, Benchmark and Efficient Evaluation. (arXiv:2202.01069v1 [cs.RO])","link":"http://arxiv.org/abs/2202.01069","description":"<p>Navigating complex indoor environments requires a deep understanding of the\nspace the robotic agent is acting into to correctly inform the navigation\nprocess of the agent towards the goal location. In recent learning-based\nnavigation approaches, the scene understanding and navigation abilities of the\nagent are achieved simultaneously by collecting the required experience in\nsimulation. Unfortunately, even if simulators represent an efficient tool to\ntrain navigation policies, the resulting models often fail when transferred\ninto the real world. One possible solution is to provide the navigation model\nwith mid-level visual representations containing important domain-invariant\nproperties of the scene. But, what are the best representations that facilitate\nthe transfer of a model to the real-world? How can they be combined? In this\nwork we address these issues by proposing a benchmark of Deep Learning\narchitectures to combine a range of mid-level visual representations, to\nperform a PointGoal navigation task following a Reinforcement Learning setup.\nAll the proposed navigation models have been trained with the Habitat simulator\non a synthetic office environment and have been tested on the same real-world\nenvironment using a real robotic platform. To efficiently assess their\nperformance in a real context, a validation tool has been proposed to generate\nrealistic navigation episodes inside the simulator. Our experiments showed that\nnavigation models can benefit from the multi-modal input and that our\nvalidation tool can provide good estimation of the expected navigation\nperformance in the real world, while saving time and resources. The acquired\nsynthetic and real 3D models of the environment, together with the code of our\nvalidation tool built on top of Habitat, are publicly available at the\nfollowing link: https://iplab.dmi.unict.it/EmbodiedVN/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosano_M/0/1/0/all/0/1\">Marco Rosano</a> (1 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1\">Antonino Furnari</a> (1 and 5), <a href=\"http://arxiv.org/find/cs/1/au:+Gulino_L/0/1/0/all/0/1\">Luigi Gulino</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Santoro_C/0/1/0/all/0/1\">Corrado Santoro</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1\">Giovanni Maria Farinella</a> (1 and 4 and 5) ((1) FPV@IPLAB - Department of Mathematics and Computer Science - University of Catania - Italy, (2) Robotics Laboratory - Department of Mathematics and Computer Science - University of Catania - Italy, (3) OrangeDev s.r.l. - Firenze - Italy, (4) Cognitive Robotics and Social Sensing Laboratory - ICAR-CNR - Palermo - Italy, (5) Next Vision s.r.l. - Catania - Italy)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unpaired Image Super-Resolution with Optimal Transport Maps. (arXiv:2202.01116v1 [eess.IV])","link":"http://arxiv.org/abs/2202.01116","description":"<p>Real-world image super-resolution (SR) tasks often do not have paired\ndatasets limiting the application of supervised techniques. As a result, the\ntasks are usually approached by unpaired techniques based on Generative\nAdversarial Networks (GANs) which yield complex training losses with several\nregularization terms such as content and identity losses. We theoretically\ninvestigate the optimization problems which arise in such models and find two\nsurprising observations. First, the learned SR map is always an optimal\ntransport (OT) map. Second, we empirically show that the learned map is biased,\ni.e., it may not actually transform the distribution of low-resolution images\nto high-resolution images. Inspired by these findings, we propose an algorithm\nfor unpaired SR which learns an unbiased OT map for the perceptual transport\ncost. Unlike existing GAN-based alternatives, our algorithm has a simple\noptimization objective reducing the neccesity to perform complex hyperparameter\nselection and use additional regularizations. At the same time, it provides\nnearly state-of-the-art performance on the large-scale unpaired AIM-19 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gazdieva_M/0/1/0/all/0/1\">Milena Gazdieva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rout_L/0/1/0/all/0/1\">Litu Rout</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korotin_A/0/1/0/all/0/1\">Alexander Korotin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Filippov_A/0/1/0/all/0/1\">Alexander Filippov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Eye for an Eye: Defending against Gradient-based Attacks with Gradients. (arXiv:2202.01117v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01117","description":"<p>Deep learning models have been shown to be vulnerable to adversarial attacks.\nIn particular, gradient-based attacks have demonstrated high success rates\nrecently. The gradient measures how each image pixel affects the model output,\nwhich contains critical information for generating malicious perturbations. In\nthis paper, we show that the gradients can also be exploited as a powerful\nweapon to defend against adversarial attacks. By using both gradient maps and\nadversarial images as inputs, we propose a Two-stream Restoration Network (TRN)\nto restore the adversarial images. To optimally restore the perturbed images\nwith two streams of inputs, a Gradient Map Estimation Mechanism is proposed to\nestimate the gradients of adversarial images, and a Fusion Block is designed in\nTRN to explore and fuse the information in two streams. Once trained, our TRN\ncan defend against a wide range of attack methods without significantly\ndegrading the performance of benign inputs. Also, our method is generalizable,\nscalable, and hard to bypass. Experimental results on CIFAR10, SVHN, and\nFashion MNIST demonstrate that our method outperforms state-of-the-art defense\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1\">Hanbin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yuan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Yu Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistically Robust Learning: Balancing Average- and Worst-case Performance. (arXiv:2202.01136v1 [cs.LG])","link":"http://arxiv.org/abs/2202.01136","description":"<p>Many of the successes of machine learning are based on minimizing an averaged\nloss function. However, it is well-known that this paradigm suffers from\nrobustness issues that hinder its applicability in safety-critical domains.\nThese issues are often addressed by training against worst-case perturbations\nof data, a technique known as adversarial training. Although empirically\neffective, adversarial training can be overly conservative, leading to\nunfavorable trade-offs between nominal performance and robustness. To this end,\nin this paper we propose a framework called probabilistic robustness that\nbridges the gap between the accurate, yet brittle average case and the robust,\nyet conservative worst case by enforcing robustness to most rather than to all\nperturbations. From a theoretical point of view, this framework overcomes the\ntrade-offs between the performance and the sample-complexity of worst-case and\naverage-case learning. From a practical point of view, we propose a novel\nalgorithm based on risk-aware optimization that effectively balances average-\nand worst-case performance at a considerably lower computational cost relative\nto adversarial training. Our results on MNIST, CIFAR-10, and SVHN illustrate\nthe advantages of this framework on the spectrum from average- to worst-case\nrobustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robey_A/0/1/0/all/0/1\">Alexander Robey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chamon_L/0/1/0/all/0/1\">Luiz F. O. Chamon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1\">George J. Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1\">Hamed Hassani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AntidoteRT: Run-time Detection and Correction of Poison Attacks on Neural Networks. (arXiv:2202.01179v1 [cs.CR])","link":"http://arxiv.org/abs/2202.01179","description":"<p>We study backdoor poisoning attacks against image classification networks,\nwhereby an attacker inserts a trigger into a subset of the training data, in\nsuch a way that at test time, this trigger causes the classifier to predict\nsome target class. %There are several techniques proposed in the literature\nthat aim to detect the attack but only a few also propose to defend against it,\nand they typically involve retraining the network which is not always possible\nin practice. We propose lightweight automated detection and correction\ntechniques against poisoning attacks, which are based on neuron patterns mined\nfrom the network using a small set of clean and poisoned test samples with\nknown labels. The patterns built based on the mis-classified samples are used\nfor run-time detection of new poisoned inputs. For correction, we propose an\ninput correction technique that uses a differential analysis to identify the\ntrigger in the detected poisoned images, which is then reset to a neutral\ncolor. Our detection and correction are performed at run-time and input level,\nwhich is in contrast to most existing work that is focused on offline\nmodel-level defenses. We demonstrate that our technique outperforms existing\ndefenses such as NeuralCleanse and STRIP on popular benchmarks such as MNIST,\nCIFAR-10, and GTSRB against the popular BadNets attack and the more complex\nDFST attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1\">Muhammad Usman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Youcheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopinath_D/0/1/0/all/0/1\">Divya Gopinath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasareanu_C/0/1/0/all/0/1\">Corina S. Pasareanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Make Some Noise: Reliable and Efficient Single-Step Adversarial Training. (arXiv:2202.01181v1 [cs.LG])","link":"http://arxiv.org/abs/2202.01181","description":"<p>Recently, Wong et al. showed that adversarial training with single-step FGSM\nleads to a characteristic failure mode named catastrophic overfitting (CO), in\nwhich a model becomes suddenly vulnerable to multi-step attacks. They showed\nthat adding a random perturbation prior to FGSM (RS-FGSM) seemed to be\nsufficient to prevent CO. However, Andriushchenko and Flammarion observed that\nRS-FGSM still leads to CO for larger perturbations, and proposed an expensive\nregularizer (GradAlign) to avoid CO. In this work, we methodically revisit the\nrole of noise and clipping in single-step adversarial training. Contrary to\nprevious intuitions, we find that using a stronger noise around the clean\nsample combined with not clipping is highly effective in avoiding CO for large\nperturbation radii. Based on these observations, we then propose Noise-FGSM\n(N-FGSM) that, while providing the benefits of single-step adversarial\ntraining, does not suffer from CO. Empirical analyses on a large suite of\nexperiments show that N-FGSM is able to match or surpass the performance of\nprevious single-step methods while achieving a 3$\\times$ speed-up.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jorge_P/0/1/0/all/0/1\">Pau de Jorge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1\">Adel Bibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volpi_R/0/1/0/all/0/1\">Riccardo Volpi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_A/0/1/0/all/0/1\">Amartya Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H. S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogez_G/0/1/0/all/0/1\">Gr&#xe9;gory Rogez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1\">Puneet K. Dokania</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VOS:Learning What You Don't Know by Virtual Outlier Synthesis. (arXiv:2202.01197v1 [cs.LG])","link":"http://arxiv.org/abs/2202.01197","description":"<p>Out-of-distribution (OOD) detection has received much attention lately due to\nits importance in the safe deployment of neural networks. One of the key\nchallenges is that models lack supervision signals from unknown data, and as a\nresult, can produce overconfident predictions on OOD data. Previous approaches\nrely on real outlier datasets for model regularization, which can be costly and\nsometimes infeasible to obtain in practice. In this paper, we present VOS, a\nnovel framework for OOD detection by adaptively synthesizing virtual outliers\nthat can meaningfully regularize the model's decision boundary during training.\nSpecifically, VOS samples virtual outliers from the low-likelihood region of\nthe class-conditional distribution estimated in the feature space. Alongside,\nwe introduce a novel unknown-aware training objective, which contrastively\nshapes the uncertainty space between the ID data and synthesized outlier data.\nVOS achieves state-of-the-art performance on both object detection and image\nclassification models, reducing the FPR95 by up to 7.87% compared to the\nprevious best method. Code is available at\nhttps://github.com/deeplearning-wisc/vos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xuefeng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Mu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Extraction of Open Space Area from High Resolution Urban Satellite Imagery. (arXiv:1103.4723v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1103.4723","description":"<p>In the 21st century, Aerial and satellite images are information rich. They\nare also complex to analyze. For GIS systems, many features require fast and\nreliable extraction of open space area from high resolution satellite imagery.\nIn this paper we will study efficient and reliable automatic extraction\nalgorithm to find out the open space area from the high resolution urban\nsatellite imagery. This automatic extraction algorithm uses some filters and\nsegmentations and grouping is applying on satellite images. And the result\nimages may use to calculate the total available open space area and the built\nup area. It may also use to compare the difference between present and past\nopen space area using historical urban satellite images of that same projection\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kodge_B/0/1/0/all/0/1\">B. G. Kodge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hiremath_P/0/1/0/all/0/1\">P. S. Hiremath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Patch-based Image Denoising Method Using Eigenvectors of the Geodesics' Gramian Matrix. (arXiv:2010.07769v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2010.07769","description":"<p>With the proliferation of sophisticated cameras in modern society, the demand\nfor accurate and visually pleasing images is increasing. However, the quality\nof an image captured by a camera may be degraded by noise. Thus, some\nprocessing of images is required to filter out the noise without losing vital\nimage features. Even though the current literature offers a variety of\ndenoising methods, the fidelity and efficacy of their denoising are sometimes\nuncertain. Thus, here we propose a novel and computationally efficient image\ndenoising method that is capable of producing accurate images. To preserve\nimage smoothness, this method inputs patches partitioned from the image rather\nthan pixels. Then, it performs denoising on the manifold underlying the\npatch-space rather than that in the image domain to better preserve the\nfeatures across the whole image. We validate the performance of this method\nagainst benchmark image processing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gajamannage_K/0/1/0/all/0/1\">Kelum Gajamannage</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paffenroth_R/0/1/0/all/0/1\">Randy Paffenroth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jayasumana_A/0/1/0/all/0/1\">Anura P. Jayasumana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simplicial Complex Representation Learning. (arXiv:2103.04046v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.04046","description":"<p>Simplicial complexes form an important class of topological spaces that are\nfrequently used in many application areas such as computer-aided design,\ncomputer graphics, and simulation. Representation learning on graphs, which are\njust 1-d simplicial complexes, has witnessed a great attention in recent years.\nHowever, there has not been enough effort to extend representation learning to\nhigher dimensional simplicial objects due to the additional complexity these\nobjects hold, especially when it comes to entire-simplicial complex\nrepresentation learning. In this work, we propose a method for simplicial\ncomplex-level representation learning that embeds a simplicial complex to a\nuniversal embedding space in a way that complex-to-complex proximity is\npreserved. Our method uses our novel geometric message passing schemes to learn\nan entire simplicial complex representation in an end-to-end fashion. We\ndemonstrate the proposed model on publicly available mesh dataset. To the best\nof our knowledge, this work presents the first method for learning simplicial\ncomplex-level representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1\">Mustafa Hajij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1\">Ghada Zamzmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papamarkou_T/0/1/0/all/0/1\">Theodore Papamarkou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maroulas_V/0/1/0/all/0/1\">Vasileios Maroulas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xuanting Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for fully automatic detection, segmentation, and Gleason Grade estimation of prostate cancer in multiparametric Magnetic Resonance Images. (arXiv:2103.12650v3 [physics.med-ph] UPDATED)","link":"http://arxiv.org/abs/2103.12650","description":"<p>The emergence of multi-parametric magnetic resonance imaging (mpMRI) has had\na profound impact on the diagnosis of prostate cancers (PCa), which is the most\nprevalent malignancy in males in the western world, enabling a better selection\nof patients for confirmation biopsy. However, analyzing these images is complex\neven for experts, hence opening an opportunity for computer-aided diagnosis\nsystems to seize. This paper proposes a fully automatic system based on Deep\nLearning that takes a prostate mpMRI from a PCa-suspect patient and, by\nleveraging the Retina U-Net detection framework, locates PCa lesions, segments\nthem, and predicts their most likely Gleason grade group (GGG). It uses 490\nmpMRIs for training/validation, and 75 patients for testing from two different\ndatasets: ProstateX and IVO (Valencia Oncology Institute Foundation). In the\ntest set, it achieves an excellent lesion-level AUC/sensitivity/specificity for\nthe GGG$\\geq$2 significance criterion of 0.96/1.00/0.79 for the ProstateX\ndataset, and 0.95/1.00/0.80 for the IVO dataset. Evaluated at a patient level,\nthe results are 0.87/1.00/0.375 in ProstateX, and 0.91/1.00/0.762 in IVO.\nFurthermore, on the online ProstateX grand challenge, the model obtained an AUC\nof 0.85 (0.87 when trained only on the ProstateX data, tying up with the\noriginal winner of the challenge). For expert comparison, IVO radiologist's\nPI-RADS 4 sensitivity/specificity were 0.88/0.56 at a lesion level, and\n0.85/0.58 at a patient level. Additional subsystems for automatic prostate\nzonal segmentation and mpMRI non-rigid sequence registration were also employed\nto produce the final fully automated system. The code for the ProstateX-trained\nsystem has been made openly available at\nhttps://github.com/OscarPellicer/prostate_lesion_detection. We hope that this\nwill represent a landmark for future research to use, compare and improve upon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Pellicer_Valero_O/0/1/0/all/0/1\">Oscar J. Pellicer-Valero</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jimenez_J/0/1/0/all/0/1\">Jos&#xe9; L. Marenco Jim&#xe9;nez</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gonzalez_Perez_V/0/1/0/all/0/1\">Victor Gonzalez-Perez</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ramon_Borja_J/0/1/0/all/0/1\">Juan Luis Casanova Ram&#xf3;n-Borja</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Garcia_I/0/1/0/all/0/1\">Isabel Mart&#xed;n Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Benito_M/0/1/0/all/0/1\">Mar&#xed;a Barrios Benito</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gomez_P/0/1/0/all/0/1\">Paula Pelechano G&#xf3;mez</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rubio_Briones_J/0/1/0/all/0/1\">Jos&#xe9; Rubio-Briones</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ruperez_M/0/1/0/all/0/1\">Mar&#xed;a Jos&#xe9; Rup&#xe9;rez</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Martin_Guerrero_J/0/1/0/all/0/1\">Jos&#xe9; D. Mart&#xed;n-Guerrero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead. (arXiv:2105.09121v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.09121","description":"<p>Deploying deep learning models in time-critical applications with limited\ncomputational resources, for instance in edge computing systems and IoT\nnetworks, is a challenging task that often relies on dynamic inference methods\nsuch as early exiting. In this paper, we introduce a novel architecture for\nearly exiting based on the vision transformer architecture, as well as a\nfine-tuning strategy that significantly increase the accuracy of early exit\nbranches compared to conventional approaches while introducing less overhead.\nThrough extensive experiments on image and audio classification as well as\naudiovisual crowd counting, we show that our method works for both\nclassification and regression problems, and in both single- and multi-modal\nsettings. Additionally, we introduce a novel method for integrating audio and\nvisual modalities within early exits in audiovisual data analysis, that can\nlead to a more fine-grained dynamic inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakhtiarnia_A/0/1/0/all/0/1\">Arian Bakhtiarnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task, Multi-Domain Deep Segmentation with Shared Representations and Contrastive Regularization for Sparse Pediatric Datasets. (arXiv:2105.10310v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.10310","description":"<p>Automatic segmentation of magnetic resonance (MR) images is crucial for\nmorphological evaluation of the pediatric musculoskeletal system in clinical\npractice. However, the accuracy and generalization performance of individual\nsegmentation models are limited due to the restricted amount of annotated\npediatric data. Hence, we propose to train a segmentation model on multiple\ndatasets, arising from different parts of the anatomy, in a multi-task and\nmulti-domain learning framework. This approach allows to overcome the inherent\nscarcity of pediatric data while benefiting from a more robust shared\nrepresentation. The proposed segmentation network comprises shared\nconvolutional filters, domain-specific batch normalization parameters that\ncompute the respective dataset statistics and a domain-specific segmentation\nlayer. Furthermore, a supervised contrastive regularization is integrated to\nfurther improve generalization capabilities, by promoting intra-domain\nsimilarity and impose inter-domain margins in embedded space. We evaluate our\ncontributions on two pediatric imaging datasets of the ankle and shoulder\njoints for bone segmentation. Results demonstrate that the proposed model\noutperforms state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boutillon_A/0/1/0/all/0/1\">Arnaud Boutillon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conze_P/0/1/0/all/0/1\">Pierre-Henri Conze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_C/0/1/0/all/0/1\">Christelle Pons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burdin_V/0/1/0/all/0/1\">Val&#xe9;rie Burdin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borotikar_B/0/1/0/all/0/1\">Bhushan Borotikar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields. (arXiv:2106.05187v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05187","description":"<p>We present implicit displacement fields, a novel representation for detailed\n3D geometry. Inspired by a classic surface deformation technique, displacement\nmapping, our method represents a complex surface as a smooth base surface plus\na displacement along the base's normal directions, resulting in a\nfrequency-based shape decomposition, where the high frequency signal is\nconstrained geometrically by the low frequency signal. Importantly, this\ndisentanglement is unsupervised thanks to a tailored architectural design that\nhas an innate frequency hierarchy by construction. We explore implicit\ndisplacement field surface reconstruction and detail transfer and demonstrate\nsuperior representational power, training stability and generalizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yifan_W/0/1/0/all/0/1\">Wang Yifan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmann_L/0/1/0/all/0/1\">Lukas Rahmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorkine_Hornung_O/0/1/0/all/0/1\">Olga Sorkine-Hornung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VidHarm: A Clip Based Dataset for Harmful Content Detection. (arXiv:2106.08323v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.08323","description":"<p>Automatically identifying harmful content in video is an important task with\na wide range of applications. However, there is a lack of professionally\nlabeled open datasets available. In this work VidHarm, an open dataset of 3589\nvideo clips from film trailers annotated by professionals, is presented. An\nanalysis of the dataset is performed, revealing among other things the relation\nbetween clip and trailer level annotations. Audiovisual models are trained on\nthe dataset and an in-depth study of modeling choices conducted. The results\nshow that performance is greatly improved by combining the visual and audio\nmodality, pre-training on large-scale video recognition datasets, and class\nbalanced sampling. Lastly, biases of the trained models are investigated using\ndiscrimination probing.\n</p>\n<p>VidHarm is openly available, and further details are available at this\nwebpage: \\url{https://vidharm.github.io/}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Edstedt_J/0/1/0/all/0/1\">Johan Edstedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_A/0/1/0/all/0/1\">Amanda Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_J/0/1/0/all/0/1\">Johan Karlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benavente_F/0/1/0/all/0/1\">Francisca Benavente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novak_A/0/1/0/all/0/1\">Anette Novak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pihlgren_G/0/1/0/all/0/1\">Gustav Grund Pihlgren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Correspondence Hallucination. (arXiv:2106.09711v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09711","description":"<p>Given a pair of partially overlapping source and target images and a keypoint\nin the source image, the keypoint's correspondent in the target image can be\neither visible, occluded or outside the field of view. Local feature matching\nmethods are only able to identify the correspondent's location when it is\nvisible, while humans can also hallucinate its location when it is occluded or\noutside the field of view through geometric reasoning. In this paper, we bridge\nthis gap by training a network to output a peaked probability distribution over\nthe correspondent's location, regardless of this correspondent being visible,\noccluded, or outside the field of view. We experimentally demonstrate that this\nnetwork is indeed able to hallucinate correspondences on pairs of images\ncaptured in scenes that were not seen at training-time. We also apply this\nnetwork to an absolute camera pose estimation problem and find it is\nsignificantly more robust than state-of-the-art local feature matching-based\ncompetitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Germain_H/0/1/0/all/0/1\">Hugo Germain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1\">Vincent Lepetit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourmaud_G/0/1/0/all/0/1\">Guillaume Bourmaud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Surface Reconstruction with Delaunay-Graph Neural Networks. (arXiv:2107.06130v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06130","description":"<p>We introduce a novel learning-based, visibility-aware, surface reconstruction\nmethod for large-scale, defect-laden point clouds. Our approach can cope with\nthe scale and variety of point cloud defects encountered in real-life\nMulti-View Stereo (MVS) acquisitions. Our method relies on a 3D Delaunay\ntetrahedralization whose cells are classified as inside or outside the surface\nby a graph neural network and an energy model solvable with a graph cut. Our\nmodel, making use of both local geometric attributes and line-of-sight\nvisibility information, is able to learn a visibility model from a small amount\nof synthetic training data and generalizes to real-life acquisitions. Combining\nthe efficiency of deep learning methods and the scalability of energy based\nmodels, our approach outperforms both learning and non learning-based\nreconstruction algorithms on two publicly available reconstruction benchmarks.\nOur code and data is available at https://github.com/raphaelsulzer/dgnn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sulzer_R/0/1/0/all/0/1\">Raphael Sulzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1\">Loic Landrieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marlet_R/0/1/0/all/0/1\">Renaud Marlet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallet_B/0/1/0/all/0/1\">Bruno Vallet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Video Captioning with Dynamic Loss Network. (arXiv:2107.11707v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11707","description":"<p>Video captioning is one of the challenging problems at the intersection of\nvision and language, having many real-life applications in video retrieval,\nvideo surveillance, assisting visually challenged people, Human-machine\ninterface, and many more. Recent deep learning based methods have shown\npromising results but are still on the lower side than other vision tasks (such\nas image classification, object detection). A significant drawback with\nexisting video captioning methods is that they are optimized over cross-entropy\nloss function, which is uncorrelated to the de facto evaluation metrics (BLEU,\nMETEOR, CIDER, ROUGE). In other words, cross-entropy is not a proper surrogate\nof the true loss function for video captioning. To mitigate this, methods like\nREINFORCE, Actor-Critic, and Minimum Risk Training (MRT) have been applied but\nhave limitations and are not very effective. This paper proposes an alternate\nsolution by introducing a dynamic loss network (DLN), providing an additional\nfeedback signal that reflects the evaluation metrics directly. Our solution\nproves to be more efficient than other solutions and can be easily adapted to\nsimilar tasks. Our results on Microsoft Research Video Description Corpus\n(MSVD) and MSR-Video to Text (MSRVTT) datasets outperform previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ullah_N/0/1/0/all/0/1\">Nasib Ullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanta_P/0/1/0/all/0/1\">Partha Pratim Mohanta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpreting Generative Adversarial Networks for Interactive Image Generation. (arXiv:2108.04896v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04896","description":"<p>Significant progress has been made by the advances in Generative Adversarial\nNetworks (GANs) for image generation. However, there lacks enough understanding\nof how a realistic image is generated by the deep representations of GANs from\na random vector. This chapter gives a summary of recent works on interpreting\ndeep generative models. The methods are categorized into the supervised, the\nunsupervised, and the embedding-guided approaches. We will see how the\nhuman-understandable concepts that emerge in the learned representation can be\nidentified and used for interactive image generation and editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Better Loss for Visual-Textual Grounding. (arXiv:2108.05308v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05308","description":"<p>Given a textual phrase and an image, the visual grounding problem is the task\nof locating the content of the image referenced by the sentence. It is a\nchallenging task that has several real-world applications in human-computer\ninteraction, image-text reference resolution, and video-text reference\nresolution. In the last years, several works have addressed this problem by\nproposing more and more large and complex models that try to capture\nvisual-textual dependencies better than before. These models are typically\nconstituted by two main components that focus on how to learn useful\nmulti-modal features for grounding and how to improve the predicted bounding\nbox of the visual mention, respectively. Finding the right learning balance\nbetween these two sub-tasks is not easy, and the current models are not\nnecessarily optimal with respect to this issue. In this work, we propose a loss\nfunction based on bounding boxes classes probabilities that: (i) improves the\nbounding boxes selection; (ii) improves the bounding boxes coordinates\nprediction. Our model, although using a simple multi-modal feature fusion\ncomponent, is able to achieve a higher accuracy than state-of-the-art models on\ntwo widely adopted datasets, reaching a better learning balance between the two\nsub-tasks mentioned above.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rigoni_D/0/1/0/all/0/1\">Davide Rigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serafini_L/0/1/0/all/0/1\">Luciano Serafini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sperduti_A/0/1/0/all/0/1\">Alessandro Sperduti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple and Efficient Reconstruction Backbone for Snapshot Compressive Imaging. (arXiv:2108.07739v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.07739","description":"<p>The emerging technology of snapshot compressive imaging (SCI) enables\ncapturing high dimensional (HD) data in an efficient way. It is generally\nimplemented by two components: an optical encoder that compresses HD signals\ninto a 2D measurement and an algorithm decoder that retrieves the HD data upon\nthe hardware-encoded measurement. Over a broad range of SCI applications,\nhyperspectral imaging (HSI) and video compressive sensing have received\nsignificant research attention in recent years. Among existing SCI\nreconstruction algorithms, deep learning-based methods stand out as their\npromising performance and efficient inference. However, the deep reconstruction\nnetwork may suffer from overlarge model size and highly-specialized network\ndesign, which inevitably lead to costly training time, high memory usage, and\nlimited flexibility, thus discouraging the deployments of SCI systems in\npractical scenarios. In this paper, we tackle the above challenges by proposing\na simple yet highly efficient reconstruction method, namely stacked residual\nnetwork (SRN), by revisiting the residual learning strategy with nested\nstructures and spatial-invariant property. The proposed SRN empowers\nhigh-fidelity data retrieval with fewer computation operations and negligible\nmodel size compared with existing networks, and also serves as a versatile\nbackbone applicable for both hyperspectral and video data. Based on the\nproposed backbone, we first develop the channel attention enhanced SRN\n(CAE-SRN) to explore the spectral inter-dependencies for fine-grained spatial\nestimation in HSI. We then employ SRN as a deep denoiser and incorporate it\ninto a generalized alternating projection (GAP) framework -- resulting in\nGAP-SRN -- to handle the video compressive sensing task. Experimental results\ndemonstrate the state-of-the-art performance, high computational efficiency of\nthe proposed SRN on two SCI applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiamian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_Z/0/1/0/all/0/1\">Zhiqiang Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DASHA: Decentralized Autofocusing System with Hierarchical Agents. (arXiv:2108.12842v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12842","description":"<p>State-of-the-art object detection models are frequently trained offline using\navailable datasets, such as ImageNet: large and overly diverse data that are\nunbalanced and hard to cluster semantically. This kind of training drops the\nobject detection performance should the change in illumination, in the\nenvironmental conditions (e.g., rain), or in the lens positioning (out-of-focus\nblur) occur. We propose a decentralized hierarchical multi-agent deep\nreinforcement learning approach for intelligently controlling the camera and\nthe lens focusing settings, leading to a significant improvement beyond the\ncapacity of the popular detection models (YOLO, Faster R-CNN, and Retina are\nconsidered). The algorithm relies on the latent representation of the camera's\nstream and, thus, it is the first method to allow a completely no-reference\ntuning of the camera, where the system trains itself to auto-focus itself.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anikina_A/0/1/0/all/0/1\">Anna Anikina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogov_O/0/1/0/all/0/1\">Oleg Y. Rogov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dylov_D/0/1/0/all/0/1\">Dmitry V. Dylov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SmartDepthSync: Open Source Synchronized Video Recording System of Smartphone RGB and Depth Camera Image Frames with Sub-millisecond Precision. (arXiv:2111.03552v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.03552","description":"<p>Nowadays, smartphones can produce a synchronized (synced) stream of\nhigh-quality data, including RGB images, inertial measurements, and other data.\nTherefore, smartphones are becoming appealing sensor systems in the robotics\ncommunity. Unfortunately, there is still the need for external supporting\nsensing hardware, such as a depth camera precisely synced with the smartphone\nsensors.\n</p>\n<p>In this paper, we propose a hardware-software recording system that presents\na heterogeneous structure and contains a smartphone and an external depth\ncamera for recording visual, depth, and inertial data that are mutually\nsynchronized. The system is synced at the time and the frame levels: every RGB\nimage frame from the smartphone camera is exposed at the same moment of time\nwith a depth camera frame with sub-millisecond precision. We provide a method\nand a tool for sync performance evaluation that can be applied to any pair of\ndepth and RGB cameras. Our system could be replicated, modified, or extended by\nemploying our open-sourced materials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faizullin_M/0/1/0/all/0/1\">Marsel Faizullin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornilova_A/0/1/0/all/0/1\">Anastasiia Kornilova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhmetyanov_A/0/1/0/all/0/1\">Azat Akhmetyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pakulev_K/0/1/0/all/0/1\">Konstantin Pakulev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadkov_A/0/1/0/all/0/1\">Andrey Sadkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_G/0/1/0/all/0/1\">Gonzalo Ferrer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Exponentially Tilted Gaussian Prior for Variational Autoencoders. (arXiv:2111.15646v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.15646","description":"<p>An important property for deep neural networks is the ability to perform\nrobust out-of-distribution detection on previously unseen data. This property\nis essential for safety purposes when deploying models for real world\napplications. Recent studies show that probabilistic generative models can\nperform poorly on this task, which is surprising given that they seek to\nestimate the likelihood of training data. To alleviate this issue, we propose\nthe exponentially tilted Gaussian prior distribution for the Variational\nAutoencoder (VAE) which pulls points onto the surface of a hyper-sphere in\nlatent space. This achieves state-of-the art results on the area under the\ncurve-receiver operator characteristics metric using just the negative\nlog-likelihood that the VAE naturally assigns. Because this prior is a simple\nmodification of the traditional VAE prior, it is faster and easier to implement\nthan competitive methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Floto_G/0/1/0/all/0/1\">Griffin Floto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kremer_S/0/1/0/all/0/1\">Stefan Kremer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nica_M/0/1/0/all/0/1\">Mihai Nica</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A formal approach to good practices in Pseudo-Labeling for Unsupervised Domain Adaptive Re-Identification. (arXiv:2112.12887v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12887","description":"<p>The use of pseudo-labels prevails in order to tackle Unsupervised Domain\nAdaptive (UDA) Re-Identification (re-ID) with the best performance. Indeed,\nthis family of approaches has given rise to several UDA re-ID specific\nframeworks, which are effective. In these works, research directions to improve\nPseudo-Labeling UDA re-ID performance are varied and mostly based on intuition\nand experiments: refining pseudo-labels, reducing the impact of errors in\npseudo-labels... It can be hard to deduce from them general good practices,\nwhich can be implemented in any Pseudo-Labeling method, to consistently improve\nits performance. To address this key question, a new theoretical view on\nPseudo-Labeling UDA re-ID is proposed. The contributions are threefold: (i) A\nnovel theoretical framework for Pseudo-Labeling UDA re-ID, formalized through a\nnew general learning upper-bound on the UDA re-ID performance. (ii) General\ngood practices for Pseudo-Labeling, directly deduced from the interpretation of\nthe proposed theoretical framework, in order to improve the target re-ID\nperformance. (iii) Extensive experiments on challenging person and vehicle\ncross-dataset re-ID tasks, showing consistent performance improvements for\nvarious state-of-the-art methods and various proposed implementations of good\npractices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dubourvieux_F/0/1/0/all/0/1\">Fabian Dubourvieux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audigier_R/0/1/0/all/0/1\">Romaric Audigier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loesch_A/0/1/0/all/0/1\">Ang&#xe9;lique Loesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainouz_S/0/1/0/all/0/1\">Samia Ainouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canu_S/0/1/0/all/0/1\">St&#xe9;phane Canu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled Latent Transformer for Interpretable Monocular Height Estimation. (arXiv:2201.06357v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06357","description":"<p>Monocular height estimation (MHE) from remote sensing imagery has high\npotential in generating 3D city models efficiently for a quick response to\nnatural disasters. Most existing works pursue higher performance. However,\nthere is little research exploring the interpretability of MHE networks. In\nthis paper, we target at exploring how deep neural networks predict height from\na single monocular image. Towards a comprehensive understanding of MHE\nnetworks, we propose to interpret them from multiple levels: 1) Neurons:\nunit-level dissection. Exploring the semantic and height selectivity of the\nlearned internal deep representations; 2) Instances: object-level\ninterpretation. Studying the effects of different semantic classes, scales, and\nspatial contexts on height estimation; 3) Attribution: pixel-level analysis.\nUnderstanding which input pixels are important for the height estimation. Based\non the multi-level interpretation, a disentangled latent Transformer network is\nproposed towards a more compact, reliable, and explainable deep model for\nmonocular height estimation. Furthermore, a novel unsupervised semantic\nsegmentation task based on height estimation is first introduced in this work.\nAdditionally, we also construct a new dataset for joint semantic segmentation\nand height estimation. Our work provides novel insights for both understanding\nand designing MHE models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhitong Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sining Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yilei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-aware deep learning methods for robust diabetic retinopathy classification. (arXiv:2201.09042v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09042","description":"<p>Automatic classification of diabetic retinopathy from retinal images has been\nwidely studied using deep neural networks with impressive results. However,\nthere is a clinical need for estimation of the uncertainty in the\nclassifications, a shortcoming of modern neural networks. Recently, approximate\nBayesian deep learning methods have been proposed for the task but the studies\nhave only considered the binary referable/non-referable diabetic retinopathy\nclassification applied to benchmark datasets. We present novel results by\nsystematically investigating a clinical dataset and a clinically relevant\n5-class classification scheme, in addition to benchmark datasets and the binary\nclassification scheme. Moreover, we derive a connection between uncertainty\nmeasures and classifier risk, from which we develop a new uncertainty measure.\nWe observe that the previously proposed entropy-based uncertainty measure\ngeneralizes to the clinical dataset on the binary classification scheme but not\non the 5-class scheme, whereas our new uncertainty measure generalizes to the\nlatter case.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaskari_J/0/1/0/all/0/1\">Joel Jaskari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahlsten_J/0/1/0/all/0/1\">Jaakko Sahlsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damoulas_T/0/1/0/all/0/1\">Theodoros Damoulas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoblauch_J/0/1/0/all/0/1\">Jeremias Knoblauch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkka_S/0/1/0/all/0/1\">Simo S&#xe4;rkk&#xe4;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karkkainen_L/0/1/0/all/0/1\">Leo K&#xe4;rkk&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hietala_K/0/1/0/all/0/1\">Kustaa Hietala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaski_K/0/1/0/all/0/1\">Kimmo Kaski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plug & Play Attacks: Towards Robust and Flexible Model Inversion Attacks. (arXiv:2201.12179v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12179","description":"<p>Model inversion attacks (MIAs) aim to create synthetic images that reflect\nthe class-wise characteristics from a target classifier's training data by\nexploiting the model's learned knowledge. Previous research has developed\ngenerative MIAs using generative adversarial networks (GANs) as image priors\nthat are tailored to a specific target model. This makes the attacks time- and\nresource-consuming, inflexible, and susceptible to distributional shifts\nbetween datasets. To overcome these drawbacks, we present Plug &amp; Play Attacks\nthat loosen the dependency between the target model and image prior and enable\nthe use of a single trained GAN to attack a broad range of targets with only\nminor attack adjustments needed. Moreover, we show that powerful MIAs are\npossible even with publicly available pre-trained GANs and under strong\ndistributional shifts, whereas previous approaches fail to produce meaningful\nresults. Our extensive evaluation confirms the improved robustness and\nflexibility of Plug &amp; Play Attacks and their ability to create high-quality\nimages revealing sensitive class characteristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1\">Lukas Struppek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1\">Dominik Hintersdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correia_A/0/1/0/all/0/1\">Antonio De Almeida Correia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adler_A/0/1/0/all/0/1\">Antonia Adler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-paced learning to improve text row detection in historical documents with missing labels. (arXiv:2201.12216v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12216","description":"<p>An important preliminary step of optical character recognition systems is the\ndetection of text rows. To address this task in the context of historical data\nwith missing labels, we propose a self-paced learning algorithm capable of\nimproving the row detection performance. We conjecture that pages with more\nground-truth bounding boxes are less likely to have missing annotations. Based\non this hypothesis, we sort the training examples in descending order with\nrespect to the number of ground-truth bounding boxes, and organize them into k\nbatches. Using our self-paced learning method, we train a row detector over k\niterations, progressively adding batches with less ground-truth annotations. At\neach iteration, we combine the ground-truth bounding boxes with pseudo-bounding\nboxes (bounding boxes predicted by the model itself) using non-maximum\nsuppression, and we include the resulting annotations at the next training\niteration. We demonstrate that our self-paced learning strategy brings\nsignificant performance gains on two data sets of historical documents,\nimproving the average precision of YOLOv4 with more than 12% on one data set\nand 39% on the other.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaman_M/0/1/0/all/0/1\">Mihaela Gaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadamiyan_L/0/1/0/all/0/1\">Lida Ghadamiyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1\">Marius Popescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The impact of removing head movements on audio-visual speech enhancement. (arXiv:2202.00538v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2202.00538","description":"<p>This paper investigates the impact of head movements on audio-visual speech\nenhancement (AVSE). Although being a common conversational feature, head\nmovements have been ignored by past and recent studies: they challenge today's\nlearning-based methods as they often degrade the performance of models that are\ntrained on clean, frontal, and steady face images. To alleviate this problem,\nwe propose to use robust face frontalization (RFF) in combination with an AVSE\nmethod based on a variational auto-encoder (VAE) model. We briefly describe the\nbasic ingredients of the proposed pipeline and we perform experiments with a\nrecently released audio-visual dataset. In the light of these experiments, and\nbased on three standard metrics, namely STOI, PESQ and SI-SDR, we conclude that\nRFF improves the performance of AVSE by a considerable margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhiqi Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghi_M/0/1/0/all/0/1\">Mostafa Sadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horaud_R/0/1/0/all/0/1\">Radu Horaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donley_J/0/1/0/all/0/1\">Jacob Donley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Anurag Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPAGHETTI: Editing Implicit Shapes Through Part Aware Generation. (arXiv:2201.13168v1 [cs.GR] CROSS LISTED)","link":"http://arxiv.org/abs/2201.13168","description":"<p>Neural implicit fields are quickly emerging as an attractive representation\nfor learning based techniques. However, adopting them for 3D shape modeling and\nediting is challenging. We introduce a method for $\\mathbf{E}$diting\n$\\mathbf{I}$mplicit $\\mathbf{S}$hapes $\\mathbf{T}$hrough $\\mathbf{P}$art\n$\\mathbf{A}$ware $\\mathbf{G}$enera$\\mathbf{T}$ion, permuted in short as\nSPAGHETTI. Our architecture allows for manipulation of implicit shapes by means\nof transforming, interpolating and combining shape segments together, without\nrequiring explicit part supervision. SPAGHETTI disentangles shape part\nrepresentation into extrinsic and intrinsic geometric information. This\ncharacteristic enables a generative framework with part-level control. The\nmodeling capabilities of SPAGHETTI are demonstrated using an interactive\ngraphical interface, where users can directly edit neural implicit shapes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hertz_A/0/1/0/all/0/1\">Amir Hertz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perel_O/0/1/0/all/0/1\">Or Perel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorkine_Hornung_O/0/1/0/all/0/1\">Olga Sorkine-Hornung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}