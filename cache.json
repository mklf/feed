{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-27T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Do Trajectories Encode Verb Meaning?. (arXiv:2206.11953v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11953","description":"<p>Distributional models learn representations of words from text, but are\ncriticized for their lack of grounding, or the linking of text to the\nnon-linguistic world. Grounded language models have had success in learning to\nconnect concrete categories like nouns and adjectives to the world via images\nand videos, but can struggle to isolate the meaning of the verbs themselves\nfrom the context in which they typically occur. In this paper, we investigate\nthe extent to which trajectories (i.e. the position and rotation of objects\nover time) naturally encode verb semantics. We build a procedurally generated\nagent-object-interaction dataset, obtain human annotations for the verbs that\noccur in this data, and compare several methods for representation learning\ngiven the trajectories. We find that trajectories correlate as-is with some\nverbs (e.g., fall), and that additional abstraction via self-supervised\npretraining can further capture nuanced differences in verb meaning (e.g., roll\nvs. slide).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ebert_D/0/1/0/all/0/1\">Dylan Ebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Disability Lens towards Biases in GPT-3 Generated Open-Ended Languages. (arXiv:2206.11993v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11993","description":"<p>Language models (LM) are becoming prevalent in many language-based\napplication spaces globally. Although these LMs are improving our day-to-day\ninteractions with digital products, concerns remain whether open-ended\nlanguages or text generated from these models reveal any biases toward a\nspecific group of people, thereby risking the usability of a certain product.\nThere is a need to identify whether these models possess bias to improve the\nfairness in these models. This gap motivates our ongoing work, where we\nmeasured the two aspects of bias in GPT-3 generated text through a disability\nlens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amin_A/0/1/0/all/0/1\">Akhter Al Amin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_K/0/1/0/all/0/1\">Kazi Sinthia Kabir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A multi-model-based deep learning framework for short text multiclass classification with the imbalanced and extremely small data set. (arXiv:2206.12027v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12027","description":"<p>Text classification plays an important role in many practical applications.\nIn the real world, there are extremely small datasets. Most existing methods\nadopt pre-trained neural network models to handle this kind of dataset.\nHowever, these methods are either difficult to deploy on mobile devices because\nof their large output size or cannot fully extract the deep semantic\ninformation between phrases and clauses. This paper proposes a multimodel-based\ndeep learning framework for short-text multiclass classification with an\nimbalanced and extremely small data set. Our framework mainly includes five\nlayers: The encoder layer uses DISTILBERT to obtain context-sensitive dynamic\nword vectors that are difficult to represent in traditional feature engineering\nmethods. Since the transformer part of this layer is distilled, our framework\nis compressed. Then, we use the next two layers to extract deep semantic\ninformation. The output of the encoder layer is sent to a bidirectional LSTM\nnetwork, and the feature matrix is extracted hierarchically through the LSTM at\nthe word and sentence level to obtain the fine-grained semantic representation.\nAfter that, the max-pooling layer converts the feature matrix into a\nlower-dimensional matrix, preserving only the obvious features. Finally, the\nfeature matrix is taken as the input of a fully connected softmax layer, which\ncontains a function that can convert the predicted linear vector into the\noutput value as the probability of the text in each classification. Extensive\nexperiments on two public benchmarks demonstrate the effectiveness of our\nproposed approach on an extremely small data set. It retains the\nstate-of-the-art baseline performance in terms of precision, recall, accuracy,\nand F1 score, and through the model size, training time, and convergence epoch,\nwe can conclude that our method can be deployed faster and lighter on mobile\ndevices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tong_J/0/1/0/all/0/1\">Jiajun Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhixiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rui_X/0/1/0/all/0/1\">Xiaobin Rui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogID: A Dialogic Instruction Dataset for Improving Teaching Effectiveness in Online Environments. (arXiv:2206.12034v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12034","description":"<p>Online dialogic instructions are a set of pedagogical instructions used in\nreal-world online educational contexts to motivate students, help understand\nlearning materials, and build effective study habits. In spite of the\npopularity and advantages of online learning, the education technology and\neducational data mining communities still suffer from the lack of large-scale,\nhigh-quality, and well-annotated teaching instruction datasets to study\ncomputational approaches to automatically detect online dialogic instructions\nand further improve the online teaching effectiveness. Therefore, in this\npaper, we present a dataset of online dialogic instruction detection,\n\\textsc{DialogID}, which contains 30,431 effective dialogic instructions. These\nteaching instructions are well annotated into 8 categories. Furthermore, we\nutilize the prevalent pre-trained language models (PLMs) and propose a simple\nyet effective adversarial training learning paradigm to improve the quality and\ngeneralization of dialogic instruction detection. Extensive experiments\ndemonstrate that our approach outperforms a wide range of baseline methods. The\ndata and our code are available for research purposes from:\n\\url{https://github.com/ai4ed/DialogID}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shuyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weiqi Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SC-Ques: A Sentence Completion Question Dataset for English as a Second Language Learners. (arXiv:2206.12036v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12036","description":"<p>Sentence completion (SC) questions present a sentence with one or more blanks\nthat need to be filled in, three to five possible words or phrases as options.\nSC questions are widely used for students learning English as a Second Language\n(ESL). In this paper, we present a large-scale SC dataset, \\textsc{SC-Ques},\nwhich is made up of 292,517 ESL SC questions from real-world standardized\nEnglish examinations. Furthermore, we build a comprehensive benchmark of\nautomatically solving the SC questions by training the large-scale pre-trained\nlanguage models on the proposed \\textsc{SC-Ques} dataset. We conduct detailed\nanalysis of the baseline models performance, limitations and trade-offs. The\ndata and our code are available for research purposes from:\n\\url{https://github.com/ai4ed/SC-Ques}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiongqiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shuyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weiqi Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Text-to-Speech Based on Latent Representation of Speaking Styles Using Spontaneous Dialogue. (arXiv:2206.12040v1 [eess.AS])","link":"http://arxiv.org/abs/2206.12040","description":"<p>The recent text-to-speech (TTS) has achieved quality comparable to that of\nhumans; however, its application in spoken dialogue has not been widely\nstudied. This study aims to realize a TTS that closely resembles human\ndialogue. First, we record and transcribe actual spontaneous dialogues. Then,\nthe proposed dialogue TTS is trained in two stages: first stage, variational\nautoencoder (VAE)-VITS or Gaussian mixture variational autoencoder (GMVAE)-VITS\nis trained, which introduces an utterance-level latent variable into\nvariational inference with adversarial learning for end-to-end text-to-speech\n(VITS), a recently proposed end-to-end TTS model. A style encoder that extracts\na latent speaking style representation from speech is trained jointly with TTS.\nIn the second stage, a style predictor is trained to predict the speaking style\nto be synthesized from dialogue history. During inference, by passing the\nspeaking style representation predicted by the style predictor to\nVAE/GMVAE-VITS, speech can be synthesized in a style appropriate to the context\nof the dialogue. Subjective evaluation results demonstrate that the proposed\nmethod outperforms the original VITS in terms of dialogue-level naturalness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mitsui_K/0/1/0/all/0/1\">Kentaro Mitsui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_T/0/1/0/all/0/1\">Tianyu Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sawada_K/0/1/0/all/0/1\">Kei Sawada</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hono_Y/0/1/0/all/0/1\">Yukiya Hono</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nankaku_Y/0/1/0/all/0/1\">Yoshihiko Nankaku</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tokuda_K/0/1/0/all/0/1\">Keiichi Tokuda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classifying Unstructured Clinical Notes via Automatic Weak Supervision. (arXiv:2206.12088v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12088","description":"<p>Healthcare providers usually record detailed notes of the clinical care\ndelivered to each patient for clinical, research, and billing purposes. Due to\nthe unstructured nature of these narratives, providers employ dedicated staff\nto assign diagnostic codes to patients' diagnoses using the International\nClassification of Diseases (ICD) coding system. This manual process is not only\ntime-consuming but also costly and error-prone. Prior work demonstrated\npotential utility of Machine Learning (ML) methodology in automating this\nprocess, but it has relied on large quantities of manually labeled data to\ntrain the models. Additionally, diagnostic coding systems evolve with time,\nwhich makes traditional supervised learning strategies unable to generalize\nbeyond local applications. In this work, we introduce a general\nweakly-supervised text classification framework that learns from class-label\ndescriptions only, without the need to use any human-labeled documents. It\nleverages the linguistic domain knowledge stored within pre-trained language\nmodels and the data programming framework to assign code labels to individual\ntexts. We demonstrate the efficacy and flexibility of our method by comparing\nit to state-of-the-art weak text classifiers across four real-world text\nclassification datasets, in addition to assigning ICD codes to medical notes in\nthe publicly available MIMIC-III database.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chufan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_M/0/1/0/all/0/1\">Mononito Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jieshi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubrawski_A/0/1/0/all/0/1\">Artur Dubrawski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified BERT for Few-shot Natural Language Understanding. (arXiv:2206.12094v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12094","description":"<p>Even as pre-trained language models share a semantic encoder, natural\nlanguage understanding suffers from a diversity of output schemas. In this\npaper, we propose UBERT, a unified bidirectional language understanding model\nbased on BERT framework, which can universally model the training objects of\ndifferent NLU tasks through a biaffine network. Specifically, UBERT encodes\nprior knowledge from various aspects, uniformly constructing learning\nrepresentations across multiple NLU tasks, which is conducive to enhancing the\nability to capture common semantic understanding. Using the biaffine to model\nscores pair of the start and end position of the original text, various\nclassification and extraction structures can be converted into a universal,\nspan-decoding approach. Experiments show that UBERT achieves the\nstate-of-the-art performance on 7 NLU tasks, 14 datasets on few-shot and\nzero-shot setting, and realizes the unification of extensive information\nextraction and linguistic reasoning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">JunYu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Ping Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">JiaXing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_R/0/1/0/all/0/1\">RuYi Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jing Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do You Know My Emotion? Emotion-Aware Strategy Recognition towards a Persuasive Dialogue System. (arXiv:2206.12101v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12101","description":"<p>Persuasive strategy recognition task requires the system to recognize the\nadopted strategy of the persuader according to the conversation. However,\nprevious methods mainly focus on the contextual information, little is known\nabout incorporating the psychological feedback, i.e. emotion of the persuadee,\nto predict the strategy. In this paper, we propose a Cross-channel Feedback\nmemOry Network (CFO-Net) to leverage the emotional feedback to iteratively\nmeasure the potential benefits of strategies and incorporate them into the\ncontextual-aware dialogue information. Specifically, CFO-Net designs a feedback\nmemory module, including strategy pool and feedback pool, to obtain\nemotion-aware strategy representation. The strategy pool aims to store\nhistorical strategies and the feedback pool is to obtain updated strategy\nweight based on feedback emotional information. Furthermore, a cross-channel\nfusion predictor is developed to make a mutual interaction between the\nemotion-aware strategy representation and the contextual-aware dialogue\ninformation for strategy recognition. Experimental results on\n\\textsc{PersuasionForGood} confirm that the proposed model CFO-Net is effective\nto improve the performance on M-F1 from 61.74 to 65.41.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Luxi Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yajing Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MVP: Multi-task Supervised Pre-training for Natural Language Generation. (arXiv:2206.12131v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12131","description":"<p>Pre-trained language models (PLMs) have achieved notable success in natural\nlanguage generation (NLG) tasks. Up to now, most of the PLMs are pre-trained in\nan unsupervised manner using large-scale general corpus. In the meanwhile, an\nincreasing number of models pre-trained with less labeled data showcase\nsuperior performance compared to unsupervised models. Motivated by the success\nof supervised pre-training, we propose Multi-task superVised Pre-training (MVP)\nfor natural language generation. For pre-training the text generation model\nMVP, we collect a labeled pre-training corpus from 45 datasets over seven\ngeneration tasks. For each task, we further pre-train specific soft prompts to\nstimulate the model capacity in performing a specific task. Extensive\nexperiments have demonstrated the effectiveness of our supervised pre-training\nin a number of NLG tasks, and our general methods achieve state-of-the-art\nperformance on 12 of 17 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capture Salient Historical Information: A Fast and Accurate Non-Autoregressive Model for Multi-turn Spoken Language Understanding. (arXiv:2206.12209v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12209","description":"<p>Spoken Language Understanding (SLU), a core component of the task-oriented\ndialogue system, expects a shorter inference facing the impatience of human\nusers. Existing work increases inference speed by designing non-autoregressive\nmodels for single-turn SLU tasks but fails to apply to multi-turn SLU in\nconfronting the dialogue history. The intuitive idea is to concatenate all\nhistorical utterances and utilize the non-autoregressive models directly.\nHowever, this approach seriously misses the salient historical information and\nsuffers from the uncoordinated-slot problems. To overcome those shortcomings,\nwe propose a novel model for multi-turn SLU named Salient History Attention\nwith Layer-Refined Transformer (SHA-LRT), which composes of an SHA module, a\nLayer-Refined Mechanism (LRM), and a Slot Label Generation (SLG) task. SHA\ncaptures salient historical information for the current dialogue from both\nhistorical utterances and results via a well-designed history-attention\nmechanism. LRM predicts preliminary SLU results from Transformer's middle\nstates and utilizes them to guide the final prediction, and SLG obtains the\nsequential dependency information for the non-autoregressive encoder.\nExperiments on public datasets indicate that our model significantly improves\nmulti-turn SLU performance (17.5% on Overall) with accelerating (nearly 15\ntimes) the inference process over the state-of-the-art baseline as well as\neffective on the single-turn SLU tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lizhi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+jia_W/0/1/0/all/0/1\">Weijia jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenmian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prosody Cloning in Zero-Shot Multispeaker Text-to-Speech. (arXiv:2206.12229v1 [cs.SD])","link":"http://arxiv.org/abs/2206.12229","description":"<p>The cloning of a speaker's voice using an untranscribed reference sample is\none of the great advances of modern neural text-to-speech (TTS) methods.\nApproaches for mimicking the prosody of a transcribed reference audio have also\nbeen proposed recently. In this work, we bring these two tasks together for the\nfirst time through utterance level normalization in conjunction with an\nutterance level speaker embedding. We further introduce a lightweight aligner\nfor extracting fine-grained prosodic features, that can be finetuned on\nindividual samples within seconds. We show that it is possible to clone the\nvoice of a speaker as well as the prosody of a spoken reference independently\nwithout any degradation in quality and high similarity to both original voice\nand prosody, as our objective evaluation and human study show. All of our code\nand trained models are available, alongside static and interactive demos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lux_F/0/1/0/all/0/1\">Florian Lux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_J/0/1/0/all/0/1\">Julia Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Noise-Resistant Mean Teaching for Weakly Supervised Fake News Detection. (arXiv:2206.12260v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12260","description":"<p>Fake news spreads at an unprecedented speed, reaches global audiences and\nposes huge risks to users and communities. Most existing fake news detection\nalgorithms focus on building supervised training models on a large amount of\nmanually labeled data, which is expensive to acquire or often unavailable. In\nthis work, we propose a novel label noise-resistant mean teaching approach\n(LNMT) for weakly supervised fake news detection. LNMT leverages unlabeled news\nand feedback comments of users to enlarge the amount of training data and\nfacilitates model training by generating refined labels as weak supervision.\nSpecifically, LNMT automatically assigns initial weak labels to unlabeled\nsamples based on semantic correlation and emotional association between news\ncontent and the comments. Moreover, in order to suppress the noises in weak\nlabels, LNMT establishes a mean teacher framework equipped with label\npropagation and label reliability estimation. The framework measures a weak\nlabel similarity matrix between the teacher and student networks, and\npropagates different valuable weak label information to refine the weak labels.\nMeanwhile, it exploits the consistency between the output class likelihood\nvectors of the two networks to evaluate the reliability of the weak labels and\nincorporates the reliability into model optimization to alleviate the negative\neffect of noisy weak labels. Extensive experiments show the superior\nperformance of LNMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jingyi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Sentence Simplification via Dependency Parsing. (arXiv:2206.12261v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12261","description":"<p>Text simplification is the task of rewriting a text so that it is readable\nand easily understood. In this paper, we propose a simple yet novel\nunsupervised sentence simplification system that harnesses parsing structures\ntogether with sentence embeddings to produce linguistically effective\nsimplifications. This means our model is capable of introducing substantial\nmodifications to simplify a sentence while maintaining its original semantics\nand adequate fluency. We establish the unsupervised state-of-the-art at 39.13\nSARI on TurkCorpus set and perform competitively against supervised baselines\non various quality metrics. Furthermore, we demonstrate our framework's\nextensibility to other languages via a proof-of-concept on Vietnamese data.\nCode for reproduction is published at \\url{https://github.com/isVy08/USDP}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1\">Vy Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1\">Wray Buntine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emoji-based Fine-grained Attention Network for Sentiment Analysis in the Microblog Comments. (arXiv:2206.12262v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12262","description":"<p>Microblogs have become a social platform for people to express their emotions\nin real-time, and it is a trend to analyze user emotional tendencies from the\ninformation on Microblogs. The dynamic features of emojis can affect the\nsentiment polarity of microblog texts. Since existing models seldom consider\nthe diversity of emoji sentiment polarity,the paper propose a microblog\nsentiment classification model based on ALBERT-FAET. We obtain text embedding\nvia ALBERT pretraining model and learn the inter-emoji embedding with an\nattention-based LSTM network. In addition, a fine-grained attention mechanism\nis proposed to capture the word-level interactions between plain text and\nemoji. Finally, we concatenate these features and feed them into a CNN\nclassifier to predict the sentiment labels of the microblogs. To verify the\neffectiveness of the model and the fine-grained attention network, we conduct\ncomparison experiments and ablation experiments. The comparison experiments\nshow that the model outperforms previous methods in three evaluation indicators\n(accuracy, precision, and recall) and the model can significantly improve\nsentiment classification. The ablation experiments show that compared with\nALBERT-AET, the proposed model ALBERT-FAET is better in the metrics, indicating\nthat the fine-grained attention network can understand the diversified\ninformation of emoticons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Deng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kejian_L/0/1/0/all/0/1\">Liu Kejian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuanyuan_F/0/1/0/all/0/1\">Feng Yuanyuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weihao_L/0/1/0/all/0/1\">Li Weihao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness of Explanation Methods for NLP Models. (arXiv:2206.12284v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12284","description":"<p>Explanation methods have emerged as an important tool to highlight the\nfeatures responsible for the predictions of neural networks. There is mounting\nevidence that many explanation methods are rather unreliable and susceptible to\nmalicious manipulations. In this paper, we particularly aim to understand the\nrobustness of explanation methods in the context of text modality. We provide\ninitial insights and results towards devising a successful adversarial attack\nagainst text explanations. To our knowledge, this is the first attempt to\nevaluate the adversarial robustness of an explanation method. Our experiments\nshow the explanation method can be largely disturbed for up to 86% of the\ntested samples with small changes in the input sentence and its semantics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atmakuri_S/0/1/0/all/0/1\">Shriya Atmakuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chheda_T/0/1/0/all/0/1\">Tejas Chheda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandula_D/0/1/0/all/0/1\">Dinesh Kandula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_N/0/1/0/all/0/1\">Nishant Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Taesung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuinhof_H/0/1/0/all/0/1\">Hessel Tuinhof</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text and author-level political inference using heterogeneous knowledge representations. (arXiv:2206.12293v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12293","description":"<p>The inference of politically-charged information from text data is a popular\nresearch topic in Natural Language Processing (NLP) at both text- and\nauthor-level. In recent years, studies of this kind have been implemented with\nthe aid of representations from transformers such as BERT. Despite considerable\nsuccess, however, we may ask whether results may be improved even further by\ncombining transformed-based models with additional knowledge representations.\nTo shed light on this issue, the present work describes a series of experiments\nto compare alternative model configurations for political inference from text\nin both English and Portuguese languages. Results suggest that certain text\nrepresentations - in particular, the combined use of BERT pre-trained language\nmodels with a syntactic dependency model - may outperform the alternatives\nacross multiple experimental settings, making a potentially strong case for\nfurther research in the use of heterogeneous text representations in these and\npossibly other NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_S/0/1/0/all/0/1\">Samuel Caetano da Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paraboni_I/0/1/0/all/0/1\">Ivandre Paraboni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Rhetorical Structure Theory-based descriptions of observed behaviour. (arXiv:2206.12294v1 [cs.AI])","link":"http://arxiv.org/abs/2206.12294","description":"<p>In a previous paper, we have proposed a set of concepts, axiom schemata and\nalgorithms that can be used by agents to learn to describe their behaviour,\ngoals, capabilities, and environment. The current paper proposes a new set of\nconcepts, axiom schemata and algorithms that allow the agent to learn new\ndescriptions of an observed behaviour (e.g., perplexing actions), of its actor\n(e.g., undesired propositions or actions), and of its environment (e.g.,\nincompatible propositions). Each learned description (e.g., a certain action\nprevents another action from being performed in the future) is represented by a\nrelationship between entities (either propositions or actions) and is learned\nby the agent, just by observation, using domain-independent axiom schemata and\nor learning algorithms. The relations used by agents to represent the\ndescriptions they learn were inspired on the Theory of Rhetorical Structure\n(RST). The main contribution of the paper is the relation family Although,\ninspired on the RST relation Concession. The accurate definition of the\nrelations of the family Although involves a set of deontic concepts whose\ndefinition and corresponding algorithms are presented. The relations of the\nfamily Although, once extracted from the agent's observations, express surprise\nat the observed behaviour and, in certain circumstances, present a\njustification for it.\n</p>\n<p>The paper shows results of the presented proposals in a demonstration\nscenario, using implemented software.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Botelho_L/0/1/0/all/0/1\">Luis Botelho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nunes_L/0/1/0/all/0/1\">Luis Nunes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_R/0/1/0/all/0/1\">Ricardo Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopes_R/0/1/0/all/0/1\">Rui J. Lopes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using BERT Embeddings to Model Word Importance in Conversational Transcripts for Deaf and Hard of Hearing Users. (arXiv:2206.12368v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12368","description":"<p>Deaf and hard of hearing individuals regularly rely on captioning while\nwatching live TV. Live TV captioning is evaluated by regulatory agencies using\nvarious caption evaluation metrics. However, caption evaluation metrics are\noften not informed by preferences of DHH users or how meaningful the captions\nare. There is a need to construct caption evaluation metrics that take the\nrelative importance of words in a transcript into account. We conducted\ncorrelation analysis between two types of word embeddings and human-annotated\nlabeled word-importance scores in existing corpus. We found that normalized\ncontextualized word embeddings generated using BERT correlated better with\nmanually annotated importance scores than word2vec-based word embeddings. We\nmake available a pairing of word embeddings and their human-annotated\nimportance scores. We also provide proof-of-concept utility by training word\nimportance models, achieving an F1-score of 0.57 in the 6-class word importance\nclassification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amin_A/0/1/0/all/0/1\">Akhter Al Amin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_S/0/1/0/all/0/1\">Saad Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alm_C/0/1/0/all/0/1\">Cecilia O. Alm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huenerfauth_M/0/1/0/all/0/1\">Matt Huenerfauth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QAGAN: Adversarial Approach To Learning Domain Invariant Language Features. (arXiv:2206.12388v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12388","description":"<p>Training models that are robust to data domain shift has gained an increasing\ninterest both in academia and industry. Question-Answering language models,\nbeing one of the typical problem in Natural Language Processing (NLP) research,\nhas received much success with the advent of large transformer models. However,\nexisting approaches mostly work under the assumption that data is drawn from\nsame distribution during training and testing which is unrealistic and\nnon-scalable in the wild.\n</p>\n<p>In this paper, we explore adversarial training approach towards learning\ndomain-invariant features so that language models can generalize well to\nout-of-domain datasets. We also inspect various other ways to boost our model\nperformance including data augmentation by paraphrasing sentences, conditioning\nend of answer span prediction on the start word, and carefully designed\nannealing function. Our initial results show that in combination with these\nmethods, we are able to achieve $15.2\\%$ improvement in EM score and $5.6\\%$\nboost in F1 score on out-of-domain validation dataset over the baseline. We\nalso dissect our model outputs and visualize the model hidden-states by\nprojecting them onto a lower-dimensional space, and discover that our specific\nadversarial training approach indeed encourages the model to learn domain\ninvariant embedding and bring them closer in the multi-dimensional space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_S/0/1/0/all/0/1\">Shubham Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaiyue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Exploratory Study on Utilising the Web of Linked Data for Product Data Mining. (arXiv:2109.01411v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01411","description":"<p>The Linked Open Data practice has led to a significant growth of structured\ndata on the Web in the last decade. Such structured data describe real-world\nentities in a machine-readable way, and have created an unprecedented\nopportunity for research in the field of Natural Language Processing. However,\nthere is a lack of studies on how such data can be used, for what kind of\ntasks, and to what extent they can be useful for these tasks. This work focuses\non the e-commerce domain to explore methods of utilising such structured data\nto create language resources that may be used for product classification and\nlinking. We process billions of structured data points in the form of RDF\nn-quads, to create multi-million words of product-related corpora that are\nlater used in three different ways for creating of language resources: training\nword embedding models, continued pre-training of BERT-like language models, and\ntraining Machine Translation models that are used as a proxy to generate\nproduct-related keywords. Our evaluation on an extensive set of benchmarks\nshows word embeddings to be the most reliable and consistent method to improve\nthe accuracy on both tasks (with up to 6.9 percentage points in macro-average\nF1 on some datasets). The other two methods however, are not as useful. Our\nanalysis shows that this could be due to a number of reasons, including the\nbiased domain representation in the structured data and lack of vocabulary\ncoverage. We share our datasets and discuss how our lessons learned could be\ntaken forward to inform future research in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Lexicon Reader: Reduce Pronunciation Errors in End-to-end TTS by Leveraging External Textual Knowledge. (arXiv:2110.09698v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2110.09698","description":"<p>End-to-end TTS requires a large amount of speech/text paired data to cover\nall necessary knowledge, particularly how to pronounce different words in\ndiverse contexts, so that a neural model may learn such knowledge accordingly.\nBut in real applications, such high demand of training data is hard to be\nsatisfied and additional knowledge often needs to be injected manually. For\nexample, to capture pronunciation knowledge on languages without regular\northography, a complicated grapheme-to-phoneme pipeline needs to be built based\non a large structured pronunciation lexicon, leading to extra, sometimes high,\ncosts to extend neural TTS to such languages. In this paper, we propose a\nframework to learn to automatically extract knowledge from unstructured\nexternal resources using a novel Token2Knowledge attention module. The\nframework is applied to build a TTS model named Neural Lexicon Reader that\nextracts pronunciations from raw lexicon texts in an end-to-end manner.\nExperiments show the proposed model significantly reduces pronunciation errors\nin low-resource, end-to-end Chinese TTS, and the lexicon-reading capability can\nbe transferred to other languages with a smaller amount of data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mutian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingzhou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soong_F/0/1/0/all/0/1\">Frank K. Soong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simplified Variant of G\\\"odel's Ontological Argument. (arXiv:2202.06264v2 [cs.LO] UPDATED)","link":"http://arxiv.org/abs/2202.06264","description":"<p>A simplified variant of G\\\"odel's ontological argument is presented. The\nsimplified argument is valid already in basic modal logics K or KT, it does not\nsuffer from modal collapse, and it avoids the rather complex predicates of\nessence (Ess.) and necessary existence (NE) as used by G\\\"odel. The variant\npresented has been obtained as a side result of a series of theory\nsimplification experiments conducted in interaction with a modern proof\nassistant system. The starting point for these experiments was the computer\nencoding of G\\\"odel's argument, and then automated reasoning techniques were\nsystematically applied to arrive at the simplified variant presented. The\npresented work thus exemplifies a fruitful human-computer interaction in\ncomputational metaphysics. Whether the presented result increases or decreases\nthe attractiveness and persuasiveness of the ontological argument is a question\nI would like to pass on to philosophy and theology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benzmuller_C/0/1/0/all/0/1\">Christoph Benzm&#xfc;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiFSMN: Binary Neural Network for Keyword Spotting. (arXiv:2202.06483v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.06483","description":"<p>The deep neural networks, such as the Deep-FSMN, have been widely studied for\nkeyword spotting (KWS) applications. However, computational resources for these\nnetworks are significantly constrained since they usually run on-call on edge\ndevices. In this paper, we present BiFSMN, an accurate and extreme-efficient\nbinary neural network for KWS. We first construct a High-frequency Enhancement\nDistillation scheme for the binarization-aware training, which emphasizes the\nhigh-frequency information from the full-precision network's representation\nthat is more crucial for the optimization of the binarized network. Then, to\nallow the instant and adaptive accuracy-efficiency trade-offs at runtime, we\nalso propose a Thinnable Binarization Architecture to further liberate the\nacceleration potential of the binarized network from the topology perspective.\nMoreover, we implement a Fast Bitwise Computation Kernel for BiFSMN on ARMv8\ndevices which fully utilizes registers and increases instruction throughput to\npush the limit of deployment efficiency. Extensive experiments show that BiFSMN\noutperforms existing binarization methods by convincing margins on various\ndatasets and is even comparable with the full-precision counterpart (e.g., less\nthan 3% drop on Speech Commands V1-12). We highlight that benefiting from the\nthinnable architecture and the optimized 1-bit implementation, BiFSMN can\nachieve an impressive 22.3x speedup and 15.5x storage-saving on real-world edge\nhardware.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Haotong Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xudong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Provably Confidential Language Modelling. (arXiv:2205.01863v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.01863","description":"<p>Large language models are shown to memorize privacy information such as\nsocial security numbers in training data. Given the sheer scale of the training\ncorpus, it is challenging to screen and filter these privacy data, either\nmanually or automatically. In this paper, we propose Confidentially Redacted\nTraining (CRT), a method to train language generation models while protecting\nthe confidential segments. We borrow ideas from differential privacy (which\nsolves a related but distinct problem) and show that our method is able to\nprovably prevent unintended memorization by randomizing parts of the training\nprocess. Moreover, we show that redaction with an approximately correct\nscreening policy amplifies the confidentiality guarantee. We implement the\nmethod for both LSTM and GPT language models. Our experimental results show\nthat the models trained by CRT obtain almost the same perplexity while\npreserving strong confidentiality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xuandong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Sentence Embedding Models Performance for Patent Analysis. (arXiv:2206.02690v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.02690","description":"<p>Patent data is an important source of knowledge for innovation research.\nWhile the technological similarity between pairs of patents is a key enabling\nindicator for patent analysis. Recently researchers have been using patent\nvector space models based on different NLP embeddings models to calculate\ntechnological similarity between pairs of patents to help better understand\ninnovations, patent landscaping, technology mapping, and patent quality\nevaluation. To the best of our knowledge, there is not a comprehensive survey\nthat builds a big picture of embedding models' performance for calculating\npatent similarity indicators. Therefore, in this study, we provide an overview\nof the accuracy of these algorithms based on patent classification performance.\nIn a detailed discussion, we report the performance of the top 3 algorithms at\nsection, class, and subclass levels. The results based on the first claim of\npatents show that PatentSBERTa, Bert-for-patents, and TF-IDF Weighted Word\nEmbeddings have the best accuracy for computing sentence embeddings at the\nsubclass level. According to the first results, the performance of the models\nin different classes varies which shows researchers in patent analysis can\nutilize the results of this study for choosing the best proper model based on\nthe specific section of patent data they used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bekamiri_H/0/1/0/all/0/1\">Hamid Bekamiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hain_D/0/1/0/all/0/1\">Daniel S. Hain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurowetzki_R/0/1/0/all/0/1\">Roman Jurowetzki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-in-Graph Network for Automatic Gene Ontology Description Generation. (arXiv:2206.05311v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2206.05311","description":"<p>Gene Ontology (GO) is the primary gene function knowledge base that enables\ncomputational tasks in biomedicine. The basic element of GO is a term, which\nincludes a set of genes with the same function. Existing research efforts of GO\nmainly focus on predicting gene term associations. Other tasks, such as\ngenerating descriptions of new terms, are rarely pursued. In this paper, we\npropose a novel task: GO term description generation. This task aims to\nautomatically generate a sentence that describes the function of a GO term\nbelonging to one of the three categories, i.e., molecular function, biological\nprocess, and cellular component. To address this task, we propose a\nGraph-in-Graph network that can efficiently leverage the structural information\nof GO. The proposed network introduces a two-layer graph: the first layer is a\ngraph of GO terms where each node is also a graph (gene graph). Such a\nGraph-in-Graph network can derive the biological functions of GO terms and\ngenerate proper descriptions. To validate the effectiveness of the proposed\nnetwork, we build three large-scale benchmark datasets. By incorporating the\nproposed Graph-in-Graph network, the performances of seven different\nsequence-to-sequence models can be substantially boosted across all evaluation\nmetrics, with up to 34.7%, 14.5%, and 39.1% relative improvements in BLEU,\nROUGE-L, and METEOR, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woicik_A/0/1/0/all/0/1\">Adelaide Woicik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-LexSum: Real-World Summaries of Civil Rights Lawsuits at Multiple Granularities. (arXiv:2206.10883v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.10883","description":"<p>With the advent of large language models, methods for abstractive\nsummarization have made great strides, creating potential for use in\napplications to aid knowledge workers processing unwieldy document collections.\nOne such setting is the Civil Rights Litigation Clearinghouse (CRLC)\n(https://clearinghouse.net),which posts information about large-scale civil\nrights lawsuits, serving lawyers, scholars, and the general public. Today,\nsummarization in the CRLC requires extensive training of lawyers and law\nstudents who spend hours per case understanding multiple relevant documents in\norder to produce high-quality summaries of key events and outcomes. Motivated\nby this ongoing real-world summarization effort, we introduce Multi-LexSum, a\ncollection of 9,280 expert-authored summaries drawn from ongoing CRLC writing.\nMulti-LexSum presents a challenging multi-document summarization task given the\nlength of the source documents, often exceeding two hundred pages per case.\nFurthermore, Multi-LexSum is distinct from other datasets in its multiple\ntarget summaries, each at a different granularity (ranging from one-sentence\n\"extreme\" summaries to multi-paragraph narrations of over five hundred words).\nWe present extensive analysis demonstrating that despite the high-quality\nsummaries in the training data (adhering to strict content and style\nguidelines), state-of-the-art summarization models perform poorly on this task.\nWe release Multi-LexSum for further research in summarization methods as well\nas to facilitate development of applications to assist in the CRLC's mission at\nhttps://multilexsum.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zejiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lauren Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahlberg_N/0/1/0/all/0/1\">Nathan Dahlberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlanger_M/0/1/0/all/0/1\">Margo Schlanger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GEMv2: Multilingual NLG Benchmarking in a Single Line of Code. (arXiv:2206.11249v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.11249","description":"<p>Evaluation in machine learning is usually informed by past choices, for\nexample which datasets or metrics to use. This standardization enables the\ncomparison on equal footing using leaderboards, but the evaluation choices\nbecome sub-optimal as better alternatives arise. This problem is especially\npertinent in natural language generation which requires ever-improving suites\nof datasets, metrics, and human evaluation to make definitive claims. To make\nfollowing best model evaluation practices easier, we introduce GEMv2. The new\nversion of the Generation, Evaluation, and Metrics Benchmark introduces a\nmodular infrastructure for dataset, model, and metric developers to benefit\nfrom each others work. GEMv2 supports 40 documented datasets in 51 languages.\nModels for all datasets can be evaluated online and our interactive data card\ncreation and rendering tools make it easier to add new datasets to the living\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Abhik Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahendiran_A/0/1/0/all/0/1\">Abinaya Mahendiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alex Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1\">Alexandros Papangelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMillan_Major_A/0/1/0/all/0/1\">Angelina McMillan-Major</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shvets_A/0/1/0/all/0/1\">Anna Shvets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_A/0/1/0/all/0/1\">Ashish Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bingsheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chaobin You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomson_C/0/1/0/all/0/1\">Craig Thomson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garbacea_C/0/1/0/all/0/1\">Cristina Garbacea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1\">Daniel Deutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkatzia_D/0/1/0/all/0/1\">Dimitra Gkatzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_E/0/1/0/all/0/1\">Elizabeth Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladhak_F/0/1/0/all/0/1\">Faisal Ladhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginter_F/0/1/0/all/0/1\">Filip Ginter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1\">Hendrik Strobelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1\">Hiroaki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novikova_J/0/1/0/all/0/1\">Jekaterina Novikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanerva_J/0/1/0/all/0/1\">Jenna Kanerva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chim_J/0/1/0/all/0/1\">Jenny Chim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiawei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clive_J/0/1/0/all/0/1\">Jordan Clive</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1\">Joshua Maynez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juraska_J/0/1/0/all/0/1\">Juraj Juraska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhole_K/0/1/0/all/0/1\">Kaustubh Dhole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1\">Khyathi Raghavi Chandu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Beltrachini_L/0/1/0/all/0/1\">Laura Perez-Beltrachini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leonardo F. R. Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tunstall_L/0/1/0/all/0/1\">Lewis Tunstall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pushkarna_M/0/1/0/all/0/1\">Mahima Pushkarna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creutz_M/0/1/0/all/0/1\">Mathias Creutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1\">Michael White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1\">Mihir Sanjay Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eddine_M/0/1/0/all/0/1\">Moussa Kamal Eddine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daheim_N/0/1/0/all/0/1\">Nico Daheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramani_N/0/1/0/all/0/1\">Nishant Subramani</a>, et al. (28 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Agriculture-Vision Challenge 2022 -- The Runner-Up Solution for Agricultural Pattern Recognition via Transformer-based Models. (arXiv:2206.11920v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11920","description":"<p>The Agriculture-Vision Challenge in CVPR is one of the most famous and\ncompetitive challenges for global researchers to break the boundary between\ncomputer vision and agriculture sectors, aiming at agricultural pattern\nrecognition from aerial images. In this paper, we propose our solution to the\nthird Agriculture-Vision Challenge in CVPR 2022. We leverage a data\npre-processing scheme and several Transformer-based models as well as data\naugmentation techniques to achieve a mIoU of 0.582, accomplishing the 2nd place\nin this challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhicheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Jui-Hsin Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1\">Chen Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1\">Zhongcheng Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Galaxy Foundation Models with Hybrid Contrastive Learning. (arXiv:2206.11927v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11927","description":"<p>New astronomical tasks are often related to earlier tasks for which labels\nhave already been collected. We adapt the contrastive framework BYOL to\nleverage those labels as a pretraining task while also enforcing augmentation\ninvariance. For large-scale pretraining, we introduce GZ-Evo v0.1, a set of\n96.5M volunteer responses for 552k galaxy images plus a further 1.34M\ncomparable unlabelled galaxies. Most of the 206 GZ-Evo answers are unknown for\nany given galaxy, and so our pretraining task uses a Dirichlet loss that\nnaturally handles unknown answers. GZ-Evo pretraining, with or without hybrid\nlearning, improves on direct training even with plentiful downstream labels\n(+4% accuracy with 44k labels). Our hybrid pretraining/contrastive method\nfurther improves downstream accuracy vs. pretraining or contrastive learning,\nespecially in the low-label transfer regime (+6% accuracy with 750 labels).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Walmsley_M/0/1/0/all/0/1\">Mike Walmsley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slijepcevic_I/0/1/0/all/0/1\">Inigo Val Slijepcevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowles_M/0/1/0/all/0/1\">Micah Bowles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaife_A/0/1/0/all/0/1\">Anna M. M. Scaife</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TIAger: Tumor-Infiltrating Lymphocyte Scoring in Breast Cancer for the TiGER Challenge. (arXiv:2206.11943v1 [eess.IV])","link":"http://arxiv.org/abs/2206.11943","description":"<p>The quantification of tumor-infiltrating lymphocytes (TILs) has been shown to\nbe an independent predictor for prognosis of breast cancer patients. Typically,\npathologists give an estimate of the proportion of the stromal region that\ncontains TILs to obtain a TILs score. The Tumor InfiltratinG lymphocytes in\nbreast cancER (TiGER) challenge, aims to assess the prognostic significance of\ncomputer-generated TILs scores for predicting survival as part of a Cox\nproportional hazards model. For this challenge, as the TIAger team, we have\ndeveloped an algorithm to first segment tumor vs. stroma, before localising the\ntumor bulk region for TILs detection. Finally, we use these outputs to generate\na TILs score for each case. On preliminary testing, our approach achieved a\ntumor-stroma weighted Dice score of 0.791 and a FROC score of 0.572 for\nlymphocytic detection. For predicting survival, our model achieved a C-index of\n0.719. These results achieved first place across the preliminary testing\nleaderboards of the TiGER challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shephard_A/0/1/0/all/0/1\">Adam Shephard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jahanifar_M/0/1/0/all/0/1\">Mostafa Jahanifar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Ruoyu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dawood_M/0/1/0/all/0/1\">Muhammad Dawood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Graham_S/0/1/0/all/0/1\">Simon Graham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sidlauskas_K/0/1/0/all/0/1\">Kastytis Sidlauskas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khurram_S/0/1/0/all/0/1\">Syed Ali Khurram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raza_S/0/1/0/all/0/1\">Shan E Ahmed Raza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNeRF: Time and Memory Conscious U-Shaped Network for Training Neural Radiance Fields. (arXiv:2206.11952v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11952","description":"<p>Neural Radiance Fields (NeRFs) increase reconstruction detail for novel view\nsynthesis and scene reconstruction, with applications ranging from large static\nscenes to dynamic human motion. However, the increased resolution and\nmodel-free nature of such neural fields come at the cost of high training times\nand excessive memory requirements. Recent advances improve the inference time\nby using complementary data structures yet these methods are ill-suited for\ndynamic scenes and often increase memory consumption. Little has been done to\nreduce the resources required at training time. We propose a method to exploit\nthe redundancy of NeRF's sample-based computations by partially sharing\nevaluations across neighboring sample points. Our UNeRF architecture is\ninspired by the UNet, where spatial resolution is reduced in the middle of the\nnetwork and information is shared between adjacent samples. Although this\nchange violates the strict and conscious separation of view-dependent\nappearance and view-independent density estimation in the NeRF method, we show\nthat it improves novel view synthesis. We also introduce an alternative\nsubsampling strategy which shares computation while minimizing any violation of\nview invariance. UNeRF is a plug-in module for the original NeRF network. Our\nmajor contributions include reduction of the memory footprint, improved\naccuracy, and reduced amortized processing time both during training and\ninference. With only weak assumptions on locality, we achieve improved resource\nutilization on a variety of neural radiance fields tasks. We demonstrate\napplications to the novel view synthesis of static scenes as well as dynamic\nhuman shape and motion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuganesan_A/0/1/0/all/0/1\">Abiramy Kuganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shih-yang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Little_J/0/1/0/all/0/1\">James J. Little</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1\">Helge Rhodin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Second Place Solution for The 4th Large-scale Video Object Segmentation Challenge--Track 3: Referring Video Object Segmentation. (arXiv:2206.12035v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12035","description":"<p>The referring video object segmentation task (RVOS) aims to segment object\ninstances in a given video referred by a language expression in all video\nframes. Due to the requirement of understanding cross-modal semantics within\nindividual instances, this task is more challenging than the traditional\nsemi-supervised video object segmentation where the ground truth object masks\nin the first frame are given. With the great achievement of Transformer in\nobject detection and object segmentation, RVOS has been made remarkable\nprogress where ReferFormer achieved the state-of-the-art performance. In this\nwork, based on the strong baseline framework--ReferFormer, we propose several\ntricks to boost further, including cyclical learning rates, semi-supervised\napproach, and test-time augmentation inference. The improved ReferFormer ranks\n2nd place on CVPR2022 Referring Youtube-VOS Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Leilei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bo Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Feng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fengliang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuchen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongbin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Protecting President Zelenskyy against Deep Fakes. (arXiv:2206.12043v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12043","description":"<p>The 2022 Russian invasion of Ukraine is being fought on two fronts: a brutal\nground war and a duplicitous disinformation campaign designed to conceal and\njustify Russia's actions. This campaign includes at least one example of a\ndeep-fake video purportedly showing Ukrainian President Zelenskyy admitting\ndefeat and surrendering. In anticipation of future attacks of this form, we\ndescribe a facial and gestural behavioral model that captures distinctive\ncharacteristics of Zelenskyy's speaking style. Trained on over eight hours of\nauthentic video from four different settings, we show that this behavioral\nmodel can distinguish Zelenskyy from deep-fake imposters.This model can play an\nimportant role -- particularly during the fog of war -- in distinguishing the\nreal from the fake.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bohacek_M/0/1/0/all/0/1\">Maty&#xe1;&#x161; Boh&#xe1;&#x10d;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farid_H/0/1/0/all/0/1\">Hany Farid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bilateral Network with Channel Splitting Network and Transformer for Thermal Image Super-Resolution. (arXiv:2206.12046v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12046","description":"<p>In recent years, the Thermal Image Super-Resolution (TISR) problem has become\nan attractive research topic. TISR would been used in a wide range of fields,\nincluding military, medical, agricultural and animal ecology. Due to the\nsuccess of PBVS-2020 and PBVS-2021 workshop challenge, the result of TISR keeps\nimproving and attracts more researchers to sign up for PBVS-2022 challenge. In\nthis paper, we will introduce the technical details of our submission to\nPBVS-2022 challenge designing a Bilateral Network with Channel Splitting\nNetwork and Transformer(BN-CSNT) to tackle the TISR problem. Firstly, we\ndesigned a context branch based on channel splitting network with transformer\nto obtain sufficient context information. Secondly, we designed a spatial\nbranch with shallow transformer to extract low level features which can\npreserve the spatial information. Finally, for the context branch in order to\nfuse the features from channel splitting network and transformer, we proposed\nan attention refinement module, and then features from context branch and\nspatial branch are fused by proposed feature fusion module. The proposed method\ncan achieve PSNR=33.64, SSIM=0.9263 for x4 and PSNR=21.08, SSIM=0.7803 for x2\nin the PBVS-2022 challenge test dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bo Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Leilei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fengliang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongbin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SDF-StyleGAN: Implicit SDF-Based StyleGAN for 3D Shape Generation. (arXiv:2206.12055v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12055","description":"<p>We present a StyleGAN2-based deep learning approach for 3D shape generation,\ncalled SDF-StyleGAN, with the aim of reducing visual and geometric\ndissimilarity between generated shapes and a shape collection. We extend\nStyleGAN2 to 3D generation and utilize the implicit signed distance function\n(SDF) as the 3D shape representation, and introduce two novel global and local\nshape discriminators that distinguish real and fake SDF values and gradients to\nsignificantly improve shape geometry and visual quality. We further complement\nthe evaluation metrics of 3D generative models with the shading-image-based\nFr\\'echet inception distance (FID) scores to better assess visual quality and\nshape distribution of the generated shapes. Experiments on shape generation\ndemonstrate the superior performance of SDF-StyleGAN over the state-of-the-art.\nWe further demonstrate the efficacy of SDF-StyleGAN in various tasks based on\nGAN inversion, including shape reconstruction, shape completion from partial\npoint clouds, single-view image-based shape generation, and shape style\nediting. Extensive ablation studies justify the efficacy of our framework\ndesign. Our code and trained models are available at\nhttps://github.com/Zhengxinyang/SDF-StyleGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xin-Yang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng-Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1\">Xin Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mutual Information-guided Knowledge Transfer for Novel Class Discovery. (arXiv:2206.12063v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12063","description":"<p>We tackle the novel class discovery problem, aiming to discover novel classes\nin unlabeled data based on labeled data from seen classes. The main challenge\nis to transfer knowledge contained in the seen classes to unseen ones. Previous\nmethods mostly transfer knowledge through sharing representation space or joint\nlabel space. However, they tend to neglect the class relation between seen and\nunseen categories, and thus the learned representations are less effective for\nclustering unseen classes. In this paper, we propose a principle and general\nmethod to transfer semantic knowledge between seen and unseen classes. Our\ninsight is to utilize mutual information to measure the relation between seen\nclasses and unseen classes in a restricted label space and maximizing mutual\ninformation promotes transferring semantic knowledge. To validate the\neffectiveness and generalization of our method, we conduct extensive\nexperiments both on novel class discovery and general novel class discovery\nsettings. Our results show that the proposed method outperforms previous SOTA\nby a significant margin on several benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuanyang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruijie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhitong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning of Features between Images and LiDAR. (arXiv:2206.12071v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12071","description":"<p>Image and Point Clouds provide different information for robots. Finding the\ncorrespondences between data from different sensors is crucial for various\ntasks such as localization, mapping, and navigation. Learning-based descriptors\nhave been developed for single sensors; there is little work on cross-modal\nfeatures. This work treats learning cross-modal features as a dense contrastive\nlearning problem. We propose a Tuple-Circle loss function for cross-modality\nfeature learning. Furthermore, to learn good features and not lose generality,\nwe developed a variant of widely used PointNet++ architecture for point cloud\nand U-Net CNN architecture for images. Moreover, we conduct experiments on a\nreal-world dataset to show the effectiveness of our loss function and network\nstructure. We show that our models indeed learn information from both images as\nwell as LiDAR by visualizing the features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saripalli_S/0/1/0/all/0/1\">Srikanth Saripalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaskRange: A Mask-classification Model for Range-view based LiDAR Segmentation. (arXiv:2206.12073v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12073","description":"<p>Range-view based LiDAR segmentation methods are attractive for practical\napplications due to their direct inheritance from efficient 2D CNN\narchitectures. In literature, most range-view based methods follow the\nper-pixel classification paradigm. Recently, in the image segmentation domain,\nanother paradigm formulates segmentation as a mask-classification problem and\nhas achieved remarkable performance. This raises an interesting question: can\nthe mask-classification paradigm benefit the range-view based LiDAR\nsegmentation and achieve better performance than the counterpart per-pixel\nparadigm? To answer this question, we propose a unified mask-classification\nmodel, MaskRange, for the range-view based LiDAR semantic and panoptic\nsegmentation. Along with the new paradigm, we also propose a novel data\naugmentation method to deal with overfitting, context-reliance, and\nclass-imbalance problems. Extensive experiments are conducted on the\nSemanticKITTI benchmark. Among all published range-view based methods, our\nMaskRange achieves state-of-the-art performance with $66.10$ mIoU on semantic\nsegmentation and promising results with $53.10$ PQ on panoptic segmentation\nwith high efficiency. Our code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengzhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_H/0/1/0/all/0/1\">Hui Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A novel approach for glaucoma classification by wavelet neural networks using graph-based, statisitcal features of qualitatively improved images. (arXiv:2206.12099v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12099","description":"<p>In this paper, we have proposed a new glaucoma classification approach that\nemploys a wavelet neural network (WNN) on optimally enhanced retinal images\nfeatures. To avoid tedious and error prone manual analysis of retinal images by\nophthalmologists, computer aided diagnosis (CAD) substantially aids in robust\ndiagnosis. Our objective is to introduce a CAD system with a fresh approach.\nRetinal image quality improvement is attempted in two phases. The retinal image\npreprocessing phase improves the brightness and contrast of the image through\nquantile based histogram modification. It is followed by the image enhancement\nphase, which involves multi scale morphological operations using image specific\ndynamic structuring elements for the retinal structure enrichment. Graph based\nretinal image features in terms of Local Graph Structures (LGS) and Graph\nShortest Path (GSP) statistics are extracted from various directions along with\nthe statistical features from the enhanced retinal dataset. WNN is employed to\nclassify glaucoma retinal images with a suitable wavelet activation function.\nThe performance of the WNN classifier is compared with multilayer perceptron\nneural networks with various datasets. The results show our approach is\nsuperior to the existing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santosh_N/0/1/0/all/0/1\">N. Krishna Santosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barpanda_D/0/1/0/all/0/1\">Dr. Soubhagya Sankar Barpanda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dissecting U-net for Seismic Application: An In-Depth Study on Deep Learning Multiple Removal. (arXiv:2206.12112v1 [eess.IV])","link":"http://arxiv.org/abs/2206.12112","description":"<p>Seismic processing often requires suppressing multiples that appear when\ncollecting data. To tackle these artifacts, practitioners usually rely on Radon\ntransform-based algorithms as post-migration gather conditioning. However, such\ntraditional approaches are both time-consuming and parameter-dependent, making\nthem fairly complex. In this work, we present a deep learning-based alternative\nthat provides competitive results, while reducing its usage's complexity, and\nhence democratizing its applicability. We observe an excellent performance of\nour network when inferring complex field data, despite the fact of being solely\ntrained on synthetics. Furthermore, extensive experiments show that our\nproposal can preserve the inherent characteristics of the data, avoiding\nundesired over-smoothed results, while removing the multiples. Finally, we\nconduct an in-depth analysis of the model, where we pinpoint the effects of the\nmain hyperparameters with physical events. To the best of our knowledge, this\nstudy pioneers the unboxing of neural networks for the demultiple process,\nhelping the user to gain insights into the inside running of the network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Durall_R/0/1/0/all/0/1\">Ricard Durall</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghanim_A/0/1/0/all/0/1\">Ammar Ghanim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ettrich_N/0/1/0/all/0/1\">Norman Ettrich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keuper_J/0/1/0/all/0/1\">Janis Keuper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self Supervised Learning for Few Shot Hyperspectral Image Classification. (arXiv:2206.12117v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12117","description":"<p>Deep learning has proven to be a very effective approach for Hyperspectral\nImage (HSI) classification. However, deep neural networks require large\nannotated datasets to generalize well. This limits the applicability of deep\nlearning for HSI classification, where manually labelling thousands of pixels\nfor every scene is impractical. In this paper, we propose to leverage Self\nSupervised Learning (SSL) for HSI classification. We show that by pre-training\nan encoder on unlabeled pixels using Barlow-Twins, a state-of-the-art SSL\nalgorithm, we can obtain accurate models with a handful of labels. Experimental\nresults demonstrate that this approach significantly outperforms vanilla\nsupervised learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Braham_N/0/1/0/all/0/1\">Nassim Ait Ali Braham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lichao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mairal_J/0/1/0/all/0/1\">Julien Mairal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Some theoretical results on discrete contour trees. (arXiv:2206.12123v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12123","description":"<p>Contour trees have been developed to visualize or encode scalar data in\nimaging technologies and scientific simulations. Contours are defined on a\ncontinuous scalar field. For discrete data, a continuous function is first\ninterpolated, where contours are then defined. In this paper we define a\ndiscrete contour tree, called the iso-tree, on a scalar graph, and discuss its\nproperties. We show that the iso-tree model works for data of all dimensions,\nand develop an axiomatic system formalizing the discrete contour structures. We\nalso report an isomorphism between iso-trees and augmented contour trees,\nshowing that contour tree algorithms can be used to compute discrete contour\ntrees, and vice versa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuqing Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive Learning. (arXiv:2206.12126v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12126","description":"<p>Spatiotemporal predictive learning aims to generate future frames by learning\nfrom historical frames. In this paper, we investigate existing methods and\npresent a general framework of spatiotemporal predictive learning, in which the\nspatial encoder and decoder capture intra-frame features and the middle\ntemporal module catches inter-frame correlations. While the mainstream methods\nemploy recurrent units to capture long-term temporal dependencies, they suffer\nfrom low computational efficiency due to their unparallelizable architectures.\nTo parallelize the temporal module, we propose the Temporal Attention Unit\n(TAU), which decomposes the temporal attention into intra-frame statical\nattention and inter-frame dynamical attention. Moreover, while the mean squared\nerror loss focuses on intra-frame errors, we introduce a novel differential\ndivergence regularization to take inter-frame variations into account.\nExtensive experiments demonstrate that the proposed method enables the derived\nmodel to achieve competitive performance on various spatiotemporal prediction\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Cheng Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhangyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongjie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Excavating RoI Attention for Underwater Object Detection. (arXiv:2206.12128v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12128","description":"<p>Self-attention is one of the most successful designs in deep learning, which\ncalculates the similarity of different tokens and reconstructs the feature\nbased on the attention matrix. Originally designed for NLP, self-attention is\nalso popular in computer vision, and can be categorized into pixel-level\nattention and patch-level attention. In object detection, RoI features can be\nseen as patches from base feature maps. This paper aims to apply the attention\nmodule to RoI features to improve performance. Instead of employing an original\nself-attention module, we choose the external attention module, a modified\nself-attention with reduced parameters. With the proposed double head structure\nand the Positional Encoding module, our method can achieve promising\nperformance in object detection. The comprehensive experiments show that it\nachieves promising performance, especially in the underwater object detection\ndataset. The code will be avaiable in:\nhttps://github.com/zsyasd/Excavating-RoI-Attention-for-Underwater-Object-Detection\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xutao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_P/0/1/0/all/0/1\">Pinhao Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Representation Learning for Robust Retinal Disease Detection from Optical Coherence Tomography Images. (arXiv:2206.12136v1 [eess.IV])","link":"http://arxiv.org/abs/2206.12136","description":"<p>Ophthalmic images may contain identical-looking pathologies that can cause\nfailure in automated techniques to distinguish different retinal degenerative\ndiseases. Additionally, reliance on large annotated datasets and lack of\nknowledge distillation can restrict ML-based clinical support systems'\ndeployment in real-world environments. To improve the robustness and\ntransferability of knowledge, an enhanced feature-learning module is required\nto extract meaningful spatial representations from the retinal subspace. Such a\nmodule, if used effectively, can detect unique disease traits and differentiate\nthe severity of such retinal degenerative pathologies. In this work, we propose\na robust disease detection architecture with three learning heads, i) A\nsupervised encoder for retinal disease classification, ii) An unsupervised\ndecoder for the reconstruction of disease-specific spatial information, and\niii) A novel representation learning module for learning the similarity between\nencoder-decoder feature and enhancing the accuracy of the model. Our\nexperimental results on two publicly available OCT datasets illustrate that the\nproposed model outperforms existing state-of-the-art models in terms of\naccuracy, interpretability, and robustness for out-of-distribution retinal\ndisease detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kamran_S/0/1/0/all/0/1\">Sharif Amit Kamran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hossain_K/0/1/0/all/0/1\">Khondker Fariha Hossain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tavakkoli_A/0/1/0/all/0/1\">Alireza Tavakkoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zuckerbrod_S/0/1/0/all/0/1\">Stewart Lee Zuckerbrod</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baker_S/0/1/0/all/0/1\">Salah A. Baker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmented Reality-Empowered Network Planning Services for Private Networks. (arXiv:2206.12139v1 [cs.NI])","link":"http://arxiv.org/abs/2206.12139","description":"<p>To support Industry 4.0 applications with haptics and human-machine\ninteraction, the sixth generation (6G) requires a new framework that is fully\nautonomous, visual, and interactive. In this paper, we propose a novel\nframework for private network planning services, providing an end-to-end\nsolution that receives visual and sensory data from the user device,\nreconstructs the 3D network environment and performs network planning on the\nserver, and visualizes the network performance with augmented reality (AR) on\nthe display of the user devices. The solution is empowered by three key\ntechnical components: 1) vision- and sensor fusion-based 3D environment\nreconstruction, 2) ray tracing-based radio map generation and network planning,\nand 3) AR-empowered network visualization enabled by real-time camera\nrelocalization. We conducted the proof-of-concept in a Bosch plant in Germany\nand showed good network coverage of the optimized antenna location, as well as\nhigh accuracy in both environment reconstruction and camera relocalization. We\nalso achieved real-time AR-supported network monitoring with an end-to-end\nlatency of about 32 ms per frame.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tianlun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchenko_N/0/1/0/all/0/1\">Nikolaj Marchenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient and Robust Training of Dense Object Nets for Multi-Object Robot Manipulation. (arXiv:2206.12145v1 [cs.RO])","link":"http://arxiv.org/abs/2206.12145","description":"<p>We propose a framework for robust and efficient training of Dense Object Nets\n(DON) with a focus on multi-object robot manipulation scenarios. DON is a\npopular approach to obtain dense, view-invariant object descriptors, which can\nbe used for a multitude of downstream tasks in robot manipulation, such as,\npose estimation, state representation for control, etc.. However, the original\nwork focused training on singulated objects, with limited results on\ninstance-specific, multi-object applications. Additionally, a complex data\ncollection pipeline, including 3D reconstruction and mask annotation of each\nobject, is required for training. In this paper, we further improve the\nefficacy of DON with a simplified data collection and training regime, that\nconsistently yields higher precision and enables robust tracking of keypoints\nwith less data requirements. In particular, we focus on training with\nmulti-object data instead of singulated objects, combined with a well-chosen\naugmentation scheme. We additionally propose an alternative loss formulation to\nthe original pixelwise formulation that offers better results and is less\nsensitive to hyperparameters. Finally, we demonstrate the robustness and\naccuracy of our proposed framework on a real-world robotic grasping task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adrian_D/0/1/0/all/0/1\">David B. Adrian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kupcsik_A/0/1/0/all/0/1\">Andras Gabor Kupcsik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spies_M/0/1/0/all/0/1\">Markus Spies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_H/0/1/0/all/0/1\">Heiko Neumann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimized Views Photogrammetry: Precision Analysis and A Large-scale Case Study in Qingdao. (arXiv:2206.12216v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12216","description":"<p>UAVs have become one of the widely used remote sensing platforms and played a\ncritical role in the construction of smart cities. However, due to the complex\nenvironment in urban scenes, secure and accurate data acquisition brings great\nchallenges to 3D modeling and scene updating. Optimal trajectory planning of\nUAVs and accurate data collection of onboard cameras are non-trivial issues in\nurban modeling. This study presents the principle of optimized views\nphotogrammetry and verifies its precision and potential in large-scale 3D\nmodeling. Different from oblique photogrammetry, optimized views photogrammetry\nuses rough models to generate and optimize UAV trajectories, which is achieved\nthrough the consideration of model point reconstructability and view point\nredundancy. Based on the principle of optimized views photogrammetry, this\nstudy first conducts a precision analysis of 3D models by using UAV images of\noptimized views photogrammetry and then executes a large-scale case study in\nthe urban region of Qingdao city, China, to verify its engineering potential.\nBy using GCPs for image orientation precision analysis and TLS (terrestrial\nlaser scanning) point clouds for model quality analysis, experimental results\nshow that optimized views photogrammetry could construct stable image\nconnection networks and could achieve comparable image orientation accuracy.\nBenefiting from the accurate image acquisition strategy, the quality of mesh\nmodels significantly improves, especially for urban areas with serious\nocclusions, in which 3 to 5 times of higher accuracy has been achieved.\nBesides, the case study in Qingdao city verifies that optimized views\nphotogrammetry can be a reliable and powerful solution for the large-scale 3D\nmodeling in complex urban scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenshuai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">San Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Zoom Lens: A Novel Physical-World Attack to DNNs. (arXiv:2206.12251v1 [cs.CR])","link":"http://arxiv.org/abs/2206.12251","description":"<p>Although deep neural networks (DNNs) are known to be fragile, no one has\nstudied the effects of zooming-in and zooming-out of images in the physical\nworld on DNNs performance. In this paper, we demonstrate a novel physical\nadversarial attack technique called Adversarial Zoom Lens (AdvZL), which uses a\nzoom lens to zoom in and out of pictures of the physical world, fooling DNNs\nwithout changing the characteristics of the target object. The proposed method\nis so far the only adversarial attack technique that does not add physical\nadversarial perturbation attack DNNs. In a digital environment, we construct a\ndata set based on AdvZL to verify the antagonism of equal-scale enlarged images\nto DNNs. In the physical environment, we manipulate the zoom lens to zoom in\nand out of the target object, and generate adversarial samples. The\nexperimental results demonstrate the effectiveness of AdvZL in both digital and\nphysical environments. We further analyze the antagonism of the proposed data\nset to the improved DNNs. On the other hand, we provide a guideline for defense\nagainst AdvZL by means of adversarial training. Finally, we look into the\nthreat possibilities of the proposed approach to future autonomous driving and\nvariant attack ideas similar to the proposed attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chengyin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weiwen Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfoAT: Improving Adversarial Training Using the Information Bottleneck Principle. (arXiv:2206.12292v1 [cs.LG])","link":"http://arxiv.org/abs/2206.12292","description":"<p>Adversarial training (AT) has shown excellent high performance in defending\nagainst adversarial examples. Recent studies demonstrate that examples are not\nequally important to the final robustness of models during AT, that is, the\nso-called hard examples that can be attacked easily exhibit more influence than\nrobust examples on the final robustness. Therefore, guaranteeing the robustness\nof hard examples is crucial for improving the final robustness of the model.\nHowever, defining effective heuristics to search for hard examples is still\ndifficult. In this article, inspired by the information bottleneck (IB)\nprinciple, we uncover that an example with high mutual information of the input\nand its associated latent representation is more likely to be attacked. Based\non this observation, we propose a novel and effective adversarial training\nmethod (InfoAT). InfoAT is encouraged to find examples with high mutual\ninformation and exploit them efficiently to improve the final robustness of\nmodels. Experimental results show that InfoAT achieves the best robustness\namong different datasets and models in comparison with several state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengting Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongnian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Daoqiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic extraction of coronary arteries using deep learning in invasive coronary angiograms. (arXiv:2206.12300v1 [eess.IV])","link":"http://arxiv.org/abs/2206.12300","description":"<p>Accurate extraction of coronary arteries from invasive coronary angiography\n(ICA) is important in clinical decision-making for the diagnosis and risk\nstratification of coronary artery disease (CAD). In this study, we develop a\nmethod using deep learning to automatically extract the coronary artery lumen.\nMethods. A deep learning model U-Net 3+, which incorporates the full-scale skip\nconnections and deep supervisions, was proposed for automatic extraction of\ncoronary arteries from ICAs. Transfer learning and a hybrid loss function were\nemployed in this novel coronary artery extraction framework. Results. A data\nset containing 616 ICAs obtained from 210 patients was used. In the technical\nevaluation, the U-Net 3+ achieved a Dice score of 0.8942 and a sensitivity of\n0.8735, which is higher than U-Net ++ (Dice score: 0.8814, the sensitivity of\n0.8331) and U-net (Dice score: 0.8799, the sensitivity of 0.8305). Conclusion.\nOur study demonstrates that the U-Net 3+ is superior to other segmentation\nframeworks for the automatic extraction of the coronary arteries from ICAs.\nThis result suggests great promise for clinical use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meng_Y/0/1/0/all/0/1\">Yinghui Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_Z/0/1/0/all/0/1\">Zhenglong Du</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_M/0/1/0/all/0/1\">Minghao Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pienta_D/0/1/0/all/0/1\">Drew Pienta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zhihui Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Weihua Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to train accurate BNNs for embedded systems?. (arXiv:2206.12322v1 [cs.LG])","link":"http://arxiv.org/abs/2206.12322","description":"<p>A key enabler of deploying convolutional neural networks on\nresource-constrained embedded systems is the binary neural network (BNN). BNNs\nsave on memory and simplify computation by binarizing both features and\nweights. Unfortunately, binarization is inevitably accompanied by a severe\ndecrease in accuracy. To reduce the accuracy gap between binary and\nfull-precision networks, many repair methods have been proposed in the recent\npast, which we have classified and put into a single overview in this chapter.\nThe repair methods are divided into two main branches, training techniques and\nnetwork topology changes, which can further be split into smaller categories.\nThe latter category introduces additional cost (energy consumption or\nadditional area) for an embedded system, while the former does not. From our\noverview, we observe that progress has been made in reducing the accuracy gap,\nbut BNN papers are not aligned on what repair methods should be used to get\nhighly accurate BNNs. Therefore, this chapter contains an empirical review that\nevaluates the benefits of many repair methods in isolation over the\nResNet-20\\&amp;CIFAR10 and ResNet-18\\&amp;CIFAR100 benchmarks. We found three repair\ncategories most beneficial: feature binarizer, feature normalization, and\ndouble residual. Based on this review we discuss future directions and research\nopportunities. We sketch the benefit and costs associated with BNNs on embedded\nsystems because it remains to be seen whether BNNs will be able to close the\naccuracy gap while staying highly energy-efficient on resource-constrained\nembedded systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Putter_F/0/1/0/all/0/1\">Floran de Putter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corporaal_H/0/1/0/all/0/1\">Henk Corporaal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation-free PVC for Cardiac SPECT using a Densely-connected Multi-dimensional Dynamic Network. (arXiv:2206.12344v1 [eess.IV])","link":"http://arxiv.org/abs/2206.12344","description":"<p>In nuclear imaging, limited resolution causes partial volume effects (PVEs)\nthat affect image sharpness and quantitative accuracy. Partial volume\ncorrection (PVC) methods incorporating high-resolution anatomical information\nfrom CT or MRI have been demonstrated to be effective. However, such\nanatomical-guided methods typically require tedious image registration and\nsegmentation steps. Accurately segmented organ templates are also hard to\nobtain, particularly in cardiac SPECT imaging, due to the lack of hybrid\nSPECT/CT scanners with high-end CT and associated motion artifacts. Slight\nmis-registration/mis-segmentation would result in severe degradation in image\nquality after PVC. In this work, we develop a deep-learning-based method for\nfast cardiac SPECT PVC without anatomical information and associated organ\nsegmentation. The proposed network involves a densely-connected\nmulti-dimensional dynamic mechanism, allowing the convolutional kernels to be\nadapted based on the input images, even after the network is fully trained.\nIntramyocardial blood volume (IMBV) is introduced as an additional\nclinical-relevant loss function for network optimization. The proposed network\ndemonstrated promising performance on 28 canine studies acquired on a GE\nDiscovery NM/CT 570c dedicated cardiac SPECT scanner with a 64-slice CT using\nTechnetium-99m-labeled red blood cells. This work showed that the proposed\nnetwork with densely-connected dynamic mechanism produced superior results\ncompared with the same network without such mechanism. Results also showed that\nthe proposed network without anatomical information could produce images with\nstatistically comparable IMBV measurements to the images generated by\nanatomical-guided PVC methods, which could be helpful in clinical translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xie_H/0/1/0/all/0/1\">Huidong Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_L/0/1/0/all/0/1\">Luyao Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Greco_K/0/1/0/all/0/1\">Kathleen Greco</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiongchao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_B/0/1/0/all/0/1\">Bo Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feher_A/0/1/0/all/0/1\">Attila Feher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stendahl_J/0/1/0/all/0/1\">John C. Stendahl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boutagy_N/0/1/0/all/0/1\">Nabil Boutagy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kyriakides_T/0/1/0/all/0/1\">Tassos C. Kyriakides</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sinusas_A/0/1/0/all/0/1\">Albert J. Sinusas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Megapixel Image Generation with Step-Unrolled Denoising Autoencoders. (arXiv:2206.12351v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12351","description":"<p>An ongoing trend in generative modelling research has been to push sample\nresolutions higher whilst simultaneously reducing computational requirements\nfor training and sampling. We aim to push this trend further via the\ncombination of techniques - each component representing the current pinnacle of\nefficiency in their respective areas. These include vector-quantized GAN\n(VQ-GAN), a vector-quantization (VQ) model capable of high levels of lossy -\nbut perceptually insignificant - compression; hourglass transformers, a highly\nscaleable self-attention model; and step-unrolled denoising autoencoders\n(SUNDAE), a non-autoregressive (NAR) text generative model. Unexpectedly, our\nmethod highlights weaknesses in the original formulation of hourglass\ntransformers when applied to multidimensional data. In light of this, we\npropose modifications to the resampling mechanism, applicable in any task\napplying hierarchical transformers to multidimensional data. Additionally, we\ndemonstrate the scalability of SUNDAE to long sequence lengths - four times\nlonger than prior work. Our proposed framework scales to high-resolutions\n($1024 \\times 1024$) and trains quickly (2-4 days). Crucially, the trained\nmodel produces diverse and realistic megapixel samples in approximately 2\nseconds on a consumer-grade GPU (GTX 1080Ti). In general, the framework is\nflexible: supporting an arbitrary number of sampling steps, sample-wise\nself-stopping, self-correction capabilities, conditional generation, and a NAR\nformulation that allows for arbitrary inpainting masks. We obtain FID scores of\n10.56 on FFHQ256 - close to the original VQ-GAN in less than half the sampling\nsteps - and 21.85 on FFHQ1024 in only 100 sampling steps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McKinney_A/0/1/0/all/0/1\">Alex F. McKinney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willcocks_C/0/1/0/all/0/1\">Chris G. Willcocks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HM3D-ABO: A Photo-realistic Dataset for Object-centric Multi-view 3D Reconstruction. (arXiv:2206.12356v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12356","description":"<p>Reconstructing 3D objects is an important computer vision task that has wide\napplication in AR/VR. Deep learning algorithm developed for this task usually\nrelies on an unrealistic synthetic dataset, such as ShapeNet and Things3D. On\nthe other hand, existing real-captured object-centric datasets usually do not\nhave enough annotation to enable supervised training or reliable evaluation. In\nthis technical report, we present a photo-realistic object-centric dataset\nHM3D-ABO. It is constructed by composing realistic indoor scene and realistic\nobject. For each configuration, we provide multi-view RGB observations, a\nwater-tight mesh model for the object, ground truth depth map and object mask.\nThe proposed dataset could also be useful for tasks such as camera pose\nestimation and novel-view synthesis. The dataset generation code is released at\nhttps://github.com/zhenpeiyang/HM3D-ABO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenpei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zaiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qixing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Distillation with Mixed Sample Augmentation. (arXiv:2206.12370v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12370","description":"<p>Mixed Sample Regularization (MSR), such as MixUp or CutMix, is a powerful\ndata augmentation strategy to generalize convolutional neural networks.\nPrevious empirical analysis has illustrated an orthogonal performance gain\nbetween MSR and the conventional offline Knowledge Distillation (KD). To be\nmore specific, student networks can be enhanced with the involvement of MSR in\nthe training stage of the sequential distillation. Yet, the interplay between\nMSR and online knowledge distillation, a stronger distillation paradigm, where\nan ensemble of peer students learn mutually from each other, remains\nunexplored. To bridge the gap, we make the first attempt at incorporating\nCutMix into online distillation, where we empirically observe a significant\nimprovement. Encouraged by this fact, we propose an even stronger MSR\nspecifically for online distillation, named as Cut^nMix. Furthermore, a novel\nonline distillation framework is designed upon Cut^nMix, to enhance the\ndistillation with feature level mutual learning and a self-ensemble teacher.\nComprehensive evaluations on CIFAR10 and CIFAR100 with six network\narchitectures show that our approach can consistently outperform\nstate-of-the-art distillation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yiqing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuzhe Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QReg: On Regularization Effects of Quantization. (arXiv:2206.12372v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12372","description":"<p>In this paper we study the effects of quantization in DNN training. We\nhypothesize that weight quantization is a form of regularization and the amount\nof regularization is correlated with the quantization level (precision). We\nconfirm our hypothesis by providing analytical study and empirical results. By\nmodeling weight quantization as a form of additive noise to weights, we explore\nhow this noise propagates through the network at training time. We then show\nthat the magnitude of this noise is correlated with the level of quantization.\nTo confirm our analytical study, we performed an extensive list of experiments\nsummarized in this paper in which we show that the regularization effects of\nquantization can be seen in various vision tasks and models, over various\ndatasets. Based on our study, we propose that 8-bit quantization provides a\nreliable form of regularization in different vision tasks and models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+AskariHemmat_M/0/1/0/all/0/1\">MohammadHossein AskariHemmat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemmat_R/0/1/0/all/0/1\">Reyhane Askari Hemmat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_A/0/1/0/all/0/1\">Alex Hoffman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazarevich_I/0/1/0/all/0/1\">Ivan Lazarevich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saboori_E/0/1/0/all/0/1\">Ehsan Saboori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mastropietro_O/0/1/0/all/0/1\">Olivier Mastropietro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savaria_Y/0/1/0/all/0/1\">Yvon Savaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_J/0/1/0/all/0/1\">Jean-Pierre David</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defending Backdoor Attacks on Vision Transformer via Patch Processing. (arXiv:2206.12381v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12381","description":"<p>Vision Transformers (ViTs) have a radically different architecture with\nsignificantly less inductive bias than Convolutional Neural Networks. Along\nwith the improvement in performance, security and robustness of ViTs are also\nof great importance to study. In contrast to many recent works that exploit the\nrobustness of ViTs against adversarial examples, this paper investigates a\nrepresentative causative attack, i.e., backdoor. We first examine the\nvulnerability of ViTs against various backdoor attacks and find that ViTs are\nalso quite vulnerable to existing attacks. However, we observe that the\nclean-data accuracy and backdoor attack success rate of ViTs respond\ndistinctively to patch transformations before the positional encoding. Then,\nbased on this finding, we propose an effective method for ViTs to defend both\npatch-based and blending-based trigger backdoor attacks via patch processing.\nThe performances are evaluated on several benchmark datasets, including\nCIFAR10, GTSRB, and TinyImageNet, which show the proposed novel defense is very\nsuccessful in mitigating backdoor attacks for ViTs. To the best of our\nknowledge, this paper presents the first defensive strategy that utilizes a\nunique characteristic of ViTs against backdoor attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doan_K/0/1/0/all/0/1\">Khoa D. Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lao_Y/0/1/0/all/0/1\">Yingjie Lao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Peng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-Driven Stylization of Video Objects. (arXiv:2206.12396v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12396","description":"<p>We tackle the task of stylizing video objects in an intuitive and semantic\nmanner following a user-specified text prompt. This is a challenging task as\nthe resulting video must satisfy multiple properties: (1) it has to be\ntemporally consistent and avoid jittering or similar artifacts, (2) the\nresulting stylization must preserve both the global semantics of the object and\nits fine-grained details, and (3) it must adhere to the user-specified text\nprompt. To this end, our method stylizes an object in a video according to a\nglobal target text prompt that describes the global semantics and a local\ntarget text prompt that describes the local semantics. To modify the style of\nan object, we harness the representational power of CLIP to get a similarity\nscore between (1) the local target text and a set of local stylized views, and\n(2) a global target text and a set of stylized global views. We use a\npretrained atlas decomposition network to propagate the edits in a temporally\nconsistent manner. We demonstrate that our method can generate consistent style\nchanges in time for a variety of objects and videos, that adhere to the\nspecification of the target texts. We also show how varying the specificity of\nthe target texts, and augmenting the texts with a set of prefixes results in\nstylizations with different levels of detail. Full results are given on our\nproject webpage:\nhttps://sloeschcke.github.io/Text-Driven-Stylization-of-Video-Objects/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loeschcke_S/0/1/0/all/0/1\">Sebastian Loeschcke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benaim_S/0/1/0/all/0/1\">Sagie Benaim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZSON: Zero-Shot Object-Goal Navigation using Multimodal Goal Embeddings. (arXiv:2206.12403v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12403","description":"<p>We present a scalable approach for learning open-world object-goal navigation\n(ObjectNav) -- the task of asking a virtual robot (agent) to find any instance\nof an object in an unexplored environment (e.g., \"find a sink\"). Our approach\nis entirely zero-shot -- i.e., it does not require ObjectNav rewards or\ndemonstrations of any kind. Instead, we train on the image-goal navigation\n(ImageNav) task, in which agents find the location where a picture (i.e., goal\nimage) was captured. Specifically, we encode goal images into a multimodal,\nsemantic embedding space to enable training semantic-goal navigation\n(SemanticNav) agents at scale in unannotated 3D environments (e.g., HM3D).\nAfter training, SemanticNav agents can be instructed to find objects described\nin free-form natural language (e.g., \"sink\", \"bathroom sink\", etc.) by\nprojecting language goals into the same multimodal, semantic embedding space.\nAs a result, our approach enables open-world ObjectNav. We extensively evaluate\nour agents on three ObjectNav datasets (Gibson, HM3D, and MP3D) and observe\nabsolute improvements in success of 4.2% - 20.0% over existing zero-shot\nmethods. For reference, these gains are similar or better than the 5%\nimprovement in success between the Habitat 2020 and 2021 ObjectNav challenge\nwinners. In an open-world setting, we discover that our agents can generalize\nto compound instructions with a room explicitly mentioned (e.g., \"Find a\nkitchen sink\") and when the target room can be inferred (e.g., \"Find a sink and\na stove\").\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1\">Arjun Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_G/0/1/0/all/0/1\">Gunjan Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devnani_B/0/1/0/all/0/1\">Bhavika Devnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1\">Judy Hoffman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RARTS: An Efficient First-Order Relaxed Architecture Search Method. (arXiv:2008.03901v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2008.03901","description":"<p>Differentiable architecture search (DARTS) is an effective method for\ndata-driven neural network design based on solving a bilevel optimization\nproblem. Despite its success in many architecture search tasks, there are still\nsome concerns about the accuracy of first-order DARTS and the efficiency of the\nsecond-order DARTS. In this paper, we formulate a single level alternative and\na relaxed architecture search (RARTS) method that utilizes the whole dataset in\narchitecture learning via both data and network splitting, without involving\nmixed second derivatives of the corresponding loss functions like DARTS. In our\nformulation of network splitting, two networks with different but related\nweights cooperate in search of a shared architecture. The advantage of RARTS\nover DARTS is justified by a convergence theorem and an analytically solvable\nmodel. Moreover, RARTS outperforms DARTS and its variants in accuracy and\nsearch efficiency, as shown in adequate experimental results. For the task of\nsearching topological architecture, i.e., the edges and the operations, RARTS\nobtains a higher accuracy and 60\\% reduction of computational cost than\nsecond-order DARTS on CIFAR-10. RARTS continues to out-perform DARTS upon\ntransfer to ImageNet and is on par with recent variants of DARTS even though\nour innovation is purely on the training algorithm without modifying search\nspace. For the task of searching width, i.e., the number of channels in\nconvolutional layers, RARTS also outperforms the traditional network pruning\nbenchmarks. Further experiments on the public architecture search benchmark\nlike NATS-Bench also support the preeminence of RARTS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fanghui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yingyong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jack Xin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Exit Semantic Segmentation Networks. (arXiv:2106.03527v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03527","description":"<p>Semantic segmentation arises as the backbone of many vision systems, spanning\nfrom self-driving cars and robot navigation to augmented reality and\nteleconferencing. Frequently operating under stringent latency constraints\nwithin a limited resource envelope, optimising for efficient execution becomes\nimportant. At the same time, the heterogeneous capabilities of the target\nplatforms and diverse constraints of different applications require the design\nand training of multiple target-specific segmentation models, leading to\nexcessive maintenance costs. To this end, we propose a framework for converting\nstate-of-the-art segmentation CNNs to Multi-Exit Semantic Segmentation (MESS)\nnetworks: specially trained models that employ parametrised early exits along\ntheir depth to i) dynamically save computation during inference on easier\nsamples and ii) save training and maintenance cost by offering a post-training\ncustomisable speed-accuracy trade-off. Designing and training such networks\nnaively can hurt performance. Thus, we propose novel two-staged training scheme\nfor multi-exit networks. Furthermore, the parametrisation of MESS enables\nco-optimising the number, placement and architecture of the attached\nsegmentation heads along with the exit policy, upon deployment via exhaustive\nsearch in &lt;1GPUh. This allows MESS to rapidly adapt to the device capabilities\nand application requirements for each target use-case, offering a\ntrain-once-deploy-everywhere solution. MESS variants achieve latency gains of\nup to 2.83x with the same accuracy, or 5.33 pp higher accuracy for the same\ncomputational budget, compared to the original backbone network. Lastly, MESS\ndelivers orders of magnitude faster architecture selection, compared to\nstate-of-the-art techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kouris_A/0/1/0/all/0/1\">Alexandros Kouris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1\">Stylianos I. Venieris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskaridis_S/0/1/0/all/0/1\">Stefanos Laskaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas D. Lane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hard hat wearing detection based on head keypoint localization. (arXiv:2106.10944v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10944","description":"<p>In recent years, a lot of attention is paid to deep learning methods in the\ncontext of vision-based construction site safety systems, especially regarding\npersonal protective equipment. However, despite all this attention, there is\nstill no reliable way to establish the relationship between workers and their\nhard hats. To answer this problem a combination of deep learning, object\ndetection and head keypoint localization, with simple rule-based reasoning is\nproposed in this article. In tests, this solution surpassed the previous\nmethods based on the relative bounding box position of different instances, as\nwell as direct detection of hard hat wearers and non-wearers. The results show\nthat the conjunction of novel deep learning methods with humanly-interpretable\nrule-based systems can result in a solution that is both reliable and can\nsuccessfully mimic manual, on-site supervision. This work is the next step in\nthe development of fully autonomous construction site safety systems and shows\nthat there is still room for improvement in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wojcik_B/0/1/0/all/0/1\">Bartosz W&#xf3;jcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarski_M/0/1/0/all/0/1\">Mateusz &#x17b;arski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ksiazek_K/0/1/0/all/0/1\">Kamil Ksi&#x105;&#x17c;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miszczak_J/0/1/0/all/0/1\">Jaros&#x142;aw Adam Miszczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skibniewski_M/0/1/0/all/0/1\">Miros&#x142;aw Jan Skibniewski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved-Mask R-CNN: Towards an Accurate Generic MSK MRI instance segmentation platform (Data from the Osteoarthritis Initiative). (arXiv:2107.12889v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.12889","description":"<p>Objective assessment of Magnetic Resonance Imaging (MRI) scans of\nosteoarthritis (OA) can address the limitation of the current OA assessment.\nSegmentation of bone, cartilage, and joint fluid is necessary for the OA\nobjective assessment. Most of the proposed segmentation methods are not\nperforming instance segmentation and suffer from class imbalance problems. This\nstudy deployed Mask R-CNN instance segmentation and improved it (improved-Mask\nR-CNN (iMaskRCNN)) to obtain a more accurate generalized segmentation for\nOA-associated tissues. Training and validation of the method were performed\nusing 500 MRI knees from the Osteoarthritis Initiative (OAI) dataset and 97 MRI\nscans of patients with symptomatic hip OA. Three modifications to Mask R-CNN\nyielded the iMaskRCNN: adding a 2nd ROIAligned block, adding an extra decoder\nlayer to the mask-header, and connecting them by a skip connection. The results\nwere assessed using Hausdorff distance, dice score, and coefficients of\nvariation (CoV). The iMaskRCNN led to improved bone and cartilage segmentation\ncompared to Mask RCNN as indicated with the increase in dice score from 95% to\n98% for the femur, 95% to 97% for tibia, 71% to 80% for femoral cartilage, and\n81% to 82% for tibial cartilage. For the effusion detection, dice improved with\niMaskRCNN 72% versus MaskRCNN 71%. The CoV values for effusion detection\nbetween Reader1 and Mask R-CNN (0.33), Reader1 and iMaskRCNN (0.34), Reader2\nand Mask R-CNN (0.22), Reader2 and iMaskRCNN (0.29) are close to CoV between\ntwo readers (0.21), indicating a high agreement between the human readers and\nboth Mask R-CNN and iMaskRCNN. Mask R-CNN and iMaskRCNN can reliably and\nsimultaneously extract different scale articular tissues involved in OA,\nforming the foundation for automated assessment of OA. The iMaskRCNN results\nshow that the modification improved the network performance around the edges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Felfeliyan_B/0/1/0/all/0/1\">Banafshe Felfeliyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hareendranathan_A/0/1/0/all/0/1\">Abhilash Hareendranathan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuntze_G/0/1/0/all/0/1\">Gregor Kuntze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jaremko_J/0/1/0/all/0/1\">Jacob L. Jaremko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ronsky_J/0/1/0/all/0/1\">Janet L. Ronsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localized Shape Modelling with Global Coherence: An Inverse Spectral Approach. (arXiv:2108.02161v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02161","description":"<p>Many natural shapes have most of their characterizing features concentrated\nover a few regions in space. For example, humans and animals have distinctive\nhead shapes, while inorganic objects like chairs and airplanes are made of\nwell-localized functional parts with specific geometric features. Often, these\nfeatures are strongly correlated -- a modification of facial traits in a\nquadruped should induce changes to the body structure. However, in shape\nmodelling applications, these types of edits are among the hardest ones; they\nrequire high precision, but also a global awareness of the entire shape. Even\nin the deep learning era, obtaining manipulable representations that satisfy\nsuch requirements is an open problem posing significant constraints. In this\nwork, we address this problem by defining a data-driven model upon a family of\nlinear operators (variants of the mesh Laplacian), whose spectra capture global\nand local geometric properties of the shape at hand. Modifications to these\nspectra are translated to semantically valid deformations of the corresponding\nsurface. By explicitly decoupling the global from the local surface features,\nour pipeline allows to perform local edits while simultaneously maintaining a\nglobal stylistic coherence. We empirically demonstrate how our learning-based\nmodel generalizes to shape representations not seen at training time, and we\nsystematically analyze different choices of local operators over diverse shape\ncategories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pegoraro_M/0/1/0/all/0/1\">Marco Pegoraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melzi_S/0/1/0/all/0/1\">Simone Melzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castellani_U/0/1/0/all/0/1\">Umberto Castellani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marin_R/0/1/0/all/0/1\">Riccardo Marin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1\">Emanuele Rodol&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised domain adaptation for clinician pose estimation and instance segmentationin the operating room. (arXiv:2108.11801v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11801","description":"<p>The fine-grained localization of clinicians in the operating room (OR) is a\nkey component to design the new generation of OR support systems. Computer\nvision models for person pixel-based segmentation and body-keypoints detection\nare needed to better understand the clinical activities and the spatial layout\nof the OR. This is challenging, not only because OR images are very different\nfrom traditional vision datasets, but also because data and annotations are\nhard to collect and generate in the OR due to privacy concerns. To address\nthese concerns, we first study how joint person pose estimation and instance\nsegmentation can be performed on low resolutions images with downsampling\nfactors from 1x to 12x. Second, to address the domain shift and the lack of\nannotations, we propose a novel unsupervised domain adaptation method, called\nAdaptOR, to adapt a model from an in-the-wild labeled source domain to a\nstatistically different unlabeled target domain. We propose to exploit explicit\ngeometric constraints on the different augmentations of the unlabeled target\ndomain image to generate accurate pseudo labels and use these pseudo labels to\ntrain the model on high- and low-resolution OR images in a self-training\nframework. Furthermore, we propose disentangled feature normalization to handle\nthe statistically different source and target domain data. Extensive\nexperimental results with detailed ablation studies on the two OR datasets\nMVOR+ and TUM-OR-test show the effectiveness of our approach against strongly\nconstructed baselines, especially on the low-resolution privacy-preserving OR\nimages. Finally, we show the generality of our method as a semi-supervised\nlearning (SSL) method on the large-scale COCO dataset, where we achieve\ncomparable results with as few as 1% of labeled supervision against a model\ntrained with 100% labeled supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastav_V/0/1/0/all/0/1\">Vinkle Srivastav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangi_A/0/1/0/all/0/1\">Afshin Gangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1\">Nicolas Padoy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"F3: Fair and Federated Face Attribute Classification with Heterogeneous Data. (arXiv:2109.02351v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.02351","description":"<p>Fairness across different demographic groups is an essential criterion for\nface-related tasks, Face Attribute Classification (FAC) being a prominent\nexample. Apart from this trend, Federated Learning (FL) is increasingly gaining\ntraction as a scalable paradigm for distributed training. Existing FL\napproaches require data homogeneity to ensure fairness. However, this\nassumption is too restrictive in real-world settings. We propose F3, a novel FL\nframework for fair FAC under data heterogeneity. F3 adopts multiple heuristics\nto improve fairness across different demographic groups without requiring data\nhomogeneity assumption. We demonstrate the efficacy of F3 by reporting\nempirically observed fairness measures and accuracy guarantees on popular face\ndatasets. Our results suggest that F3 strikes a practical balance between\naccuracy and fairness for FAC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanaparthy_S/0/1/0/all/0/1\">Samhita Kanaparthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padala_M/0/1/0/all/0/1\">Manisha Padala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damle_S/0/1/0/all/0/1\">Sankarshan Damle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gujar_S/0/1/0/all/0/1\">Sujit Gujar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ABO: Dataset and Benchmarks for Real-World 3D Object Understanding. (arXiv:2110.06199v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06199","description":"<p>We introduce Amazon Berkeley Objects (ABO), a new large-scale dataset\ndesigned to help bridge the gap between real and virtual 3D worlds. ABO\ncontains product catalog images, metadata, and artist-created 3D models with\ncomplex geometries and physically-based materials that correspond to real,\nhousehold objects. We derive challenging benchmarks that exploit the unique\nproperties of ABO and measure the current limits of the state-of-the-art on\nthree open problems for real-world 3D object understanding: single-view 3D\nreconstruction, material estimation, and cross-domain multi-view object\nretrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Collins_J/0/1/0/all/0/1\">Jasmine Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1\">Shubham Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1\">Kenan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luthra_A/0/1/0/all/0/1\">Achleshwar Luthra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Leon Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gundogdu_E/0/1/0/all/0/1\">Erhan Gundogdu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vicente_T/0/1/0/all/0/1\">Tomas F. Yago Vicente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dideriksen_T/0/1/0/all/0/1\">Thomas Dideriksen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_H/0/1/0/all/0/1\">Himanshu Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guillaumin_M/0/1/0/all/0/1\">Matthieu Guillaumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vertebrae localization, segmentation and identification using a graph optimization and an anatomic consistency cycle. (arXiv:2110.12177v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.12177","description":"<p>Vertebrae localization, segmentation and identification in CT images is key\nto numerous clinical applications. While deep learning strategies have brought\nto this field significant improvements over recent years, transitional and\npathological vertebrae are still plaguing most existing approaches as a\nconsequence of their poor representation in training datasets. Alternatively,\nproposed non-learning based methods take benefit of prior knowledge to handle\nsuch particular cases. In this work we propose to combine both strategies. To\nthis purpose we introduce an iterative cycle in which individual vertebrae are\nrecursively localized, segmented and identified using deep-networks, while\nanatomic consistency is enforced using statistical priors. In this strategy,\nthe transitional vertebrae identification is handled by encoding their\nconfigurations in a graphical model that aggregates local deep-network\npredictions into an anatomically consistent final result. Our approach achieves\nstate-of-the-art results on the VerSe20 challenge benchmark, and outperforms\nall methods on transitional vertebrae as well as the generalization to the\nVerSe19 challenge benchmark. Furthermore, our method can detect and report\ninconsistent spine regions that do not satisfy the anatomic consistency priors.\nOur code and model are openly available for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1\">Di Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boyer_E/0/1/0/all/0/1\">Edmond Boyer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pujades_S/0/1/0/all/0/1\">Sergi Pujades</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lymphoma segmentation from 3D PET-CT images using a deep evidential network. (arXiv:2201.13078v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.13078","description":"<p>An automatic evidential segmentation method based on Dempster-Shafer theory\nand deep learning is proposed to segment lymphomas from three-dimensional\nPositron Emission Tomography (PET) and Computed Tomography (CT) images. The\narchitecture is composed of a deep feature-extraction module and an evidential\nlayer. The feature extraction module uses an encoder-decoder framework to\nextract semantic feature vectors from 3D inputs. The evidential layer then uses\nprototypes in the feature space to compute a belief function at each voxel\nquantifying the uncertainty about the presence or absence of a lymphoma at this\nlocation. Two evidential layers are compared, based on different ways of using\ndistances to prototypes for computing mass functions. The whole model is\ntrained end-to-end by minimizing the Dice loss function. The proposed\ncombination of deep feature extraction and evidential segmentation is shown to\noutperform the baseline UNet model as well as three other state-of-the-art\nmodels on a dataset of 173 patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Ling Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_S/0/1/0/all/0/1\">Su Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decazes_P/0/1/0/all/0/1\">Pierre Decazes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denoeux_T/0/1/0/all/0/1\">Thierry Denoeux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spelunking the Deep: Guaranteed Queries on General Neural Implicit Surfaces via Range Analysis. (arXiv:2202.02444v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02444","description":"<p>Neural implicit representations, which encode a surface as the level set of a\nneural network applied to spatial coordinates, have proven to be remarkably\neffective for optimizing, compressing, and generating 3D geometry. Although\nthese representations are easy to fit, it is not clear how to best evaluate\ngeometric queries on the shape, such as intersecting against a ray or finding a\nclosest point. The predominant approach is to encourage the network to have a\nsigned distance property. However, this property typically holds only\napproximately, leading to robustness issues, and holds only at the conclusion\nof training, inhibiting the use of queries in loss functions. Instead, this\nwork presents a new approach to perform queries directly on general neural\nimplicit functions for a wide range of existing architectures. Our key tool is\nthe application of range analysis to neural networks, using automatic\narithmetic rules to bound the output of a network over a region; we conduct a\nstudy of range analysis on neural networks, and identify variants of affine\narithmetic which are highly effective. We use the resulting bounds to develop\ngeometric queries including ray casting, intersection testing, constructing\nspatial hierarchies, fast mesh extraction, closest-point evaluation, evaluating\nbulk properties, and more. Our queries can be efficiently evaluated on GPUs,\nand offer concrete accuracy guarantees even on randomly-initialized networks,\nenabling their use in training objectives and beyond. We also show a\npreliminary application to inverse rendering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharp_N/0/1/0/all/0/1\">Nicholas Sharp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobson_A/0/1/0/all/0/1\">Alec Jacobson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rebalanced Siamese Contrastive Mining for Long-Tailed Recognition. (arXiv:2203.11506v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11506","description":"<p>Deep neural networks perform poorly on heavily class-imbalanced datasets.\nGiven the promising performance of contrastive learning, we propose Rebalanced\nSiamese Contrastive Mining (ResCom) to tackle imbalanced recognition. Based on\nthe mathematical analysis and simulation results, we claim that supervised\ncontrastive learning suffers a dual class-imbalance problem at both the\noriginal batch and Siamese batch levels, which is more serious than long-tailed\nclassification learning. In this paper, at the original batch level, we\nintroduce a class-balanced supervised contrastive loss to assign adaptive\nweights for different classes. At the Siamese batch level, we present a\nclass-balanced queue, which maintains the same number of keys for all classes.\nFurthermore, we note that the imbalanced contrastive loss gradient with respect\nto the contrastive logits can be decoupled into the positives and negatives,\nand easy positives and easy negatives will make the contrastive gradient\nvanish. We propose supervised hard positive and negative pairs mining to pick\nup informative pairs for contrastive computation and improve representation\nlearning. Finally, to approximately maximize the mutual information between the\ntwo views, we propose Siamese Balanced Softmax and joint it with the\ncontrastive loss for one-stage training. Extensive experiments demonstrate that\nResCom outperforms the previous methods by large margins on multiple\nlong-tailed recognition benchmarks. Our code and models are made publicly\navailable at: https://github.com/dvlab-research/ResCom.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhisheng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiequan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_E/0/1/0/all/0/1\">Eric Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning for laboratory earthquake prediction and autoregressive forecasting of fault zone stress. (arXiv:2203.13313v2 [physics.geo-ph] UPDATED)","link":"http://arxiv.org/abs/2203.13313","description":"<p>Earthquake forecasting and prediction have long and in some cases sordid\nhistories but recent work has rekindled interest based on advances in early\nwarning, hazard assessment for induced seismicity and successful prediction of\nlaboratory earthquakes. In the lab, frictional stick-slip events provide an\nanalog for earthquakes and the seismic cycle. Labquakes are ideal targets for\nmachine learning (ML) because they can be produced in long sequences under\ncontrolled conditions. Recent works show that ML can predict several aspects of\nlabquakes using fault zone acoustic emissions. Here, we generalize these\nresults and explore deep learning (DL) methods for labquake prediction and\nautoregressive (AR) forecasting. DL improves existing ML methods of labquake\nprediction. AR methods allow forecasting at future horizons via iterative\npredictions. We demonstrate that DL models based on Long-Short Term Memory\n(LSTM) and Convolution Neural Networks predict labquakes under several\nconditions, and that fault zone stress can be predicted with fidelity,\nconfirming that acoustic energy is a fingerprint of fault zone stress. We\npredict also time to start of failure (TTsF) and time to the end of Failure\n(TTeF) for labquakes. Interestingly, TTeF is successfully predicted in all\nseismic cycles, while the TTsF prediction varies with the amount of preseismic\nfault creep. We report AR methods to forecast the evolution of fault stress\nusing three sequence modeling frameworks: LSTM, Temporal Convolution Network\nand Transformer Network. AR forecasting is distinct from existing predictive\nmodels, which predict only a target variable at a specific time. The results\nfor forecasting beyond a single seismic cycle are limited but encouraging. Our\nML/DL models outperform the state-of-the-art and our autoregressive model\nrepresents a novel framework that could enhance current methods of earthquake\nforecasting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Laurenti_L/0/1/0/all/0/1\">Laura Laurenti</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tinti_E/0/1/0/all/0/1\">Elisa Tinti</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Galasso_F/0/1/0/all/0/1\">Fabio Galasso</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Franco_L/0/1/0/all/0/1\">Luca Franco</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Marone_C/0/1/0/all/0/1\">Chris Marone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Intra- and Inter-Video Relation for Surgical Semantic Scene Segmentation. (arXiv:2203.15251v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15251","description":"<p>Automatic surgical scene segmentation is fundamental for facilitating\ncognitive intelligence in the modern operating theatre. Previous works rely on\nconventional aggregation modules (e.g., dilated convolution, convolutional\nLSTM), which only make use of the local context. In this paper, we propose a\nnovel framework STswinCL that explores the complementary intra- and inter-video\nrelations to boost segmentation performance, by progressively capturing the\nglobal context. We firstly develop a hierarchy Transformer to capture\nintra-video relation that includes richer spatial and temporal cues from\nneighbor pixels and previous frames. A joint space-time window shift scheme is\nproposed to efficiently aggregate these two cues into each pixel embedding.\nThen, we explore inter-video relation via pixel-to-pixel contrastive learning,\nwhich well structures the global embedding space. A multi-source contrast\ntraining objective is developed to group the pixel embeddings across videos\nwith the ground-truth guidance, which is crucial for learning the global\nproperty of the whole data. We extensively validate our approach on two public\nsurgical video benchmarks, including EndoVis18 Challenge and CaDIS dataset.\nExperimental results demonstrate the promising performance of our method, which\nconsistently exceeds previous state-of-the-art approaches. Code is available at\nhttps://github.com/YuemingJin/STswinCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zixu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation for Cardiac Segmentation: Towards Structure Mutual Information Maximization. (arXiv:2204.09334v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.09334","description":"<p>Unsupervised domain adaptation approaches have recently succeeded in various\nmedical image segmentation tasks. The reported works often tackle the domain\nshift problem by aligning the domain-invariant features and minimizing the\ndomain-specific discrepancies. That strategy works well when the difference\nbetween a specific domain and between different domains is slight. However, the\ngeneralization ability of these models on diverse imaging modalities remains a\nsignificant challenge. This paper introduces UDA-VAE++, an unsupervised domain\nadaptation framework for cardiac segmentation with a compact loss function\nlower bound. To estimate this new lower bound, we develop a novel Structure\nMutual Information Estimation (SMIE) block with a global estimator, a local\nestimator, and a prior information matching estimator to maximize the mutual\ninformation between the reconstruction and segmentation tasks. Specifically, we\ndesign a novel sequential reparameterization scheme that enables information\nflow and variance correction from the low-resolution latent space to the\nhigh-resolution latent space. Comprehensive experiments on benchmark cardiac\nsegmentation datasets demonstrate that our model outperforms previous\nstate-of-the-art qualitatively and quantitatively. The code is available at\nhttps://github.com/LOUEY233/Toward-Mutual-Information}{https://github.com/LOUEY233/Toward-Mutual-Information\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_C/0/1/0/all/0/1\">Changjie Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_S/0/1/0/all/0/1\">Shen Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_G/0/1/0/all/0/1\">Gaurav Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Quality of Pose-varied Face Restoration with Local Weak Feature Sensing and GAN Prior. (arXiv:2205.14377v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14377","description":"<p>Facial semantic guidance (including facial landmarks, facial heatmaps, and\nfacial parsing maps) and facial generative adversarial networks (GAN) prior\nhave been widely used in blind face restoration (BFR) in recent years. Although\nexisting BFR methods have achieved good performance in ordinary cases, these\nsolutions have limited resilience when applied to face images with serious\ndegradation and pose-varied (e.g., looking right, looking left, laughing, etc.)\nin real-world scenarios. In this work, we propose a well-designed blind face\nrestoration network with generative facial prior. The proposed network is\nmainly comprised of an asymmetric codec and a StyleGAN2 prior network. In the\nasymmetric codec, we adopt a mixed multi-path residual block (MMRB) to\ngradually extract weak texture features of input images, which can better\npreserve the original facial features and avoid excessive fantasy. The MMRB can\nalso be plug-and-play in other networks. Furthermore, thanks to the affluent\nand diverse facial priors of the StyleGAN2 model, we adopt a fine-tuned\napproach to flexibly restore natural and realistic facial details. Besides, a\nnovel self-supervised training strategy is specially designed for face\nrestoration tasks to fit the distribution closer to the target and maintain\ntraining stability. Extensive experiments on both synthetic and real-world\ndatasets demonstrate that our model achieves superior performance to the prior\nart for face restoration and face super-resolution tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Kai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Renhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Gang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1\">Bin Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RankSim: Ranking Similarity Regularization for Deep Imbalanced Regression. (arXiv:2205.15236v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.15236","description":"<p>Data imbalance, in which a plurality of the data samples come from a small\nproportion of labels, poses a challenge in training deep neural networks.\nUnlike classification, in regression the labels are continuous, potentially\nboundless, and form a natural ordering. These distinct features of regression\ncall for new techniques that leverage the additional information encoded in\nlabel-space relationships. This paper presents the RankSim (ranking similarity)\nregularizer for deep imbalanced regression, which encodes an inductive bias\nthat samples that are closer in label space should also be closer in feature\nspace. In contrast to recent distribution smoothing based approaches, RankSim\ncaptures both nearby and distant relationships: for a given data sample,\nRankSim encourages the sorted list of its neighbors in label space to match the\nsorted list of its neighbors in feature space. RankSim is complementary to\nconventional imbalanced learning techniques, including re-weighting, two-stage\ntraining, and distribution smoothing, and lifts the state-of-the-art\nperformance on three imbalanced regression benchmarks: IMDB-WIKI-DIR,\nAgeDB-DIR, and STS-B-DIR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_G/0/1/0/all/0/1\">Greg Mori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_F/0/1/0/all/0/1\">Frederick Tung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Definition Map Generation Technologies For Autonomous Driving. (arXiv:2206.05400v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2206.05400","description":"<p>Autonomous driving has been among the most popular and challenging topics in\nthe past few years. On the road to achieving full autonomy, researchers have\nutilized various sensors, such as LiDAR, camera, Inertial Measurement Unit\n(IMU), and GPS, and developed intelligent algorithms for autonomous driving\napplications such as object detection, object segmentation, obstacle avoidance,\nand path planning. High-definition (HD) maps have drawn lots of attention in\nrecent years. Because of the high precision and informative level of HD maps in\nlocalization, it has immediately become one of the critical components of\nautonomous driving. From big organizations like Baidu Apollo, NVIDIA, and\nTomTom to individual researchers, researchers have created HD maps for\ndifferent scenes and purposes for autonomous driving. It is necessary to review\nthe state-of-the-art methods for HD map generation. This paper reviews recent\nHD map generation technologies that leverage both 2D and 3D map generation.\nThis review introduces the concept of HD maps and their usefulness in\nautonomous driving and gives a detailed overview of HD map generation\ntechniques. We will also discuss the limitations of the current HD map\ngeneration technologies to motivate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1\">Zhibin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_S/0/1/0/all/0/1\">Sabir Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1\">Haoxiang Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xianke Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Sampling: Exploring Out-of-Distribution data for Re-balancing Long-tailed datasets. (arXiv:2206.08802v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.08802","description":"<p>Deep neural networks usually perform poorly when the training dataset suffers\nfrom extreme class imbalance. Recent studies found that directly training with\nout-of-distribution data (i.e., open-set samples) in a semi-supervised manner\nwould harm the generalization performance. In this work, we theoretically show\nthat out-of-distribution data can still be leveraged to augment the minority\nclasses from a Bayesian perspective. Based on this motivation, we propose a\nnovel method called Open-sampling, which utilizes open-set noisy labels to\nre-balance the class priors of the training dataset. For each open-set\ninstance, the label is sampled from our pre-defined distribution that is\ncomplementary to the distribution of original class priors. We empirically show\nthat Open-sampling not only re-balances the class priors but also encourages\nthe neural network to learn separable representations. Extensive experiments\ndemonstrate that our proposed method significantly outperforms existing data\nre-balancing methods and can boost the performance of existing state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hongxin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1\">Lue Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Renchunzi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1\">Bo An</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voxel-MAE: Masked Autoencoders for Pre-training Large-scale Point Clouds. (arXiv:2206.09900v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.09900","description":"<p>Mask-based pre-training has achieved great success for self-supervised\nlearning in image, video, and language, without manually annotated supervision.\nHowever, it has not yet been studied about large-scale point clouds with\nredundant spatial information in autonomous driving. As the number of\nlarge-scale point clouds is huge, it is impossible to reconstruct the input\npoint clouds. In this paper, we propose a mask voxel classification network for\nlarge-scale point clouds pre-training. Our key idea is to divide the point\nclouds into voxel representations and classify whether the voxel contains point\nclouds. This simple strategy makes the network to be voxel-aware of the object\nshape, thus improving the performance of the downstream tasks, such as 3D\nobject detection. Our Voxel-MAE with even a 90% masking ratio can still learn\nrepresentative features for the high spatial redundancy of large-scale point\nclouds. We also validate the effectiveness of Voxel-MAE in unsupervised domain\nadaptative tasks, which proves the generalization ability of Voxel-MAE. Our\nVoxel-MAE proves that it is feasible to pre-train large-scale point clouds\nwithout data annotations to enhance the perception ability of the autonomous\nvehicle. Extensive experiments show great effectiveness of our pre-trained\nmodel with 3D object detectors (SECOND, CenterPoint, and PV-RCNN) on three\npopular datasets (KITTI, Waymo, and nuScenes). Codes are publicly available at\nhttps://github.com/chaytonmin/Voxel-MAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_C/0/1/0/all/0/1\">Chen Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dawei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Liang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yiming Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bin Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Optimizing OCR for Accessibility. (arXiv:2206.10254v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.10254","description":"<p>Visual cues such as structure, emphasis, and icons play an important role in\nefficient information foraging by sighted individuals and make for a\npleasurable reading experience. Blind, low-vision and other print-disabled\nindividuals miss out on these cues since current OCR and text-to-speech\nsoftware ignore them, resulting in a tedious reading experience. We identify\nfour semantic goals for an enjoyable listening experience, and identify\nsyntactic visual cues that help make progress towards these goals. Empirically,\nwe find that preserving even one or two visual cues in aural form significantly\nenhances the experience for listening to print content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mowar_P/0/1/0/all/0/1\">Peya Mowar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganu_T/0/1/0/all/0/1\">Tanuja Ganu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_S/0/1/0/all/0/1\">Saikat Guha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Vocabulary Object Detection with Proposal Mining and Prediction Equalization. (arXiv:2206.11134v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.11134","description":"<p>Open-vocabulary object detection (OVD) aims to scale up vocabulary size to\ndetect objects of novel categories beyond the training vocabulary. Recent work\nresorts to the rich knowledge in pre-trained vision-language models. However,\nexisting methods are ineffective in proposal-level vision-language alignment.\nMeanwhile, the models usually suffer from confidence bias toward base\ncategories and perform worse on novel ones. To overcome the challenges, we\npresent MEDet, a novel and effective OVD framework with proposal mining and\nprediction equalization. First, we design an online proposal mining to refine\nthe inherited vision-semantic knowledge from coarse to fine, allowing for\nproposal-level detection-oriented feature alignment. Second, based on causal\ninference theory, we introduce a class-wise backdoor adjustment to reinforce\nthe predictions on novel categories to improve the overall OVD performance.\nExtensive experiments on COCO and LVIS benchmarks verify the superiority of\nMEDet over the competing approaches in detecting objects of novel categories,\ne.g., 32.6% AP50 on COCO and 22.4% mask mAP on LVIS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peixian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1\">Kekai Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengdan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LidarMultiNet: Unifying LiDAR Semantic Segmentation, 3D Object Detection, and Panoptic Segmentation in a Single Multi-task Network. (arXiv:2206.11428v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.11428","description":"<p>This technical report presents the 1st place winning solution for the Waymo\nOpen Dataset 3D semantic segmentation challenge 2022. Our network, termed\nLidarMultiNet, unifies the major LiDAR perception tasks such as 3D semantic\nsegmentation, object detection, and panoptic segmentation in a single\nframework. At the core of LidarMultiNet is a strong 3D voxel-based\nencoder-decoder network with a novel Global Context Pooling (GCP) module\nextracting global contextual features from a LiDAR frame to complement its\nlocal features. An optional second stage is proposed to refine the first-stage\nsegmentation or generate accurate panoptic segmentation results. Our solution\nachieves a mIoU of 71.13 and is the best for most of the 22 classes on the\nWaymo 3D semantic segmentation test set, outperforming all the other 3D\nsemantic segmentation methods on the official leaderboard. We demonstrate for\nthe first time that major LiDAR perception tasks can be unified in a single\nstrong network that can be trained end-to-end.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1\">Dongqiangzi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weijia Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zixiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yufei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Panqu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foroosh_H/0/1/0/all/0/1\">Hassan Foroosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entropy-driven Sampling and Training Scheme for Conditional Diffusion Generation. (arXiv:2206.11474v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.11474","description":"<p>Denoising Diffusion Probabilistic Model (DDPM) is able to make flexible\nconditional image generation from prior noise to real data, by introducing an\nindependent noise-aware classifier to provide conditional gradient guidance at\neach time step of denoising process. However, due to the ability of classifier\nto easily discriminate an incompletely generated image only with high-level\nstructure, the gradient, which is a kind of class information guidance, tends\nto vanish early, leading to the collapse from conditional generation process\ninto the unconditional process. To address this problem, we propose two simple\nbut effective approaches from two perspectives. For sampling procedure, we\nintroduce the entropy of predicted distribution as the measure of guidance\nvanishing level and propose an entropy-aware scaling method to adaptively\nrecover the conditional semantic guidance. For training stage, we propose the\nentropy-aware optimization objectives to alleviate the overconfident prediction\nfor noisy data.On ImageNet1000 256x256, with our proposed sampling scheme and\ntrained classifier, the pretrained conditional and unconditional DDPM model can\nachieve 10.89% (4.59 to 4.09) and 43.5% (12 to 6.78) FID improvement\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guangcong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Taiping Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shoudong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Structure from Motion for UAV Images via Weighted Connected Dominating Set. (arXiv:2206.11499v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.11499","description":"<p>Incremental Structure from Motion (ISfM) has been widely used for UAV image\norientation. Its efficiency, however, decreases dramatically due to the\nsequential constraint. Although the divide-and-conquer strategy has been\nutilized for efficiency improvement, cluster merging becomes difficult or\ndepends on seriously designed overlap structures. This paper proposes an\nalgorithm to extract the global model for cluster merging and designs a\nparallel SfM solution to achieve efficient and accurate UAV image orientation.\nFirst, based on vocabulary tree retrieval, match pairs are selected to\nconstruct an undirected weighted match graph, whose edge weights are calculated\nby considering both the number and distribution of feature matches. Second, an\nalgorithm, termed weighted connected dominating set (WCDS), is designed to\nachieve the simplification of the match graph and build the global model, which\nincorporates the edge weight in the graph node selection and enables the\nsuccessful reconstruction of the global model. Third, the match graph is\nsimultaneously divided into compact and non-overlapped clusters. After the\nparallel reconstruction, cluster merging is conducted with the aid of common 3D\npoints between the global and cluster models. Finally, by using three UAV\ndatasets that are captured by classical oblique and recent optimized views\nphotogrammetry, the validation of the proposed solution is verified through\ncomprehensive analysis and comparison. The experimental results demonstrate\nthat the proposed parallel SfM can achieve 17.4 times efficiency improvement\nand comparative orientation accuracy. In absolute BA, the geo-referencing\naccuracy is approximately 2.0 and 3.0 times the GSD (Ground Sampling Distance)\nvalue in the horizontal and vertical directions, respectively. For parallel\nSfM, the proposed solution is a more reliable alternative.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">San Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wanshou Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Short-range forecasts of global precipitation using deep learning-augmented numerical weather prediction. (arXiv:2206.11669v2 [physics.ao-ph] UPDATED)","link":"http://arxiv.org/abs/2206.11669","description":"<p>Precipitation governs Earth's hydroclimate, and its daily spatiotemporal\nfluctuations have major socioeconomic effects. Advances in Numerical weather\nprediction (NWP) have been measured by the improvement of forecasts for various\nphysical fields such as temperature and pressure; however, large biases exist\nin precipitation prediction. We augment the output of the well-known NWP model\nCFSv2 with deep learning to create a hybrid model that improves short-range\nglobal precipitation at 1-, 2-, and 3-day lead times. To hybridise, we address\nthe sphericity of the global data by using modified DLWP-CS architecture which\ntransforms all the fields to cubed-sphere projection. Dynamical model\nprecipitation and surface temperature outputs are fed into a modified DLWP-CS\n(UNET) to forecast ground truth precipitation. While CFSv2's average bias is +5\nto +7 mm/day over land, the multivariate deep learning model decreases it to\nwithin -1 to +1 mm/day. Hurricane Katrina in 2005, Hurricane Ivan in 2004,\nChina floods in 2010, India floods in 2005, and Myanmar storm Nargis in 2008\nare used to confirm the substantial enhancement in the skill for the hybrid\ndynamical-deep learning model. CFSv2 typically shows a moderate to large bias\nin the spatial pattern and overestimates the precipitation at short-range time\nscales. The proposed deep learning augmented NWP model can address these biases\nand vastly improve the spatial pattern and magnitude of predicted\nprecipitation. Deep learning enhanced CFSv2 reduces mean bias by 8x over\nimportant land regions for 1 day lead compared to CFSv2. The spatio-temporal\ndeep learning system opens pathways to further the precision and accuracy in\nglobal short-range precipitation forecasts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Singh_M/0/1/0/all/0/1\">Manmeet Singh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+B_V/0/1/0/all/0/1\">Vaisakh S B</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Acharya_N/0/1/0/all/0/1\">Nachiketa Acharya</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rao_S/0/1/0/all/0/1\">Suryachandra A Rao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kumar_B/0/1/0/all/0/1\">Bipin Kumar</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_Z/0/1/0/all/0/1\">Zong-Liang Yang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Niyogi_D/0/1/0/all/0/1\">Dev Niyogi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Clinically Assisted Colorectal Polyp Recognition via Structured Cross-modal Representation Consistency. (arXiv:2206.11826v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.11826","description":"<p>The colorectal polyps classification is a critical clinical examination. To\nimprove the classification accuracy, most computer-aided diagnosis algorithms\nrecognize colorectal polyps by adopting Narrow-Band Imaging (NBI). However, the\nNBI usually suffers from missing utilization in real clinic scenarios since the\nacquisition of this specific image requires manual switching of the light mode\nwhen polyps have been detected by using White-Light (WL) images. To avoid the\nabove situation, we propose a novel method to directly achieve accurate\nwhite-light colonoscopy image classification by conducting structured\ncross-modal representation consistency. In practice, a pair of multi-modal\nimages, i.e. NBI and WL, are fed into a shared Transformer to extract\nhierarchical feature representations. Then a novel designed Spatial Attention\nModule (SAM) is adopted to calculate the similarities between the class token\nand patch tokens %from multi-levels for a specific modality image. By aligning\nthe class tokens and spatial attention maps of paired NBI and WL images at\ndifferent levels, the Transformer achieves the ability to keep both global and\nlocal representation consistency for the above two modalities. Extensive\nexperimental results illustrate the proposed method outperforms the recent\nstudies with a margin, realizing multi-modal prediction with a single\nTransformer while greatly improving the classification accuracy when only with\nWL images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Weijie Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Ye Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruimao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yiwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Li Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/"}}]}]}