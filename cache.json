{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-22T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Optimize_Prime@DravidianLangTech-ACL2022: Abusive Comment Detection in Tamil. (arXiv:2204.09675v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09675","description":"<p>This paper tries to address the problem of abusive comment detection in\nlow-resource indic languages. Abusive comments are statements that are\noffensive to a person or a group of people. These comments are targeted toward\nindividuals belonging to specific ethnicities, genders, caste, race, sexuality,\netc. Abusive Comment Detection is a significant problem, especially with the\nrecent rise in social media users. This paper presents the approach used by our\nteam - Optimize_Prime, in the ACL 2022 shared task \"Abusive Comment Detection\nin Tamil.\" This task detects and classifies YouTube comments in Tamil and\nTamil- English Codemixed format into multiple categories. We have used three\nmethods to optimize our results: Ensemble models, Recurrent Neural Networks,\nand Transformers. In the Tamil data, MuRIL and XLM-RoBERTA were our best\nperforming models with a macro-averaged f1 score of 0.43. Furthermore, for the\nCode-mixed data, MuRIL and M-BERT provided sub-lime results, with a\nmacro-averaged f1 score of 0.45.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patankar_S/0/1/0/all/0/1\">Shantanu Patankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_O/0/1/0/all/0/1\">Omkar Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litake_O/0/1/0/all/0/1\">Onkar Litake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandke_A/0/1/0/all/0/1\">Aditya Mandke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadam_D/0/1/0/all/0/1\">Dipali Kadam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"yosm: A new yoruba sentiment corpus for movie reviews. (arXiv:2204.09711v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09711","description":"<p>A movie that is thoroughly enjoyed and recommended by an individual might be\nhated by another. One characteristic of humans is the ability to have feelings\nwhich could be positive or negative. To automatically classify and study human\nfeelings, an aspect of natural language processing, sentiment analysis and\nopinion mining were designed to understand human feelings regarding several\nissues which could affect a product, a social media platforms, government, or\nsocietal discussions or even movies. Several works on sentiment analysis have\nbeen done on high resource languages while low resources languages like Yoruba\nhave been sidelined. Due to the scarcity of datasets and linguistic\narchitectures that will suit low resource languages, African languages \"low\nresource languages\" have been ignored and not fully explored. For this reason,\nour attention is placed on Yoruba to explore sentiment analysis on reviews of\nNigerian movies. The data comprised 1500 movie reviews that were sourced from\nIMDB, Rotten Tomatoes, Letterboxd, Cinemapointer and Nollyrated. We develop\nsentiment classification models using the state-of-the-art pre-trained language\nmodels like mBERT and AfriBERTa to classify the movie reviews.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shode_I/0/1/0/all/0/1\">Iyanuoluwa Shode</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_A/0/1/0/all/0/1\">Anna Feldman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Pre-Trained Transformers for Biologically Inspired Design. (arXiv:2204.09714v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09714","description":"<p>Biological systems in nature have evolved for millions of years to adapt and\nsurvive the environment. Many features they developed can be inspirational and\nbeneficial for solving technical problems in modern industries. This leads to a\nnovel form of design-by-analogy called bio-inspired design (BID). Although BID\nas a design method has been proven beneficial, the gap between biology and\nengineering continuously hinders designers from effectively applying the\nmethod. Therefore, we explore the recent advance of artificial intelligence\n(AI) for a computational approach to bridge the gap. This paper proposes a\ngenerative design approach based on the pre-trained language model (PLM) to\nautomatically retrieve and map biological analogy and generate BID in the form\nof natural language. The latest generative pre-trained transformer, namely\nGPT-3, is used as the base PLM. Three types of design concept generators are\nidentified and fine-tuned from the PLM according to the looseness of the\nproblem space representation. Machine evaluators are also fine-tuned to assess\nthe correlation between the domains within the generated BID concepts. The\napproach is then tested via a case study in which the fine-tuned models are\napplied to generate and evaluate light-weighted flying car concepts inspired by\nnature. The results show our approach can generate BID concepts with good\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Language Model Size in Cross-Device Federated Learning. (arXiv:2204.09715v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09715","description":"<p>Most studies in cross-device federated learning focus on small models, due to\nthe server-client communication and on-device computation bottlenecks. In this\nwork, we leverage various techniques for mitigating these bottlenecks to train\nlarger language models in cross-device federated learning. With systematic\napplications of partial model training, quantization, efficient transfer\nlearning, and communication-efficient optimizers, we are able to train a $21$M\nparameter Transformer that achieves the same perplexity as that of a similarly\nsized LSTM with $\\sim10\\times$ smaller client-to-server communication cost and\n$11\\%$ lower perplexity than smaller LSTMs commonly studied in literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ro_J/0/1/0/all/0/1\">Jae Hun Ro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breiner_T/0/1/0/all/0/1\">Theresa Breiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McConnaughey_L/0/1/0/all/0/1\">Lara McConnaughey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1\">Ananda Theertha Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shankar Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathews_R/0/1/0/all/0/1\">Rajiv Mathews</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Specific Fine-tuning of Denoising Sequence-to-Sequence Models for Natural Language Summarization. (arXiv:2204.09716v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09716","description":"<p>Summarization of long-form text data is a problem especially pertinent in\nknowledge economy jobs such as medicine and finance, that require continuously\nremaining informed on a sophisticated and evolving body of knowledge. As such,\nisolating and summarizing key content automatically using Natural Language\nProcessing (NLP) techniques holds the potential for extensive time savings in\nthese industries. We explore applications of a state-of-the-art NLP model\n(BART), and explore strategies for tuning it to optimal performance using data\naugmentation and various fine-tuning strategies. We show that our end-to-end\nfine-tuning approach can result in a 5-6\\% absolute ROUGE-1 improvement over an\nout-of-the-box pre-trained BART summarizer when tested on domain specific data,\nand make available our end-to-end pipeline to achieve these results on finance,\nmedical, or other user-specified domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parker_B/0/1/0/all/0/1\">Brydon Parker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolov_A/0/1/0/all/0/1\">Alik Sokolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1\">Mahtab Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalebic_M/0/1/0/all/0/1\">Matt Kalebic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocak_S/0/1/0/all/0/1\">Sedef Akinli Kocak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shai_O/0/1/0/all/0/1\">Ofer Shai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LSTM-RASA Based Agri Farm Assistant for Farmers. (arXiv:2204.09717v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09717","description":"<p>The application of Deep Learning and Natural Language based ChatBots are\ngrowing rapidly in recent years. They are used in many fields like customer\nsupport, reservation system and as personal assistant. The Enterprises are\nusing such ChatBots to serve their customers in a better and efficient manner.\nEven after such technological advancement, the expert advice does not reach the\nfarmers on timely manner. The farmers are still largely dependent on their\npeers knowledge in solving the problems they face in their field. These\ntechnologies have not been effectively used to give the required information to\nfarmers on timely manner. This project aims to implement a closed domain\nChatBot for the field of Agriculture Farmers Assistant. Farmers can have\nconversation with the Chatbot and get the expert advice in their field. Farmers\nAssistant is based on RASA Open Source Framework. The Chatbot identifies the\nintent and entity from user utterances and retrieve the remedy from the\ndatabase and share it with the user. We tested the Bot with existing data and\nit showed promising results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Darapaneni_N/0/1/0/all/0/1\">Narayana Darapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_S/0/1/0/all/0/1\">Selvakumar Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_R/0/1/0/all/0/1\">Raghul V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivaraman_V/0/1/0/all/0/1\">Venkatesh Sivaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1\">Sunil Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paduri_A/0/1/0/all/0/1\">Anwesh Reddy Paduri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching Writers to Content Writing Tasks. (arXiv:2204.09718v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09718","description":"<p>Businesses need content. In various forms and formats and for varied\npurposes. In fact, the content marketing industry is set to be worth $412.88\nbillion by the end of 2021. However, according to the Content Marketing\nInstitute, creating engaging content is the #1 challenge that marketers face\ntoday. We under-stand that producing great content requires great writers who\nunderstand the business and can weave their message into reader (and search\nengine) friendly content. In this project, the team has attempted to bridge the\ngap between writers and projects by using AI and ML tools. We used NLP\ntechniques to analyze thou-sands of publicly available business articles\n(corpora) to extract various defining factors for each writing sample. Through\nthis project we aim to automate the highly time-consuming, and often biased\ntask of manually shortlisting the most suitable writer for a given content\nwriting requirement. We believe that a tool like this will have far reaching\npositive implications for both parties - businesses looking for suitable talent\nfor niche writing jobs as well as experienced writers and Subject Matter\nExperts (SMEs) wanting to lend their services to content marketing projects.\nThe business gets the content they need, the content writer/ SME gets a chance\nto leverage his or her talent, while the reader gets authentic content that\nadds real value.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Darapaneni_N/0/1/0/all/0/1\">Narayana Darapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhakuni_C/0/1/0/all/0/1\">Chandrashekhar Bhakuni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_U/0/1/0/all/0/1\">Ujjval Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_K/0/1/0/all/0/1\">Khamir Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sardna_V/0/1/0/all/0/1\">Vikas Sardna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Prabir Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paduri_A/0/1/0/all/0/1\">Anwesh Reddy Paduri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Progress in Conversational AI. (arXiv:2204.09719v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09719","description":"<p>Conversational artificial intelligence (AI) is becoming an increasingly\npopular topic among industry and academia. With the fast development of neural\nnetwork-based models, a lot of neural-based conversational AI system are\ndeveloped. We will provide a brief review of the recent progress in the\nConversational AI, including the commonly adopted techniques, notable works,\nfamous competitions from academia and industry and widely used datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zijun Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruirui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingda Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Does Syntax Mediate Neural Language Model Performance? Evidence from Dropout Probes. (arXiv:2204.09722v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09722","description":"<p>Recent causal probing literature reveals when language models and syntactic\nprobes use similar representations. Such techniques may yield \"false negative\"\ncausality results: models may use representations of syntax, but probes may\nhave learned to use redundant encodings of the same syntactic information. We\ndemonstrate that models do encode syntactic information redundantly and\nintroduce a new probe design that guides probes to consider all syntactic\ninformation present in embeddings. Using these probes, we find evidence for the\nuse of syntax in models where prior methods did not, allowing us to boost model\nperformance by injecting syntactic information into representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tucker_M/0/1/0/all/0/1\">Mycal Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisape_T/0/1/0/all/0/1\">Tiwalayo Eisape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_P/0/1/0/all/0/1\">Peng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1\">Julie Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Res-CNN-BiLSTM Network for overcoming Mental Health Disturbances caused due to Cyberbullying through Social Media. (arXiv:2204.09738v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09738","description":"<p>Mental Health Disturbance has many reasons and cyberbullying is one of the\nmajor causes that does exploitation using social media as an instrument. The\ncyberbullying is done on the basis of Religion, Ethnicity, Age and Gender which\nis a sensitive psychological issue. This can be addressed using Natural\nLanguage Processing with Deep Learning, since social media is the medium and it\ngenerates massive form of data in textual form. Such data can be leveraged to\nfind the semantics and derive what type of cyberbullying is done and who are\nthe people involved for early measures. Since deriving semantics is essential\nwe proposed a Hybrid Deep Learning Model named 1-Dimensional\nCNN-Bidirectional-LSTMs with Residuals shortly known as Res-CNN-BiLSTM. In this\npaper we have proposed the architecture and compared its performance with\ndifferent approaches of Embedding Deep Learning Algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raunak Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanvinde_N/0/1/0/all/0/1\">Nandan Kanvinde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-label classification for biomedical literature: an overview of the BioCreative VII LitCovid Track for COVID-19 literature topic annotations. (arXiv:2204.09781v1 [cs.DL])","link":"http://arxiv.org/abs/2204.09781","description":"<p>The COVID-19 pandemic has been severely impacting global society since\nDecember 2019. Massive research has been undertaken to understand the\ncharacteristics of the virus and design vaccines and drugs. The related\nfindings have been reported in biomedical literature at a rate of about 10,000\narticles on COVID-19 per month. Such rapid growth significantly challenges\nmanual curation and interpretation. For instance, LitCovid is a literature\ndatabase of COVID-19-related articles in PubMed, which has accumulated more\nthan 200,000 articles with millions of accesses each month by users worldwide.\nOne primary curation task is to assign up to eight topics (e.g., Diagnosis and\nTreatment) to the articles in LitCovid. Despite the continuing advances in\nbiomedical text mining methods, few have been dedicated to topic annotations in\nCOVID-19 literature. To close the gap, we organized the BioCreative LitCovid\ntrack to call for a community effort to tackle automated topic annotation for\nCOVID-19 literature. The BioCreative LitCovid dataset, consisting of over\n30,000 articles with manually reviewed topics, was created for training and\ntesting. It is one of the largest multilabel classification datasets in\nbiomedical scientific literature. 19 teams worldwide participated and made 80\nsubmissions in total. Most teams used hybrid systems based on transformers. The\nhighest performing submissions achieved 0.8875, 0.9181, and 0.9394 for macro\nF1-score, micro F1-score, and instance-based F1-score, respectively. The level\nof participation and results demonstrate a successful track and help close the\ngap between dataset curation and method development. The dataset is publicly\navailable via https://ftp.ncbi.nlm.nih.gov/pub/lu/LitCovid/biocreative/ for\nbenchmarking and further development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allot_A/0/1/0/all/0/1\">Alexis Allot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leaman_R/0/1/0/all/0/1\">Robert Leaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogan_R/0/1/0/all/0/1\">Rezarta Islamaj Do&#x11f;an</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jingcheng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Li Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kai_W/0/1/0/all/0/1\">Wang Kai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuefu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagherzadeh_P/0/1/0/all/0/1\">Parsa Bagherzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergler_S/0/1/0/all/0/1\">Sabine Bergler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatnagar_A/0/1/0/all/0/1\">Aakash Bhatnagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhavsar_N/0/1/0/all/0/1\">Nidhir Bhavsar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yung-Chun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sheng-Jie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wentai Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongtong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavchioski_I/0/1/0/all/0/1\">Ilija Tavchioski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shubo Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otmakhova_Y/0/1/0/all/0/1\">Yulia Otmakhova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yepes_A/0/1/0/all/0/1\">Antonio Jimeno Yepes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufour_R/0/1/0/all/0/1\">Richard Dufour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labrak_Y/0/1/0/all/0/1\">Yanis Labrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_N/0/1/0/all/0/1\">Niladri Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_K/0/1/0/all/0/1\">Kushagri Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laleye_F/0/1/0/all/0/1\">Fr&#xe9;jus Laleye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakotoson_L/0/1/0/all/0/1\">Lo&#xef;c Rakotoson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chersoni_E/0/1/0/all/0/1\">Emmanuele Chersoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinghang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_A/0/1/0/all/0/1\">Annemarie Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujari_S/0/1/0/all/0/1\">Subhash Chandra Pujari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chizhikova_M/0/1/0/all/0/1\">Mariia Chizhikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivadasan_N/0/1/0/all/0/1\">Naveen Sivadasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivadasan_N/0/1/0/all/0/1\">Naveen Sivadasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing. (arXiv:2204.09817v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09817","description":"<p>Multi-modal data abounds in biomedicine, such as radiology images and\nreports. Interpreting this data at scale is essential for improving clinical\ncare and accelerating clinical research. Biomedical text with its complex\nsemantics poses additional challenges in vision-language modelling compared to\nthe general domain, and previous work has used insufficiently adapted models\nthat lack domain-specific language understanding. In this paper, we show that\nprincipled textual semantic modelling can substantially improve contrastive\nlearning in self-supervised vision--language processing. We release a language\nmodel that achieves state-of-the-art results in radiology natural language\ninference through its improved vocabulary and novel language pretraining\nobjective leveraging semantics and discourse characteristics in radiology\nreports. Further, we propose a self-supervised joint vision--language approach\nwith a focus on better text modelling. It establishes new state of the art\nresults on a wide range of publicly available benchmarks, in part by leveraging\nour new domain-specific language model. We release a new dataset with\nlocally-aligned phrase grounding annotations by radiologists to facilitate the\nstudy of complex semantic modelling in biomedical vision--language processing.\nA broad evaluation, including on this new dataset, shows that our contrastive\nlearning approach, aided by textual-semantic modelling, outperforms prior\nmethods in segmentation tasks, despite only using a global-alignment objective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boecking_B/0/1/0/all/0/1\">Benedikt Boecking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bannur_S/0/1/0/all/0/1\">Shruthi Bannur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Daniel C. Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwaighofer_A/0/1/0/all/0/1\">Anton Schwaighofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyland_S/0/1/0/all/0/1\">Stephanie Hyland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetscherek_M/0/1/0/all/0/1\">Maria Wetscherek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nori_A/0/1/0/all/0/1\">Aditya Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Valle_J/0/1/0/all/0/1\">Javier Alvarez-Valle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oktay_O/0/1/0/all/0/1\">Ozan Oktay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Masked Image Reconstruction Network for Document-level Relation Extraction. (arXiv:2204.09851v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09851","description":"<p>Document-level relation extraction aims to extract relations among entities\nwithin a document. Compared with its sentence-level counterpart, Document-level\nrelation extraction requires inference over multiple sentences to extract\ncomplex relational triples. Previous research normally complete reasoning\nthrough information propagation on the mention-level or entity-level\ndocument-graphs, regardless of the correlations between the relationships. In\nthis paper, we propose a novel Document-level Relation Extraction model based\non a Masked Image Reconstruction network (DRE-MIR), which models inference as a\nmasked image reconstruction problem to capture the correlations between\nrelationships. Specifically, we first leverage an encoder module to get the\nfeatures of entities and construct the entity-pair matrix based on the\nfeatures. After that, we look on the entity-pair matrix as an image and then\nrandomly mask it and restore it through an inference module to capture the\ncorrelations between the relationships. We evaluate our model on three public\ndocument-level relation extraction datasets, i.e. DocRED, CDR, and GDA.\nExperimental results demonstrate that our model achieves state-of-the-art\nperformance on these three datasets and has excellent robustness against the\nnoises during the inference process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yidong Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Model-Agnostic Data Manipulation Method for Persona-based Dialogue Generation. (arXiv:2204.09867v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09867","description":"<p>Towards building intelligent dialogue agents, there has been a growing\ninterest in introducing explicit personas in generation models. However, with\nlimited persona-based dialogue data at hand, it may be difficult to train a\ndialogue generation model well. We point out that the data challenges of this\ngeneration task lie in two aspects: first, it is expensive to scale up current\npersona-based dialogue datasets; second, each data sample in this task is more\ncomplex to learn with than conventional dialogue data. To alleviate the above\ndata issues, we propose a data manipulation method, which is model-agnostic to\nbe packed with any persona-based dialogue generation model to improve its\nperformance. The original training samples will first be distilled and thus\nexpected to be fitted more easily. Next, we show various effective ways that\ncan diversify such easier distilled data. A given base model will then be\ntrained via the constructed data curricula, i.e. first on augmented distilled\nsamples and then on original ones. Experiments illustrate the superiority of\nour method with two strong base dialogue models (Transformer encoder-decoder\nand GPT2).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1\">Wei Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Neural Topic Modelling Better than Clustering? An Empirical Study on Clustering with Contextual Embeddings for Topics. (arXiv:2204.09874v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09874","description":"<p>Recent work incorporates pre-trained word embeddings such as BERT embeddings\ninto Neural Topic Models (NTMs), generating highly coherent topics. However,\nwith high-quality contextualized document representations, do we really need\nsophisticated neural models to obtain coherent and interpretable topics? In\nthis paper, we conduct thorough experiments showing that directly clustering\nhigh-quality sentence embeddings with an appropriate word selecting method can\ngenerate more coherent and diverse topics than NTMs, achieving also higher\nefficiency and simplicity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Ling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namazi_Rad_M/0/1/0/all/0/1\">Mohammad-Reza Namazi-Rad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Attention-Based Model for Predicting Contextual Informativeness and Curriculum Learning Applications. (arXiv:2204.09885v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09885","description":"<p>Both humans and machines learn the meaning of unknown words through\ncontextual information in a sentence, but not all contexts are equally helpful\nfor learning. We introduce an effective method for capturing the level of\ncontextual informativeness with respect to a given target word. Our study makes\nthree main contributions. First, we develop models for estimating contextual\ninformativeness, focusing on the instructional aspect of sentences. Our\nattention-based approach using pre-trained embeddings demonstrates\nstate-of-the-art performance on our single-context dataset and an existing\nmulti-sentence context dataset. Second, we show how our model identifies key\ncontextual elements in a sentence that are likely to contribute most to a\nreader's understanding of the target word. Third, we examine how our contextual\ninformativeness model, originally developed for vocabulary learning\napplications for students, can be used for developing better training curricula\nfor word embedding models in batch learning and few-shot machine learning\nsettings. We believe our results open new possibilities for applications that\nsupport language learning for both human and machine learners\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1\">Sungjin Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_Thompson_K/0/1/0/all/0/1\">Kevyn Collins-Thompson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spurious Correlations in Reference-Free Evaluation of Text Generation. (arXiv:2204.09890v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09890","description":"<p>Model-based, reference-free evaluation metrics have been proposed as a fast\nand cost-effective approach to evaluate Natural Language Generation (NLG)\nsystems. Despite promising recent results, we find evidence that reference-free\nevaluation metrics of summarization and dialog generation may be relying on\nspurious correlations with measures such as word overlap, perplexity, and\nlength. We further observe that for text summarization, these metrics have high\nerror rates when ranking current state-of-the-art abstractive summarization\nsystems. We demonstrate that these errors can be mitigated by explicitly\ndesigning evaluation metrics to avoid spurious features in reference-free\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladhak_F/0/1/0/all/0/1\">Faisal Ladhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task recommendation system for scientific papers with high-way networks. (arXiv:2204.09930v1 [cs.IR])","link":"http://arxiv.org/abs/2204.09930","description":"<p>Finding and selecting the most relevant scientific papers from a large number\nof papers written in a research community is one of the key challenges for\nresearchers these days. As we know, much information around research interest\nfor scholars and academicians belongs to papers they read. Analysis and\nextracting contextual features from these papers could help us to suggest the\nmost related paper to them. In this paper, we present a multi-task\nrecommendation system (RS) that predicts a paper recommendation and generates\nits meta-data such as keywords. The system is implemented as a three-stage deep\nneural network encoder that tries to maps longer sequences of text to an\nembedding vector and learns simultaneously to predict the recommendation rate\nfor a particular user and the paper's keywords. The motivation behind this\napproach is that the paper's topics expressed as keywords are a useful\npredictor of preferences of researchers. To achieve this goal, we use a system\ncombination of RNNs, Highway and Convolutional Neural Networks to train\nend-to-end a context-aware collaborative matrix. Our application uses Highway\nnetworks to train the system very deep, combine the benefits of RNN and CNN to\nfind the most important factor and make latent representation. Highway Networks\nallow us to enhance the traditional RNN and CNN pipeline by learning more\nsophisticated semantic structural representations. Using this method we can\nalso overcome the cold start problem and learn latent features over large\nsequences of text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karimi_A/0/1/0/all/0/1\">Aram Karimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobnik_S/0/1/0/all/0/1\">Simon Dobnik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recovering Patient Journeys: A Corpus of Biomedical Entities and Relations on Twitter (BEAR). (arXiv:2204.09952v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09952","description":"<p>Text mining and information extraction for the medical domain has focused on\nscientific text generated by researchers. However, their direct access to\nindividual patient experiences or patient-doctor interactions can be limited.\nInformation provided on social media, e.g., by patients and their relatives,\ncomplements the knowledge in scientific text. It reflects the patient's journey\nand their subjective perspective on the process of developing symptoms, being\ndiagnosed and offered a treatment, being cured or learning to live with a\nmedical condition. The value of this type of data is therefore twofold:\nFirstly, it offers direct access to people's perspectives. Secondly, it might\ncover information that is not available elsewhere, including self-treatment or\nself-diagnoses. Named entity recognition and relation extraction are methods to\nstructure information that is available in unstructured text. However, existing\nmedical social media corpora focused on a comparably small set of entities and\nrelations and particular domains, rather than putting the patient into the\ncenter of analyses. With this paper we contribute a corpus with a rich set of\nannotation layers following the motivation to uncover and model patients'\njourneys and experiences in more detail. We label 14 entity classes (incl.\nenvironmental factors, diagnostics, biochemical processes, patients'\nquality-of-life descriptions, pathogens, medical conditions, and treatments)\nand 20 relation classes (e.g., prevents, influences, interactions, causes) most\nof which have not been considered before for social media data. The publicly\navailable dataset consists of 2,100 tweets with approx. 6,000 entity and 3,000\nrelation annotations. In a corpus analysis we find that over 80 % of documents\ncontain relevant entities. Over 50 % of tweets express relations which we\nconsider essential for uncovering patients' narratives about their journeys.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wuhrl_A/0/1/0/all/0/1\">Amelie W&#xfc;hrl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEAM-Atreides at SemEval-2022 Task 11: On leveraging data augmentation and ensemble to recognize complex Named Entities in Bangla. (arXiv:2204.09964v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09964","description":"<p>Many areas, such as the biological and healthcare domain, artistic works, and\norganization names, have nested, overlapping, discontinuous entity mentions\nthat may even be syntactically or semantically ambiguous in practice.\nTraditional sequence tagging algorithms are unable to recognize these complex\nmentions because they may violate the assumptions upon which sequence tagging\nschemes are founded. In this paper, we describe our contribution to SemEval\n2022 Task 11 on identifying such complex Named Entities. We have leveraged the\nensemble of multiple ELECTRA-based models that were exclusively pretrained on\nthe Bangla language with the performance of ELECTRA-based models pretrained on\nEnglish to achieve competitive performance on the Track-11. Besides providing a\nsystem description, we will also present the outcomes of our experiments on\narchitectural decisions, dataset augmentations, and post-competition findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tasnim_N/0/1/0/all/0/1\">Nazia Tasnim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shihab_M/0/1/0/all/0/1\">Md. Istiak Hossain Shihab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sushmit_A/0/1/0/all/0/1\">Asif Shahriyar Sushmit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethard_S/0/1/0/all/0/1\">Steven Bethard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeque_F/0/1/0/all/0/1\">Farig Sadeque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Standing on the Shoulders of Giant Frozen Language Models. (arXiv:2204.10019v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10019","description":"<p>Huge pretrained language models (LMs) have demonstrated surprisingly good\nzero-shot capabilities on a wide variety of tasks. This gives rise to the\nappealing vision of a single, versatile model with a wide range of\nfunctionalities across disparate applications. However, current leading\ntechniques for leveraging a \"frozen\" LM -- i.e., leaving its weights untouched\n-- still often underperform fine-tuning approaches which modify these weights\nin a task-dependent way. Those, in turn, suffer forgetfulness and compromise\nversatility, suggesting a tradeoff between performance and versatility. The\nmain message of this paper is that current frozen-model techniques such as\nprompt tuning are only the tip of the iceberg, and more powerful methods for\nleveraging frozen LMs can do just as well as fine tuning in challenging domains\nwithout sacrificing the underlying model's versatility. To demonstrate this, we\nintroduce three novel methods for leveraging frozen models: input-dependent\nprompt tuning, frozen readers, and recursive LMs, each of which vastly improves\non current frozen-model approaches. Indeed, some of our methods even outperform\nfine-tuning approaches in domains currently dominated by the latter. The\ncomputational cost of each method is higher than that of existing frozen model\nmethods, but still negligible relative to a single pass through a huge frozen\nLM. Each of these methods constitutes a meaningful contribution in its own\nright, but by presenting these contributions together we aim to convince the\nreader of a broader message that goes beyond the details of any given method:\nthat frozen models have untapped potential and that fine-tuning is often\nunnecessary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1\">Yoav Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalmedigos_I/0/1/0/all/0/1\">Itay Dalmedigos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_O/0/1/0/all/0/1\">Ori Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_Y/0/1/0/all/0/1\">Yoel Zeldes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jannai_D/0/1/0/all/0/1\">Daniel Jannai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhlgay_D/0/1/0/all/0/1\">Dor Muhlgay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osin_Y/0/1/0/all/0/1\">Yoni Osin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lieber_O/0/1/0/all/0/1\">Opher Lieber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenz_B/0/1/0/all/0/1\">Barak Lenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalev_Shwartz_S/0/1/0/all/0/1\">Shai Shalev-Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leyton_Brown_K/0/1/0/all/0/1\">Kevin Leyton-Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoham_Y/0/1/0/all/0/1\">Yoav Shoham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding. (arXiv:2204.10050v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10050","description":"<p>This paper presents the shared task on Multilingual Idiomaticity Detection\nand Sentence Embedding, which consists of two subtasks: (a) a binary\nclassification one aimed at identifying whether a sentence contains an\nidiomatic expression, and (b) a task based on semantic text similarity which\nrequires the model to adequately represent potentially idiomatic expressions in\ncontext. Each subtask includes different settings regarding the amount of\ntraining data. Besides the task description, this paper introduces the datasets\nin English, Portuguese, and Galician and their annotation procedure, the\nevaluation metrics, and a summary of the participant systems and their results.\nThe task had close to 100 registered participants organised into twenty five\nteams making over 650 and 150 submissions in the practice and evaluation phases\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madabushi_H/0/1/0/all/0/1\">Harish Tayyar Madabushi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gow_Smith_E/0/1/0/all/0/1\">Edward Gow-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_M/0/1/0/all/0/1\">Marcos Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Idiart_M/0/1/0/all/0/1\">Marco Idiart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villavicencio_A/0/1/0/all/0/1\">Aline Villavicencio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying and Characterizing Active Citizens who Refute Misinformation in Social Media. (arXiv:2204.10080v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10080","description":"<p>The phenomenon of misinformation spreading in social media has developed a\nnew form of active citizens who focus on tackling the problem by refuting posts\nthat might contain misinformation. Automatically identifying and characterizing\nthe behavior of such active citizens in social media is an important task in\ncomputational social science for complementing studies in misinformation\nanalysis. In this paper, we study this task across different social media\nplatforms (i.e., Twitter and Weibo) and languages (i.e., English and Chinese)\nfor the first time. To this end, (1) we develop and make publicly available a\nnew dataset of Weibo users mapped into one of the two categories (i.e.,\nmisinformation posters or active citizens); (2) we evaluate a battery of\nsupervised models on our new Weibo dataset and an existing Twitter dataset\nwhich we repurpose for the task; and (3) we present an extensive analysis of\nthe differences in language use between the two user categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yida Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_P/0/1/0/all/0/1\">Pu Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OTExtSum: Extractive Text Summarisation with Optimal Transport. (arXiv:2204.10086v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10086","description":"<p>Extractive text summarisation aims to select salient sentences from a\ndocument to form a short yet informative summary. While learning-based methods\nhave achieved promising results, they have several limitations, such as\ndependence on expensive training and lack of interpretability. Therefore, in\nthis paper, we propose a novel non-learning-based method by for the first time\nformulating text summarisation as an Optimal Transport (OT) problem, namely\nOptimal Transport Extractive Summariser (OTExtSum). Optimal sentence extraction\nis conceptualised as obtaining an optimal summary that minimises the\ntransportation cost to a given document regarding their semantic distributions.\nSuch a cost is defined by the Wasserstein distance and used to measure the\nsummary's semantic coverage of the original document. Comprehensive experiments\non four challenging and widely used datasets - MultiNews, PubMed, BillSum, and\nCNN/DM demonstrate that our proposed method outperforms the state-of-the-art\nnon-learning-based methods and several recent learning-based methods in terms\nof the ROUGE metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1\">Peggy Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Kun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junbin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiyong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gated Multimodal Fusion with Contrastive Learning for Turn-taking Prediction in Human-robot Dialogue. (arXiv:2204.10172v1 [eess.AS])","link":"http://arxiv.org/abs/2204.10172","description":"<p>Turn-taking, aiming to decide when the next speaker can start talking, is an\nessential component in building human-robot spoken dialogue systems. Previous\nstudies indicate that multimodal cues can facilitate this challenging task.\nHowever, due to the paucity of public multimodal datasets, current methods are\nmostly limited to either utilizing unimodal features or simplistic multimodal\nensemble models. Besides, the inherent class imbalance in real scenario, e.g.\nsentence ending with short pause will be mostly regarded as the end of turn,\nalso poses great challenge to the turn-taking decision. In this paper, we first\ncollect a large-scale annotated corpus for turn-taking with over 5,000 real\nhuman-robot dialogues in speech and text modalities. Then, a novel gated\nmultimodal fusion mechanism is devised to utilize various information\nseamlessly for turn-taking prediction. More importantly, to tackle the data\nimbalance issue, we design a simple yet effective data augmentation method to\nconstruct negative instances without supervision and apply contrastive learning\nto obtain better feature representations. Extensive experiments are conducted\nand the results demonstrate the superiority and competitiveness of our model\nover several state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jiudong Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_P/0/1/0/all/0/1\">Peiying Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_M/0/1/0/all/0/1\">Mingchao Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1\">Meng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Script Knowledge from Pre-Trained Models. (arXiv:2204.10176v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10176","description":"<p>Script knowledge is critical for humans to understand the broad daily tasks\nand routine activities in the world. Recently researchers have explored the\nlarge-scale pre-trained language models (PLMs) to perform various script\nrelated tasks, such as story generation, temporal ordering of event, future\nevent prediction and so on. However, it's still not well studied in terms of\nhow well the PLMs capture the script knowledge. To answer this question, we\ndesign three probing tasks: inclusive sub-event selection, starting sub-event\nselection and temporal ordering to investigate the capabilities of PLMs with\nand without fine-tuning. The three probing tasks can be further used to\nautomatically induce a script for each main event given all the possible\nsub-events. Taking BERT as a case study, by analyzing its performance on script\ninduction as well as each individual probing task, we conclude that the\nstereotypical temporal knowledge among the sub-events is well captured in BERT,\nhowever the inclusive or starting sub-event knowledge is barely encoded.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zijian Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Doctor XAvIer: Explainable Diagnosis using Physician-Patient Dialogues and XAI Evaluation. (arXiv:2204.10178v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10178","description":"<p>We introduce Doctor XAvIer, a BERT-based diagnostic system that extracts\nrelevant clinical data from transcribed patient-doctor dialogues and explains\npredictions using feature attribution methods. We present a novel performance\nplot and evaluation metric for feature attribution methods: Feature Attribution\nDropping (FAD) curve and its Normalized Area Under the Curve (N-AUC). FAD curve\nanalysis shows that integrated gradients outperforms Shapley values in\nexplaining diagnosis classification. Doctor XAvIer outperforms the baseline\nwith 0.97 F1-score in named entity recognition and symptom pertinence\nclassification and 0.91 F1-score in diagnosis classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ngai_H/0/1/0/all/0/1\">Hillary Ngai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1\">Frank Rudzicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WordAlchemy: A transformer-based Reverse Dictionary. (arXiv:2204.10181v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10181","description":"<p>A reverse dictionary takes a target word's description as input and returns\nthe words that fit the description. Reverse Dictionaries are useful for new\nlanguage learners, anomia patients, and for solving common tip-of-the-tongue\nproblems (lethologica). Currently, there does not exist any Reverse Dictionary\nprovider with support for any Indian Language. We present a novel open-source\ncross-lingual reverse dictionary system with support for Indian languages. In\nthis paper, we propose a transformer-based deep learning approach to tackle the\nlimitations faced by the existing systems using the mT5 model. This\narchitecture uses the Translation Language Modeling (TLM) technique, rather\nthan the conventional BERT's Masked Language Modeling (MLM) technique.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mane_D/0/1/0/all/0/1\">Dr. Sunil B. Mane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_H/0/1/0/all/0/1\">Harshal Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madaswar_K/0/1/0/all/0/1\">Kanhaiya Madaswar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadavarte_P/0/1/0/all/0/1\">Pranav Sadavarte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Social Media Sentiment Analysis for Cryptocurrency Market Prediction. (arXiv:2204.10185v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10185","description":"<p>In this paper, we explore the usability of different natural language\nprocessing models for the sentiment analysis of social media applied to\nfinancial market prediction, using the cryptocurrency domain as a reference. We\nstudy how the different sentiment metrics are correlated with the price\nmovements of Bitcoin. For this purpose, we explore different methods to\ncalculate the sentiment metrics from a text finding most of them not very\naccurate for this prediction task. We find that one of the models outperforms\nmore than 20 other public ones and makes it possible to fine-tune it\nefficiently given its interpretable nature. Thus we confirm that interpretable\nartificial intelligence and natural language processing methods might be more\nvaluable practically than non-explainable and non-interpretable ones. In the\nend, we analyse potential causal connections between the different sentiment\nmetrics and the price movements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raheman_A/0/1/0/all/0/1\">Ali Raheman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolonin_A/0/1/0/all/0/1\">Anton Kolonin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fridkins_I/0/1/0/all/0/1\">Igors Fridkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ansari_I/0/1/0/all/0/1\">Ikram Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishwas_M/0/1/0/all/0/1\">Mukul Vishwas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Topic Modeling of Psychotherapy Sessions. (arXiv:2204.10189v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10189","description":"<p>In this work, we compare different neural topic modeling methods in learning\nthe topical propensities of different psychiatric conditions from the\npsychotherapy session transcripts parsed from speech recordings. We also\nincorporate temporal modeling to put this additional interpretability to action\nby parsing out topic similarities as a time series in a turn-level resolution.\nWe believe this topic modeling framework can offer interpretable insights for\nthe therapist to optimally decide his or her strategy and improve the\npsychotherapy effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Baihan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouneffouf_D/0/1/0/all/0/1\">Djallel Bouneffouf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cecchi_G/0/1/0/all/0/1\">Guillermo Cecchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tejwani_R/0/1/0/all/0/1\">Ravi Tejwani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating User Radicalization: A Novel Dataset for Identifying Fine-Grained Temporal Shifts in Opinion. (arXiv:2204.10190v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10190","description":"<p>There is an increasing need for the ability to model fine-grained opinion\nshifts of social media users, as concerns about the potential polarizing social\neffects increase. However, the lack of publicly available datasets that are\nsuitable for the task presents a major challenge. In this paper, we introduce\nan innovative annotated dataset for modeling subtle opinion fluctuations and\ndetecting fine-grained stances. The dataset includes a sufficient amount of\nstance polarity and intensity labels per user over time and within entire\nconversational threads, thus making subtle opinion fluctuations detectable both\nin long term and in short term. All posts are annotated by non-experts and a\nsignificant portion of the data is also annotated by experts. We provide a\nstrategy for recruiting suitable non-experts. Our analysis of the\ninter-annotator agreements shows that the resulting annotations obtained from\nthe majority vote of the non-experts are of comparable quality to the\nannotations of the experts. We provide analyses of the stance evolution in\nshort term and long term levels, a comparison of language usage between users\nwith vacillating and resolute attitudes, and fine-grained stance detection\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sakketou_F/0/1/0/all/0/1\">Flora Sakketou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahnala_A/0/1/0/all/0/1\">Allison Lahnala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogel_L/0/1/0/all/0/1\">Liane Vogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flek_L/0/1/0/all/0/1\">Lucie Flek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Residue-Based Natural Language Adversarial Attack Detection. (arXiv:2204.10192v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10192","description":"<p>Deep learning based systems are susceptible to adversarial attacks, where a\nsmall, imperceptible change at the input alters the model prediction. However,\nto date the majority of the approaches to detect these attacks have been\ndesigned for image processing systems. Many popular image adversarial detection\napproaches are able to identify adversarial examples from embedding feature\nspaces, whilst in the NLP domain existing state of the art detection approaches\nsolely focus on input text features, without consideration of model embedding\nspaces. This work examines what differences result when porting these image\ndesigned strategies to Natural Language Processing (NLP) tasks - these\ndetectors are found to not port over well. This is expected as NLP systems have\na very different form of input: discrete and sequential in nature, rather than\nthe continuous and fixed size inputs for images. As an equivalent model-focused\nNLP detection approach, this work proposes a simple sentence-embedding\n\"residue\" based detector to identify adversarial examples. On many tasks, it\nout-performs ported image domain detectors and recent state of the art NLP\nspecific detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vyas Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark Gales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Structure based Query Graph Prediction for Question Answering over Knowledge Graph. (arXiv:2204.10194v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10194","description":"<p>Building query graphs from natural language questions is an important step in\ncomplex question answering over knowledge graph (Complex KGQA). In general, a\nquestion can be correctly answered if its query graph is built correctly and\nthe right answer is then retrieved by issuing the query graph against the KG.\nTherefore, this paper focuses on query graph generation from natural language\nquestions. Existing approaches for query graph generation ignore the semantic\nstructure of a question, resulting in a large number of noisy query graph\ncandidates that undermine prediction accuracies. In this paper, we define six\nsemantic structures from common questions in KGQA and develop a novel\nStructure-BERT to predict the semantic structure of a question. By doing so, we\ncan first filter out noisy candidate query graphs by the predicted semantic\nstructures, and then rank the remaining candidates with a BERT-based ranking\nmodel. Extensive experiments on two popular benchmarks MetaQA and\nWebQuestionsSP (WSP) demonstrate the effectiveness of our method as compared to\nstate-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingchen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jonathan Shihao Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IIITDWD-ShankarB@ Dravidian-CodeMixi-HASOC2021: mBERT based model for identification of offensive content in south Indian languages. (arXiv:2204.10195v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10195","description":"<p>In recent years, there has been a lot of focus on offensive content. The\namount of offensive content generated by social media is increasing at an\nalarming rate. This created a greater need to address this issue than ever\nbefore. To address these issues, the organizers of \"Dravidian-Code Mixed\nHASOC-2020\" have created two challenges. Task 1 involves identifying offensive\ncontent in Malayalam data, whereas Task 2 includes Malayalam and Tamil Code\nMixed Sentences. Our team participated in Task 2. In our suggested model, we\nexperiment with multilingual BERT to extract features, and three different\nclassifiers are used on extracted features. Our model received a weighted F1\nscore of 0.70 for Malayalam data and was ranked fifth; we also received a\nweighted F1 score of 0.573 for Tamil Code Mixed data and were ranked eleventh.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biradar_S/0/1/0/all/0/1\">Shankar Biradar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saumya_S/0/1/0/all/0/1\">Sunil Saumya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Hate Speech Detection from Bengali Memes and Texts. (arXiv:2204.10196v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10196","description":"<p>Numerous works have been proposed to employ machine learning (ML) and deep\nlearning (DL) techniques to utilize textual data from social media for\nanti-social behavior analysis such as cyberbullying, fake news propagation, and\nhate speech mainly for highly resourced languages like English. However,\ndespite having a lot of diversity and millions of native speakers, some\nlanguages such as Bengali are under-resourced, which is due to a lack of\ncomputational resources for natural language processing (NLP). Like English,\nBengali social media content also includes images along with texts (e.g.,\nmultimodal contents are posted by embedding short texts into images on\nFacebook), only the textual data is not enough to judge them (e.g., to\ndetermine they are hate speech). In those cases, images might give extra\ncontext to properly judge. This paper is about hate speech detection from\nmultimodal Bengali memes and texts. We prepared the only multimodal hate speech\ndetection dataset1 for a kind of problem for Bengali. We train several neural\narchitectures (i.e., neural networks like Bi-LSTM/Conv-LSTM with word\nembeddings, EfficientNet + transformer architectures such as monolingual Bangla\nBERT, multilingual BERT-cased/uncased, and XLM-RoBERTa) jointly analyze textual\nand visual information for hate speech detection. The Conv-LSTM and XLM-RoBERTa\nmodels performed best for texts, yielding F1 scores of 0.78 and 0.82,\nrespectively. As of memes, ResNet152 and DenseNet201 models yield F1 scores of\n0.78 and 0.7, respectively. The multimodal fusion of mBERT-uncased +\nEfficientNet-B1 performed the best, yielding an F1 score of 0.80. Our study\nsuggests that memes are moderately useful for hate speech detection in Bengali,\nbut none of the multimodal models outperform unimodal models analyzing only\ntextual data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Md. Rezaul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Sumon Kanti Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tanhim Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Aware Language Modeling for Goal-Oriented Dialogue Systems. (arXiv:2204.10198v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10198","description":"<p>Goal-oriented dialogue systems face a trade-off between fluent language\ngeneration and task-specific control. While supervised learning with large\nlanguage models is capable of producing realistic text, how to steer such\nresponses towards completing a specific task without sacrificing language\nquality remains an open question. In this work, we formulate goal-oriented\ndialogue as a partially observed Markov decision process, interpreting the\nlanguage model as a representation of both the dynamics and the policy. This\nview allows us to extend techniques from learning-based control, such as task\nrelabeling, to derive a simple and effective method to finetune language models\nin a goal-aware way, leading to significantly improved task performance. We\nadditionally introduce a number of training strategies that serve to better\nfocus the model on the task at hand. We evaluate our method, Context-Aware\nLanguage Models (CALM), on a practical flight-booking task using AirDialogue.\nEmpirically, CALM outperforms the state-of-the-art method by 7% in terms of\ntask success, matching human-level task performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Snell_C/0/1/0/all/0/1\">Charlie Snell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sherry Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Justin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Exploratory Study on Code Attention in BERT. (arXiv:2204.10200v1 [cs.SE])","link":"http://arxiv.org/abs/2204.10200","description":"<p>Many recent models in software engineering introduced deep neural models\nbased on the Transformer architecture or use transformer-based Pre-trained\nLanguage Models (PLM) trained on code. Although these models achieve the state\nof the arts results in many downstream tasks such as code summarization and bug\ndetection, they are based on Transformer and PLM, which are mainly studied in\nthe Natural Language Processing (NLP) field. The current studies rely on the\nreasoning and practices from NLP for these models in code, despite the\ndifferences between natural languages and programming languages. There is also\nlimited literature on explaining how code is modeled.\n</p>\n<p>Here, we investigate the attention behavior of PLM on code and compare it\nwith natural language. We pre-trained BERT, a Transformer based PLM, on code\nand explored what kind of information it learns, both semantic and syntactic.\nWe run several experiments to analyze the attention values of code constructs\non each other and what BERT learns in each layer. Our analyses show that BERT\npays more attention to syntactic entities, specifically identifiers and\nseparators, in contrast to the most attended token [CLS] in NLP. This\nobservation motivated us to leverage identifiers to represent the code sequence\ninstead of the [CLS] token when used for code clone detection. Our results show\nthat employing embeddings from identifiers increases the performance of BERT by\n605% and 4% F1-score in its lower layers and the upper layers, respectively.\nWhen identifiers' embeddings are used in CodeBERT, a code-based PLM, the\nperformance is improved by 21-24% in the F1-score of clone detection. The\nfindings can benefit the research community by using code-specific\nrepresentations instead of applying the common embeddings used in NLP, and open\nnew directions for developing smaller models with similar performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rishab Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fuxiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fard_F/0/1/0/all/0/1\">Fatemeh Fard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_D/0/1/0/all/0/1\">David Lo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Usage-based learning of grammatical categories. (arXiv:2204.10201v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10201","description":"<p>Human languages use a wide range of grammatical categories to constrain which\nwords or phrases can fill certain slots in grammatical patterns and to express\nadditional meanings, such as tense or aspect, through morpho-syntactic means.\nThese grammatical categories, which are most often language-specific and\nchanging over time, are difficult to define and learn. This paper raises the\nquestion how these categories can be acquired and where they have come from. We\nexplore a usage-based approach. This means that categories and grammatical\nconstructions are selected and aligned by their success in language\ninteractions. We report on a multi-agent experiment in which agents are endowed\nwith mechanisms for understanding and producing utterances as well as\nmechanisms for expanding their inventories using a meta-level learning process\nbased on pro- and anti-unification. We show that a categorial type network\nwhich has scores based on the success in a language interaction leads to the\nspontaneous formation of grammatical categories in tandem with the formation of\ngrammatical patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Steels_L/0/1/0/all/0/1\">Luc Steels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eecke_P/0/1/0/all/0/1\">Paul Van Eecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beuls_K/0/1/0/all/0/1\">Katrien Beuls</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Numerical Reasoning to Extract Phenotypes from Clinical Text by Leveraging External Knowledge. (arXiv:2204.10202v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10202","description":"<p>Extracting phenotypes from clinical text has been shown to be useful for a\nvariety of clinical use cases such as identifying patients with rare diseases.\nHowever, reasoning with numerical values remains challenging for phenotyping in\nclinical text, for example, temperature 102F representing Fever. Current\nstate-of-the-art phenotyping models are able to detect general phenotypes, but\nperform poorly when they detect phenotypes requiring numerical reasoning. We\npresent a novel unsupervised methodology leveraging external knowledge and\ncontextualized word embeddings from ClinicalBERT for numerical reasoning in a\nvariety of phenotypic contexts. Comparing against unsupervised benchmarks, it\nshows a substantial performance improvement with absolute gains on generalized\nRecall and F1 scores up to 79% and 71%, respectively. In the supervised\nsetting, it also surpasses the performance of alternative approaches with\nabsolute gains on generalized Recall and F1 scores up to 70% and 44%,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanwar_A/0/1/0/all/0/1\">Ashwani Tanwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ive_J/0/1/0/all/0/1\">Julia Ive</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vibhor Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Answer Verification Methods for Question Answering-Based Summarization Evaluation Metrics. (arXiv:2204.10206v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10206","description":"<p>Question answering-based summarization evaluation metrics must automatically\ndetermine whether the QA model's prediction is correct or not, a task known as\nanswer verification. In this work, we benchmark the lexical answer verification\nmethods which have been used by current QA-based metrics as well as two more\nsophisticated text comparison methods, BERTScore and LERC. We find that LERC\nout-performs the other methods in some settings while remaining statistically\nindistinguishable from lexical overlap in others. However, our experiments\nreveal that improved verification performance does not necessarily translate to\noverall QA-based metric quality: In some scenarios, using a worse verification\nmethod -- or using none at all -- has comparable performance to using the best\nverification method, a result that we attribute to properties of the datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1\">Daniel Deutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-Examining System-Level Correlations of Automatic Summarization Evaluation Metrics. (arXiv:2204.10216v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10216","description":"<p>How reliably an automatic summarization evaluation metric replicates human\njudgments of summary quality is quantified by system-level correlations. We\nidentify two ways in which the definition of the system-level correlation is\ninconsistent with how metrics are used to evaluate systems in practice and\npropose changes to rectify this disconnect. First, we calculate the system\nscore for an automatic metric using the full test set instead of the subset of\nsummaries judged by humans, which is currently standard practice. We\ndemonstrate how this small change leads to more precise estimates of\nsystem-level correlations. Second, we propose to calculate correlations only on\npairs of systems that are separated by small differences in automatic scores\nwhich are commonly observed in practice. This allows us to demonstrate that our\nbest estimate of the correlation of ROUGE to human judgments is near 0 in\nrealistic scenarios. The results from the analyses point to the need to collect\nmore high-quality human judgments and to improve automatic metrics when\ndifferences in system scores are small.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1\">Daniel Deutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dror_R/0/1/0/all/0/1\">Rotem Dror</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Query-Based Summarization of Crisis-Related Social Media: An Abstractive Approach Using Transformers. (arXiv:2204.10230v1 [cs.IR])","link":"http://arxiv.org/abs/2204.10230","description":"<p>Relevant and timely information collected from social media during crises can\nbe an invaluable resource for emergency management. However, extracting this\ninformation remains a challenging task, particularly when dealing with social\nmedia postings in multiple languages. This work proposes a cross-lingual method\nfor retrieving and summarizing crisis-relevant information from social media\npostings. We describe a uniform way of expressing various information needs\nthrough structured queries and a way of creating summaries answering those\ninformation needs. The method is based on multilingual transformers embeddings.\nQueries are written in one of the languages supported by the embeddings, and\nthe extracted sentences can be in any of the other languages supported.\nAbstractive summaries are created by transformers. The evaluation, done by\ncrowdsourcing evaluators and emergency management experts, and carried out on\ncollections extracted from Twitter during five large-scale disasters spanning\nten languages, shows the flexibility of our approach. The generated summaries\nare regarded as more focused, structured, and coherent than existing\nstate-of-the-art methods, and experts compare them favorably against summaries\ncreated by existing, state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vitiugin_F/0/1/0/all/0/1\">Fedor Vitiugin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castillo_C/0/1/0/all/0/1\">Carlos Castillo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpaceE: Knowledge Graph Embedding by Relational Linear Transformation in the Entity Space. (arXiv:2204.10245v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10245","description":"<p>Translation distance based knowledge graph embedding (KGE) methods, such as\nTransE and RotatE, model the relation in knowledge graphs as translation or\nrotation in the vector space. Both translation and rotation are injective; that\nis, the translation or rotation of different vectors results in different\nresults. In knowledge graphs, different entities may have a relation with the\nsame entity; for example, many actors starred in one movie. Such a\nnon-injective relation pattern cannot be well modeled by the translation or\nrotation operations in existing translation distance based KGE methods. To\ntackle the challenge, we propose a translation distance-based KGE method called\nSpaceE to model relations as linear transformations. The proposed SpaceE embeds\nboth entities and relations in knowledge graphs as matrices and SpaceE\nnaturally models non-injective relations with singular linear transformations.\nWe theoretically demonstrate that SpaceE is a fully expressive model with the\nability to infer multiple desired relation patterns, including symmetry,\nskew-symmetry, inversion, Abelian composition, and non-Abelian composition.\nExperimental results on link prediction datasets illustrate that SpaceE\nsubstantially outperforms many previous translation distance based knowledge\ngraph embedding methods, especially on datasets with many non-injective\nrelations. The code is available based on the PaddlePaddle deep learning\nplatform https://www.paddlepaddle.org.cn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jinxing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yunfeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mingming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Conservative are Language Models? Adapting to the Introduction of Gender-Neutral Pronouns. (arXiv:2204.10281v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10281","description":"<p>Gender-neutral pronouns have recently been introduced in many languages to a)\ninclude non-binary people and b) as a generic singular. Recent results from\npsycho-linguistics suggest that gender-neutral pronouns (in Swedish) are not\nassociated with human processing difficulties. This, we show, is in sharp\ncontrast with automated processing. We show that gender-neutral pronouns in\nDanish, English, and Swedish are associated with higher perplexity, more\ndispersed attention patterns, and worse downstream performance. We argue that\nsuch conservativity in language models may limit widespread adoption of\ngender-neutral pronouns and must therefore be resolved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brandl_S/0/1/0/all/0/1\">Stephanie Brandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1\">Ruixiang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Revise References for Faithful Summarization. (arXiv:2204.10290v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10290","description":"<p>In many real-world scenarios with naturally occurring datasets, reference\nsummaries are noisy and contain information that cannot be inferred from the\nsource text. On large news corpora, removing low quality samples has been shown\nto reduce model hallucinations. Yet, this method is largely untested for\nsmaller, noisier corpora. To improve reference quality while retaining all\ndata, we propose a new approach: to revise--not remove--unsupported reference\ncontent. Without ground-truth supervision, we construct synthetic unsupported\nalternatives to supported sentences and use contrastive learning to\ndiscourage/encourage (un)faithful revisions. At inference, we vary style codes\nto over-generate revisions of unsupported reference sentences and select a\nfinal revision which balances faithfulness and abstraction. We extract a small\ncorpus from a noisy source--the Electronic Health Record (EHR)--for the task of\nsummarizing a hospital admission from multiple notes. Training models on\noriginal, filtered, and revised references, we find (1) learning from revised\nreferences reduces the hallucination rate substantially more than filtering\n(18.4\\% vs 3.8\\%), (2) learning from abstractive (vs extractive) revisions\nimproves coherence, relevance, and faithfulness, (3) beyond redress of noisy\ndata, the revision task has standalone value for the task: as a pre-training\nobjective and as a post-hoc editor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adams_G/0/1/0/all/0/1\">Griffin Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shing_H/0/1/0/all/0/1\">Han-Chin Shing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winestock_C/0/1/0/all/0/1\">Christopher Winestock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhadad_N/0/1/0/all/0/1\">No&#xe9;mie Elhadad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hierarchical N-Gram Framework for Zero-Shot Link Prediction. (arXiv:2204.10293v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10293","description":"<p>Due to the incompleteness of knowledge graphs (KGs), zero-shot link\nprediction (ZSLP) which aims to predict unobserved relations in KGs has\nattracted recent interest from researchers. A common solution is to use textual\nfeatures of relations (e.g., surface name or textual descriptions) as auxiliary\ninformation to bridge the gap between seen and unseen relations. Current\napproaches learn an embedding for each word token in the text. These methods\nlack robustness as they suffer from the out-of-vocabulary (OOV) problem.\nMeanwhile, models built on character n-grams have the capability of generating\nexpressive representations for OOV words. Thus, in this paper, we propose a\nHierarchical N-Gram framework for Zero-Shot Link Prediction (HNZSLP), which\nconsiders the dependencies among character n-grams of the relation surface name\nfor ZSLP. Our approach works by first constructing a hierarchical n-gram graph\non the surface name to model the organizational structure of n-grams that leads\nto the surface name. A GramTransformer, based on the Transformer is then\npresented to model the hierarchical n-gram graph to construct the relation\nembedding for ZSLP. Experimental results show the proposed HNZSLP achieved\nstate-of-the-art performance on two ZSLP datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingchen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junfan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensah_S/0/1/0/all/0/1\">Samuel Mensah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiulong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yang Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings. (arXiv:2204.10298v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10298","description":"<p>We propose DiffCSE, an unsupervised contrastive learning framework for\nlearning sentence embeddings. DiffCSE learns sentence embeddings that are\nsensitive to the difference between the original sentence and an edited\nsentence, where the edited sentence is obtained by stochastically masking out\nthe original sentence and then sampling from a masked language model. We show\nthat DiffSCE is an instance of equivariant contrastive learning (Dangovski et\nal., 2021), which generalizes contrastive learning and learns representations\nthat are insensitive to certain types of augmentations and sensitive to other\n\"harmful\" types of augmentations. Our experiments show that DiffCSE achieves\nstate-of-the-art results among unsupervised sentence representation learning\nmethods, outperforming unsupervised SimCSE by 2.3 absolute points on semantic\ntextual similarity tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Sung Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dangovski_R/0/1/0/all/0/1\">Rumen Dangovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hongyin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soljacic_M/0/1/0/all/0/1\">Marin Solja&#x10d;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lessons on Parameter Sharing across Layers in Transformers. (arXiv:2104.06022v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06022","description":"<p>We propose a parameter sharing method for Transformers (Vaswani et al.,\n2017). The proposed approach relaxes a widely used technique, which shares\nparameters for one layer with all layers such as Universal Transformers\n(Dehghani et al., 2019), to increase the efficiency in the computational time.\nWe propose three strategies: Sequence, Cycle, and Cycle (rev) to assign\nparameters to each layer. Experimental results show that the proposed\nstrategies are efficient in the parameter size and computational time.\nMoreover, we indicate that the proposed strategies are also effective in the\nconfiguration where we use many training data such as the recent WMT\ncompetition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takase_S/0/1/0/all/0/1\">Sho Takase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiyono_S/0/1/0/all/0/1\">Shun Kiyono</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Out of Context: A New Clue for Context Modeling of Aspect-based Sentiment Analysis. (arXiv:2106.10816v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.10816","description":"<p>Aspect-based sentiment analysis (ABSA) aims to predict the sentiment\nexpressed in a review with respect to a given aspect. The core of ABSA is to\nmodel the interaction between the context and given aspect to extract the\naspect-related information. In prior work, attention mechanisms and dependency\ngraph networks are commonly adopted to capture the relations between the\ncontext and given aspect. And the weighted sum of context hidden states is used\nas the final representation fed to the classifier. However, the information\nrelated to the given aspect may be already discarded and adverse information\nmay be retained in the context modeling processes of existing models. This\nproblem cannot be solved by subsequent modules and there are two reasons:\nfirst, their operations are conducted on the encoder-generated context hidden\nstates, whose value cannot change after the encoder; second, existing encoders\nonly consider the context while not the given aspect. To address this problem,\nwe argue the given aspect should be considered as a new clue out of context in\nthe context modeling process. As for solutions, we design several aspect-aware\ncontext encoders based on different backbones: an aspect-aware LSTM and three\naspect-aware BERTs. They are dedicated to generate aspect-aware hidden states\nwhich are tailored for ABSA task. In these aspect-aware context encoders, the\nsemantics of the given aspect is used to regulate the information flow.\nConsequently, the aspect-related information can be retained and\naspect-irrelevant information can be excluded in the generated hidden states.\nWe conduct extensive experiments on several benchmark datasets with empirical\nanalysis, demonstrating the efficacies and advantages of our proposed\naspect-aware context encoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_B/0/1/0/all/0/1\">Bowen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor W. Tsang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoMPM: Context Modeling with Speaker's Pre-trained Memory Tracking for Emotion Recognition in Conversation. (arXiv:2108.11626v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11626","description":"<p>As the use of interactive machines grow, the task of Emotion Recognition in\nConversation (ERC) became more important. If the machine-generated sentences\nreflect emotion, more human-like sympathetic conversations are possible. Since\nemotion recognition in conversation is inaccurate if the previous utterances\nare not taken into account, many studies reflect the dialogue context to\nimprove the performances. Many recent approaches show performance improvement\nby combining knowledge into modules learned from external structured data.\nHowever, structured data is difficult to access in non-English languages,\nmaking it difficult to extend to other languages. Therefore, we extract the\npre-trained memory using the pre-trained language model as an extractor of\nexternal knowledge. We introduce CoMPM, which combines the speaker's\npre-trained memory with the context model, and find that the pre-trained memory\nsignificantly improves the performance of the context model. CoMPM achieves the\nfirst or second performance on all data and is state-of-the-art among systems\nthat do not leverage structured data. In addition, our method shows that it can\nbe extended to other languages because structured knowledge is not required,\nunlike previous methods. Our code is available on github\n(https://github.com/rungjoo/CoMPM).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wooin Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization. (arXiv:2108.13684v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13684","description":"<p>Despite recent progress in abstractive summarization, systems still suffer\nfrom faithfulness errors. While prior work has proposed models that improve\nfaithfulness, it is unclear whether the improvement comes from an increased\nlevel of extractiveness of the model outputs as one naive way to improve\nfaithfulness is to make summarization models more extractive. In this work, we\npresent a framework for evaluating the effective faithfulness of summarization\nsystems, by generating a faithfulnessabstractiveness trade-off curve that\nserves as a control at different operating points on the abstractiveness\nspectrum. We then show that the Maximum Likelihood Estimation (MLE) baseline as\nwell as a recently proposed method for improving faithfulness, are both worse\nthan the control at the same level of abstractiveness. Finally, we learn a\nselector to identify the most faithful and abstractive summary for a given\ndocument, and show that this system can attain higher faithfulness scores in\nhuman evaluations while being more abstractive than the baseline system on two\ndatasets. Moreover, we show that our system is able to achieve a better\nfaithfulness-abstractiveness trade-off than the control at the same level of\nabstractiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ladhak_F/0/1/0/all/0/1\">Faisal Ladhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1\">Claire Cardie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Prompt-Based Models Really Understand the Meaning of their Prompts?. (arXiv:2109.01247v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01247","description":"<p>Recently, a boom of papers has shown extraordinary progress in zero-shot and\nfew-shot learning with various prompt-based models. It is commonly argued that\nprompts help models to learn faster in the same way that humans learn faster\nwhen provided with task instructions expressed in natural language. In this\nstudy, we experiment with over 30 prompt templates manually written for natural\nlanguage inference (NLI). We find that models learn just as fast with many\nprompts that are intentionally irrelevant or even pathologically misleading as\nthey do with instructively \"good\" prompts. Further, such patterns hold even for\nmodels as large as 175 billion parameters (Brown et al., 2020) as well as the\nrecently proposed instruction-tuned models which are trained on hundreds of\nprompts (Sanh et al., 2022). That is, instruction-tuned models often produce\ngood predictions with irrelevant and misleading prompts even at zero shots. In\nsum, notwithstanding prompt-based models' impressive improvement, we find\nevidence of serious limitations that question the degree to which such\nimprovement is derived from models understanding task instructions in ways\nanalogous to humans' use of task instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Packed Levitated Marker for Entity and Relation Extraction. (arXiv:2109.06067v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06067","description":"<p>Recent entity and relation extraction works focus on investigating how to\nobtain a better span representation from the pre-trained encoder. However, a\nmajor limitation of existing works is that they ignore the interrelation\nbetween spans (pairs). In this work, we propose a novel span representation\napproach, named Packed Levitated Markers (PL-Marker), to consider the\ninterrelation between the spans (pairs) by strategically packing the markers in\nthe encoder. In particular, we propose a neighborhood-oriented packing\nstrategy, which considers the neighbor spans integrally to better model the\nentity boundary information. Furthermore, for those more complicated span pair\nclassification tasks, we design a subject-oriented packing strategy, which\npacks each subject and all its objects to model the interrelation between the\nsame-subject span pairs. The experimental results show that, with the enhanced\nmarker feature, our model advances baselines on six NER benchmarks, and obtains\na 4.1%-4.3% strict relation F1 improvement with higher speed over previous\nstate-of-the-art models on ACE04 and ACE05.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1\">Deming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Attention Sparsity in Transformers. (arXiv:2109.12188v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12188","description":"<p>Transformers' quadratic complexity with respect to the input sequence length\nhas motivated a body of work on efficient sparse approximations to softmax. An\nalternative path, used by entmax transformers, consists of having built-in\nexact sparse attention; however this approach still requires quadratic\ncomputation. In this paper, we propose Sparsefinder, a simple model trained to\nidentify the sparsity pattern of entmax attention before computing it. We\nexperiment with three variants of our method, based on distances, quantization,\nand clustering, on two tasks: machine translation (attention in the decoder)\nand masked language modeling (encoder-only). Our work provides a new angle to\nstudy model efficiency by doing extensive analysis of the tradeoff between the\nsparsity and recall of the predicted attention graph. This allows for detailed\ncomparison between different models along their Pareto curves, important to\nguide future benchmarks for sparse attention models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Treviso_M/0/1/0/all/0/1\">Marcos Treviso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gois_A/0/1/0/all/0/1\">Ant&#xf3;nio G&#xf3;is</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_E/0/1/0/all/0/1\">Erick Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Sequence Training of Attention Models using Approximative Recombination. (arXiv:2110.09245v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.09245","description":"<p>Sequence discriminative training is a great tool to improve the performance\nof an automatic speech recognition system. It does, however, necessitate a sum\nover all possible word sequences, which is intractable to compute in practice.\nCurrent state-of-the-art systems with unlimited label context circumvent this\nproblem by limiting the summation to an n-best list of relevant competing\nhypotheses obtained from beam search.\n</p>\n<p>This work proposes to perform (approximative) recombinations of hypotheses\nduring beam search, if they share a common local history. The error that is\nincurred by the approximation is analyzed and it is shown that using this\ntechnique the effective beam size can be increased by several orders of\nmagnitude without significantly increasing the computational requirements.\nLastly, it is shown that this technique can be used to effectively perform\nsequence discriminative training for attention-based encoder-decoder acoustic\nmodels on the LibriSpeech task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wynands_N/0/1/0/all/0/1\">Nils-Philipp Wynands</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_W/0/1/0/all/0/1\">Wilfried Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosendahl_J/0/1/0/all/0/1\">Jan Rosendahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surfer100: Generating Surveys From Web Resources on Wikipedia-style. (arXiv:2112.06377v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06377","description":"<p>Fast-developing fields such as Artificial Intelligence (AI) often outpace the\nefforts of encyclopedic sources such as Wikipedia, which either do not\ncompletely cover recently-introduced topics or lack such content entirely. As a\nresult, methods for automatically producing content are valuable tools to\naddress this information overload. We show that recent advances in pretrained\nlanguage modeling can be combined for a two-stage extractive and abstractive\napproach for Wikipedia lead paragraph generation. We extend this approach to\ngenerate longer Wikipedia-style summaries with sections and examine how such\nmethods struggle in this application through detailed studies with 100\nreference human-collected surveys. This is the first study on utilizing web\nresources for long Wikipedia-style summaries to the best of our knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawamura_R/0/1/0/all/0/1\">Rina Kawamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tae_J/0/1/0/all/0/1\">Jaesung Tae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Sally Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mizutani_T/0/1/0/all/0/1\">Tomoe Mizutani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Curriculum Learning for Emotion Recognition in Conversation. (arXiv:2112.11718v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.11718","description":"<p>Emotion recognition in conversation (ERC) aims to detect the emotion label\nfor each utterance. Motivated by recent studies which have proven that feeding\ntraining examples in a meaningful order rather than considering them randomly\ncan boost the performance of models, we propose an ERC-oriented hybrid\ncurriculum learning framework. Our framework consists of two curricula: (1)\nconversation-level curriculum (CC); and (2) utterance-level curriculum (UC). In\nCC, we construct a difficulty measurer based on \"emotion shift\" frequency\nwithin a conversation, then the conversations are scheduled in an \"easy to\nhard\" schema according to the difficulty score returned by the difficulty\nmeasurer. For UC, it is implemented from an emotion-similarity perspective,\nwhich progressively strengthens the model's ability in identifying the\nconfusing emotions. With the proposed model-agnostic hybrid curriculum learning\nstrategy, we observe significant performance boosts over a wide range of\nexisting ERC models and we are able to achieve new state-of-the-art results on\nfour public ERC datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yue Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Longjun Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiased Learning from Naturally Imbalanced Pseudo-Labels. (arXiv:2201.01490v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.01490","description":"<p>Pseudo-labels are confident predictions made on unlabeled target data by a\nclassifier trained on labeled source data. They are widely used for adapting a\nmodel to unlabeled data, e.g., in a semi-supervised learning setting.\n</p>\n<p>Our key insight is that pseudo-labels are naturally imbalanced due to\nintrinsic data similarity, even when a model is trained on balanced source data\nand evaluated on balanced target data. If we address this previously unknown\nimbalanced classification problem arising from pseudo-labels instead of\nground-truth training labels, we could remove model biases towards false\nmajorities created by pseudo-labels.\n</p>\n<p>We propose a novel and effective debiased learning method with pseudo-labels,\nbased on counterfactual reasoning and adaptive margins: The former removes the\nclassifier response bias, whereas the latter adjusts the margin of each class\naccording to the imbalance of pseudo-labels. Validated by extensive\nexperimentation, our simple debiased learning delivers significant accuracy\ngains over the state-of-the-art on ImageNet-1K: 26% for semi-supervised\nlearning with 0.2% annotations and 9% for zero-shot learning. Our code is\navailable at: https://github.com/frank-xwang/debiased-pseudo-labeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_L/0/1/0/all/0/1\">Long Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Czech Grammar Error Correction with a Large and Diverse Corpus. (arXiv:2201.05590v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05590","description":"<p>We introduce a large and diverse Czech corpus annotated for grammatical error\ncorrection (GEC) with the aim to contribute to the still scarce data resources\nin this domain for languages other than English. The Grammar Error Correction\nCorpus for Czech (GECCC) offers a variety of four domains, covering error\ndistributions ranging from high error density essays written by non-native\nspeakers, to website texts, where errors are expected to be much less common.\nWe compare several Czech GEC systems, including several Transformer-based ones,\nsetting a strong baseline to future research. Finally, we meta-evaluate common\nGEC metrics against human judgements on our data. We make the new Czech GEC\ncorpus publicly available under the CC BY-SA 4.0 license at\n<a href=\"http://hdl.handle.net/11234/1-4639\">this http URL</a> .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naplava_J/0/1/0/all/0/1\">Jakub N&#xe1;plava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Straka_M/0/1/0/all/0/1\">Milan Straka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strakova_J/0/1/0/all/0/1\">Jana Strakov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosen_A/0/1/0/all/0/1\">Alexandr Rosen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation. (arXiv:2201.05955v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05955","description":"<p>A recurring challenge of crowdsourcing NLP datasets at scale is that human\nwriters often rely on repetitive patterns when crafting examples, leading to a\nlack of linguistic diversity. We introduce a novel approach for dataset\ncreation based on worker and AI collaboration, which brings together the\ngenerative strength of language models and the evaluative strength of humans.\nStarting with an existing dataset, MultiNLI for natural language inference\n(NLI), our approach uses dataset cartography to automatically identify examples\nthat demonstrate challenging reasoning patterns, and instructs GPT-3 to compose\nnew examples with similar patterns. Machine generated examples are then\nautomatically filtered, and finally revised and labeled by human crowdworkers.\nThe resulting dataset, WANLI, consists of 108,079 NLI examples and presents\nunique empirical strengths over existing NLI datasets. Remarkably, training a\nmodel on WANLI instead of MultiNLI (which is $4$ times larger) improves\nperformance on seven out-of-domain test sets we consider, including by 11% on\nHANS and 9% on Adversarial NLI. Moreover, combining MultiNLI with WANLI is more\neffective than combining it with other NLI augmentation sets. Our results\ndemonstrate the potential of natural language generation techniques to curate\nNLP datasets of enhanced quality and diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alisa Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiFSMN: Binary Neural Network for Keyword Spotting. (arXiv:2202.06483v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.06483","description":"<p>The deep neural networks, such as the Deep-FSMN, have been widely studied for\nkeyword spotting (KWS) applications. However, computational resources for these\nnetworks are significantly constrained since they usually run on-call on edge\ndevices. In this paper, we present BiFSMN, an accurate and extreme-efficient\nbinary neural network for KWS. We first construct a High-frequency Enhancement\nDistillation scheme for the binarization-aware training, which emphasizes the\nhigh-frequency information from the full-precision network's representation\nthat is more crucial for the optimization of the binarized network. Then, to\nallow the instant and adaptive accuracy-efficiency trade-offs at runtime, we\nalso propose a Thinnable Binarization Architecture to further liberate the\nacceleration potential of the binarized network from the topology perspective.\nMoreover, we implement a Fast Bitwise Computation Kernel for BiFSMN on ARMv8\ndevices which fully utilizes registers and increases instruction throughput to\npush the limit of deployment efficiency. Extensive experiments show that BiFSMN\noutperforms existing binarization methods by convincing margins on various\ndatasets and is even comparable with the full-precision counterpart (e.g., less\nthan 3% drop on Speech Commands V1-12). We highlight that benefiting from the\nthinnable architecture and the optimized 1-bit implementation, BiFSMN can\nachieve an impressive 22.3x speedup and 15.5x storage-saving on real-world edge\nhardware.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Haotong Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xudong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Extractive Opinion Summarization Using Sparse Coding. (arXiv:2203.07921v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07921","description":"<p>Opinion summarization is the task of automatically generating summaries that\nencapsulate information from multiple user reviews. We present Semantic\nAutoencoder (SemAE) to perform extractive opinion summarization in an\nunsupervised manner. SemAE uses dictionary learning to implicitly capture\nsemantic information from the review and learns a latent representation of each\nsentence over semantic units. A semantic unit is supposed to capture an\nabstract semantic concept. Our extractive summarization algorithm leverages the\nrepresentations to identify representative opinions among hundreds of reviews.\nSemAE is also able to perform controllable summarization to generate\naspect-specific summaries. We report strong performance on SPACE and AMAZON\ndatasets, and perform experiments to investigate the functioning of our model.\nOur code is publicly available at https://github.com/brcsomnath/SemAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Somnath Basu Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses. (arXiv:2203.10750v4 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.10750","description":"<p>In this paper, we develop a new multi-singer Chinese neural singing voice\nsynthesis (SVS) system named WeSinger. To improve the accuracy and naturalness\nof synthesized singing voice, we design several specifical modules and\ntechniques: 1) A deep bi-directional LSTM based duration model with multi-scale\nrhythm loss and post-processing step; 2) A Transformer-alike acoustic model\nwith progressive pitch-weighted decoder loss; 3) a 24 kHz pitch-aware LPCNet\nneural vocoder to produce high-quality singing waveforms; 4) A novel data\naugmentation method with multi-singer pre-training for stronger robustness and\nnaturalness. To our knowledge, WeSinger is the first SVS system to adopt 24 kHz\nLPCNet and multi-singer pre-training simultaneously. Both quantitative and\nqualitative evaluation results demonstrate the effectiveness of WeSinger in\nterms of accuracy and naturalness, and WeSinger achieves state-of-the-art\nperformance on the recently public Chinese singing corpus Opencpop. Some\nsynthesized singing samples are available online\n(https://zzw922cn.github.io/wesinger/).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zewang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yibin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinhui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Li Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometry-Aware Supertagging with Heterogeneous Dynamic Convolutions. (arXiv:2203.12235v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12235","description":"<p>The syntactic categories of categorial grammar formalisms are structured\nunits made of smaller, indivisible primitives, bound together by the underlying\ngrammar's category formation rules. In the trending approach of constructive\nsupertagging, neural models are increasingly made aware of the internal\ncategory structure, which in turn enables them to more reliably predict rare\nand out-of-vocabulary categories, with significant implications for grammars\npreviously deemed too complex to find practical use. In this work, we revisit\nconstructive supertagging from a graph-theoretic perspective, and propose a\nframework based on heterogeneous dynamic graph convolutions aimed at exploiting\nthe distinctive structure of a supertagger's output space. We test our approach\non a number of categorial grammar datasets spanning different languages and\ngrammar formalisms, achieving substantial improvements over previous state of\nthe art scores. Code will be made available at\nhttps://github.com/konstantinosKokos/dynamic-graph-supertagging\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kogkalidis_K/0/1/0/all/0/1\">Konstantinos Kogkalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moortgat_M/0/1/0/all/0/1\">Michael Moortgat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Brazilian Court Documents Clustered by Similarity Together Using Natural Language Processing Approaches with Transformers. (arXiv:2204.07182v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2204.07182","description":"<p>Recent advances in Artificial intelligence (AI) have leveraged promising\nresults in solving complex problems in the area of Natural Language Processing\n(NLP), being an important tool to help in the expeditious resolution of\njudicial proceedings in the legal area. In this context, this work targets the\nproblem of detecting the degree of similarity between judicial documents that\ncan be achieved in the inference group, by applying six NLP techniques based on\ntransformers, namely BERT, GPT-2 and RoBERTa pre-trained in the Brazilian\nPortuguese language and the same specialized using 210,000 legal proceedings.\nDocuments were pre-processed and had their content transformed into a vector\nrepresentation using these NLP techniques. Unsupervised learning was used to\ncluster the lawsuits, calculating the quality of the model based on the cosine\nof the distance between the elements of the group to its centroid. We noticed\nthat models based on transformers present better performance when compared to\nprevious research, highlighting the RoBERTa model specialized in the Brazilian\nPortuguese language, making it possible to advance in the current state of the\nart in the area of NLP applied to the legal sector.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_R/0/1/0/all/0/1\">Raphael Souza de Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nascimento_E/0/1/0/all/0/1\">Erick Giovani Sperandio Nascimento</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate detection of sepsis at ED triage using machine learning with clinical natural language processing. (arXiv:2204.07657v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.07657","description":"<p>Sepsis is a life-threatening condition with organ dysfunction and is a\nleading cause of death and critical illness worldwide. Accurate detection of\nsepsis during emergency department triage would allow early initiation of lab\nanalysis, antibiotic administration, and other sepsis treatment protocols. The\npurpose of this study was to determine whether EHR data can be extracted and\nsynthesized with the latest machine learning algorithms (KATE Sepsis) and\nclinical natural language processing to produce accurate sepsis models, and\ncompare KATE Sepsis performance with existing sepsis screening protocols, such\nas SIRS and qSOFA. A machine learning model (KATE Sepsis) was developed using\npatient encounters with triage data from 16 participating hospitals. KATE\nSepsis, SIRS, standard screening (SIRS with source of infection) and qSOFA were\ntested in three settings. Cohort-A was a retrospective analysis on medical\nrecords from a single Site 1. Cohort-B was a prospective analysis of Site 1.\nCohort-C was a retrospective analysis on Site 1 with 15 additional sites.\nAcross all cohorts, KATE Sepsis demonstrates an AUC of 0.94-0.963 with\n73-74.87% TPR and 3.76-7.17% FPR. Standard screening demonstrates an AUC of\n0.682-0.726 with 39.39-51.19% TPR and 2.9-6.02% FPR. The qSOFA protocol\ndemonstrates an AUC of 0.544-0.56, with 10.52-13.18% TPR and 1.22-1.68% FPR.\nFor severe sepsis, across all cohorts, KATE Sepsis demonstrates an AUC of\n0.935-0.972 with 70-82.26% TPR and 4.64-8.62% FPR. For septic shock, across all\ncohorts, KATE Sepsis demonstrates an AUC of 0.96-0.981 with 85.71-89.66% TPR\nand 4.85-8.8% FPR. SIRS, standard screening, and qSOFA demonstrate low AUC and\nTPR for severe sepsis and septic shock detection. KATE Sepsis provided\nsubstantially better sepsis detection performance in triage than commonly used\nscreening protocols.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivanov_O/0/1/0/all/0/1\">Oleksandr Ivanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molander_K/0/1/0/all/0/1\">Karin Molander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunne_R/0/1/0/all/0/1\">Robert Dunne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Stephen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masek_K/0/1/0/all/0/1\">Kevin Masek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_E/0/1/0/all/0/1\">Erica Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lisa Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Travers_D/0/1/0/all/0/1\">Debbie Travers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brecher_D/0/1/0/all/0/1\">Deena Brecher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delaney_D/0/1/0/all/0/1\">Deb Delaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montgomery_K/0/1/0/all/0/1\">Kyla Montgomery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reilly_C/0/1/0/all/0/1\">Christian Reilly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Persua: A Visual Interactive System to Enhance the Persuasiveness of Arguments in Online Discussion. (arXiv:2204.07741v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2204.07741","description":"<p>Persuading people to change their opinions is a common practice in online\ndiscussion forums on topics ranging from political campaigns to relationship\nconsultation. Enhancing people's ability to write persuasive arguments could\nnot only practice their critical thinking and reasoning but also contribute to\nthe effectiveness and civility in online communication. It is, however, not an\neasy task in online discussion settings where written words are the primary\ncommunication channel. In this paper, we derived four design goals for a tool\nthat helps users improve the persuasiveness of arguments in online discussions\nthrough a survey with 123 online forum users and interviews with five debating\nexperts. To satisfy these design goals, we analyzed and built a labeled dataset\nof fine-grained persuasive strategies (i.e., logos, pathos, ethos, and\nevidence) in 164 arguments with high ratings on persuasiveness from\nChangeMyView, a popular online discussion forum. We then designed an\ninteractive visual system, Persua, which provides example-based guidance on\npersuasive strategies to enhance the persuasiveness of arguments. In\nparticular, the system constructs portfolios of arguments based on different\npersuasive strategies applied to a given discussion topic. It then presents\nconcrete examples based on the difference between the portfolios of user input\nand high-quality arguments in the dataset. A between-subjects study shows\nsuggestive evidence that Persua encourages users to submit more times for\nfeedback and helps users improve more on the persuasiveness of their arguments\nthan a baseline system. Finally, a set of design considerations was summarized\nto guide future intelligent systems that improve the persuasiveness in text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1\">Meng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_F/0/1/0/all/0/1\">Fei Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojuan Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis. (arXiv:2204.07955v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07955","description":"<p>As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment\nAnalysis (MABSA) has attracted increasing attention in recent years. However,\nprevious approaches either (i) use separately pre-trained visual and textual\nmodels, which ignore the crossmodal alignment or (ii) use vision-language\nmodels pre-trained with general pre-training tasks, which are inadequate to\nidentify finegrained aspects, opinions, and their alignments across modalities.\nTo tackle these limitations, we propose a task-specific Vision-Language\nPre-training framework for MABSA (VLPMABSA), which is a unified multimodal\nencoder-decoder architecture for all the pretraining and downstream tasks. We\nfurther design three types of task-specific pre-training tasks from the\nlanguage, vision, and multimodal modalities, respectively. Experimental results\nshow that our approach generally outperforms the state-of-the-art approaches on\nthree MABSA subtasks. Further analysis demonstrates the effectiveness of each\npretraining task. The source code is publicly released at\nhttps://github.com/NUSTM/VLP-MABSA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_Y/0/1/0/all/0/1\">Yan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianfei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_R/0/1/0/all/0/1\">Rui Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Psycho-linguistic Analysis of BitChute. (arXiv:2204.08078v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2204.08078","description":"<p>In order to better support researchers, journalist, and practitioners in\ntheir use of the MeLa-BitChute dataset for exploration and investigative\nreporting, we provide new psycho-linguistic metadata for the videos, comments,\nand channels in the dataset using LIWC22. This paper describes that metadata\nand methods to filter the data using the metadata. In addition, we provide\nbasic analysis and comparison of the language on BitChute to other social media\nplatforms. The MeLa-BitChute dataset and LIWC metadata described in this paper\ncan be found at:\nhttps://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/KRD1VS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Horne_B/0/1/0/all/0/1\">Benjamin D. Horne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Execute Actions or Ask Clarification Questions. (arXiv:2204.08373v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08373","description":"<p>Collaborative tasks are ubiquitous activities where a form of communication\nis required in order to reach a joint goal. Collaborative building is one of\nsuch tasks. We wish to develop an intelligent builder agent in a simulated\nbuilding environment (Minecraft) that can build whatever users wish to build by\njust talking to the agent. In order to achieve this goal, such agents need to\nbe able to take the initiative by asking clarification questions when further\ninformation is needed. Existing works on Minecraft Corpus Dataset only learn to\nexecute instructions neglecting the importance of asking for clarifications. In\nthis paper, we extend the Minecraft Corpus Dataset by annotating all builder\nutterances into eight types, including clarification questions, and propose a\nnew builder agent model capable of determining when to ask or execute\ninstructions. Experimental results show that our model achieves\nstate-of-the-art performance on the collaborative building task with a\nsubstantial improvement. We also define two new tasks, the learning to ask task\nand the joint learning task. The latter consists of solving both collaborating\nbuilding and learning to ask tasks jointly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhengxiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yue Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1\">Aldo Lipani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing for the Usage of Grammatical Number. (arXiv:2204.08831v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08831","description":"<p>A central quest of probing is to uncover how pre-trained models encode a\nlinguistic property within their representations. An encoding, however, might\nbe spurious-i.e., the model might not rely on it when making predictions. In\nthis paper, we try to find encodings that the model actually uses, introducing\na usage-based probing setup. We first choose a behavioral task which cannot be\nsolved without using the linguistic property. Then, we attempt to remove the\nproperty by intervening on the model's representations. We contend that, if an\nencoding is used by the model, its removal should harm the performance on the\nchosen behavioral task. As a case study, we focus on how BERT encodes\ngrammatical number, and on how it uses this encoding to solve the number\nagreement task. Experimentally, we find that BERT relies on a linear encoding\nof grammatical number to produce the correct behavioral output. We also find\nthat BERT uses a separate encoding of grammatical number for nouns and verbs.\nFinally, we identify in which layers information about grammatical number is\ntransferred from a noun to its head verb.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lasri_K/0/1/0/all/0/1\">Karim Lasri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenci_A/0/1/0/all/0/1\">Alessandro Lenci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poibeau_T/0/1/0/all/0/1\">Thierry Poibeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Situational Perception Guided Image Matting. (arXiv:2204.09276v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09276","description":"<p>Most automatic matting methods try to separate the salient foreground from\nthe background. However, the insufficient quantity and subjective bias of the\ncurrent existing matting datasets make it difficult to fully explore the\nsemantic association between object-to-object and object-to-environment in a\ngiven image. In this paper, we propose a Situational Perception Guided Image\nMatting (SPG-IM) method that mitigates subjective bias of matting annotations\nand captures sufficient situational perception information for better global\nsaliency distilled from the visual-to-textual task. SPG-IM can better associate\ninter-objects and object-to-environment saliency, and compensate the subjective\nnature of image matting and its expensive annotation. We also introduce a\ntextual Semantic Transformation (TST) module that can effectively transform and\nintegrate the semantic feature stream to guide the visual representations. In\naddition, an Adaptive Focal Transformation (AFT) Refinement Network is proposed\nto adaptively switch multi-scale receptive fields and focal points to enhance\nboth global and local details. Extensive experiments demonstrate the\neffectiveness of situational perception guidance from the visual-to-textual\ntasks on image matting, and our model outperforms the state-of-the-art methods.\nWe also analyze the significance of different components in our model. The code\nwill be released soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiake Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Continuous Integrate-and-Fire for Adaptive Simultaneous Speech Translation. (arXiv:2204.09595v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.09595","description":"<p>Simultaneous speech translation (SimulST) is a challenging task aiming to\ntranslate streaming speech before the complete input is observed. A SimulST\nsystem generally includes two components: the pre-decision that aggregates the\nspeech information and the policy that decides to read or write. While recent\nworks had proposed various strategies to improve the pre-decision, they mainly\nadopt the fixed wait-k policy, leaving the adaptive policies rarely explored.\nThis paper proposes to model the adaptive policy by adapting the Continuous\nIntegrate-and-Fire (CIF). Compared with monotonic multihead attention (MMA),\nour method has the advantage of simpler computation, superior quality at low\nlatency, and better generalization to long utterances. We conduct experiments\non the MuST-C V2 dataset and show the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chih-Chiang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceiving the World: Question-guided Reinforcement Learning for Text-based Games. (arXiv:2204.09597v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.09597","description":"<p>Text-based games provide an interactive way to study natural language\nprocessing. While deep reinforcement learning has shown effectiveness in\ndeveloping the game playing agent, the low sample efficiency and the large\naction space remain to be the two major challenges that hinder the DRL from\nbeing applied in the real world. In this paper, we address the challenges by\nintroducing world-perceiving modules, which automatically decompose tasks and\nprune actions by answering questions about the environment. We then propose a\ntwo-phase training framework to decouple language learning from reinforcement\nlearning, which further improves the sample efficiency. The experimental\nresults show that the proposed method significantly improves the performance\nand sample efficiency. Besides, it shows robustness against compound error and\nlimited pre-training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yunqiu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Ling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yali Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengqi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Spatially-Preserving Flattening for Location-Aware Classification of Findings in Chest X-Rays. (arXiv:2204.09676v1 [eess.IV])","link":"http://arxiv.org/abs/2204.09676","description":"<p>Chest X-rays have become the focus of vigorous deep learning research in\nrecent years due to the availability of large labeled datasets. While\nclassification of anomalous findings is now possible, ensuring that they are\ncorrectly localized still remains challenging, as this requires recognition of\nanomalies within anatomical regions. Existing deep learning networks for\nfine-grained anomaly classification learn location-specific findings using\narchitectures where the location and spatial contiguity information is lost\nduring the flattening step before classification. In this paper, we present a\nnew spatially preserving deep learning network that preserves location and\nshape information through auto-encoding of feature maps during flattening. The\nfeature maps, auto-encoder and classifier are then trained in an end-to-end\nfashion to enable location aware classification of findings in chest X-rays.\nResults are shown on a large multi-hospital chest X-ray dataset indicating a\nsignificant improvement in the quality of finding classification over\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Srivathsa_N/0/1/0/all/0/1\">Neha Srivathsa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahmood_R/0/1/0/all/0/1\">Razi Mahmood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Syeda_Mahmood_T/0/1/0/all/0/1\">Tanveer Syeda-Mahmood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FS-NCSR: Increasing Diversity of the Super-Resolution Space via Frequency Separation and Noise-Conditioned Normalizing Flow. (arXiv:2204.09679v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09679","description":"<p>Super-resolution suffers from an innate ill-posed problem that a single\nlow-resolution (LR) image can be from multiple high-resolution (HR) images.\nRecent studies on the flow-based algorithm solve this ill-posedness by learning\nthe super-resolution space and predicting diverse HR outputs. Unfortunately,\nthe diversity of the super-resolution outputs is still unsatisfactory, and the\noutputs from the flow-based model usually suffer from undesired artifacts which\ncauses low-quality outputs. In this paper, we propose FS-NCSR which produces\ndiverse and high-quality super-resolution outputs using frequency separation\nand noise conditioning compared to the existing flow-based approaches. As the\nsharpness and high-quality detail of the image rely on its high-frequency\ninformation, FS-NCSR only estimates the high-frequency information of the\nhigh-resolution outputs without redundant low-frequency components. Through\nthis, FS-NCSR significantly improves the diversity score without significant\nimage quality degradation compared to the NCSR, the winner of the previous\nNTIRE 2021 challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Ki-Ung Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_D/0/1/0/all/0/1\">Dongseok Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kang-wook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jae-young Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Younggeun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complete identification of complex salt-geometries from inaccurate migrated images using Deep Learning. (arXiv:2204.09710v1 [physics.geo-ph])","link":"http://arxiv.org/abs/2204.09710","description":"<p>Delimiting salt inclusions from migrated images is a time-consuming activity\nthat relies on highly human-curated analysis and is subject to interpretation\nerrors or limitations of the methods available. We propose to use migrated\nimages produced from an inaccurate velocity model (with a reasonable\napproximation of sediment velocity, but without salt inclusions) to predict the\ncorrect salt inclusions shape using a Convolutional Neural Network (CNN). Our\napproach relies on subsurface Common Image Gathers to focus the sediments'\nreflections around the zero offset and to spread the energy of salt reflections\nover large offsets. Using synthetic data, we trained a U-Net to use\ncommon-offset subsurface images as input channels for the CNN and the correct\nsalt-masks as network output. The network learned to predict the salt\ninclusions masks with high accuracy; moreover, it also performed well when\napplied to synthetic benchmark data sets that were not previously introduced.\nOur training process tuned the U-Net to successfully learn the shape of complex\nsalt bodies from partially focused subsurface offset images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Muller_A/0/1/0/all/0/1\">Ana Paula O.Muller</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Costa_J/0/1/0/all/0/1\">Jess&#xe9; C. Costa</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bom_C/0/1/0/all/0/1\">Clecio R. Bom</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Faria_E/0/1/0/all/0/1\">Elisangela L. Faria</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Klatt_M/0/1/0/all/0/1\">Matheus Klatt</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Teixeira_G/0/1/0/all/0/1\">Gabriel Teixeira</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Albuquerque_M/0/1/0/all/0/1\">Marcelo P. de Albuquerque</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Albuquerque_M/0/1/0/all/0/1\">Marcio P. de Albuquerque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Decoders with MultiModal Regularization for Cross-Modal Food Retrieval. (arXiv:2204.09730v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09730","description":"<p>Cross-modal image-recipe retrieval has gained significant attention in recent\nyears. Most work focuses on improving cross-modal embeddings using unimodal\nencoders, that allow for efficient retrieval in large-scale databases, leaving\naside cross-attention between modalities which is more computationally\nexpensive. We propose a new retrieval framework, T-Food (Transformer Decoders\nwith MultiModal Regularization for Cross-Modal Food Retrieval) that exploits\nthe interaction between modalities in a novel regularization scheme, while\nusing only unimodal encoders at test time for efficient retrieval. We also\ncapture the intra-dependencies between recipe entities with a dedicated recipe\nencoder, and propose new variants of triplet losses with dynamic margins that\nadapt to the difficulty of the task. Finally, we leverage the power of the\nrecent Vision and Language Pretraining (VLP) models such as CLIP for the image\nencoder. Our approach outperforms existing approaches by a large margin on the\nRecipe1M dataset. Specifically, we achieve absolute improvements of 8.1 % (72.6\nR@1) and +10.9 % (44.6 R@1) on the 1k and 10k test sets respectively. The code\nis available here:https://github.com/mshukor/TFood\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shukor_M/0/1/0/all/0/1\">Mustafa Shukor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couairon_G/0/1/0/all/0/1\">Guillaume Couairon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grechka_A/0/1/0/all/0/1\">Asya Grechka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time-based Self-supervised Learning for Wireless Capsule Endoscopy. (arXiv:2204.09773v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09773","description":"<p>State-of-the-art machine learning models, and especially deep learning ones,\nare significantly data-hungry; they require vast amounts of manually labeled\nsamples to function correctly. However, in most medical imaging fields,\nobtaining said data can be challenging. Not only the volume of data is a\nproblem, but also the imbalances within its classes; it is common to have many\nmore images of healthy patients than of those with pathology. Computer-aided\ndiagnostic systems suffer from these issues, usually over-designing their\nmodels to perform accurately. This work proposes using self-supervised learning\nfor wireless endoscopy videos by introducing a custom-tailored method that does\nnot initially need labels or appropriate balance. We prove that using the\ninferred inherent structure learned by our method, extracted from the temporal\naxis, improves the detection rate on several domain-specific applications even\nunder severe imbalance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pascual_G/0/1/0/all/0/1\">Guillem Pascual</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laiz_P/0/1/0/all/0/1\">Pablo Laiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_A/0/1/0/all/0/1\">Albert Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenzek_H/0/1/0/all/0/1\">Hagen Wenzek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitria_J/0/1/0/all/0/1\">Jordi Vitri&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segui_S/0/1/0/all/0/1\">Santi Segu&#xed;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention in Reasoning: Dataset, Analysis, and Modeling. (arXiv:2204.09774v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09774","description":"<p>While attention has been an increasingly popular component in deep neural\nnetworks to both interpret and boost the performance of models, little work has\nexamined how attention progresses to accomplish a task and whether it is\nreasonable. In this work, we propose an Attention with Reasoning capability\n(AiR) framework that uses attention to understand and improve the process\nleading to task outcomes. We first define an evaluation metric based on a\nsequence of atomic reasoning operations, enabling a quantitative measurement of\nattention that considers the reasoning process. We then collect human\neye-tracking and answer correctness data, and analyze various machine and human\nattention mechanisms on their reasoning capability and how they impact task\nperformance. To improve the attention and reasoning ability of visual question\nanswering models, we propose to supervise the learning of attention\nprogressively along the reasoning process and to differentiate the correct and\nincorrect attention patterns. We demonstrate the effectiveness of the proposed\nframework in analyzing and modeling attention with better reasoning capability\nand task performance. The code and data are available at\nhttps://github.com/szzexpoi/AiR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Ming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinhui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Focus Image Fusion based on Gradient Transform. (arXiv:2204.09777v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09777","description":"<p>Multi-focus image fusion is a challenging field of study that aims to provide\na completely focused image by integrating focused and un-focused pixels. Most\nexisting methods suffer from shift variance, misregistered images, and\ndata-dependent. In this study, we introduce a novel gradient information-based\nmulti-focus image fusion method that is robust for the aforementioned problems.\nThe proposed method first generates gradient images from original images by\nusing Halftoning-Inverse Halftoning (H-IH) transform. Then, Energy of Gradient\n(EOG) and Standard Deviation functions are used as the focus measurement on the\ngradient images to form a fused image. Finally, in order to enhance the fused\nimage a decision fusion approach is applied with the majority voting method.\nThe proposed method is compared with 17 different novel and conventional\ntechniques both visually and objectively. For objective evaluation, 6 different\nquantitative metrics are used. It is observed that the proposed method is\npromising according to visual evaluation and 83.3% success is achieved by being\nfirst in five out of six metrics according to objective evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Turgut_S/0/1/0/all/0/1\">Sultan Sevgi Turgut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oral_M/0/1/0/all/0/1\">Mustafa Oral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Scale Features and Parallel Transformers Based Image Quality Assessment. (arXiv:2204.09779v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09779","description":"<p>With the increase in multimedia content, the type of distortions associated\nwith multimedia is also increasing. This problem of image quality assessment is\nexpanded well in the PIPAL dataset, which is still an open problem to solve for\nresearchers. Although, recently proposed transformers networks have already\nbeen used in the literature for image quality assessment. At the same time, we\nnotice that multi-scale feature extraction has proven to be a promising\napproach for image quality assessment. However, the way transformer networks\nare used for image quality assessment until now lacks these properties of\nmulti-scale feature extraction. We utilized this fact in our approach and\nproposed a new architecture by integrating these two promising quality\nassessment techniques of images. Our experimentation on various datasets,\nincluding the PIPAL dataset, demonstrates that the proposed integration\ntechnique outperforms existing algorithms. The source code of the proposed\nalgorithm is available online: https://github.com/KomalPal9610/IQA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keshari_A/0/1/0/all/0/1\">Abhisek Keshari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komal/0/1/0/all/0/1\">Komal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadbhawna/0/1/0/all/0/1\">Sadbhawna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subudhi_B/0/1/0/all/0/1\">Badri Subudhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiPathGAN: Structure Preserving Stain Normalization using Unsupervised Multi-domain Adversarial Network with Perception Loss. (arXiv:2204.09782v1 [eess.IV])","link":"http://arxiv.org/abs/2204.09782","description":"<p>Histopathology relies on the analysis of microscopic tissue images to\ndiagnose disease. A crucial part of tissue preparation is staining whereby a\ndye is used to make the salient tissue components more distinguishable.\nHowever, differences in laboratory protocols and scanning devices result in\nsignificant confounding appearance variation in the corresponding images. This\nvariation increases both human error and the inter-rater variability, as well\nas hinders the performance of automatic or semi-automatic methods. In the\npresent paper we introduce an unsupervised adversarial network to translate\n(and hence normalize) whole slide images across multiple data acquisition\ndomains. Our key contributions are: (i) an adversarial architecture which\nlearns across multiple domains with a single generator-discriminator network\nusing an information flow branch which optimizes for perceptual loss, and (ii)\nthe inclusion of an additional feature extraction network during training which\nguides the transformation network to keep all the structural features in the\ntissue image intact. We: (i) demonstrate the effectiveness of the proposed\nmethod firstly on H\\&amp;E slides of 120 cases of kidney cancer, as well as (ii)\nshow the benefits of the approach on more general problems, such as flexible\nillumination based natural image enhancement and light source adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nazki_H/0/1/0/all/0/1\">Haseeb Nazki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arandjelovic_O/0/1/0/all/0/1\">Ognjen Arandjelovi&#x107;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Um_I/0/1/0/all/0/1\">InHwa Um</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harrison_D/0/1/0/all/0/1\">David Harrison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SELMA: SEmantic Large-scale Multimodal Acquisitions in Variable Weather, Daytime and Viewpoints. (arXiv:2204.09788v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09788","description":"<p>Accurate scene understanding from multiple sensors mounted on cars is a key\nrequirement for autonomous driving systems. Nowadays, this task is mainly\nperformed through data-hungry deep learning techniques that need very large\namounts of data to be trained. Due to the high cost of performing segmentation\nlabeling, many synthetic datasets have been proposed. However, most of them\nmiss the multi-sensor nature of the data, and do not capture the significant\nchanges introduced by the variation of daytime and weather conditions. To fill\nthese gaps, we introduce SELMA, a novel synthetic dataset for semantic\nsegmentation that contains more than 30K unique waypoints acquired from 24\ndifferent sensors including RGB, depth, semantic cameras and LiDARs, in 27\ndifferent atmospheric and daytime conditions, for a total of more than 20M\nsamples. SELMA is based on CARLA, an open-source simulator for generating\nsynthetic data in autonomous driving scenarios, that we modified to increase\nthe variability and the diversity in the scenes and class sets, and to align it\nwith other benchmark datasets. As shown by the experimental evaluation, SELMA\nallows the efficient training of standard and multi-modal deep learning\narchitectures, and achieves remarkable results on real-world data. SELMA is\nfree and publicly available, thus supporting open science and research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Testolina_P/0/1/0/all/0/1\">Paolo Testolina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbato_F/0/1/0/all/0/1\">Francesco Barbato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michieli_U/0/1/0/all/0/1\">Umberto Michieli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giordani_M/0/1/0/all/0/1\">Marco Giordani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanuttigh_P/0/1/0/all/0/1\">Pietro Zanuttigh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zorzi_M/0/1/0/all/0/1\">Michele Zorzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Gaussian Mixture Model for Realtime Roadside LiDAR Object Detection. (arXiv:2204.09804v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09804","description":"<p>Background modeling is widely used for intelligent surveillance systems to\ndetect the moving targets by subtracting the static background components. Most\nroadside LiDAR object detection methods filter out foreground points by\ncomparing new points to pre-trained background references based on descriptive\nstatistics over many frames (e.g., voxel density, slopes, maximum distance).\nThese solutions are not efficient under heavy traffic, and parameter values are\nhard to transfer from one scenario to another. In early studies, the\nvideo-based background modeling methods were considered not suitable for\nroadside LiDAR surveillance systems due to the sparse and unstructured point\nclouds data. In this paper, the raw LiDAR data were transformed into a\nmulti-dimensional tensor structure based on the elevation and azimuth value of\neach LiDAR point. With this high-order data representation, we break the\nbarrier to allow the efficient Gaussian Mixture Model (GMM) method for roadside\nLiDAR background modeling. The probabilistic GMM is built with superior agility\nand real-time capability. The proposed Method was compared against two\nstate-of-the-art roadside LiDAR background models and evaluated based on point\nlevel, object level, and path level, demonstrating better robustness under\nheavy traffic and challenging weather. This multimodal GMM method is capable of\nhandling dynamic backgrounds with noisy measurements and substantially enhances\nthe infrastructure-based LiDAR object detection, whereby various 3D modeling\nfor smart city applications could be created\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Peter J. Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yi Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parametric Level-sets Enhanced To Improve Reconstruction (PaLEnTIR). (arXiv:2204.09815v1 [math.NA])","link":"http://arxiv.org/abs/2204.09815","description":"<p>In this paper, we consider the restoration and reconstruction of piecewise\nconstant objects in two and three dimensions using PaLEnTIR, a significantly\nenhanced Parametric level set (PaLS) model relative to the current\nstate-of-the-art. The primary contribution of this paper is a new PaLS\nformulation which requires only a single level set function to recover a scene\nwith piecewise constant objects possessing multiple unknown contrasts. Our\nmodel offers distinct advantages over current approaches to the multi-contrast,\nmulti-object problem, all of which require multiple level sets and explicit\nestimation of the contrast magnitudes. Given upper and lower bounds on the\ncontrast, our approach is able to recover objects with any distribution of\ncontrasts and eliminates the need to know either the number of contrasts in a\ngiven scene or their values. We provide an iterative process for finding these\nspace-varying contrast limits. Relative to most PaLS methods which employ\nradial basis functions (RBFs), our model makes use of non-isotropic basis\nfunctions, thereby expanding the class of shapes that a PaLS model of a given\ncomplexity can approximate. Finally, PaLEnTIR improves the conditioning of the\nJacobian matrix required as part of the parameter identification process and\nconsequently accelerates the optimization methods by controlling the magnitude\nof the PaLS expansion coefficients, fixing the centers of the basis functions,\nand the uniqueness of parametric to image mappings provided by the new\nparameterization. We demonstrate the performance of the new approach using both\n2D and 3D variants of X-ray computed tomography, diffuse optical tomography\n(DOT), denoising, deconvolution problems. Application to experimental sparse CT\ndata and simulated data with different types of noise are performed to further\nvalidate the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Ozsar_E/0/1/0/all/0/1\">Ege Ozsar</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kilmer_M/0/1/0/all/0/1\">Misha Kilmer</a>, <a href=\"http://arxiv.org/find/math/1/au:+Miller_E/0/1/0/all/0/1\">Eric Miller</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sturler_E/0/1/0/all/0/1\">Eric de Sturler</a>, <a href=\"http://arxiv.org/find/math/1/au:+Saibaba_A/0/1/0/all/0/1\">Arvind Saibaba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing. (arXiv:2204.09817v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09817","description":"<p>Multi-modal data abounds in biomedicine, such as radiology images and\nreports. Interpreting this data at scale is essential for improving clinical\ncare and accelerating clinical research. Biomedical text with its complex\nsemantics poses additional challenges in vision-language modelling compared to\nthe general domain, and previous work has used insufficiently adapted models\nthat lack domain-specific language understanding. In this paper, we show that\nprincipled textual semantic modelling can substantially improve contrastive\nlearning in self-supervised vision--language processing. We release a language\nmodel that achieves state-of-the-art results in radiology natural language\ninference through its improved vocabulary and novel language pretraining\nobjective leveraging semantics and discourse characteristics in radiology\nreports. Further, we propose a self-supervised joint vision--language approach\nwith a focus on better text modelling. It establishes new state of the art\nresults on a wide range of publicly available benchmarks, in part by leveraging\nour new domain-specific language model. We release a new dataset with\nlocally-aligned phrase grounding annotations by radiologists to facilitate the\nstudy of complex semantic modelling in biomedical vision--language processing.\nA broad evaluation, including on this new dataset, shows that our contrastive\nlearning approach, aided by textual-semantic modelling, outperforms prior\nmethods in segmentation tasks, despite only using a global-alignment objective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boecking_B/0/1/0/all/0/1\">Benedikt Boecking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bannur_S/0/1/0/all/0/1\">Shruthi Bannur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Daniel C. Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwaighofer_A/0/1/0/all/0/1\">Anton Schwaighofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyland_S/0/1/0/all/0/1\">Stephanie Hyland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetscherek_M/0/1/0/all/0/1\">Maria Wetscherek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nori_A/0/1/0/all/0/1\">Aditya Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Valle_J/0/1/0/all/0/1\">Javier Alvarez-Valle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oktay_O/0/1/0/all/0/1\">Ozan Oktay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimMC: Simple Masked Contrastive Learning of Skeleton Representations for Unsupervised Person Re-Identification. (arXiv:2204.09826v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09826","description":"<p>Recent advances in skeleton-based person re-identification (re-ID) obtain\nimpressive performance via either hand-crafted skeleton descriptors or skeleton\nrepresentation learning with deep learning paradigms. However, they typically\nrequire skeletal pre-modeling and label information for training, which leads\nto limited applicability of these methods. In this paper, we focus on\nunsupervised skeleton-based person re-ID, and present a generic Simple Masked\nContrastive learning (SimMC) framework to learn effective representations from\nunlabeled 3D skeletons for person re-ID. Specifically, to fully exploit\nskeleton features within each skeleton sequence, we first devise a masked\nprototype contrastive learning (MPC) scheme to cluster the most typical\nskeleton features (skeleton prototypes) from different subsequences randomly\nmasked from raw sequences, and contrast the inherent similarity between\nskeleton features and different prototypes to learn discriminative skeleton\nrepresentations without using any label. Then, considering that different\nsubsequences within the same sequence usually enjoy strong correlations due to\nthe nature of motion continuity, we propose the masked intra-sequence\ncontrastive learning (MIC) to capture intra-sequence pattern consistency\nbetween subsequences, so as to encourage learning more effective skeleton\nrepresentations for person re-ID. Extensive experiments validate that the\nproposed SimMC outperforms most state-of-the-art skeleton-based methods. We\nfurther show its scalability and efficiency in enhancing the performance of\nexisting models. Our codes are available at https://github.com/Kali-Hac/SimMC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_H/0/1/0/all/0/1\">Haocong Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast AdvProp. (arXiv:2204.09838v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09838","description":"<p>Adversarial Propagation (AdvProp) is an effective way to improve recognition\nmodels, leveraging adversarial examples. Nonetheless, AdvProp suffers from the\nextremely slow training speed, mainly because: a) extra forward and backward\npasses are required for generating adversarial examples; b) both original\nsamples and their adversarial counterparts are used for training (i.e.,\n2$\\times$ data). In this paper, we introduce Fast AdvProp, which aggressively\nrevamps AdvProp's costly training components, rendering the method nearly as\ncheap as the vanilla training. Specifically, our modifications in Fast AdvProp\nare guided by the hypothesis that disentangled learning with adversarial\nexamples is the key for performance improvements, while other training recipes\n(e.g., paired clean and adversarial training samples, multi-step adversarial\nattackers) could be largely simplified.\n</p>\n<p>Our empirical results show that, compared to the vanilla training baseline,\nFast AdvProp is able to further model performance on a spectrum of visual\nbenchmarks, without incurring extra training cost. Additionally, our ablations\nfind Fast AdvProp scales better if larger models are used, is compatible with\nexisting data augmentation methods (i.e., Mixup and CutMix), and can be easily\nadapted to other recognition tasks like object detection. The code is available\nhere: https://github.com/meijieru/fast_advprop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jieru Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yucheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yutong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yixiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiscale Analysis for Improving Texture Classification. (arXiv:2204.09841v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09841","description":"<p>Information from an image occurs over multiple and distinct spatial scales.\nImage pyramid multiresolution representations are a useful data structure for\nimage analysis and manipulation over a spectrum of spatial scales. This paper\nemploys the Gaussian-Laplacian pyramid to treat different spatial frequency\nbands of a texture separately. First, we generate three images corresponding to\nthree levels of the Gaussian-Laplacian pyramid for an input image to capture\nintrinsic details. Then we aggregate features extracted from gray and color\ntexture images using bio-inspired texture descriptors, information-theoretic\nmeasures, gray-level co-occurrence matrix features, and Haralick statistical\nfeatures into a single feature vector. Such an aggregation aims at producing\nfeatures that characterize textures to their maximum extent, unlike employing\neach descriptor separately, which may lose some relevant textural information\nand reduce the classification performance. The experimental results on texture\nand histopathologic image datasets have shown the advantages of the proposed\nmethod compared to state-of-the-art approaches. Such findings emphasize the\nimportance of multiscale image analysis and corroborate that the descriptors\nmentioned above are complementary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ataky_S/0/1/0/all/0/1\">Steve T. M. Ataky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saqui_D/0/1/0/all/0/1\">Diego Saqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matos_J/0/1/0/all/0/1\">Jonathan de Matos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Britto_A/0/1/0/all/0/1\">Alceu S. Britto Jr.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koerich_A/0/1/0/all/0/1\">Alessandro L. Koerich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unseen Object Instance Segmentation with Fully Test-time RGB-D Embeddings Adaptation. (arXiv:2204.09847v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09847","description":"<p>Segmenting unseen objects is a crucial ability for the robot since it may\nencounter new environments during the operation. Recently, a popular solution\nis leveraging RGB-D features of large-scale synthetic data and directly\napplying the model to unseen real-world scenarios. However, even though depth\ndata have fair generalization ability, the domain shift due to the Sim2Real gap\nis inevitable, which presents a key challenge to the unseen object instance\nsegmentation (UOIS) model. To tackle this problem, we re-emphasize the\nadaptation process across Sim2Real domains in this paper. Specifically, we\npropose a framework to conduct the Fully Test-time RGB-D Embeddings Adaptation\n(FTEA) based on parameters of the BatchNorm layer. To construct the learning\nobjective for test-time back-propagation, we propose a novel non-parametric\nentropy objective that can be implemented without explicit classification\nlayers. Moreover, we design a cross-modality knowledge distillation module to\nencourage the information transfer during test time. The proposed method can be\nefficiently conducted with test-time images, without requiring annotations or\nrevisiting the large-scale synthetic training data. Besides significant time\nsavings, the proposed method consistently improves segmentation results on both\noverlap and boundary metrics, achieving state-of-the-art performances on two\nreal-world RGB-D image datasets. We hope our work could draw attention to the\ntest-time adaptation and reveal a promising direction for robot perception in\nunseen environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Siqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Aligned Feature Fusion for Multimodal Object Detection. (arXiv:2204.09848v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09848","description":"<p>To achieve accurate and robust object detection in the real-world scenario,\nvarious forms of images are incorporated, such as color, thermal, and depth.\nHowever, multimodal data often suffer from the position shift problem, i.e.,\nthe image pair is not strictly aligned, making one object has different\npositions in different modalities. For the deep learning method, this problem\nmakes it difficult to fuse multimodal features and puzzles the convolutional\nneural network (CNN) training. In this article, we propose a general multimodal\ndetector named aligned region CNN (AR-CNN) to tackle the position shift\nproblem. First, a region feature (RF) alignment module with adjacent similarity\nconstraint is designed to consistently predict the position shift between two\nmodalities and adaptively align the cross-modal RFs. Second, we propose a novel\nregion of interest (RoI) jitter strategy to improve the robustness to\nunexpected shift patterns. Third, we present a new multimodal feature fusion\nmethod that selects the more reliable feature and suppresses the less useful\none via feature reweighting. In addition, by locating bounding boxes in both\nmodalities and building their relationships, we provide novel multimodal\nlabeling named KAIST-Paired. Extensive experiments on 2-D and 3-D object\ndetection, RGB-T, and RGB-D datasets demonstrate the effectiveness and\nrobustness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiangyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zhen Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_H/0/1/0/all/0/1\">Hong Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning to Guide Scientifically Relevant Categorization of Martian Terrain Images. (arXiv:2204.09854v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09854","description":"<p>Automatic terrain recognition in Mars rover images is an important problem\nnot just for navigation, but for scientists interested in studying rock types,\nand by extension, conditions of the ancient Martian paleoclimate and\nhabitability. Existing approaches to label Martian terrain either involve the\nuse of non-expert annotators producing taxonomies of limited granularity (e.g.\nsoil, sand, bedrock, float rock, etc.), or rely on generic class discovery\napproaches that tend to produce perceptual classes such as rover parts and\nlandscape, which are irrelevant to geologic analysis. Expert-labeled datasets\ncontaining granular geological/geomorphological terrain categories are rare or\ninaccessible to public, and sometimes require the extraction of relevant\ncategorical information from complex annotations. In order to facilitate the\ncreation of a dataset with detailed terrain categories, we present a\nself-supervised method that can cluster sedimentary textures in images captured\nfrom the Mast camera onboard the Curiosity rover (Mars Science Laboratory). We\nthen present a qualitative analysis of these clusters and describe their\ngeologic significance via the creation of a set of granular terrain categories.\nThe precision and geologic validation of these automatically discovered\nclusters suggest that our methods are promising for the rapid classification of\nimportant geologic features and will therefore facilitate our long-term goal of\nproducing a large, granular, and publicly available dataset for Mars terrain\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Panambur_T/0/1/0/all/0/1\">Tejas Panambur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_D/0/1/0/all/0/1\">Deep Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_M/0/1/0/all/0/1\">Melissa Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milliken_R/0/1/0/all/0/1\">Ralph Milliken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Learned_Miller_E/0/1/0/all/0/1\">Erik Learned-Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parente_M/0/1/0/all/0/1\">Mario Parente</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Remote Sensing Cross-Modal Text-Image Retrieval Based on Global and Local Information. (arXiv:2204.09860v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09860","description":"<p>Cross-modal remote sensing text-image retrieval (RSCTIR) has recently become\nan urgent research hotspot due to its ability of enabling fast and flexible\ninformation extraction on remote sensing (RS) images. However, current RSCTIR\nmethods mainly focus on global features of RS images, which leads to the\nneglect of local features that reflect target relationships and saliency. In\nthis article, we first propose a novel RSCTIR framework based on global and\nlocal information (GaLR), and design a multi-level information dynamic fusion\n(MIDF) module to efficaciously integrate features of different levels. MIDF\nleverages local information to correct global information, utilizes global\ninformation to supplement local information, and uses the dynamic addition of\nthe two to generate prominent visual representation. To alleviate the pressure\nof the redundant targets on the graph convolution network (GCN) and to improve\nthe model s attention on salient instances during modeling local features, the\nde-noised representation matrix and the enhanced adjacency matrix (DREA) are\ndevised to assist GCN in producing superior local representations. DREA not\nonly filters out redundant features with high similarity, but also obtains more\npowerful local features by enhancing the features of prominent objects.\nFinally, to make full use of the information in the similarity matrix during\ninference, we come up with a plug-and-play multivariate rerank (MR) algorithm.\nThe algorithm utilizes the k nearest neighbors of the retrieval results to\nperform a reverse search, and improves the performance by combining multiple\ncomponents of bidirectional retrieval. Extensive experiments on public datasets\nstrongly demonstrate the state-of-the-art performance of GaLR methods on the\nRSCTIR task. The code of GaLR method, MR algorithm, and corresponding files\nhave been made available at https://github.com/xiaoyuan1996/GaLR .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhiqiang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenkai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1\">Changyuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_X/0/1/0/all/0/1\">Xuee Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Kun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pixel2Mesh++: 3D Mesh Generation and Refinement from Multi-View Images. (arXiv:2204.09866v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09866","description":"<p>We study the problem of shape generation in 3D mesh representation from a\nsmall number of color images with or without camera poses. While many previous\nworks learn to hallucinate the shape directly from priors, we adopt to further\nimprove the shape quality by leveraging cross-view information with a graph\nconvolution network. Instead of building a direct mapping function from images\nto 3D shape, our model learns to predict series of deformations to improve a\ncoarse shape iteratively. Inspired by traditional multiple view geometry\nmethods, our network samples nearby area around the initial mesh's vertex\nlocations and reasons an optimal deformation using perceptual feature\nstatistics built from multiple input images. Extensive experiments show that\nour model produces accurate 3D shapes that are not only visually plausible from\nthe input perspectives, but also well aligned to arbitrary viewpoints. With the\nhelp of physically driven architecture, our model also exhibits generalization\ncapability across different semantic categories, and the number of input\nimages. Model analysis experiments show that our model is robust to the quality\nof the initial mesh and the error of camera pose, and can be combined with a\ndifferentiable renderer for test-time optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1\">Chao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinda Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Chenjie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring a Fine-Grained Multiscale Method for Cross-Modal Remote Sensing Image Retrieval. (arXiv:2204.09868v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09868","description":"<p>Remote sensing (RS) cross-modal text-image retrieval has attracted extensive\nattention for its advantages of flexible input and efficient query. However,\ntraditional methods ignore the characteristics of multi-scale and redundant\ntargets in RS image, leading to the degradation of retrieval accuracy. To cope\nwith the problem of multi-scale scarcity and target redundancy in RS multimodal\nretrieval task, we come up with a novel asymmetric multimodal feature matching\nnetwork (AMFMN). Our model adapts to multi-scale feature inputs, favors\nmulti-source retrieval methods, and can dynamically filter redundant features.\nAMFMN employs the multi-scale visual self-attention (MVSA) module to extract\nthe salient features of RS image and utilizes visual features to guide the text\nrepresentation. Furthermore, to alleviate the positive samples ambiguity caused\nby the strong intraclass similarity in RS image, we propose a triplet loss\nfunction with dynamic variable margin based on prior similarity of sample\npairs. Finally, unlike the traditional RS image-text dataset with coarse text\nand higher intraclass similarity, we construct a fine-grained and more\nchallenging Remote sensing Image-Text Match dataset (RSITMD), which supports RS\nimage retrieval through keywords and sentence separately and jointly.\nExperiments on four RS text-image datasets demonstrate that the proposed model\ncan achieve state-of-the-art performance in cross-modal RS text-image retrieval\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhiqiang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenkai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Kun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chubo Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physics vs. Learned Priors: Rethinking Camera and Algorithm Design for Task-Specific Imaging. (arXiv:2204.09871v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09871","description":"<p>Cameras were originally designed using physics-based heuristics to capture\naesthetic images. In recent years, there has been a transformation in camera\ndesign from being purely physics-driven to increasingly data-driven and\ntask-specific. In this paper, we present a framework to understand the building\nblocks of this nascent field of end-to-end design of camera hardware and\nalgorithms. As part of this framework, we show how methods that exploit both\nphysics and data have become prevalent in imaging and computer vision,\nunderscoring a key trend that will continue to dominate the future of\ntask-specific camera design. Finally, we share current barriers to progress in\nend-to-end design, and hypothesize how these barriers can be overcome.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klinghoffer_T/0/1/0/all/0/1\">Tzofi Klinghoffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somasundaram_S/0/1/0/all/0/1\">Siddharth Somasundaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwary_K/0/1/0/all/0/1\">Kushagra Tiwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1\">Ramesh Raskar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Persistent-Transient Duality in Human Behavior Modeling. (arXiv:2204.09875v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09875","description":"<p>We propose to model the persistent-transient duality in human behavior using\na parent-child multi-channel neural network, which features a parent persistent\nchannel that manages the global dynamics and children transient channels that\nare initiated and terminated on-demand to handle detailed interactive actions.\nThe short-lived transient sessions are managed by a proposed Transient Switch.\nThe neural framework is trained to discover the structure of the duality\nautomatically. Our model shows superior performances in human-object\ninteraction motion prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hung Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1\">Vuong Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Truyen Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNLL: A Semi-supervised Approach For Continual Noisy Label Learning. (arXiv:2204.09881v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09881","description":"<p>The task of continual learning requires careful design of algorithms that can\ntackle catastrophic forgetting. However, the noisy label, which is inevitable\nin a real-world scenario, seems to exacerbate the situation. While very few\nstudies have addressed the issue of continual learning under noisy labels, long\ntraining time and complicated training schemes limit their applications in most\ncases. In contrast, we propose a simple purification technique to effectively\ncleanse the online data stream that is both cost-effective and more accurate.\nAfter purification, we perform fine-tuning in a semi-supervised fashion that\nensures the participation of all available samples. Training in this fashion\nhelps us learn a better representation that results in state-of-the-art (SOTA)\nperformance. Through extensive experimentation on 3 benchmark datasets, MNIST,\nCIFAR10 and CIFAR100, we show the effectiveness of our proposed approach. We\nachieve a 24.8% performance gain for CIFAR10 with 20% noise over previous SOTA\nmethods. Our code is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1\">Nazmul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalid_U/0/1/0/all/0/1\">Umar Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esmaeili_A/0/1/0/all/0/1\">Ashkan Esmaeili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahnavard_N/0/1/0/all/0/1\">Nazanin Rahnavard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Color Invariant Skin Segmentation. (arXiv:2204.09882v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09882","description":"<p>This paper addresses the problem of automatically detecting human skin in\nimages without reliance on color information. A primary motivation of the work\nhas been to achieve results that are consistent across the full range of skin\ntones, even while using a training dataset that is significantly biased toward\nlighter skin tones. Previous skin-detection methods have used color cues almost\nexclusively, and we present a new approach that performs well in the absence of\nsuch information. A key aspect of the work is dataset repair through\naugmentation that is applied strategically during training, with the goal of\ncolor invariant feature learning to enhance generalization. We have\ndemonstrated the concept using two architectures, and experimental results show\nimprovements in both precision and recall for most Fitzpatrick skin tones in\nthe benchmark ECU dataset. We further tested the system with the RFW dataset to\nshow that the proposed method performs much more consistently across different\nethnicities, thereby reducing the chance of bias based on skin color. To\ndemonstrate the effectiveness of our work, extensive experiments were performed\non grayscale images as well as images obtained under unconstrained illumination\nand with artificial filters. Source code:\nhttps://github.com/HanXuMartin/Color-Invariant-Skin-Segmentation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Han Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Abhijit Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbott_A/0/1/0/all/0/1\">A. Lynn Abbott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Video Interpolation by Learning Multilayered 2.5D Motion Fields. (arXiv:2204.09900v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09900","description":"<p>The problem of video frame interpolation is to increase the temporal\nresolution of a low frame-rate video, by interpolating novel frames between\nexisting temporally sparse frames. This paper presents a self-supervised\napproach to video frame interpolation that requires only a single video. We\npose the video as a set of layers. Each layer is parameterized by two implicit\nneural networks -- one for learning a static frame and the other for a\ntime-varying motion field corresponding to video dynamics. Together they\nrepresent an occlusion-free subset of the scene with a pseudo-depth channel. To\nmodel inter-layer occlusions, all layers are lifted to the 2.5D space so that\nthe frontal layer occludes distant layers. This is done by assigning each layer\na depth channel, which we call `pseudo-depth', whose partial order defines the\nocclusion between layers. The pseudo-depths are converted to visibility values\nthrough a fully differentiable SoftMin function so that closer layers are more\nvisible than layers in a distance. On the other hand, we parameterize the video\nmotions by solving an ordinary differentiable equation (ODE) defined on a\ntime-varying neural velocity field that guarantees valid motions. This implicit\nneural representation learns the video as a space-time continuum, allowing\nframe interpolation at any temporal resolution. We demonstrate the\neffectiveness of our method on real-world datasets, where our method achieves\ncomparable performance to state-of-the-arts that require ground truth labels\nfor training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Ziang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shihao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond the Prototype: Divide-and-conquer Proxies for Few-shot Segmentation. (arXiv:2204.09903v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09903","description":"<p>Few-shot segmentation, which aims to segment unseen-class objects given only\na handful of densely labeled samples, has received widespread attention from\nthe community. Existing approaches typically follow the prototype learning\nparadigm to perform meta-inference, which fails to fully exploit the underlying\ninformation from support image-mask pairs, resulting in various segmentation\nfailures, e.g., incomplete objects, ambiguous boundaries, and distractor\nactivation. To this end, we propose a simple yet versatile framework in the\nspirit of divide-and-conquer. Specifically, a novel self-reasoning scheme is\nfirst implemented on the annotated support image, and then the coarse\nsegmentation mask is divided into multiple regions with different properties.\nLeveraging effective masked average pooling operations, a series of\nsupport-induced proxies are thus derived, each playing a specific role in\nconquering the above challenges. Moreover, we devise a unique parallel decoder\nstructure that integrates proxies with similar attributes to boost the\ndiscrimination power. Our proposed approach, named divide-and-conquer proxies\n(DCP), allows for the development of appropriate and reliable information as a\nguide at the \"episode\" level, not just about the object cues themselves.\nExtensive experiments on PASCAL-5i and COCO-20i demonstrate the superiority of\nDCP over conventional prototype-based approaches (up to 5~10% on average),\nwhich also establishes a new state-of-the-art. Code is available at\ngithub.com/chunbolang/DCP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1\">Chunbo Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_B/0/1/0/all/0/1\">Binfei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Infographics Wizard: Flexible Infographics Authoring and Design Exploration. (arXiv:2204.09904v1 [cs.HC])","link":"http://arxiv.org/abs/2204.09904","description":"<p>Infographics are an aesthetic visual representation of information following\nspecific design principles of human perception. Designing infographics can be a\ntedious process for non-experts and time-consuming, even for professional\ndesigners. With the help of designers, we propose a semi-automated infographic\nframework for general structured and flow-based infographic design generation.\nFor novice designers, our framework automatically creates and ranks infographic\ndesigns for a user-provided text with no requirement for design input. However,\nexpert designers can still provide custom design inputs to customize the\ninfographics. We will also contribute an individual visual group (VG) designs\ndataset (in SVG), along with a 1k complete infographic image dataset with\nsegmented VGs in this work. Evaluation results confirm that by using our\nframework, designers from all expertise levels can generate generic infographic\ndesigns faster than existing methods while maintaining the same quality as\nhand-designed infographics templates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_A/0/1/0/all/0/1\">Anjul Tyagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_P/0/1/0/all/0/1\">Pushkar Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khurana_S/0/1/0/all/0/1\">Swasti Khurana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_K/0/1/0/all/0/1\">Klaus Mueller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient End-to-End Deep Neural Network for Interstitial Lung Disease Recognition and Classification. (arXiv:2204.09909v1 [eess.IV])","link":"http://arxiv.org/abs/2204.09909","description":"<p>The automated Interstitial Lung Diseases (ILDs) classification technique is\nessential for assisting clinicians during the diagnosis process. Detecting and\nclassifying ILDs patterns is a challenging problem. This paper introduces an\nend-to-end deep convolution neural network (CNN) for classifying ILDs patterns.\nThe proposed model comprises four convolutional layers with different kernel\nsizes and Rectified Linear Unit (ReLU) activation function, followed by batch\nnormalization and max-pooling with a size equal to the final feature map size\nwell as four dense layers. We used the ADAM optimizer to minimize categorical\ncross-entropy. A dataset consisting of 21328 image patches of 128 CT scans with\nfive classes is taken to train and assess the proposed model. A comparison\nstudy showed that the presented model outperformed pre-trained CNNs and\nfive-fold cross-validation on the same dataset. For ILDs pattern\nclassification, the proposed approach achieved the accuracy scores of 99.09%\nand the average F score of 97.9%, outperforming three pre-trained CNNs. These\noutcomes show that the proposed model is relatively state-of-the-art in\nprecision, recall, f score, and accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Junayed_M/0/1/0/all/0/1\">Masum Shah Junayed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jeny_A/0/1/0/all/0/1\">Afsana Ahsan Jeny</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Islam_M/0/1/0/all/0/1\">Md Baharul Islam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahmed_I/0/1/0/all/0/1\">Ikhtiar Ahmed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shah_A/0/1/0/all/0/1\">A F M Shahen Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPGNet: Cascade Point-Grid Fusion Network for Real-Time LiDAR Semantic Segmentation. (arXiv:2204.09914v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09914","description":"<p>LiDAR semantic segmentation essential for advanced autonomous driving is\nrequired to be accurate, fast, and easy-deployed on mobile platforms. Previous\npoint-based or sparse voxel-based methods are far away from real-time\napplications since time-consuming neighbor searching or sparse 3D convolution\nare employed. Recent 2D projection-based methods, including range view and\nmulti-view fusion, can run in real time, but suffer from lower accuracy due to\ninformation loss during the 2D projection. Besides, to improve the performance,\nprevious methods usually adopt test time augmentation (TTA), which further\nslows down the inference process. To achieve a better speed-accuracy trade-off,\nwe propose Cascade Point-Grid Fusion Network (CPGNet), which ensures both\neffectiveness and efficiency mainly by the following two techniques: 1) the\nnovel Point-Grid (PG) fusion block extracts semantic features mainly on the 2D\nprojected grid for efficiency, while summarizes both 2D and 3D features on 3D\npoint for minimal information loss; 2) the proposed transformation consistency\nloss narrows the gap between the single-time model inference and TTA. The\nexperiments on the SemanticKITTI and nuScenes benchmarks demonstrate that the\nCPGNet without ensemble models or TTA is comparable with the state-of-the-art\nRPVNet, while it runs 4.7 times faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hongyu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhua Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perception Visualization: Seeing Through the Eyes of a DNN. (arXiv:2204.09920v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09920","description":"<p>Artificial intelligence (AI) systems power the world we live in. Deep neural\nnetworks (DNNs) are able to solve tasks in an ever-expanding landscape of\nscenarios, but our eagerness to apply these powerful models leads us to focus\non their performance and deprioritises our ability to understand them. Current\nresearch in the field of explainable AI tries to bridge this gap by developing\nvarious perturbation or gradient-based explanation techniques. For images,\nthese techniques fail to fully capture and convey the semantic information\nneeded to elucidate why the model makes the predictions it does. In this work,\nwe develop a new form of explanation that is radically different in nature from\ncurrent explanation methods, such as Grad-CAM. Perception visualization\nprovides a visual representation of what the DNN perceives in the input image\nby depicting what visual patterns the latent representation corresponds to.\nVisualizations are obtained through a reconstruction model that inverts the\nencoded features, such that the parameters and predictions of the original\nmodels are not modified. Results of our user study demonstrate that humans can\nbetter understand and predict the system's decisions when perception\nvisualizations are available, thus easing the debugging and deployment of deep\nmodels as trusted systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giulivi_L/0/1/0/all/0/1\">Loris Giulivi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carman_M/0/1/0/all/0/1\">Mark James Carman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boracchi_G/0/1/0/all/0/1\">Giacomo Boracchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Training of A Two-Stage Framework for Video Restoration. (arXiv:2204.09924v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09924","description":"<p>As a widely studied task, video restoration aims to enhance the quality of\nthe videos with multiple potential degradations, such as noises, blurs and\ncompression artifacts. Among video restorations, compressed video quality\nenhancement and video super-resolution are two of the main tacks with\nsignificant values in practical scenarios. Recently, recurrent neural networks\nand transformers attract increasing research interests in this field, due to\ntheir impressive capability in sequence-to-sequence modeling. However, the\ntraining of these models is not only costly but also relatively hard to\nconverge, with gradient exploding and vanishing problems. To cope with these\nproblems, we proposed a two-stage framework including a multi-frame recurrent\nnetwork and a single-frame transformer. Besides, multiple training strategies,\nsuch as transfer learning and progressive training, are developed to shorten\nthe training time and improve the model performance. Benefiting from the above\ntechnical contributions, our solution wins two champions and a runner-up in the\nNTIRE 2022 super-resolution and quality enhancement of compressed video\nchallenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Meisong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Q/0/1/0/all/0/1\">Qunliang Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_M/0/1/0/all/0/1\">Minglang Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lai Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huaida Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-scale Knowledge Distillation for Unsupervised Person Re-Identification. (arXiv:2204.09931v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09931","description":"<p>Unsupervised person re-identification is a challenging and promising task in\nthe computer vision. Nowadays unsupervised person re-identification methods\nhave achieved great improvements by training with pseudo labels. However, the\nappearance and label noise are less explicitly studied in the unsupervised\nmanner. To relieve the effects of appearance noise the global features\ninvolved, we also take into account the features from two local views and\nproduce multi-scale features. We explore the knowledge distillation to filter\nlabel noise, Specifically, we first train a teacher model from noisy pseudo\nlabels in a iterative way, and then use the teacher model to guide the learning\nof our student model. In our setting, the student model could converge fast in\nthe supervision of the teacher model thus reduce the interference of noisy\nlabels as the teacher model greatly suffered. After carefully handling the\nnoises in the feature learning, Our multi-scale knowledge distillation are\nproven to be very effective in the unsupervised re-identification. Extensive\nexperiments on three popular person re-identification datasets demonstrate the\nsuperiority of our method. Especially, our approach achieves a state-of-the-art\naccuracy 85.7% @mAP or 94.3% @Rank-1 on the challenging Market-1501 benchmark\nwith ResNet-50 under the fully unsupervised setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_L/0/1/0/all/0/1\">Long Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_X/0/1/0/all/0/1\">Xiao Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1\">Haoang Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Invariant Model with Graph Convolutional Network for Mammogram Classification. (arXiv:2204.09954v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09954","description":"<p>Due to its safety-critical property, the image-based diagnosis is desired to\nachieve robustness on out-of-distribution (OOD) samples. A natural way towards\nthis goal is capturing only clinically disease-related features, which is\ncomposed of macroscopic attributes (e.g., margins, shapes) and microscopic\nimage-based features (e.g., textures) of lesion-related areas. However, such\ndisease-related features are often interweaved with data-dependent (but disease\nirrelevant) biases during learning, disabling the OOD generalization. To\nresolve this problem, we propose a novel framework, namely Domain Invariant\nModel with Graph Convolutional Network (DIM-GCN), which only exploits invariant\ndisease-related features from multiple domains. Specifically, we first propose\na Bayesian network, which explicitly decomposes the latent variables into\ndisease-related and other disease-irrelevant parts that are provable to be\ndisentangled from each other. Guided by this, we reformulate the objective\nfunction based on Variational Auto-Encoder, in which the encoder in each domain\nhas two branches: the domain-independent and -dependent ones, which\nrespectively encode disease-related and -irrelevant features. To better capture\nthe macroscopic features, we leverage the observed clinical attributes as a\ngoal for reconstruction, via Graph Convolutional Network (GCN). Finally, we\nonly implement the disease-related features for prediction. The effectiveness\nand utility of our method are demonstrated by the superior OOD generalization\nperformance over others on mammogram benign/malignant diagnosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Churan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xinwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fandong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Referring Expression Comprehension via Cross-Level Multi-Modal Fusion. (arXiv:2204.09957v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09957","description":"<p>As an important and challenging problem in vision-language tasks, referring\nexpression comprehension (REC) aims to localize the target object specified by\na given referring expression. Recently, most of the state-of-the-art REC\nmethods mainly focus on multi-modal fusion while overlooking the inherent\nhierarchical information contained in visual and language encoders. Considering\nthat REC requires visual and textual hierarchical information for accurate\ntarget localization, and encoders inherently extract features in a hierarchical\nfashion, we propose to effectively utilize the rich hierarchical information\ncontained in different layers of visual and language encoders. To this end, we\ndesign a Cross-level Multi-modal Fusion (CMF) framework, which gradually\nintegrates visual and textual features of multi-layer through intra- and\ninter-modal. Experimental results on RefCOCO, RefCOCO+, RefCOCOg, and\nReferItGame datasets demonstrate the proposed framework achieves significant\nperformance improvements over state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_P/0/1/0/all/0/1\">Peihan Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Wei Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yongjian Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChildPredictor: A Child Face Prediction Framework with Disentangled Learning. (arXiv:2204.09962v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09962","description":"<p>The appearances of children are inherited from their parents, which makes it\nfeasible to predict them. Predicting realistic children's faces may help settle\nmany social problems, such as age-invariant face recognition, kinship\nverification, and missing child identification. It can be regarded as an\nimage-to-image translation task. Existing approaches usually assume domain\ninformation in the image-to-image translation can be interpreted by \"style\",\ni.e., the separation of image content and style. However, such separation is\nimproper for the child face prediction, because the facial contours between\nchildren and parents are not the same. To address this issue, we propose a new\ndisentangled learning strategy for children's face prediction. We assume that\nchildren's faces are determined by genetic factors (compact family features,\ne.g., face contour), external factors (facial attributes irrelevant to\nprediction, such as moustaches and glasses), and variety factors (individual\nproperties for each child). On this basis, we formulate predictions as a\nmapping from parents' genetic factors to children's genetic factors, and\ndisentangle them from external and variety factors. In order to obtain accurate\ngenetic factors and perform the mapping, we propose a ChildPredictor framework.\nIt transfers human faces to genetic factors by encoders and back by generators.\nThen, it learns the relationship between the genetic factors of parents and\nchildren through a mapping function. To ensure the generated faces are\nrealistic, we collect a large Family Face Database to train ChildPredictor and\nevaluate it on the FF-Database validation set. Experimental results demonstrate\nthat ChildPredictor is superior to other well-known image-to-image translation\nmethods in predicting realistic and diverse child faces. Implementation codes\ncan be found at https://github.com/zhaoyuzhi/ChildPredictor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuzhi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Po_L/0/1/0/all/0/1\">Lai-Man Po</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuehui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yujia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Chun-Kit Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_C/0/1/0/all/0/1\">Chiu-Sing Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_W/0/1/0/all/0/1\">Weifeng Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wing-Yin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Buhua Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-Guided Convolutional Neural Network for Cross-View Geolocalization. (arXiv:2204.09967v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09967","description":"<p>Ground-to-aerial geolocalization refers to localizing a ground-level query\nimage by matching it to a reference database of geo-tagged aerial imagery. This\nis very challenging due to the huge perspective differences in visual\nappearances and geometric configurations between these two views. In this work,\nwe propose a novel Transformer-guided convolutional neural network (TransGCNN)\narchitecture, which couples CNN-based local features with Transformer-based\nglobal representations for enhanced representation learning. Specifically, our\nTransGCNN consists of a CNN backbone extracting feature map from an input image\nand a Transformer head modeling global context from the CNN map. In particular,\nour Transformer head acts as a spatial-aware importance generator to select\nsalient CNN features as the final feature representation. Such a coupling\nprocedure allows us to leverage a lightweight Transformer network to greatly\nenhance the discriminative capability of the embedded features. Furthermore, we\ndesign a dual-branch Transformer head network to combine image features from\nmulti-scale windows in order to improve details of the global feature\nrepresentation. Extensive experiments on popular benchmark datasets demonstrate\nthat our model achieves top-1 accuracy of 94.12\\% and 84.92\\% on CVUSA and\nCVACT_val, respectively, which outperforms the second-performing baseline with\nless than 50% parameters and almost 2x higher frame rate, therefore achieving a\npreferable accuracy-efficiency tradeoff.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Teng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Shujuan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daikun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changyin Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DGECN: A Depth-Guided Edge Convolutional Network for End-to-End 6D Pose Estimation. (arXiv:2204.09983v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09983","description":"<p>Monocular 6D pose estimation is a fundamental task in computer vision.\nExisting works often adopt a two-stage pipeline by establishing correspondences\nand utilizing a RANSAC algorithm to calculate 6 degrees-of-freedom (6DoF) pose.\nRecent works try to integrate differentiable RANSAC algorithms to achieve an\nend-to-end 6D pose estimation. However, most of them hardly consider the\ngeometric features in 3D space, and ignore the topology cues when performing\ndifferentiable RANSAC algorithms. To this end, we proposed a Depth-Guided Edge\nConvolutional Network (DGECN) for 6D pose estimation task. We have made efforts\nfrom the following three aspects: 1) We take advantages ofestimated depth\ninformation to guide both the correspondences-extraction process and the\ncascaded differentiable RANSAC algorithm with geometric information. 2)We\nleverage the uncertainty ofthe estimated depth map to improve accuracy and\nrobustness ofthe output 6D pose. 3) We propose a differentiable\nPerspective-n-Point(PnP) algorithm via edge convolution to explore the topology\nrelations between 2D-3D correspondences. Experiments demonstrate that our\nproposed network outperforms current works on both effectiveness and\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tuo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Fei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanping Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shengjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chunxia Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arbitrary Bit-width Network: A Joint Layer-Wise Quantization and Adaptive Inference Approach. (arXiv:2204.09992v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09992","description":"<p>Conventional model quantization methods use a fixed quantization scheme to\ndifferent data samples, which ignores the inherent \"recognition difficulty\"\ndifferences between various samples. We propose to feed different data samples\nwith varying quantization schemes to achieve a data-dependent dynamic\ninference, at a fine-grained layer level. However, enabling this adaptive\ninference with changeable layer-wise quantization schemes is challenging\nbecause the combination of bit-widths and layers is growing exponentially,\nmaking it extremely difficult to train a single model in such a vast searching\nspace and use it in practice. To solve this problem, we present the Arbitrary\nBit-width Network (ABN), where the bit-widths of a single deep network can\nchange at runtime for different data samples, with a layer-wise granularity.\nSpecifically, first we build a weight-shared layer-wise quantizable\n\"super-network\" in which each layer can be allocated with multiple bit-widths\nand thus quantized differently on demand. The super-network provides a\nconsiderably large number of combinations of bit-widths and layers, each of\nwhich can be used during inference without retraining or storing myriad models.\nSecond, based on the well-trained super-network, each layer's runtime bit-width\nselection decision is modeled as a Markov Decision Process (MDP) and solved by\nan adaptive inference strategy accordingly. Experiments show that the\nsuper-network can be built without accuracy degradation, and the bit-widths\nallocation of each layer can be adjusted to deal with various inputs on the\nfly. On ImageNet classification, we achieve 1.1% top1 accuracy improvement\nwhile saving 36.2% BitOps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_H/0/1/0/all/0/1\">Haoyu Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_K/0/1/0/all/0/1\">Kai Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yifei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Learning the Invisible in Photoacoustic Tomography with Flat Directionally Sensitive Detector. (arXiv:2204.10001v1 [eess.IV])","link":"http://arxiv.org/abs/2204.10001","description":"<p>In photoacoustic tomography (PAT) with flat sensor, we routinely encounter\ntwo types of limited data. The first is due to using a finite sensor and is\nespecially perceptible if the region of interest is large relatively to the\nsensor or located farther away from the sensor. In this paper, we focus on the\nsecond type caused by a varying sensitivity of the sensor to the incoming\nwavefront direction which can be modelled as binary i.e. by a cone of\nsensitivity. Such visibility conditions result, in Fourier domain, in a\nrestriction of both the image and the data to a bowtie, akin to the one\ncorresponding to the range of the forward operator. The visible ranges, in\nimage and data domains, are related by the wavefront direction mapping. We\nadapt the wedge restricted Curvelet decomposition, we previously proposed for\nthe representation of the full PAT data, to separate the visible and invisible\nwavefronts in the image. We optimally combine fast approximate operators with\ntailored deep neural network architectures into efficient learned\nreconstruction methods which perform reconstruction of the visible coefficients\nand the invisible coefficients are learned from a training set of similar data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_B/0/1/0/all/0/1\">Bolin Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Betcke_M/0/1/0/all/0/1\">Marta M. Betcke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fluctuation-based Outlier Detection. (arXiv:2204.10007v1 [cs.LG])","link":"http://arxiv.org/abs/2204.10007","description":"<p>Outlier detection is an important topic in machine learning and has been used\nin a wide range of applications. Outliers are objects that are few in number\nand deviate from the majority of objects. As a result of these two properties,\nwe show that outliers are susceptible to a mechanism called fluctuation. This\narticle proposes a method called fluctuation-based outlier detection (FBOD)\nthat achieves a low linear time complexity and detects outliers purely based on\nthe concept of fluctuation without employing any distance, density or isolation\nmeasure. Fundamentally different from all existing methods. FBOD first converts\nthe Euclidean structure datasets into graphs by using random links, then\npropagates the feature value according to the connection of the graph. Finally,\nby comparing the difference between the fluctuation of an object and its\nneighbors, FBOD determines the object with a larger difference as an outlier.\nThe results of experiments comparing FBOD with seven state-of-the-art\nalgorithms on eight real-world tabular datasets and three video datasets show\nthat FBOD outperforms its competitors in the majority of cases and that FBOD\nhas only 5% of the execution time of the fastest algorithm. The experiment\ncodes are available at:\nhttps://github.com/FluctuationOD/Fluctuation-based-Outlier-Detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xusheng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_E/0/1/0/all/0/1\">Enguang Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhenzhen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Fewer Labels: Support Pair Active Learning for Person Re-identification. (arXiv:2204.10008v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10008","description":"<p>Supervised-learning based person re-identification (re-id) require a large\namount of manual labeled data, which is not applicable in practical re-id\ndeployment. In this work, we propose a Support Pair Active Learning (SPAL)\nframework to lower the manual labeling cost for large-scale person\nreidentification. The support pairs can provide the most informative\nrelationships and support the discriminative feature learning. Specifically, we\nfirstly design a dual uncertainty selection strategy to iteratively discover\nsupport pairs and require human annotations. Afterwards, we introduce a\nconstrained clustering algorithm to propagate the relationships of labeled\nsupport pairs to other unlabeled samples. Moreover, a hybrid learning strategy\nconsisting of an unsupervised contrastive loss and a supervised support pair\nloss is proposed to learn the discriminative re-id feature representation. The\nproposed overall framework can effectively lower the labeling cost by mining\nand leveraging the critical support pairs. Extensive experiments demonstrate\nthe superiority of the proposed method over state-of-the-art active learning\nmethods on large-scale person re-id benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Dapeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minxian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding the Domain Gap in LiDAR Object Detection Networks. (arXiv:2204.10024v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10024","description":"<p>In order to make autonomous driving a reality, artificial neural networks\nhave to work reliably in the open-world. However, the open-world is vast and\ncontinuously changing, so it is not technically feasible to collect and\nannotate training datasets which accurately represent this domain. Therefore,\nthere are always domain gaps between training datasets and the open-world which\nmust be understood. In this work, we investigate the domain gaps between\nhigh-resolution and low-resolution LiDAR sensors in object detection networks.\nUsing a unique dataset, which enables us to study sensor resolution domain gaps\nindependent of other effects, we show two distinct domain gaps - an inference\ndomain gap and a training domain gap. The inference domain gap is characterised\nby a strong dependence on the number of LiDAR points per object, while the\ntraining gap shows no such dependence. These fndings show that different\napproaches are required to close these inference and training domain gaps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Richter_J/0/1/0/all/0/1\">Jasmine Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faion_F/0/1/0/all/0/1\">Florian Faion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Di Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_P/0/1/0/all/0/1\">Paul Benedikt Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sielecki_P/0/1/0/all/0/1\">Piotr Sielecki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaeser_C/0/1/0/all/0/1\">Claudius Glaeser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Neuron Coverage Needed to Make Person Detection More Robust?. (arXiv:2204.10027v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10027","description":"<p>The growing use of deep neural networks (DNNs) in safety- and\nsecurity-critical areas like autonomous driving raises the need for their\nsystematic testing. Coverage-guided testing (CGT) is an approach that applies\nmutation or fuzzing according to a predefined coverage metric to find inputs\nthat cause misbehavior. With the introduction of a neuron coverage metric, CGT\nhas also recently been applied to DNNs. In this work, we apply CGT to the task\nof person detection in crowded scenes. The proposed pipeline uses YOLOv3 for\nperson detection and includes finding DNN bugs via sampling and mutation, and\nsubsequent DNN retraining on the updated training set. To be a bug, we require\na mutated image to cause a significant performance drop compared to a clean\ninput. In accordance with the CGT, we also consider an additional requirement\nof increased coverage in the bug definition. In order to explore several types\nof robustness, our approach includes natural image transformations,\ncorruptions, and adversarial examples generated with the Daedalus attack. The\nproposed framework has uncovered several thousand cases of incorrect DNN\nbehavior. The relative change in mAP performance of the retrained models\nreached on average between 26.21\\% and 64.24\\% for different robustness types.\nHowever, we have found no evidence that the investigated coverage metrics can\nbe advantageously used to improve robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pavlitskaya_S/0/1/0/all/0/1\">Svetlana Pavlitskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yikmis_S/0/1/0/all/0/1\">&#x15e;iyar Y&#x131;km&#x131;&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1\">J. Marius Z&#xf6;llner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Dataset and Transformer for Stereoscopic Video Super-Resolution. (arXiv:2204.10039v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10039","description":"<p>Stereo video super-resolution (SVSR) aims to enhance the spatial resolution\nof the low-resolution video by reconstructing the high-resolution video. The\nkey challenges in SVSR are preserving the stereo-consistency and\ntemporal-consistency, without which viewers may experience 3D fatigue. There\nare several notable works on stereoscopic image super-resolution, but there is\nlittle research on stereo video super-resolution. In this paper, we propose a\nnovel Transformer-based model for SVSR, namely Trans-SVSR. Trans-SVSR comprises\ntwo key novel components: a spatio-temporal convolutional self-attention layer\nand an optical flow-based feed-forward layer that discovers the correlation\nacross different video frames and aligns the features. The parallax attention\nmechanism (PAM) that uses the cross-view information to consider the\nsignificant disparities is used to fuse the stereo views. Due to the lack of a\nbenchmark dataset suitable for the SVSR task, we collected a new stereoscopic\nvideo dataset, SVSR-Set, containing 71 full high-definition (HD) stereo videos\ncaptured using a professional stereo camera. Extensive experiments on the\ncollected dataset, along with two other datasets, demonstrate that the\nTrans-SVSR can achieve competitive performance compared to the state-of-the-art\nmethods. Project code and additional results are available at\nhttps://github.com/H-deep/Trans-SVSR/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imani_H/0/1/0/all/0/1\">Hassan Imani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Baharul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_L/0/1/0/all/0/1\">Lai-Kuan Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Shape Completion via Adversarial Shape Priors. (arXiv:2204.10060v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10060","description":"<p>We present a novel neural implicit shape method for partial point cloud\ncompletion. To that end, we combine a conditional Deep-SDF architecture with\nlearned, adversarial shape priors. More specifically, our network converts\npartial inputs into a global latent code and then recovers the full geometry\nvia an implicit, signed distance generator. Additionally, we train a PointNet++\ndiscriminator that impels the generator to produce plausible, globally\nconsistent reconstructions. In that way, we effectively decouple the challenges\nof predicting shapes that are both realistic, i.e. imitate the training set's\npose distribution, and accurate in the sense that they replicate the partial\ninput observations. In our experiments, we demonstrate state-of-the-art\nperformance for completing partial shapes, considering both man-made objects\n(e.g. airplanes, chairs, ...) and deformable shape categories (human bodies).\nFinally, we show that our adversarial training approach leads to visually\nplausible reconstructions that are highly consistent in recovering missing\nparts of a given object.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saroha_A/0/1/0/all/0/1\">Abhishek Saroha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenberger_M/0/1/0/all/0/1\">Marvin Eisenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yenamandra_T/0/1/0/all/0/1\">Tarun Yenamandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Absolute Wrong Makes Better: Boosting Weakly Supervised Object Detection via Negative Deterministic Information. (arXiv:2204.10068v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10068","description":"<p>Weakly supervised object detection (WSOD) is a challenging task, in which\nimage-level labels (e.g., categories of the instances in the whole image) are\nused to train an object detector. Many existing methods follow the standard\nmultiple instance learning (MIL) paradigm and have achieved promising\nperformance. However, the lack of deterministic information leads to part\ndomination and missing instances. To address these issues, this paper focuses\non identifying and fully exploiting the deterministic information in WSOD. We\ndiscover that negative instances (i.e. absolutely wrong instances), ignored in\nmost of the previous studies, normally contain valuable deterministic\ninformation. Based on this observation, we here propose a negative\ndeterministic information (NDI) based method for improving WSOD, namely\nNDI-WSOD. Specifically, our method consists of two stages: NDI collecting and\nexploiting. In the collecting stage, we design several processes to identify\nand distill the NDI from negative instances online. In the exploiting stage, we\nutilize the extracted NDI to construct a novel negative contrastive learning\nmechanism and a negative guided instance selection strategy for dealing with\nthe issues of part domination and missing instances, respectively. Experimental\nresults on several public benchmarks including VOC 2007, VOC 2012 and MS COCO\nshow that our method achieves satisfactory performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanchun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangrong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zelin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Licheng Jiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn from Unpaired Data for Image Restoration: A Variational Bayes Approach. (arXiv:2204.10090v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10090","description":"<p>Collecting paired training data is difficult in practice, but the unpaired\nsamples broadly exist. Current approaches aim at generating synthesized\ntraining data from the unpaired samples by exploring the relationship between\nthe corrupted and clean data. This work proposes LUD-VAE, a deep generative\nmethod to learn the joint probability density function from data sampled from\nmarginal distributions. Our approach is based on a carefully designed\nprobabilistic graphical model in which the clean and corrupted data domains are\nconditionally independent. Using variational inference, we maximize the\nevidence lower bound (ELBO) to estimate the joint probability density function.\nFurthermore, we show that the ELBO is computable without paired samples under\nthe inference invariant assumption. This property provides the mathematical\nrationale of our approach in the unpaired setting. Finally, we apply our method\nto real-world image denoising and super-resolution tasks and train the models\nusing the synthetic data generated by the LUD-VAE. Experimental results\nvalidate the advantages of our method over other learnable approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Dihan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaisheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_C/0/1/0/all/0/1\">Chenglong Bao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R2-Trans:Fine-Grained Visual Categorization with Redundancy Reduction. (arXiv:2204.10095v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10095","description":"<p>Fine-grained visual categorization (FGVC) aims to discriminate similar\nsubcategories, whose main challenge is the large intraclass diversities and\nsubtle inter-class differences. Existing FGVC methods usually select\ndiscriminant regions found by a trained model, which is prone to neglect other\npotential discriminant information. On the other hand, the massive interactions\nbetween the sequence of image patches in ViT make the resulting class-token\ncontain lots of redundant information, which may also impacts FGVC performance.\nIn this paper, we present a novel approach for FGVC, which can simultaneously\nmake use of partial yet sufficient discriminative information in environmental\ncues and also compress the redundant information in class-token with respect to\nthe target. Specifically, our model calculates the ratio of high-weight regions\nin a batch, adaptively adjusts the masking threshold and achieves moderate\nextraction of background information in the input space. Moreover, we also use\nthe Information Bottleneck~(IB) approach to guide our network to learn a\nminimum sufficient representations in the feature space. Experimental results\non three widely-used benchmark datasets verify that our approach can achieve\noutperforming performance than other state-of-the-art approaches and baseline\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Shuo Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shujian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinge You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAF-NAU: Gramian Angular Field encoded Neighborhood Attention U-Net for Pixel-Wise Hyperspectral Image Classification. (arXiv:2204.10099v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10099","description":"<p>Hyperspectral image (HSI) classification is the most vibrant area of research\nin the hyperspectral community due to the rich spectral information contained\nin HSI can greatly aid in identifying objects of interest. However, inherent\nnon-linearity between materials and the corresponding spectral profiles brings\ntwo major challenges in HSI classification: interclass similarity and\nintraclass variability. Many advanced deep learning methods have attempted to\naddress these issues from the perspective of a region/patch-based approach,\ninstead of a pixel-based alternate. However, the patch-based approaches\nhypothesize that neighborhood pixels of a target pixel in a fixed spatial\nwindow belong to the same class. And this assumption is not always true. To\naddress this problem, we herein propose a new deep learning architecture,\nnamely Gramian Angular Field encoded Neighborhood Attention U-Net (GAF-NAU),\nfor pixel-based HSI classification. The proposed method does not require\nregions or patches centered around a raw target pixel to perform 2D-CNN based\nclassification, instead, our approach transforms 1D pixel vector in HSI into 2D\nangular feature space using Gramian Angular Field (GAF) and then embed it to a\nnew neighborhood attention network to suppress irrelevant angular feature while\nemphasizing on pertinent features useful for HSI classification task.\nEvaluation results on three publicly available HSI datasets demonstrate the\nsuperior performance of the proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paheding_S/0/1/0/all/0/1\">Sidike Paheding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_A/0/1/0/all/0/1\">Abel A. Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasaragod_A/0/1/0/all/0/1\">Anush Kasaragod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oommen_T/0/1/0/all/0/1\">Thomas Oommen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Working memory inspired hierarchical video decomposition with transformative representations. (arXiv:2204.10105v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10105","description":"<p>Video decomposition is very important to extract moving foreground objects\nfrom complex backgrounds in computer vision, machine learning, and medical\nimaging, e.g., extracting moving contrast-filled vessels from the complex and\nnoisy backgrounds of X-ray coronary angiography (XCA). However, the challenges\ncaused by dynamic backgrounds, overlapping heterogeneous environments and\ncomplex noises still exist in video decomposition. To solve these problems,\nthis study is the first to introduce a flexible visual working memory model in\nvideo decomposition tasks to provide interpretable and high-performance\nhierarchical deep architecture, integrating the transformative representations\nbetween sensory and control layers from the perspective of visual and cognitive\nneuroscience. Specifically, robust PCA unrolling networks acting as a\nstructure-regularized sensor layer decompose XCA into sparse/low-rank\nstructured representations to separate moving contrast-filled vessels from\nnoisy and complex backgrounds. Then, patch recurrent convolutional LSTM\nnetworks with a backprojection module embody unstructured random\nrepresentations of the control layer in working memory, recurrently projecting\nspatiotemporally decomposed nonlocal patches into orthogonal subspaces for\nheterogeneous vessel retrieval and interference suppression. This video\ndecomposition deep architecture effectively restores the heterogeneous profiles\nof intensity and the geometries of moving objects against the complex\nbackground interferences. Experiments show that the proposed method\nsignificantly outperforms state-of-the-art methods in accurate moving\ncontrast-filled vessel extraction with excellent flexibility and computational\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Binjie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1\">Haohao Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yueqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Song Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Model-Based Super-Resolution with Non-uniform Blur. (arXiv:2204.10109v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10109","description":"<p>We propose a state-of-the-art method for super-resolution with non-uniform\nblur. Single-image super-resolution methods seek to restore a high-resolution\nimage from blurred, subsampled, and noisy measurements. Despite their\nimpressive performance, existing techniques usually assume a uniform blur\nkernel. Hence, these techniques do not generalize well to the more general case\nof non-uniform blur. Instead, in this paper, we address the more realistic and\ncomputationally challenging case of spatially-varying blur. To this end, we\nfirst propose a fast deep plug-and-play algorithm, based on linearized ADMM\nsplitting techniques, which can solve the super-resolution problem with\nspatially-varying blur. Second, we unfold our iterative algorithm into a single\nnetwork and train it end-to-end. In this way, we overcome the intricacy of\nmanually tuning the parameters involved in the optimization scheme. Our\nalgorithm presents remarkable performance and generalizes well after a single\ntraining to a large family of spatially-varying blur kernels, noise levels and\nscale factors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laroche_C/0/1/0/all/0/1\">Charles Laroche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almansa_A/0/1/0/all/0/1\">Andr&#xe9;s Almansa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tassano_M/0/1/0/all/0/1\">Matias Tassano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OSSO: Obtaining Skeletal Shape from Outside. (arXiv:2204.10129v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10129","description":"<p>We address the problem of inferring the anatomic skeleton of a person, in an\narbitrary pose, from the 3D surface of the body; i.e. we predict the inside\n(bones) from the outside (skin). This has many applications in medicine and\nbiomechanics. Existing state-of-the-art biomechanical skeletons are detailed\nbut do not easily generalize to new subjects. Additionally, computer vision and\ngraphics methods that predict skeletons are typically heuristic, not learned\nfrom data, do not leverage the full 3D body surface, and are not validated\nagainst ground truth. To our knowledge, our system, called OSSO (Obtaining\nSkeletal Shape from Outside), is the first to learn the mapping from the 3D\nbody surface to the internal skeleton from real data. We do so using 1000 male\nand 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit\na parametric 3D body shape model (STAR) to capture the body surface and a novel\npart-based 3D skeleton model to capture the bones. This provides inside/outside\ntraining pairs. We model the statistical variation of full skeletons using PCA\nin a pose-normalized space. We then train a regressor from body shape\nparameters to skeleton shape parameters and refine the skeleton to satisfy\nconstraints on physical plausibility. Given an arbitrary 3D body shape and\npose, OSSO predicts a realistic skeleton inside. In contrast to previous work,\nwe evaluate the accuracy of the skeleton shape quantitatively on held-out DXA\nscans, outperforming the state-of-the-art. We also show 3D skeleton prediction\nfrom varied and challenging 3D bodies. The code to infer a skeleton from a body\nshape is available for research at https://osso.is.tue.mpg.de/, and the dataset\nof paired outer surface (skin) and skeleton (bone) meshes is available as a\nBiobank Returned Dataset. This research has been conducted using the UK Biobank\nResource.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keller_M/0/1/0/all/0/1\">Marilyn Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuffi_S/0/1/0/all/0/1\">Silvia Zuffi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujades_S/0/1/0/all/0/1\">Sergi Pujades</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Fast, Flexible, and Robust Low-Light Image Enhancement. (arXiv:2204.10137v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10137","description":"<p>Existing low-light image enhancement techniques are mostly not only difficult\nto deal with both visual quality and computational efficiency but also commonly\ninvalid in unknown complex scenarios. In this paper, we develop a new\nSelf-Calibrated Illumination (SCI) learning framework for fast, flexible, and\nrobust brightening images in real-world low-light scenarios. To be specific, we\nestablish a cascaded illumination learning process with weight sharing to\nhandle this task. Considering the computational burden of the cascaded pattern,\nwe construct the self-calibrated module which realizes the convergence between\nresults of each stage, producing the gains that only use the single basic block\nfor inference (yet has not been exploited in previous works), which drastically\ndiminishes computation cost. We then define the unsupervised training loss to\nelevate the model capability that can adapt to general scenes. Further, we make\ncomprehensive explorations to excavate SCI's inherent properties (lacking in\nexisting works) including operation-insensitive adaptability (acquiring stable\nperformance under the settings of different simple operations) and\nmodel-irrelevant generality (can be applied to illumination-based existing\nworks to improve performance). Finally, plenty of experiments and ablation\nstudies fully indicate our superiority in both quality and efficiency.\nApplications on low-light face detection and nighttime semantic segmentation\nfully reveal the latent practical values for SCI. The source code is available\nat https://github.com/vis-opt-group/SCI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Long Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Risheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhongxuan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple EffNet/ResNet Architectures for Melanoma Classification. (arXiv:2204.10142v1 [eess.IV])","link":"http://arxiv.org/abs/2204.10142","description":"<p>Melanoma is the most malignant skin tumor and usually cancerates from normal\nmoles, which is difficult to distinguish benign from malignant in the early\nstage. Therefore, many machine learning methods are trying to make auxiliary\nprediction. However, these methods attach more attention to the image data of\nsuspected tumor, and focus on improving the accuracy of image classification,\nbut ignore the significance of patient-level contextual information for disease\ndiagnosis in actual clinical diagnosis. To make more use of patient information\nand improve the accuracy of diagnosis, we propose a new melanoma classification\nmodel based on EffNet and Resnet. Our model not only uses images within the\nsame patient but also consider patient-level contextual information for better\ncancer prediction. The experimental results demonstrated that the proposed\nmodel achieved 0.981 ACC. Furthermore, we note that the overall ROC value of\nthe model is 0.976 which is better than the previous state-of-the-art\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xue_J/0/1/0/all/0/1\">Jiaqi Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_C/0/1/0/all/0/1\">Chentian Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_X/0/1/0/all/0/1\">Xuan Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A case for using rotation invariant features in state of the art feature matchers. (arXiv:2204.10144v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10144","description":"<p>The aim of this paper is to demonstrate that a state of the art feature\nmatcher (LoFTR) can be made more robust to rotations by simply replacing the\nbackbone CNN with a steerable CNN which is equivariant to translations and\nimage rotations. It is experimentally shown that this boost is obtained without\nreducing performance on ordinary illumination and viewpoint matching sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bokman_G/0/1/0/all/0/1\">Georg B&#xf6;kman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahl_F/0/1/0/all/0/1\">Fredrik Kahl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebFace260M: A Benchmark for Million-Scale Deep Face Recognition. (arXiv:2204.10149v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10149","description":"<p>Face benchmarks empower the research community to train and evaluate\nhigh-performance face recognition systems. In this paper, we contribute a new\nmillion-scale recognition benchmark, containing uncurated 4M identities/260M\nfaces (WebFace260M) and cleaned 2M identities/42M faces (WebFace42M) training\ndata, as well as an elaborately designed time-constrained evaluation protocol.\nFirstly, we collect 4M name lists and download 260M faces from the Internet.\nThen, a Cleaning Automatically utilizing Self-Training (CAST) pipeline is\ndevised to purify the tremendous WebFace260M, which is efficient and scalable.\nTo the best of our knowledge, the cleaned WebFace42M is the largest public face\nrecognition training set and we expect to close the data gap between academia\nand industry. Referring to practical deployments, Face Recognition Under\nInference Time conStraint (FRUITS) protocol and a new test set with rich\nattributes are constructed. Besides, we gather a large-scale masked face\nsub-set for biometrics assessment under COVID-19. For a comprehensive\nevaluation of face matchers, three recognition tasks are performed under\nstandard, masked and unbiased settings, respectively. Equipped with this\nbenchmark, we delve into million-scale face recognition problems. A distributed\nframework is developed to train face recognition models efficiently without\ntampering with the performance. Enabled by WebFace42M, we reduce 40% failure\nrate on the challenging IJB-C set and rank 3rd among 430 entries on NIST-FRVT.\nEven 10% data (WebFace4M) shows superior performance compared with the public\ntraining sets. Furthermore, comprehensive baselines are established under the\nFRUITS-100/500/1000 milliseconds protocols. The proposed benchmark shows\nenormous potential on standard, masked and unbiased face recognition scenarios.\nOur WebFace260M website is https://www.face-benchmark.org.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiankang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yun Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinze Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiagang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1\">Dalong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Person Video Dataset Annotation Method of Spatio-Temporally Actions. (arXiv:2204.10160v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10160","description":"<p>Spatio-temporal action detection is an important and challenging problem in\nvideo understanding. However, the application of the existing large-scale\nspatio-temporal action datasets in specific fields is limited, and there is\ncurrently no public tool for making spatio-temporal action datasets, it takes a\nlot of time and effort for researchers to customize the spatio-temporal action\ndatasets, so we propose a multi-Person video dataset Annotation Method of\nspatio-temporally actions.First, we use ffmpeg to crop the videos and frame the\nvideos; then use yolov5 to detect human in the video frame, and then use deep\nsort to detect the ID of the human in the video frame. By processing the\ndetection results of yolov5 and deep sort, we can get the annotation file of\nthe spatio-temporal action dataset to complete the work of customizing the\nspatio-temporal action dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated analysis of fibrous cap in intravascular optical coherence tomography images of coronary arteries. (arXiv:2204.10162v1 [cs.LG])","link":"http://arxiv.org/abs/2204.10162","description":"<p>Thin-cap fibroatheroma (TCFA) and plaque rupture have been recognized as the\nmost frequent risk factor for thrombosis and acute coronary syndrome.\nIntravascular optical coherence tomography (IVOCT) can identify TCFA and assess\ncap thickness, which provides an opportunity to assess plaque vulnerability. We\ndeveloped an automated method that can detect lipidous plaque and assess\nfibrous cap thickness in IVOCT images. This study analyzed a total of 4,360\nIVOCT image frames of 77 lesions among 41 patients. To improve segmentation\nperformance, preprocessing included lumen segmentation, pixel-shifting, and\nnoise filtering on the raw polar (r, theta) IVOCT images. We used the\nDeepLab-v3 plus deep learning model to classify lipidous plaque pixels. After\nlipid detection, we automatically detected the outer border of the fibrous cap\nusing a special dynamic programming algorithm and assessed the cap thickness.\nOur method provided excellent discriminability of lipid plaque with a\nsensitivity of 85.8% and A-line Dice coefficient of 0.837. By comparing lipid\nangle measurements between two analysts following editing of our automated\nsoftware, we found good agreement by Bland-Altman analysis (difference 6.7+/-17\ndegree; mean 196 degree). Our method accurately detected the fibrous cap from\nthe detected lipid plaque. Automated analysis required a significant\nmodification for only 5.5% frames. Furthermore, our method showed a good\nagreement of fibrous cap thickness between two analysts with Bland-Altman\nanalysis (4.2+/-14.6 micron; mean 175 micron), indicating little bias between\nusers and good reproducibility of the measurement. We developed a fully\nautomated method for fibrous cap quantification in IVOCT images, resulting in\ngood agreement with determinations by analysts. The method has great potential\nto enable highly automated, repeatable, and comprehensive evaluations of TCFAs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Juhwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1\">Gabriel T. R. Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gharaibeh_Y/0/1/0/all/0/1\">Yazan Gharaibeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolluru_C/0/1/0/all/0/1\">Chaitanya Kolluru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimin_V/0/1/0/all/0/1\">Vladislav N. Zimin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dallan_L/0/1/0/all/0/1\">Luis A. P. Dallan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Justin N. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoori_A/0/1/0/all/0/1\">Ammar Hoori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Kindi_S/0/1/0/all/0/1\">Sadeer G. Al-Kindi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guagliumi_G/0/1/0/all/0/1\">Giulio Guagliumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bezerra_H/0/1/0/all/0/1\">Hiram G. Bezerra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1\">David L. Wilson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BTranspose: Bottleneck Transformers for Human Pose Estimation with Self-Supervised Pre-Training. (arXiv:2204.10209v1 [cs.LG])","link":"http://arxiv.org/abs/2204.10209","description":"<p>The task of 2D human pose estimation is challenging as the number of\nkeypoints is typically large (~ 17) and this necessitates the use of robust\nneural network architectures and training pipelines that can capture the\nrelevant features from the input image. These features are then aggregated to\nmake accurate heatmap predictions from which the final keypoints of human body\nparts can be inferred. Many papers in literature use CNN-based architectures\nfor the backbone, and/or combine it with a transformer, after which the\nfeatures are aggregated to make the final keypoint predictions [1]. In this\npaper, we consider the recently proposed Bottleneck Transformers [2], which\ncombine CNN and multi-head self attention (MHSA) layers effectively, and we\nintegrate it with a Transformer encoder and apply it to the task of 2D human\npose estimation. We consider different backbone architectures and pre-train\nthem using the DINO self-supervised learning method [3], this pre-training is\nfound to improve the overall prediction accuracy. We call our model BTranspose,\nand experiments show that on the COCO validation set, our model achieves an AP\nof 76.4, which is competitive with other methods such as [1] and has fewer\nnetwork parameters. Furthermore, we also present the dependencies of the final\npredicted keypoints on both the MHSA block and the Transformer encoder layers,\nproviding clues on the image sub-regions the network attends to at the mid and\nhigh levels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_K/0/1/0/all/0/1\">Kaushik Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_D/0/1/0/all/0/1\">Devesh Upadhyay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SmartPortraits: Depth Powered Handheld Smartphone Dataset of Human Portraits for State Estimation, Reconstruction and Synthesis. (arXiv:2204.10211v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10211","description":"<p>We present a dataset of 1000 video sequences of human portraits recorded in\nreal and uncontrolled conditions by using a handheld smartphone accompanied by\nan external high-quality depth camera. The collected dataset contains 200\npeople captured in different poses and locations and its main purpose is to\nbridge the gap between raw measurements obtained from a smartphone and\ndownstream applications, such as state estimation, 3D reconstruction, view\nsynthesis, etc. The sensors employed in data collection are the smartphone's\ncamera and Inertial Measurement Unit (IMU), and an external Azure Kinect DK\ndepth camera software synchronized with sub-millisecond precision to the\nsmartphone system. During the recording, the smartphone flash is used to\nprovide a periodic secondary source of lightning. Accurate mask of the foremost\nperson is provided as well as its impact on the camera alignment accuracy. For\nevaluation purposes, we compare multiple state-of-the-art camera alignment\nmethods by using a Motion Capture system. We provide a smartphone\nvisual-inertial benchmark for portrait capturing, where we report results for\nmultiple methods and motivate further use of the provided trajectories,\navailable in the dataset, in view synthesis and 3D reconstruction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kornilova_A/0/1/0/all/0/1\">Anastasiia Kornilova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faizullin_M/0/1/0/all/0/1\">Marsel Faizullin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pakulev_K/0/1/0/all/0/1\">Konstantin Pakulev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadkov_A/0/1/0/all/0/1\">Andrey Sadkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukushkin_D/0/1/0/all/0/1\">Denis Kukushkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhmetyanov_A/0/1/0/all/0/1\">Azat Akhmetyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtyamov_T/0/1/0/all/0/1\">Timur Akhtyamov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taherinejad_H/0/1/0/all/0/1\">Hekmat Taherinejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_G/0/1/0/all/0/1\">Gonzalo Ferrer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OCTOPUS -- optical coherence tomography plaque and stent analysis software. (arXiv:2204.10212v1 [eess.IV])","link":"http://arxiv.org/abs/2204.10212","description":"<p>Compared with other imaging modalities, intravascular optical coherence\ntomography (IVOCT) has significant advantages for guiding percutaneous coronary\ninterventions. To aid IVOCT research studies, we developed the Optical\nCoherence TOmography PlaqUe and Stent (OCTOPUS) analysis software. To automate\nimage analysis results, the software includes several important algorithmic\nsteps: pre-processing, deep learning plaque segmentation, machine learning\nidentification of stent struts, and registration of pullbacks. Interactive\nvisualization and manual editing of segmentations were included in the\nsoftware. Quantifications include stent deployment characteristics (e.g., stent\nstrut malapposition), strut level analysis, calcium angle, and calcium\nthickness measurements. Interactive visualizations include (x,y) anatomical, en\nface, and longitudinal views with optional overlays. Underlying plaque\nsegmentation algorithm yielded excellent pixel-wise results (86.2% sensitivity\nand 0.781 F1 score). Using OCTOPUS on 34 new pullbacks, we determined that\nfollowing automated segmentation, only 13% and 23% of frames needed any manual\ntouch up for detailed lumen and calcification labeling, respectively. Only up\nto 3.8% of plaque pixels were modified, leading to an average editing time of\nonly 7.5 seconds/frame, an approximately 80% reduction compared to manual\nanalysis. Regarding stent analysis, sensitivity and precision were both greater\nthan 90%, and each strut was successfully classified as either covered or\nuncovered with high sensitivity (94%) and specificity (90%). We introduced and\nevaluated the clinical application of a highly automated software package,\nOCTOPUS, for quantitative plaque and stent analysis in IVOCT images. The\nsoftware is currently used as an offline tool for research purposes; however,\nthe software's embedded algorithms may also be useful for real-time treatment\nplanning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Juhwan Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1\">Justin N. Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gharaibeh_Y/0/1/0/all/0/1\">Yazan Gharaibeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zimin_V/0/1/0/all/0/1\">Vladislav N. Zimin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dallan_L/0/1/0/all/0/1\">Luis A. P. Dallan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pereira_G/0/1/0/all/0/1\">Gabriel T. R. Pereira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vergara_Martel_A/0/1/0/all/0/1\">Armando Vergara-Martel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kolluru_C/0/1/0/all/0/1\">Chaitanya Kolluru</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoori_A/0/1/0/all/0/1\">Ammar Hoori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bezerra_H/0/1/0/all/0/1\">Hiram G. Bezerra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wilson_D/0/1/0/all/0/1\">David L. Wilson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Planes vs. Chairs: Category-guided 3D shape learning without any 3D cues. (arXiv:2204.10235v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10235","description":"<p>We present a novel 3D shape reconstruction method which learns to predict an\nimplicit 3D shape representation from a single RGB image. Our approach uses a\nset of single-view images of multiple object categories without viewpoint\nannotation, forcing the model to learn across multiple object categories\nwithout 3D supervision. To facilitate learning with such minimal supervision,\nwe use category labels to guide shape learning with a novel categorical metric\nlearning approach. We also utilize adversarial and viewpoint regularization\ntechniques to further disentangle the effects of viewpoint and shape. We obtain\nthe first results for large-scale (more than 50 categories) single-viewpoint\nshape prediction using a single model without any 3D cues. We are also the\nfirst to examine and quantify the benefit of class information in single-view\nsupervised 3D shape reconstruction. Our method achieves superior performance\nover state-of-the-art methods on ShapeNet-13, ShapeNet-55 and Pascal3D+.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zixuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stojanov_S/0/1/0/all/0/1\">Stefan Stojanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thai_A/0/1/0/all/0/1\">Anh Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1\">James M. Rehg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HEATGait: Hop-Extracted Adjacency Technique in Graph Convolution based Gait Recognition. (arXiv:2204.10238v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10238","description":"<p>Biometric authentication using gait has become a promising field due to its\nunobtrusive nature. Recent approaches in model-based gait recognition\ntechniques utilize spatio-temporal graphs for the elegant extraction of gait\nfeatures. However, existing methods often rely on multi-scale operators for\nextracting long-range relationships among joints resulting in biased weighting.\nIn this paper, we present HEATGait, a gait recognition system that improves the\nexisting multi-scale graph convolution by efficient hop-extraction technique to\nalleviate the issue. Combined with preprocessing and augmentation techniques,\nwe propose a powerful feature extractor that utilizes ResGCN to achieve\nstate-of-the-art performance in model-based gait recognition on the CASIA-B\ngait dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md. Bakhtiar Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_T/0/1/0/all/0/1\">Tasnim Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Md. Hasanul Kabir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Examination of Bias of Facial Analysis based BMI Prediction Models. (arXiv:2204.10262v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10262","description":"<p>Obesity is one of the most important public health problems that the world is\nfacing today. A recent trend is in the development of intervention tools that\npredict BMI using facial images for weight monitoring and management to combat\nobesity. Most of these studies used BMI annotated facial image datasets that\nmainly consisted of Caucasian subjects. Research on bias evaluation of\nface-based gender-, age-classification, and face recognition systems suggest\nthat these technologies perform poorly for women, dark-skinned people, and\nolder adults. The bias of facial analysis-based BMI prediction tools has not\nbeen studied until now. This paper evaluates the bias of facial-analysis-based\nBMI prediction models across Caucasian and African-American Males and Females.\nExperimental investigations on the gender, race, and BMI balanced version of\nthe modified MORPH-II dataset suggested that the error rate in BMI prediction\nwas least for Black Males and highest for White Females. Further, the\npsychology-related facial features correlated with weight suggested that as the\nBMI increases, the changes in the facial region are more prominent for Black\nMales and the least for White Females. This is the reason for the least error\nrate of the facial analysis-based BMI prediction tool for Black Males and\nhighest for White Females.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_H/0/1/0/all/0/1\">Hera Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rattani_A/0/1/0/all/0/1\">Ajita Rattani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricanek_K/0/1/0/all/0/1\">Karl Ricanek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_T/0/1/0/all/0/1\">Twyla Hill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DooDLeNet: Double DeepLab Enhanced Feature Fusion for Thermal-color Semantic Segmentation. (arXiv:2204.10266v1 [cs.LG])","link":"http://arxiv.org/abs/2204.10266","description":"<p>In this paper we present a new approach for feature fusion between RGB and\nLWIR Thermal images for the task of semantic segmentation for driving\nperception. We propose DooDLeNet, a double DeepLab architecture with\nspecialized encoder-decoders for thermal and color modalities and a shared\ndecoder for final segmentation. We combine two strategies for feature fusion:\nconfidence weighting and correlation weighting. We report state-of-the-art mean\nIoU results on the MF dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frigo_O/0/1/0/all/0/1\">Oriel Frigo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Gaffe_L/0/1/0/all/0/1\">Lucien Martin-Gaff&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wacongne_C/0/1/0/all/0/1\">Catherine Wacongne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Share With Thy Neighbors: Single-View Reconstruction by Cross-Instance Consistency. (arXiv:2204.10310v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10310","description":"<p>Approaches to single-view reconstruction typically rely on viewpoint\nannotations, silhouettes, the absence of background, multiple views of the same\ninstance, a template shape, or symmetry. We avoid all of these supervisions and\nhypotheses by leveraging explicitly the consistency between images of different\nobject instances. As a result, our method can learn from large collections of\nunlabelled images depicting the same object category. Our main contributions\nare two approaches to leverage cross-instance consistency: (i) progressive\nconditioning, a training strategy to gradually specialize the model from\ncategory to instances in a curriculum learning fashion; (ii) swap\nreconstruction, a loss enforcing consistency between instances having similar\nshape or texture. Critical to the success of our method are also: our\nstructured autoencoding architecture decomposing an image into explicit shape,\ntexture, pose, and background; an adapted formulation of differential\nrendering, and; a new optimization scheme alternating between 3D and pose\nlearning. We compare our approach, UNICORN, both on the diverse synthetic\nShapeNet dataset - the classical benchmark for methods requiring multiple views\nas supervision - and on standard real-image benchmarks (Pascal3D+ Car, CUB-200)\nfor which most methods require known templates and silhouette annotations. We\nalso showcase applicability to more challenging real-world collections\n(CompCars, LSUN), where silhouettes are not available and images are not\ncropped around the object.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Monnier_T/0/1/0/all/0/1\">Tom Monnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1\">Matthew Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1\">Alexei A. Efros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubry_M/0/1/0/all/0/1\">Mathieu Aubry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Human Action Recognition with Skeletal Graph Laplacian and Self-Supervised Viewpoints Invariance. (arXiv:2204.10312v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10312","description":"<p>This paper presents a novel end-to-end method for the problem of\nskeleton-based unsupervised human action recognition. We propose a new\narchitecture with a convolutional autoencoder that uses graph Laplacian\nregularization to model the skeletal geometry across the temporal dynamics of\nactions. Our approach is robust towards viewpoint variations by including a\nself-supervised gradient reverse layer that ensures generalization across\ncamera views. The proposed method is validated on NTU-60 and NTU-120\nlarge-scale datasets in which it outperforms all prior unsupervised\nskeleton-based approaches on the cross-subject, cross-view, and cross-setup\nprotocols. Although unsupervised, our learnable representation allows our\nmethod even to surpass a few supervised skeleton-based action recognition\nmethods. The code is available in:\nwww.github.com/IIT-PAVIS/UHAR_Skeletal_Laplacian\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paoletti_G/0/1/0/all/0/1\">Giancarlo Paoletti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavazza_J/0/1/0/all/0/1\">Jacopo Cavazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyan_C/0/1/0/all/0/1\">Cigdem Beyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bue_A/0/1/0/all/0/1\">Alessio Del Bue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Contrastive Learning by Permuting Cluster Assignments. (arXiv:2204.10314v1 [cs.LG])","link":"http://arxiv.org/abs/2204.10314","description":"<p>Contrastive learning has gained popularity as an effective self-supervised\nrepresentation learning technique. Several research directions improve\ntraditional contrastive approaches, e.g., prototypical contrastive methods\nbetter capture the semantic similarity among instances and reduce the\ncomputational burden by considering cluster prototypes or cluster assignments,\nwhile adversarial instance-wise contrastive methods improve robustness against\na variety of attacks. To the best of our knowledge, no prior work jointly\nconsiders robustness, cluster-wise semantic similarity and computational\nefficiency. In this work, we propose SwARo, an adversarial contrastive\nframework that incorporates cluster assignment permutations to generate\nrepresentative adversarial samples. We evaluate SwARo on multiple benchmark\ndatasets and against various white-box and black-box attacks, obtaining\nconsistent improvements over state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahed_M/0/1/0/all/0/1\">Muntasir Wahed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabassum_A/0/1/0/all/0/1\">Afrina Tabassum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1\">Ismini Lourentzou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature anomaly detection system (FADS) for intelligent manufacturing. (arXiv:2204.10318v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10318","description":"<p>Anomaly detection is important for industrial automation and part quality\nassurance, and while humans can easily detect anomalies in components given a\nfew examples, designing a generic automated system that can perform at human or\nabove human capabilities remains a challenge. In this work, we present a simple\nnew anomaly detection algorithm called FADS (feature-based anomaly detection\nsystem) which leverages pretrained convolutional neural networks (CNN) to\ngenerate a statistical model of nominal inputs by observing the activation of\nthe convolutional filters. During inference the system compares the\nconvolutional filter activation of the new input to the statistical model and\nflags activations that are outside the expected range of values and therefore\nlikely an anomaly. By using a pretrained network, FADS demonstrates excellent\nperformance similar to or better than other machine learning approaches to\nanomaly detection while at the same time FADS requires no tuning of the CNN\nweights. We demonstrate FADS ability by detecting process parameter changes on\na custom dataset of additively manufactured lattices. The FADS localization\nalgorithm shows that textural differences that are visible on the surface can\nbe used to detect process parameter changes. In addition, we test FADS on\nbenchmark datasets, such as the MVTec Anomaly Detection dataset, and report\ngood results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garland_A/0/1/0/all/0/1\">Anthony Garland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potter_K/0/1/0/all/0/1\">Kevin Potter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1\">Matt Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TorchSparse: Efficient Point Cloud Inference Engine. (arXiv:2204.10319v1 [cs.LG])","link":"http://arxiv.org/abs/2204.10319","description":"<p>Deep learning on point clouds has received increased attention thanks to its\nwide applications in AR/VR and autonomous driving. These applications require\nlow latency and high accuracy to provide real-time user experience and ensure\nuser safety. Unlike conventional dense workloads, the sparse and irregular\nnature of point clouds poses severe challenges to running sparse CNNs\nefficiently on the general-purpose hardware. Furthermore, existing sparse\nacceleration techniques for 2D images do not translate to 3D point clouds. In\nthis paper, we introduce TorchSparse, a high-performance point cloud inference\nengine that accelerates the sparse convolution computation on GPUs. TorchSparse\ndirectly optimizes the two bottlenecks of sparse convolution: irregular\ncomputation and data movement. It applies adaptive matrix multiplication\ngrouping to trade computation for better regularity, achieving 1.4-1.5x speedup\nfor matrix multiplication. It also optimizes the data movement by adopting\nvectorized, quantized and fused locality-aware memory access, reducing the\nmemory movement cost by 2.7x. Evaluated on seven representative models across\nthree benchmark datasets, TorchSparse achieves 1.6x and 1.5x measured\nend-to-end speedup over the state-of-the-art MinkowskiEngine and SpConv,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haotian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhijian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiuyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yujun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SelfD: Self-Learning Large-Scale Driving Policies From the Web. (arXiv:2204.10320v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10320","description":"<p>Effectively utilizing the vast amounts of ego-centric navigation data that is\nfreely available on the internet can advance generalized intelligent systems,\ni.e., to robustly scale across perspectives, platforms, environmental\nconditions, scenarios, and geographical locations. However, it is difficult to\ndirectly leverage such large amounts of unlabeled and highly diverse data for\ncomplex 3D reasoning and planning tasks. Consequently, researchers have\nprimarily focused on its use for various auxiliary pixel- and image-level\ncomputer vision tasks that do not consider an ultimate navigational objective.\nIn this work, we introduce SelfD, a framework for learning scalable driving by\nutilizing large amounts of online monocular images. Our key idea is to leverage\niterative semi-supervised training when learning imitative agents from\nunlabeled data. To handle unconstrained viewpoints, scenes, and camera\nparameters, we train an image-based model that directly learns to plan in the\nBird's Eye View (BEV) space. Next, we use unlabeled data to augment the\ndecision-making knowledge and robustness of an initially trained model via\nself-training. In particular, we propose a pseudo-labeling step which enables\nmaking full use of highly diverse demonstration data through \"hypothetical\"\nplanning-based data augmentation. We employ a large dataset of publicly\navailable YouTube videos to train SelfD and comprehensively analyze its\ngeneralization benefits across challenging navigation scenarios. Without\nrequiring any additional data collection or annotation efforts, SelfD\ndemonstrates consistent improvements (by up to 24%) in driving performance\nevaluation on nuScenes, Argoverse, Waymo, and CARLA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jimuyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Ruizhao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohn_Bar_E/0/1/0/all/0/1\">Eshed Ohn-Bar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Future Object Prediction with a Spatiotemporal Detection Transformer. (arXiv:2204.10321v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10321","description":"<p>We explore future object prediction -- a challenging problem where all\nobjects visible in a future video frame are to be predicted. We propose to\ntackle this problem end-to-end by training a detection transformer to directly\noutput future objects. In order to make accurate predictions about the future,\nit is necessary to capture the dynamics in the scene, both of other objects and\nof the ego-camera. We extend existing detection transformers in two ways to\ncapture the scene dynamics. First, we experiment with three different\nmechanisms that enable the model to spatiotemporally process multiple frames.\nSecond, we feed ego-motion information to the model via cross-attention. We\nshow that both of these cues substantially improve future object prediction\nperformance. Our final approach learns to capture the dynamics and make\npredictions on par with an oracle for 100 ms prediction horizons, and\noutperform baselines for longer prediction horizons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tonderski_A/0/1/0/all/0/1\">Adam Tonderski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnander_J/0/1/0/all/0/1\">Joakim Johnander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_C/0/1/0/all/0/1\">Christoffer Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+%7B%5CAA%7Dstrom_K/0/1/0/all/0/1\">Kalle &#xc5;str&#xf6;m</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Deep Hashing Methods. (arXiv:2003.03369v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.03369","description":"<p>Nearest neighbor search aims to obtain the samples in the database with the\nsmallest distances from them to the queries, which is a fundamental problem in\nvarious domains, such as computer vision, recommendation systems and machine\nlearning. Hashing is one of the most widely used methods for its computational\nand storage efficiency. With the development of deep learning, deep hashing\nmethods show more advantages than traditional methods. In this survey, we\ndetailedly investigate current deep hashing algorithms including deep\nsupervised hashing and deep unsupervised hashing. Specifically, we categorize\ndeep supervised hashing methods into pairwise methods, ranking-based methods,\npointwise methods as well as quantization according to how measuring the\nsimilarities of the learned hash codes. Moreover, deep unsupervised hashing is\ncategorized into similarity reconstruction-based methods, pseudo-label-based\nmethods and prediction-free self-supervised learning-based methods based on\ntheir semantic learning manners. We also introduce three related important\ntopics including semi-supervised deep hashing, domain adaption deep hashing and\nmulti-modal deep hashing. Meanwhile, we present some commonly used public\ndatasets and the scheme to measure the performance of deep hashing algorithms.\nFinally, we discuss some potential research directions in conclusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haixin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Daqing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1\">Minghua Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAM: Self-supervised Learning of Pixel-wise Anatomical Embeddings in Radiological Images. (arXiv:2012.02383v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.02383","description":"<p>Radiological images such as computed tomography (CT) and X-rays render\nanatomy with intrinsic structures. Being able to reliably locate the same\nanatomical structure across varying images is a fundamental task in medical\nimage analysis. In principle it is possible to use landmark detection or\nsemantic segmentation for this task, but to work well these require large\nnumbers of labeled data for each anatomical structure and sub-structure of\ninterest. A more universal approach would learn the intrinsic structure from\nunlabeled images. We introduce such an approach, called Self-supervised\nAnatomical eMbedding (SAM). SAM generates semantic embeddings for each image\npixel that describes its anatomical location or body part. To produce such\nembeddings, we propose a pixel-level contrastive learning framework. A\ncoarse-to-fine strategy ensures both global and local anatomical information\nare encoded. Negative sample selection strategies are designed to enhance the\nembedding's discriminability. Using SAM, one can label any point of interest on\na template image and then locate the same body part in other images by simple\nnearest neighbor searching. We demonstrate the effectiveness of SAM in multiple\ntasks with 2D and 3D image modalities. On a chest CT dataset with 19 landmarks,\nSAM outperforms widely-used registration algorithms while only taking 0.23\nseconds for inference. On two X-ray datasets, SAM, with only one labeled\ntemplate image, surpasses supervised methods trained on 50 labeled images. We\nalso apply SAM on whole-body follow-up lesion matching in CT and obtain an\naccuracy of 91%. SAM can also be applied for improving image registration and\ninitializing CNN weights.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Ke Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jinzheng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Dakai Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_S/0/1/0/all/0/1\">Shun Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Dazhou Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_A/0/1/0/all/0/1\">Adam P. Harrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Youbao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingjing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Le Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"F-SIOL-310: A Robotic Dataset and Benchmark for Few-Shot Incremental Object Learning. (arXiv:2103.12242v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2103.12242","description":"<p>Deep learning has achieved remarkable success in object recognition tasks\nthrough the availability of large scale datasets like ImageNet. However, deep\nlearning systems suffer from catastrophic forgetting when learning\nincrementally without replaying old data. For real-world applications, robots\nalso need to incrementally learn new objects. Further, since robots have\nlimited human assistance available, they must learn from only a few examples.\nHowever, very few object recognition datasets and benchmarks exist to test\nincremental learning capability for robotic vision. Further, there is no\ndataset or benchmark specifically designed for incremental object learning from\na few examples. To fill this gap, we present a new dataset termed F-SIOL-310\n(Few-Shot Incremental Object Learning) which is specifically captured for\ntesting few-shot incremental object learning capability for robotic vision. We\nalso provide benchmarks and evaluations of 8 incremental learning algorithms on\nF-SIOL-310 for future comparisons. Our results demonstrate that the few-shot\nincremental object learning problem for robotic vision is far from being\nsolved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayub_A/0/1/0/all/0/1\">Ali Ayub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_A/0/1/0/all/0/1\">Alan R. Wagner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discrete Cosine Transform Network for Guided Depth Map Super-Resolution. (arXiv:2104.06977v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.06977","description":"<p>Guided depth super-resolution (GDSR) is an essential topic in multi-modal\nimage processing, which reconstructs high-resolution (HR) depth maps from\nlow-resolution ones collected with suboptimal conditions with the help of HR\nRGB images of the same scene. To solve the challenges in interpreting the\nworking mechanism, extracting cross-modal features and RGB texture\nover-transferred, we propose a novel Discrete Cosine Transform Network (DCTNet)\nto alleviate the problems from three aspects. First, the Discrete Cosine\nTransform (DCT) module reconstructs the multi-channel HR depth features by\nusing DCT to solve the channel-wise optimization problem derived from the image\ndomain. Second, we introduce a semi-coupled feature extraction module that uses\nshared convolutional kernels to extract common information and private kernels\nto extract modality-specific information. Third, we employ an edge attention\nmechanism to highlight the contours informative for guided upsampling.\nExtensive quantitative and qualitative evaluations demonstrate the\neffectiveness of our DCTNet, which outperforms previous state-of-the-art\nmethods with a relatively small number of parameters. The code is available at\n\\url{https://github.com/Zhaozixiang1228/GDSR-DCTNet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zixiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiangshe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zudi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning on Monocular Object Pose Detection and Tracking: A Comprehensive Overview. (arXiv:2105.14291v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14291","description":"<p>Object pose detection and tracking has recently attracted increasing\nattention due to its wide applications in many areas, such as autonomous\ndriving, robotics, and augmented reality. Among methods for object pose\ndetection and tracking, deep learning is the most promising one that has shown\nbetter performance than others. However, survey study about the latest\ndevelopment of deep learning-based methods is lacking. Therefore, this study\npresents a comprehensive review of recent progress in object pose detection and\ntracking that belongs to the deep learning technical route. To achieve a more\nthorough introduction, the scope of this study is limited to methods taking\nmonocular RGB/RGBD data as input and covering three kinds of major tasks:\ninstance-level monocular object pose detection, category-level monocular object\npose detection, and monocular object pose tracking. In our work, metrics,\ndatasets, and methods of both detection and tracking are presented in detail.\nComparative results of current state-of-the-art methods on several publicly\navailable datasets are also presented, together with insightful observations\nand inspiring future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhaoxin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yazhi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image2Point: 3D Point-Cloud Understanding with Pretrained 2D ConvNets. (arXiv:2106.04180v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04180","description":"<p>3D point-clouds and 2D images are different visual representations of the\nphysical world. While human vision can understand both representations,\ncomputer vision models designed for 2D image and 3D point-cloud understanding\nare quite different. Our paper explores the potential of transferring 2D model\narchitectures and weights to understand 3D point-clouds, by empirically\ninvestigating the feasibility of the transfer, the benefits of the transfer,\nand shedding light on why the transfer works. We discover that we can indeed\nuse the same architecture and pretrained weights of a neural net model to\nunderstand both images and point-clouds. Specifically, we transfer the\nimage-pretrained model to a point-cloud model by copying or inflating the\nweights. We find that \\textbf{f}inetuning the transformed\n\\textbf{i}mage-\\textbf{p}retrained models (FIP) with minimal efforts -- only on\ninput, output, and normalization layers -- can achieve competitive performance\non 3D point-cloud classification, beating a wide range of point-cloud models\nthat adopt task-specific architectures and use a variety of tricks. When\nfinetuning the whole model, the performance improves even further. Meanwhile,\nFIP improves data efficiency, reaching up to 10.0 top-1 accuracy percent on\nfew-shot classification. It also speeds up the training of point-cloud models\nby up to 11.1x for a target accuracy (e.g., 90 \\% accuracy). Lastly, we provide\nan explanation of the image to point-cloud transfer from the aspect of\n\\textit{neural collapse}. The code is available at:\n\\url{https://github.com/chenfengxu714/image2point}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shijia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galanti_T/0/1/0/all/0/1\">Tomer Galanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bichen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiangyu Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_B/0/1/0/all/0/1\">Bohan Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1\">Wei Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1\">Peter Vajda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1\">Masayoshi Tomizuka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface. (arXiv:2107.06393v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06393","description":"<p>Modeling complex phenomena typically involves the use of both discrete and\ncontinuous variables. Such a setting applies across a wide range of problems,\nfrom identifying trends in time-series data to performing effective\ncompositional scene understanding in images. Here, we propose Hybrid Memoised\nWake-Sleep (HMWS), an algorithm for effective inference in such hybrid\ndiscrete-continuous models. Prior approaches to learning suffer as they need to\nperform repeated expensive inner-loop discrete inference. We build on a recent\napproach, Memoised Wake-Sleep (MWS), which alleviates part of the problem by\nmemoising discrete variables, and extend it to allow for a principled and\neffective way to handle continuous variables by learning a separate recognition\nmodel used for importance-sampling based approximate inference and\nmarginalization. We evaluate HMWS in the GP-kernel learning and 3D scene\nunderstanding domains, and show that it outperforms current state-of-the-art\ninference methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Tuan Anh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_K/0/1/0/all/0/1\">Katherine M. Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hewitt_L/0/1/0/all/0/1\">Luke Hewitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_K/0/1/0/all/0/1\">Kevin Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1\">N. Siddharth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gershman_S/0/1/0/all/0/1\">Samuel J. Gershman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Certified Robustness for Ensemble Models and Beyond. (arXiv:2107.10873v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.10873","description":"<p>Recent studies show that deep neural networks (DNN) are vulnerable to\nadversarial examples, which aim to mislead DNNs by adding perturbations with\nsmall magnitude. To defend against such attacks, both empirical and theoretical\ndefense approaches have been extensively studied for a single ML model. In this\nwork, we aim to analyze and provide the certified robustness for ensemble ML\nmodels, together with the sufficient and necessary conditions of robustness for\ndifferent ensemble protocols. Although ensemble models are shown more robust\nthan a single model empirically; surprisingly, we find that in terms of the\ncertified robustness the standard ensemble models only achieve marginal\nimprovement compared to a single model. Thus, to explore the conditions that\nguarantee to provide certifiably robust ensemble ML models, we first prove that\ndiversified gradient and large confidence margin are sufficient and necessary\nconditions for certifiably robust ensemble models under the model-smoothness\nassumption. We then provide the bounded model-smoothness analysis based on the\nproposed Ensemble-before-Smoothing strategy. We also prove that an ensemble\nmodel can always achieve higher certified robustness than a single base model\nunder mild conditions. Inspired by the theoretical findings, we propose the\nlightweight Diversity Regularized Training (DRT) to train certifiably robust\nensemble ML models. Extensive experiments show that our DRT enhanced ensembles\ncan consistently achieve higher certified robustness than existing single and\nensemble ML models, demonstrating the state-of-the-art certified L2-robustness\non MNIST, CIFAR-10, and ImageNet datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuolin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaojun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The CORSMAL benchmark for the prediction of the properties of containers. (arXiv:2107.12719v3 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2107.12719","description":"<p>The contactless estimation of the weight of a container and the amount of its\ncontent manipulated by a person are key pre-requisites for safe human-to-robot\nhandovers. However, opaqueness and transparencies of the container and the\ncontent, and variability of materials, shapes, and sizes, make this estimation\ndifficult. In this paper, we present a range of methods and an open framework\nto benchmark acoustic and visual perception for the estimation of the capacity\nof a container, and the type, mass, and amount of its content. The framework\nincludes a dataset, specific tasks and performance measures. We conduct an\nin-depth comparative analysis of methods that used this framework and\naudio-only or vision-only baselines designed from related works. Based on this\nanalysis, we can conclude that audio-only and audio-visual classifiers are\nsuitable for the estimation of the type and amount of the content using\ndifferent types of convolutional neural networks, combined with either\nrecurrent neural networks or a majority voting strategy, whereas computer\nvision methods are suitable to determine the capacity of the container using\nregression and geometric approaches. Classifying the content type and level\nusing only audio achieves a weighted average F1-score up to 81% and 97%,\nrespectively. Estimating the container capacity with vision-only approaches and\nestimating the filling mass with audio-visual multi-stage approaches reach up\nto 65% weighted average capacity and mass scores. These results show that there\nis still room for improvement on the design of new methods. These new methods\ncan be ranked and compared on the individual leaderboards provided by our open\nframework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xompero_A/0/1/0/all/0/1\">Alessio Xompero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donaher_S/0/1/0/all/0/1\">Santiago Donaher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iashin_V/0/1/0/all/0/1\">Vladimir Iashin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palermo_F/0/1/0/all/0/1\">Francesca Palermo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solak_G/0/1/0/all/0/1\">G&#xf6;khan Solak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coppola_C/0/1/0/all/0/1\">Claudio Coppola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_R/0/1/0/all/0/1\">Reina Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagao_Y/0/1/0/all/0/1\">Yuichi Nagao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hachiuma_R/0/1/0/all/0/1\">Ryo Hachiuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Chuanlin Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1\">Rosa H. M. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christmann_G/0/1/0/all/0/1\">Guilherme Christmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jyun-Ting Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neeharika_G/0/1/0/all/0/1\">Gonuguntla Neeharika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chinnakotla Krishna Teja Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1\">Dinesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehman_B/0/1/0/all/0/1\">Bakhtawar Ur Rehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1\">Andrea Cavallaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Raising context awareness in motion forecasting. (arXiv:2109.08048v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.08048","description":"<p>Learning-based trajectory prediction models have encountered great success,\nwith the promise of leveraging contextual information in addition to motion\nhistory. Yet, we find that state-of-the-art forecasting methods tend to overly\nrely on the agent's current dynamics, failing to exploit the semantic\ncontextual cues provided at its input. To alleviate this issue, we introduce\nCAB, a motion forecasting model equipped with a training procedure designed to\npromote the use of semantic contextual information. We also introduce two novel\nmetrics - dispersion and convergence-to-range - to measure the temporal\nconsistency of successive forecasts, which we found missing in standard\nmetrics. Our method is evaluated on the widely adopted nuScenes Prediction\nbenchmark as well as on a subset of the most difficult examples from this\nbenchmark. The code is available at github.com/valeoai/CAB\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Younes_H/0/1/0/all/0/1\">H&#xe9;di Ben-Younes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zablocki_E/0/1/0/all/0/1\">&#xc9;loi Zablocki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Micka&#xeb;l Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Random Dilated Shapelet Transform: A New Approach for Time Series Shapelets. (arXiv:2109.13514v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.13514","description":"<p>Shapelet-based algorithms are widely used for time series classification\nbecause of their ease of interpretation, but they are currently outperformed by\nrecent state-of-the-art approaches. We present a new formulation of time series\nshapelets including the notion of dilation, and we introduce a new shapelet\nfeature to enhance their discriminative power for classification. Experiments\nperformed on 112 datasets show that our method improves on the state-of-the-art\nshapelet algorithm, and achieves comparable accuracy to recent state-of-the-art\napproaches, without sacrificing neither scalability, nor interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guillaume_A/0/1/0/all/0/1\">Antoine Guillaume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vrain_C/0/1/0/all/0/1\">Christel Vrain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wael_E/0/1/0/all/0/1\">Elloumi Wael</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A transformer-based deep learning approach for classifying brain metastases into primary organ sites using clinical whole brain MRI. (arXiv:2110.03588v6 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.03588","description":"<p>Treatment decisions for brain metastatic disease rely on knowledge of the\nprimary organ site, and currently made with biopsy and histology. Here we\ndevelop a novel deep learning approach for accurate non-invasive digital\nhistology with whole-brain MRI data. Our IRB-approved single-site retrospective\nstudy was comprised of patients (n=1,399) referred for MRI treatment-planning\nand gamma knife radiosurgery over 21 years. Contrast-enhanced T1-weighted and\nT2-weighted Fluid-Attenuated Inversion Recovery brain MRI exams (n=1,582) were\npreprocessed and input to the proposed deep learning workflow for tumor\nsegmentation, modality transfer, and primary site classification into one of\nfive classes. Ten-fold cross-validation generated overall AUC of 0.878\n(95%CI:0.873,0.883), lung class AUC of 0.889 (95%CI:0.883,0.895), breast class\nAUC of 0.873 (95%CI:0.860,0.886), melanoma class AUC of 0.852\n(95%CI:0.842,0.862), renal class AUC of 0.830 (95%CI:0.809,0.851), and other\nclass AUC of 0.822 (95%CI:0.805,0.839). These data establish that whole-brain\nimaging features are discriminative to allow accurate diagnosis of the primary\norgan site of malignancy. Our end-to-end deep radiomic approach has great\npotential for classifying metastatic tumor types from whole-brain MRI images.\nFurther refinement may offer an invaluable clinical tool to expedite primary\ncancer site identification for precision treatment and improved outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Namjoshi_S/0/1/0/all/0/1\">Sanjeev V. Namjoshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McTyre_E/0/1/0/all/0/1\">Emory McTyre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Topaloglu_U/0/1/0/all/0/1\">Umit Topaloglu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barcus_R/0/1/0/all/0/1\">Richard Barcus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_M/0/1/0/all/0/1\">Michael D. Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cramer_C/0/1/0/all/0/1\">Christina K. Cramer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Debinski_W/0/1/0/all/0/1\">Waldemar Debinski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gurcan_M/0/1/0/all/0/1\">Metin N. Gurcan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lesser_G/0/1/0/all/0/1\">Glenn J. Lesser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_H/0/1/0/all/0/1\">Hui-Kuan Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munden_R/0/1/0/all/0/1\">Reginald F. Munden</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pasche_B/0/1/0/all/0/1\">Boris C. Pasche</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sai_K/0/1/0/all/0/1\">Kiran Kumar Solingapuram Sai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strowd_R/0/1/0/all/0/1\">Roy E. Strowd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tatter_S/0/1/0/all/0/1\">Stephen B. Tatter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watabe_K/0/1/0/all/0/1\">Kounosuke Watabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Whitlow_C/0/1/0/all/0/1\">Christopher T. Whitlow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Audio-Visual Representation Learning with Relaxed Cross-Modal Synchronicity. (arXiv:2111.05329v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.05329","description":"<p>We present CrissCross, a self-supervised framework for learning audio-visual\nrepresentations. A novel notion is introduced in our framework whereby in\naddition to learning the intra-modal and standard synchronous cross-modal\nrelations, CrissCross also learns asynchronous cross-modal relationships. We\nshow that by relaxing the temporal synchronicity between the audio and visual\nmodalities, the network learns strong generalized representations. Our\nexperiments show that strong augmentations for both audio and visual modalities\nwith relaxation of cross-modal temporal synchronicity optimize performance. To\npretrain our proposed framework, we use 3 different datasets with varying\nsizes, Kinetics-Sound, Kinetics400, and AudioSet. The learned representations\nare evaluated on a number of downstream tasks namely action recognition, sound\nclassification, and retrieval. CrissCross shows state-of-the-art performances\non action recognition (UCF101 and HMDB51) and sound classification (ESC50 and\nDCASE). The codes and pretrained models will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_P/0/1/0/all/0/1\">Pritam Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Domain Adaptation for Pavement Crack Detection. (arXiv:2111.10101v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10101","description":"<p>Deep learning-based pavement cracks detection methods often require\nlarge-scale labels with detailed crack location information to learn accurate\npredictions. In practice, however, crack locations are very difficult to be\nmanually annotated due to various visual patterns of pavement crack. In this\npaper, we propose a Deep Domain Adaptation-based Crack Detection Network\n(DDACDN), which learns to take advantage of the source domain knowledge to\npredict the multi-category crack location information in the target domain,\nwhere only image-level labels are available. Specifically, DDACDN first\nextracts crack features from both the source and target domain by a two-branch\nweights-shared backbone network. And in an effort to achieve the cross-domain\nadaptation, an intermediate domain is constructed by aggregating the\nthree-scale features from the feature space of each domain to adapt the crack\nfeatures from the source domain to the target domain. Finally, the network\ninvolves the knowledge of both domains and is trained to recognize and localize\npavement cracks. To facilitate accurate training and validation for domain\nadaptation, we use two challenging pavement crack datasets CQU-BPDD and\nRDD2020. Furthermore, we construct a new large-scale Bituminous Pavement\nMulti-label Disease Dataset named CQU-BPMDD, which contains 38994\nhigh-resolution pavement disease images to further evaluate the robustness of\nour model. Extensive experiments demonstrate that DDACDN outperforms\nstate-of-the-art pavement crack detection methods in predicting the crack\nlocation on the target domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huijun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chunhua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yongxin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Z/0/1/0/all/0/1\">Zhimin Ruan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Well Do Sparse Imagenet Models Transfer?. (arXiv:2111.13445v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13445","description":"<p>Transfer learning is a classic paradigm by which models pretrained on large\n\"upstream\" datasets are adapted to yield good results on \"downstream\"\nspecialized datasets. Generally, more accurate models on the \"upstream\" dataset\ntend to provide better transfer accuracy \"downstream\". In this work, we perform\nan in-depth investigation of this phenomenon in the context of convolutional\nneural networks (CNNs) trained on the ImageNet dataset, which have been pruned\n- that is, compressed by sparsifying their connections. We consider transfer\nusing unstructured pruned models obtained by applying several state-of-the-art\npruning methods, including magnitude-based, second-order, re-growth,\nlottery-ticket, and regularization approaches, in the context of twelve\nstandard transfer tasks. In a nutshell, our study shows that sparse models can\nmatch or even outperform the transfer performance of dense models, even at high\nsparsities, and, while doing so, can lead to significant inference and even\ntraining speedups. At the same time, we observe and analyze significant\ndifferences in the behaviour of different pruning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iofinova_E/0/1/0/all/0/1\">Eugenia Iofinova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peste_A/0/1/0/all/0/1\">Alexandra Peste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtz_M/0/1/0/all/0/1\">Mark Kurtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1\">Dan Alistarh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer. (arXiv:2111.13824v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13824","description":"<p>Network quantization significantly reduces model inference complexity and has\nbeen widely used in real-world deployments. However, most existing quantization\nmethods have been developed mainly on Convolutional Neural Networks (CNN), and\nsuffer severe degradation when applied to fully quantized vision transformers.\nIn this work, we demonstrate that many of these difficulties arise because of\nserious inter-channel variation in LayerNorm inputs, and present, Power-of-Two\nFactor (PTF), a systematic method to reduce the performance degradation and\ninference complexity of fully quantized vision transformers. In addition,\nobserving an extreme non-uniform distribution in attention maps, we propose\nLog-Int-Softmax (LIS) to sustain that and simplify inference by using 4-bit\nquantization and the BitShift operator. Comprehensive experiments on various\ntransformer-based architectures and benchmarks show that our Fully Quantized\nVision Transformer (FQ-ViT) outperforms previous works while even using lower\nbit-width on attention maps. For instance, we reach 84.89% top-1 accuracy with\nViT-L on ImageNet and 50.8 mAP with Cascade Mask R-CNN (Swin-S) on COCO. To our\nknowledge, we are the first to achieve lossless accuracy degradation (~1%) on\nfully quantized vision transformers. Code is available at\nhttps://github.com/linyang-zhh/FQ-ViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peiqin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuchang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Cluster Contrastive learning for Object Re-Identification. (arXiv:2112.04662v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04662","description":"<p>Recently, cluster contrastive learning has been proven effective for object\nReID by computing the contrastive loss between the individual features and the\ncluster memory. However, existing methods that use the individual features to\nmomentum update the cluster memory will fluctuate over the training examples,\nespecially for the outlier samples. Unlike the individual-based updating\nmechanism, the centroid-based updating mechanism that applies the mean feature\nof each cluster to update the cluster memory can reduce the impact of\nindividual samples. Therefore, we formulate the individual-based updating and\ncentroid-based updating mechanisms in a unified cluster contrastive framework,\nnamed Dual Cluster Contrastive framework (DCC), which maintains two types of\nmemory banks: individual and centroid cluster memory banks. Significantly, the\nindividual cluster memory considers just one individual at a time to take a\nsingle step for updating. The centroid cluster memory applies the mean feature\nof each cluster to update the corresponding cluster memory. During\noptimization, besides the vallina contrastive loss of each memory, a cross-view\nconsistency constraint is applied to exchange the benefits of two memories for\ngenerating a discriminative description for the object ReID. Note that DCC can\nbe easily applied for unsupervised or supervised object ReID by using\nground-truth labels or the generated pseudo-labels. Extensive experiments on\nthree benchmarks, \\emph{e.g.,} Market-1501, MSMT17, and VeRi-776, under\n\\textbf{supervised Object ReID} and \\textbf{unsupervised Object ReID}\ndemonstrate the superiority of the proposed DCC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Hantao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MantissaCam: Learning Snapshot High-dynamic-range Imaging with Perceptually-based In-pixel Irradiance Encoding. (arXiv:2112.05221v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.05221","description":"<p>The ability to image high-dynamic-range (HDR) scenes is crucial in many\ncomputer vision applications. The dynamic range of conventional sensors,\nhowever, is fundamentally limited by their well capacity, resulting in\nsaturation of bright scene parts. To overcome this limitation, emerging sensors\noffer in-pixel processing capabilities to encode the incident irradiance. Among\nthe most promising encoding schemes is modulo wrapping, which results in a\ncomputational photography problem where the HDR scene is computed by an\nirradiance unwrapping algorithm from the wrapped low-dynamic-range (LDR) sensor\nimage. Here, we design a neural network--based algorithm that outperforms\nprevious irradiance unwrapping methods and we design a perceptually inspired\n\"mantissa\" encoding scheme that more efficiently wraps an HDR scene into an LDR\nsensor. Combined with our reconstruction framework, MantissaCam achieves\nstate-of-the-art results among modulo-type snapshot HDR imaging approaches. We\ndemonstrate the efficacy of our method in simulation and show benefits of our\nalgorithm on modulo images captured with a prototype implemented with a\nprogrammable sensor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+So_H/0/1/0/all/0/1\">Haley M. So</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martel_J/0/1/0/all/0/1\">Julien N.P. Martel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dudek_P/0/1/0/all/0/1\">Piotr Dudek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-vehicle Cooperative Visual Perception for Autonomous Driving under Complex Road and Traffic Scenarios. (arXiv:2112.09298v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09298","description":"<p>Human-vehicle cooperative driving has become the critical technology of\nautonomous driving, which reduces the workload of human drivers. However, the\ncomplex and uncertain road environments bring great challenges to the visual\nperception of cooperative systems. And the perception characteristics of\nautonomous driving differ from manual driving a lot. To enhance the visual\nperception capability of human-vehicle cooperative driving, this paper proposed\na cooperative visual perception model. 506 images of complex road and traffic\nscenarios were collected as the data source. Then this paper improved the\nobject detection algorithm of autonomous vehicles. The mean perception accuracy\nof traffic elements reached 75.52%. By the image fusion method, the gaze points\nof human drivers were fused with vehicles' monitoring screens. Results revealed\nthat cooperative visual perception could reflect the riskiest zone and predict\nthe trajectory of conflict objects more precisely. The findings can be applied\nin improving the visual perception algorithms and providing accurate data for\nplanning and control.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiyue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1\">Cailin Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuchuan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qijun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NICE-SLAM: Neural Implicit Scalable Encoding for SLAM. (arXiv:2112.12130v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12130","description":"<p>Neural implicit representations have recently shown encouraging results in\nvarious domains, including promising progress in simultaneous localization and\nmapping (SLAM). Nevertheless, existing methods produce over-smoothed scene\nreconstructions and have difficulty scaling up to large scenes. These\nlimitations are mainly due to their simple fully-connected network architecture\nthat does not incorporate local information in the observations. In this paper,\nwe present NICE-SLAM, a dense SLAM system that incorporates multi-level local\ninformation by introducing a hierarchical scene representation. Optimizing this\nrepresentation with pre-trained geometric priors enables detailed\nreconstruction on large indoor scenes. Compared to recent neural implicit SLAM\nsystems, our approach is more scalable, efficient, and robust. Experiments on\nfive challenging datasets demonstrate competitive results of NICE-SLAM in both\nmapping and tracking quality. Project page:\nhttps://pengsongyou.github.io/nice-slam\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zihan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Songyou Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larsson_V/0/1/0/all/0/1\">Viktor Larsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhaopeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1\">Martin R. Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiased Learning from Naturally Imbalanced Pseudo-Labels. (arXiv:2201.01490v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.01490","description":"<p>Pseudo-labels are confident predictions made on unlabeled target data by a\nclassifier trained on labeled source data. They are widely used for adapting a\nmodel to unlabeled data, e.g., in a semi-supervised learning setting.\n</p>\n<p>Our key insight is that pseudo-labels are naturally imbalanced due to\nintrinsic data similarity, even when a model is trained on balanced source data\nand evaluated on balanced target data. If we address this previously unknown\nimbalanced classification problem arising from pseudo-labels instead of\nground-truth training labels, we could remove model biases towards false\nmajorities created by pseudo-labels.\n</p>\n<p>We propose a novel and effective debiased learning method with pseudo-labels,\nbased on counterfactual reasoning and adaptive margins: The former removes the\nclassifier response bias, whereas the latter adjusts the margin of each class\naccording to the imbalance of pseudo-labels. Validated by extensive\nexperimentation, our simple debiased learning delivers significant accuracy\ngains over the state-of-the-art on ImageNet-1K: 26% for semi-supervised\nlearning with 0.2% annotations and 9% for zero-shot learning. Our code is\navailable at: https://github.com/frank-xwang/debiased-pseudo-labeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_L/0/1/0/all/0/1\">Long Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tackling the Class Imbalance Problem of Deep Learning Based Head and Neck Organ Segmentation. (arXiv:2201.01636v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01636","description":"<p>The segmentation of organs at risk (OAR) is a required precondition for the\ncancer treatment with image guided radiation therapy. The automation of the\nsegmentation task is therefore of high clinical relevance. Deep Learning (DL)\nbased medical image segmentation is currently the most successful approach, but\nsuffers from the over-presence of the background class and the anatomically\ngiven organ size difference, which is most severe in the head and neck (HAN)\narea. To tackle the HAN area specific class imbalance problem we first optimize\nthe patch-size of the currently best performing general purpose segmentation\nframework, the nnU-Net, based on the introduced class imbalance measurement,\nand second, introduce the class adaptive Dice loss to further compensate for\nthe highly imbalanced setting. Both the patch-size and the loss function are\nparameters with direct influence on the class imbalance and their optimization\nleads to a 3\\% increase of the Dice score and 22% reduction of the 95%\nHausdorff distance compared to the baseline, finally reaching $0.8\\pm0.15$ and\n$3.17\\pm1.7$ mm for the segmentation of seven HAN organs using a single and\nsimple neural network. The patch-size optimization and the class adaptive Dice\nloss are both simply integrable in current DL based segmentation approaches and\nallow to increase the performance for class imbalanced segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tappeiner_E/0/1/0/all/0/1\">Elias Tappeiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welk_M/0/1/0/all/0/1\">Martin Welk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_R/0/1/0/all/0/1\">Rainer Schubert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SRVIO: Super Robust Visual Inertial Odometry for dynamic environments and challenging Loop-closure conditions. (arXiv:2201.05386v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.05386","description":"<p>There has been extensive research on visual localization and odometry for\nautonomous robots and virtual reality during the past decades. Traditionally,\nthis problem has been solved with the help of expensive sensors, such as\nlidars. Nowadays, the focus of the leading research in this field is on robust\nlocalization using more economic sensors, such as cameras and IMUs.\nConsequently, geometric visual localization methods have become more accurate\nin time. However, these methods still suffer from significant loss and\ndivergence in challenging environments, such as a room full of moving people.\nScientists started using deep neural networks (DNNs) to mitigate this problem.\nThe main idea behind using DNNs is to better understand challenging aspects of\nthe data and overcome complex conditions such as the movement of a dynamic\nobject in front of the camera that covers the full view of the camera, extreme\nlighting conditions, and high speed of the camera. Prior end-to-end DNN methods\nhave overcome some of these challenges. However, no general and robust\nframework is available to overcome all challenges together. In this paper, we\nhave combined geometric and DNN-based methods to have the generality and speed\nof geometric SLAM frameworks and overcome most of these challenging conditions\nwith the help of DNNs and deliver the most robust framework so far. To do so,\nwe have designed a framework based on Vins-Mono, and show that it is able to\nachieve state-of-the-art results on TUM-Dynamic, TUM-VI, ADVIO, and EuRoC\ndatasets compared to geometric and end-to-end DNN based SLAMs. Our proposed\nframework could also achieve outstanding results on extreme simulated cases\nresembling the aforementioned challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samadzadeh_A/0/1/0/all/0/1\">Ali Samadzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickabadi_A/0/1/0/all/0/1\">Ahmad Nickabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoAlign: Pixel-Instance Feature Aggregation for Multi-Modal 3D Object Detection. (arXiv:2201.06493v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06493","description":"<p>Object detection through either RGB images or the LiDAR point clouds has been\nextensively explored in autonomous driving. However, it remains challenging to\nmake these two data sources complementary and beneficial to each other. In this\npaper, we propose \\textit{AutoAlign}, an automatic feature fusion strategy for\n3D object detection. Instead of establishing deterministic correspondence with\ncamera projection matrix, we model the mapping relationship between the image\nand point clouds with a learnable alignment map. This map enables our model to\nautomate the alignment of non-homogenous features in a dynamic and data-driven\nmanner. Specifically, a cross-attention feature alignment module is devised to\nadaptively aggregate \\textit{pixel-level} image features for each voxel. To\nenhance the semantic consistency during feature alignment, we also design a\nself-supervised cross-modal feature interaction module, through which the model\ncan learn feature aggregation with \\textit{instance-level} feature guidance.\nExtensive experimental results show that our approach can lead to 2.3 mAP and\n7.0 mAP improvements on the KITTI and nuScenes datasets, respectively. Notably,\nour best model reaches 70.9 NDS on the nuScenes testing leaderboard, achieving\ncompetitive performance among various state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zehui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Liangji Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qinghong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Feng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resistance Training using Prior Bias: toward Unbiased Scene Graph Generation. (arXiv:2201.06794v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06794","description":"<p>Scene Graph Generation (SGG) aims to build a structured representation of a\nscene using objects and pairwise relationships, which benefits downstream\ntasks. However, current SGG methods usually suffer from sub-optimal scene graph\ngeneration because of the long-tailed distribution of training data. To address\nthis problem, we propose Resistance Training using Prior Bias (RTPB) for the\nscene graph generation. Specifically, RTPB uses a distributed-based prior bias\nto improve models' detecting ability on less frequent relationships during\ntraining, thus improving the model generalizability on tail categories. In\naddition, to further explore the contextual information of objects and\nrelationships, we design a contextual encoding backbone network, termed as Dual\nTransformer (DTrans). We perform extensive experiments on a very popular\nbenchmark, VG150, to demonstrate the effectiveness of our method for the\nunbiased scene graph generation. In specific, our RTPB achieves an improvement\nof over 10% under the mean recall when applied to current SGG methods.\nFurthermore, DTrans with RTPB outperforms nearly all state-of-the-art methods\nwith a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Video Representation Learning with Cascade Positive Retrieval. (arXiv:2201.07989v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.07989","description":"<p>Self-supervised video representation learning has been shown to effectively\nimprove downstream tasks such as video retrieval and action recognition. In\nthis paper, we present the Cascade Positive Retrieval (CPR) that successively\nmines positive examples w.r.t. the query for contrastive learning in a cascade\nof stages. Specifically, CPR exploits multiple views of a query example in\ndifferent modalities, where an alternative view may help find another positive\nexample dissimilar in the query view. We explore the effects of possible CPR\nconfigurations in ablations including the number of mining stages, the top\nsimilar example selection ratio in each stage, and progressive training with an\nincremental number of the final Top-k selection. The overall mining quality is\nmeasured to reflect the recall across training set classes. CPR reaches a\nmedian class mining recall of 83.3%, outperforming previous work by 5.5%.\nImplementation-wise, CPR is complementary to pretext tasks and can be easily\napplied to previous work. In the evaluation of pretraining on UCF101, CPR\nconsistently improves existing work and even achieves state-of-the-art R@1 of\n56.7% and 24.4% in video retrieval as well as 83.8% and 54.8% in action\nrecognition on UCF101 and HMDB51. The code is available at\nhttps://github.com/necla-ml/CPR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Cheng-En Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1\">Farley Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yu Hen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadav_A/0/1/0/all/0/1\">Asim Kadav</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sketch2PQ: Freeform Planar Quadrilateral Mesh Design via a Single Sketch. (arXiv:2201.09367v3 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2201.09367","description":"<p>The freeform architectural modeling process often involves two important\nstages: concept design and digital modeling. In the first stage, architects\nusually sketch the overall 3D shape and the panel layout on a physical or\ndigital paper briefly. In the second stage, a digital 3D model is created using\nthe sketch as a reference. The digital model needs to incorporate geometric\nrequirements for its components, such as the planarity of panels due to\nconsideration of construction costs, which can make the modeling process more\nchallenging. In this work, we present a novel sketch-based system to bridge the\nconcept design and digital modeling of freeform roof-like shapes represented as\nplanar quadrilateral (PQ) meshes. Our system allows the user to sketch the\nsurface boundary and contour lines under axonometric projection and supports\nthe sketching of occluded regions. In addition, the user can sketch feature\nlines to provide directional guidance to the PQ mesh layout. Given the 2D\nsketch input, we propose a deep neural network to infer in real-time the\nunderlying surface shape along with a dense conjugate direction field, both of\nwhich are used to extract the final PQ mesh. To train and validate our network,\nwe generate a large synthetic dataset that mimics architect sketching of\nfreeform quadrilateral patches. The effectiveness and usability of our system\nare demonstrated with quantitative and qualitative evaluation as well as user\nstudies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabi_W/0/1/0/all/0/1\">Wassim Jabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bailin Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Hash Naturally Sorts. (arXiv:2201.13322v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.13322","description":"<p>Learning to hash pictures a list-wise sorting problem. Its testing metrics,\ne.g., mean-average precision, count on a sorted candidate list ordered by\npair-wise code similarity. However, scarcely does one train a deep hashing\nmodel with the sorted results end-to-end because of the non-differentiable\nnature of the sorting operation. This inconsistency in the objectives of\ntraining and test may lead to sub-optimal performance since the training loss\noften fails to reflect the actual retrieval metric. In this paper, we tackle\nthis problem by introducing Naturally-Sorted Hashing (NSH). We sort the Hamming\ndistances of samples' hash codes and accordingly gather their latent\nrepresentations for self-supervised training. Thanks to the recent advances in\ndifferentiable sorting approximations, the hash head receives gradients from\nthe sorter so that the hash encoder can be optimized along with the training\nprocedure. Additionally, we describe a novel Sorted Noise-Contrastive\nEstimation (SortedNCE) loss that selectively picks positive and negative\nsamples for contrastive learning, which allows NSH to mine data semantic\nrelations during training in an unsupervised manner. Our extensive experiments\nshow the proposed NSH model significantly outperforms the existing unsupervised\nhashing methods on three benchmarked datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiaguo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Menghan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Scene BERT: Improving object detection by searching for challenging groups of data. (arXiv:2202.03651v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03651","description":"<p>Modern computer vision applications rely on learning-based perception modules\nparameterized with neural networks for tasks like object detection. These\nmodules frequently have low expected error overall but high error on atypical\ngroups of data due to biases inherent in the training process. In building\nautonomous vehicles (AV), this problem is an especially important challenge\nbecause their perception modules are crucial to the overall system performance.\nAfter identifying failures in AV, a human team will comb through the associated\ndata to group perception failures that share common causes. More data from\nthese groups is then collected and annotated before retraining the model to fix\nthe issue. In other words, error groups are found and addressed in hindsight.\nOur main contribution is a pseudo-automatic method to discover such groups in\nforesight by performing causal interventions on simulated scenes. To keep our\ninterventions on the data manifold, we utilize masked language models. We\nverify that the prioritized groups found via intervention are challenging for\nthe object detector and show that retraining with data collected from these\ngroups helps inordinately compared to adding more IID data. We also plan to\nrelease software to run interventions in simulated scenes, which we hope will\nbenefit the causality community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Resnick_C/0/1/0/all/0/1\">Cinjon Resnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litany_O/0/1/0/all/0/1\">Or Litany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_A/0/1/0/all/0/1\">Amlan Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreis_K/0/1/0/all/0/1\">Karsten Kreis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_J/0/1/0/all/0/1\">James Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Convolutional Networks for Multi-modality Medical Imaging: Methods, Architectures, and Clinical Applications. (arXiv:2202.08916v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.08916","description":"<p>Image-based characterization and disease understanding involve integrative\nanalysis of morphological, spatial, and topological information across\nbiological scales. The development of graph convolutional networks (GCNs) has\ncreated the opportunity to address this information complexity via graph-driven\narchitectures, since GCNs can perform feature aggregation, interaction, and\nreasoning with remarkable flexibility and efficiency. These GCNs capabilities\nhave spawned a new wave of research in medical imaging analysis with the\noverarching goal of improving quantitative disease understanding, monitoring,\nand diagnosis. Yet daunting challenges remain for designing the important\nimage-to-graph transformation for multi-modality medical imaging and gaining\ninsights into model interpretation and enhanced clinical decision support. In\nthis review, we present recent GCNs developments in the context of medical\nimage analysis including imaging data from radiology and histopathology. We\ndiscuss the fast-growing use of graph network architectures in medical image\nanalysis to improve disease diagnosis and patient outcomes in clinical\npractice. To foster cross-disciplinary research, we present GCNs technical\nadvancements, emerging medical applications, identify common challenges in the\nuse of image-based GCNs and their extensions in model interpretation,\nlarge-scale benchmarks that promise to transform the scope of medical image\nstudies and related graph-driven medical research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ding_K/0/1/0/all/0/1\">Kexin Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zichen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qiao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_C/0/1/0/all/0/1\">Corey W. Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitri N. Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Energy-Efficient Parking Analytics System using Deep Reinforcement Learning. (arXiv:2202.08973v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08973","description":"<p>Advances in deep vision techniques and ubiquity of smart cameras will drive\nthe next generation of video analytics. However, video analytics applications\nconsume vast amounts of energy as both deep learning techniques and cameras are\npower-hungry. In this paper, we focus on a parking video analytics platform and\npropose RL-CamSleep, a deep reinforcement learning-based technique, to actuate\nthe cameras to reduce the energy footprint while retaining the system's\nutility. Our key insight is that many video-analytics applications do not\nalways need to be operational, and we can design policies to activate video\nanalytics only when necessary. Moreover, our work is complementary to existing\nwork that focuses on improving hardware and software efficiency. We evaluate\nour approach on a city-scale parking dataset having 76 streets spread across\nthe city. Our analysis demonstrates how streets have various parking patterns,\nhighlighting the importance of an adaptive policy. Our approach can learn such\nan adaptive policy that can reduce the average energy consumption by 76.38% and\nachieve an average accuracy of more than 98% in performing video analytics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_Y/0/1/0/all/0/1\">Yoones Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Stephen Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosse_D/0/1/0/all/0/1\">Daniel Mosse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OUR-GAN: One-shot Ultra-high-Resolution Generative Adversarial Networks. (arXiv:2202.13799v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13799","description":"<p>We propose OUR-GAN, the first one-shot ultra-high-resolution (UHR) image\nsynthesis framework that generates non-repetitive images with 4K or higher\nresolution from a single training image. OUR-GAN generates a visually coherent\nimage at low resolution and then gradually increases the resolution by\nsuper-resolution. Since OUR-GAN learns from a real UHR image, it can synthesize\nlarge-scale shapes with fine details while maintaining long-range coherence,\nwhich is difficult with conventional generative models that generate large\nimages based on the patch distribution learned from relatively small images.\nOUR-GAN applies seamless subregion-wise super-resolution that synthesizes 4k or\nhigher UHR images with limited memory, preventing discontinuity at the\nboundary. Additionally, OUR-GAN improves visual coherence maintaining diversity\nby adding vertical positional embeddings to the feature maps. In experiments on\nthe ST4K and RAISE datasets, OUR-GAN exhibited improved fidelity, visual\ncoherency, and diversity compared with existing methods. The synthesized images\nare presented at https://anonymous-62348.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_D/0/1/0/all/0/1\">Donghwee Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Junseok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hayeong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_M/0/1/0/all/0/1\">Minjae Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1\">Injung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Update Compression for Deep Neural Networks on the Edge. (arXiv:2203.04516v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04516","description":"<p>An increasing number of artificial intelligence (AI) applications involve the\nexecution of deep neural networks (DNNs) on edge devices. Many practical\nreasons motivate the need to update the DNN model on the edge device\npost-deployment, such as refining the model, concept drift, or outright change\nin the learning task. In this paper, we consider the scenario where retraining\ncan be done on the server side based on a copy of the DNN model, with only the\nnecessary data transmitted to the edge to update the deployed model. However,\ndue to bandwidth constraints, we want to minimise the transmission required to\nachieve the update. We develop a simple approach based on matrix factorisation\nto compress the model update -- this differs from compressing the model itself.\nThe key idea is to preserve existing knowledge in the current model and\noptimise only small additional parameters for the update which can be used to\nreconstitute the model on the edge. We compared our method to similar\ntechniques used in federated learning; our method usually requires less than\nhalf of the update size of existing methods to achieve the same accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakhshi_A/0/1/0/all/0/1\">Ali Bakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batista_G/0/1/0/all/0/1\">Gustavo Batista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_B/0/1/0/all/0/1\">Brian Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Tat-Jun Chin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Bracket High Dynamic Range Imaging with Event Cameras. (arXiv:2203.06622v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.06622","description":"<p>Modern high dynamic range (HDR) imaging pipelines align and fuse multiple low\ndynamic range (LDR) images captured at different exposure times. While these\nmethods work well in static scenes, dynamic scenes remain a challenge since the\nLDR images still suffer from saturation and noise. In such scenarios, event\ncameras would be a valid complement, thanks to their higher temporal resolution\nand dynamic range. In this paper, we propose the first multi-bracket HDR\npipeline combining a standard camera with an event camera. Our results show\nbetter overall robustness when using events, with improvements in PSNR by up to\n5dB on synthetic data and up to 0.7dB on real-world data. We also introduce a\nnew dataset containing bracketed LDR images with aligned events and HDR ground\ntruth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Messikommer_N/0/1/0/all/0/1\">Nico Messikommer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gehrig_D/0/1/0/all/0/1\">Daniel Gehrig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tulyakov_S/0/1/0/all/0/1\">Stepan Tulyakov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Erbach_J/0/1/0/all/0/1\">Julius Erbach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bochicchio_A/0/1/0/all/0/1\">Alfredo Bochicchio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuanyou Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sat-NeRF: Learning Multi-View Satellite Photogrammetry With Transient Objects and Shadow Modeling Using RPC Cameras. (arXiv:2203.08896v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08896","description":"<p>We introduce the Satellite Neural Radiance Field (Sat-NeRF), a new end-to-end\nmodel for learning multi-view satellite photogrammetry in the wild. Sat-NeRF\ncombines some of the latest trends in neural rendering with native satellite\ncamera models, represented by rational polynomial coefficient (RPC) functions.\nThe proposed method renders new views and infers surface models of similar\nquality to those obtained with traditional state-of-the-art stereo pipelines.\nMulti-date images exhibit significant changes in appearance, mainly due to\nvarying shadows and transient objects (cars, vegetation). Robustness to these\nchallenges is achieved by a shadow-aware irradiance model and uncertainty\nweighting to deal with transient phenomena that cannot be explained by the\nposition of the sun. We evaluate Sat-NeRF using WorldView-3 images from\ndifferent locations and stress the advantages of applying a bundle adjustment\nto the satellite camera models prior to training. This boosts the network\nperformance and can optionally be used to extract additional cues for depth\nsupervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mari_R/0/1/0/all/0/1\">Roger Mar&#xed;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Facciolo_G/0/1/0/all/0/1\">Gabriele Facciolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehret_T/0/1/0/all/0/1\">Thibaud Ehret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Triangulation as a Form of Self-Supervision for 3D Human Pose Estimation. (arXiv:2203.15865v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15865","description":"<p>Supervised approaches to 3D pose estimation from single images are remarkably\neffective when labeled data is abundant. Therefore, much of the recent\nattention has shifted towards semi and (or) weakly supervised learning.\nGenerating an effective form of supervision with little annotations still poses\nmajor challenges in crowded scenes. However, since it is easy to observe a\nscene from multiple cameras, we propose to impose multi-view geometrical\nconstraints by means of a differentiable triangulation and to use it as form of\nself-supervision during training when no labels are available. We therefore\ntrain a 2D pose estimator in such a way that its predictions correspond to the\nre-projection of the triangulated 3D one and train an auxiliary network on them\nto produce the final 3D poses. We complement the triangulation with a weighting\nmechanism that nullify the impact of noisy predictions caused by self-occlusion\nor occlusion from other subjects. Our experimental results on Human3.6M and\nMPI-INF-3DHP substantiate the significance of our weighting strategy where we\nobtain state-of-the-art results in the semi and weakly supervised learning\nsetup. We also contribute a new multi-player sports dataset that features\nocclusion, and show the effectiveness of our algorithm over baseline\ntriangulation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Soumava Kumar Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Citraro_L/0/1/0/all/0/1\">Leonardo Citraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honari_S/0/1/0/all/0/1\">Sina Honari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Linear Attention for Fast and Accurate Keypoint Matching. (arXiv:2204.07731v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07731","description":"<p>Recently Transformers have provided state-of-the-art performance in sparse\nmatching, crucial to realize high-performance 3D vision applications. Yet,\nthese Transformers lack efficiency due to the quadratic computational\ncomplexity of their attention mechanism. To solve this problem, we employ an\nefficient linear attention for the linear computational complexity. Then, we\npropose a new attentional aggregation that achieves high accuracy by\naggregating both the global and local information from sparse keypoints. To\nfurther improve the efficiency, we propose the joint learning of feature\nmatching and description. Our learning enables simpler and faster matching than\nSinkhorn, often used in matching the learned descriptors from Transformers. Our\nmethod achieves competitive performance with only 0.84M learnable parameters\nagainst the bigger SOTAs, SuperGlue (12M parameters) and SGMNet (30M\nparameters), on three benchmarks, HPatch, ETH, and Aachen Day-Night.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suwanwimolkul_S/0/1/0/all/0/1\">Suwichaya Suwanwimolkul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komorita_S/0/1/0/all/0/1\">Satoshi Komorita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Attention Methods in Deep Learning: An In-Depth Survey. (arXiv:2204.07756v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07756","description":"<p>Inspired by the human cognitive system, attention is a mechanism that\nimitates the human cognitive awareness about specific information, amplifying\ncritical details to focus more on the essential aspects of data. Deep learning\nhas employed attention to boost performance for many applications.\nInterestingly, the same attention design can suit processing different data\nmodalities and can easily be incorporated into large networks. Furthermore,\nmultiple complementary attention mechanisms can be incorporated in one network.\nHence, attention techniques have become extremely attractive. However, the\nliterature lacks a comprehensive survey specific to attention techniques to\nguide researchers in employing attention in their deep models. Note that,\nbesides being demanding in terms of training data and computational resources,\ntransformers only cover a single category in self-attention out of the many\ncategories available. We fill this gap and provide an in-depth survey of 50\nattention techniques categorizing them by their most prominent features. We\ninitiate our discussion by introducing the fundamental concepts behind the\nsuccess of attention mechanism. Next, we furnish some essentials such as the\nstrengths and limitations of each attention category, describe their\nfundamental building blocks, basic formulations with primary usage, and\napplications specifically for computer vision. We also discuss the challenges\nand open questions related to attention mechanism in general. Finally, we\nrecommend possible future research directions for deep attention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassanin_M/0/1/0/all/0/1\">Mohammed Hassanin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radwan_I/0/1/0/all/0/1\">Ibrahim Radwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad S Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping LiDAR and Camera Measurements in a Dual Top-View Grid Representation Tailored for Automated Vehicles. (arXiv:2204.07887v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07887","description":"<p>We present a generic evidential grid mapping pipeline designed for imaging\nsensors such as LiDARs and cameras. Our grid-based evidential model contains\nsemantic estimates for cell occupancy and ground separately. We specify the\nestimation steps for input data represented by point sets, but mainly focus on\ninput data represented by images such as disparity maps or LiDAR range images.\nInstead of relying on an external ground segmentation only, we deduce occupancy\nevidence by analyzing the surface orientation around measurements. We conduct\nexperiments and evaluate the presented method using LiDAR and stereo camera\ndata recorded in real traffic scenarios. Our method estimates cell occupancy\nrobustly and with a high level of detail while maximizing efficiency and\nminimizing the dependency to external processing modules.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Richter_S/0/1/0/all/0/1\">Sven Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bieder_F/0/1/0/all/0/1\">Frank Bieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wirges_S/0/1/0/all/0/1\">Sascha Wirges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1\">Christoph Stiller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis. (arXiv:2204.07955v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07955","description":"<p>As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment\nAnalysis (MABSA) has attracted increasing attention in recent years. However,\nprevious approaches either (i) use separately pre-trained visual and textual\nmodels, which ignore the crossmodal alignment or (ii) use vision-language\nmodels pre-trained with general pre-training tasks, which are inadequate to\nidentify finegrained aspects, opinions, and their alignments across modalities.\nTo tackle these limitations, we propose a task-specific Vision-Language\nPre-training framework for MABSA (VLPMABSA), which is a unified multimodal\nencoder-decoder architecture for all the pretraining and downstream tasks. We\nfurther design three types of task-specific pre-training tasks from the\nlanguage, vision, and multimodal modalities, respectively. Experimental results\nshow that our approach generally outperforms the state-of-the-art approaches on\nthree MABSA subtasks. Further analysis demonstrates the effectiveness of each\npretraining task. The source code is publicly released at\nhttps://github.com/NUSTM/VLP-MABSA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_Y/0/1/0/all/0/1\">Yan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianfei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_R/0/1/0/all/0/1\">Rui Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NICO++: Towards Better Benchmarking for Domain Generalization. (arXiv:2204.08040v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08040","description":"<p>Despite the remarkable performance that modern deep neural networks have\nachieved on independent and identically distributed (I.I.D.) data, they can\ncrash under distribution shifts. Most current evaluation methods for domain\ngeneralization (DG) adopt the leave-one-out strategy as a compromise on the\nlimited number of domains. We propose a large-scale benchmark with extensive\nlabeled domains named NICO++ along with more rational evaluation methods for\ncomprehensively evaluating DG algorithms. To evaluate DG datasets, we propose\ntwo metrics to quantify covariate shift and concept shift, respectively. Two\nnovel generalization bounds from the perspective of data construction are\nproposed to prove that limited concept shift and significant covariate shift\nfavor the evaluation capability for generalization. Through extensive\nexperiments, NICO++ shows its superior evaluation capability compared with\ncurrent DG datasets and its contribution in alleviating unfairness caused by\nthe leak of oracle knowledge in model selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yue He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renzhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Han Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zheyan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration. (arXiv:2204.08058v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08058","description":"<p>Multimodal video-audio-text understanding and generation can benefit from\ndatasets that are narrow but rich. The narrowness allows bite-sized challenges\nthat the research community can make progress on. The richness ensures we are\nmaking progress along the core challenges. To this end, we present a\nlarge-scale video-audio-text dataset MUGEN, collected using the open-sourced\nplatform game CoinRun [11]. We made substantial modifications to make the game\nricher by introducing audio and enabling new interactions. We trained RL agents\nwith different objectives to navigate the game and interact with 13 objects and\ncharacters. This allows us to automatically extract a large collection of\ndiverse videos and associated audio. We sample 375K video clips (3.2s each) and\ncollect text descriptions from human annotators. Each video has additional\nannotations that are extracted automatically from the game engine, such as\naccurate semantic maps for each frame and templated textual descriptions.\nAltogether, MUGEN can help progress research in many tasks in multimodal\nunderstanding and generation. We benchmark representative approaches on tasks\ninvolving video-audio-text retrieval and generation. Our dataset and code are\nreleased at: https://mugen-org.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1\">Thomas Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guan Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_S/0/1/0/all/0/1\">Sasha Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Harry Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Songwei Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qiyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Consistency Regularization for Semi-supervised Change Detection in Remote Sensing Images. (arXiv:2204.08454v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08454","description":"<p>Remote-sensing (RS) Change Detection (CD) aims to detect \"changes of\ninterest\" from co-registered bi-temporal images. The performance of existing\ndeep supervised CD methods is attributed to the large amounts of annotated data\nused to train the networks. However, annotating large amounts of remote sensing\nimages is labor-intensive and expensive, particularly with bi-temporal images,\nas it requires pixel-wise comparisons by a human expert. On the other hand, we\noften have access to unlimited unlabeled multi-temporal RS imagery thanks to\never-increasing earth observation programs. In this paper, we propose a simple\nyet effective way to leverage the information from unlabeled bi-temporal images\nto improve the performance of CD approaches. More specifically, we propose a\nsemi-supervised CD model in which we formulate an unsupervised CD loss in\naddition to the supervised Cross-Entropy (CE) loss by constraining the output\nchange probability map of a given unlabeled bi-temporal image pair to be\nconsistent under the small random perturbations applied on the deep feature\ndifference map that is obtained by subtracting their latent feature\nrepresentations. Experiments conducted on two publicly available CD datasets\nshow that the proposed semi-supervised CD method can reach closer to the\nperformance of supervised CD even with access to as little as 10% of the\nannotated training data. Code available at https://github.com/wgcban/SemiCD\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not All Tokens Are Equal: Human-centric Visual Analysis via Token Clustering Transformer. (arXiv:2204.08680v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08680","description":"<p>Vision transformers have achieved great successes in many computer vision\ntasks. Most methods generate vision tokens by splitting an image into a regular\nand fixed grid and treating each cell as a token. However, not all regions are\nequally important in human-centric vision tasks, e.g., the human body needs a\nfine representation with many tokens, while the image background can be modeled\nby a few tokens. To address this problem, we propose a novel Vision\nTransformer, called Token Clustering Transformer (TCFormer), which merges\ntokens by progressive clustering, where the tokens can be merged from different\nlocations with flexible shapes and sizes. The tokens in TCFormer can not only\nfocus on important areas but also adjust the token shapes to fit the semantic\nconcept and adopt a fine resolution for regions containing critical details,\nwhich is beneficial to capturing detailed information. Extensive experiments\nshow that TCFormer consistently outperforms its counterparts on different\nchallenging human-centric tasks and datasets, including whole-body pose\nestimation on COCO-WholeBody and 3D human mesh reconstruction on 3DPW. Code is\navailable at https://github.com/zengwang430521/TCFormer.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Sheng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Missing Annotations for Incremental Learning in Object Detection. (arXiv:2204.08766v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08766","description":"<p>Despite the recent advances in the field of object detection, common\narchitectures are still ill-suited to incrementally detect new categories over\ntime. They are vulnerable to catastrophic forgetting: they forget what has been\nalready learned while updating their parameters in absence of the original\ntraining data. Previous works extended standard classification methods in the\nobject detection task, mainly adopting the knowledge distillation framework.\nHowever, we argue that object detection introduces an additional problem, which\nhas been overlooked. While objects belonging to new classes are learned thanks\nto their annotations, if no supervision is provided for other objects that may\nstill be present in the input, the model learns to associate them to background\nregions. We propose to handle these missing annotations by revisiting the\nstandard knowledge distillation framework. Our approach outperforms current\nstate-of-the-art methods in every setting of the Pascal-VOC dataset. We further\npropose an extension to instance segmentation, outperforming the other\nbaselines. Code can be found here: https://github.com/fcdl94/MMA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cermelli_F/0/1/0/all/0/1\">Fabio Cermelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geraci_A/0/1/0/all/0/1\">Antonino Geraci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fontanel_D/0/1/0/all/0/1\">Dario Fontanel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Domain-Incremental Learning Approach to Drive in All Weather Conditions. (arXiv:2204.08817v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08817","description":"<p>Although deep neural networks enable impressive visual perception performance\nfor autonomous driving, their robustness to varying weather conditions still\nrequires attention. When adapting these models for changed environments, such\nas different weather conditions, they are prone to forgetting previously\nlearned information. This catastrophic forgetting is typically addressed via\nincremental learning approaches which usually re-train the model by either\nkeeping a memory bank of training samples or keeping a copy of the entire model\nor model parameters for each scenario. While these approaches show impressive\nresults, they can be prone to scalability issues and their applicability for\nautonomous driving in all weather conditions has not been shown. In this paper\nwe propose DISC -- Domain Incremental through Statistical Correction -- a\nsimple online zero-forgetting approach which can incrementally learn new tasks\n(i.e weather conditions) without requiring re-training or expensive memory\nbanks. The only information we store for each task are the statistical\nparameters as we categorize each domain by the change in first and second order\nstatistics. Thus, as each task arrives, we simply 'plug and play' the\nstatistical vectors for the corresponding task into the model and it\nimmediately starts to perform well on that task. We show the efficacy of our\napproach by testing it for object detection in a challenging domain-incremental\nautonomous driving scenario where we encounter different adverse weather\nconditions, such as heavy rain, fog, and snow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirza_M/0/1/0/all/0/1\">M. Jehanzeb Mirza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masana_M/0/1/0/all/0/1\">Marc Masana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Possegger_H/0/1/0/all/0/1\">Horst Possegger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischof_H/0/1/0/all/0/1\">Horst Bischof</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MANIQA: Multi-dimension Attention Network for No-Reference Image Quality Assessment. (arXiv:2204.08958v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08958","description":"<p>No-Reference Image Quality Assessment (NR-IQA) aims to assess the perceptual\nquality of images in accordance with human subjective perception.\nUnfortunately, existing NR-IQA methods are far from meeting the needs of\npredicting accurate quality scores on GAN-based distortion images. To this end,\nwe propose Multi-dimension Attention Network for no-reference Image Quality\nAssessment (MANIQA) to improve the performance on GAN-based distortion. We\nfirstly extract features via ViT, then to strengthen global and local\ninteractions, we propose the Transposed Attention Block (TAB) and the Scale\nSwin Transformer Block (SSTB). These two modules apply attention mechanisms\nacross the channel and spatial dimension, respectively. In this\nmulti-dimensional manner, the modules cooperatively increase the interaction\namong different regions of images globally and locally. Finally, a dual branch\nstructure for patch-weighted quality prediction is applied to predict the final\nscore depending on the weight of each patch's score. Experimental results\ndemonstrate that MANIQA outperforms state-of-the-art methods on four standard\ndatasets (LIVE, TID2013, CSIQ, and KADID-10K) by a large margin. Besides, our\nmethod ranked first place in the final testing phase of the NTIRE 2022\nPerceptual Image Quality Assessment Challenge Track 2: No-Reference. Codes and\nmodels are available at https://github.com/IIGROUP/MANIQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sidi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuwei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lao_S/0/1/0/all/0/1\">Shanshan Lao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yuan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Mingdeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rendering Nighttime Image Via Cascaded Color and Brightness Compensation. (arXiv:2204.08970v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08970","description":"<p>Image signal processing (ISP) is crucial for camera imaging, and neural\nnetworks (NN) solutions are extensively deployed for daytime scenes. The lack\nof sufficient nighttime image dataset and insights on nighttime illumination\ncharacteristics poses a great challenge for high-quality rendering using\nexisting NN ISPs. To tackle it, we first built a high-resolution nighttime\nRAW-RGB (NR2R) dataset with white balance and tone mapping annotated by expert\nprofessionals. Meanwhile, to best capture the characteristics of nighttime\nillumination light sources, we develop the CBUnet, a two-stage NN ISP to\ncascade the compensation of color and brightness attributes. Experiments show\nthat our method has better visual quality compared to traditional ISP pipeline,\nand is ranked at the second place in the NTIRE 2022 Night Photography Rendering\nChallenge for two tracks by respective People's and Professional Photographer's\nchoices. The code and relevant materials are avaiable on our website:\nhttps://njuvision.github.io/CBUnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Si Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhan Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstruction-Aware Prior Distillation for Semi-supervised Point Cloud Completion. (arXiv:2204.09186v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09186","description":"<p>Point clouds scanned by real-world sensors are always incomplete, irregular,\nand noisy, making the point cloud completion task become increasingly more\nimportant. Though many point cloud completion methods have been proposed, most\nof them require a large number of paired complete-incomplete point clouds for\ntraining, which is labor exhausted. In contrast, this paper proposes a novel\nReconstruction-Aware Prior Distillation semi-supervised point cloud completion\nmethod named RaPD, which takes advantage of a two-stage training scheme to\nreduce the dependence on a large-scale paired dataset. In training stage 1, the\nso-called deep semantic prior is learned from both unpaired complete and\nunpaired incomplete point clouds using a reconstruction-aware pretraining\nprocess. While in training stage 2, we introduce a semi-supervised prior\ndistillation process, where an encoder-decoder-based completion network is\ntrained by distilling the prior into the network utilizing only a small number\nof paired training samples. A self-supervised completion module is further\nintroduced, excavating the value of a large number of unpaired incomplete point\nclouds, leading to an increase in the network's performance. Extensive\nexperiments on several widely used datasets demonstrate that RaPD, the first\nsemi-supervised point cloud completion method, achieves superior performance to\nprevious methods on both homologous and heterologous scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhaoxin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kejian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sound-Guided Semantic Video Generation. (arXiv:2204.09273v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09273","description":"<p>The recent success in StyleGAN demonstrates that pre-trained StyleGAN latent\nspace is useful for realistic video generation. However, the generated motion\nin the video is usually not semantically meaningful due to the difficulty of\ndetermining the direction and magnitude in the StyleGAN latent space. In this\npaper, we propose a framework to generate realistic videos by leveraging\nmultimodal (sound-image-text) embedding space. As sound provides the temporal\ncontexts of the scene, our framework learns to generate a video that is\nsemantically consistent with sound. First, our sound inversion module maps the\naudio directly into the StyleGAN latent space. We then incorporate the\nCLIP-based multimodal embedding space to further provide the audio-visual\nrelationships. Finally, the proposed frame generator learns to find the\ntrajectory in the latent space which is coherent with the corresponding sound\nand generates a video in a hierarchical manner. We provide the new\nhigh-resolution landscape video dataset (audio-visual pair) for the\nsound-guided video generation task. The experiments show that our model\noutperforms the state-of-the-art methods in terms of video quality. We further\nshow several applications including image and video editing to verify the\neffectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seung Hyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_G/0/1/0/all/0/1\">Gyeongrok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byeon_W/0/1/0/all/0/1\">Wonmin Byeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1\">Jihyun Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chanyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_W/0/1/0/all/0/1\">Won Jeong Ryoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sang Ho Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinkyu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangpil Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Situational Perception Guided Image Matting. (arXiv:2204.09276v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09276","description":"<p>Most automatic matting methods try to separate the salient foreground from\nthe background. However, the insufficient quantity and subjective bias of the\ncurrent existing matting datasets make it difficult to fully explore the\nsemantic association between object-to-object and object-to-environment in a\ngiven image. In this paper, we propose a Situational Perception Guided Image\nMatting (SPG-IM) method that mitigates subjective bias of matting annotations\nand captures sufficient situational perception information for better global\nsaliency distilled from the visual-to-textual task. SPG-IM can better associate\ninter-objects and object-to-environment saliency, and compensate the subjective\nnature of image matting and its expensive annotation. We also introduce a\ntextual Semantic Transformation (TST) module that can effectively transform and\nintegrate the semantic feature stream to guide the visual representations. In\naddition, an Adaptive Focal Transformation (AFT) Refinement Network is proposed\nto adaptively switch multi-scale receptive fields and focal points to enhance\nboth global and local details. Extensive experiments demonstrate the\neffectiveness of situational perception guidance from the visual-to-textual\ntasks on image matting, and our model outperforms the state-of-the-art methods.\nWe also analyze the significance of different components in our model. The code\nwill be released soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiake Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequential Point Clouds: A Survey. (arXiv:2204.09337v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09337","description":"<p>Point cloud has drawn more and more research attention as well as real-world\napplications. However, many of these applications (e.g. autonomous driving and\nrobotic manipulation) are actually based on sequential point clouds (i.e. four\ndimensions) because the information of the static point cloud data could\nprovide is still limited. Recently, researchers put more and more effort into\nsequential point clouds. This paper presents an extensive review of the deep\nlearning-based methods for sequential point cloud research including dynamic\nflow estimation, object detection \\&amp; tracking, point cloud segmentation, and\npoint cloud forecasting. This paper further summarizes and compares the\nquantitative results of the reviewed methods over the public benchmark\ndatasets. Finally, this paper is concluded by discussing the challenges in the\ncurrent sequential point cloud research and pointing out insightful potential\nfuture research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yingli Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ESS: Learning Event-based Semantic Segmentation from Still Images. (arXiv:2203.10016v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2203.10016","description":"<p>Retrieving accurate semantic information in challenging high dynamic range\n(HDR) and high-speed conditions remains an open challenge for image-based\nalgorithms due to severe image degradations. Event cameras promise to address\nthese challenges since they feature a much higher dynamic range and are\nresilient to motion blur. Nonetheless, semantic segmentation with event cameras\nis still in its infancy which is chiefly due to the novelty of the sensor, and\nthe lack of high-quality, labeled datasets. In this work, we introduce ESS,\nwhich tackles this problem by directly transferring the semantic segmentation\ntask from existing labeled image datasets to unlabeled events via unsupervised\ndomain adaptation (UDA). Compared to existing UDA methods, our approach aligns\nrecurrent, motion-invariant event embeddings with image embeddings. For this\nreason, our method neither requires video data nor per-pixel alignment between\nimages and events and, crucially, does not need to hallucinate motion from\nstill images. Additionally, to spur further research in event-based semantic\nsegmentation, we introduce DSEC-Semantic, the first large-scale event-based\ndataset with fine-grained labels. We show that using image labels alone, ESS\noutperforms existing UDA approaches, and when combined with event labels, it\neven outperforms state-of-the-art supervised approaches on both DDD17 and\nDSEC-Semantic. Finally, ESS is general-purpose, which unlocks the vast amount\nof existing labeled image datasets and paves the way for new and exciting\nresearch directions in new fields previously inaccessible for event cameras.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhaoning Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messikommer_N/0/1/0/all/0/1\">Nico Messikommer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrig_D/0/1/0/all/0/1\">Daniel Gehrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}