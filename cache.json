{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-03T01:30:00Z","channels":[{"title":"cs.AI updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.AI","description":"Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"The Role of Explainability in Assuring Safety of Machine Learning in Healthcare. (arXiv:2109.00520v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00520","description":"<p>Established approaches to assuring safety-critical systems and software are\ndifficult to apply to systems employing machine learning (ML). In many cases,\nML is used on ill-defined problems, e.g. optimising sepsis treatment, where\nthere is no clear, pre-defined specification against which to assess validity.\nThis problem is exacerbated by the \"opaque\" nature of ML where the learnt model\nis not amenable to human scrutiny. Explainable AI methods have been proposed to\ntackle this issue by producing human-interpretable representations of ML models\nwhich can help users to gain confidence and build trust in the ML system.\nHowever, there is not much work explicitly investigating the role of\nexplainability for safety assurance in the context of ML development. This\npaper identifies ways in which explainable AI methods can contribute to safety\nassurance of ML-based systems. It then uses a concrete ML-based clinical\ndecision support system, concerning weaning of patients from mechanical\nventilation, to demonstrate how explainable AI methods can be employed to\nproduce evidence to support safety assurance. The results are also represented\nin a safety argument to show where, and in what way, explainable AI methods can\ncontribute to a safety case. Overall, we conclude that explainable AI methods\nhave a valuable role in safety assurance of ML-based systems in healthcare but\nthat they are not sufficient in themselves to assure safety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDermid_J/0/1/0/all/0/1\">John McDermid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawton_T/0/1/0/all/0/1\">Tom Lawton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habli_I/0/1/0/all/0/1\">Ibrahim Habli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Catastrophic Interference in Reinforcement Learning: A Solution Based on Context Division and Knowledge Distillation. (arXiv:2109.00525v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00525","description":"<p>The powerful learning ability of deep neural networks enables reinforcement\nlearning (RL) agents to learn competent control policies directly from\nhigh-dimensional and continuous environments. In theory, to achieve stable\nperformance, neural networks assume i.i.d. inputs, which unfortunately does no\nhold in the general RL paradigm where the training data is temporally\ncorrelated and non-stationary. This issue may lead to the phenomenon of\n\"catastrophic interference\" and the collapse in performance as later training\nis likely to overwrite and interfer with previously learned policies. In this\npaper, we introduce the concept of \"context\" into single-task RL and develop a\nnovel scheme, termed as Context Division and Knowledge Distillation (CDaKD)\ndriven RL, to divide all states experienced during training into a series of\ncontexts. Its motivation is to mitigate the challenge of aforementioned\ncatastrophic interference in deep RL, thereby improving the stability and\nplasticity of RL models. At the heart of CDaKD is a value function,\nparameterized by a neural network feature extractor shared across all contexts,\nand a set of output heads, each specializing on an individual context. In\nCDaKD, we exploit online clustering to achieve context division, and\ninterference is further alleviated by a knowledge distillation regularization\nterm on the output layers for learned contexts. In addition, to effectively\nobtain the context division in high-dimensional state spaces (e.g., image\ninputs), we perform clustering in the lower-dimensional representation space of\na randomly initialized convolutional encoder, which is fixed throughout\ntraining. Our results show that, with various replay memory capacities, CDaKD\ncan consistently improve the performance of existing RL algorithms on classic\nOpenAI Gym tasks and the more complex high-dimensional Atari tasks, incurring\nonly moderate computational overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tiantian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xueqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1\">Bin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bo Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Search Engines with Interactive Agents. (arXiv:2109.00527v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00527","description":"<p>Can machines learn to use a search engine as an interactive tool for finding\ninformation? That would have far reaching consequences for making the world's\nknowledge more accessible. This paper presents first steps in designing agents\nthat learn meta-strategies for contextual query refinements. Our approach uses\nmachine reading to guide the selection of refinement terms from aggregated\nsearch results. Agents are then empowered with simple but effective search\noperators to exert fine-grained and transparent control over queries and search\nresults. We develop a novel way of generating synthetic search sessions, which\nleverages the power of transformer-based generative language models through\n(self-)supervised learning. We also present a reinforcement learning agent with\ndynamically constrained actions that can learn interactive search strategies\ncompletely from scratch. In both cases, we obtain significant improvements over\none-shot search with a strong information retrieval baseline. Finally, we\nprovide an in-depth analysis of the learned search policies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adolphs_L/0/1/0/all/0/1\">Leonard Adolphs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boerschinger_B/0/1/0/all/0/1\">Benjamin Boerschinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buck_C/0/1/0/all/0/1\">Christian Buck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huebscher_M/0/1/0/all/0/1\">Michelle Chen Huebscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciaramita_M/0/1/0/all/0/1\">Massimiliano Ciaramita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espeholt_L/0/1/0/all/0/1\">Lasse Espeholt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1\">Thomas Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilcher_Y/0/1/0/all/0/1\">Yannic Kilcher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Quantum Reinforcement Learning via Evolutionary Optimization. (arXiv:2109.00540v1 [quant-ph])","link":"http://arxiv.org/abs/2109.00540","description":"<p>Recent advance in classical reinforcement learning (RL) and quantum\ncomputation (QC) points to a promising direction of performing RL on a quantum\ncomputer. However, potential applications in quantum RL are limited by the\nnumber of qubits available in the modern quantum devices. Here we present two\nframeworks of deep quantum RL tasks using a gradient-free evolution\noptimization: First, we apply the amplitude encoding scheme to the Cart-Pole\nproblem; Second, we propose a hybrid framework where the quantum RL agents are\nequipped with hybrid tensor network-variational quantum circuit (TN-VQC)\narchitecture to handle inputs with dimensions exceeding the number of qubits.\nThis allows us to perform quantum RL on the MiniGrid environment with\n147-dimensional inputs. We demonstrate the quantum advantage of parameter\nsaving using the amplitude encoding. The hybrid TN-VQC architecture provides a\nnatural way to perform efficient compression of the input dimension, enabling\nfurther quantum RL applications on noisy intermediate-scale quantum devices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Chen_S/0/1/0/all/0/1\">Samuel Yen-Chi Chen</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Huang_C/0/1/0/all/0/1\">Chih-Min Huang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Hsing_C/0/1/0/all/0/1\">Chia-Wei Hsing</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Goan_H/0/1/0/all/0/1\">Hsi-Sheng Goan</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kao_Y/0/1/0/all/0/1\">Ying-Jer Kao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Improving Adversarial Training of NLP Models. (arXiv:2109.00544v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00544","description":"<p>Adversarial training, a method for learning robust deep neural networks,\nconstructs adversarial examples during training. However, recent methods for\ngenerating NLP adversarial examples involve combinatorial search and expensive\nsentence encoders for constraining the generated instances. As a result, it\nremains challenging to use vanilla adversarial training to improve NLP models'\nperformance, and the benefits are mainly uninvestigated. This paper proposes a\nsimple and improved vanilla adversarial training process for NLP, which we name\nAttacking to Training ($\\texttt{A2T}$). The core part of $\\texttt{A2T}$ is a\nnew and cheaper word substitution attack optimized for vanilla adversarial\ntraining. We use $\\texttt{A2T}$ to train BERT and RoBERTa models on IMDB,\nRotten Tomatoes, Yelp, and SNLI datasets. Our results show that it is possible\nto train empirically robust NLP models using a much cheaper adversary. We\ndemonstrate that vanilla adversarial training with $\\texttt{A2T}$ can improve\nan NLP model's robustness to the attack it was originally trained with and also\ndefend the model against other types of attacks. Furthermore, we show that\n$\\texttt{A2T}$ can improve NLP models' standard accuracy, cross-domain\ngeneralization, and interpretability. Code is available at\n<a href=\"http://github.com/jinyongyoo/A2T\">this http URL</a> .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jin Yong Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaVoA: Face-Voice Association Favours Ambiguous Speaker Detection. (arXiv:2109.00577v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00577","description":"<p>The strong relation between face and voice can aid active speaker detection\nsystems when faces are visible, even in difficult settings, when the face of a\nspeaker is not clear or when there are several people in the same scene. By\nbeing capable of estimating the frontal facial representation of a person from\nhis/her speech, it becomes easier to determine whether he/she is a potential\ncandidate for being classified as an active speaker, even in challenging cases\nin which no mouth movement is detected from any person in that same scene. By\nincorporating a face-voice association neural network into an existing\nstate-of-the-art active speaker detection model, we introduce FaVoA (Face-Voice\nAssociation Ambiguous Speaker Detector), a neural network model that can\ncorrectly classify particularly ambiguous scenarios. FaVoA not only finds\npositive associations, but helps to rule out non-matching face-voice\nassociations, where a face does not match a voice. Its use of a\ngated-bimodal-unit architecture for the fusion of those models offers a way to\nquantitatively determine how much each modality contributes to the\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_H/0/1/0/all/0/1\">Hugo Carneiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00590","description":"<p>Web search is fundamentally multimodal and multihop. Often, even before\nasking a question we choose to go directly to image search to find our answers.\nFurther, rarely do we find an answer from a single source but aggregate\ninformation and reason through implications. Despite the frequency of this\neveryday occurrence, at present, there is no unified question answering\nbenchmark that requires a single model to answer long-form natural language\nquestions from text and open-ended visual sources -- akin to a human's\nexperience. We propose to bridge this gap between the natural language and\ncomputer vision communities with WebQA. We show that A. our multihop text\nqueries are difficult for a large-scale transformer model, and B. existing\nmulti-modal transformers and visual representations do not perform well on\nopen-domain visual queries. Our challenge for the community is to create a\nunified multimodal reasoning model that seamlessly transitions and reasons\nregardless of the source modality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yingshan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_M/0/1/0/all/0/1\">Mridu Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_H/0/1/0/all/0/1\">Hisami Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Guihong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fight Fire with Fire: Fine-tuning Hate Detectors using Large Samples of Generated Hate Speech. (arXiv:2109.00591v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00591","description":"<p>Automatic hate speech detection is hampered by the scarcity of labeled\ndatasetd, leading to poor generalization. We employ pretrained language models\n(LMs) to alleviate this data bottleneck. We utilize the GPT LM for generating\nlarge amounts of synthetic hate speech sequences from available labeled\nexamples, and leverage the generated data in fine-tuning large pretrained LMs\non hate detection. An empirical study using the models of BERT, RoBERTa and\nALBERT, shows that this approach improves generalization significantly and\nconsistently within and across data distributions. In fact, we find that\ngenerating relevant labeled hate speech sequences is preferable to using\nout-of-domain, and sometimes also within-domain, human-labeled examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wullach_T/0/1/0/all/0/1\">Tomer Wullach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adler_A/0/1/0/all/0/1\">Amir Adler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minkov_E/0/1/0/all/0/1\">Einat Minkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning compositional programs with arguments and sampling. (arXiv:2109.00619v1 [cs.AI])","link":"http://arxiv.org/abs/2109.00619","description":"<p>One of the most challenging goals in designing intelligent systems is\nempowering them with the ability to synthesize programs from data. Namely,\ngiven specific requirements in the form of input/output pairs, the goal is to\ntrain a machine learning model to discover a program that satisfies those\nrequirements. A recent class of methods exploits combinatorial search\nprocedures and deep learning to learn compositional programs. However, they\nusually generate only toy programs using a domain-specific language that does\nnot provide any high-level feature, such as function arguments, which reduces\ntheir applicability in real-world settings. We extend upon a state of the art\nmodel, AlphaNPI, by learning to generate functions that can accept arguments.\nThis improvement will enable us to move closer to real computer programs.\nMoreover, we investigate employing an Approximate version of Monte Carlo Tree\nSearch (A-MCTS) to speed up convergence. We showcase the potential of our\napproach by learning the Quicksort algorithm, showing how the ability to deal\nwith arguments is crucial for learning and generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toni_G/0/1/0/all/0/1\">Giovanni De Toni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erculiani_L/0/1/0/all/0/1\">Luca Erculiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1\">Andrea Passerini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Multi-Centroid Template Matching Algorithm and Its Application to Cough Detection. (arXiv:2109.00630v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00630","description":"<p>Cough is a major symptom of respiratory-related diseases. There exists a\ntremendous amount of work in detecting coughs from audio but there has been no\neffort to identify coughs from solely inertial measurement unit (IMU). Coughing\ncauses motion across the whole body and especially on the neck and head.\nTherefore, head motion data during coughing captured by a head-worn IMU sensor\ncould be leveraged to detect coughs using a template matching algorithm. In\ntime series template matching problems, K-Nearest Neighbors (KNN) combined with\nelastic distance measurement (esp. Dynamic Time Warping (DTW)) achieves\noutstanding performance. However, it is often regarded as prohibitively\ntime-consuming. Nearest Centroid Classifier is thereafter proposed. But the\naccuracy is comprised of only one centroid obtained for each class.\nCentroid-based Classifier performs clustering and averaging for each cluster,\nbut requires manually setting the number of clusters. We propose a novel\nself-tuning multi-centroid template-matching algorithm, which can automatically\nadjust the number of clusters to balance accuracy and inference time. Through\nexperiments conducted on synthetic datasets and a real-world earbud-based cough\ndataset, we demonstrate the superiority of our proposed algorithm and present\nthe result of cough detection with a single accelerometer sensor on the earbuds\nplatform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shibo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemati_E/0/1/0/all/0/1\">Ebrahim Nemati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_T/0/1/0/all/0/1\">Tousif Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Mahbubur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_J/0/1/0/all/0/1\">Jilong Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_A/0/1/0/all/0/1\">Alex Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable deep melody generation via hierarchical music structure representation. (arXiv:2109.00663v1 [cs.SD])","link":"http://arxiv.org/abs/2109.00663","description":"<p>Recent advances in deep learning have expanded possibilities to generate\nmusic, but generating a customizable full piece of music with consistent\nlong-term structure remains a challenge. This paper introduces MusicFrameworks,\na hierarchical music structure representation and a multi-step generative\nprocess to create a full-length melody guided by long-term repetitive\nstructure, chord, melodic contour, and rhythm constraints. We first organize\nthe full melody with section and phrase-level structure. To generate melody in\neach phrase, we generate rhythm and basic melody using two separate\ntransformer-based networks, and then generate the melody conditioned on the\nbasic melody, rhythm and chords in an auto-regressive manner. By factoring\nmusic generation into sub-problems, our approach allows simpler models and\nrequires less data. To customize or add variety, one can alter chords, basic\nmelody, and rhythm structure in the music frameworks, letting our networks\ngenerate the melody accordingly. Additionally, we introduce new features to\nencode musical positional information, rhythm patterns, and melodic contours\nbased on musical domain knowledge. A listening test reveals that melodies\ngenerated by our method are rated as good as or better than human-composed\nmusic in the POP909 dataset about half the time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Shuqi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zeyu Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1\">Celso Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dannenberg_R/0/1/0/all/0/1\">Roger B. Dannenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TabFairGAN: Fair Tabular Data Generation with Generative Adversarial Networks. (arXiv:2109.00666v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00666","description":"<p>With the increasing reliance on automated decision making, the issue of\nalgorithmic fairness has gained increasing importance. In this paper, we\npropose a Generative Adversarial Network for tabular data generation. The model\nincludes two phases of training. In the first phase, the model is trained to\naccurately generate synthetic data similar to the reference dataset. In the\nsecond phase we modify the value function to add fairness constraint, and\ncontinue training the network to generate data that is both accurate and fair.\nWe test our results in both cases of unconstrained, and constrained fair data\ngeneration. In the unconstrained case, i.e. when the model is only trained in\nthe first phase and is only meant to generate accurate data following the same\njoint probability distribution of the real data, the results show that the\nmodel beats state-of-the-art GANs proposed in the literature to produce\nsynthetic tabular data. Also, in the constrained case in which the first phase\nof training is followed by the second phase, we train the network and test it\non four datasets studied in the fairness literature and compare our results\nwith another state-of-the-art pre-processing method, and present the promising\nresults that it achieves. Comparing to other studies utilizing GANs for fair\ndata generation, our model is comparably more stable by using only one critic,\nand also by avoiding major problems of original GAN model, such as\nmode-dropping and non-convergence, by implementing a Wasserstein GAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajabi_A/0/1/0/all/0/1\">Amirarsalan Rajabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garibay_O/0/1/0/all/0/1\">Ozlem Ozmen Garibay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AnANet: Modeling Association and Alignment for Cross-modal Correlation Classification. (arXiv:2109.00693v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00693","description":"<p>The explosive increase of multimodal data makes a great demand in many\ncross-modal applications that follow the strict prior related assumption. Thus\nresearchers study the definition of cross-modal correlation category and\nconstruct various classification systems and predictive models. However, those\nsystems pay more attention to the fine-grained relevant types of cross-modal\ncorrelation, ignoring lots of implicit relevant data which are often divided\ninto irrelevant types. What's worse is that none of previous predictive models\nmanifest the essence of cross-modal correlation according to their definition\nat the modeling stage. In this paper, we present a comprehensive analysis of\nthe image-text correlation and redefine a new classification system based on\nimplicit association and explicit alignment. To predict the type of image-text\ncorrelation, we propose the Association and Alignment Network according to our\nproposed definition (namely AnANet) which implicitly represents the global\ndiscrepancy and commonality between image and text and explicitly captures the\ncross-modal local relevance. The experimental results on our constructed new\nimage-text correlation dataset show the effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Nan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruike Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Wenji Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Algorithms For Fair Clustering with a New Fairness Notion. (arXiv:2109.00708v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00708","description":"<p>We revisit the problem of fair clustering, first introduced by Chierichetti\net al., that requires each protected attribute to have approximately equal\nrepresentation in every cluster; i.e., a balance property. Existing solutions\nto fair clustering are either not scalable or do not achieve an optimal\ntrade-off between clustering objective and fairness. In this paper, we propose\na new notion of fairness, which we call $tau$-fair fairness, that strictly\ngeneralizes the balance property and enables a fine-grained efficiency vs.\nfairness trade-off. Furthermore, we show that simple greedy round-robin based\nalgorithms achieve this trade-off efficiently. Under a more general setting of\nmulti-valued protected attributes, we rigorously analyze the theoretical\nproperties of the our algorithms. Our experimental results suggest that the\nproposed solution outperforms all the state-of-the-art algorithms and works\nexceptionally well even for a large number of clusters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shivam Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghalme_G/0/1/0/all/0/1\">Ganesh Ghalme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1\">Narayanan C. Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shweta Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RF-LighGBM: A probabilistic ensemble way to predict customer repurchase behaviour in community e-commerce. (arXiv:2109.00724v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00724","description":"<p>It is reported that the number of online payment users in China has reached\n854 million; with the emergence of community e-commerce platforms, the trend of\nintegration of e-commerce and social applications is increasingly intense.\nCommunity e-commerce is not a mature and sound comprehensive e-commerce with\nfewer categories and low brand value. To effectively retain community users and\nfully explore customer value has become an important challenge for community\ne-commerce operators. Given the above problems, this paper uses the data-driven\nmethod to study the prediction of community e-commerce customers' repurchase\nbehaviour. The main research contents include 1. Given the complex problem of\nfeature engineering, the classic model RFM in the field of customer\nrelationship management is improved, and an improved model is proposed to\ndescribe the characteristics of customer buying behaviour, which includes five\nindicators. 2. In view of the imbalance of machine learning training samples in\nSMOTE-ENN, a training sample balance using SMOTE-ENN is proposed. The\nexperimental results show that the machine learning model can be trained more\neffectively on balanced samples. 3. Aiming at the complexity of the parameter\nadjustment process, an automatic hyperparameter optimization method based on\nthe TPE method was proposed. Compared with other methods, the model's\nprediction performance is improved, and the training time is reduced by more\nthan 450%. 4. Aiming at the weak prediction ability of a single model, the soft\nvoting based RF-LightgBM model was proposed. The experimental results show that\nthe RF-LighTGBM model proposed in this paper can effectively predict customer\nrepurchase behaviour, and the F1 value is 0.859, which is better than the\nsingle model and previous research results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liping Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xiaxia Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Some Inapproximability Results of MAP Inference and Exponentiated Determinantal Point Processes. (arXiv:2109.00727v1 [cs.DS])","link":"http://arxiv.org/abs/2109.00727","description":"<p>We study the computational complexity of two hard problems on determinantal\npoint processes (DPPs). One is maximum a posteriori (MAP) inference, i.e., to\nfind a principal submatrix having the maximum determinant. The other is\nprobabilistic inference on exponentiated DPPs (E-DPPs), which can sharpen or\nweaken the diversity preference of DPPs with an exponent parameter $p$. We\nprove the following complexity-theoretic hardness results that explain the\ndifficulty in approximating MAP inference and the normalizing constant for\nE-DPPs.\n</p>\n<p>1. Unconstrained MAP inference for an $n \\times n$ matrix is NP-hard to\napproximate within a factor of $2^{\\beta n}$, where $\\beta = 10^{-10^{13}} $.\nThis result improves upon a $(\\frac{9}{8}-\\epsilon)$-factor inapproximability\ngiven by Kulesza and Taskar (2012).\n</p>\n<p>2. Log-determinant maximization is NP-hard to approximate within a factor of\n$\\frac{5}{4}$ for the unconstrained case and within a factor of\n$1+10^{-10^{13}}$ for the size-constrained monotone case.\n</p>\n<p>3. The normalizing constant for E-DPPs of any (fixed) constant exponent $p\n\\geq \\beta^{-1} = 10^{10^{13}}$ is NP-hard to approximate within a factor of\n$2^{\\beta pn}$. This gives a(nother) negative answer to open questions posed by\nKulesza and Taskar (2012); Ohsaka and Matsuoka (2020).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ohsaka_N/0/1/0/all/0/1\">Naoto Ohsaka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConQX: Semantic Expansion of Spoken Queries for Intent Detection based on Conditioned Text Generation. (arXiv:2109.00729v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00729","description":"<p>Intent detection of spoken queries is a challenging task due to their noisy\nstructure and short length. To provide additional information regarding the\nquery and enhance the performance of intent detection, we propose a method for\nsemantic expansion of spoken queries, called ConQX, which utilizes the text\ngeneration ability of an auto-regressive language model, GPT-2. To avoid\noff-topic text generation, we condition the input query to a structured context\nwith prompt mining. We then apply zero-shot, one-shot, and few-shot learning.\nWe lastly use the expanded queries to fine-tune BERT and RoBERTa for intent\ndetection. The experimental results show that the performance of intent\ndetection can be improved by our semantic expansion method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Eyup Halit Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toraman_C/0/1/0/all/0/1\">Cagri Toraman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Energy-Efficient Multi-Orchestrator Mobile Edge Learning. (arXiv:2109.00757v1 [cs.NI])","link":"http://arxiv.org/abs/2109.00757","description":"<p>Mobile Edge Learning (MEL) is a collaborative learning paradigm that features\ndistributed training of Machine Learning (ML) models over edge devices (e.g.,\nIoT devices). In MEL, possible coexistence of multiple learning tasks with\ndifferent datasets may arise. The heterogeneity in edge devices' capabilities\nwill require the joint optimization of the learners-orchestrator association\nand task allocation. To this end, we aim to develop an energy-efficient\nframework for learners-orchestrator association and learning task allocation,\nin which each orchestrator gets associated with a group of learners with the\nsame learning task based on their communication channel qualities and\ncomputational resources, and allocate the tasks accordingly. Therein, a multi\nobjective optimization problem is formulated to minimize the total energy\nconsumption and maximize the learning tasks' accuracy. However, solving such\noptimization problem requires centralization and the presence of the whole\nenvironment information at a single entity, which becomes impractical in\nlarge-scale systems. To reduce the solution complexity and to enable solution\ndecentralization, we propose lightweight heuristic algorithms that can achieve\nnear-optimal performance and facilitate the trade-offs between energy\nconsumption, accuracy, and solution complexity. Simulation results show that\nthe proposed approaches reduce the energy consumption significantly while\nexecuting multiple learning tasks compared to recent state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Allahham_M/0/1/0/all/0/1\">Mhd Saria Allahham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorour_S/0/1/0/all/0/1\">Sameh Sorour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Amr Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1\">Aiman Erbad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1\">Mohsen Guizani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VIbCReg: Variance-Invariance-better-Covariance Regularization for Self-Supervised Learning on Time Series. (arXiv:2109.00783v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00783","description":"<p>Self-supervised learning for image representations has recently had many\nbreakthroughs with respect to linear evaluation and fine-tuning evaluation.\nThese approaches rely on both cleverly crafted loss functions and training\nsetups to avoid the feature collapse problem. In this paper, we improve on the\nrecently proposed VICReg paper, which introduced a loss function that does not\nrely on specialized training loops to converge to useful representations. Our\nmethod improves on a covariance term proposed in VICReg, and in addition we\naugment the head of the architecture by an IterNorm layer that greatly\naccelerates convergence of the model. Our model achieves superior performance\non linear evaluation and fine-tuning evaluation on a subset of the UCR time\nseries classification archive and the PTB-XL ECG dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Daesoo Lee</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Aune_E/0/1/0/all/0/1\">Erlend Aune</a> (1 and 2) ((1) Norwegian University of Science and Technology, (2) BI Norwegian Business School)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NASI: Label- and Data-agnostic Neural Architecture Search at Initialization. (arXiv:2109.00817v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00817","description":"<p>Recent years have witnessed a surging interest in Neural Architecture Search\n(NAS). Various algorithms have been proposed to improve the search efficiency\nand effectiveness of NAS, i.e., to reduce the search cost and improve the\ngeneralization performance of the selected architectures, respectively.\nHowever, the search efficiency of these algorithms is severely limited by the\nneed for model training during the search process. To overcome this limitation,\nwe propose a novel NAS algorithm called NAS at Initialization (NASI) that\nexploits the capability of a Neural Tangent Kernel in being able to\ncharacterize the converged performance of candidate architectures at\ninitialization, hence allowing model training to be completely avoided to boost\nthe search efficiency. Besides the improved search efficiency, NASI also\nachieves competitive search effectiveness on various datasets like CIFAR-10/100\nand ImageNet. Further, NASI is shown to be label- and data-agnostic under mild\nconditions, which guarantees the transferability of architectures selected by\nour NASI over different datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1\">Yao Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shaofeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zhongxiang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ooi_B/0/1/0/all/0/1\">Beng Chin Ooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Low_B/0/1/0/all/0/1\">Bryan Kian Hsiang Low</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SlowFast Rolling-Unrolling LSTMs for Action Anticipation in Egocentric Videos. (arXiv:2109.00829v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00829","description":"<p>Action anticipation in egocentric videos is a difficult task due to the\ninherently multi-modal nature of human actions. Additionally, some actions\nhappen faster or slower than others depending on the actor or surrounding\ncontext which could vary each time and lead to different predictions. Based on\nthis idea, we build upon RULSTM architecture, which is specifically designed\nfor anticipating human actions, and propose a novel attention-based technique\nto evaluate, simultaneously, slow and fast features extracted from three\ndifferent modalities, namely RGB, optical flow, and extracted objects. Two\nbranches process information at different time scales, i.e., frame-rates, and\nseveral fusion schemes are considered to improve prediction accuracy. We\nperform extensive experiments on EpicKitchens-55 and EGTEA Gaze+ datasets, and\ndemonstrate that our technique systematically improves the results of RULSTM\narchitecture for Top-5 accuracy metric at different anticipation times.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Osman_N/0/1/0/all/0/1\">Nada Osman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camporese_G/0/1/0/all/0/1\">Guglielmo Camporese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coscia_P/0/1/0/all/0/1\">Pasquale Coscia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballan_L/0/1/0/all/0/1\">Lamberto Ballan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knot invariants and their relations: a topological perspective. (arXiv:2109.00831v1 [math.AT])","link":"http://arxiv.org/abs/2109.00831","description":"<p>This work brings methods from topological data analysis to knot theory and\ndevelops new data analysis tools inspired by this application. We explore a\nvast collection of knot invariants and relations between then using Mapper and\nBall Mapper algorithms. In particular, we develop versions of the Ball Mapper\nalgorithm that incorporate symmetries and other relations within the data, and\nprovide ways to compare data arising from different descriptors, such as knot\ninvariants. Additionally, we extend the Mapper construction to the case where\nthe range of the lens function is high dimensional rather than a 1-dimensional\nspace, that also provides ways of visualizing functions between\nhigh-dimensional spaces. We illustrate the use of these techniques on knot\ntheory data and draw attention to potential implications of our findings in\nknot theory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Pawel_D/0/1/0/all/0/1\">D&#x142;otko Pawe&#x142;</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gurnari_D/0/1/0/all/0/1\">Davide Gurnari</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sazdanovic_R/0/1/0/all/0/1\">Radmila Sazdanovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Automated Framework for Supporting Data-Governance Rule Compliance in Decentralized MIMO Contexts. (arXiv:2109.00838v1 [cs.AI])","link":"http://arxiv.org/abs/2109.00838","description":"<p>We propose Dr.Aid, a logic-based AI framework for automated compliance\nchecking of data governance rules over data-flow graphs. The rules are modelled\nusing a formal language based on situation calculus and are suitable for\ndecentralized contexts with multi-input-multi-output (MIMO) processes. Dr.Aid\nmodels data rules and flow rules and checks compliance by reasoning about the\npropagation, combination, modification and application of data rules over the\ndata flow graphs. Our approach is driven and evaluated by real-world datasets\nusing provenance graphs from data-intensive research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imposing Relation Structure in Language-Model Embeddings Using Contrastive Learning. (arXiv:2109.00840v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00840","description":"<p>Though language model text embeddings have revolutionized NLP research, their\nability to capture high-level semantic information, such as relations between\nentities in text, is limited. In this paper, we propose a novel contrastive\nlearning framework that trains sentence embeddings to encode the relations in a\ngraph structure. Given a sentence (unstructured text) and its graph, we use\ncontrastive learning to impose relation-related structure on the token-level\nrepresentations of the sentence obtained with a CharacterBERT (El Boukkouri et\nal.,2020) model. The resulting relation-aware sentence embeddings achieve\nstate-of-the-art results on the relation extraction task using only a simple\nKNN classifier, thereby demonstrating the success of the proposed method.\nAdditional visualization by a tSNE analysis shows the effectiveness of the\nlearned representation space compared to baselines. Furthermore, we show that\nwe can learn a different space for named entity recognition, again using a\ncontrastive learning objective, and demonstrate how to successfully combine\nboth representation spaces in an entity-relation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Theodoropoulos_C/0/1/0/all/0/1\">Christos Theodoropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coman_A/0/1/0/all/0/1\">Andrei C. Coman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPU-accelerated Optimal Path Planning in Stochastic Dynamic Environments. (arXiv:2109.00857v1 [cs.AI])","link":"http://arxiv.org/abs/2109.00857","description":"<p>Autonomous marine vehicles play an essential role in many ocean science and\nengineering applications. Planning time and energy optimal paths for these\nvehicles to navigate in stochastic dynamic ocean environments is essential to\nreduce operational costs. In some missions, they must also harvest solar, wind,\nor wave energy (modeled as a stochastic scalar field) and move in optimal paths\nthat minimize net energy consumption. Markov Decision Processes (MDPs) provide\na natural framework for sequential decision-making for robotic agents in such\nenvironments. However, building a realistic model and solving the modeled MDP\nbecomes computationally expensive in large-scale real-time applications,\nwarranting the need for parallel algorithms and efficient implementation. In\nthe present work, we introduce an efficient end-to-end GPU-accelerated\nalgorithm that (i) builds the MDP model (computing transition probabilities and\nexpected one-step rewards); and (ii) solves the MDP to compute an optimal\npolicy. We develop methodical and algorithmic solutions to overcome the limited\nglobal memory of GPUs by (i) using a dynamic reduced-order representation of\nthe ocean flows, (ii) leveraging the sparse nature of the state transition\nprobability matrix, (iii) introducing a neighbouring sub-grid concept and (iv)\nproving that it is sufficient to use only the stochastic scalar field's mean to\ncompute the expected one-step rewards for missions involving energy harvesting\nfrom the environment; thereby saving memory and reducing the computational\neffort. We demonstrate the algorithm on a simulated stochastic dynamic\nenvironment and highlight that it builds the MDP model and computes the optimal\npolicy 600-1000x faster than conventional CPU implementations, making it\nsuitable for real-time use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_R/0/1/0/all/0/1\">Rohit Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramani_D/0/1/0/all/0/1\">Deepak Subramani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VORRT-COLREGs: A Hybrid Velocity Obstacles and RRT Based COLREGs-Compliant Path Planner for Autonomous Surface Vessels. (arXiv:2109.00862v1 [cs.RO])","link":"http://arxiv.org/abs/2109.00862","description":"<p>This paper presents VORRT-COLREGs, a hybrid technique that combines velocity\nobstacles (VO) and rapidly-exploring random trees (RRT) to generate safe\ntrajectories for autonomous surface vessels (ASVs) while following nautical\nrules of the road. RRT generates a set of way points and the velocity obstacles\nmethod ensures safe travel between way points. We also ensure that the actions\nof ASVs do not violate maritime collision guidelines. Earlier work has used RRT\nand VO separately to generate paths for ASVs. However, RRT does not handle\nhighly dynamic situations well and and VO seems most suitable as a local path\nplanner. Combining both approaches, VORRT-COLREGs is a global path planner that\nuses a joint forward simulation to ensure that generated paths remain valid and\ncollision free as the situation changes. Experiments were conducted in\ndifferent types of collision scenarios and with different numbers of ASVs.\nResults show that VORRT-COLREGS generated collision regulations (COLREGs)\ncomplaint paths in open ocean scenarios. Furthermore, VORRT-COLREGS\nsuccessfully generated compliant paths within traffic separation schemes. These\nresults show the applicability of our technique for generating paths for ASVs\nin different collision scenarios. To the best of our knowledge, this is the\nfirst work that combines velocity obstacles and RRT to produce safe and COLREGs\ncomplaint path for ASVs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dubey_R/0/1/0/all/0/1\">Rahul Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Louis_S/0/1/0/all/0/1\">Sushil J Louis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Habitual and Reflective Control in Hierarchical Predictive Coding. (arXiv:2109.00866v1 [cs.AI])","link":"http://arxiv.org/abs/2109.00866","description":"<p>In cognitive science, behaviour is often separated into two types. Reflexive\ncontrol is habitual and immediate, whereas reflective is deliberative and time\nconsuming. We examine the argument that Hierarchical Predictive Coding (HPC)\ncan explain both types of behaviour as a continuum operating across a\nmulti-layered network, removing the need for separate circuits in the brain. On\nthis view, \"fast\" actions may be triggered using only the lower layers of the\nHPC schema, whereas more deliberative actions need higher layers. We\ndemonstrate that HPC can distribute learning throughout its hierarchy, with\nhigher layers called into use only as required.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kinghorn_P/0/1/0/all/0/1\">Paul F. Kinghorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millidge_B/0/1/0/all/0/1\">Beren Millidge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buckley_C/0/1/0/all/0/1\">Christopher L. Buckley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MACRPO: Multi-Agent Cooperative Recurrent Policy Optimization. (arXiv:2109.00882v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00882","description":"<p>This work considers the problem of learning cooperative policies in\nmulti-agent settings with partially observable and non-stationary environments\nwithout a communication channel. We focus on improving information sharing\nbetween agents and propose a new multi-agent actor-critic method called\n\\textit{Multi-Agent Cooperative Recurrent Proximal Policy Optimization}\n(MACRPO). We propose two novel ways of integrating information across agents\nand time in MACRPO: First, we use a recurrent layer in critic's network\narchitecture and propose a new framework to use a meta-trajectory to train the\nrecurrent layer. This allows the network to learn the cooperation and dynamics\nof interactions between agents, and also handle partial observability. Second,\nwe propose a new advantage function that incorporates other agents' rewards and\nvalue functions. We evaluate our algorithm on three challenging multi-agent\nenvironments with continuous and discrete action spaces, Deepdrive-Zero,\nMulti-Walker, and Particle environment. We compare the results with several\nablations and state-of-the-art multi-agent algorithms such as QMIX and MADDPG\nand also single-agent methods with shared parameters between agents such as\nIMPALA and APEX. The results show superior performance against other\nalgorithms. The code is available online at\nhttps://github.com/kargarisaac/macrpo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kargar_E/0/1/0/all/0/1\">Eshagh Kargar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrki_V/0/1/0/all/0/1\">Ville Kyrki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Perceived Multi-modal Pretraining in E-commerce. (arXiv:2109.00895v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00895","description":"<p>In this paper, we address multi-modal pretraining of product data in the\nfield of E-commerce. Current multi-modal pretraining methods proposed for image\nand text modalities lack robustness in the face of modality-missing and\nmodality-noise, which are two pervasive problems of multi-modal product data in\nreal E-commerce scenarios. To this end, we propose a novel method, K3M, which\nintroduces knowledge modality in multi-modal pretraining to correct the noise\nand supplement the missing of image and text modalities. The modal-encoding\nlayer extracts the features of each modality. The modal-interaction layer is\ncapable of effectively modeling the interaction of multiple modalities, where\nan initial-interactive feature fusion model is designed to maintain the\nindependence of image modality and text modality, and a structure aggregation\nmodule is designed to fuse the information of image, text, and knowledge\nmodalities. We pretrain K3M with three pretraining tasks, including masked\nobject modeling (MOM), masked language modeling (MLM), and link prediction\nmodeling (LPM). Experimental results on a real-world E-commerce dataset and a\nseries of product-based downstream tasks demonstrate that K3M achieves\nsignificant improvements in performances than the baseline and state-of-the-art\nmethods when modality-noise or modality-missing exists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yushan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1\">Huaixiao Tou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_G/0/1/0/all/0/1\">Ganqiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effect of the output activation function on the probabilities and errors in medical image segmentation. (arXiv:2109.00903v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00903","description":"<p>The sigmoid activation is the standard output activation function in binary\nclassification and segmentation with neural networks. Still, there exist a\nvariety of other potential output activation functions, which may lead to\nimproved results in medical image segmentation. In this work, we consider how\nthe asymptotic behavior of different output activation and loss functions\naffects the prediction probabilities and the corresponding segmentation errors.\nFor cross entropy, we show that a faster rate of change of the activation\nfunction correlates with better predictions, while a slower rate of change can\nimprove the calibration of probabilities. For dice loss, we found that the\narctangent activation function is superior to the sigmoid function.\nFurthermore, we provide a test space for arbitrary output activation functions\nin the area of medical image segmentation. We tested seven activation functions\nin combination with three loss functions on four different medical image\nsegmentation tasks to provide a classification of which function is best suited\nin this application scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nieradzik_L/0/1/0/all/0/1\">Lars Nieradzik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheuermann_G/0/1/0/all/0/1\">Gerik Scheuermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saur_D/0/1/0/all/0/1\">Dorothee Saur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillmann_C/0/1/0/all/0/1\">Christina Gillmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparsifying the Update Step in Graph Neural Networks. (arXiv:2109.00909v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00909","description":"<p>Message-Passing Neural Networks (MPNNs), the most prominent Graph Neural\nNetwork (GNN) framework, celebrate much success in the analysis of\ngraph-structured data. Concurrently, the sparsification of Neural Network\nmodels attracts a great amount of academic and industrial interest. In this\npaper, we conduct a structured study of the effect of sparsification on the\ntrainable part of MPNNs known as the Update step. To this end, we design a\nseries of models to successively sparsify the linear transform in the Update\nstep. Specifically, we propose the ExpanderGNN model with a tuneable\nsparsification rate and the Activation-Only GNN, which has no linear transform\nin the Update step. In agreement with a growing trend in the literature, the\nsparsification paradigm is changed by initialising sparse neural network\narchitectures rather than expensively sparsifying already trained\narchitectures. Our novel benchmark models enable a better understanding of the\ninfluence of the Update step on model performance and outperform existing\nsimplified benchmark models such as the Simple Graph Convolution. The\nExpanderGNNs, and in some cases the Activation-Only models, achieve performance\non par with their vanilla counterparts on several downstream tasks while\ncontaining significantly fewer trainable parameters. In experiments with\nmatching parameter numbers, our benchmark models outperform the\nstate-of-the-art GNN models. Our code is publicly available at:\nhttps://github.com/ChangminWu/ExpanderGNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lutzeyer_J/0/1/0/all/0/1\">Johannes F. Lutzeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Changmin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task learning from fixed-wing UAV images for 2D/3D city modeling. (arXiv:2109.00918v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00918","description":"<p>Single-task learning in artificial neural networks will be able to learn the\nmodel very well, and the benefits brought by transferring knowledge thus become\nlimited. In this regard, when the number of tasks increases (e.g., semantic\nsegmentation, panoptic segmentation, monocular depth estimation, and 3D point\ncloud), duplicate information may exist across tasks, and the improvement\nbecomes less significant. Multi-task learning has emerged as a solution to\nknowledge-transfer issues and is an approach to scene understanding which\ninvolves multiple related tasks each with potentially limited training data.\nMulti-task learning improves generalization by leveraging the domain-specific\ninformation contained in the training data of related tasks. In urban\nmanagement applications such as infrastructure development, traffic monitoring,\nsmart 3D cities, and change detection, automated multi-task data analysis for\nscene understanding based on the semantic, instance, and panoptic annotation,\nas well as monocular depth estimation, is required to generate precise urban\nmodels. In this study, a common framework for the performance assessment of\nmulti-task learning methods from fixed-wing UAV images for 2D/3D city modeling\nis presented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bayanlou_M/0/1/0/all/0/1\">Mohammad R. Bayanlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoshboresh_Masouleh_M/0/1/0/all/0/1\">Mehdi Khoshboresh-Masouleh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Multimodal fusion via Mutual Dependency Maximisation. (arXiv:2109.00922v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00922","description":"<p>Multimodal sentiment analysis is a trending area of research, and the\nmultimodal fusion is one of its most active topic. Acknowledging humans\ncommunicate through a variety of channels (i.e visual, acoustic, linguistic),\nmultimodal systems aim at integrating different unimodal representations into a\nsynthetic one. So far, a consequent effort has been made on developing complex\narchitectures allowing the fusion of these modalities. However, such systems\nare mainly trained by minimising simple losses such as $L_1$ or cross-entropy.\nIn this work, we investigate unexplored penalties and propose a set of new\nobjectives that measure the dependency between modalities. We demonstrate that\nour new penalties lead to a consistent improvement (up to $4.3$ on accuracy)\nacross a large variety of state-of-the-art models on two well-known sentiment\nanalysis datasets: \\texttt{CMU-MOSI} and \\texttt{CMU-MOSEI}. Our method not\nonly achieves a new SOTA on both datasets but also produces representations\nthat are more robust to modality drops. Finally, a by-product of our methods\nincludes a statistical network which can be used to interpret the high\ndimensional representations learnt by the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chapuis_E/0/1/0/all/0/1\">Emile Chapuis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labeau_M/0/1/0/all/0/1\">Matthieu Labeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chloe Clavel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autonomous Curiosity for Real-Time Training Onboard Robotic Agents. (arXiv:2109.00927v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00927","description":"<p>Learning requires both study and curiosity. A good learner is not only good\nat extracting information from the data given to it, but also skilled at\nfinding the right new information to learn from. This is especially true when a\nhuman operator is required to provide the ground truth - such a source should\nonly be queried sparingly. In this work, we address the problem of curiosity as\nit relates to online, real-time, human-in-the-loop training of an object\ndetection algorithm onboard a robotic platform, one where motion produces new\nviews of the subject. We propose a deep reinforcement learning approach that\ndecides when to ask the human user for ground truth, and when to move. Through\na series of experiments, we demonstrate that our agent learns a movement and\nrequest policy that is at least 3x more effective at using human user\ninteractions to train an object detector than untrained approaches, and is\ngeneralizable to a variety of subjects and environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teng_E/0/1/0/all/0/1\">Ervin Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iannucci_B/0/1/0/all/0/1\">Bob Iannucci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Algorithms for Intelligent Traffic Signal Control. (arXiv:2109.00937v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00937","description":"<p>In this paper, methods have been explored to effectively optimise traffic\nsignal control to minimise waiting times and queue lengths, thereby increasing\ntraffic flow. The traffic intersection was first defined as a Markov Decision\nProcess, and a state representation, actions and rewards were chosen.\nSimulation of Urban MObility (SUMO) was used to simulate an intersection and\nthen compare a Round Robin Scheduler, a Feedback Control mechanism and two\nReinforcement Learning techniques - Deep Q Network (DQN) and Advantage\nActor-Critic (A2C), as the policy for the traffic signal in the simulation\nunder different scenarios. Finally, the methods were tested on a simulation of\na real-world intersection in Bengaluru, India.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_H/0/1/0/all/0/1\">Hrishit Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masti_V/0/1/0/all/0/1\">Vibha Masti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veerendranath_V/0/1/0/all/0/1\">Vishruth Veerendranath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_D/0/1/0/all/0/1\">Dr. S Natarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAM: Explainable Visual Similarity and Classification via Gradient Activation Maps. (arXiv:2109.00951v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00951","description":"<p>We present Gradient Activation Maps (GAM) - a machinery for explaining\npredictions made by visual similarity and classification models. By gleaning\nlocalized gradient and activation information from multiple network layers, GAM\noffers improved visual explanations, when compared to existing alternatives.\nThe algorithmic advantages of GAM are explained in detail, and validated\nempirically, where it is shown that GAM outperforms its alternatives across\nvarious tasks and datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barkan_O/0/1/0/all/0/1\">Oren Barkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armstrong_O/0/1/0/all/0/1\">Omri Armstrong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertz_A/0/1/0/all/0/1\">Amir Hertz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1\">Avi Caciularu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_O/0/1/0/all/0/1\">Ori Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkiel_I/0/1/0/all/0/1\">Itzik Malkiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koenigstein_N/0/1/0/all/0/1\">Noam Koenigstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrouSPI-Net: Spatio-temporal attention on parallel atrous convolutions and U-GRUs for skeletal pedestrian crossing prediction. (arXiv:2109.00953v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00953","description":"<p>Understanding the behaviors and intentions of pedestrians is still one of the\nmain challenges for vehicle autonomy, as accurate predictions of their\nintentions can guarantee their safety and driving comfort of vehicles. In this\npaper, we address pedestrian crossing prediction in urban traffic environments\nby linking the dynamics of a pedestrian's skeleton to a binary crossing\nintention. We introduce TrouSPI-Net: a context-free, lightweight, multi-branch\npredictor. TrouSPI-Net extracts spatio-temporal features for different time\nresolutions by encoding pseudo-images sequences of skeletal joints' positions\nand processes them with parallel attention modules and atrous convolutions. The\nproposed approach is then enhanced by processing features such as relative\ndistances of skeletal joints, bounding box positions, or ego-vehicle speed with\nU-GRUs. Using the newly proposed evaluation procedures for two large public\nnaturalistic data sets for studying pedestrian behavior in traffic: JAAD and\nPIE, we evaluate TrouSPI-Net and analyze its performance. Experimental results\nshow that TrouSPI-Net achieved 0.76 F1 score on JAAD and 0.80 F1 score on PIE,\ntherefore outperforming current state-of-the-art while being lightweight and\ncontext-free.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gesnouin_J/0/1/0/all/0/1\">Joseph Gesnouin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechberti_S/0/1/0/all/0/1\">Steve Pechberti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanciulescu_B/0/1/0/all/0/1\">Bogdan Stanciulescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1\">Fabien Moutarde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Infrared Image Super-Resolution via Heterogeneous Convolutional WGAN. (arXiv:2109.00960v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00960","description":"<p>Image super-resolution is important in many fields, such as surveillance and\nremote sensing. However, infrared (IR) images normally have low resolution\nsince the optical equipment is relatively expensive. Recently, deep learning\nmethods have dominated image super-resolution and achieved remarkable\nperformance on visible images; however, IR images have received less attention.\nIR images have fewer patterns, and hence, it is difficult for deep neural\nnetworks (DNNs) to learn diverse features from IR images. In this paper, we\npresent a framework that employs heterogeneous convolution and adversarial\ntraining, namely, heterogeneous kernel-based super-resolution Wasserstein GAN\n(HetSRWGAN), for IR image super-resolution. The HetSRWGAN algorithm is a\nlightweight GAN architecture that applies a plug-and-play heterogeneous\nkernel-based residual block. Moreover, a novel loss function that employs image\ngradients is adopted, which can be applied to an arbitrary model. The proposed\nHetSRWGAN achieves consistently better performance in both qualitative and\nquantitative evaluations. According to the experimental results, the whole\ntraining process is more stable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongsong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zetao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guoming Pang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Dedicated CDCL Strategies for PB Solvers. (arXiv:2109.01013v1 [cs.AI])","link":"http://arxiv.org/abs/2109.01013","description":"<p>Current implementations of pseudo-Boolean (PB) solvers working on native PB\nconstraints are based on the CDCL architecture which empowers highly efficient\nmodern SAT solvers. In particular, such PB solvers not only implement a\n(cutting-planes-based) conflict analysis procedure, but also complementary\nstrategies for components that are crucial for the efficiency of CDCL, namely\nbranching heuristics, learned constraint deletion and restarts. However, these\nstrategies are mostly reused by PB solvers without considering the particular\nform of the PB constraints they deal with. In this paper, we present and\nevaluate different ways of adapting CDCL strategies to take the specificities\nof PB constraints into account while preserving the behavior they have in the\nclausal setting. We implemented these strategies in two different solvers,\nnamely Sat4j (for which we consider three configurations) and RoundingSat. Our\nexperiments show that these dedicated strategies allow to improve, sometimes\nsignificantly, the performance of these solvers, both on decision and\noptimization problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berre_D/0/1/0/all/0/1\">Daniel Le Berre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallon_R/0/1/0/all/0/1\">Romain Wallon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing possible failure modes in physics-informed neural networks. (arXiv:2109.01050v1 [cs.LG])","link":"http://arxiv.org/abs/2109.01050","description":"<p>Recent work in scientific machine learning has developed so-called\nphysics-informed neural network (PINN) models. The typical approach is to\nincorporate physical domain knowledge as soft constraints on an empirical loss\nfunction and use existing machine learning methodologies to train the model. We\ndemonstrate that, while existing PINN methodologies can learn good models for\nrelatively trivial problems, they can easily fail to learn relevant physical\nphenomena even for simple PDEs. In particular, we analyze several distinct\nsituations of widespread physical interest, including learning differential\nequations with convection, reaction, and diffusion operators. We provide\nevidence that the soft regularization in PINNs, which involves differential\noperators, can introduce a number of subtle problems, including making the\nproblem ill-conditioned. Importantly, we show that these possible failure modes\nare not due to the lack of expressivity in the NN architecture, but that the\nPINN's setup makes the loss landscape very hard to optimize. We then describe\ntwo promising solutions to address these failure modes. The first approach is\nto use curriculum regularization, where the PINN's loss term starts from a\nsimple PDE regularization, and becomes progressively more complex as the NN\ngets trained. The second approach is to pose the problem as a\nsequence-to-sequence learning task, rather than learning to predict the entire\nspace-time at once. Extensive testing shows that we can achieve up to 1-2\norders of magnitude lower error with these methods as compared to regular PINN\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishnapriyan_A/0/1/0/all/0/1\">Aditi S. Krishnapriyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_S/0/1/0/all/0/1\">Shandian Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirby_R/0/1/0/all/0/1\">Robert M. Kirby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards disease-aware image editing of chest X-rays. (arXiv:2109.01071v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01071","description":"<p>Disease-aware image editing by means of generative adversarial networks\n(GANs) constitutes a promising avenue for advancing the use of AI in the\nhealthcare sector. Here, we present a proof of concept of this idea. While\nGAN-based techniques have been successful in generating and manipulating\nnatural images, their application to the medical domain, however, is still in\nits infancy. Working with the CheXpert data set, we show that StyleGAN can be\ntrained to generate realistic chest X-rays. Inspired by the Cyclic Reverse\nGenerator (CRG) framework, we train an encoder that allows for faithfully\ninverting the generator on synthetic X-rays and provides organ-level\nreconstructions of real ones. Employing a guided manipulation of latent codes,\nwe confer the medical condition of cardiomegaly (increased heart size) onto\nreal X-rays from healthy patients. This work was presented in the Medical\nImaging meets Neurips Workshop 2020, which was held as part of the 34th\nConference on Neural Information Processing Systems (NeurIPS 2020) in\nVancouver, Canada\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_S/0/1/0/all/0/1\">Sai Niranjan Ramachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saboo_A/0/1/0/all/0/1\">Aakash Saboo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dierkes_K/0/1/0/all/0/1\">Kai Dierkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keles_H/0/1/0/all/0/1\">Hacer Yalim Keles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Classification for Predicting Multi-level Product Categories. (arXiv:2109.01084v1 [cs.LG])","link":"http://arxiv.org/abs/2109.01084","description":"<p>In an online shopping platform, a detailed classification of the products\nfacilitates user navigation. It also helps online retailers keep track of the\nprice fluctuations in a certain industry or special discounts on a specific\nproduct category. Moreover, an automated classification system may help to\npinpoint incorrect or subjective categories suggested by an operator. In this\nstudy, we focus on product title classification of the grocery products. We\nperform a comprehensive comparison of six different text classification models\nto establish a strong baseline for this task, which involves testing both\ntraditional and recent machine learning methods. In our experiments, we\ninvestigate the generalizability of the trained models to the products of other\nonline retailers, the dynamic masking of infeasible subcategories for\npretrained language models, and the benefits of incorporating product titles in\nmultiple languages. Our numerical results indicate that dynamic masking of\nsubcategories is effective in improving prediction accuracy. In addition, we\nobserve that using bilingual product titles is generally beneficial, and neural\nnetwork-based models perform significantly better than SVM and XGBoost models.\nLastly, we investigate the reasons for the misclassified products and propose\nfuture research directions to further enhance the prediction models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jahanshahi_H/0/1/0/all/0/1\">Hadi Jahanshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozyegen_O/0/1/0/all/0/1\">Ozan Ozyegen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cevik_M/0/1/0/all/0/1\">Mucahit Cevik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulut_B/0/1/0/all/0/1\">Beste Bulut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yigit_D/0/1/0/all/0/1\">Deniz Yigit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonen_F/0/1/0/all/0/1\">Fahrettin F. Gonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basar_A/0/1/0/all/0/1\">Ay&#x15f;e Ba&#x15f;ar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On-target Adaptation. (arXiv:2109.01087v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01087","description":"<p>Domain adaptation seeks to mitigate the shift between training on the\n\\emph{source} domain and testing on the \\emph{target} domain. Most adaptation\nmethods rely on the source data by joint optimization over source data and\ntarget data. Source-free methods replace the source data with a source model by\nfine-tuning it on target. Either way, the majority of the parameter updates for\nthe model representation and the classifier are derived from the source, and\nnot the target. However, target accuracy is the goal, and so we argue for\noptimizing as much as possible on the target data. We show significant\nimprovement by on-target adaptation, which learns the representation purely\nfrom target data while taking only the source predictions for supervision. In\nthe long-tailed classification setting, we show further improvement by\non-target class distribution learning, which learns the (im)balance of classes\nfrom target data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dequan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaoteng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1\">Sayna Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1\">Evan Shelhamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation. (arXiv:2109.01115v1 [cs.RO])","link":"http://arxiv.org/abs/2109.01115","description":"<p>We study the problem of learning a range of vision-based manipulation tasks\nfrom a large offline dataset of robot interaction. In order to accomplish this,\nhumans need easy and effective ways of specifying tasks to the robot. Goal\nimages are one popular form of task specification, as they are already grounded\nin the robot's observation space. However, goal images also have a number of\ndrawbacks: they are inconvenient for humans to provide, they can over-specify\nthe desired behavior leading to a sparse reward signal, or under-specify task\ninformation in the case of non-goal reaching tasks. Natural language provides a\nconvenient and flexible alternative for task specification, but comes with the\nchallenge of grounding language in the robot's observation space. To scalably\nlearn this grounding we propose to leverage offline robot datasets (including\nhighly sub-optimal, autonomously collected data) with crowd-sourced natural\nlanguage labels. With this data, we learn a simple classifier which predicts if\na change in state completes a language instruction. This provides a\nlanguage-conditioned reward function that can then be used for offline\nmulti-task RL. In our experiments, we find that on language-conditioned\nmanipulation tasks our approach outperforms both goal-image specifications and\nlanguage conditioned imitation techniques by more than 25%, and is able to\nperform visuomotor tasks from natural language, such as \"open the right drawer\"\nand \"move the stapler\", on a Franka Emika Panda robot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1\">Suraj Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_E/0/1/0/all/0/1\">Eric Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kevin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1\">Brian Ichter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Reasoning Engine for the Gamification of Loop-Invariant Discovery. (arXiv:2109.01121v1 [cs.AI])","link":"http://arxiv.org/abs/2109.01121","description":"<p>We describe the design and implementation of a reasoning engine that\nfacilitates the gamification of loop-invariant discovery. Our reasoning engine\nenables students, computational agents and regular software engineers with no\nformal methods expertise to collaboratively prove interesting theorems about\nsimple programs using browser-based, online games. Within an hour, players are\nable to specify and verify properties of programs that are beyond the\ncapabilities of fully-automated tools. The hour limit includes the time for\nsetting up the system, completing a short tutorial explaining game play and\nreasoning about simple imperative programs. Players are never required to\nunderstand formal proofs; they only provide insights by proposing invariants.\nThe reasoning engine is responsible for managing and evaluating the proposed\ninvariants, as well as generating actionable feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Walter_A/0/1/0/all/0/1\">Andrew Walter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooper_S/0/1/0/all/0/1\">Seth Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manolios_P/0/1/0/all/0/1\">Panagiotis Manolios</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking the Robustness of Instance Segmentation Models. (arXiv:2109.01123v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01123","description":"<p>This paper presents a comprehensive evaluation of instance segmentation\nmodels with respect to real-world image corruptions and out-of-domain image\ncollections, e.g. datasets collected with different set-ups than the training\ndatasets the models learned from. The out-of-domain image evaluation shows the\ngeneralization capability of models, an essential aspect of real-world\napplications, and an extensively studied topic of domain adaptation. These\npresented robustness and generalization evaluations are important when\ndesigning instance segmentation models for real-world applications and picking\nan off-the-shelf pretrained model to directly use for the task at hand.\nSpecifically, this benchmark study includes state-of-the-art network\narchitectures, network backbones, normalization layers, models trained starting\nfrom scratch or ImageNet pretrained networks, and the effect of multi-task\ntraining on robustness and generalization. Through this study, we gain several\ninsights e.g. we find that normalization layers play an essential role in\nrobustness, ImageNet pretraining does not help the robustness and the\ngeneralization of models, excluding JPEG corruption, and network backbones and\ncopy-paste augmentations affect robustness significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Altindis_S/0/1/0/all/0/1\">Said Fahri Altindis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalva_Y/0/1/0/all/0/1\">Yusuf Dalva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dundar_A/0/1/0/all/0/1\">Aysegul Dundar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Prompt for Vision-Language Models. (arXiv:2109.01134v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01134","description":"<p>Vision-language pre-training has recently emerged as a promising alternative\nfor representation learning. It shifts from the tradition of using images and\ndiscrete labels for learning a fixed set of weights, seen as visual concepts,\nto aligning images and raw text for two separate encoders. Such a paradigm\nbenefits from a broader source of supervision and allows zero-shot transfer to\ndownstream tasks since visual concepts can be diametrically generated from\nnatural language, known as prompt. In this paper, we identify that a major\nchallenge of deploying such models in practice is prompt engineering. This is\nbecause designing a proper prompt, especially for context words surrounding a\nclass name, requires domain expertise and typically takes a significant amount\nof time for words tuning since a slight change in wording could have a huge\nimpact on performance. Moreover, different downstream tasks require specific\ndesigns, further hampering the efficiency of deployment. To overcome this\nchallenge, we propose a novel approach named context optimization (CoOp). The\nmain idea is to model context in prompts using continuous representations and\nperform end-to-end learning from data while keeping the pre-trained parameters\nfixed. In this way, the design of task-relevant prompts can be fully automated.\nExperiments on 11 datasets show that CoOp effectively turns pre-trained\nvision-language models into data-efficient visual learners, requiring as few as\none or two shots to beat hand-crafted prompts with a decent margin and able to\ngain significant improvements when using more shots (e.g., at 16 shots the\naverage gain is around 17% with the highest reaching over 50%). CoOp also\nexhibits strong robustness to distribution shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable robotic systems: Understanding goal-driven actions in a reinforcement learning scenario. (arXiv:2006.13615v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2006.13615","description":"<p>Robotic systems are more present in our society everyday. In human-robot\nenvironments, it is crucial that end-users may correctly understand their\nrobotic team-partners, in order to collaboratively complete a task. To increase\naction understanding, users demand more explainability about the decisions by\nthe robot in particular situations. Recently, explainable robotic systems have\nemerged as an alternative focused not only on completing a task satisfactorily,\nbut also on justifying, in a human-like manner, the reasons that lead to making\na decision. In reinforcement learning scenarios, a great effort has been\nfocused on providing explanations using data-driven approaches, particularly\nfrom the visual input modality in deep learning-based systems. In this work, we\nfocus rather on the decision-making process of reinforcement learning agents\nperforming a task in a robotic scenario. Experimental results are obtained\nusing 3 different set-ups, namely, a deterministic navigation task, a\nstochastic navigation task, and a continuous visual-based sorting object task.\nAs a way to explain the goal-driven robot's actions, we use the probability of\nsuccess computed by three different proposed approaches: memory-based,\nlearning-based, and introspection-based. The difference between these\napproaches is the amount of memory required to compute or estimate the\nprobability of success as well as the kind of reinforcement learning\nrepresentation where they could be used. In this regard, we use the\nmemory-based approach as a baseline since it is obtained directly from the\nagent's observations. When comparing the learning-based and the\nintrospection-based approaches to this baseline, both are found to be suitable\nalternatives to compute the probability of success, obtaining high levels of\nsimilarity when compared using both the Pearson's correlation and the mean\nsquared error.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cruz_F/0/1/0/all/0/1\">Francisco Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dazeley_R/0/1/0/all/0/1\">Richard Dazeley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vamplew_P/0/1/0/all/0/1\">Peter Vamplew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_I/0/1/0/all/0/1\">Ithan Moreira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GMH: A General Multi-hop Reasoning Model for KG Completion. (arXiv:2010.07620v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2010.07620","description":"<p>Knowledge graphs are essential for numerous downstream natural language\nprocessing applications, but are typically incomplete with many facts missing.\nThis results in research efforts on multi-hop reasoning task, which can be\nformulated as a search process and current models typically perform short\ndistance reasoning. However, the long-distance reasoning is also vital with the\nability to connect the superficially unrelated entities. To the best of our\nknowledge, there lacks a general framework that approaches multi-hop reasoning\nin mixed long-short distance reasoning scenarios. We argue that there are two\nkey issues for a general multi-hop reasoning model: i) where to go, and ii)\nwhen to stop. Therefore, we propose a general model which resolves the issues\nwith three modules: 1) the local-global knowledge module to estimate the\npossible paths, 2) the differentiated action dropout module to explore a\ndiverse set of paths, and 3) the adaptive stopping search module to avoid over\nsearching. The comprehensive results on three datasets demonstrate the\nsuperiority of our model with significant improvements against baselines in\nboth short and long distance reasoning scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hongru Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Ning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenglu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"USCL: Pretraining Deep Ultrasound Image Diagnosis Model through Video Contrastive Representation Learning. (arXiv:2011.13066v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.13066","description":"<p>Most deep neural networks (DNNs) based ultrasound (US) medical image analysis\nmodels use pretrained backbones (e.g., ImageNet) for better model\ngeneralization. However, the domain gap between natural and medical images\ncauses an inevitable performance bottleneck. To alleviate this problem, an US\ndataset named US-4 is constructed for direct pretraining on the same domain. It\ncontains over 23,000 images from four US video sub-datasets. To learn robust\nfeatures from US-4, we propose an US semi-supervised contrastive learning\nmethod, named USCL, for pretraining. In order to avoid high similarities\nbetween negative pairs as well as mine abundant visual features from limited US\nvideos, USCL adopts a sample pair generation method to enrich the feature\ninvolved in a single step of contrastive optimization. Extensive experiments on\nseveral downstream tasks show the superiority of USCL pretraining against\nImageNet pretraining and other state-of-the-art (SOTA) pretraining approaches.\nIn particular, USCL pretrained backbone achieves fine-tuning accuracy of over\n94% on POCUS dataset, which is 10% higher than 84% of the ImageNet pretrained\nmodel. The source codes of this work are available at\nhttps://github.<a href=\"/abs/com/9836328\">com/9836328</a>47/USCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixiong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Cheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Changfeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yongfang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factorizing Perception and Policy for Interactive Instruction Following. (arXiv:2012.03208v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2012.03208","description":"<p>Performing simple household tasks based on language directives is very\nnatural to humans, yet it remains an open challenge for AI agents. The\n'interactive instruction following' task attempts to make progress towards\nbuilding agents that jointly navigate, interact, and reason in the environment\nat every step. To address the multifaceted problem, we propose a model that\nfactorizes the task into interactive perception and action policy streams with\nenhanced components and name it as MOCA, a Modular Object-Centric Approach. We\nempirically validate that MOCA outperforms prior arts by significant margins on\nthe ALFRED benchmark with improved generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kunal Pratap Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhambri_S/0/1/0/all/0/1\">Suvaansh Bhambri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byeonghwi Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jonghyun Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Real-World Adversarial Patches through 3D Modeling of Complex Target Scenes. (arXiv:2102.05334v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.05334","description":"<p>Adversarial examples have proven to be a concerning threat to deep learning\nmodels, particularly in the image domain. However, while many studies have\nexamined adversarial examples in the real world, most of them relied on 2D\nphotos of the attack scene. As a result, the attacks proposed may have limited\neffectiveness when implemented in realistic environments with 3D objects or\nvaried conditions. There are few studies on adversarial learning that use 3D\nobjects, and in many cases, other researchers are unable to replicate the\nreal-world evaluation process. In this study, we present a framework that uses\n3D modeling to craft adversarial patches for an existing real-world scene. Our\napproach uses a 3D digital approximation of the scene as a simulation of the\nreal world. With the ability to add and manipulate any element in the digital\nscene, our framework enables the attacker to improve the adversarial patch's\nimpact in real-world settings. We use the framework to create a patch for an\neveryday scene and evaluate its performance using a novel evaluation process\nthat ensures that our results are reproducible in both the digital space and\nthe real world. Our evaluation results show that the framework can generate\nadversarial patches that are robust to different settings in the real world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mathov_Y/0/1/0/all/0/1\">Yael Mathov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rokach_L/0/1/0/all/0/1\">Lior Rokach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1\">Yuval Elovici</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physical Reasoning Using Dynamics-Aware Models. (arXiv:2102.10336v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2102.10336","description":"<p>A common approach to solving physical reasoning tasks is to train a value\nlearner on example tasks. A limitation of such an approach is that it requires\nlearning about object dynamics solely from reward values assigned to the final\nstate of a rollout of the environment. This study aims to address this\nlimitation by augmenting the reward value with self-supervised signals about\nobject dynamics. Specifically, we train the model to characterize the\nsimilarity of two environment rollouts, jointly with predicting the outcome of\nthe reasoning task. This similarity can be defined as a distance measure\nbetween the trajectory of objects in the two rollouts, or learned directly from\npixels using a contrastive formulation. Empirically, we find that this approach\nleads to substantial performance improvements on the PHYRE benchmark for\nphysical reasoning (Bakhtin et al., 2019), establishing a new state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_E/0/1/0/all/0/1\">Eltayeb Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakhtin_A/0/1/0/all/0/1\">Anton Bakhtin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maaten_L/0/1/0/all/0/1\">Laurens van der Maaten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1\">Rohit Girdhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CURE: Code-Aware Neural Machine Translation for Automatic Program Repair. (arXiv:2103.00073v4 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2103.00073","description":"<p>Automatic program repair (APR) is crucial to improve software reliability.\nRecently, neural machine translation (NMT) techniques have been used to fix\nsoftware bugs automatically. While promising, these approaches have two major\nlimitations. Their search space often does not contain the correct fix, and\ntheir search strategy ignores software knowledge such as strict code syntax.\nDue to these limitations, existing NMT-based techniques underperform the best\ntemplate-based approaches.\n</p>\n<p>We propose CURE, a new NMT-based APR technique with three major novelties.\nFirst, CURE pre-trains a programming language (PL) model on a large software\ncodebase to learn developer-like source code before the APR task. Second, CURE\ndesigns a new code-aware search strategy that finds more correct fixes by\nfocusing on compilable patches and patches that are close in length to the\nbuggy code. Finally, CURE uses a subword tokenization technique to generate a\nsmaller search space that contains more correct fixes.\n</p>\n<p>Our evaluation on two widely-used benchmarks shows that CURE correctly fixes\n57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR\ntechniques on both benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lutellier_T/0/1/0/all/0/1\">Thibaud Lutellier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Lin Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data-Centric Framework for Composable NLP Workflows. (arXiv:2103.01834v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.01834","description":"<p>Empirical natural language processing (NLP) systems in application domains\n(e.g., healthcare, finance, education) involve interoperation among multiple\ncomponents, ranging from data ingestion, human annotation, to text retrieval,\nanalysis, generation, and visualization. We establish a unified open-source\nframework to support fast development of such sophisticated NLP workflows in a\ncomposable manner. The framework introduces a uniform data representation to\nencode heterogeneous results by a wide range of NLP tasks. It offers a large\nrepository of processors for NLP tasks, visualization, and annotation, which\ncan be easily assembled with full interoperability under the unified\nrepresentation. The highly extensible framework allows plugging in custom\nprocessors from external off-the-shelf NLP and deep learning libraries. The\nwhole framework is delivered through two modularized yet integratable\nopen-source projects, namely Forte (for workflow infrastructure and NLP\nfunction processors) and Stave (for user interaction, visualization, and\nannotation).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guanxiong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukkittu_A/0/1/0/all/0/1\">Avinash Bukkittu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Mansi Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pengzhi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Atif Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhavi_S/0/1/0/all/0/1\">Swapnil Singhavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zecong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haoran Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"White Box Methods for Explanations of Convolutional Neural Networks in Image Classification Tasks. (arXiv:2104.02548v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02548","description":"<p>In recent years, deep learning has become prevalent to solve applications\nfrom multiple domains. Convolutional Neural Networks (CNNs) particularly have\ndemonstrated state of the art performance for the task of image classification.\nHowever, the decisions made by these networks are not transparent and cannot be\ndirectly interpreted by a human. Several approaches have been proposed to\nexplain to understand the reasoning behind a prediction made by a network. In\nthis paper, we propose a topology of grouping these methods based on their\nassumptions and implementations. We focus primarily on white box methods that\nleverage the information of the internal architecture of a network to explain\nits decision. Given the task of image classification and a trained CNN, this\nwork aims to provide a comprehensive and detailed overview of a set of methods\nthat can be used to create explanation maps for a particular image, that assign\nan importance score to each pixel of the image based on its contribution to the\ndecision of the network. We also propose a further classification of the white\nbox methods based on their implementations to enable better comparisons and\nhelp researchers find methods best suited for different scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayyar_M/0/1/0/all/0/1\">Meghna P Ayyar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benois_Pineau_J/0/1/0/all/0/1\">Jenny Benois-Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemmari_A/0/1/0/all/0/1\">Akka Zemmari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Autodidactic Universe. (arXiv:2104.03902v2 [hep-th] UPDATED)","link":"http://arxiv.org/abs/2104.03902","description":"<p>We present an approach to cosmology in which the Universe learns its own\nphysical laws. It does so by exploring a landscape of possible laws, which we\nexpress as a certain class of matrix models. We discover maps that put each of\nthese matrix models in correspondence with both a gauge/gravity theory and a\nmathematical model of a learning machine, such as a deep recurrent, cyclic\nneural network. This establishes a correspondence between each solution of the\nphysical theory and a run of a neural network. This correspondence is not an\nequivalence, partly because gauge theories emerge from $N \\rightarrow \\infty $\nlimits of the matrix models, whereas the same limits of the neural networks\nused here are not well-defined. We discuss in detail what it means to say that\nlearning takes place in autodidactic systems, where there is no supervision. We\npropose that if the neural network model can be said to learn without\nsupervision, the same can be said for the corresponding physical theory. We\nconsider other protocols for autodidactic physical systems, such as\noptimization of graph variety, subset-replication using self-attention and\nlook-ahead, geometrogenesis guided by reinforcement learning, structural\nlearning using renormalization group techniques, and extensions. These\nprotocols together provide a number of directions in which to explore the\norigin of physical laws based on putting machine learning architectures in\ncorrespondence with physical theories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/hep-th/1/au:+Alexander_S/0/1/0/all/0/1\">Stephon Alexander</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Cunningham_W/0/1/0/all/0/1\">William J. Cunningham</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Lanier_J/0/1/0/all/0/1\">Jaron Lanier</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Smolin_L/0/1/0/all/0/1\">Lee Smolin</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Stanojevic_S/0/1/0/all/0/1\">Stefan Stanojevic</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Toomey_M/0/1/0/all/0/1\">Michael W. Toomey</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Wecker_D/0/1/0/all/0/1\">Dave Wecker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Commonsense Explanation in Dialogue Response Generation. (arXiv:2104.09574v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.09574","description":"<p>Humans use commonsense reasoning (CSR) implicitly to produce natural and\ncoherent responses in conversations. Aiming to close the gap between current\nresponse generation (RG) models and human communication abilities, we want to\nunderstand why RG models respond as they do by probing RG model's understanding\nof commonsense reasoning that elicits proper responses. We formalize the\nproblem by framing commonsense as a latent variable in the RG task and using\nexplanations for responses as textual form of commonsense. We collect 6k\nannotated explanations justifying responses from four dialogue datasets and ask\nhumans to verify them and propose two probing settings to evaluate RG models'\nCSR capabilities. Probing results show that models fail to capture the logical\nrelations between commonsense explanations and responses and fine-tuning on\nin-domain data and increasing model sizes do not lead to understanding of CSR\nfor RG. We hope our study motivates more research in making RG models emulate\nthe human reasoning process in pursuit of smooth human-AI communication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jandaghi_P/0/1/0/all/0/1\">Pegah Jandaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Justin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models. (arXiv:2105.00827v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.00827","description":"<p>Transformer-based pretrained language models (PLMs) have started a new era in\nmodern natural language processing (NLP). These models combine the power of\ntransformers, transfer learning, and self-supervised learning (SSL). Following\nthe success of these models in the general domain, the biomedical research\ncommunity has developed various in-domain PLMs starting from BioBERT to the\nlatest BioELECTRA and BioALBERT models. We strongly believe there is a need for\na survey paper that can provide a comprehensive survey of various\ntransformer-based biomedical pretrained language models (BPLMs). In this\nsurvey, we start with a brief overview of foundational concepts like\nself-supervised learning, embedding layer and transformer encoder layers. We\ndiscuss core concepts of transformer-based PLMs like pretraining methods,\npretraining tasks, fine-tuning methods, and various embedding types specific to\nbiomedical domain. We introduce a taxonomy for transformer-based BPLMs and then\ndiscuss all the models. We discuss various challenges and present possible\nsolutions. We conclude by highlighting some of the open issues which will drive\nthe research community to further improve transformer-based BPLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_K/0/1/0/all/0/1\">Katikapalli Subramanyam Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasekharan_A/0/1/0/all/0/1\">Ajit Rajasekharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangeetha_S/0/1/0/all/0/1\">Sivanesan Sangeetha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Software Engineering for AI-Based Systems: A Survey. (arXiv:2105.01984v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2105.01984","description":"<p>AI-based systems are software systems with functionalities enabled by at\nleast one AI component (e.g., for image- and speech-recognition, and autonomous\ndriving). AI-based systems are becoming pervasive in society due to advances in\nAI. However, there is limited synthesized knowledge on Software Engineering\n(SE) approaches for building, operating, and maintaining AI-based systems. To\ncollect and analyze state-of-the-art knowledge about SE for AI-based systems,\nwe conducted a systematic mapping study. We considered 248 studies published\nbetween January 2010 and March 2020. SE for AI-based systems is an emerging\nresearch area, where more than 2/3 of the studies have been published since\n2018. The most studied properties of AI-based systems are dependability and\nsafety. We identified multiple SE approaches for AI-based systems, which we\nclassified according to the SWEBOK areas. Studies related to software testing\nand software quality are very prevalent, while areas like software maintenance\nseem neglected. Data-related issues are the most recurrent challenges. Our\nresults are valuable for: researchers, to quickly understand the state of the\nart and learn which topics need more research; practitioners, to learn about\nthe approaches and challenges that SE entails for AI-based systems; and,\neducators, to bridge the gap among SE and AI in their curricula.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Fernandez_S/0/1/0/all/0/1\">Silverio Mart&#xed;nez-Fern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogner_J/0/1/0/all/0/1\">Justus Bogner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franch_X/0/1/0/all/0/1\">Xavier Franch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oriol_M/0/1/0/all/0/1\">Marc Oriol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siebert_J/0/1/0/all/0/1\">Julien Siebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trendowicz_A/0/1/0/all/0/1\">Adam Trendowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vollmer_A/0/1/0/all/0/1\">Anna Maria Vollmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_S/0/1/0/all/0/1\">Stefan Wagner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmenter: Transformer for Semantic Segmentation. (arXiv:2105.05633v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.05633","description":"<p>Image segmentation is often ambiguous at the level of individual image\npatches and requires contextual information to reach label consensus. In this\npaper we introduce Segmenter, a transformer model for semantic segmentation. In\ncontrast to convolution-based methods, our approach allows to model global\ncontext already at the first layer and throughout the network. We build on the\nrecent Vision Transformer (ViT) and extend it to semantic segmentation. To do\nso, we rely on the output embeddings corresponding to image patches and obtain\nclass labels from these embeddings with a point-wise linear decoder or a mask\ntransformer decoder. We leverage models pre-trained for image classification\nand show that we can fine-tune them on moderate sized datasets available for\nsemantic segmentation. The linear decoder allows to obtain excellent results\nalready, but the performance can be further improved by a mask transformer\ngenerating class masks. We conduct an extensive ablation study to show the\nimpact of the different parameters, in particular the performance is better for\nlarge models and small patch sizes. Segmenter attains excellent results for\nsemantic segmentation. It outperforms the state of the art on both ADE20K and\nPascal Context datasets and is competitive on Cityscapes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Strudel_R/0/1/0/all/0/1\">Robin Strudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_R/0/1/0/all/0/1\">Ricardo Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding peacefulness through the world news. (arXiv:2106.00306v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2106.00306","description":"<p>Peacefulness is a principal dimension of well-being for all humankind and is\nthe way out of inequity and every single form of violence. Thus, its\nmeasurement has lately drawn the attention of researchers and policy-makers.\nDuring the last years, novel digital data streams have drastically changed the\nresearch in this field. In the current study, we exploit information extracted\nfrom Global Data on Events, Location, and Tone (GDELT) digital news database,\nto capture peacefulness through the Global Peace Index (GPI). Applying\npredictive machine learning models, we demonstrate that news media attention\nfrom GDELT can be used as a proxy for measuring GPI at a monthly level.\nAdditionally, we use the SHAP methodology to obtain the most important\nvariables that drive the predictions. This analysis highlights each country's\nprofile and provides explanations for the predictions overall, and particularly\nfor the errors and the events that drive these errors. We believe that digital\ndata exploited by Social Good researchers, policy-makers, and peace-builders,\nwith data science tools as powerful as machine learning, could contribute to\nmaximize the societal benefits and minimize the risks to peacefulness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voukelatou_V/0/1/0/all/0/1\">Vasiliki Voukelatou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miliou_I/0/1/0/all/0/1\">Ioanna Miliou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannotti_F/0/1/0/all/0/1\">Fosca Giannotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappalardo_L/0/1/0/all/0/1\">Luca Pappalardo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PonderNet: Learning to Ponder. (arXiv:2107.05407v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.05407","description":"<p>In standard neural networks the amount of computation used grows with the\nsize of the inputs, but not with the complexity of the problem being learnt. To\novercome this limitation we introduce PonderNet, a new algorithm that learns to\nadapt the amount of computation based on the complexity of the problem at hand.\nPonderNet learns end-to-end the number of computational steps to achieve an\neffective compromise between training prediction accuracy, computational cost\nand generalization. On a complex synthetic problem, PonderNet dramatically\nimproves performance over previous adaptive computation methods and\nadditionally succeeds at extrapolation tests where traditional neural networks\nfail. Also, our method matched the current state of the art results on a real\nworld question and answering dataset, but using less compute. Finally,\nPonderNet reached state of the art results on a complex task designed to test\nthe reasoning capabilities of neural networks.1\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banino_A/0/1/0/all/0/1\">Andrea Banino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaguer_J/0/1/0/all/0/1\">Jan Balaguer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1\">Charles Blundell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Answer-Set Programs for Reasoning about Counterfactual Interventions and Responsibility Scores for Classification. (arXiv:2107.10159v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2107.10159","description":"<p>We describe how answer-set programs can be used to declaratively specify\ncounterfactual interventions on entities under classification, and reason about\nthem. In particular, they can be used to define and compute responsibility\nscores as attribution-based explanations for outcomes from classification\nmodels. The approach allows for the inclusion of domain knowledge and supports\nquery answering. A detailed example with a naive-Bayes classifier is presented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bertossi_L/0/1/0/all/0/1\">Leopoldo Bertossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_G/0/1/0/all/0/1\">Gabriela Reyes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leaf Recognition Using Convolutional Neural Networks Based Features. (arXiv:2108.01808v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01808","description":"<p>There is a warning light for the loss of plant habitats worldwide that\nentails concerted efforts to conserve plant biodiversity. Thus, plant species\nclassification is of crucial importance to address this environmental\nchallenge. In recent years, there is a considerable increase in the number of\nstudies related to plant taxonomy. While some researchers try to improve their\nrecognition performance using novel approaches, others concentrate on\ncomputational optimization for their framework. In addition, a few studies are\ndiving into feature extraction to gain significantly in terms of accuracy. In\nthis paper, we propose an effective method for the leaf recognition problem. In\nour proposed approach, a leaf goes through some pre-processing to extract its\nrefined color image, vein image, xy-projection histogram, handcrafted shape,\ntexture features, and Fourier descriptors. These attributes are then\ntransformed into a better representation by neural network-based encoders\nbefore a support vector machine (SVM) model is utilized to classify different\nleaves. Overall, our approach performs a state-of-the-art result on the Flavia\nleaf dataset, achieving the accuracy of 99.58\\% on test sets under random\n10-fold cross-validation and bypassing the previous methods. We also release\nour codes (Scripts are available at\nhttps://github.com/dinhvietcuong1996/LeafRecognition) for contributing to the\nresearch community in the leaf classification problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quach_B/0/1/0/all/0/1\">Boi M. Quach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuong_D/0/1/0/all/0/1\">Dinh V. Cuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1\">Nhung Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_D/0/1/0/all/0/1\">Dang Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh T. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imperceptible Adversarial Examples by Spatial Chroma-Shift. (arXiv:2108.02502v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02502","description":"<p>Deep Neural Networks have been shown to be vulnerable to various kinds of\nadversarial perturbations. In addition to widely studied additive noise based\nperturbations, adversarial examples can also be created by applying a per pixel\nspatial drift on input images. While spatial transformation based adversarial\nexamples look more natural to human observers due to absence of additive noise,\nthey still possess visible distortions caused by spatial transformations. Since\nthe human vision is more sensitive to the distortions in the luminance compared\nto those in chrominance channels, which is one of the main ideas behind the\nlossy visual multimedia compression standards, we propose a spatial\ntransformation based perturbation method to create adversarial examples by only\nmodifying the color components of an input image. While having competitive\nfooling rates on CIFAR-10 and NIPS2017 Adversarial Learning Challenge datasets,\nexamples created with the proposed method have better scores with regards to\nvarious perceptual quality metrics. Human visual perception studies validate\nthat the examples are more natural looking and often indistinguishable from\ntheir original counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aydin_A/0/1/0/all/0/1\">Ayberk Aydin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_D/0/1/0/all/0/1\">Deniz Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karli_B/0/1/0/all/0/1\">Berat Tuna Karli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanoglu_O/0/1/0/all/0/1\">Oguz Hanoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Temizel_A/0/1/0/all/0/1\">Alptekin Temizel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Chatbot Per Person: Creating Personalized Chatbots based on Implicit User Profiles. (arXiv:2108.09355v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09355","description":"<p>Personalized chatbots focus on endowing chatbots with a consistent\npersonality to behave like real users, give more informative responses, and\nfurther act as personal assistants. Existing personalized approaches tried to\nincorporate several text descriptions as explicit user profiles. However, the\nacquisition of such explicit profiles is expensive and time-consuming, thus\nbeing impractical for large-scale real-world applications. Moreover, the\nrestricted predefined profile neglects the language behavior of a real user and\ncannot be automatically updated together with the change of user interests. In\nthis paper, we propose to learn implicit user profiles automatically from\nlarge-scale user dialogue history for building personalized chatbots.\nSpecifically, leveraging the benefits of Transformer on language understanding,\nwe train a personalized language model to construct a general user profile from\nthe user's historical responses. To highlight the relevant historical responses\nto the input post, we further establish a key-value memory network of\nhistorical post-response pairs, and build a dynamic post-aware user profile.\nThe dynamic profile mainly describes what and how the user has responded to\nsimilar posts in history. To explicitly utilize users' frequently used words,\nwe design a personalized decoder to fuse two decoding strategies, including\ngenerating a word from the generic vocabulary and copying one word from the\nuser's personalized vocabulary. Experiments on two real-world datasets show the\nsignificant improvement of our model compared with existing methods. Our code\nis available at https://github.com/zhengyima/DHAP\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengyi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Hanxun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Accuracy of Permutation DAG Search using Best Order Score Search. (arXiv:2108.10141v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2108.10141","description":"<p>The Sparsest Permutation (SP) algorithm is accurate but limited to about 9\nvariables in practice; the Greedy Sparest Permutation (GSP) algorithm is faster\nbut less weak theoretically. A compromise can be given, the Best Order Score\nSearch, which gives results as accurate as SP but for much larger and denser\ngraphs. BOSS (Best Order Score Search) is more accurate for two reason: (a) It\nassumes the \"brute faithfuness\" assumption, which is weaker than faithfulness,\nand (b) it uses a different traversal of permutations than the depth first\ntraversal used by GSP, obtained by taking each variable in turn and moving it\nto the position in the permutation that optimizes the model score. Results are\ngiven comparing BOSS to several related papers in the literature in terms of\nperformance, for linear, Gaussian data. In all cases, with the proper parameter\nsettings, accuracy of BOSS is lifted considerably with respect to competing\napproaches. In configurations tested, models with 60 variables are feasible\nwith large samples out to about an average degree of 12 in reasonable time,\nwith near-perfect accuracy, and sparse models with an average degree of 4 are\nfeasible out to about 300 variables on a laptop, again with near-perfect\naccuracy. Mixed continuous discrete and all-discrete datasets were also tested.\nThe mixed data analysis showed advantage for BOSS over GES more apparent at\nhigher depths with the same score; the discrete data analysis showed a very\nsmall advantage for BOSS over GES with the same score, perhaps not enough to\nprefer it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramsey_J/0/1/0/all/0/1\">Joseph D. Ramsey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smoothing Dialogue States for Open Conversational Machine Reading. (arXiv:2108.12599v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12599","description":"<p>Conversational machine reading (CMR) requires machines to communicate with\nhumans through multi-turn interactions between two salient dialogue states of\ndecision making and question generation processes. In open CMR settings, as the\nmore realistic scenario, the retrieved background knowledge would be noisy,\nwhich results in severe challenges in the information transmission. Existing\nstudies commonly train independent or pipeline systems for the two subtasks.\nHowever, those methods are trivial by using hard-label decisions to activate\nquestion generation, which eventually hinders the model performance. In this\nwork, we propose an effective gating strategy by smoothing the two dialogue\nstates in only one decoder and bridge decision making and question generation\nto provide a richer dialogue state reference. Experiments on the OR-ShARC\ndataset show the effectiveness of our method, which achieves new\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siru Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utiyama_M/0/1/0/all/0/1\">Masao Utiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumita_E/0/1/0/all/0/1\">Eiichiro Sumita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proceedings of KDD 2021 Workshop on Data-driven Humanitarian Mapping: Harnessing Human-Machine Intelligence for High-Stake Public Policy and Resilience Planning. (arXiv:2109.00100v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2109.00100","description":"<p>Humanitarian challenges, including natural disasters, food insecurity,\nclimate change, racial and gender violence, environmental crises, the COVID-19\ncoronavirus pandemic, human rights violations, and forced displacements,\ndisproportionately impact vulnerable communities worldwide. According to UN\nOCHA, 235 million people will require humanitarian assistance in 20211 .\nDespite these growing perils, there remains a notable paucity of data science\nresearch to scientifically inform equitable public policy decisions for\nimproving the livelihood of at-risk populations. Scattered data science efforts\nexist to address these challenges, but they remain isolated from practice and\nprone to algorithmic harms concerning lack of privacy, fairness,\ninterpretability, accountability, transparency, and ethics. Biases in\ndata-driven methods carry the risk of amplifying inequalities in high-stakes\npolicy decisions that impact the livelihood of millions of people.\nConsequently, proclaimed benefits of data-driven innovations remain\ninaccessible to policymakers, practitioners, and marginalized communities at\nthe core of humanitarian actions and global development. To help fill this gap,\nwe propose the Data-driven Humanitarian Mapping Research Program, which focuses\non developing novel data science methodologies that harness human-machine\nintelligence for high-stakes public policy and resilience planning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Snehalkumar/0/1/0/all/0/1\">Snehalkumar</a> (Neil) <a href=\"http://arxiv.org/find/cs/1/au:+Gaikwad_S/0/1/0/all/0/1\">S. Gaikwad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_S/0/1/0/all/0/1\">Shankar Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lunga_D/0/1/0/all/0/1\">Dalton Lunga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bondi_E/0/1/0/all/0/1\">Elizabeth Bondi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Exploration Methods in Reinforcement Learning. (arXiv:2109.00157v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.00157","description":"<p>Exploration is an essential component of reinforcement learning algorithms,\nwhere agents need to learn how to predict and control unknown and often\nstochastic environments. Reinforcement learning agents depend crucially on\nexploration to obtain informative data for the learning process as the lack of\nenough information could hinder effective learning. In this article, we provide\na survey of modern exploration methods in (Sequential) reinforcement learning,\nas well as a taxonomy of exploration methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amin_S/0/1/0/all/0/1\">Susan Amin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomrokchi_M/0/1/0/all/0/1\">Maziar Gomrokchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satija_H/0/1/0/all/0/1\">Harsh Satija</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoof_H/0/1/0/all/0/1\">Herke van Hoof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proceedings of KDD 2020 Workshop on Data-driven Humanitarian Mapping: Harnessing Human-Machine Intelligence for High-Stake Public Policy and Resilience Planning. (arXiv:2109.00435v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2109.00435","description":"<p>Humanitarian challenges, including natural disasters, food insecurity,\nclimate change, racial and gender violence, environmental crises, the COVID-19\ncoronavirus pandemic, human rights violations, and forced displacements,\ndisproportionately impact vulnerable communities worldwide. According to UN\nOCHA, 235 million people will require humanitarian assistance in 20211 .\nDespite these growing perils, there remains a notable paucity of data science\nresearch to scientifically inform equitable public policy decisions for\nimproving the livelihood of at-risk populations. Scattered data science efforts\nexist to address these challenges, but they remain isolated from practice and\nprone to algorithmic harms concerning lack of privacy, fairness,\ninterpretability, accountability, transparency, and ethics. Biases in\ndata-driven methods carry the risk of amplifying inequalities in high-stakes\npolicy decisions that impact the livelihood of millions of people.\nConsequently, proclaimed benefits of data-driven innovations remain\ninaccessible to policymakers, practitioners, and marginalized communities at\nthe core of humanitarian actions and global development. To help fill this gap,\nwe propose the Data-driven Humanitarian Mapping Research Program, which focuses\non developing novel data science methodologies that harness human-machine\nintelligence for high-stakes public policy and resilience planning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Snehalkumar/0/1/0/all/0/1\">Snehalkumar</a> (Neil) <a href=\"http://arxiv.org/find/cs/1/au:+Gaikwad_S/0/1/0/all/0/1\">S. Gaikwad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_S/0/1/0/all/0/1\">Shankar Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lunga_D/0/1/0/all/0/1\">Dalton Lunga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu-Ru Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Artificial Intelligence"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Don't Discard All the Biased Instances: Investigating a Core Assumption in Dataset Bias Mitigation Techniques. (arXiv:2109.00521v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00521","description":"<p>Existing techniques for mitigating dataset bias often leverage a biased model\nto identify biased instances. The role of these biased instances is then\nreduced during the training of the main model to enhance its robustness to\nout-of-distribution data. A common core assumption of these techniques is that\nthe main model handles biased instances similarly to the biased model, in that\nit will resort to biases whenever available. In this paper, we show that this\nassumption does not hold in general. We carry out a critical investigation on\ntwo well-known datasets in the domain, MNLI and FEVER, along with two biased\ninstance detection methods, partial-input and limited-capacity models. Our\nexperiments show that in around a third to a half of instances, the biased\nmodel is unable to predict the main model's behavior, highlighted by the\nsignificantly different parts of the input on which they base their decisions.\nBased on a manual validation, we also show that this estimate is highly in line\nwith human interpretation. Our findings suggest that down-weighting of\ninstances detected by bias detection methods, which is a widely-practiced\nprocedure, is an unnecessary waste of training data. We release our code to\nfacilitate reproducibility and future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amirkhani_H/0/1/0/all/0/1\">Hossein Amirkhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text AutoAugment: Learning Compositional Augmentation Policy for Text Classification. (arXiv:2109.00523v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00523","description":"<p>Data augmentation aims to enrich training samples for alleviating the\noverfitting issue in low-resource or class-imbalanced situations. Traditional\nmethods first devise task-specific operations such as Synonym Substitute, then\npreset the corresponding parameters such as the substitution rate artificially,\nwhich require a lot of prior knowledge and are prone to fall into the\nsub-optimum. Besides, the number of editing operations is limited in the\nprevious methods, which decreases the diversity of the augmented data and thus\nrestricts the performance gain. To overcome the above limitations, we propose a\nframework named Text AutoAugment (TAA) to establish a compositional and\nlearnable paradigm for data augmentation. We regard a combination of various\noperations as an augmentation policy and utilize an efficient Bayesian\nOptimization algorithm to automatically search for the best policy, which\nsubstantially improves the generalization capability of models. Experiments on\nsix benchmark datasets show that TAA boosts classification accuracy in\nlow-resource and class-imbalanced regimes by an average of 8.8% and 9.7%,\nrespectively, outperforming strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuhuai Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Search Engines with Interactive Agents. (arXiv:2109.00527v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00527","description":"<p>Can machines learn to use a search engine as an interactive tool for finding\ninformation? That would have far reaching consequences for making the world's\nknowledge more accessible. This paper presents first steps in designing agents\nthat learn meta-strategies for contextual query refinements. Our approach uses\nmachine reading to guide the selection of refinement terms from aggregated\nsearch results. Agents are then empowered with simple but effective search\noperators to exert fine-grained and transparent control over queries and search\nresults. We develop a novel way of generating synthetic search sessions, which\nleverages the power of transformer-based generative language models through\n(self-)supervised learning. We also present a reinforcement learning agent with\ndynamically constrained actions that can learn interactive search strategies\ncompletely from scratch. In both cases, we obtain significant improvements over\none-shot search with a strong information retrieval baseline. Finally, we\nprovide an in-depth analysis of the learned search policies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adolphs_L/0/1/0/all/0/1\">Leonard Adolphs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boerschinger_B/0/1/0/all/0/1\">Benjamin Boerschinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buck_C/0/1/0/all/0/1\">Christian Buck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huebscher_M/0/1/0/all/0/1\">Michelle Chen Huebscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciaramita_M/0/1/0/all/0/1\">Massimiliano Ciaramita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espeholt_L/0/1/0/all/0/1\">Lasse Espeholt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1\">Thomas Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilcher_Y/0/1/0/all/0/1\">Yannic Kilcher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Improving Adversarial Training of NLP Models. (arXiv:2109.00544v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00544","description":"<p>Adversarial training, a method for learning robust deep neural networks,\nconstructs adversarial examples during training. However, recent methods for\ngenerating NLP adversarial examples involve combinatorial search and expensive\nsentence encoders for constraining the generated instances. As a result, it\nremains challenging to use vanilla adversarial training to improve NLP models'\nperformance, and the benefits are mainly uninvestigated. This paper proposes a\nsimple and improved vanilla adversarial training process for NLP, which we name\nAttacking to Training ($\\texttt{A2T}$). The core part of $\\texttt{A2T}$ is a\nnew and cheaper word substitution attack optimized for vanilla adversarial\ntraining. We use $\\texttt{A2T}$ to train BERT and RoBERTa models on IMDB,\nRotten Tomatoes, Yelp, and SNLI datasets. Our results show that it is possible\nto train empirically robust NLP models using a much cheaper adversary. We\ndemonstrate that vanilla adversarial training with $\\texttt{A2T}$ can improve\nan NLP model's robustness to the attack it was originally trained with and also\ndefend the model against other types of attacks. Furthermore, we show that\n$\\texttt{A2T}$ can improve NLP models' standard accuracy, cross-domain\ngeneralization, and interpretability. Code is available at\n<a href=\"http://github.com/jinyongyoo/A2T\">this http URL</a> .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jin Yong Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Knowledge Help General NLU? An Empirical Study. (arXiv:2109.00563v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00563","description":"<p>It is often observed in knowledge-centric tasks (e.g., common sense question\nand answering, relation classification) that the integration of external\nknowledge such as entity representation into language models can help provide\nuseful information to boost the performance. However, it is still unclear\nwhether this benefit can extend to general natural language understanding (NLU)\ntasks. In this work, we empirically investigated the contribution of external\nknowledge by measuring the end-to-end performance of language models with\nvarious knowledge integration methods. We find that the introduction of\nknowledge can significantly improve the results on certain tasks while having\nno adverse effects on other tasks. We then employ mutual information to reflect\nthe difference brought by knowledge and a neural interpretation model to reveal\nhow a language model utilizes external knowledge. Our study provides valuable\ninsights and guidance for practitioners to equip NLP models with knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DILBERT: Customized Pre-Training for Domain Adaptation withCategory Shift, with an Application to Aspect Extraction. (arXiv:2109.00571v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00571","description":"<p>The rise of pre-trained language models has yielded substantial progress in\nthe vast majority of Natural Language Processing (NLP) tasks. However, a\ngeneric approach towards the pre-training procedure can naturally be\nsub-optimal in some cases. Particularly, fine-tuning a pre-trained language\nmodel on a source domain and then applying it to a different target domain,\nresults in a sharp performance decline of the eventual classifier for many\nsource-target domain pairs. Moreover, in some NLP tasks, the output categories\nsubstantially differ between domains, making adaptation even more challenging.\nThis, for example, happens in the task of aspect extraction, where the aspects\nof interest of reviews of, e.g., restaurants or electronic devices may be very\ndifferent. This paper presents a new fine-tuning scheme for BERT, which aims to\naddress the above challenges. We name this scheme DILBERT: Domain Invariant\nLearning with BERT, and customize it for aspect extraction in the unsupervised\ndomain adaptation setting. DILBERT harnesses the categorical information of\nboth the source and the target domains to guide the pre-training process\ntowards a more domain and category invariant representation, thus closing the\ngap between the domains. We show that DILBERT yields substantial improvements\nover state-of-the-art baselines while using a fraction of the unlabeled data,\nparticularly in more challenging domain adaptation setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lekhtman_E/0/1/0/all/0/1\">Entony Lekhtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziser_Y/0/1/0/all/0/1\">Yftah Ziser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00590","description":"<p>Web search is fundamentally multimodal and multihop. Often, even before\nasking a question we choose to go directly to image search to find our answers.\nFurther, rarely do we find an answer from a single source but aggregate\ninformation and reason through implications. Despite the frequency of this\neveryday occurrence, at present, there is no unified question answering\nbenchmark that requires a single model to answer long-form natural language\nquestions from text and open-ended visual sources -- akin to a human's\nexperience. We propose to bridge this gap between the natural language and\ncomputer vision communities with WebQA. We show that A. our multihop text\nqueries are difficult for a large-scale transformer model, and B. existing\nmulti-modal transformers and visual representations do not perform well on\nopen-domain visual queries. Our challenge for the community is to create a\nunified multimodal reasoning model that seamlessly transitions and reasons\nregardless of the source modality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yingshan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_M/0/1/0/all/0/1\">Mridu Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_H/0/1/0/all/0/1\">Hisami Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Guihong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fight Fire with Fire: Fine-tuning Hate Detectors using Large Samples of Generated Hate Speech. (arXiv:2109.00591v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00591","description":"<p>Automatic hate speech detection is hampered by the scarcity of labeled\ndatasetd, leading to poor generalization. We employ pretrained language models\n(LMs) to alleviate this data bottleneck. We utilize the GPT LM for generating\nlarge amounts of synthetic hate speech sequences from available labeled\nexamples, and leverage the generated data in fine-tuning large pretrained LMs\non hate detection. An empirical study using the models of BERT, RoBERTa and\nALBERT, shows that this approach improves generalization significantly and\nconsistently within and across data distributions. In fact, we find that\ngenerating relevant labeled hate speech sequences is preferable to using\nout-of-domain, and sometimes also within-domain, human-labeled examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wullach_T/0/1/0/all/0/1\">Tomer Wullach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adler_A/0/1/0/all/0/1\">Amir Adler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minkov_E/0/1/0/all/0/1\">Einat Minkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latin writing styles analysis with Machine Learning: New approach to old questions. (arXiv:2109.00601v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00601","description":"<p>In the Middle Ages texts were learned by heart and spread using oral means of\ncommunication from generation to generation. Adaptation of the art of prose and\npoems allowed keeping particular descriptions and compositions characteristic\nfor many literary genres. Taking into account such a specific construction of\nliterature composed in Latin, we can search for and indicate the probability\npatterns of familiar sources of specific narrative texts. Consideration of\nNatural Language Processing tools allowed us the transformation of textual\nobjects into numerical ones and then application of machine learning algorithms\nto extract information from the dataset. We carried out the task consisting of\nthe practical use of those concepts and observation to create a tool for\nanalyzing narrative texts basing on open-source databases. The tool focused on\ncreating specific search tools resources which could enable us detailed\nsearching throughout the text. The main objectives of the study take into\naccount finding similarities between sentences and between documents. Next, we\napplied machine learning algorithms on chosen texts to calculate specific\nfeatures of them (for instance authorship or centuries) and to recognize\nsources of anonymous texts with a certain percentage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bernardo_A/0/1/0/all/0/1\">Arianna Di Bernardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poetto_S/0/1/0/all/0/1\">Simone Poetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sillano_P/0/1/0/all/0/1\">Pietro Sillano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villata_B/0/1/0/all/0/1\">Beatrice Villata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_W/0/1/0/all/0/1\">Weronika S&#xf3;jka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietka_Danilewicz_Z/0/1/0/all/0/1\">Zofia Pi&#x119;tka-Danilewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pranke_P/0/1/0/all/0/1\">Piotr Pranke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point-of-Interest Type Prediction using Text and Images. (arXiv:2109.00602v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00602","description":"<p>Point-of-interest (POI) type prediction is the task of inferring the type of\na place from where a social media post was shared. Inferring a POI's type is\nuseful for studies in computational social science including sociolinguistics,\ngeosemiotics, and cultural geography, and has applications in geosocial\nnetworking technologies such as recommendation and visualization systems. Prior\nefforts in POI type prediction focus solely on text, without taking visual\ninformation into account. However in reality, the variety of modalities, as\nwell as their semiotic relationships with one another, shape communication and\ninteractions in social media. This paper presents a study on POI type\nprediction using multimodal information from text and images available at\nposting time. For that purpose, we enrich a currently available data set for\nPOI type prediction with the images that accompany the text messages. Our\nproposed method extracts relevant information from each modality to effectively\ncapture interactions between text and image achieving a macro F1 of 47.21\nacross eight categories significantly outperforming the state-of-the-art method\nfor POI type prediction based on text-only methods. Finally, we provide a\ndetailed analysis to shed light on cross-modal interactions and the limitations\nof our best performing model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Villegas_D/0/1/0/all/0/1\">Danae S&#xe1;nchez Villegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An unsupervised framework for tracing textual sources of moral change. (arXiv:2109.00608v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00608","description":"<p>Morality plays an important role in social well-being, but people's moral\nperception is not stable and changes over time. Recent advances in natural\nlanguage processing have shown that text is an effective medium for informing\nmoral change, but no attempt has been made to quantify the origins of these\nchanges. We present a novel unsupervised framework for tracing textual sources\nof moral change toward entities through time. We characterize moral change with\nprobabilistic topical distributions and infer the source text that exerts\nprominent influence on the moral time course. We evaluate our framework on a\ndiverse set of data ranging from social media to news articles. We show that\nour framework not only captures fine-grained human moral judgments, but also\nidentifies coherent source topics of moral change triggered by historical\nevents. We apply our methodology to analyze the news in the COVID-19 pandemic\nand demonstrate its utility in identifying sources of moral change in\nhigh-impact and real-time social events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_A/0/1/0/all/0/1\">Aida Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zining Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1\">Frank Rudzicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Algorithme de recherche approximative dans un dictionnaire fond\\'e sur une distance d'\\'edition d\\'efinie par blocs. (arXiv:2109.00624v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00624","description":"<p>We propose an algorithm for approximative dictionary lookup, where altered\nstrings are matched against reference forms. The algorithm makes use of a\ndivergence function between strings -- broadly belonging to the family of edit\ndistances; it finds dictionary entries whose distance to the search string is\nbelow a certain threshold. The divergence function is not the classical edit\ndistance (DL distance); it is adaptable to a particular corpus, and is based on\nelementary alteration costs defined on character blocks, rather than on\nindividual characters.\n</p>\n<p>Nous proposons un algorithme de recherche approximative de cha\\^ines dans un\ndictionnaire \\`a partir de formes alt\\'er\\'ees. Cet algorithme est fond\\'e sur\nune fonction de divergence entre cha\\^ines~ -- une sorte de distance\nd'\\'edition: il recherche des entr\\'ees pour lesquelles la distance \\`a la\ncha\\^ine cherch\\'ee est inf\\'erieure \\`a un certain seuil. La fonction\nutilis\\'ee n'est pas la distance d'\\'edition classique (distance DL); elle est\nadapt\\'ee \\`a un corpus, et se fonde sur la prise en compte de co\\^uts\nd'alt\\'eration \\'el\\'ementaires d\\'efinis non pas sur des caract\\`eres, mais\nsur des sous-cha\\^ines (des blocs de caract\\`eres).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaillant_P/0/1/0/all/0/1\">Pascal Vaillant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tree-constrained Pointer Generator for End-to-end Contextual Speech Recognition. (arXiv:2109.00627v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00627","description":"<p>Contextual knowledge is important for real-world automatic speech recognition\n(ASR) applications. In this paper, a novel tree-constrained pointer generator\n(TCPGen) component is proposed that incorporates such knowledge as a list of\nbiasing words into both attention-based encoder-decoder and transducer\nend-to-end ASR models in a neural-symbolic way. TCPGen structures the biasing\nwords into an efficient prefix tree to serve as its symbolic input and creates\na neural shortcut between the tree and the final ASR output distribution to\nfacilitate recognising biasing words during decoding. Systems were trained and\nevaluated on the Librispeech corpus where biasing words were extracted at the\nscales of an utterance, a chapter, or a book to simulate different application\nscenarios. Experimental results showed that TCPGen consistently improved word\nerror rates (WERs) compared to the baselines, and in particular, achieved\nsignificant WER reductions on the biasing words. TCPGen is highly efficient: it\ncan handle 5,000 biasing words and distractors and only add a small overhead to\nmemory use and computation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Ensemble Approach for Annotating Source Code Identifiers with Part-of-speech Tags. (arXiv:2109.00629v1 [cs.SE])","link":"http://arxiv.org/abs/2109.00629","description":"<p>This paper presents an ensemble part-of-speech tagging approach for source\ncode identifiers. Ensemble tagging is a technique that uses machine-learning\nand the output from multiple part-of-speech taggers to annotate natural\nlanguage text at a higher quality than the part-of-speech taggers are able to\nobtain independently. Our ensemble uses three state-of-the-art part-of-speech\ntaggers: SWUM, POSSE, and Stanford. We study the quality of the ensemble's\nannotations on five different types of identifier names: function, class,\nattribute, parameter, and declaration statement at the level of both individual\nwords and full identifier names. We also study and discuss the weaknesses of\nour tagger to promote the future amelioration of these problems through further\nresearch. Our results show that the ensemble achieves 75\\% accuracy at the\nidentifier level and 84-86\\% accuracy at the word level. This is an increase of\n+17\\% points at the identifier level from the closest independent\npart-of-speech tagger.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Newman_C/0/1/0/all/0/1\">Christian D. Newman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decker_M/0/1/0/all/0/1\">Michael J. Decker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AlSuhaibani_R/0/1/0/all/0/1\">Reem S. AlSuhaibani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peruma_A/0/1/0/all/0/1\">Anthony Peruma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohapatra_S/0/1/0/all/0/1\">Satyajit Mohapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishnoi_T/0/1/0/all/0/1\">Tejal Vishnoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mkaouer_M/0/1/0/all/0/1\">Mohamed W. Mkaouer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheldon_T/0/1/0/all/0/1\">Timothy J. Sheldon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_E/0/1/0/all/0/1\">Emily Hill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The VoicePrivacy 2020 Challenge: Results and findings. (arXiv:2109.00648v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00648","description":"<p>This paper presents the results and analyses stemming from the first\nVoicePrivacy 2020 Challenge which focuses on developing anonymization solutions\nfor speech technology. We provide a systematic overview of the challenge design\nwith an analysis of submitted systems and evaluation results. In particular, we\ndescribe the voice anonymization task and datasets used for system development\nand evaluation. Also, we present different attack models and the associated\nobjective and subjective evaluation metrics. We introduce two anonymization\nbaselines and provide a summary description of the anonymization systems\ndeveloped by the challenge participants. We report objective and subjective\nevaluation results for baseline and submitted systems. In addition, we present\nexperimental results for alternative privacy metrics and attack models\ndeveloped as a part of the post-evaluation analysis. Finally, we summarize our\ninsights and observations that will influence the design of the next\nVoicePrivacy challenge edition and some directions for future voice\nanonymization research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomashenko_N/0/1/0/all/0/1\">Natalia Tomashenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_E/0/1/0/all/0/1\">Emmanuel Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patino_J/0/1/0/all/0/1\">Jose Patino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1\">Brij Mohan Lal Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noe_P/0/1/0/all/0/1\">Paul-Gauthier No&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nautsch_A/0/1/0/all/0/1\">Andreas Nautsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_N/0/1/0/all/0/1\">Nicholas Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OBrien_B/0/1/0/all/0/1\">Benjamin O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanclu_A/0/1/0/all/0/1\">Ana&#xef;s Chanclu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonastre_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Bonastre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todisco_M/0/1/0/all/0/1\">Massimiliano Todisco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maouche_M/0/1/0/all/0/1\">Mohamed Maouche</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Making the Most of Dialogue Characteristics for Neural Chat Translation. (arXiv:2109.00668v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00668","description":"<p>Neural Chat Translation (NCT) aims to translate conversational text between\nspeakers of different languages. Despite the promising performance of\nsentence-level and context-aware neural machine translation models, there still\nremain limitations in current NCT models because the inherent dialogue\ncharacteristics of chat, such as dialogue coherence and speaker personality,\nare neglected. In this paper, we propose to promote the chat translation by\nintroducing the modeling of dialogue characteristics into the NCT model. To\nthis end, we design four auxiliary tasks including monolingual response\ngeneration, cross-lingual response generation, next utterance discrimination,\nand speaker identification. Together with the main chat translation task, we\noptimize the NCT model through the training objectives of all these tasks. By\nthis means, the NCT model can be enhanced by capturing the inherent dialogue\ncharacteristics, thus generating more coherent and speaker-relevant\ntranslations. Comprehensive experiments on four language directions\n(English-German and English-Chinese) verify the effectiveness and superiority\nof the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chulun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Exploration in Quality Filtering of Text Data. (arXiv:2109.00698v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00698","description":"<p>While conventional wisdom suggests that more aggressively filtering data from\nlow-quality sources like Common Crawl always monotonically improves the quality\nof training data, we find that aggressive filtering can in fact lead to a\ndecrease in model quality on a wide array of downstream tasks for a GPT-like\nlanguage model. We speculate that this is because optimizing sufficiently\nstrongly for a proxy metric harms performance on the true objective, suggesting\na need for more robust filtering objectives when attempting to filter more\naggressively. We hope this work leads to detailed analysis of the effects of\ndataset filtering design choices on downstream model performance in future\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Leo Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ShopTalk: A System for Conversational Faceted Search. (arXiv:2109.00702v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00702","description":"<p>We present ShopTalk, a multi-turn conversational faceted search system for\nshopping that is designed to handle large and complex schemas that are beyond\nthe scope of state of the art slot-filling systems. ShopTalk decouples dialog\nmanagement from fulfillment, thereby allowing the dialog understanding system\nto be domain-agnostic and not tied to the particular shopping application. The\ndialog understanding system consists of a deep-learned Contextual Language\nUnderstanding module, which interprets user utterances, and a primarily\nrules-based Dialog-State Tracker (DST), which updates the dialog state and\nformulates search requests intended for the fulfillment engine. The interface\nbetween the two modules consists of a minimal set of domain-agnostic \"intent\noperators,\" which instruct the DST on how to update the dialog state. ShopTalk\nwas deployed in 2020 on the Google Assistant for Shopping searches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manku_G/0/1/0/all/0/1\">Gurmeet Manku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1\">James Lee-Thorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanagal_B/0/1/0/all/0/1\">Bhargav Kanagal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jingchen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pearson_Z/0/1/0/all/0/1\">Zach Pearson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anjorin_E/0/1/0/all/0/1\">Ebenezer Anjorin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhe_S/0/1/0/all/0/1\">Sudeep Gandhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_I/0/1/0/all/0/1\">Ilya Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosswog_J/0/1/0/all/0/1\">Jim Rosswog</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghai_S/0/1/0/all/0/1\">Sumit Sanghai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_M/0/1/0/all/0/1\">Michael Pohl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_L/0/1/0/all/0/1\">Larry Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_D/0/1/0/all/0/1\">D. Sivakumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LightNER: A Lightweight Generative Framework with Prompt-guided Attention for Low-resource NER. (arXiv:2109.00720v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00720","description":"<p>NER in low-resource languages or domains suffers from inadequate training\ndata. Existing transfer learning approaches for low-resource NER usually have\nthe challenge that the target domain has different label sets compared with a\nresource-rich source domain, which can be concluded as class transfer and\ndomain transfer problems. In this paper, we propose a lightweight generative\nframework with prompt-guided attention for low-resource NER (LightNER) to\naddress these issues. Concretely, instead of tackling the problem by training\nlabel-specific discriminative classifiers, we convert sequence labeling to\ngenerate the entity pointer index sequence and entity categories without any\nlabel-specific classifiers, which can address the class transfer issue. We\nfurther propose prompt-guided attention by incorporating continuous prompts\ninto the self-attention layer to re-modulate the attention and adapt\npre-trained weights. Note that we only tune those continuous prompts with the\nwhole parameter of the pre-trained language model fixed, thus, making our\napproach lightweight and flexible for low-resource scenarios and can better\ntransfer knowledge across domains. Experimental results show that by tuning\nonly 0.16% of the parameters, LightNER can obtain comparable performance in the\nstandard setting and outperform standard sequence labeling and prototype-based\nmethods in low-resource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond. (arXiv:2109.00725v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00725","description":"<p>A fundamental goal of scientific research is to learn about causal\nrelationships. However, despite its critical role in the life and social\nsciences, causality has not had the same importance in Natural Language\nProcessing (NLP), which has traditionally placed more emphasis on predictive\ntasks. This distinction is beginning to fade, with an emerging area of\ninterdisciplinary research at the convergence of causal inference and language\nprocessing. Still, research on causality in NLP remains scattered across\ndomains without unified definitions, benchmark datasets and clear articulations\nof the remaining challenges. In this survey, we consolidate research across\nacademic areas and situate it in the broader NLP landscape. We introduce the\nstatistical challenge of estimating causal effects, encompassing settings where\ntext is used as an outcome, treatment, or as a means to address confounding. In\naddition, we explore potential uses of causal inference to improve the\nperformance, robustness, fairness, and interpretability of NLP models. We thus\nprovide a unified overview of causal inference for the computational\nlinguistics community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1\">Amir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keith_K/0/1/0/all/0/1\">Katherine A. Keith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzoor_E/0/1/0/all/0/1\">Emaad Manzoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1\">Reid Pryzant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_D/0/1/0/all/0/1\">Dhanya Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wood_Doughty_Z/0/1/0/all/0/1\">Zach Wood-Doughty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimmer_J/0/1/0/all/0/1\">Justin Grimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_M/0/1/0/all/0/1\">Margaret E. Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stewart_B/0/1/0/all/0/1\">Brandon M. Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veitch_V/0/1/0/all/0/1\">Victor Veitch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConQX: Semantic Expansion of Spoken Queries for Intent Detection based on Conditioned Text Generation. (arXiv:2109.00729v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00729","description":"<p>Intent detection of spoken queries is a challenging task due to their noisy\nstructure and short length. To provide additional information regarding the\nquery and enhance the performance of intent detection, we propose a method for\nsemantic expansion of spoken queries, called ConQX, which utilizes the text\ngeneration ability of an auto-regressive language model, GPT-2. To avoid\noff-topic text generation, we condition the input query to a structured context\nwith prompt mining. We then apply zero-shot, one-shot, and few-shot learning.\nWe lastly use the expanded queries to fine-tune BERT and RoBERTa for intent\ndetection. The experimental results show that the performance of intent\ndetection can be improved by our semantic expansion method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Eyup Halit Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toraman_C/0/1/0/all/0/1\">Cagri Toraman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural News Recommendation with Collaborative News Encoding and Structural User Encoding. (arXiv:2109.00750v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00750","description":"<p>Automatic news recommendation has gained much attention from the academic\ncommunity and industry. Recent studies reveal that the key to this task lies\nwithin the effective representation learning of both news and users. Existing\nworks typically encode news title and content separately while neglecting their\nsemantic interaction, which is inadequate for news text comprehension. Besides,\nprevious models encode user browsing history without leveraging the structural\ncorrelation of user browsed news to reflect user interests explicitly. In this\nwork, we propose a news recommendation framework consisting of collaborative\nnews encoding (CNE) and structural user encoding (SUE) to enhance news and user\nrepresentation learning. CNE equipped with bidirectional LSTMs encodes news\ntitle and content collaboratively with cross-selection and cross-attention\nmodules to learn semantic-interactive news representations. SUE utilizes graph\nconvolutional networks to extract cluster-structural features of user history,\nfollowed by intra-cluster and inter-cluster attention modules to learn\nhierarchical user interest representations. Experiment results on the MIND\ndataset validate the effectiveness of our model to improve the performance of\nnews recommendation. Our code is released at\nhttps://github.com/Veason-silverbullet/NNR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhiming Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MWPToolkit: An Open-Source Framework for Deep Learning-Based Math Word Problem Solvers. (arXiv:2109.00799v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00799","description":"<p>Developing automatic Math Word Problem (MWP) solvers has been an interest of\nNLP researchers since the 1960s. Over the last few years, there are a growing\nnumber of datasets and deep learning-based methods proposed for effectively\nsolving MWPs. However, most existing methods are benchmarked soly on one or two\ndatasets, varying in different configurations, which leads to a lack of\nunified, standardized, fair, and comprehensive comparison between methods. This\npaper presents MWPToolkit, the first open-source framework for solving MWPs. In\nMWPToolkit, we decompose the procedure of existing MWP solvers into multiple\ncore components and decouple their models into highly reusable modules. We also\nprovide a hyper-parameter search function to boost the performance. In total,\nwe implement and compare 17 MWP solvers on 4 widely-used single equation\ngeneration benchmarks and 2 multiple equations generation benchmarks. These\nfeatures enable our MWPToolkit to be suitable for researchers to reproduce\nadvanced baseline models and develop new MWP solvers quickly. Code and\ndocuments are available at https://github.com/LYH-YF/MWPToolkit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yihuai Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yunshi Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bing Tian Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1\">Ee-Peng Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imposing Relation Structure in Language-Model Embeddings Using Contrastive Learning. (arXiv:2109.00840v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00840","description":"<p>Though language model text embeddings have revolutionized NLP research, their\nability to capture high-level semantic information, such as relations between\nentities in text, is limited. In this paper, we propose a novel contrastive\nlearning framework that trains sentence embeddings to encode the relations in a\ngraph structure. Given a sentence (unstructured text) and its graph, we use\ncontrastive learning to impose relation-related structure on the token-level\nrepresentations of the sentence obtained with a CharacterBERT (El Boukkouri et\nal.,2020) model. The resulting relation-aware sentence embeddings achieve\nstate-of-the-art results on the relation extraction task using only a simple\nKNN classifier, thereby demonstrating the success of the proposed method.\nAdditional visualization by a tSNE analysis shows the effectiveness of the\nlearned representation space compared to baselines. Furthermore, we show that\nwe can learn a different space for named entity recognition, again using a\ncontrastive learning objective, and demonstrate how to successfully combine\nboth representation spaces in an entity-relation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Theodoropoulos_C/0/1/0/all/0/1\">Christos Theodoropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coman_A/0/1/0/all/0/1\">Andrei C. Coman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. (arXiv:2109.00859v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00859","description":"<p>Pre-trained models for Natural Languages (NL) like BERT and GPT have been\nrecently shown to transfer well to Programming Languages (PL) and largely\nbenefit a broad set of code-related tasks. Despite their success, most current\nmethods either rely on an encoder-only (or decoder-only) pre-training that is\nsuboptimal for generation (resp. understanding) tasks or process the code\nsnippet in the same way as NL, neglecting the special characteristics of PL\nsuch as token types. We present CodeT5, a unified pre-trained encoder-decoder\nTransformer model that better leverages the code semantics conveyed from the\ndeveloper-assigned identifiers. Our model employs a unified framework to\nseamlessly support both code understanding and generation tasks and allows for\nmulti-task learning. Besides, we propose a novel identifier-aware pre-training\ntask that enables the model to distinguish which code tokens are identifiers\nand to recover them when they are masked. Furthermore, we propose to exploit\nthe user-written code comments with a bimodal dual generation task for better\nNL-PL alignment. Comprehensive experiments show that CodeT5 significantly\noutperforms prior methods on understanding tasks such as code defect detection\nand clone detection, and generation tasks across various directions including\nPL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better\ncapture semantic information from code. Our code and pre-trained models are\nreleased at https: //github.com/salesforce/CodeT5 .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weishi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Perceived Multi-modal Pretraining in E-commerce. (arXiv:2109.00895v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00895","description":"<p>In this paper, we address multi-modal pretraining of product data in the\nfield of E-commerce. Current multi-modal pretraining methods proposed for image\nand text modalities lack robustness in the face of modality-missing and\nmodality-noise, which are two pervasive problems of multi-modal product data in\nreal E-commerce scenarios. To this end, we propose a novel method, K3M, which\nintroduces knowledge modality in multi-modal pretraining to correct the noise\nand supplement the missing of image and text modalities. The modal-encoding\nlayer extracts the features of each modality. The modal-interaction layer is\ncapable of effectively modeling the interaction of multiple modalities, where\nan initial-interactive feature fusion model is designed to maintain the\nindependence of image modality and text modality, and a structure aggregation\nmodule is designed to fuse the information of image, text, and knowledge\nmodalities. We pretrain K3M with three pretraining tasks, including masked\nobject modeling (MOM), masked language modeling (MLM), and link prediction\nmodeling (LPM). Experimental results on a real-world E-commerce dataset and a\nseries of product-based downstream tasks demonstrate that K3M achieves\nsignificant improvements in performances than the baseline and state-of-the-art\nmethods when modality-noise or modality-missing exists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yushan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1\">Huaixiao Tou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_G/0/1/0/all/0/1\">Ganqiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiEURLEX -- A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer. (arXiv:2109.00904v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00904","description":"<p>We introduce MULTI-EURLEX, a new multilingual dataset for topic\nclassification of legal documents. The dataset comprises 65k European Union\n(EU) laws, officially translated in 23 languages, annotated with multiple\nlabels from the EUROVOC taxonomy. We highlight the effect of temporal concept\ndrift and the importance of chronological, instead of random splits. We use the\ndataset as a testbed for zero-shot cross-lingual transfer, where we exploit\nannotated training documents in one language (source) to classify documents in\nanother language (target). We find that fine-tuning a multilingually pretrained\nmodel (XLM-ROBERTA, MT5) in a single source language leads to catastrophic\nforgetting of multilingual knowledge and, consequently, poor zero-shot transfer\nto other languages. Adaptation strategies, namely partial fine-tuning,\nadapters, BITFIT, LNFIT, originally proposed to accelerate fine-tuning for new\nend-tasks, help retain multilingual knowledge from pretraining, substantially\nimproving zero-shot cross-lingual transfer, but their impact also depends on\nthe pretrained model used and the size of the label set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergadiotis_M/0/1/0/all/0/1\">Manos Fergadiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-To-Fine And Cross-Lingual ASR Transfer. (arXiv:2109.00916v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00916","description":"<p>End-to-end neural automatic speech recognition systems achieved recently\nstate-of-the-art results, but they require large datasets and extensive\ncomputing resources. Transfer learning has been proposed to overcome these\ndifficulties even across languages, e.g., German ASR trained from an English\nmodel. We experiment with much less related languages, reusing an English model\nfor Czech ASR. To simplify the transfer, we propose to use an intermediate\nalphabet, Czech without accents, and document that it is a highly effective\nstrategy. The technique is also useful on Czech data alone, in the style of\ncoarse-to-fine training. We achieve substantial eductions in training time as\nwell as word error rate (WER).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Polak_P/0/1/0/all/0/1\">Peter Pol&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Multimodal fusion via Mutual Dependency Maximisation. (arXiv:2109.00922v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00922","description":"<p>Multimodal sentiment analysis is a trending area of research, and the\nmultimodal fusion is one of its most active topic. Acknowledging humans\ncommunicate through a variety of channels (i.e visual, acoustic, linguistic),\nmultimodal systems aim at integrating different unimodal representations into a\nsynthetic one. So far, a consequent effort has been made on developing complex\narchitectures allowing the fusion of these modalities. However, such systems\nare mainly trained by minimising simple losses such as $L_1$ or cross-entropy.\nIn this work, we investigate unexplored penalties and propose a set of new\nobjectives that measure the dependency between modalities. We demonstrate that\nour new penalties lead to a consistent improvement (up to $4.3$ on accuracy)\nacross a large variety of state-of-the-art models on two well-known sentiment\nanalysis datasets: \\texttt{CMU-MOSI} and \\texttt{CMU-MOSEI}. Our method not\nonly achieves a new SOTA on both datasets but also produces representations\nthat are more robust to modality drops. Finally, a by-product of our methods\nincludes a statistical network which can be used to interpret the high\ndimensional representations learnt by the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chapuis_E/0/1/0/all/0/1\">Emile Chapuis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labeau_M/0/1/0/all/0/1\">Matthieu Labeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chloe Clavel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaker-Conditioned Hierarchical Modeling for Automated Speech Scoring. (arXiv:2109.00928v1 [eess.AS])","link":"http://arxiv.org/abs/2109.00928","description":"<p>Automatic Speech Scoring (ASS) is the computer-assisted evaluation of a\ncandidate's speaking proficiency in a language. ASS systems face many\nchallenges like open grammar, variable pronunciations, and unstructured or\nsemi-structured content. Recent deep learning approaches have shown some\npromise in this domain. However, most of these approaches focus on extracting\nfeatures from a single audio, making them suffer from the lack of\nspeaker-specific context required to model such a complex task. We propose a\nnovel deep learning technique for non-native ASS, called speaker-conditioned\nhierarchical modeling. In our technique, we take advantage of the fact that\noral proficiency tests rate multiple responses for a candidate. We extract\ncontext vectors from these responses and feed them as additional\nspeaker-specific context to our network to score a particular response. We\ncompare our technique with strong baselines and find that such modeling\nimproves the model's average performance by 6.92% (maximum = 12.86%, minimum =\n4.51%). We further show both quantitative and qualitative insights into the\nimportance of this additional context in solving the problem of ASS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Singla_Y/0/1/0/all/0/1\">Yaman Kumar Singla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1\">Avykat Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bagga_S/0/1/0/all/0/1\">Shaurya Bagga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krishnamurthy_B/0/1/0/all/0/1\">Balaji Krishnamurthy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coordinating Narratives and the Capitol Riots on Parler. (arXiv:2109.00945v1 [cs.SI])","link":"http://arxiv.org/abs/2109.00945","description":"<p>Coordinated disinformation campaigns are used to influence social media\nusers, potentially leading to offline violence. In this study, we introduce a\ngeneral methodology to uncover coordinated messaging through analysis of user\nparleys on Parler. The proposed method constructs a user-to-user coordination\nnetwork graph induced by a user-to-text graph and a text-to-text similarity\ngraph. The text-to-text graph is constructed based on the textual similarity of\nParler posts. We study three influential groups of users in the 6 January 2020\nCapitol riots and detect networks of coordinated user clusters that are all\nposting similar textual content in support of different disinformation\nnarratives related to the U.S. 2020 elections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ng_L/0/1/0/all/0/1\">Lynnette Hui Xian Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruickshank_I/0/1/0/all/0/1\">Iain Cruickshank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carley_K/0/1/0/all/0/1\">Kathleen M. Carley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LegaLMFiT: Efficient Short Legal Text Classification with LSTM Language Model Pre-Training. (arXiv:2109.00993v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00993","description":"<p>Large Transformer-based language models such as BERT have led to broad\nperformance improvements on many NLP tasks. Domain-specific variants of these\nmodels have demonstrated excellent performance on a variety of specialised\ntasks. In legal NLP, BERT-based models have led to new state-of-the-art results\non multiple tasks. The exploration of these models has demonstrated the\nimportance of capturing the specificity of the legal language and its\nvocabulary. However, such approaches suffer from high computational costs,\nleading to a higher ecological impact and lower accessibility. Our findings,\nfocusing on English language legal text, show that lightweight LSTM-based\nLanguage Models are able to capture enough information from a small legal text\npretraining corpus and achieve excellent performance on short legal text\nclassification tasks. This is achieved with a significantly reduced\ncomputational overhead compared to BERT-based models. However, our method also\nshows degraded performance on a more complex task, multi-label classification\nof longer documents, highlighting the limitations of this lightweight approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clavie_B/0/1/0/all/0/1\">Benjamin Clavi&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gheewala_A/0/1/0/all/0/1\">Akshita Gheewala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briton_P/0/1/0/all/0/1\">Paul Briton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alphonsus_M/0/1/0/all/0/1\">Marc Alphonsus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labiyaad_R/0/1/0/all/0/1\">Rym Labiyaad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccoli_F/0/1/0/all/0/1\">Francesco Piccoli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TravelBERT: Pre-training Language Model Incorporating Domain-specific Heterogeneous Knowledge into A Unified Representation. (arXiv:2109.01048v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01048","description":"<p>Existing technologies expand BERT from different perspectives, e.g. designing\ndifferent pre-training tasks, different semantic granularities and different\nmodel architectures. Few models consider expanding BERT from different text\nformats. In this paper, we propose a heterogeneous knowledge language model\n(HKLM), a unified pre-trained language model (PLM) for all forms of text,\nincluding unstructured text, semi-structured text and well-structured text. To\ncapture the corresponding relations among these multi-format knowledge, our\napproach uses masked language model objective to learn word knowledge, uses\ntriple classification objective and title matching objective to learn entity\nknowledge and topic knowledge respectively. To obtain the aforementioned\nmulti-format text, we construct a corpus in the tourism domain and conduct\nexperiments on 5 tourism NLP datasets. The results show that our approach\noutperforms the pre-training of plain text using only 1/4 of the data. The\ncode, datasets, corpus and knowledge graph will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongyin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1\">Zhiheng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jinghui Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skim-Attention: Learning to Focus via Document Layout. (arXiv:2109.01078v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01078","description":"<p>Transformer-based pre-training techniques of text and layout have proven\neffective in a number of document understanding tasks. Despite this success,\nmultimodal pre-training models suffer from very high computational and memory\ncosts. Motivated by human reading strategies, this paper presents\nSkim-Attention, a new attention mechanism that takes advantage of the structure\nof the document and its layout. Skim-Attention only attends to the\n2-dimensional position of the words in a document. Our experiments show that\nSkim-Attention obtains a lower perplexity than prior works, while being more\ncomputationally efficient. Skim-Attention can be further combined with\nlong-range Transformers to efficiently process long documents. We also show how\nSkim-Attention can be used off-the-shelf as a mask for any Pre-trained Language\nModel, allowing to improve their performance while restricting attention.\nFinally, we show the emergence of a document structure representation in\nSkim-Attention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Laura Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staiano_J/0/1/0/all/0/1\">Jacopo Staiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piwowarski_B/0/1/0/all/0/1\">Benjamin Piwowarski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?. (arXiv:2109.01100v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01100","description":"<p>Data-driven subword segmentation has become the default strategy for\nopen-vocabulary machine translation and other NLP tasks, but may not be\nsufficiently generic for optimal learning of non-concatenative morphology. We\ndesign a test suite to evaluate segmentation strategies on different types of\nmorphological phenomena in a controlled, semi-synthetic setting. In our\nexperiments, we compare how well machine translation models trained on subword-\nand character-level can translate these morphological phenomena. We find that\nlearning to analyse and generate morphologically complex surface\nrepresentations is still challenging, especially for non-concatenative\nmorphological phenomena like reduplication or vowel harmony and for rare word\nstems. Based on our results, we recommend that novel text representation\nstrategies be tested on a range of typologically diverse languages to minimise\nthe risk of adopting a strategy that inadvertently disadvantages certain\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amrhein_C/0/1/0/all/0/1\">Chantal Amrhein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-to-Sequence Learning with Latent Neural Grammars. (arXiv:2109.01135v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01135","description":"<p>Sequence-to-sequence learning with neural networks has become the de facto\nstandard for sequence prediction tasks. This approach typically models the\nlocal distribution over the next word with a powerful neural network that can\ncondition on arbitrary context. While flexible and performant, these models\noften require large datasets for training and can fail spectacularly on\nbenchmarks designed to test for compositional generalization. This work\nexplores an alternative, hierarchical approach to sequence-to-sequence learning\nwith quasi-synchronous grammars, where each node in the target tree is\ntransduced by a node in the source tree. Both the source and target trees are\ntreated as latent and induced during training. We develop a neural\nparameterization of the grammar which enables parameter sharing over the\ncombinatorial space of derivation rules without the need for manual feature\nengineering. We apply this latent neural grammar to various domains -- a\ndiagnostic language navigation task designed to test for compositional\ngeneralization (SCAN), style transfer, and small-scale machine translation --\nand find that it performs respectably compared to standard baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Iterative Multi-Knowledge Transfer Network for Aspect-Based Sentiment Analysis. (arXiv:2004.01935v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.01935","description":"<p>Aspect-based sentiment analysis (ABSA) mainly involves three subtasks: aspect\nterm extraction, opinion term extraction, and aspect-level sentiment\nclassification, which are typically handled in a separate or joint manner.\nHowever, previous approaches do not well exploit the interactive relations\namong three subtasks and do not pertinently leverage the easily available\ndocument-level labeled domain/sentiment knowledge, which restricts their\nperformances. To address these issues, we propose a novel Iterative\nMulti-Knowledge Transfer Network (IMKTN) for end-to-end ABSA. For one thing,\nthrough the interactive correlations between the ABSA subtasks, our IMKTN\ntransfers the task-specific knowledge from any two of the three subtasks to\nanother one at the token level by utilizing a well-designed routing algorithm,\nthat is, any two of the three subtasks will help the third one. For another,\nour IMKTN pertinently transfers the document-level knowledge, i.e.,\ndomain-specific and sentiment-related knowledge, to the aspect-level subtasks\nto further enhance the corresponding performance. Experimental results on three\nbenchmark datasets demonstrate the effectiveness and superiority of our\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Real-Time Question Answering via Question Generation. (arXiv:2009.05167v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.05167","description":"<p>Although deep neural networks have achieved tremendous success for question\nanswering (QA), they are still suffering from heavy computational and energy\ncost for real product deployment. Further, existing QA systems are bottlenecked\nby the encoding time of real-time questions with neural networks, thus\nsuffering from detectable latency in deployment for large-volume traffic. To\nreduce the computational cost and accelerate real-time question answering\n(RTQA) for practical usage, we propose to remove all the neural networks from\nonline QA systems, and present Ocean-Q (an Ocean of Questions), which\nintroduces a new question generation (QG) model to generate a large pool of QA\npairs offline, then in real time matches an input question with the candidate\nQA pool to predict the answer without question encoding. Ocean-Q can be readily\ndeployed in existing distributed database systems or search engine for\nlarge-scale query usage, and much greener with no additional cost for\nmaintaining large neural networks. Experiments on SQuAD(-open) and HotpotQA\nbenchmarks demonstrate that Ocean-Q is able to accelerate the fastest\nstate-of-the-art RTQA system by 4X times, with only a 3+% accuracy drop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Siqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GMH: A General Multi-hop Reasoning Model for KG Completion. (arXiv:2010.07620v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2010.07620","description":"<p>Knowledge graphs are essential for numerous downstream natural language\nprocessing applications, but are typically incomplete with many facts missing.\nThis results in research efforts on multi-hop reasoning task, which can be\nformulated as a search process and current models typically perform short\ndistance reasoning. However, the long-distance reasoning is also vital with the\nability to connect the superficially unrelated entities. To the best of our\nknowledge, there lacks a general framework that approaches multi-hop reasoning\nin mixed long-short distance reasoning scenarios. We argue that there are two\nkey issues for a general multi-hop reasoning model: i) where to go, and ii)\nwhen to stop. Therefore, we propose a general model which resolves the issues\nwith three modules: 1) the local-global knowledge module to estimate the\npossible paths, 2) the differentiated action dropout module to explore a\ndiverse set of paths, and 3) the adaptive stopping search module to avoid over\nsearching. The comprehensive results on three datasets demonstrate the\nsuperiority of our model with significant improvements against baselines in\nboth short and long distance reasoning scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hongru Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Ning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenglu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Transfer of Abstractive Summarizer to Less-resource Language. (arXiv:2012.04307v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.04307","description":"<p>Automatic text summarization extracts important information from texts and\npresents the information in the form of a summary. Abstractive summarization\napproaches progressed significantly by switching to deep neural networks, but\nresults are not yet satisfactory, especially for languages where large training\nsets do not exist. In several natural language processing tasks, a\ncross-lingual model transfer is successfully applied in less-resource\nlanguages. For summarization, the cross-lingual model transfer was not\nattempted due to a non-reusable decoder side of neural models that cannot\ncorrect target language generation. In our work, we use a pre-trained English\nsummarization model based on deep neural networks and sequence-to-sequence\narchitecture to summarize Slovene news articles. We address the problem of\ninadequate decoder by using an additional language model for the evaluation of\nthe generated text in target language. We test several cross-lingual\nsummarization models with different amounts of target data for fine-tuning. We\nassess the models with automatic evaluation measures and conduct a small-scale\nhuman evaluation. Automatic evaluation shows that the summaries of our best\ncross-lingual model are useful and of quality similar to the model trained only\nin the target language. Human evaluation shows that our best model generates\nsummaries with high accuracy and acceptable readability. However, similar to\nother abstractive models, our models are not perfect and may occasionally\nproduce misleading or absurd content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zagar_A/0/1/0/all/0/1\">Ale&#x161; &#x17d;agar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-&#x160;ikonja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Topic Coverage Approach to Evaluation of Topic Models. (arXiv:2012.06274v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2012.06274","description":"<p>Topic models are widely used unsupervised models capable of learning topics -\nweighted lists of words and documents - from large collections of text\ndocuments. When topic models are used for discovery of topics in text\ncollections, a question that arises naturally is how well the model-induced\ntopics correspond to topics of interest to the analyst. In this paper we\nrevisit and extend a so far neglected approach to topic model evaluation based\non measuring topic coverage - computationally matching model topics with a set\nof reference topics that models are expected to uncover. The approach is well\nsuited for analyzing models' performance in topic discovery and for large-scale\nanalysis of both topic models and measures of model quality. We propose new\nmeasures of coverage and evaluate, in a series of experiments, different types\nof topic models on two distinct text domains for which interest for topic\ndiscovery exists. The experiments include evaluation of model quality, analysis\nof coverage of distinct topic categories, and the analysis of the relationship\nbetween coverage and other methods of topic model evaluation. The paper\ncontributes a new supervised measure of coverage, and the first unsupervised\nmeasure of coverage. The supervised measure achieves topic matching accuracy\nclose to human agreement. The unsupervised measure correlates highly with the\nsupervised one (Spearman's $\\rho \\geq 0.95$). Other contributions include\ninsights into both topic models and different methods of model evaluation, and\nthe datasets and code for facilitating future research on topic coverage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korencic_D/0/1/0/all/0/1\">Damir Koren&#x10d;i&#x107;</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ristov_S/0/1/0/all/0/1\">Strahil Ristov</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Repar_J/0/1/0/all/0/1\">Jelena Repar</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1\">Jan &#x160;najder</a> (2) ((1) Rudjer Bo&#x161;kovi&#x107; Institute, Croatia, (2) University of Zagreb, Faculty of Electrical Engineering and Computing, Croatia)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NewsBERT: Distilling Pre-trained Language Model for Intelligent News Application. (arXiv:2102.04887v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.04887","description":"<p>Pre-trained language models (PLMs) like BERT have made great progress in NLP.\nNews articles usually contain rich textual information, and PLMs have the\npotentials to enhance news text modeling for various intelligent news\napplications like news recommendation and retrieval. However, most existing\nPLMs are in huge size with hundreds of millions of parameters. Many online news\napplications need to serve millions of users with low latency tolerance, which\nposes huge challenges to incorporating PLMs in these scenarios. Knowledge\ndistillation techniques can compress a large PLM into a much smaller one and\nmeanwhile keeps good performance. However, existing language models are\npre-trained and distilled on general corpus like Wikipedia, which has some gaps\nwith the news domain and may be suboptimal for news intelligence. In this\npaper, we propose NewsBERT, which can distill PLMs for efficient and effective\nnews intelligence. In our approach, we design a teacher-student joint learning\nand distillation framework to collaboratively learn both teacher and student\nmodels, where the student model can learn from the learning experience of the\nteacher model. In addition, we propose a momentum distillation method by\nincorporating the gradients of teacher model into the update of student model\nto better transfer useful knowledge learned by the teacher model. Extensive\nexperiments on two real-world datasets with three tasks show that NewsBERT can\neffectively improve the model performance in various intelligent news\napplications with much smaller models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data-Centric Framework for Composable NLP Workflows. (arXiv:2103.01834v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.01834","description":"<p>Empirical natural language processing (NLP) systems in application domains\n(e.g., healthcare, finance, education) involve interoperation among multiple\ncomponents, ranging from data ingestion, human annotation, to text retrieval,\nanalysis, generation, and visualization. We establish a unified open-source\nframework to support fast development of such sophisticated NLP workflows in a\ncomposable manner. The framework introduces a uniform data representation to\nencode heterogeneous results by a wide range of NLP tasks. It offers a large\nrepository of processors for NLP tasks, visualization, and annotation, which\ncan be easily assembled with full interoperability under the unified\nrepresentation. The highly extensible framework allows plugging in custom\nprocessors from external off-the-shelf NLP and deep learning libraries. The\nwhole framework is delivered through two modularized yet integratable\nopen-source projects, namely Forte (for workflow infrastructure and NLP\nfunction processors) and Stave (for user interaction, visualization, and\nannotation).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guanxiong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukkittu_A/0/1/0/all/0/1\">Avinash Bukkittu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Mansi Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pengzhi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Atif Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhavi_S/0/1/0/all/0/1\">Swapnil Singhavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zecong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haoran Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradual Fine-Tuning for Low-Resource Domain Adaptation. (arXiv:2103.02205v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.02205","description":"<p>Fine-tuning is known to improve NLP models by adapting an initial model\ntrained on more plentiful but less domain-salient examples to data in a target\ndomain. Such domain adaptation is typically done using one stage of\nfine-tuning. We demonstrate that gradually fine-tuning in a multi-stage process\ncan yield substantial further gains and can be applied without modifying the\nmodel or learning objective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebner_S/0/1/0/all/0/1\">Seth Ebner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yarmohammadi_M/0/1/0/all/0/1\">Mahsa Yarmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1\">Aaron Steven White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1\">Kenton Murray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conceptual similarity and communicative need shape colexification: an experimental study. (arXiv:2103.11024v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11024","description":"<p>Colexification refers to the phenomenon of multiple meanings sharing one word\nin a language. Cross-linguistic lexification patterns have been shown to be\nlargely predictable, as similar concepts are often colexified. We test a recent\nclaim that, beyond this general tendency, communicative needs play an important\nrole in shaping colexification patterns. We approach this question by means of\na series of human experiments, using an artificial language communication game\nparadigm. Our results across four experiments match the previous\ncross-linguistic findings: all other things being equal, speakers do prefer to\ncolexify similar concepts. However, we also find evidence supporting the\ncommunicative need hypothesis: when faced with a frequent need to distinguish\nsimilar pairs of meanings, speakers adjust their colexification preferences to\nmaintain communicative efficiency, and avoid colexifying those similar meanings\nwhich need to be distinguished in communication. This research provides further\nevidence to support the argument that languages are shaped by the needs and\npreferences of their speakers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karjus_A/0/1/0/all/0/1\">Andres Karjus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blythe_R/0/1/0/all/0/1\">Richard A. Blythe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirby_S/0/1/0/all/0/1\">Simon Kirby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kenny Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Aware Graph-Enhanced GPT-2 for Dialogue State Tracking. (arXiv:2104.04466v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04466","description":"<p>Dialogue State Tracking is central to multi-domain task-oriented dialogue\nsystems, responsible for extracting information from user utterances. We\npresent a novel hybrid architecture that augments GPT-2 with representations\nderived from Graph Attention Networks in such a way to allow causal, sequential\nprediction of slot values. The model architecture captures inter-slot\nrelationships and dependencies across domains that otherwise can be lost in\nsequential prediction. We report improvements in state tracking performance in\nMultiWOZ 2.0 against a strong GPT-2 baseline and investigate a simplified\nsparse training scenario in which DST models are trained only on session-level\nannotations but evaluated at the turn level. We further report detailed\nanalyses to demonstrate the effectiveness of graph models in DST by showing\nthat the proposed graph modules capture inter-slot dependencies and improve the\npredictions of values that are common to multiple domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weizhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_B/0/1/0/all/0/1\">Bo-Hsian Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byrne_B/0/1/0/all/0/1\">Bill Byrne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Power of Scale for Parameter-Efficient Prompt Tuning. (arXiv:2104.08691v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08691","description":"<p>In this work, we explore \"prompt tuning\", a simple yet effective mechanism\nfor learning \"soft prompts\" to condition frozen language models to perform\nspecific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft\nprompts are learned through backpropagation and can be tuned to incorporate\nsignal from any number of labeled examples. Our end-to-end learned approach\noutperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably,\nthrough ablations on model size using T5, we show that prompt tuning becomes\nmore competitive with scale: as models exceed billions of parameters, our\nmethod \"closes the gap\" and matches the strong performance of model tuning\n(where all model weights are tuned). This finding is especially relevant in\nthat large models are costly to share and serve, and the ability to reuse one\nfrozen model for multiple downstream tasks can ease this burden. Our method can\nbe seen as a simplification of the recently proposed \"prefix tuning\" of Li and\nLiang (2021), and we provide a comparison to this and other similar approaches.\nFinally, we show that conditioning a frozen model with soft prompts confers\nbenefits in robustness to domain transfer, as compared to full model tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lester_B/0/1/0/all/0/1\">Brian Lester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Rfou_R/0/1/0/all/0/1\">Rami Al-Rfou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Commonsense Explanation in Dialogue Response Generation. (arXiv:2104.09574v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.09574","description":"<p>Humans use commonsense reasoning (CSR) implicitly to produce natural and\ncoherent responses in conversations. Aiming to close the gap between current\nresponse generation (RG) models and human communication abilities, we want to\nunderstand why RG models respond as they do by probing RG model's understanding\nof commonsense reasoning that elicits proper responses. We formalize the\nproblem by framing commonsense as a latent variable in the RG task and using\nexplanations for responses as textual form of commonsense. We collect 6k\nannotated explanations justifying responses from four dialogue datasets and ask\nhumans to verify them and propose two probing settings to evaluate RG models'\nCSR capabilities. Probing results show that models fail to capture the logical\nrelations between commonsense explanations and responses and fine-tuning on\nin-domain data and increasing model sizes do not lead to understanding of CSR\nfor RG. We hope our study motivates more research in making RG models emulate\nthe human reasoning process in pursuit of smooth human-AI communication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jandaghi_P/0/1/0/all/0/1\">Pegah Jandaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Justin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models. (arXiv:2105.00827v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.00827","description":"<p>Transformer-based pretrained language models (PLMs) have started a new era in\nmodern natural language processing (NLP). These models combine the power of\ntransformers, transfer learning, and self-supervised learning (SSL). Following\nthe success of these models in the general domain, the biomedical research\ncommunity has developed various in-domain PLMs starting from BioBERT to the\nlatest BioELECTRA and BioALBERT models. We strongly believe there is a need for\na survey paper that can provide a comprehensive survey of various\ntransformer-based biomedical pretrained language models (BPLMs). In this\nsurvey, we start with a brief overview of foundational concepts like\nself-supervised learning, embedding layer and transformer encoder layers. We\ndiscuss core concepts of transformer-based PLMs like pretraining methods,\npretraining tasks, fine-tuning methods, and various embedding types specific to\nbiomedical domain. We introduce a taxonomy for transformer-based BPLMs and then\ndiscuss all the models. We discuss various challenges and present possible\nsolutions. We conclude by highlighting some of the open issues which will drive\nthe research community to further improve transformer-based BPLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_K/0/1/0/all/0/1\">Katikapalli Subramanyam Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasekharan_A/0/1/0/all/0/1\">Ajit Rajasekharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangeetha_S/0/1/0/all/0/1\">Sivanesan Sangeetha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fastformer: Additive Attention Can Be All You Need. (arXiv:2108.09084v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09084","description":"<p>Transformer is a powerful model for text understanding. However, it is\ninefficient due to its quadratic complexity to input sequence length. Although\nthere are many methods on Transformer acceleration, they are still either\ninefficient on long sequences or not effective enough. In this paper, we\npropose Fastformer, which is an efficient Transformer model based on additive\nattention. In Fastformer, instead of modeling the pair-wise interactions\nbetween tokens, we first use additive attention mechanism to model global\ncontexts, and then further transform each token representation based on its\ninteraction with global context representations. In this way, Fastformer can\nachieve effective context modeling with linear complexity. Extensive\nexperiments on five datasets show that Fastformer is much more efficient than\nmany existing Transformer models and can meanwhile achieve comparable or even\nbetter long text modeling performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer. (arXiv:2108.09193v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09193","description":"<p>Transformer has achieved great success in NLP. However, the quadratic\ncomplexity of the self-attention mechanism in Transformer makes it inefficient\nin handling long sequences. Many existing works explore to accelerate\nTransformers by computing sparse self-attention instead of a dense one, which\nusually attends to tokens at certain positions or randomly selected tokens.\nHowever, manually selected or random tokens may be uninformative for context\nmodeling. In this paper, we propose Smart Bird, which is an efficient and\neffective Transformer with learnable sparse attention. In Smart Bird, we first\ncompute a sketched attention matrix with a single-head low-dimensional\nTransformer, which aims to find potential important interactions between\ntokens. We then sample token pairs based on their probability scores derived\nfrom the sketched attention matrix to generate different sparse attention index\nmatrices for different attention heads. Finally, we select token embeddings\naccording to the index matrices to form the input of sparse attention networks.\nExtensive experiments on six benchmark datasets for different tasks validate\nthe efficiency and effectiveness of Smart Bird in text modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_B/0/1/0/all/0/1\">Binxing Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Chatbot Per Person: Creating Personalized Chatbots based on Implicit User Profiles. (arXiv:2108.09355v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09355","description":"<p>Personalized chatbots focus on endowing chatbots with a consistent\npersonality to behave like real users, give more informative responses, and\nfurther act as personal assistants. Existing personalized approaches tried to\nincorporate several text descriptions as explicit user profiles. However, the\nacquisition of such explicit profiles is expensive and time-consuming, thus\nbeing impractical for large-scale real-world applications. Moreover, the\nrestricted predefined profile neglects the language behavior of a real user and\ncannot be automatically updated together with the change of user interests. In\nthis paper, we propose to learn implicit user profiles automatically from\nlarge-scale user dialogue history for building personalized chatbots.\nSpecifically, leveraging the benefits of Transformer on language understanding,\nwe train a personalized language model to construct a general user profile from\nthe user's historical responses. To highlight the relevant historical responses\nto the input post, we further establish a key-value memory network of\nhistorical post-response pairs, and build a dynamic post-aware user profile.\nThe dynamic profile mainly describes what and how the user has responded to\nsimilar posts in history. To explicitly utilize users' frequently used words,\nwe design a personalized decoder to fuse two decoding strategies, including\ngenerating a word from the generic vocabulary and copying one word from the\nuser's personalized vocabulary. Experiments on two real-world datasets show the\nsignificant improvement of our model compared with existing methods. Our code\nis available at https://github.com/zhengyima/DHAP\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengyi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Hanxun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12202","description":"<p>In joint entity and relation extraction, existing work either sequentially\nencode task-specific features, leading to an imbalance in inter-task feature\ninteraction where features extracted later have no direct contact with those\nthat come first. Or they encode entity features and relation features in a\nparallel manner, meaning that feature representation learning for each task is\nlargely independent of each other except for input sharing. We propose a\npartition filter network to model two-way interaction between tasks properly,\nwhere feature encoding is decomposed into two steps: partition and filter. In\nour encoder, we leverage two gates: entity and relation gate, to segment\nneurons into two task partitions and one shared partition. The shared partition\nrepresents inter-task information valuable to both tasks and is evenly shared\nacross two tasks to ensure proper two-way interaction. The task partitions\nrepresent intra-task information and are formed through concerted efforts of\nboth gates, making sure that encoding of task-specific features is dependent\nupon each other. Experiment results on five public datasets show that our model\nperforms significantly better than previous approaches. In addition, contrary\nto what previous work claims, our auxiliary experiments suggest that relation\nprediction is contributory to named entity prediction in a non-negligible way.\nThe source code can be found at https://github.com/Coopercoppers/PFN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tree Decomposition Attention for AMR-to-Text Generation. (arXiv:2108.12300v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12300","description":"<p>Text generation from AMR requires mapping a semantic graph to a string that\nit annotates. Transformer-based graph encoders, however, poorly capture vertex\ndependencies that may benefit sequence prediction. To impose order on an\nencoder, we locally constrain vertex self-attention using a graph's tree\ndecomposition. Instead of forming a full query-key bipartite graph, we restrict\nattention to vertices in parent, subtree, and same-depth bags of a vertex. This\nhierarchical context lends both sparsity and structure to vertex state updates.\nWe apply dynamic programming to derive a forest of tree decompositions,\nchoosing the most structurally similar tree to the AMR. Our system outperforms\na self-attentive baseline by 1.6 BLEU and 1.8 chrF++.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lisa Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gildea_D/0/1/0/all/0/1\">Daniel Gildea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Tree Decomposition Parsers for AMR-to-Text Generation. (arXiv:2108.12304v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12304","description":"<p>Graph encoders in AMR-to-text generation models often rely on neighborhood\nconvolutions or global vertex attention. While these approaches apply to\ngeneral graphs, AMRs may be amenable to encoders that target their tree-like\nstructure. By clustering edges into a hierarchy, a tree decomposition\nsummarizes graph structure. Our model encodes a derivation forest of tree\ndecompositions and extracts an expected tree. From tree node embeddings, it\nbuilds graph edge features used in vertex attention of the graph encoder.\nEncoding TD forests instead of shortest-pairwise paths in a self-attentive\nbaseline raises BLEU by 0.7 and chrF++ by 0.3. The forest encoder also\nsurpasses a convolutional baseline for molecular property prediction by 1.92%\nROC-AUC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lisa Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gildea_D/0/1/0/all/0/1\">Daniel Gildea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smoothing Dialogue States for Open Conversational Machine Reading. (arXiv:2108.12599v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12599","description":"<p>Conversational machine reading (CMR) requires machines to communicate with\nhumans through multi-turn interactions between two salient dialogue states of\ndecision making and question generation processes. In open CMR settings, as the\nmore realistic scenario, the retrieved background knowledge would be noisy,\nwhich results in severe challenges in the information transmission. Existing\nstudies commonly train independent or pipeline systems for the two subtasks.\nHowever, those methods are trivial by using hard-label decisions to activate\nquestion generation, which eventually hinders the model performance. In this\nwork, we propose an effective gating strategy by smoothing the two dialogue\nstates in only one decoder and bridge decision making and question generation\nto provide a richer dialogue state reference. Experiments on the OR-ShARC\ndataset show the effectiveness of our method, which achieves new\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siru Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utiyama_M/0/1/0/all/0/1\">Masao Utiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumita_E/0/1/0/all/0/1\">Eiichiro Sumita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Conditional Extreme Value Theory for Open Set Video Domain Adaptation. (arXiv:2109.00522v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00522","description":"<p>With the advent of media streaming, video action recognition has become\nprogressively important for various applications, yet at the high expense of\nrequiring large-scale data labelling. To overcome the problem of expensive data\nlabelling, domain adaptation techniques have been proposed that transfers\nknowledge from fully labelled data (i.e., source domain) to unlabelled data\n(i.e., target domain). The majority of video domain adaptation algorithms are\nproposed for closed-set scenarios in which all the classes are shared among the\ndomains. In this work, we propose an open-set video domain adaptation approach\nto mitigate the domain discrepancy between the source and target data, allowing\nthe target data to contain additional classes that do not belong to the source\ndomain. Different from previous works, which only focus on improving accuracy\nfor shared classes, we aim to jointly enhance the alignment of shared classes\nand recognition of unknown samples. Towards this goal, class-conditional\nextreme value theory is applied to enhance the unknown recognition.\nSpecifically, the entropy values of target samples are modelled as generalised\nextreme value distributions, which allows separating unknown samples lying in\nthe tail of the distribution. To alleviate the negative transfer issue, weights\ncomputed by the distance from the sample entropy to the threshold are leveraged\nin adversarial learning in the sense that confident source and target samples\nare aligned, and unconfident samples are pushed away. The proposed method has\nbeen thoroughly evaluated on both small-scale and large-scale cross-domain\nvideo datasets and achieved the state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuoxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yadan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1\">Mahsa Baktashmotlagh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Limits of Pseudo Ground Truth in Visual Camera Re-localisation. (arXiv:2109.00524v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00524","description":"<p>Benchmark datasets that measure camera pose accuracy have driven progress in\nvisual re-localisation research. To obtain poses for thousands of images, it is\ncommon to use a reference algorithm to generate pseudo ground truth. Popular\nchoices include Structure-from-Motion (SfM) and\nSimultaneous-Localisation-and-Mapping (SLAM) using additional sensors like\ndepth cameras if available. Re-localisation benchmarks thus measure how well\neach method replicates the results of the reference algorithm. This begs the\nquestion whether the choice of the reference algorithm favours a certain family\nof re-localisation methods. This paper analyzes two widely used re-localisation\ndatasets and shows that evaluation outcomes indeed vary with the choice of the\nreference algorithm. We thus question common beliefs in the re-localisation\nliterature, namely that learning-based scene coordinate regression outperforms\nclassical feature-based methods, and that RGB-D-based methods outperform\nRGB-based methods. We argue that any claims on ranking re-localisation methods\nshould take the type of the reference algorithm, and the similarity of the\nmethods to the reference algorithm, into account.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brachmann_E/0/1/0/all/0/1\">Eric Brachmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Humenberger_M/0/1/0/all/0/1\">Martin Humenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rother_C/0/1/0/all/0/1\">Carsten Rother</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattler_T/0/1/0/all/0/1\">Torsten Sattler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransforMesh: A Transformer Network for Longitudinal modeling of Anatomical Meshes. (arXiv:2109.00532v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00532","description":"<p>The longitudinal modeling of neuroanatomical changes related to Alzheimer's\ndisease (AD) is crucial for studying the progression of the disease. To this\nend, we introduce TransforMesh, a spatio-temporal network based on transformers\nthat models longitudinal shape changes on 3D anatomical meshes. While\ntransformer and mesh networks have recently shown impressive performances in\nnatural language processing and computer vision, their application to medical\nimage analysis has been very limited. To the best of our knowledge, this is the\nfirst work that combines transformer and mesh networks. Our results show that\nTransforMesh can model shape trajectories better than other baseline\narchitectures that do not capture temporal dependencies. Moreover, we also\nexplore the capabilities of TransforMesh in detecting structural anomalies of\nthe hippocampus in patients developing AD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarasua_I/0/1/0/all/0/1\">Ignacio Sarasua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1\">Sebastian Polsterl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fair Representation: Guaranteeing Approximate Multiple Group Fairness for Unknown Tasks. (arXiv:2109.00545v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00545","description":"<p>Motivated by scenarios where data is used for diverse prediction tasks, we\nstudy whether fair representation can be used to guarantee fairness for unknown\ntasks and for multiple fairness notions simultaneously. We consider seven group\nfairness notions that cover the concepts of independence, separation, and\ncalibration. Against the backdrop of the fairness impossibility results, we\nexplore approximate fairness. We prove that, although fair representation might\nnot guarantee fairness for all prediction tasks, it does guarantee fairness for\nan important subset of tasks -- the tasks for which the representation is\ndiscriminative. Specifically, all seven group fairness notions are linearly\ncontrolled by fairness and discriminativeness of the representation. When an\nincompatibility exists between different fairness notions, fair and\ndiscriminative representation hits the sweet spot that approximately satisfies\nall notions. Motivated by our theoretical findings, we propose to learn both\nfair and discriminative representations using pretext loss which\nself-supervises learning, and Maximum Mean Discrepancy as a fair regularizer.\nExperiments on tabular, image, and face datasets show that using the learned\nrepresentation, downstream predictions that we are unaware of when learning the\nrepresentation indeed become fairer for seven group fairness notions, and the\nfairness guarantees computed from our theoretical results are all valid.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xudong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1\">Yongkang Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan Kankanhalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pulmonary Disease Classification Using Globally Correlated Maximum Likelihood: an Auxiliary Attention mechanism for Convolutional Neural Networks. (arXiv:2109.00573v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00573","description":"<p>Convolutional neural networks (CNN) are now being widely used for classifying\nand detecting pulmonary abnormalities in chest radiographs. Two complementary\ngeneralization properties of CNNs, translation invariance and equivariance, are\nparticularly useful in detecting manifested abnormalities associated with\npulmonary disease, regardless of their spatial locations within the image.\nHowever, these properties also come with the loss of exact spatial information\nand global relative positions of abnormalities detected in local regions.\nGlobal relative positions of such abnormalities may help distinguish similar\nconditions, such as COVID-19 and viral pneumonia. In such instances, a global\nattention mechanism is needed, which CNNs do not support in their traditional\narchitectures that aim for generalization afforded by translation invariance\nand equivariance. Vision Transformers provide a global attention mechanism, but\nlack translation invariance and equivariance, requiring significantly more\ntraining data samples to match generalization of CNNs. To address the loss of\nspatial information and global relations between features, while preserving the\ninductive biases of CNNs, we present a novel technique that serves as an\nauxiliary attention mechanism to existing CNN architectures, in order to\nextract global correlations between salient features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Verenich_E/0/1/0/all/0/1\">Edward Verenich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_T/0/1/0/all/0/1\">Tobias Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velasquez_A/0/1/0/all/0/1\">Alvaro Velasquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Nazar Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_F/0/1/0/all/0/1\">Faraz Hussain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active label cleaning: Improving dataset quality under resource constraints. (arXiv:2109.00574v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00574","description":"<p>Imperfections in data annotation, known as label noise, are detrimental to\nthe training of machine learning models and have an often-overlooked\nconfounding effect on the assessment of model performance. Nevertheless,\nemploying experts to remove label noise by fully re-annotating large datasets\nis infeasible in resource-constrained settings, such as healthcare. This work\nadvocates for a data-driven approach to prioritising samples for re-annotation\n- which we term \"active label cleaning\". We propose to rank instances according\nto estimated label correctness and labelling difficulty of each sample, and\nintroduce a simulation framework to evaluate relabelling efficacy. Our\nexperiments on natural images and on a new medical imaging benchmark show that\ncleaning noisy labels mitigates their negative impact on model training,\nevaluation, and selection. Crucially, the proposed active label cleaning\nenables correcting labels up to 4 times more effectively than typical random\nselection in realistic conditions, making better use of experts' valuable time\nfor improving dataset quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bernhardt_M/0/1/0/all/0/1\">Melanie Bernhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Daniel C. Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanno_R/0/1/0/all/0/1\">Ryutaro Tanno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwaighofer_A/0/1/0/all/0/1\">Anton Schwaighofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tezcan_K/0/1/0/all/0/1\">Kerem C. Tezcan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monteiro_M/0/1/0/all/0/1\">Miguel Monteiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bannur_S/0/1/0/all/0/1\">Shruthi Bannur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nori_A/0/1/0/all/0/1\">Aditya Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Valle_J/0/1/0/all/0/1\">Javier Alvarez-Valle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oktay_O/0/1/0/all/0/1\">Ozan Oktay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00590","description":"<p>Web search is fundamentally multimodal and multihop. Often, even before\nasking a question we choose to go directly to image search to find our answers.\nFurther, rarely do we find an answer from a single source but aggregate\ninformation and reason through implications. Despite the frequency of this\neveryday occurrence, at present, there is no unified question answering\nbenchmark that requires a single model to answer long-form natural language\nquestions from text and open-ended visual sources -- akin to a human's\nexperience. We propose to bridge this gap between the natural language and\ncomputer vision communities with WebQA. We show that A. our multihop text\nqueries are difficult for a large-scale transformer model, and B. existing\nmulti-modal transformers and visual representations do not perform well on\nopen-domain visual queries. Our challenge for the community is to create a\nunified multimodal reasoning model that seamlessly transitions and reasons\nregardless of the source modality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yingshan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_M/0/1/0/all/0/1\">Mridu Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_H/0/1/0/all/0/1\">Hisami Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Guihong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An End-to-End learnable Flow Regularized Model for Brain Tumor Segmentation. (arXiv:2109.00622v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00622","description":"<p>Many segmentation tasks for biomedical images can be modeled as the\nminimization of an energy function and solved by a class of max-flow and\nmin-cut optimization algorithms. However, the segmentation accuracy is\nsensitive to the contrasting of semantic features of different segmenting\nobjects, as the traditional energy function usually uses hand-crafted features\nin their energy functions. To address these limitations, we propose to\nincorporate end-to-end trainable neural network features into the energy\nfunctions. Our deep neural network features are extracted from the\ndown-sampling and up-sampling layers with skip-connections of a U-net. In the\ninference stage, the learned features are fed into the energy functions. And\nthe segmentations are solved in a primal-dual form by ADMM solvers. In the\ntraining stage, we train our neural networks by optimizing the energy function\nin the primal form with regularizations on the min-cut and flow-conservation\nfunctions, which are derived from the optimal conditions in the dual form. We\nevaluate our methods, both qualitatively and quantitatively, in a brain tumor\nsegmentation task. As the energy minimization model achieves a balance on\nsensitivity and smooth boundaries, we would show how our segmentation contours\nevolve actively through iterations as ensemble references for doctor diagnosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhanghexuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingchen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Field-Based Plot Extraction Using UAV RGB Images. (arXiv:2109.00632v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00632","description":"<p>Unmanned Aerial Vehicles (UAVs) have become popular for use in plant\nphenotyping of field based crops, such as maize and sorghum, due to their\nability to acquire high resolution data over field trials. Field experiments,\nwhich may comprise thousands of plants, are planted according to experimental\ndesigns to evaluate varieties or management practices. For many types of\nphenotyping analysis, we examine smaller groups of plants known as \"plots.\" In\nthis paper, we propose a new plot extraction method that will segment a UAV\nimage into plots. We will demonstrate that our method achieves higher plot\nextraction accuracy than existing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Changye Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baireddy_S/0/1/0/all/0/1\">Sriram Baireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_E/0/1/0/all/0/1\">Enyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crawford_M/0/1/0/all/0/1\">Melba Crawford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delp_E/0/1/0/all/0/1\">Edward J. Delp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for Efficient Multi-Stage Vision Transformers. (arXiv:2109.00642v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00642","description":"<p>Vision Transformer (ViT) demonstrates that Transformer for natural language\nprocessing can be applied to computer vision tasks and result in comparable\nperformance to convolutional neural networks (CNN), which have been studied and\nadopted in computer vision for years. This naturally raises the question of how\nthe performance of ViT can be advanced with design techniques of CNN. To this\nend, we propose to incorporate two techniques and present ViT-ResNAS, an\nefficient multi-stage ViT architecture designed with neural architecture search\n(NAS). First, we propose residual spatial reduction to decrease sequence\nlengths for deeper layers and utilize a multi-stage architecture. When reducing\nlengths, we add skip connections to improve performance and stabilize training\ndeeper networks. Second, we propose weight-sharing NAS with multi-architectural\nsampling. We enlarge a network and utilize its sub-networks to define a search\nspace. A super-network covering all sub-networks is then trained for fast\nevaluation of their performance. To efficiently train the super-network, we\npropose to sample and train multiple sub-networks with one forward-backward\npass. After that, evolutionary search is performed to discover high-performance\nnetwork architectures. Experiments on ImageNet demonstrate that ViT-ResNAS\nachieves better accuracy-MACs and accuracy-throughput trade-offs than the\noriginal DeiT and other strong baselines of ViT. Code is available at\nhttps://github.com/yilunliao/vit-search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yi-Lun Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karaman_S/0/1/0/all/0/1\">Sertac Karaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sze_V/0/1/0/all/0/1\">Vivienne Sze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dash: Semi-Supervised Learning with Dynamic Thresholding. (arXiv:2109.00650v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00650","description":"<p>While semi-supervised learning (SSL) has received tremendous attentions in\nmany machine learning tasks due to its successful use of unlabeled data,\nexisting SSL algorithms use either all unlabeled examples or the unlabeled\nexamples with a fixed high-confidence prediction during the training progress.\nHowever, it is possible that too many correct/wrong pseudo labeled examples are\neliminated/selected. In this work we develop a simple yet powerful framework,\nwhose key idea is to select a subset of training examples from the unlabeled\ndata when performing existing SSL methods so that only the unlabeled examples\nwith pseudo labels related to the labeled data will be used to train models.\nThe selection is performed at each updating iteration by only keeping the\nexamples whose losses are smaller than a given threshold that is dynamically\nadjusted through the iteration. Our proposed approach, Dash, enjoys its\nadaptivity in terms of unlabeled data selection and its theoretical guarantee.\nSpecifically, we theoretically establish the convergence rate of Dash from the\nview of non-convex optimization. Finally, we empirically demonstrate the\neffectiveness of the proposed method in comparison with state-of-the-art over\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lei Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jinxing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1\">Qi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu-Feng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baigui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variable Augmented Network for Invertible Modality Synthesis-Fusion. (arXiv:2109.00670v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00670","description":"<p>As an effective way to integrate the information contained in multiple\nmedical images under different modalities, medical image synthesis and fusion\nhave emerged in various clinical applications such as disease diagnosis and\ntreatment planning. In this paper, an invertible and variable augmented network\n(iVAN) is proposed for medical image synthesis and fusion. In iVAN, the channel\nnumber of the network input and output is the same through variable\naugmentation technology, and data relevance is enhanced, which is conducive to\nthe generation of characterization information. Meanwhile, the invertible\nnetwork is used to achieve the bidirectional inference processes. Due to the\ninvertible and variable augmentation schemes, iVAN can not only be applied to\nthe mappings of multi-input to one-output and multi-input to multi-output, but\nalso be applied to one-input to multi-output. Experimental results demonstrated\nthat the proposed method can obtain competitive or superior performance in\ncomparison to representative medical image synthesis and fusion methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruirui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cailian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regional Adversarial Training for Better Robust Generalization. (arXiv:2109.00678v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00678","description":"<p>Adversarial training (AT) has been demonstrated as one of the most promising\ndefense methods against various adversarial attacks. To our knowledge, existing\nAT-based methods usually train with the locally most adversarial perturbed\npoints and treat all the perturbed points equally, which may lead to\nconsiderably weaker adversarial robust generalization on test data. In this\nwork, we introduce a new adversarial training framework that considers the\ndiversity as well as characteristics of the perturbed points in the vicinity of\nbenign samples. To realize the framework, we propose a Regional Adversarial\nTraining (RAT) defense method that first utilizes the attack path generated by\nthe typical iterative attack method of projected gradient descent (PGD), and\nconstructs an adversarial region based on the attack path. Then, RAT samples\ndiverse perturbed training points efficiently inside this region, and utilizes\na distance-aware label smoothing mechanism to capture our intuition that\nperturbed points at different locations should have different impact on the\nmodel performance. Extensive experiments on several benchmark datasets show\nthat RAT consistently makes significant improvement on standard adversarial\ntraining (SAT), and exhibits better robust generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chuanbiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yanbo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yicheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Baoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Face Video Inpainting via UV Mapping. (arXiv:2109.00681v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00681","description":"<p>This paper addresses the problem of face video inpainting. Existing video\ninpainting methods target primarily at natural scenes with repetitive patterns.\nThey do not make use of any prior knowledge of the face to help retrieve\ncorrespondences for the corrupted face. They therefore only achieve sub-optimal\nresults, particularly for faces under large pose and expression variations\nwhere face components appear very differently across frames. In this paper, we\npropose a two-stage deep learning method for face video inpainting. We employ\n3DMM as our 3D face prior to transform a face between the image space and the\nUV (texture) space. In Stage I, we perform face inpainting in the UV space.\nThis helps to largely remove the influence of face poses and expressions and\nmakes the learning task much easier with well aligned face features. We\nintroduce a frame-wise attention module to fully exploit correspondences in\nneighboring frames to assist the inpainting task. In Stage II, we transform the\ninpainted face regions back to the image space and perform face video\nrefinement that inpaints any background regions not covered in Stage I and also\nrefines the inpainted face regions. Extensive experiments have been carried out\nwhich show our method can significantly outperform methods based merely on 2D\ninformation, especially for faces under large pose and expression variations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenfang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaofeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kwan-Yee K. Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AnANet: Modeling Association and Alignment for Cross-modal Correlation Classification. (arXiv:2109.00693v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00693","description":"<p>The explosive increase of multimodal data makes a great demand in many\ncross-modal applications that follow the strict prior related assumption. Thus\nresearchers study the definition of cross-modal correlation category and\nconstruct various classification systems and predictive models. However, those\nsystems pay more attention to the fine-grained relevant types of cross-modal\ncorrelation, ignoring lots of implicit relevant data which are often divided\ninto irrelevant types. What's worse is that none of previous predictive models\nmanifest the essence of cross-modal correlation according to their definition\nat the modeling stage. In this paper, we present a comprehensive analysis of\nthe image-text correlation and redefine a new classification system based on\nimplicit association and explicit alignment. To predict the type of image-text\ncorrelation, we propose the Association and Alignment Network according to our\nproposed definition (namely AnANet) which implicitly represents the global\ndiscrepancy and commonality between image and text and explicitly captures the\ncross-modal local relevance. The experimental results on our constructed new\nimage-text correlation dataset show the effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Nan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruike Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Wenji Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FBSNet: A Fast Bilateral Symmetrical Network for Real-Time Semantic Segmentation. (arXiv:2109.00699v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00699","description":"<p>Real-time semantic segmentation, which can be visually understood as the\npixel-level classification task on the input image, currently has broad\napplication prospects, especially in the fast-developing fields of autonomous\ndriving and drone navigation. However, the huge burden of calculation together\nwith redundant parameters are still the obstacles to its technological\ndevelopment. In this paper, we propose a Fast Bilateral Symmetrical Network\n(FBSNet) to alleviate the above challenges. Specifically, FBSNet employs a\nsymmetrical encoder-decoder structure with two branches, semantic information\nbranch, and spatial detail branch. The semantic information branch is the main\nbranch with deep network architecture to acquire the contextual information of\nthe input image and meanwhile acquire sufficient receptive field. While spatial\ndetail branch is a shallow and simple network used to establish local\ndependencies of each pixel for preserving details, which is essential for\nrestoring the original resolution during the decoding phase. Meanwhile, a\nfeature aggregation module (FAM) is designed to effectively combine the output\nfeatures of the two branches. The experimental results of Cityscapes and CamVid\nshow that the proposed FBSNet can strike a good balance between accuracy and\nefficiency. Specifically, it obtains 70.9\\% and 68.9\\% mIoU along with the\ninference speed of 90 fps and 120 fps on these two test datasets, respectively,\nwith only 0.62 million parameters on a single RTX 2080Ti GPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Guangwei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guoan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huimin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning 3D Mineral Prospectivity from 3D Geological Models with Convolutional Neural Networks: Application to a Structure-controlled Hydrothermal Gold Deposit. (arXiv:2109.00756v1 [physics.geo-ph])","link":"http://arxiv.org/abs/2109.00756","description":"<p>The three-dimensional (3D) geological models are the typical and key data\nsource in the 3D mineral prospecitivity modeling. Identifying\nprospectivity-informative predictor variables from the 3D geological models is\na challenging and tedious task. Motivated by the ability of convolutional\nneural networks (CNNs) to learn the intrinsic features, in this paper, we\npresent a novel method that leverages CNNs to learn 3D mineral prospectivity\nfrom the 3D geological models. By exploiting the learning ability of CNNs, the\npresented method allows for disentangling complex correlation to the\nmineralization and thus opens a door to circumvent the tedious work for\ndesigning the predictor variables. Specifically, to explore the unstructured 3D\ngeological models with the CNNs whose input should be structured, we develop a\n2D CNN framework in which the geometry of geological boundary is compiled and\nreorganized into multi-channel images and fed into the CNN. This ensures an\neffective and efficient training of CNNs while allowing the prospective model\nto approximate the ore-forming process. The presented method is applied to a\ntypical structure-controlled hydrothermal deposit, the Dayingezhuang gold\ndeposit, eastern China, in which the presented method was compared with the\nprospectivity modeling methods using hand-designed predictor variables. The\nresults demonstrate the presented method capacitates a performance boost of the\n3D prospectivity modeling and empowers us to decrease work-load and prospecting\nrisk in prediction of deep-seated orebodies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Deng_H/0/1/0/all/0/1\">Hao Deng</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zheng_Y/0/1/0/all/0/1\">Yang Zheng</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chen_J/0/1/0/all/0/1\">Jin Chen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yu_S/0/1/0/all/0/1\">Shuyan Yu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Xiao_K/0/1/0/all/0/1\">Keyan Xiao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mao_X/0/1/0/all/0/1\">Xiancheng Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct PET Image Reconstruction Incorporating Deep Image Prior and a Forward Projection Model. (arXiv:2109.00768v1 [physics.med-ph])","link":"http://arxiv.org/abs/2109.00768","description":"<p>Convolutional neural networks (CNNs) have recently achieved remarkable\nperformance in positron emission tomography (PET) image reconstruction. In\nparticular, CNN-based direct PET image reconstruction, which directly generates\nthe reconstructed image from the sinogram, has potential applicability to PET\nimage enhancements because it does not require image reconstruction algorithms,\nwhich often produce some artifacts. However, these deep learning-based, direct\nPET image reconstruction algorithms have the disadvantage that they require a\nlarge number of high-quality training datasets. In this study, we propose an\nunsupervised direct PET image reconstruction method that incorporates a deep\nimage prior framework. Our proposed method incorporates a forward projection\nmodel with a loss function to achieve unsupervised direct PET image\nreconstruction from sinograms. To compare our proposed direct reconstruction\nmethod with the filtered back projection (FBP) and maximum likelihood\nexpectation maximization (ML-EM) algorithms, we evaluated using Monte Carlo\nsimulation data of brain [$^{18}$F]FDG PET scans. The results demonstrate that\nour proposed direct reconstruction quantitatively and qualitatively outperforms\nthe FBP and ML-EM algorithms with respect to peak signal-to-noise ratio and\nstructural similarity index.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Hashimoto_F/0/1/0/all/0/1\">Fumio Hashimoto</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ote_K/0/1/0/all/0/1\">Kibo Ote</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Self-training for Image Classification through Self-supervision. (arXiv:2109.00778v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00778","description":"<p>Self-training is a simple semi-supervised learning approach: Unlabelled\nexamples that attract high-confidence predictions are labelled with their\npredictions and added to the training set, with this process being repeated\nmultiple times. Recently, self-supervision -- learning without manual\nsupervision by solving an automatically-generated pretext task -- has gained\nprominence in deep learning. This paper investigates three different ways of\nincorporating self-supervision into self-training to improve accuracy in image\nclassification: self-supervision as pretraining only, self-supervision\nperformed exclusively in the first iteration of self-training, and\nself-supervision added to every iteration of self-training. Empirical results\non the SVHN, CIFAR-10, and PlantVillage datasets, using both training from\nscratch, and Imagenet-pretrained weights, show that applying self-supervision\nonly in the first iteration of self-training can greatly improve accuracy, for\na modest increase in computation time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahito_A/0/1/0/all/0/1\">Attaullah Sahito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_E/0/1/0/all/0/1\">Eibe Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfahringer_B/0/1/0/all/0/1\">Bernhard Pfahringer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Photorealistic Rendering of Layered Materials: A Multispectral Approach. (arXiv:2109.00780v1 [cs.GR])","link":"http://arxiv.org/abs/2109.00780","description":"<p>We present multispectral rendering techniques for visualizing layered\nmaterials found in biological specimens. We are the first to use acquired data\nfrom the near-infrared and ultraviolet spectra for non-photorealistic rendering\n(NPR). Several plant and animal species are more comprehensively understood by\nmultispectral analysis. However, traditional NPR techniques ignore unique\ninformation outside the visible spectrum. We introduce algorithms and\nprinciples for processing wavelength dependent surface normals and reflectance.\nOur registration and feature detection methods are used to formulate\nstylization effects not considered by current NPR methods including: Spectral\nBand Shading which isolates and emphasizes shape features at specific\nwavelengths at multiple scales. Experts in our user study demonstrate the\neffectiveness of our system for applications in the biological sciences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toler_Franklin_C/0/1/0/all/0/1\">Corey Toler-Franklin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_S/0/1/0/all/0/1\">Shashank Ranjan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer of Pretrained Model Weights Substantially Improves Semi-Supervised Image Classification. (arXiv:2109.00788v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00788","description":"<p>Deep neural networks produce state-of-the-art results when trained on a large\nnumber of labeled examples but tend to overfit when small amounts of labeled\nexamples are used for training. Creating a large number of labeled examples\nrequires considerable resources, time, and effort. If labeling new data is not\nfeasible, so-called semi-supervised learning can achieve better generalisation\nthan purely supervised learning by employing unlabeled instances as well as\nlabeled ones. The work presented in this paper is motivated by the observation\nthat transfer learning provides the opportunity to potentially further improve\nperformance by exploiting models pretrained on a similar domain. More\nspecifically, we explore the use of transfer learning when performing\nsemi-supervised learning using self-learning. The main contribution is an\nempirical evaluation of transfer learning using different combinations of\nsimilarity metric learning methods and label propagation algorithms in\nsemi-supervised learning. We find that transfer learning always substantially\nimproves the model's accuracy when few labeled examples are available,\nregardless of the type of loss used for training the neural network. This\nfinding is obtained by performing extensive experiments on the SVHN, CIFAR10,\nand Plant Village image classification datasets and applying pretrained weights\nfrom Imagenet for transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahito_A/0/1/0/all/0/1\">Attaullah Sahito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_E/0/1/0/all/0/1\">Eibe Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfahringer_B/0/1/0/all/0/1\">Bernhard Pfahringer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Learning using Siamese Networks. (arXiv:2109.00794v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00794","description":"<p>Neural networks have been successfully used as classification models yielding\nstate-of-the-art results when trained on a large number of labeled samples.\nThese models, however, are more difficult to train successfully for\nsemi-supervised problems where small amounts of labeled instances are available\nalong with a large number of unlabeled instances. This work explores a new\ntraining method for semi-supervised learning that is based on similarity\nfunction learning using a Siamese network to obtain a suitable embedding. The\nlearned representations are discriminative in Euclidean space, and hence can be\nused for labeling unlabeled instances using a nearest-neighbor classifier.\nConfident predictions of unlabeled instances are used as true labels for\nretraining the Siamese network on the expanded training set. This process is\napplied iteratively. We perform an empirical study of this iterative\nself-training algorithm. For improving unlabeled predictions, local learning\nwith global consistency [22] is also evaluated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahito_A/0/1/0/all/0/1\">Attaullah Sahito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_E/0/1/0/all/0/1\">Eibe Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfahringer_B/0/1/0/all/0/1\">Bernhard Pfahringer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Zero-Shot Sign Language Recognition. (arXiv:2109.00796v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00796","description":"<p>Zero-Shot Learning (ZSL) has rapidly advanced in recent years. Towards\novercoming the annotation bottleneck in the Sign Language Recognition (SLR), we\nexplore the idea of Zero-Shot Sign Language Recognition (ZS-SLR) with no\nannotated visual examples, by leveraging their textual descriptions. In this\nway, we propose a multi-modal Zero-Shot Sign Language Recognition (ZS-SLR)\nmodel harnessing from the complementary capabilities of deep features fused\nwith the skeleton-based ones. A Transformer-based model along with a C3D model\nis used for hand detection and deep features extraction, respectively. To make\na trade-off between the dimensionality of the skeletonbased and deep features,\nwe use an Auto-Encoder (AE) on top of the Long Short Term Memory (LSTM)\nnetwork. Finally, a semantic space is used to map the visual features to the\nlingual embedding of the class labels, achieved via the Bidirectional Encoder\nRepresentations from Transformers (BERT) model. Results on four large-scale\ndatasets, RKS-PERSIANSIGN, First-Person, ASLVID, and isoGD, show the\nsuperiority of the proposed model compared to state-of-the-art alternatives in\nZS-SLR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rastgoo_R/0/1/0/all/0/1\">Razieh Rastgoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiani_K/0/1/0/all/0/1\">Kourosh Kiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabokrou_M/0/1/0/all/0/1\">Mohammad Sabokrou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anatomical-Guided Attention Enhances Unsupervised PET Image Denoising Performance. (arXiv:2109.00802v1 [physics.med-ph])","link":"http://arxiv.org/abs/2109.00802","description":"<p>Although supervised convolutional neural networks (CNNs) often outperform\nconventional alternatives for denoising positron emission tomography (PET)\nimages, they require many low- and high-quality reference PET image pairs.\nHerein, we propose an unsupervised 3D PET image denoising method based on\nanatomical information-guided attention mechanism. Our proposed magnetic\nresonance-guided deep decoder (MR-GDD) utilizes the spatial details and\nsemantic features of MR-guidance image more effectively by introducing\nencoder-decoder and deep decoder subnetworks. Moreover, the specific shapes and\npatterns of the guidance image do not affect the denoised PET image, because\nthe guidance image is input to the network through an attention gate. Monte\nCarlo simulation using the [$^{18}$F]fluoro-2-deoxy-D-glucose (FDG) shows that\nthe proposed method outperforms other denoising algorithms in terms of the\nhighest peak signal-to-noise ratio and structural similarity (28.33 dB/0.886).\nFurthermore, we experimentally visualized the behavior of the optimization\nprocess, which is often unknown in unsupervised CNN-based restoration problems.\nFor preclinical (using [$^{18}$F]FDG and [$^{11}$C]raclopride) and clinical\n(using [$^{18}$F]florbetapir) studies, the proposed method demonstrates\nstate-of-the-art denoising performance while retaining spatial resolution and\nquantitative accuracy, despite using only a single architecture for various\nnoisy PET images with 1/10th of the full counts. These results suggest that the\nproposed MR-GDD can reduce PET scan times and PET tracer doses considerably\nwithout impacting patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Onishi_Y/0/1/0/all/0/1\">Yuya Onishi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hashimoto_F/0/1/0/all/0/1\">Fumio Hashimoto</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ote_K/0/1/0/all/0/1\">Kibo Ote</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ohba_H/0/1/0/all/0/1\">Hiroyuki Ohba</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ota_R/0/1/0/all/0/1\">Ryosuke Ota</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yoshikawa_E/0/1/0/all/0/1\">Etsuji Yoshikawa</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ouchi_Y/0/1/0/all/0/1\">Yasuomi Ouchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Single-Shot MultiBox Detector and YOLO Deep Learning Models for the Detection of Tomatoes in a Greenhouse. (arXiv:2109.00810v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00810","description":"<p>The development of robotic solutions for agriculture requires advanced\nperception capabilities that can work reliably in any crop stage. For example,\nto automatise the tomato harvesting process in greenhouses, the visual\nperception system needs to detect the tomato in any life cycle stage (flower to\nthe ripe tomato). The state-of-the-art for visual tomato detection focuses\nmainly on ripe tomato, which has a distinctive colour from the background. This\npaper contributes with an annotated visual dataset of green and reddish\ntomatoes. This kind of dataset is uncommon and not available for research\npurposes. This will enable further developments in edge artificial intelligence\nfor in situ and in real-time visual tomato detection required for the\ndevelopment of harvesting robots. Considering this dataset, five deep learning\nmodels were selected, trained and benchmarked to detect green and reddish\ntomatoes grown in greenhouses. Considering our robotic platform specifications,\nonly the Single-Shot MultiBox Detector (SSD) and YOLO architectures were\nconsidered. The results proved that the system can detect green and reddish\ntomatoes, even those occluded by leaves. SSD MobileNet v2 had the best\nperformance when compared against SSD Inception v2, SSD ResNet 50, SSD ResNet\n101 and YOLOv4 Tiny, reaching an F1-score of 66.15%, an mAP of 51.46% and an\ninference time of 16.44 ms with the NVIDIA Turing Architecture platform, an\nNVIDIA Tesla T4, with 12 GB. YOLOv4 Tiny also had impressive results, mainly\nconcerning inferring times of about 5 ms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Magalhaes_S/0/1/0/all/0/1\">Sandro A. Magalh&#xe3;es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_L/0/1/0/all/0/1\">Lu&#xed;s Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_G/0/1/0/all/0/1\">Germano Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_F/0/1/0/all/0/1\">Filipe N. Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cunha_m/0/1/0/all/0/1\">m&#xe1;rio Cunha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dias_J/0/1/0/all/0/1\">Jorge Dias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_A/0/1/0/all/0/1\">Ant&#xf3;nio P. Moreira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Built Year Prediction from Buddha Face with Heterogeneous Labels. (arXiv:2109.00812v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00812","description":"<p>Buddha statues are a part of human culture, especially of the Asia area, and\nthey have been alongside human civilisation for more than 2,000 years. As\nhistory goes by, due to wars, natural disasters, and other reasons, the records\nthat show the built years of Buddha statues went missing, which makes it an\nimmense work for historians to estimate the built years. In this paper, we\npursue the idea of building a neural network model that automatically estimates\nthe built years of Buddha statues based only on their face images. Our model\nuses a loss function that consists of three terms: an MSE loss that provides\nthe basis for built year estimation; a KL divergence-based loss that handles\nthe samples with both an exact built year and a possible range of built years\n(e.g., dynasty or centuries) estimated by historians; finally a regularisation\nthat utilises both labelled and unlabelled samples based on manifold\nassumption. By combining those three terms in the training process, we show\nthat our method is able to estimate built years for given images with 37.5\nyears of a mean absolute error on the test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yiming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaigh_C/0/1/0/all/0/1\">Cheikh Brahim El Vaigh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1\">Yuta Nakashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renoust_B/0/1/0/all/0/1\">Benjamin Renoust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagahara_H/0/1/0/all/0/1\">Hajime Nagahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujioka_Y/0/1/0/all/0/1\">Yutaka Fujioka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning-based mitosis detection in breast cancer histologic samples. (arXiv:2109.00816v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00816","description":"<p>This is the submission for mitosis detection in the context of the MIDOG 2021\nchallenge. It is based on the two-stage objection model Faster RCNN as well as\nDenseNet as a backbone for the neural network architecture. It achieves a\nF1-score of 0.6645 on the Preliminary Test Phase Leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Halmes_M/0/1/0/all/0/1\">Michel Halmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heuberger_H/0/1/0/all/0/1\">Hippolyte Heuberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berlemont_S/0/1/0/all/0/1\">Sylvain Berlemont</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rotation Invariance and Extensive Data Augmentation: a strategy for the Mitosis Domain Generalization (MIDOG) Challenge. (arXiv:2109.00823v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00823","description":"<p>Automated detection of mitotic figures in histopathology images is a\nchallenging task: here, we present the different steps that describe the\nstrategy we applied to participate in the MIDOG 2021 competition. The purpose\nof the competition was to evaluate the generalization of solutions to images\nacquired with unseen target scanners (hidden for the participants) under the\nconstraint of using training data from a limited set of four independent source\nscanners. Given this goal and constraints, we joined the challenge by proposing\na straight-forward solution based on a combination of state-of-the-art deep\nlearning methods with the aim of yielding robustness to possible\nscanner-related distributional shifts at inference time. Our solution combines\nmethods that were previously shown to be efficient for mitosis detection: hard\nnegative mining, extensive data augmentation, rotation-invariant convolutional\nnetworks.\n</p>\n<p>We trained five models with different splits of the provided dataset. The\nsubsequent classifiers produced F1-scores with a mean and standard deviation of\n0.747+/-0.032 on the test splits. The resulting ensemble constitutes our\ncandidate algorithm: its automated evaluation on the preliminary test set of\nthe challenge returned a F1-score of 0.6828.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lafarge_M/0/1/0/all/0/1\">Maxime W. Lafarge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koelzer_V/0/1/0/all/0/1\">Viktor H. Koelzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SlowFast Rolling-Unrolling LSTMs for Action Anticipation in Egocentric Videos. (arXiv:2109.00829v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00829","description":"<p>Action anticipation in egocentric videos is a difficult task due to the\ninherently multi-modal nature of human actions. Additionally, some actions\nhappen faster or slower than others depending on the actor or surrounding\ncontext which could vary each time and lead to different predictions. Based on\nthis idea, we build upon RULSTM architecture, which is specifically designed\nfor anticipating human actions, and propose a novel attention-based technique\nto evaluate, simultaneously, slow and fast features extracted from three\ndifferent modalities, namely RGB, optical flow, and extracted objects. Two\nbranches process information at different time scales, i.e., frame-rates, and\nseveral fusion schemes are considered to improve prediction accuracy. We\nperform extensive experiments on EpicKitchens-55 and EGTEA Gaze+ datasets, and\ndemonstrate that our technique systematically improves the results of RULSTM\narchitecture for Top-5 accuracy metric at different anticipation times.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Osman_N/0/1/0/all/0/1\">Nada Osman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camporese_G/0/1/0/all/0/1\">Guglielmo Camporese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coscia_P/0/1/0/all/0/1\">Pasquale Coscia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballan_L/0/1/0/all/0/1\">Lamberto Ballan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stain-Robust Mitotic Figure Detection for the Mitosis Domain Generalization Challenge. (arXiv:2109.00853v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00853","description":"<p>The detection of mitotic figures from different scanners/sites remains an\nimportant topic of research, owing to its potential in assisting clinicians\nwith tumour grading. The MItosis DOmain Generalization (MIDOG) challenge aims\nto test the robustness of detection models on unseen data from multiple\nscanners for this task. We present a short summary of the approach employed by\nthe TIA Centre team to address this challenge. Our approach is based on a\nhybrid detection model, where mitotic candidates are segmented on stain\nnormalised images, before being refined by a deep learning classifier.\nCross-validation on the training images achieved the F1-score of 0.786 and\n0.765 on the preliminary test set, demonstrating the generalizability of our\nmodel to unseen data from new scanners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jahanifar_M/0/1/0/all/0/1\">Mostafa Jahanifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shephard_A/0/1/0/all/0/1\">Adam Shephard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajeddin_N/0/1/0/all/0/1\">Neda Zamani Tajeddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bashir_R/0/1/0/all/0/1\">R.M. Saad Bashir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilal_M/0/1/0/all/0/1\">Mohsin Bilal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khurram_S/0/1/0/all/0/1\">Syed Ali Khurram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minhas_F/0/1/0/all/0/1\">Fayyaz Minhas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Models for Multi-Illumination Color Constancy. (arXiv:2109.00863v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00863","description":"<p>In this paper, the aim is multi-illumination color constancy. However, most\nof the existing color constancy methods are designed for single light sources.\nFurthermore, datasets for learning multiple illumination color constancy are\nlargely missing. We propose a seed (physics driven) based multi-illumination\ncolor constancy method. GANs are exploited to model the illumination estimation\nproblem as an image-to-image domain translation problem. Additionally, a novel\nmulti-illumination data augmentation method is proposed. Experiments on single\nand multi-illumination datasets show that our methods outperform sota methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Partha Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karaoglu_S/0/1/0/all/0/1\">Sezer Karaoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gevers_T/0/1/0/all/0/1\">Theo Gevers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real World Robustness from Systematic Noise. (arXiv:2109.00864v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00864","description":"<p>Systematic error, which is not determined by chance, often refers to the\ninaccuracy (involving either the observation or measurement process) inherent\nto a system. In this paper, we exhibit some long-neglected but\nfrequent-happening adversarial examples caused by systematic error. More\nspecifically, we find the trained neural network classifier can be fooled by\ninconsistent implementations of image decoding and resize. This tiny difference\nbetween these implementations often causes an accuracy drop from training to\ndeployment. To benchmark these real-world adversarial examples, we propose\nImageNet-S dataset, which enables researchers to measure a classifier's\nrobustness to systematic error. For example, we find a normal ResNet-50 trained\non ImageNet can have 1%-5% accuracy difference due to the systematic error.\nTogether our evaluation and dataset may aid future work toward real-world\nrobustness and practical generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ruihao Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing domain adaptation techniques for mitosis detection in multi-scanner breast cancer histopathology images. (arXiv:2109.00869v1 [eess.IV])","link":"http://arxiv.org/abs/2109.00869","description":"<p>Breast cancer is the most prevalent cancer worldwide and over two million new\ncases are diagnosed each year. As part of the tumour grading process,\nhistopathologists manually count how many cells are dividing, in a biological\nprocess called mitosis. Artificial intelligence (AI) methods have been\ndeveloped to automatically detect mitotic figures, however these methods often\nperform poorly when applied to data from outside of the original (training)\ndomain, i.e. they do not generalise well to histology images created using\nvaried staining protocols or digitised using different scanners. Style\ntransfer, a form of domain adaptation, provides the means to transform images\nfrom different domains to a shared visual appearance and have been adopted in\nvarious applications to mitigate the issue of domain shift. In this paper we\ntrain two mitosis detection models and two style transfer methods and evaluate\nthe usefulness of the latter for improving mitosis detection performance in\nimages digitised using different scanners. We found that the best of these\nmodels, U-Net without style transfer, achieved an F1-score of 0.693 on the\nMIDOG 2021 preliminary test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Breen_J/0/1/0/all/0/1\">Jack Breen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zucker_K/0/1/0/all/0/1\">Kieran Zucker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Orsi_N/0/1/0/all/0/1\">Nicolas Orsi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hall_G/0/1/0/all/0/1\">Geoff Hall</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DVM-CAR: A large-scale automotive dataset for visual marketing research and applications. (arXiv:2109.00881v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00881","description":"<p>The automotive industry is being transformed by technologies, applications\nand services ranging from sensors to big data analytics and to artificial\nintelligence. In this paper, we present our multidisciplinary initiative of\ncreating a publicly available dataset to facilitate the visual-related\nmarketing research and applications in automotive industry such as automotive\nexterior design, consumer analytics and sales modelling. We are motivated by\nthe fact that there is growing interest in product aesthetics but there is no\nlarge-scale dataset available that covers a wide range of variables and\ninformation. We summarise the common issues faced by marketing researchers and\ncomputer scientists through a user survey study, and design our dataset to\nalleviate these issues. Our dataset contains 1.4 million images from 899 car\nmodels as well as their corresponding car model specification and sales\ninformation over more than ten years in the UK market. To the best of our\nknowledge, this is the very first large-scale automotive dataset which contains\nimages, text and sales information from multiple sources over a long period of\ntime. We describe the detailed data structure and the preparation steps, which\nwe believe has the methodological contribution to the multi-source data fusion\nand sharing. In addition, we discuss three dataset application examples to\nillustrate the value of our dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bowei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Lan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1\">Shigang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ounis_I/0/1/0/all/0/1\">Iadh Ounis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOON: Multi-Hash Codes Joint Learning for Cross-Media Retrieval. (arXiv:2109.00883v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00883","description":"<p>In recent years, cross-media hashing technique has attracted increasing\nattention for its high computation efficiency and low storage cost. However,\nthe existing approaches still have some limitations, which need to be explored.\n1) A fixed hash length (e.g., 16bits or 32bits) is predefined before learning\nthe binary codes. Therefore, these models need to be retrained when the hash\nlength changes, that consumes additional computation power, reducing the\nscalability in practical applications. 2) Existing cross-modal approaches only\nexplore the information in the original multimedia data to perform the hash\nlearning, without exploiting the semantic information contained in the learned\nhash codes. To this end, we develop a novel Multiple hash cOdes jOint learNing\nmethod (MOON) for cross-media retrieval. Specifically, the developed MOON\nsynchronously learns the hash codes with multiple lengths in a unified\nframework. Besides, to enhance the underlying discrimination, we combine the\nclues from the multimodal data, semantic labels and learned hash codes for hash\nlearning. As far as we know, the proposed MOON is the first work to\nsimultaneously learn different length hash codes without retraining in\ncross-media retrieval. Experiments on several databases show that our MOON can\nachieve promising performance, outperforming some recent competitive shallow\nand deep methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Donglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">He-Feng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1\">Josef Kittler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tracking Hand Hygiene Gestures with Leap Motion Controller. (arXiv:2109.00884v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00884","description":"<p>The process of hand washing, according to the WHO, is divided into stages\nwith clearly defined two handed dynamic gestures. In this paper, videos of hand\nwashing experts are segmented and analyzed with the goal of extracting their\ncorresponding features. These features can be further processed in software to\nclassify particular hand movements, determine whether the stages have been\nsuccessfully completed by the user and also assess the quality of washing.\nHaving identified the important features, a 3D gesture tracker, the Leap Motion\nController (LEAP), was used to track and detect the hand features associated\nwith these stages. With the help of sequential programming and threshold\nvalues, the hand features were combined together to detect the initiation and\ncompletion of a sample WHO Stage 2 (Rub hands Palm to Palm). The LEAP provides\naccurate raw positional data for tracking single hand gestures and two hands in\nseparation but suffers from occlusion when hands are in contact. Other than\nhand hygiene the approaches shown here can be applied in other biomedical\napplications requiring close hand gesture analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1\">Rashmi Bakshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courtney_J/0/1/0/all/0/1\">Jane Courtney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berry_D/0/1/0/all/0/1\">Damon Berry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavin_G/0/1/0/all/0/1\">Graham Gavin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Learning for Target Tracking and Background Subtraction in Satellite Imagery. (arXiv:2109.00885v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00885","description":"<p>This paper describes an unsupervised machine learning methodology capable of\ntarget tracking and background suppression via a novel dual-model approach.\n``Jekyll`` produces a video bit-mask describing an estimate of the locations of\nmoving objects, and ``Hyde`` outputs a pseudo-background frame to subtract from\nthe original input image sequence. These models were trained with a\ncustom-modified version of Cross Entropy Loss.\n</p>\n<p>Simulated data were used to compare the performance of Jekyll and Hyde\nagainst a more traditional supervised Machine Learning approach. The results\nfrom these comparisons show that the unsupervised methods developed are\ncompetitive in output quality with supervised techniques, without the\nassociated cost of acquiring labeled training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kent_J/0/1/0/all/0/1\">Jonathan S. Kent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wamsley_C/0/1/0/all/0/1\">Charles C. Wamsley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flateau_D/0/1/0/all/0/1\">Davin Flateau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferguson_A/0/1/0/all/0/1\">Amber Ferguson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrast Limited Adaptive Histogram Equalization (CLAHE) Approach for Enhancement of the Microstructures of Friction Stir Welded Joints. (arXiv:2109.00886v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00886","description":"<p>Image processing algorithms are finding various applications in manufacturing\nand materials industries such as identification of cracks in the fabricated\nsamples, calculating the geometrical properties of the given microstructure,\npresence of surface defects, etc. The present work deals with the application\nof Contrast Limited Adaptive Histogram Equalization (CLAHE) algorithm for\nimproving the quality of the microstructure images of the Friction Stir Welded\njoints. The obtained results showed that the obtained value of quantitative\nmetric features such as Entropy value and RMS Contrast value were high which\nresulted in enhanced microstructure images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Akshansh Mishra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dealing with Distribution Mismatch in Semi-supervised Deep Learning for Covid-19 Detection Using Chest X-ray Images: A Novel Approach Using Feature Densities. (arXiv:2109.00889v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00889","description":"<p>In the context of the global coronavirus pandemic, different deep learning\nsolutions for infected subject detection using chest X-ray images have been\nproposed. However, deep learning models usually need large labelled datasets to\nbe effective. Semi-supervised deep learning is an attractive alternative, where\nunlabelled data is leveraged to improve the overall model's accuracy. However,\nin real-world usage settings, an unlabelled dataset might present a different\ndistribution than the labelled dataset (i.e. the labelled dataset was sampled\nfrom a target clinic and the unlabelled dataset from a source clinic). This\nresults in a distribution mismatch between the unlabelled and labelled\ndatasets. In this work, we assess the impact of the distribution mismatch\nbetween the labelled and the unlabelled datasets, for a semi-supervised model\ntrained with chest X-ray images, for COVID-19 detection. Under strong\ndistribution mismatch conditions, we found an accuracy hit of almost 30\\%,\nsuggesting that the unlabelled dataset distribution has a strong influence in\nthe behaviour of the model. Therefore, we propose a straightforward approach to\ndiminish the impact of such distribution mismatch. Our proposed method uses a\ndensity approximation of the feature space. It is built upon the target dataset\nto filter out the observations in the source unlabelled dataset that might harm\nthe accuracy of the semi-supervised model. It assumes that a small labelled\nsource dataset is available together with a larger source unlabelled dataset.\nOur proposed method does not require any model training, it is simple and\ncomputationally cheap. We compare our proposed method against two popular state\nof the art out-of-distribution data detectors, which are also cheap and simple\nto implement. In our tests, our method yielded accuracy gains of up to 32\\%,\nwhen compared to the previous state of the art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Calderon_Ramirez_S/0/1/0/all/0/1\">Saul Calderon-Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shengxiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elizondo_D/0/1/0/all/0/1\">David Elizondo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moemeni_A/0/1/0/all/0/1\">Armaghan Moemeni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-World Application of Various Trajectory Planning Algorithms on MIT RACECAR. (arXiv:2109.00890v1 [cs.RO])","link":"http://arxiv.org/abs/2109.00890","description":"<p>In the project, the vehicle was first controlled with ROS. For this purpose,\nthe necessary nodes were prepared to be controlled with a joystick. Afterwards,\nDWA(Dynamic Window Approach), TEB(Timed-Elastic Band) and APF(Artificial\nPotential Field) path planning algorithms were applied to MIT RACECAR,\nrespectively. These algorithms have advantages and disadvantages against each\nother on different issues. For this reason, a scenario was created to compare\nalgorithms. On a curved double lane road created according to this scenario,\nMIT RACECAR has to follow the lanes and when it encounters an obstacle, it has\nto change lanes without leaving the road and pass without hitting the obstacle.\nIn addition, an image processing algorithm was developed to obtain the position\ninformation of the lanes needed to implement this scenario. This algorithm\ndetects the target point by processing the image taken from the ZED camera and\ngives the target point information to the path planning algorithm.\n</p>\n<p>After the necessary tools were created, the algorithms were tested against\nthe scenario. In these tests, measurements such as how many obstacles the\nalgorithm successfully passed, how simple routes it chose, and computational\ncosts they have. According to these results, although it was not the algorithm\nthat successfully passed the most obstacles, APF was chosen due to its low\nprocessing load and simple working logic. It was believed that with its\nuncomplicated structure, APF would also provide advantages in the future stages\nof the project.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kose_O/0/1/0/all/0/1\">Oguzhan Kose</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Fine-grained Image Classification with Generative Adversarial Networks and Facial Landmark Detection. (arXiv:2109.00891v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00891","description":"<p>Fine-grained classification remains a challenging task because distinguishing\ncategories needs learning complex and local differences. Diversity in the pose,\nscale, and position of objects in an image makes the problem even more\ndifficult. Although the recent Vision Transformer models achieve high\nperformance, they need an extensive volume of input data. To encounter this\nproblem, we made the best use of GAN-based data augmentation to generate extra\ndataset instances. Oxford-IIIT Pets was our dataset of choice for this\nexperiment. It consists of 37 breeds of cats and dogs with variations in scale,\nposes, and lighting, which intensifies the difficulty of the classification\ntask. Furthermore, we enhanced the performance of the recent Generative\nAdversarial Network (GAN), StyleGAN2-ADA model to generate more realistic\nimages while preventing overfitting to the training set. We did this by\ntraining a customized version of MobileNetV2 to predict animal facial\nlandmarks; then, we cropped images accordingly. Lastly, we combined the\nsynthetic images with the original dataset and compared our proposed method\nwith standard GANs augmentation and no augmentation with different subsets of\ntraining data. We validated our work by evaluating the accuracy of fine-grained\nimage classification on the recent Vision Transformer (ViT) Model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Darvish_M/0/1/0/all/0/1\">Mahdi Darvish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pouramini_M/0/1/0/all/0/1\">Mahsa Pouramini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahador_H/0/1/0/all/0/1\">Hamid Bahador</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KITTI-CARLA: a KITTI-like dataset generated by CARLA Simulator. (arXiv:2109.00892v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00892","description":"<p>KITTI-CARLA is a dataset built from the CARLA v0.9.10 simulator using a\nvehicle with sensors identical to the KITTI dataset. The vehicle thus has a\nVelodyne HDL64 LiDAR positioned in the middle of the roof and two color cameras\nsimilar to Point Grey Flea 2. The positions of the LiDAR and cameras are the\nsame as the setup used in KITTI. The objective of this dataset is to test\napproaches of semantic segmentation LiDAR and/or images, odometry LiDAR and/or\nimage in synthetic data and to compare with the results obtained on real data\nlike KITTI. This dataset thus makes it possible to improve transfer learning\nmethods from a synthetic dataset to a real dataset. We created 7 sequences with\n5000 frames in each sequence in the 7 maps of CARLA providing different\nenvironments (city, suburban area, mountain, rural area, highway...). The\ndataset is available at: <a href=\"http://npm3d.fr\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deschaud_J/0/1/0/all/0/1\">Jean-Emmanuel Deschaud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Open Set Recognition. (arXiv:2109.00893v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00893","description":"<p>Open Set Recognition (OSR) is about dealing with unknown situations that were\nnot learned by the models during training. In this paper, we provide a survey\nof existing works about OSR and distinguish their respective advantages and\ndisadvantages to help out new researchers interested in the subject. The\ncategorization of OSR models is provided along with an extensive summary of\nrecent progress. Additionally, the relationships between OSR and its related\ntasks including multi-class classification and novelty detection are analyzed.\nIt is concluded that OSR can appropriately deal with unknown instances in the\nreal-world where capturing all possible classes in the training data is not\npractical. Lastly, applications of OSR are highlighted and some new directions\nfor future research topics are suggested.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_A/0/1/0/all/0/1\">Atefeh Mahdavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_M/0/1/0/all/0/1\">Marco Carvalho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Wind Power Curve Modeling Via Machine Vision: A Self-learning Deep Convolutional Network Based Method. (arXiv:2109.00894v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00894","description":"<p>This paper develops a novel self-training U-net (STU-net) based method for\nthe automated WPC model generation without requiring data pre-processing. The\nself-training (ST) process of STU-net has two steps. First, different from\ntraditional studies regarding the WPC modeling as a curve fitting problem, in\nthis paper, we renovate the WPC modeling formulation from a machine vision\naspect. To develop sufficiently diversified training samples, we synthesize\nsupervisory control and data acquisition (SCADA) data based on a set of S-shape\nfunctions depicting WPCs. These synthesized SCADA data and WPC functions are\nvisualized as images and paired as training samples(I_x, I_wpc). A U-net is\nthen developed to approximate the model recovering I_wpc from I_x. The\ndeveloped U-net is applied into observed SCADA data and can successfully\ngenerate the I_wpc. Moreover, we develop a pixel mapping and correction process\nto derive a mathematical form f_wpc representing I_wpcgenerated previously. The\nproposed STU-net only needs to train once and does not require any data\npreprocessing in applications. Numerical experiments based on 76 WTs are\nconducted to validate the superiority of the proposed method by benchmarking\nagainst classical WPC modeling methods. To demonstrate the repeatability of the\npresented research, we release our code at https://github.com/IkeYang/STU-net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Luoxiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Long Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zijun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Perceived Multi-modal Pretraining in E-commerce. (arXiv:2109.00895v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00895","description":"<p>In this paper, we address multi-modal pretraining of product data in the\nfield of E-commerce. Current multi-modal pretraining methods proposed for image\nand text modalities lack robustness in the face of modality-missing and\nmodality-noise, which are two pervasive problems of multi-modal product data in\nreal E-commerce scenarios. To this end, we propose a novel method, K3M, which\nintroduces knowledge modality in multi-modal pretraining to correct the noise\nand supplement the missing of image and text modalities. The modal-encoding\nlayer extracts the features of each modality. The modal-interaction layer is\ncapable of effectively modeling the interaction of multiple modalities, where\nan initial-interactive feature fusion model is designed to maintain the\nindependence of image modality and text modality, and a structure aggregation\nmodule is designed to fuse the information of image, text, and knowledge\nmodalities. We pretrain K3M with three pretraining tasks, including masked\nobject modeling (MOM), masked language modeling (MLM), and link prediction\nmodeling (LPM). Experimental results on a real-world E-commerce dataset and a\nseries of product-based downstream tasks demonstrate that K3M achieves\nsignificant improvements in performances than the baseline and state-of-the-art\nmethods when modality-noise or modality-missing exists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yushan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1\">Huaixiao Tou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_G/0/1/0/all/0/1\">Ganqiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Solution of an Elastic Net Regularization for Dementia Knowledge Discovery using Deep Learning. (arXiv:2109.00896v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00896","description":"<p>Background and Aim: Accurate classification of Magnetic Resonance Images\n(MRI) is essential to accurately predict Mild Cognitive Impairment (MCI) to\nAlzheimer's Disease (AD) conversion. Meanwhile, deep learning has been\nsuccessfully implemented to classify and predict dementia disease. However, the\naccuracy of MRI image classification is low. This paper aims to increase the\naccuracy and reduce the processing time of classification through Deep Learning\nArchitecture by using Elastic Net Regularization in Feature Selection.\nMethodology: The proposed system consists of Convolutional Neural Network (CNN)\nto enhance the accuracy of classification and prediction by using Elastic Net\nRegularization. Initially, the MRI images are fed into CNN for features\nextraction through convolutional layers alternate with pooling layers, and then\nthrough a fully connected layer. After that, the features extracted are\nsubjected to Principle Component Analysis (PCA) and Elastic Net Regularization\nfor feature selection. Finally, the selected features are used as an input to\nExtreme Machine Learning (EML) for the classification of MRI images. Results:\nThe result shows that the accuracy of the proposed solution is better than the\ncurrent system. In addition to that, the proposed method has improved the\nclassification accuracy by 5% on average and reduced the processing time by 30\n~ 40 seconds on average. Conclusion: The proposed system is focused on\nimproving the accuracy and processing time of MCI converters/non-converters\nclassification. It consists of features extraction, feature selection, and\nclassification using CNN, FreeSurfer, PCA, Elastic Net, Extreme Machine\nLearning. Finally, this study enhances the accuracy and the processing time by\nusing Elastic Net Regularization, which provides important selected features\nfor classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shrestha_K/0/1/0/all/0/1\">Kshitiz Shrestha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alsadoon_O/0/1/0/all/0/1\">Omar Hisham Alsadoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alsadoon_A/0/1/0/all/0/1\">Abeer Alsadoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_T/0/1/0/all/0/1\">Tarik A. Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_R/0/1/0/all/0/1\">Rasha S. Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_P/0/1/0/all/0/1\">P.W.C. Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jerew_O/0/1/0/all/0/1\">Oday D. Jerew</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CE-Dedup: Cost-Effective Convolutional Neural Nets Training based on Image Deduplication. (arXiv:2109.00899v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00899","description":"<p>Attributed to the ever-increasing large image datasets, Convolutional Neural\nNetworks (CNNs) have become popular for vision-based tasks. It is generally\nadmirable to have larger-sized datasets for higher network training accuracies.\nHowever, the impact of dataset quality has not to be involved. It is reasonable\nto assume the near-duplicate images exist in the datasets. For instance, the\nStreet View House Numbers (SVHN) dataset having cropped house plate digits from\n0 to 9 are likely to have repetitive digits from the same/similar house plates.\nRedundant images may take up a certain portion of the dataset without\nconsciousness. While contributing little to no accuracy improvement for the\nCNNs training, these duplicated images unnecessarily pose extra resource and\ncomputation consumption. To this end, this paper proposes a framework to assess\nthe impact of the near-duplicate images on CNN training performance, called\nCE-Dedup. Specifically, CE-Dedup associates a hashing-based image deduplication\napproach with downstream CNNs-based image classification tasks. CE-Dedup\nbalances the tradeoff between a large deduplication ratio and a stable accuracy\nby adjusting the deduplication threshold. The effectiveness of CE-Dedup is\nvalidated through extensive experiments on well-known CNN benchmarks. On one\nhand, while maintaining the same validation accuracy, CE-Dedup can reduce the\ndataset size by 23%. On the other hand, when allowing a small validation\naccuracy drop (by 5%), CE-Dedup can trim the dataset size by 75%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_L/0/1/0/all/0/1\">Liqiong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xue Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-temporal-spectral-angular observation model that integrates observations from UAV and mobile mapping vehicle for better urban mapping. (arXiv:2109.00900v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00900","description":"<p>In a complex urban scene, observation from a single sensor unavoidably leads\nto voids in observations, failing to describe urban objects in a comprehensive\nmanner. In this paper, we propose a spatio-temporal-spectral-angular\nobservation model to integrate observations from UAV and mobile mapping vehicle\nplatform, realizing a joint, coordinated observation operation from both air\nand ground. We develop a multi-source remote sensing data acquisition system to\neffectively acquire multi-angle data of complex urban scenes. Multi-source data\nfusion solves the missing data problem caused by occlusion and achieves\naccurate, rapid, and complete collection of holographic spatial and temporal\ninformation in complex urban scenes. We carried out an experiment on Baisha\nTown, Chongqing, China and obtained multi-sensor, multi-angle data from UAV and\nmobile mapping vehicle. We first extracted the point cloud from UAV and then\nintegrated the UAV and mobile mapping vehicle point cloud. The integrated\nresults combined both the characteristic of UAV and mobile mapping vehicle\npoint cloud, confirming the practicability of the proposed joint data\nacquisition platform and the effectiveness of spatio-temporal-spectral-angular\nobservation model. Compared with the observation from UAV or mobile mapping\nvehicle alone, the integrated system provides an effective data acquisition\nsolution towards comprehensive urban monitoring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhenfeng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gui Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Deren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhipeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effect of the output activation function on the probabilities and errors in medical image segmentation. (arXiv:2109.00903v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00903","description":"<p>The sigmoid activation is the standard output activation function in binary\nclassification and segmentation with neural networks. Still, there exist a\nvariety of other potential output activation functions, which may lead to\nimproved results in medical image segmentation. In this work, we consider how\nthe asymptotic behavior of different output activation and loss functions\naffects the prediction probabilities and the corresponding segmentation errors.\nFor cross entropy, we show that a faster rate of change of the activation\nfunction correlates with better predictions, while a slower rate of change can\nimprove the calibration of probabilities. For dice loss, we found that the\narctangent activation function is superior to the sigmoid function.\nFurthermore, we provide a test space for arbitrary output activation functions\nin the area of medical image segmentation. We tested seven activation functions\nin combination with three loss functions on four different medical image\nsegmentation tasks to provide a classification of which function is best suited\nin this application scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nieradzik_L/0/1/0/all/0/1\">Lars Nieradzik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheuermann_G/0/1/0/all/0/1\">Gerik Scheuermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saur_D/0/1/0/all/0/1\">Dorothee Saur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillmann_C/0/1/0/all/0/1\">Christina Gillmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Automated Approach for the Recognition of Bengali License Plates. (arXiv:2109.00906v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00906","description":"<p>Automatic Number Plate Recognition (ALPR) is a system for automatically\nidentifying the license plates of any vehicle. This process is important for\ntracking, ticketing, and any billing system, among other things. With the use\nof information and communication technology (ICT), all systems are being\nautomated, including the vehicle tracking system. This study proposes a hybrid\nmethod for detecting license plates using characters from them. Our captured\nimage information was used for the recognition procedure in Bangladeshi\nvehicles, which is the topic of this study. Here, for license plate detection,\nthe YOLO model was used where 81% was correctly predicted. And then, for\nlicense plate segmentation, Otsu's Thresholding was used and eventually, for\ncharacter recognition, the CNN model was applied. This model will allow the\nvehicle's automated license plate detection system to avoid any misuse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">Md Abdullah Al Nasim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1\">Atiqul Islam Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muna_J/0/1/0/all/0/1\">Jannatun Naeem Muna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_F/0/1/0/all/0/1\">Faisal Muhammad Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FA-GAN: Feature-Aware GAN for Text to Image Synthesis. (arXiv:2109.00907v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00907","description":"<p>Text-to-image synthesis aims to generate a photo-realistic image from a given\nnatural language description. Previous works have made significant progress\nwith Generative Adversarial Networks (GANs). Nonetheless, it is still hard to\ngenerate intact objects or clear textures (Fig 1). To address this issue, we\npropose Feature-Aware Generative Adversarial Network (FA-GAN) to synthesize a\nhigh-quality image by integrating two techniques: a self-supervised\ndiscriminator and a feature-aware loss. First, we design a self-supervised\ndiscriminator with an auxiliary decoder so that the discriminator can extract\nbetter representation. Secondly, we introduce a feature-aware loss to provide\nthe generator more direct supervision by employing the feature representation\nfrom the self-supervised discriminator. Experiments on the MS-COCO dataset show\nthat our proposed method significantly advances the state-of-the-art FID score\nfrom 28.92 to 24.58.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeon_E/0/1/0/all/0/1\">Eunyeong Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kunhee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daijin Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiHPF: Bilateral High-Pass Filters for Robust Deepfake Detection. (arXiv:2109.00911v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00911","description":"<p>The advancement in numerous generative models has a two-fold effect: a simple\nand easy generation of realistic synthesized images, but also an increased risk\nof malicious abuse of those images. Thus, it is important to develop a\ngeneralized detector for synthesized images of any GAN model or object\ncategory, including those unseen during the training phase. However, the\nconventional methods heavily depend on the training settings, which cause a\ndramatic decline in performance when tested with unknown domains. To resolve\nthe issue and obtain a generalized detection ability, we propose Bilateral\nHigh-Pass Filters (BiHPF), which amplify the effect of the frequency-level\nartifacts that are known to be found in the synthesized images of generative\nmodels. Numerous experimental results validate that our method outperforms\nother state-of-the-art methods, even when tested with unseen domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Yonghyun Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Seungjai Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joe_S/0/1/0/all/0/1\">Seongho Joe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwon_Y/0/1/0/all/0/1\">Youngjune Gwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jongwon Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task learning from fixed-wing UAV images for 2D/3D city modeling. (arXiv:2109.00918v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00918","description":"<p>Single-task learning in artificial neural networks will be able to learn the\nmodel very well, and the benefits brought by transferring knowledge thus become\nlimited. In this regard, when the number of tasks increases (e.g., semantic\nsegmentation, panoptic segmentation, monocular depth estimation, and 3D point\ncloud), duplicate information may exist across tasks, and the improvement\nbecomes less significant. Multi-task learning has emerged as a solution to\nknowledge-transfer issues and is an approach to scene understanding which\ninvolves multiple related tasks each with potentially limited training data.\nMulti-task learning improves generalization by leveraging the domain-specific\ninformation contained in the training data of related tasks. In urban\nmanagement applications such as infrastructure development, traffic monitoring,\nsmart 3D cities, and change detection, automated multi-task data analysis for\nscene understanding based on the semantic, instance, and panoptic annotation,\nas well as monocular depth estimation, is required to generate precise urban\nmodels. In this study, a common framework for the performance assessment of\nmulti-task learning methods from fixed-wing UAV images for 2D/3D city modeling\nis presented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bayanlou_M/0/1/0/all/0/1\">Mohammad R. Bayanlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoshboresh_Masouleh_M/0/1/0/all/0/1\">Mehdi Khoshboresh-Masouleh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reiterative Domain Aware Multi-target Adaptation. (arXiv:2109.00919v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00919","description":"<p>Most domain adaptation methods focus on single-source-single-target\nadaptation setting. Multi-target domain adaptation is a powerful extension in\nwhich a single classifier is learned for multiple unlabeled target domains. To\nbuild a multi-target classifier, it is crucial to effectively aggregate\nfeatures from the labeled source and different unlabeled target domains.\nTowards this, recently introduced Domain-aware Curriculum Graph Co-Teaching\n(D-CGCT) exploits dual classifier head, one of which is based on the graph\nneural network. D-CGCT uses a sequential adaptation strategy that adapts one\ndomain at a time starting from the target domains that are more similar to the\nsource, assuming that the network finds it easier to adapt to such target\ndomains. However, we argue that there is no easier domain or difficult domain\nin absolute sense and each domain can have samples showing different\ncharacteristics. Following this cue, we propose Reiterative D-CGCT (RD-CGCT)\nthat obtains better adaptation performance by reiterating multiple times over\neach target domain, while keeping the total number of iterations as same.\nRD-CGCT further improves the adaptation performance by considering more source\nsamples than training samples in the training minibatch. Proposed RD-CGCT\nsignificantly improves the performance over D-CGCT for Office-Home and Office31\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sudipan Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classifying Organisms and Artefacts By Their Shapes. (arXiv:2109.00920v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00920","description":"<p>We often wish to classify objects by their shapes. Indeed, the study of\nshapes is an important part of many scientific fields such as evolutionary\nbiology, structural biology, image processing, and archaeology. The most\nwidely-used method of shape analysis, Geometric Morphometrics, assumes that\nthat the mathematical space in which shapes are represented is linear. However,\nit has long been known that shape space is, in fact, rather more complicated,\nand certainly non-linear. Diffeomorphic methods that take this non-linearity\ninto account, and so give more accurate estimates of the distances among\nshapes, exist but have rarely been applied to real-world problems. Using a\nmachine classifier, we tested the ability of several of these methods to\ndescribe and classify the shapes of a variety of organic and man-made objects.\nWe find that one method, the Square-Root Velocity Function (SRVF), is superior\nto all others, including a standard Geometric Morphometric method\n(eigenshapes). We also show that computational shape classifiers outperform\nhuman experts, and that the SRVF shortest-path between shapes can be used to\nestimate the shapes of intermediate steps in evolutionary series. Diffeomorphic\nshape analysis methods, we conclude, now provide practical and effective\nsolutions to many shape description and classification problems in the natural\nand human sciences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salili_James_A/0/1/0/all/0/1\">Arianna Salili-James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mackay_A/0/1/0/all/0/1\">Anne Mackay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Alvarez_E/0/1/0/all/0/1\">Emilio Rodriguez-Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Perez_D/0/1/0/all/0/1\">Diana Rodriguez-Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannack_T/0/1/0/all/0/1\">Thomas Mannack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawlings_T/0/1/0/all/0/1\">Timothy A. Rawlings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmer_A/0/1/0/all/0/1\">A. Richard Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todd_J/0/1/0/all/0/1\">Jonathan Todd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riutta_T/0/1/0/all/0/1\">Terhi E. Riutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macinnis_Ng_C/0/1/0/all/0/1\">Cate Macinnis-Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhitong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davies_M/0/1/0/all/0/1\">Megan Davies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorpe_Z/0/1/0/all/0/1\">Zinnia Thorpe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marsland_S/0/1/0/all/0/1\">Stephen Marsland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leroi_A/0/1/0/all/0/1\">Armand M. Leroi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ideals and Virtual Realities. (arXiv:2109.00926v1 [physics.ed-ph])","link":"http://arxiv.org/abs/2109.00926","description":"<p>A main step for world progress is to keep sharing ever-present Ideals for\nscience and education within today Virtual Realities. On-line education is\ntransforming human society to new levels in the way people teach and learn\nduring the ongoing SARS-CoV-2 pandemic. There is an increasing interest in\nhaving more and more reliable, fast and simple apps to communicate and also to\nrecord, assemble and distribute videos and lectures in the fields of Physics &amp;\nMaths still using traditional didactic methods. We describe here how to\naccurately reproduce chalkboard classes for the popular YouTube video platform\nusing OpenEyA-YT. The audience can thus be expanded over continents to help\nmitigate the effects of physical isolation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Canessa_E/0/1/0/all/0/1\">E. Canessa</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tenze_L/0/1/0/all/0/1\">L. Tenze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autonomous Curiosity for Real-Time Training Onboard Robotic Agents. (arXiv:2109.00927v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00927","description":"<p>Learning requires both study and curiosity. A good learner is not only good\nat extracting information from the data given to it, but also skilled at\nfinding the right new information to learn from. This is especially true when a\nhuman operator is required to provide the ground truth - such a source should\nonly be queried sparingly. In this work, we address the problem of curiosity as\nit relates to online, real-time, human-in-the-loop training of an object\ndetection algorithm onboard a robotic platform, one where motion produces new\nviews of the subject. We propose a deep reinforcement learning approach that\ndecides when to ask the human user for ground truth, and when to move. Through\na series of experiments, we demonstrate that our agent learns a movement and\nrequest policy that is at least 3x more effective at using human user\ninteractions to train an object detector than untrained approaches, and is\ngeneralizable to a variety of subjects and environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teng_E/0/1/0/all/0/1\">Ervin Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iannucci_B/0/1/0/all/0/1\">Bob Iannucci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact of Attention on Adversarial Robustness of Image Classification Models. (arXiv:2109.00936v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00936","description":"<p>Adversarial attacks against deep learning models have gained significant\nattention and recent works have proposed explanations for the existence of\nadversarial examples and techniques to defend the models against these attacks.\nAttention in computer vision has been used to incorporate focused learning of\nimportant features and has led to improved accuracy. Recently, models with\nattention mechanisms have been proposed to enhance adversarial robustness.\nFollowing this context, this work aims at a general understanding of the impact\nof attention on adversarial robustness. This work presents a comparative study\nof adversarial robustness of non-attention and attention based image\nclassification models trained on CIFAR-10, CIFAR-100 and Fashion MNIST datasets\nunder the popular white box and black box attacks. The experimental results\nshow that the robustness of attention based models may be dependent on the\ndatasets used i.e. the number of classes involved in the classification. In\ncontrast to the datasets with less number of classes, attention based models\nare observed to show better robustness towards classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Prachi Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SetMargin Loss applied to Deep Keystroke Biometrics with Circle Packing Interpretation. (arXiv:2109.00938v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00938","description":"<p>This work presents a new deep learning approach for keystroke biometrics\nbased on a novel Distance Metric Learning method (DML). DML maps input data\ninto a learned representation space that reveals a \"semantic\" structure based\non distances. In this work, we propose a novel DML method specifically designed\nto address the challenges associated to free-text keystroke identification\nwhere the classes used in learning and inference are disjoint. The proposed\nSetMargin Loss (SM-L) extends traditional DML approaches with a learning\nprocess guided by pairs of sets instead of pairs of samples, as done\ntraditionally. The proposed learning strategy allows to enlarge inter-class\ndistances while maintaining the intra-class structure of keystroke dynamics. We\nanalyze the resulting representation space using the mathematical problem known\nas Circle Packing, which provides neighbourhood structures with a theoretical\nmaximum inter-class distance. We finally prove experimentally the effectiveness\nof the proposed approach on a challenging task: keystroke biometric\nidentification over a large set of 78,000 subjects. Our method achieves\nstate-of-the-art accuracy on a comparison performed with the best existing\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acien_A/0/1/0/all/0/1\">Alejandro Acien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1\">Ruben Tolosana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serna_I/0/1/0/all/0/1\">Ignacio Serna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Robustness for Unsupervised Domain Adaptation. (arXiv:2109.00946v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00946","description":"<p>Extensive Unsupervised Domain Adaptation (UDA) studies have shown great\nsuccess in practice by learning transferable representations across a labeled\nsource domain and an unlabeled target domain with deep models. However,\nprevious works focus on improving the generalization ability of UDA models on\nclean examples without considering the adversarial robustness, which is crucial\nin real-world applications. Conventional adversarial training methods are not\nsuitable for the adversarial robustness on the unlabeled target domain of UDA\nsince they train models with adversarial examples generated by the supervised\nloss function. In this work, we leverage intermediate representations learned\nby multiple robust ImageNet models to improve the robustness of UDA models. Our\nmethod works by aligning the features of the UDA model with the robust features\nlearned by ImageNet pre-trained models along with domain adaptation training.\nIt utilizes both labeled and unlabeled domains and instills robustness without\nany adversarial intervention or label requirement during domain adaptation\ntraining. Experimental results show that our method significantly improves\nadversarial robustness compared to the baseline while keeping clean accuracy on\nvarious UDA benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Awais_M/0/1/0/all/0/1\">Muhammad Awais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fengwei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lanqing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Sung-Ho Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAM: Explainable Visual Similarity and Classification via Gradient Activation Maps. (arXiv:2109.00951v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00951","description":"<p>We present Gradient Activation Maps (GAM) - a machinery for explaining\npredictions made by visual similarity and classification models. By gleaning\nlocalized gradient and activation information from multiple network layers, GAM\noffers improved visual explanations, when compared to existing alternatives.\nThe algorithmic advantages of GAM are explained in detail, and validated\nempirically, where it is shown that GAM outperforms its alternatives across\nvarious tasks and datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barkan_O/0/1/0/all/0/1\">Oren Barkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armstrong_O/0/1/0/all/0/1\">Omri Armstrong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertz_A/0/1/0/all/0/1\">Amir Hertz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1\">Avi Caciularu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_O/0/1/0/all/0/1\">Ori Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkiel_I/0/1/0/all/0/1\">Itzik Malkiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koenigstein_N/0/1/0/all/0/1\">Noam Koenigstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrouSPI-Net: Spatio-temporal attention on parallel atrous convolutions and U-GRUs for skeletal pedestrian crossing prediction. (arXiv:2109.00953v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00953","description":"<p>Understanding the behaviors and intentions of pedestrians is still one of the\nmain challenges for vehicle autonomy, as accurate predictions of their\nintentions can guarantee their safety and driving comfort of vehicles. In this\npaper, we address pedestrian crossing prediction in urban traffic environments\nby linking the dynamics of a pedestrian's skeleton to a binary crossing\nintention. We introduce TrouSPI-Net: a context-free, lightweight, multi-branch\npredictor. TrouSPI-Net extracts spatio-temporal features for different time\nresolutions by encoding pseudo-images sequences of skeletal joints' positions\nand processes them with parallel attention modules and atrous convolutions. The\nproposed approach is then enhanced by processing features such as relative\ndistances of skeletal joints, bounding box positions, or ego-vehicle speed with\nU-GRUs. Using the newly proposed evaluation procedures for two large public\nnaturalistic data sets for studying pedestrian behavior in traffic: JAAD and\nPIE, we evaluate TrouSPI-Net and analyze its performance. Experimental results\nshow that TrouSPI-Net achieved 0.76 F1 score on JAAD and 0.80 F1 score on PIE,\ntherefore outperforming current state-of-the-art while being lightweight and\ncontext-free.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gesnouin_J/0/1/0/all/0/1\">Joseph Gesnouin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechberti_S/0/1/0/all/0/1\">Steve Pechberti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanciulescu_B/0/1/0/all/0/1\">Bogdan Stanciulescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1\">Fabien Moutarde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sk-Unet Model with Fourier Domain for Mitosis Detection. (arXiv:2109.00957v1 [eess.IV])","link":"http://arxiv.org/abs/2109.00957","description":"<p>Mitotic count is the most important morphological feature of breast cancer\ngrading. Many deep learning-based methods have been proposed but suffer from\ndomain shift. In this work, we construct a Fourier-based segmentation model for\nmitosis detection to address the problem. Swapping the low-frequency spectrum\nof source and target images is shown effective to alleviate the discrepancy\nbetween different scanners. Our Fourier-based segmentation method can achieve\nF1 with 0.7456 on the preliminary test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_F/0/1/0/all/0/1\">Feng Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiyue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Infrared Image Super-Resolution via Heterogeneous Convolutional WGAN. (arXiv:2109.00960v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00960","description":"<p>Image super-resolution is important in many fields, such as surveillance and\nremote sensing. However, infrared (IR) images normally have low resolution\nsince the optical equipment is relatively expensive. Recently, deep learning\nmethods have dominated image super-resolution and achieved remarkable\nperformance on visible images; however, IR images have received less attention.\nIR images have fewer patterns, and hence, it is difficult for deep neural\nnetworks (DNNs) to learn diverse features from IR images. In this paper, we\npresent a framework that employs heterogeneous convolution and adversarial\ntraining, namely, heterogeneous kernel-based super-resolution Wasserstein GAN\n(HetSRWGAN), for IR image super-resolution. The HetSRWGAN algorithm is a\nlightweight GAN architecture that applies a plug-and-play heterogeneous\nkernel-based residual block. Moreover, a novel loss function that employs image\ngradients is adopted, which can be applied to an arbitrary model. The proposed\nHetSRWGAN achieves consistently better performance in both qualitative and\nquantitative evaluations. According to the experimental results, the whole\ntraining process is more stable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongsong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zetao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guoming Pang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptive Cascade R-CNN for MItosis DOmain Generalization (MIDOG) Challenge. (arXiv:2109.00965v1 [eess.IV])","link":"http://arxiv.org/abs/2109.00965","description":"<p>We present a summary of the domain adaptive cascade R-CNN method for mitosis\ndetection of digital histopathology images. By comprehensive data augmentation\nand adapting existing popular detection architecture, our proposed method has\nachieved an F1 score of 0.7500 on the preliminary test set in MItosis DOmain\nGeneralization (MIDOG) Challenge at MICCAI2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Long_X/0/1/0/all/0/1\">Xi Long</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_Y/0/1/0/all/0/1\">Ying Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mu_X/0/1/0/all/0/1\">Xiao Mu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1\">Lian Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jingxin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate shape and phase averaging of time series through Dynamic Time Warping. (arXiv:2109.00978v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00978","description":"<p>We propose a novel time series averaging method based on Dynamic Time Warping\n(DTW). In contrast to previous methods, our algorithm preserves durational\ninformation and the distinctive durational features of the sequences due to a\nsimple conversion of the output of DTW into a time sequence and an innovative\niterative averaging process. We show that it accurately estimates the ground\ntruth mean sequences and mean temporal location of landmarks in synthetic and\nreal-world datasets and outperforms state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sioros_G/0/1/0/all/0/1\">George Sioros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nymoen_K/0/1/0/all/0/1\">Kristian Nymoen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Scene Novel View Synthesis via Deferred Spatio-temporal Consistency. (arXiv:2109.01018v1 [cs.GR])","link":"http://arxiv.org/abs/2109.01018","description":"<p>Structure from motion (SfM) enables us to reconstruct a scene via casual\ncapture from cameras at different viewpoints, and novel view synthesis (NVS)\nallows us to render a captured scene from a new viewpoint. Both are hard with\ncasual capture and dynamic scenes: SfM produces noisy and spatio-temporally\nsparse reconstructed point clouds, resulting in NVS with spatio-temporally\ninconsistent effects. We consider SfM and NVS parts together to ease the\nchallenge. First, for SfM, we recover stable camera poses, then we defer the\nrequirement for temporally-consistent points across the scene and reconstruct\nonly a sparse point cloud per timestep that is noisy in space-time. Second, for\nNVS, we present a variational diffusion formulation on depths and colors that\nlets us robustly cope with the noise by enforcing spatio-temporal consistency\nvia per-pixel reprojection weights derived from the input views. Together, this\ndeferred approach generates novel views for dynamic scenes without requiring\nchallenging spatio-temporally consistent reconstructions nor training complex\nmodels on large datasets. We demonstrate our algorithm on real-world dynamic\nscenes against classic and more recent learning-based baseline approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fulop_Balogh_B/0/1/0/all/0/1\">Beatrix-Em&#x151;ke F&#xfc;l&#xf6;p-Balogh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tursman_E/0/1/0/all/0/1\">Eleanor Tursman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tompkin_J/0/1/0/all/0/1\">James Tompkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Digne_J/0/1/0/all/0/1\">Julie Digne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonneel_N/0/1/0/all/0/1\">Nicolas Bonneel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extended Object Tracking Using Sets Of Trajectories with a PHD Filter. (arXiv:2109.01019v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01019","description":"<p>PHD filtering is a common and effective multiple object tracking (MOT)\nalgorithm used in scenarios where the number of objects and their states are\nunknown. In scenarios where each object can generate multiple measurements per\nscan, some PHD filters can estimate the extent of the objects as well as their\nkinematic properties. Most of these approaches are, however, not able to\ninherently estimate trajectories and rely on ad-hoc methods, such as different\nlabeling schemes, to build trajectories from the state estimates. This paper\npresents a Gamma Gaussian inverse Wishart mixture PHD filter that can directly\nestimate sets of trajectories of extended targets by expanding previous\nresearch on tracking sets of trajectories for point source objects to handle\nextended objects. The new filter is compared to an existing extended PHD filter\nthat uses a labeling scheme to build trajectories, and it is shown that the new\nfilter can estimate object trajectories more reliably.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sjudin_J/0/1/0/all/0/1\">Jakob Sjudin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcusson_M/0/1/0/all/0/1\">Martin Marcusson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svensson_L/0/1/0/all/0/1\">Lennart Svensson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammarstrand_L/0/1/0/all/0/1\">Lars Hammarstrand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Text recognition with Full Normalization. (arXiv:2109.01034v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01034","description":"<p>Scene text recognition has made significant progress in recent years and has\nbecome an important part of the work-flow. The widespread use of mobile devices\nopens up wide possibilities for using OCR technologies in everyday life.\nHowever, lack of training data for new research in this area remains relevant.\nIn this article, we present a new dataset consisting of real shots on\nsmartphones and demonstrate the effectiveness of profile normalization in this\ntask. In addition, the influence of various augmentations during the training\nof models for analyzing document images on smartphones is studied in detail.\nOur dataset is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zachary_N/0/1/0/all/0/1\">Nathan Zachary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carl_G/0/1/0/all/0/1\">Gerald Carl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elijah_R/0/1/0/all/0/1\">Russell Elijah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roma_H/0/1/0/all/0/1\">Hessi Roma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leer_R/0/1/0/all/0/1\">Robert Leer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amelia_J/0/1/0/all/0/1\">James Amelia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shot boundary detection method based on a new extensive dataset and mixed features. (arXiv:2109.01057v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01057","description":"<p>Shot boundary detection in video is one of the key stages of video data\nprocessing. A new method for shot boundary detection based on several video\nfeatures, such as color histograms and object boundaries, has been proposed.\nThe developed algorithm was tested on the open BBC Planet Earth [1] and RAI [2]\ndatasets, and the MSU CC datasets, based on videos used in the video codec\ncomparison conducted at MSU, as well as videos from the IBM set, were also\nplotted. The total dataset for algorithm development and testing exceeded the\nknown TRECVID datasets. Based on the test results, the proposed algorithm for\nscene change detection outperformed its counterparts with a final F-score of\n0.9794.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gushchin_A/0/1/0/all/0/1\">Alexander Gushchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antsiferova_A/0/1/0/all/0/1\">Anastasia Antsiferova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatolin_D/0/1/0/all/0/1\">Dmitriy Vatolin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"4D-Net for Learned Multi-Modal Alignment. (arXiv:2109.01066v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01066","description":"<p>We present 4D-Net, a 3D object detection approach, which utilizes 3D Point\nCloud and RGB sensing information, both in time. We are able to incorporate the\n4D information by performing a novel dynamic connection learning across various\nfeature representations and levels of abstraction, as well as by observing\ngeometric constraints. Our approach outperforms the state-of-the-art and strong\nbaselines on the Waymo Open Dataset. 4D-Net is better able to use motion cues\nand dense image information to detect distant objects more successfully.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piergiovanni_A/0/1/0/all/0/1\">AJ Piergiovanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casser_V/0/1/0/all/0/1\">Vincent Casser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael S. Ryoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1\">Anelia Angelova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLIDE: Single Image 3D Photography with Soft Layering and Depth-aware Inpainting. (arXiv:2109.01068v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01068","description":"<p>Single image 3D photography enables viewers to view a still image from novel\nviewpoints. Recent approaches combine monocular depth networks with inpainting\nnetworks to achieve compelling results. A drawback of these techniques is the\nuse of hard depth layering, making them unable to model intricate appearance\ndetails such as thin hair-like structures. We present SLIDE, a modular and\nunified system for single image 3D photography that uses a simple yet effective\nsoft layering strategy to better preserve appearance details in novel views. In\naddition, we propose a novel depth-aware training strategy for our inpainting\nmodule, better suited for the 3D photography task. The resulting SLIDE approach\nis modular, enabling the use of other components such as segmentation and\nmatting for improved layering. At the same time, SLIDE uses an efficient\nlayered depth formulation that only requires a single forward pass through the\ncomponent networks to produce high quality 3D photos. Extensive experimental\nanalysis on three view-synthesis datasets, in combination with user studies on\nin-the-wild image collections, demonstrate superior performance of our\ntechnique in comparison to existing strong baselines while being conceptually\nmuch simpler. Project page: https://varunjampani.github.io/slide\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Huiwen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sargent_K/0/1/0/all/0/1\">Kyle Sargent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_A/0/1/0/all/0/1\">Abhishek Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_R/0/1/0/all/0/1\">Richard Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krainin_M/0/1/0/all/0/1\">Michael Krainin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaeser_D/0/1/0/all/0/1\">Dominik Kaeser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1\">William T. Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salesin_D/0/1/0/all/0/1\">David Salesin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curless_B/0/1/0/all/0/1\">Brian Curless</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards disease-aware image editing of chest X-rays. (arXiv:2109.01071v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01071","description":"<p>Disease-aware image editing by means of generative adversarial networks\n(GANs) constitutes a promising avenue for advancing the use of AI in the\nhealthcare sector. Here, we present a proof of concept of this idea. While\nGAN-based techniques have been successful in generating and manipulating\nnatural images, their application to the medical domain, however, is still in\nits infancy. Working with the CheXpert data set, we show that StyleGAN can be\ntrained to generate realistic chest X-rays. Inspired by the Cyclic Reverse\nGenerator (CRG) framework, we train an encoder that allows for faithfully\ninverting the generator on synthetic X-rays and provides organ-level\nreconstructions of real ones. Employing a guided manipulation of latent codes,\nwe confer the medical condition of cardiomegaly (increased heart size) onto\nreal X-rays from healthy patients. This work was presented in the Medical\nImaging meets Neurips Workshop 2020, which was held as part of the 34th\nConference on Neural Information Processing Systems (NeurIPS 2020) in\nVancouver, Canada\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_S/0/1/0/all/0/1\">Sai Niranjan Ramachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saboo_A/0/1/0/all/0/1\">Aakash Saboo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dierkes_K/0/1/0/all/0/1\">Kai Dierkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keles_H/0/1/0/all/0/1\">Hacer Yalim Keles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cascade RCNN for MIDOG Challenge. (arXiv:2109.01085v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01085","description":"<p>Mitotic counts are one of the key indicators of breast cancer prognosis.\nHowever, accurate mitotic cell counting is still a difficult problem and is\nlabourious. Automated methods have been proposed for this task, but are usually\ndependent on the training images and show poor performance on unseen domains.\nIn this work, we present a multi-stage mitosis detection method based on a\nCascade RCNN developed to be sequentially more selective against false\npositives. On the preliminary test set, the algorithm scores an F1-score of\n0.7492.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Razavi_S/0/1/0/all/0/1\">Salar Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dambandkhameneh_F/0/1/0/all/0/1\">Fariba Dambandkhameneh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsos_D/0/1/0/all/0/1\">Dimitri Androutsos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Done_S/0/1/0/all/0/1\">Susan Done</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khademi_A/0/1/0/all/0/1\">April Khademi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On-target Adaptation. (arXiv:2109.01087v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01087","description":"<p>Domain adaptation seeks to mitigate the shift between training on the\n\\emph{source} domain and testing on the \\emph{target} domain. Most adaptation\nmethods rely on the source data by joint optimization over source data and\ntarget data. Source-free methods replace the source data with a source model by\nfine-tuning it on target. Either way, the majority of the parameter updates for\nthe model representation and the classifier are derived from the source, and\nnot the target. However, target accuracy is the goal, and so we argue for\noptimizing as much as possible on the target data. We show significant\nimprovement by on-target adaptation, which learns the representation purely\nfrom target data while taking only the source predictions for supervision. In\nthe long-tailed classification setting, we show further improvement by\non-target class distribution learning, which learns the (im)balance of classes\nfrom target data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dequan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaoteng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1\">Sayna Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1\">Evan Shelhamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Functional Correspondence Problem. (arXiv:2109.01097v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01097","description":"<p>The ability to find correspondences in visual data is the essence of most\ncomputer vision tasks. But what are the right correspondences? The task of\nvisual correspondence is well defined for two different images of same object\ninstance. In case of two images of objects belonging to same category, visual\ncorrespondence is reasonably well-defined in most cases. But what about\ncorrespondence between two objects of completely different category -- e.g., a\nshoe and a bottle? Does there exist any correspondence? Inspired by humans'\nability to: (a) generalize beyond semantic categories and; (b) infer functional\naffordances, we introduce the problem of functional correspondences in this\npaper. Given images of two objects, we ask a simple question: what is the set\nof correspondences between these two images for a given task? For example, what\nare the correspondences between a bottle and shoe for the task of pounding or\nthe task of pouring. We introduce a new dataset: FunKPoint that has ground\ntruth correspondences for 10 tasks and 20 object categories. We also introduce\na modular task-driven representation for attacking this problem and demonstrate\nthat our learned representation is effective for this task. But most\nimportantly, because our supervision signal is not bound by semantics, we show\nthat our learned representation can generalize better on few-shot\nclassification problem. We hope this paper will inspire our community to think\nbeyond semantics and focus more on cross-category generalization and learning\nrepresentations for robotics tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1\">Zihang Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purushwalkam_S/0/1/0/all/0/1\">Senthil Purushwalkam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking the Robustness of Instance Segmentation Models. (arXiv:2109.01123v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01123","description":"<p>This paper presents a comprehensive evaluation of instance segmentation\nmodels with respect to real-world image corruptions and out-of-domain image\ncollections, e.g. datasets collected with different set-ups than the training\ndatasets the models learned from. The out-of-domain image evaluation shows the\ngeneralization capability of models, an essential aspect of real-world\napplications, and an extensively studied topic of domain adaptation. These\npresented robustness and generalization evaluations are important when\ndesigning instance segmentation models for real-world applications and picking\nan off-the-shelf pretrained model to directly use for the task at hand.\nSpecifically, this benchmark study includes state-of-the-art network\narchitectures, network backbones, normalization layers, models trained starting\nfrom scratch or ImageNet pretrained networks, and the effect of multi-task\ntraining on robustness and generalization. Through this study, we gain several\ninsights e.g. we find that normalization layers play an essential role in\nrobustness, ImageNet pretraining does not help the robustness and the\ngeneralization of models, excluding JPEG corruption, and network backbones and\ncopy-paste augmentations affect robustness significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Altindis_S/0/1/0/all/0/1\">Said Fahri Altindis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalva_Y/0/1/0/all/0/1\">Yusuf Dalva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dundar_A/0/1/0/all/0/1\">Aysegul Dundar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Robust Mitotic Figure Detection with StyleGAN. (arXiv:2109.01124v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01124","description":"<p>We propose a new training scheme for domain generalization in mitotic figure\ndetection. By considering the image variance due to different scanner types as\ndifferent image styles, we have trained our detection network to be robust on\nscanner types. To expand the image variance, domain of training image is\ntransferred into arbitrary domain. The proposed style transfer module generates\ndifferent styled images from an input image with random code, eventually\ngenerating variously styled images. Our model with the proposed training scheme\nshows good performance on MIDOG Preliminary Test-Set containing scanners never\nseen before.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Youjin Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jihoon Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinah Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo. (arXiv:2109.01129v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01129","description":"<p>In this work, we present a new multi-view depth estimation method that\nutilizes both conventional SfM reconstruction and learning-based priors over\nthe recently proposed neural radiance fields (NeRF). Unlike existing neural\nnetwork based optimization method that relies on estimated correspondences, our\nmethod directly optimizes over implicit volumes, eliminating the challenging\nstep of matching pixels in indoor scenes. The key to our approach is to utilize\nthe learning-based priors to guide the optimization process of NeRF. Our system\nfirstly adapts a monocular depth network over the target scene by finetuning on\nits sparse SfM reconstruction. Then, we show that the shape-radiance ambiguity\nof NeRF still exists in indoor environments and propose to address the issue by\nemploying the adapted depth priors to monitor the sampling process of volume\nrendering. Finally, a per-pixel confidence map acquired by error computation on\nthe rendered image can be used to further improve the depth quality.\nExperiments show that our proposed framework significantly outperforms\nstate-of-the-art methods on indoor scenes, with surprising findings presented\non the effectiveness of correspondence-based optimization and NeRF-based\noptimization over the adapted depth priors. In addition, we show that the\nguided optimization scheme does not sacrifice the original synthesis capability\nof neural radiance fields, improving the rendering quality on both seen and\nnovel views. Code is available at https://github.com/weiyithu/NerfingMVS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaohui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Semi-Automated Algorithm for Volumetric Segmentation of the Left Ventricle in Temporal 3D Echocardiography Sequences. (arXiv:2109.01132v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01132","description":"<p>Purpose: Echocardiography is commonly used as a non-invasive imaging tool in\nclinical practice for the assessment of cardiac function. However, delineation\nof the left ventricle is challenging due to the inherent properties of\nultrasound imaging, such as the presence of speckle noise and the low\nsignal-to-noise ratio. Methods: We propose a semi-automated segmentation\nalgorithm for the delineation of the left ventricle in temporal 3D\nechocardiography sequences. The method requires minimal user interaction and\nrelies on a diffeomorphic registration approach. Advantages of the method\ninclude no dependence on prior geometrical information, training data, or\nregistration from an atlas. Results: The method was evaluated using\nthree-dimensional ultrasound scan sequences from 18 patients from the\nMazankowski Alberta Heart Institute, Edmonton, Canada, and compared to manual\ndelineations provided by an expert cardiologist and four other registration\nalgorithms. The segmentation approach yielded the following results over the\ncardiac cycle: a mean absolute difference of 1.01 (0.21) mm, a Hausdorff\ndistance of 4.41 (1.43) mm, and a Dice overlap score of 0.93 (0.02).\nConclusions: The method performed well compared to the four other registration\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishnaswamy_D/0/1/0/all/0/1\">Deepa Krishnaswamy</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Hareendranathan_A/0/1/0/all/0/1\">Abhilash R. Hareendranathan</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Suwatanaviroj_T/0/1/0/all/0/1\">Tan Suwatanaviroj</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Boulanger_P/0/1/0/all/0/1\">Pierre Boulanger</a> (1 and 2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Becher_H/0/1/0/all/0/1\">Harald Becher</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Noga_M/0/1/0/all/0/1\">Michelle Noga</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Punithakumar_K/0/1/0/all/0/1\">Kumaradevan Punithakumar</a> (1 and 2 and 3) ((1) Department of Radiology and Diagnostic Imaging, University of Alberta, Edmonton, AB, Canada, (2) Servier Virtual Cardiac Centre, Mazankowski Alberta Heart Institute, Edmonton, AB, Canada, (3) Department of Computing Science, University of Alberta, Edmonton, AB, Canada, (4) ABACUS, Mazankowski Alberta Heart Institute, Edmonton, AB, Canada)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Prompt for Vision-Language Models. (arXiv:2109.01134v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01134","description":"<p>Vision-language pre-training has recently emerged as a promising alternative\nfor representation learning. It shifts from the tradition of using images and\ndiscrete labels for learning a fixed set of weights, seen as visual concepts,\nto aligning images and raw text for two separate encoders. Such a paradigm\nbenefits from a broader source of supervision and allows zero-shot transfer to\ndownstream tasks since visual concepts can be diametrically generated from\nnatural language, known as prompt. In this paper, we identify that a major\nchallenge of deploying such models in practice is prompt engineering. This is\nbecause designing a proper prompt, especially for context words surrounding a\nclass name, requires domain expertise and typically takes a significant amount\nof time for words tuning since a slight change in wording could have a huge\nimpact on performance. Moreover, different downstream tasks require specific\ndesigns, further hampering the efficiency of deployment. To overcome this\nchallenge, we propose a novel approach named context optimization (CoOp). The\nmain idea is to model context in prompts using continuous representations and\nperform end-to-end learning from data while keeping the pre-trained parameters\nfixed. In this way, the design of task-relevant prompts can be fully automated.\nExperiments on 11 datasets show that CoOp effectively turns pre-trained\nvision-language models into data-efficient visual learners, requiring as few as\none or two shots to beat hand-crafted prompts with a decent margin and able to\ngain significant improvements when using more shots (e.g., at 16 shots the\naverage gain is around 17% with the highest reaching over 50%). CoOp also\nexhibits strong robustness to distribution shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Power of Points for Modeling Humans in Clothing. (arXiv:2109.01137v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01137","description":"<p>Currently it requires an artist to create 3D human avatars with realistic\nclothing that can move naturally. Despite progress on 3D scanning and modeling\nof human bodies, there is still no technology that can easily turn a static\nscan into an animatable avatar. Automating the creation of such avatars would\nenable many applications in games, social networking, animation, and AR/VR to\nname a few. The key problem is one of representation. Standard 3D meshes are\nwidely used in modeling the minimally-clothed body but do not readily capture\nthe complex topology of clothing. Recent interest has shifted to implicit\nsurface models for this task but they are computationally heavy and lack\ncompatibility with existing 3D tools. What is needed is a 3D representation\nthat can capture varied topology at high resolution and that can be learned\nfrom data. We argue that this representation has been with us all along -- the\npoint cloud. Point clouds have properties of both implicit and explicit\nrepresentations that we exploit to model 3D garment geometry on a human body.\nWe train a neural network with a novel local clothing geometric feature to\nrepresent the shape of different outfits. The network is trained from 3D point\nclouds of many types of clothing, on many bodies, in many poses, and learns to\nmodel pose-dependent clothing deformations. The geometry feature can be\noptimized to fit a previously unseen scan of a person in clothing, enabling the\nscan to be reposed realistically. Our model demonstrates superior quantitative\nand qualitative results in both multi-outfit modeling and unseen outfit\nanimation. The code is available for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qianli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinlong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Integrated Gradients with SmoothTaylor for Deep Neural Network Attribution. (arXiv:2004.10484v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.10484","description":"<p>Integrated Gradients as an attribution method for deep neural network models\noffers simple implementability. However, it suffers from noisiness of\nexplanations which affects the ease of interpretability. The SmoothGrad\ntechnique is proposed to solve the noisiness issue and smoothen the attribution\nmaps of any gradient-based attribution method. In this paper, we present\nSmoothTaylor as a novel theoretical concept bridging Integrated Gradients and\nSmoothGrad, from the Taylor's theorem perspective. We apply the methods to the\nimage classification problem, using the ILSVRC2012 ImageNet object recognition\ndataset, and a couple of pretrained image models to generate attribution maps.\nThese attribution maps are empirically evaluated using quantitative measures\nfor sensitivity and noise level. We further propose adaptive noising to\noptimize for the noise scale hyperparameter value. From our experiments, we\nfind that the SmoothTaylor approach together with adaptive noising is able to\ngenerate better quality saliency maps with lesser noise and higher sensitivity\nto the relevant points in the input space as compared to Integrated Gradients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goh_G/0/1/0/all/0/1\">Gary S. W. Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1\">Sebastian Lapuschkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_L/0/1/0/all/0/1\">Leander Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1\">Wojciech Samek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1\">Alexander Binder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Manifold Alignment for Semantically Aligned Style Transfer. (arXiv:2005.10777v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.10777","description":"<p>Most existing style transfer methods follow the assumption that styles can be\nrepresented with global statistics (e.g., Gram matrices or covariance\nmatrices), and thus address the problem by forcing the output and style images\nto have similar global statistics. An alternative is the assumption of local\nstyle patterns, where algorithms are designed to swap similar local features of\ncontent and style images. However, the limitation of these existing methods is\nthat they neglect the semantic structure of the content image which may lead to\ncorrupted content structure in the output. In this paper, we make a new\nassumption that image features from the same semantic region form a manifold\nand an image with multiple semantic regions follows a multi-manifold\ndistribution. Based on this assumption, the style transfer problem is\nformulated as aligning two multi-manifold distributions and a Manifold\nAlignment based Style Transfer (MAST) framework is proposed. The proposed\nframework allows semantically similar regions between the output and the style\nimage share similar style patterns. Moreover, the proposed manifold alignment\nmethod is flexible to allow user editing or using semantic segmentation maps as\nguidance for style transfer. To allow the method to be applicable to\nphotorealistic style transfer, we propose a new adaptive weight skip connection\nnetwork structure to preserve the content details. Extensive experiments verify\nthe effectiveness of the proposed framework for both artistic and\nphotorealistic style transfer. Code is available at\nhttps://github.com/NJUHuoJing/MAST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1\">Jing Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Shiyin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yu-Kun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Reconstruction of Novel Object Shapes from Single Images. (arXiv:2006.07752v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.07752","description":"<p>Accurately predicting the 3D shape of any arbitrary object in any pose from a\nsingle image is a key goal of computer vision research. This is challenging as\nit requires a model to learn a representation that can infer both the visible\nand occluded portions of any object using a limited training set. A training\nset that covers all possible object shapes is inherently infeasible. Such\nlearning-based approaches are inherently vulnerable to overfitting, and\nsuccessfully implementing them is a function of both the architecture design\nand the training approach. We present an extensive investigation of factors\nspecific to architecture design, training, experiment design, and evaluation\nthat influence reconstruction performance and measurement. We show that our\nproposed SDFNet achieves state-of-the-art performance on seen and unseen shapes\nrelative to existing methods GenRe and OccNet. We provide the first large-scale\nevaluation of single image shape reconstruction to unseen objects. The source\ncode, data and trained models can be found on\nhttps://github.com/rehg-lab/3DShapeGen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thai_A/0/1/0/all/0/1\">Anh Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stojanov_S/0/1/0/all/0/1\">Stefan Stojanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhya_V/0/1/0/all/0/1\">Vijay Upadhya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1\">James M. Rehg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Bayesian Evaluation Framework for Subjectively Annotated Visual Recognition Tasks. (arXiv:2007.06711v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.06711","description":"<p>An interesting development in automatic visual recognition has been the\nemergence of tasks where it is not possible to assign objective labels to\nimages, yet still feasible to collect annotations that reflect human judgements\nabout them. Machine learning-based predictors for these tasks rely on\nsupervised training that models the behavior of the annotators, i.e., what\nwould the average person's judgement be for an image? A key open question for\nthis type of work, especially for applications where inconsistency with human\nbehavior can lead to ethical lapses, is how to evaluate the epistemic\nuncertainty of trained predictors, i.e., the uncertainty that comes from the\npredictor's model. We propose a Bayesian framework for evaluating black box\npredictors in this regime, agnostic to the predictor's internal structure. The\nframework specifies how to estimate the epistemic uncertainty that comes from\nthe predictor with respect to human labels by approximating a conditional\ndistribution and producing a credible interval for the predictions and their\nmeasures of performance. The framework is successfully applied to four image\nclassification tasks that use subjective human judgements: facial beauty\nassessment, social attribute assignment, apparent age estimation, and ambiguous\nscene labeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prijatelj_D/0/1/0/all/0/1\">Derek S. Prijatelj</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+McCurrie_M/0/1/0/all/0/1\">Mel McCurrie</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Scheirer_W/0/1/0/all/0/1\">Walter J. Scheirer</a> (1) ((1) University of Notre Dame, Notre Dame, USA, (2) Perceptive Automata, Boston, USA)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable-by-design Semi-Supervised Representation Learning for COVID-19 Diagnosis from CT Imaging. (arXiv:2011.11719v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2011.11719","description":"<p>Our motivating application is a real-world problem: COVID-19 classification\nfrom CT imaging, for which we present an explainable Deep Learning approach\nbased on a semi-supervised classification pipeline that employs variational\nautoencoders to extract efficient feature embedding. We have optimized the\narchitecture of two different networks for CT images: (i) a novel conditional\nvariational autoencoder (CVAE) with a specific architecture that integrates the\nclass labels inside the encoder layers and uses side information with shared\nattention layers for the encoder, which make the most of the contextual clues\nfor representation learning, and (ii) a downstream convolutional neural network\nfor supervised classification using the encoder structure of the CVAE. With the\nexplainable classification results, the proposed diagnosis system is very\neffective for COVID-19 classification. Based on the promising results obtained\nqualitatively and quantitatively, we envisage a wide deployment of our\ndeveloped technique in large-scale clinical studies.Code is available at\nhttps://git.etrovub.be/AVSP/ct-based-covid-19-diagnostic-tool.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Berenguer_A/0/1/0/all/0/1\">Abel D&#xed;az Berenguer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sahli_H/0/1/0/all/0/1\">Hichem Sahli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joukovsky_B/0/1/0/all/0/1\">Boris Joukovsky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kvasnytsia_M/0/1/0/all/0/1\">Maryna Kvasnytsia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dirks_I/0/1/0/all/0/1\">Ine Dirks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alioscha_Perez_M/0/1/0/all/0/1\">Mitchel Alioscha-Perez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deligiannis_N/0/1/0/all/0/1\">Nikos Deligiannis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonidakis_P/0/1/0/all/0/1\">Panagiotis Gonidakis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sanchez_S/0/1/0/all/0/1\">Sebasti&#xe1;n Amador S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brahimetaj_R/0/1/0/all/0/1\">Redona Brahimetaj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Papavasileiou_E/0/1/0/all/0/1\">Evgenia Papavasileiou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chana_J/0/1/0/all/0/1\">Jonathan Cheung-Wai Chana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_S/0/1/0/all/0/1\">Shangzhen Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yixin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tilborghs_S/0/1/0/all/0/1\">Sofie Tilborghs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Willems_S/0/1/0/all/0/1\">Siri Willems</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eelbode_T/0/1/0/all/0/1\">Tom Eelbode</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bertels_J/0/1/0/all/0/1\">Jeroen Bertels</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vandermeulen_D/0/1/0/all/0/1\">Dirk Vandermeulen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maes_F/0/1/0/all/0/1\">Frederik Maes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suetens_P/0/1/0/all/0/1\">Paul Suetens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Robben_D/0/1/0/all/0/1\">David Robben</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brys_A/0/1/0/all/0/1\">Arne Brys</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smeets_D/0/1/0/all/0/1\">Dirk Smeets</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ilsen_B/0/1/0/all/0/1\">Bart Ilsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buls_N/0/1/0/all/0/1\">Nico Buls</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watte_N/0/1/0/all/0/1\">Nina Watt&#xe9;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mey_J/0/1/0/all/0/1\">Johan de Mey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Snoeckx_A/0/1/0/all/0/1\">Annemiek Snoeckx</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parizel_P/0/1/0/all/0/1\">Paul M. Parizel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guiot_J/0/1/0/all/0/1\">Julien Guiot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deprez_L/0/1/0/all/0/1\">Louis Deprez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meunier_P/0/1/0/all/0/1\">Paul Meunier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gryspeerdt_S/0/1/0/all/0/1\">Stefaan Gryspeerdt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smet_K/0/1/0/all/0/1\">Kristof De Smet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jansen_B/0/1/0/all/0/1\">Bart Jansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vandemeulebroucke_J/0/1/0/all/0/1\">Jef Vandemeulebroucke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"USCL: Pretraining Deep Ultrasound Image Diagnosis Model through Video Contrastive Representation Learning. (arXiv:2011.13066v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.13066","description":"<p>Most deep neural networks (DNNs) based ultrasound (US) medical image analysis\nmodels use pretrained backbones (e.g., ImageNet) for better model\ngeneralization. However, the domain gap between natural and medical images\ncauses an inevitable performance bottleneck. To alleviate this problem, an US\ndataset named US-4 is constructed for direct pretraining on the same domain. It\ncontains over 23,000 images from four US video sub-datasets. To learn robust\nfeatures from US-4, we propose an US semi-supervised contrastive learning\nmethod, named USCL, for pretraining. In order to avoid high similarities\nbetween negative pairs as well as mine abundant visual features from limited US\nvideos, USCL adopts a sample pair generation method to enrich the feature\ninvolved in a single step of contrastive optimization. Extensive experiments on\nseveral downstream tasks show the superiority of USCL pretraining against\nImageNet pretraining and other state-of-the-art (SOTA) pretraining approaches.\nIn particular, USCL pretrained backbone achieves fine-tuning accuracy of over\n94% on POCUS dataset, which is 10% higher than 84% of the ImageNet pretrained\nmodel. The source codes of this work are available at\nhttps://github.<a href=\"/abs/com/9836328\">com/9836328</a>47/USCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixiong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Cheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Changfeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yongfang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Siamese Basis Function Networks for Data-efficient Defect Classification in Technical Domains. (arXiv:2012.01338v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01338","description":"<p>Training deep learning models in technical domains is often accompanied by\nthe challenge that although the task is clear, insufficient data for training\nis available. In this work, we propose a novel approach based on the\ncombination of Siamese networks and radial basis function networks to perform\ndata-efficient classification without pretraining by measuring the distance\nbetween images in semantic space in a data-efficient manner. We develop the\nmodels using three technical datasets, the NEU dataset, the BSD dataset, and\nthe TEX dataset. In addition to the technical domain, we show the general\napplicability to classical datasets (cifar10 and MNIST) as well. The approach\nis tested against state-of-the-art models (Resnet50 and Resnet101) by stepwise\nreduction of the number of samples available for training. The authors show\nthat the proposed approach outperforms the state-of-the-art models in the low\ndata regime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlagenhauf_T/0/1/0/all/0/1\">Tobias Schlagenhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleischer_J/0/1/0/all/0/1\">J&#xfc;rgen Fleischer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factorizing Perception and Policy for Interactive Instruction Following. (arXiv:2012.03208v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2012.03208","description":"<p>Performing simple household tasks based on language directives is very\nnatural to humans, yet it remains an open challenge for AI agents. The\n'interactive instruction following' task attempts to make progress towards\nbuilding agents that jointly navigate, interact, and reason in the environment\nat every step. To address the multifaceted problem, we propose a model that\nfactorizes the task into interactive perception and action policy streams with\nenhanced components and name it as MOCA, a Modular Object-Centric Approach. We\nempirically validate that MOCA outperforms prior arts by significant margins on\nthe ALFRED benchmark with improved generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kunal Pratap Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhambri_S/0/1/0/all/0/1\">Suvaansh Bhambri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byeonghwi Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jonghyun Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effect of the regularization hyperparameter on deep learning-based segmentation in LGE-MRI. (arXiv:2012.05661v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2012.05661","description":"<p>The extent to which the arbitrarily selected L2 regularization hyperparameter\nvalue affects the outcome of semantic segmentation with deep learning is\ndemonstrated. Demonstrations rely on training U-net on small LGE-MRI datasets\nusing the arbitrarily selected L2 regularization values. The remaining\nhyperparameters are to be manually adjusted or tuned only when 10 % of all\nepochs are reached before the training validation accuracy reaches 90%.\nSemantic segmentation with deep learning outcomes are objectively and\nsubjectively evaluated against the manual ground truth segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rukundo_O/0/1/0/all/0/1\">Olivier Rukundo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of deep learning-based myocardial infarction quantification using Segment CMR software. (arXiv:2012.09070v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2012.09070","description":"<p>This work evaluates deep learning-based myocardial infarction (MI)\nquantification using Segment cardiovascular magnetic resonance (CMR) software.\nSegment CMR software incorporates the expectation-maximization, weighted\nintensity, a priori information (EWA) algorithm used to generate the infarct\nscar volume, infarct scar percentage, and microvascular obstruction percentage.\nHere, Segment CMR software segmentation algorithm is updated with semantic\nsegmentation with U-net to achieve and evaluate fully automated or deep\nlearning-based MI quantification. The direct observation of graphs and the\nnumber of infarcted and contoured myocardium are two options used to estimate\nthe relationship between deep learning-based MI quantification and medical\nexpert-based results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rukundo_O/0/1/0/all/0/1\">Olivier Rukundo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Opti-Enc: On the Path to the Optimal Encoder-Decoder for Thermal Image Colorization for Cross Domain Colorized Images. (arXiv:2101.06910v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2101.06910","description":"<p>Thermal images can be obtained as either grayscale images or pseudo colored\nimages based on the thermal profile of the object being captured. In this work,\nwe explore what an optimal encoder decoder might look like for creating a\nthermal-optical fused domain image. We compare the results from several\ndifferent encoder-decoder structures with different networks to answer this\nquestion. This output images obtained from our method provides information of\nboth domains jointly in a colorized image. We call this a cross domain\ncolorized image. We also present a robust registration method for thermal and\noptical pairs, which can work despite changes in resolution and the make of the\nthermal imager. Lastly, we present a unique public dataset with registered\nthermal-visual image pairs containing around 1800 images as a part of this\nwork, collected over a period of 2 years. We compare our results with prior\nliterature, show how our results are different and discuss on some future work\nthat can be explored further in this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Goswami_S/0/1/0/all/0/1\">Suranjan Goswami</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_S/0/1/0/all/0/1\">Satish Kumar Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TLRM: Task-level Relation Module for GNN-based Few-Shot Learning. (arXiv:2101.09840v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.09840","description":"<p>Recently, graph neural networks (GNNs) have shown powerful ability to handle\nfew-shot classification problem, which aims at classifying unseen samples when\ntrained with limited labeled samples per class. GNN-based few-shot learning\narchitectures mostly replace traditional metric with a learnable GNN. In the\nGNN, the nodes are set as the samples embedding, and the relationship between\ntwo connected nodes can be obtained by a network, the input of which is the\ndifference of their embedding features. We consider this method of measuring\nrelation of samples only models the sample-to-sample relation, while neglects\nthe specificity of different tasks. That is, this method of measuring relation\ndoes not take the task-level information into account. To this end, we propose\na new relation measure method, namely the task-level relation module (TLRM), to\nexplicitly model the task-level relation of one sample to all the others. The\nproposed module captures the relation representations between nodes by\nconsidering the sample-to-task instead of sample-to-sample embedding features.\nWe conducted extensive experiments on four benchmark datasets: mini-ImageNet,\ntiered-ImageNet, CUB-$200$-$2011$, and CIFAR-FS. Experimental results\ndemonstrate that the proposed module is effective for GNN-based few-shot\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yurong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuan Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Combat Noisy Labels via Classification Margins. (arXiv:2102.00751v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.00751","description":"<p>A deep neural network trained on noisy labels is known to quickly lose its\npower to discriminate clean instances from noisy ones. After the early learning\nphase has ended, the network memorizes the noisy instances, which leads to a\nsignificant degradation in its generalization performance. To resolve this\nissue, we propose MARVEL (MARgins Via Early Learning), a new robust learning\nmethod where the memorization of the noisy instances is curbed. We propose a\nnew test statistic that tracks the goodness of \"fit\" of every instance based on\nthe epoch-history of its classification margins. If its classification margin\nis small in a sequence of consecutive learning epochs, that instance is\ndeclared noisy and the network abandons learning on it. Consequently, the\nnetwork first flags a possibly noisy instance, and then waits to see if\nlearning on that instance can be improved and if not, the network learns with\nconfidence that this instance can be safely abandoned. We also propose MARVEL+,\nwhere arduous instances can be upweighted, enabling the network to focus and\nimprove its learning on them and consequently its generalization. Experimental\nresults on benchmark datasets with synthetic label noise and real-world\ndatasets show that MARVEL outperforms other baselines consistently across\ndifferent noise levels, with a significantly larger margin under asymmetric\nnoise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jason Z. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradic_J/0/1/0/all/0/1\">Jelena Bradic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Real-World Adversarial Patches through 3D Modeling of Complex Target Scenes. (arXiv:2102.05334v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.05334","description":"<p>Adversarial examples have proven to be a concerning threat to deep learning\nmodels, particularly in the image domain. However, while many studies have\nexamined adversarial examples in the real world, most of them relied on 2D\nphotos of the attack scene. As a result, the attacks proposed may have limited\neffectiveness when implemented in realistic environments with 3D objects or\nvaried conditions. There are few studies on adversarial learning that use 3D\nobjects, and in many cases, other researchers are unable to replicate the\nreal-world evaluation process. In this study, we present a framework that uses\n3D modeling to craft adversarial patches for an existing real-world scene. Our\napproach uses a 3D digital approximation of the scene as a simulation of the\nreal world. With the ability to add and manipulate any element in the digital\nscene, our framework enables the attacker to improve the adversarial patch's\nimpact in real-world settings. We use the framework to create a patch for an\neveryday scene and evaluate its performance using a novel evaluation process\nthat ensures that our results are reproducible in both the digital space and\nthe real world. Our evaluation results show that the framework can generate\nadversarial patches that are robust to different settings in the real world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mathov_Y/0/1/0/all/0/1\">Yael Mathov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rokach_L/0/1/0/all/0/1\">Lior Rokach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1\">Yuval Elovici</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"White Box Methods for Explanations of Convolutional Neural Networks in Image Classification Tasks. (arXiv:2104.02548v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02548","description":"<p>In recent years, deep learning has become prevalent to solve applications\nfrom multiple domains. Convolutional Neural Networks (CNNs) particularly have\ndemonstrated state of the art performance for the task of image classification.\nHowever, the decisions made by these networks are not transparent and cannot be\ndirectly interpreted by a human. Several approaches have been proposed to\nexplain to understand the reasoning behind a prediction made by a network. In\nthis paper, we propose a topology of grouping these methods based on their\nassumptions and implementations. We focus primarily on white box methods that\nleverage the information of the internal architecture of a network to explain\nits decision. Given the task of image classification and a trained CNN, this\nwork aims to provide a comprehensive and detailed overview of a set of methods\nthat can be used to create explanation maps for a particular image, that assign\nan importance score to each pixel of the image based on its contribution to the\ndecision of the network. We also propose a further classification of the white\nbox methods based on their implementations to enable better comparisons and\nhelp researchers find methods best suited for different scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayyar_M/0/1/0/all/0/1\">Meghna P Ayyar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benois_Pineau_J/0/1/0/all/0/1\">Jenny Benois-Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemmari_A/0/1/0/all/0/1\">Akka Zemmari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly Detection with Prototype-Guided Discriminative Latent Embeddings. (arXiv:2104.14945v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.14945","description":"<p>Recent efforts towards video anomaly detection (VAD) try to learn a deep\nautoencoder to describe normal event patterns with small reconstruction errors.\nThe video inputs with large reconstruction errors are regarded as anomalies at\nthe test time. However, these methods sometimes reconstruct abnormal inputs\nwell because of the powerful generalization ability of deep autoencoder. To\naddress this problem, we present a novel approach for anomaly detection, which\nutilizes discriminative prototypes of normal data to reconstruct video frames.\nIn this way, the model will favor the reconstruction of normal events and\ndistort the reconstruction of abnormal events. Specifically, we use a\nprototype-guided memory module to perform discriminative latent embedding. We\nintroduce a new discriminative criterion for the memory module, as well as a\nloss function correspondingly, which can encourage memory items to record the\nrepresentative embeddings of normal data, i.e. prototypes. Besides, we design a\nnovel two-branch autoencoder, which is composed of a future frame prediction\nnetwork and an RGB difference generation network that share the same encoder.\nThe stacked RGB difference contains motion information just like optical flow,\nso our model can learn temporal regularity. We evaluate the effectiveness of\nour method on three benchmark datasets and experimental results demonstrate the\nproposed method outperforms the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yuandu Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yahong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmenter: Transformer for Semantic Segmentation. (arXiv:2105.05633v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.05633","description":"<p>Image segmentation is often ambiguous at the level of individual image\npatches and requires contextual information to reach label consensus. In this\npaper we introduce Segmenter, a transformer model for semantic segmentation. In\ncontrast to convolution-based methods, our approach allows to model global\ncontext already at the first layer and throughout the network. We build on the\nrecent Vision Transformer (ViT) and extend it to semantic segmentation. To do\nso, we rely on the output embeddings corresponding to image patches and obtain\nclass labels from these embeddings with a point-wise linear decoder or a mask\ntransformer decoder. We leverage models pre-trained for image classification\nand show that we can fine-tune them on moderate sized datasets available for\nsemantic segmentation. The linear decoder allows to obtain excellent results\nalready, but the performance can be further improved by a mask transformer\ngenerating class masks. We conduct an extensive ablation study to show the\nimpact of the different parameters, in particular the performance is better for\nlarge models and small patch sizes. Segmenter attains excellent results for\nsemantic segmentation. It outperforms the state of the art on both ADE20K and\nPascal Context datasets and is competitive on Cityscapes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Strudel_R/0/1/0/all/0/1\">Robin Strudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_R/0/1/0/all/0/1\">Ricardo Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FCCDN: Feature Constraint Network for VHR Image Change Detection. (arXiv:2105.10860v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.10860","description":"<p>Change detection is the process of identifying pixelwise differences in\nbitemporal co-registered images. It is of great significance to Earth\nobservations. Recently, with the emergence of deep learning (DL), the power and\nfeasibility of deep convolutional neural network (CNN)-based methods have been\nshown in the field of change detection. However, there is still a lack of\neffective supervision for change feature learning. In this work, a feature\nconstraint change detection network (FCCDN) is proposed. We constrain features\nboth in bitemporal feature extraction and feature fusion. More specifically, we\npropose a dual encoder-decoder network backbone for the change detection task.\nAt the center of the backbone, we design a nonlocal feature pyramid network to\nextract and fuse multiscale features. To fuse bitemporal features in a robust\nway, we build a dense connection-based feature fusion module. Moreover, a\nself-supervised learning-based strategy is proposed to constrain feature\nlearning. Based on FCCDN, we achieve state-of-the-art performance on two\nbuilding change detection datasets (LEVIR-CD and WHU). On the LEVIR-CD dataset,\nwe achieve an IoU of 0.8569 and an F1 score of 0.9229. On the WHU dataset, we\nachieve an IoU of 0.8820 and an F1 score of 0.9373. Moreover, for the first\ntime, the acquisition of accurate bitemporal semantic segmentation results is\nachieved without using semantic segmentation labels. This is vital for the\napplication of change detection because it saves the cost of labeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Danfeng Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengchao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baipeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridge the Gap Between Model-based and Model-free Human Reconstruction. (arXiv:2106.06313v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06313","description":"<p>It is challenging to directly estimate the geometry of human from a single\nimage due to the high diversity and complexity of body shapes with the various\nclothing styles. Most of model-based approaches are limited to predict the\nshape and pose of a minimally clothed body with over-smoothing surface.\nAlthough capturing the fine detailed geometries, the model-free methods are\nlack of the fixed mesh topology. To address these issues, we propose a novel\ntopology-preserved human reconstruction approach by bridging the gap between\nmodel-based and model-free human reconstruction. We present an end-to-end\nneural network that simultaneously predicts the pixel-aligned implicit surface\nand the explicit mesh model built by graph convolutional neural network.\nMoreover, an extra graph convolutional neural network is employed to estimate\nthe vertex offsets between the implicit surface and parametric mesh model.\nFinally, we suggest an efficient implicit registration method to refine the\nneural network output in implicit space. Experiments on DeepHuman dataset\nshowed that our approach is effective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lixiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advances in adversarial attacks and defenses in computer vision: A survey. (arXiv:2108.00401v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00401","description":"<p>Deep Learning (DL) is the most widely used tool in the contemporary field of\ncomputer vision. Its ability to accurately solve complex problems is employed\nin vision research to learn deep neural models for a variety of tasks,\nincluding security critical applications. However, it is now known that DL is\nvulnerable to adversarial attacks that can manipulate its predictions by\nintroducing visually imperceptible perturbations in images and videos. Since\nthe discovery of this phenomenon in 2013~[1], it has attracted significant\nattention of researchers from multiple sub-fields of machine intelligence. In\n[2], we reviewed the contributions made by the computer vision community in\nadversarial attacks on deep learning (and their defenses) until the advent of\nyear 2018. Many of those contributions have inspired new directions in this\narea, which has matured significantly since witnessing the first generation\nmethods. Hence, as a legacy sequel of [2], this literature review focuses on\nthe advances in this area since 2018. To ensure authenticity, we mainly\nconsider peer-reviewed contributions published in the prestigious sources of\ncomputer vision and machine learning research. Besides a comprehensive\nliterature review, the article also provides concise definitions of technical\nterminologies for non-experts in this domain. Finally, this article discusses\nchallenges and future outlook of this direction based on the literature\nreviewed herein and [2].\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1\">Naveed Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1\">Navid Kardan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leaf Recognition Using Convolutional Neural Networks Based Features. (arXiv:2108.01808v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01808","description":"<p>There is a warning light for the loss of plant habitats worldwide that\nentails concerted efforts to conserve plant biodiversity. Thus, plant species\nclassification is of crucial importance to address this environmental\nchallenge. In recent years, there is a considerable increase in the number of\nstudies related to plant taxonomy. While some researchers try to improve their\nrecognition performance using novel approaches, others concentrate on\ncomputational optimization for their framework. In addition, a few studies are\ndiving into feature extraction to gain significantly in terms of accuracy. In\nthis paper, we propose an effective method for the leaf recognition problem. In\nour proposed approach, a leaf goes through some pre-processing to extract its\nrefined color image, vein image, xy-projection histogram, handcrafted shape,\ntexture features, and Fourier descriptors. These attributes are then\ntransformed into a better representation by neural network-based encoders\nbefore a support vector machine (SVM) model is utilized to classify different\nleaves. Overall, our approach performs a state-of-the-art result on the Flavia\nleaf dataset, achieving the accuracy of 99.58\\% on test sets under random\n10-fold cross-validation and bypassing the previous methods. We also release\nour codes (Scripts are available at\nhttps://github.com/dinhvietcuong1996/LeafRecognition) for contributing to the\nresearch community in the leaf classification problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quach_B/0/1/0/all/0/1\">Boi M. Quach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuong_D/0/1/0/all/0/1\">Dinh V. Cuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1\">Nhung Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_D/0/1/0/all/0/1\">Dang Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh T. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Dual-reference Training Data Acquisition Method for CNN-Based Image Super-Resolution. (arXiv:2108.02348v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.02348","description":"<p>For deep learning methods of image super-resolution, the most critical issue\nis whether the paired low and high resolution images for training accurately\nreflect the sampling process of real cameras. Low and high resolution\n(LR$\\sim$HR) image pairs synthesized by existing degradation models (e.g.\nbicubic downsampling) deviate from those in reality; thus the super-resolution\nCNN trained by these synthesized LR$\\sim$HR image pairs does not perform well\nwhen being applied to real images. In this paper, we propose a novel method to\ncapture a large set of realistic LR$\\sim$HR image pairs using real cameras. The\ndata acquisition is carried out under controllable lab conditions with minimum\nhuman intervention and at high throughput (about 500 image pairs per hour). The\nhigh level of automation makes it easy to produce a set of real LR$\\sim$HR\ntraining image pairs for each camera.Our innovation is to shoot images\ndisplayed on an ultra-high quality screen at different resolutions. There are\nthree distinctive advantages of our method for image super-resolution. First,\nas the LR and HR images are taken of a 3D planar surface (the screen) the\nregistration problem fits exactly to a homography model and we can display\nspecially designed markers on the image to improve the registration precision.\nSecond, the displayed digital image file can be exploited as a reference to\noptimize the high frequency content of the restored image. Third, this\nhigh-efficiency data collection method makes it possible to collect a\ncustomized dataset for each camera sensor, for which one can train a specific\nmodel for the intended camera sensor. Experimental results show that training a\nsuper-resolution CNN by our LR$\\sim$HR dataset has superior restoration\nperformance than training it by existing datasets on real world images at the\ninference stage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1\">Yanhui Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shu_X/0/1/0/all/0/1\">Xiao Shu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xiaolin Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imperceptible Adversarial Examples by Spatial Chroma-Shift. (arXiv:2108.02502v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02502","description":"<p>Deep Neural Networks have been shown to be vulnerable to various kinds of\nadversarial perturbations. In addition to widely studied additive noise based\nperturbations, adversarial examples can also be created by applying a per pixel\nspatial drift on input images. While spatial transformation based adversarial\nexamples look more natural to human observers due to absence of additive noise,\nthey still possess visible distortions caused by spatial transformations. Since\nthe human vision is more sensitive to the distortions in the luminance compared\nto those in chrominance channels, which is one of the main ideas behind the\nlossy visual multimedia compression standards, we propose a spatial\ntransformation based perturbation method to create adversarial examples by only\nmodifying the color components of an input image. While having competitive\nfooling rates on CIFAR-10 and NIPS2017 Adversarial Learning Challenge datasets,\nexamples created with the proposed method have better scores with regards to\nvarious perceptual quality metrics. Human visual perception studies validate\nthat the examples are more natural looking and often indistinguishable from\ntheir original counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aydin_A/0/1/0/all/0/1\">Ayberk Aydin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_D/0/1/0/all/0/1\">Deniz Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karli_B/0/1/0/all/0/1\">Berat Tuna Karli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanoglu_O/0/1/0/all/0/1\">Oguz Hanoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Temizel_A/0/1/0/all/0/1\">Alptekin Temizel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled High Quality Salient Object Detection. (arXiv:2108.03551v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03551","description":"<p>Aiming at discovering and locating most distinctive objects from visual\nscenes, salient object detection (SOD) plays an essential role in various\ncomputer vision systems. Coming to the era of high resolution, SOD methods are\nfacing new challenges. The major limitation of previous methods is that they\ntry to identify the salient regions and estimate the accurate objects\nboundaries simultaneously with a single regression task at low-resolution. This\npractice ignores the inherent difference between the two difficult problems,\nresulting in poor detection quality. In this paper, we propose a novel deep\nlearning framework for high-resolution SOD task, which disentangles the task\ninto a low-resolution saliency classification network (LRSCN) and a\nhigh-resolution refinement network (HRRN). As a pixel-wise classification task,\nLRSCN is designed to capture sufficient semantics at low-resolution to identify\nthe definite salient, background and uncertain image regions. HRRN is a\nregression task, which aims at accurately refining the saliency value of pixels\nin the uncertain region to preserve a clear object boundary at high-resolution\nwith limited GPU memory. It is worth noting that by introducing uncertainty\ninto the training process, our HRRN can well address the high-resolution\nrefinement task without using any high-resolution training data. Extensive\nexperiments on high-resolution saliency datasets as well as some widely used\nsaliency benchmarks show that the proposed method achieves superior performance\ncompared to the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Lv Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mofei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Creating synthetic meteorology satellite visible light images during night based on GAN method. (arXiv:2108.04330v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04330","description":"<p>Meteorology satellite visible light images is critical for meteorology\nsupport and forecast. However, there is no such kind of data during night time.\nTo overcome this, we propose a method based on deep learning to create\nsynthetic satellite visible light images during night. Specifically, to produce\nmore realistic products, we train a Generative Adversarial Networks (GAN) model\nto generate visible light images given the corresponding satellite infrared\nimages and numerical weather prediction(NWP) products. To better model the\nnonlinear relationship from infrared data and NWP products to visible light\nimages, we propose to use the channel-wise attention mechanics, e.g., SEBlock\nto quantitative weight the input channels. The experiments based on the ECMWF\nNWP products and FY-4A meteorology satellite visible light and infrared\nchannels date show that the proposed methods can be effective to create\nrealistic synthetic satellite visible light images during night.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wencong Cheng</a> (Beijing Aviation Meteorological Institute)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Calibrating Neural Radiance Fields. (arXiv:2108.13826v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.13826","description":"<p>In this work, we propose a camera self-calibration algorithm for generic\ncameras with arbitrary non-linear distortions. We jointly learn the geometry of\nthe scene and the accurate camera parameters without any calibration objects.\nOur camera model consists of a pinhole model, a fourth order radial distortion,\nand a generic noise model that can learn arbitrary non-linear camera\ndistortions. While traditional self-calibration algorithms mostly rely on\ngeometric constraints, we additionally incorporate photometric consistency.\nThis requires learning the geometry of the scene, and we use Neural Radiance\nFields (NeRF). We also propose a new geometric loss function, viz., projected\nray distance loss, to incorporate geometric consistency for complex non-linear\ncamera models. We validate our approach on standard real image datasets and\ndemonstrate that our model can learn the camera intrinsics and extrinsics\n(pose) from scratch without COLMAP initialization. Also, we show that learning\naccurate camera models in a differentiable manner allows us to improve PSNR\nover baselines. Our module is an easy-to-use plugin that can be applied to NeRF\nvariants to improve performance. The code and data are currently available at\nhttps://github.com/POSTECH-CVLab/SCNeRF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Yoonwoo Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Seokjun Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choy_C/0/1/0/all/0/1\">Christopher Choy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Animashree Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptually Optimized Deep High-Dynamic-Range Image Tone Mapping. (arXiv:2109.00180v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00180","description":"<p>We describe a deep high-dynamic-range (HDR) image tone mapping operator that\nis computationally efficient and perceptually optimized. We first decompose an\nHDR image into a normalized Laplacian pyramid, and use two deep neural networks\n(DNNs) to estimate the Laplacian pyramid of the desired tone-mapped image from\nthe normalized representation. We then end-to-end optimize the entire method\nover a database of HDR images by minimizing the normalized Laplacian pyramid\ndistance (NLPD), a recently proposed perceptual metric. Qualitative and\nquantitative experiments demonstrate that our method produces images with\nbetter visual quality, and runs the fastest among existing local tone mapping\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1\">Chenyang Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jiebin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuming Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}