{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.6","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-11-02T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Skyformer: Remodel Self-Attention with Gaussian Kernel and Nystr\\\"om Method. (arXiv:2111.00035v1 [cs.LG])","link":"http://arxiv.org/abs/2111.00035","description":"<p>Transformers are expensive to train due to the quadratic time and space\ncomplexity in the self-attention mechanism. On the other hand, although kernel\nmachines suffer from the same computation bottleneck in pairwise dot products,\nseveral approximation schemes have been successfully incorporated to\nconsiderably reduce their computational cost without sacrificing too much\naccuracy. In this work, we leverage the computation methods for kernel machines\nto alleviate the high computational cost and introduce Skyformer, which\nreplaces the softmax structure with a Gaussian kernel to stabilize the model\ntraining and adapts the Nystr\\\"om method to a non-positive semidefinite matrix\nto accelerate the computation. We further conduct theoretical analysis by\nshowing that the matrix approximation error of our proposed method is small in\nthe spectral norm. Experiments on Long Range Arena benchmark show that the\nproposed method is sufficient in getting comparable or even better performance\nthan the full self-attention while requiring fewer computation resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1\">Qi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yun Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring a Texts Fairness Dimensions Using Machine Learning Based on Social Psychological Factors. (arXiv:2111.00086v1 [cs.AI])","link":"http://arxiv.org/abs/2111.00086","description":"<p>Fairness is a principal social value that can be observed in civilisations\naround the world. A manifestations of this is in social agreements, often\ndescribed in texts, such as contracts. Yet, despite the prevalence of such, a\nfairness metric for texts describing a social act remains wanting. To address\nthis, we take a step back to consider the problem based on first principals.\nInstead of using rules or templates, we utilise social psychology literature to\ndetermine the principal factors that humans use when making a fairness\nassessment. We then attempt to digitise these using word embeddings into a\nmulti-dimensioned sentence level fairness perceptions vector to serve as an\napproximation for these fairness perceptions. The method leverages a pro-social\nbias within word embeddings, for which we obtain an F1= 81.0. A second\napproach, using PCA and ML based on the said fairness approximation vector\nproduces an F1 score of 86.2. We details improvements that can be made in the\nmethodology to incorporate the projection of sentence embedding on to a\nsubspace representation of fairness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Izzidien_A/0/1/0/all/0/1\">A. Izzidien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watson_J/0/1/0/all/0/1\">J. Watson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loe_B/0/1/0/all/0/1\">B. Loe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_P/0/1/0/all/0/1\">P. Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fitz_S/0/1/0/all/0/1\">S. Fitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stillwell_D/0/1/0/all/0/1\">D. Stillwell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Golden Rule as a Heuristic to Measure the Fairness of Texts Using Machine Learning. (arXiv:2111.00107v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00107","description":"<p>To treat others as one would wish to be treated is a common formulation of\nthe Golden Rule (GR). Yet, despite its prevalence as an axiom throughout\nhistory, no digitisation of the moral philosophy exists. In this paper we\nconsider how to digitise it so that it may be used to measure sentences such\nas: the boy harmed the girl, and categorise them as fair or unfair. A review\nand reply to criticisms of the GR is made. We share the code for the\ndigitisation of the GR, and test it with a list of sentences. Implementing two\napproaches, one using the USE, and a second using ALBERT. We find F1 scores of\n78.0, 85.0, respectively. A suggestion of how the technology may be implemented\nto avoid unfair biases in word embeddings is made - given that individuals\nwould typically not wish to be on the receiving end of an unfair act, such as\nracism, irrespective of whether the corpus being used deems such discrimination\nas praiseworthy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Izzidien_A/0/1/0/all/0/1\">A. Izzidien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watson_J/0/1/0/all/0/1\">J. Watson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loe_B/0/1/0/all/0/1\">B. Loe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_P/0/1/0/all/0/1\">P. Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fitz_S/0/1/0/all/0/1\">S. Fitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stillwell_D/0/1/0/all/0/1\">D. Stillwell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransAug: Translate as Augmentation for Sentence Embeddings. (arXiv:2111.00157v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00157","description":"<p>While contrastive learning greatly advances the representation of sentence\nembeddings, it is still limited by the size of the existing sentence datasets.\nIn this paper, we present TransAug (Translate as Augmentation), which provide\nthe first exploration of utilizing translated sentence pairs as data\naugmentation for text, and introduce a two-stage paradigm to advances the\nstate-of-the-art sentence embeddings. Instead of adopting an encoder trained in\nother languages setting, we first distill a Chinese encoder from a SimCSE\nencoder (pretrained in English), so that their embeddings are close in semantic\nspace, which can be regraded as implicit data augmentation. Then, we only\nupdate the English encoder via cross-lingual contrastive learning and frozen\nthe distilled Chinese encoder. Our approach achieves a new state-of-art on\nstandard semantic textual similarity (STS), outperforming both SimCSE and\nSentence-T5, and the best performance in corresponding tracks on transfer tasks\nevaluated by SentEval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chaochen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Debing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained Language Models. (arXiv:2111.00160v1 [cs.LG])","link":"http://arxiv.org/abs/2111.00160","description":"<p>Gigantic pre-trained models have become central to natural language\nprocessing (NLP), serving as the starting point for fine-tuning towards a range\nof downstream tasks. However, two pain points persist for this paradigm: (a) as\nthe pre-trained models grow bigger (e.g., 175B parameters for GPT-3), even the\nfine-tuning process can be time-consuming and computationally expensive; (b)\nthe fine-tuned model has the same size as its starting point by default, which\nis neither sensible due to its more specialized functionality, nor practical\nsince many fine-tuned models will be deployed in resource-constrained\nenvironments. To address these pain points, we propose a framework for\nresource- and parameter-efficient fine-tuning by leveraging the sparsity prior\nin both weight updates and the final model weights. Our proposed framework,\ndubbed Dually Sparsity-Embedded Efficient Tuning (DSEE), aims to achieve two\nkey objectives: (i) parameter efficient fine-tuning - by enforcing\nsparsity-aware weight updates on top of the pre-trained weights; and (ii)\nresource-efficient inference - by encouraging a sparse weight structure towards\nthe final fine-tuned model. We leverage sparsity in these two directions by\nexploiting both unstructured and structured sparse patterns in pre-trained\nlanguage models via magnitude-based pruning and $\\ell_1$ sparse regularization.\nExtensive experiments and in-depth investigations, with diverse network\nbackbones (i.e., BERT, GPT-2, and DeBERTa) on dozens of datasets, consistently\ndemonstrate highly impressive parameter-/training-/inference-efficiency, while\nmaintaining competitive downstream transfer performance. For instance, our\nDSEE-BERT obtains about $35\\%$ inference FLOPs savings with &lt;1% trainable\nparameters and comparable performance to conventional fine-tuning. Codes are\navailable in https://github.com/VITA-Group/DSEE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo-Labeling for Massively Multilingual Speech Recognition. (arXiv:2111.00161v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00161","description":"<p>Semi-supervised learning through pseudo-labeling has become a staple of\nstate-of-the-art monolingual speech recognition systems. In this work, we\nextend pseudo-labeling to massively multilingual speech recognition with 60\nlanguages. We propose a simple pseudo-labeling recipe that works well even with\nlow-resource languages: train a supervised multilingual model, fine-tune it\nwith semi-supervised learning on a target language, generate pseudo-labels for\nthat language, and train a final model using pseudo-labels for all languages,\neither from scratch or by fine-tuning. Experiments on the labeled Common Voice\nand unlabeled VoxPopuli datasets show that our recipe can yield a model with\nbetter performance for many languages that also transfers well to LibriSpeech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lugosch_L/0/1/0/all/0/1\">Loren Lugosch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Heterogeneous Graph Representation Learning for Short Text Classification. (arXiv:2111.00180v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00180","description":"<p>Short text classification is a fundamental task in natural language\nprocessing. It is hard due to the lack of context information and labeled data\nin practice. In this paper, we propose a new method called SHINE, which is\nbased on graph neural network (GNN), for short text classification. First, we\nmodel the short text dataset as a hierarchical heterogeneous graph consisting\nof word-level component graphs which introduce more semantic and syntactic\ninformation. Then, we dynamically learn a short document graph that facilitates\neffective label propagation among similar short texts. Thus, compared with\nexisting GNN-based methods, SHINE can better exploit interactions between nodes\nof the same types and capture similarities between short texts. Extensive\nexperiments on various benchmark short text datasets show that SHINE\nconsistently outperforms state-of-the-art methods, especially with fewer\nlabels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Quanming Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How should human translation coexist with NMT? Efficient tool for building high quality parallel corpus. (arXiv:2111.00191v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00191","description":"<p>This paper proposes a tool for efficiently constructing high-quality parallel\ncorpora with minimizing human labor and making this tool publicly available.\nOur proposed construction process is based on neural machine translation (NMT)\nto allow for it to not only coexist with human translation, but also improve\nits efficiency by combining data quality control with human translation in a\ndata-centric approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chanjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seolhwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_H/0/1/0/all/0/1\">Hyeonseok Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eo_S/0/1/0/all/0/1\">Sugyeong Eo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jaehyung Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Knowledge Augmentation for Generative Commonsense Reasoning. (arXiv:2111.00192v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00192","description":"<p>Generative commonsense reasoning is the capability of a language model to\ngenerate a sentence with a given concept-set that is based on commonsense\nknowledge. However, generative language models still struggle to provide\noutputs, and the training set does not contain patterns that are sufficient for\ngenerative commonsense reasoning. In this paper, we propose a data-centric\nmethod that uses automatic knowledge augmentation to extend commonsense\nknowledge using a machine knowledge generator. This method can generate\nsemi-golden sentences that improve the generative commonsense reasoning of a\nlanguage model without architecture modifications. Furthermore, this approach\nis a model-agnostic method and does not require human effort for data\nconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jaehyung Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chanjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eo_S/0/1/0/all/0/1\">Sugyeong Eo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_H/0/1/0/all/0/1\">Hyeonseok Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backdoor Pre-trained Models Can Transfer to All. (arXiv:2111.00197v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00197","description":"<p>Pre-trained general-purpose language models have been a dominating component\nin enabling real-world natural language processing (NLP) applications. However,\na pre-trained model with backdoor can be a severe threat to the applications.\nMost existing backdoor attacks in NLP are conducted in the fine-tuning phase by\nintroducing malicious triggers in the targeted class, thus relying greatly on\nthe prior knowledge of the fine-tuning task. In this paper, we propose a new\napproach to map the inputs containing triggers directly to a predefined output\nrepresentation of the pre-trained NLP models, e.g., a predefined output\nrepresentation for the classification token in BERT, instead of a target label.\nIt can thus introduce backdoor to a wide range of downstream tasks without any\nprior knowledge. Additionally, in light of the unique properties of triggers in\nNLP, we propose two new metrics to measure the performance of backdoor attacks\nin terms of both effectiveness and stealthiness. Our experiments with various\ntypes of triggers show that our method is widely applicable to different\nfine-tuning tasks (classification and named entity recognition) and to\ndifferent models (such as BERT, XLNet, BART), which poses a severe threat.\nFurthermore, by collaborating with the popular online model repository Hugging\nFace, the threat brought by our method has been confirmed. Finally, we analyze\nthe factors that may affect the attack performance and share insights on the\ncauses of the success of our backdoor attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lujia Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jie Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chengfang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianwei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Ting Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Magic Pyramid: Accelerating Inference with Early Exiting and Token Pruning. (arXiv:2111.00230v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00230","description":"<p>Pre-training and then fine-tuning large language models is commonly used to\nachieve state-of-the-art performance in natural language processing (NLP)\ntasks. However, most pre-trained models suffer from low inference speed.\nDeploying such large models to applications with latency constraints is\nchallenging. In this work, we focus on accelerating the inference via\nconditional computations. To achieve this, we propose a novel idea, Magic\nPyramid (MP), to reduce both width-wise and depth-wise computation via token\npruning and early exiting for Transformer-based models, particularly BERT. The\nformer manages to save the computation via removing non-salient tokens, while\nthe latter can fulfill the computation reduction by terminating the inference\nearly before reaching the final layer, if the exiting condition is met. Our\nempirical studies demonstrate that compared to previous state of arts, MP is\nnot only able to achieve a speed-adjustable inference but also to surpass token\npruning and early exiting by reducing up to 70% giga floating point operations\n(GFLOPs) with less than 0.5% accuracy drop. Token pruning and early exiting\nexpress distinctive preferences to sequences with different lengths. However,\nMP is capable of achieving an average of 8.06x speedup on two popular text\nclassification tasks, regardless of the sizes of the inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuanli He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keivanloo_I/0/1/0/all/0/1\">Iman Keivanloo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Belinda Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopalan_S/0/1/0/all/0/1\">Santosh Rajagopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chilimbi_T/0/1/0/all/0/1\">Trishul Chilimbi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EventNarrative: A large-scale Event-centric Dataset for Knowledge Graph-to-Text Generation. (arXiv:2111.00276v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00276","description":"<p>We introduce EventNarrative, a knowledge graph-to-text dataset from publicly\navailable open-world knowledge graphs. Given the recent advances in\nevent-driven Information Extraction (IE), and that prior research on\ngraph-to-text only focused on entity-driven KGs, this paper focuses on\nevent-centric data. However, our data generation system can still be adapted to\nother other types of KG data. Existing large-scale datasets in the\ngraph-to-text area are non-parallel, meaning there is a large disconnect\nbetween the KGs and text. The datasets that have a paired KG and text, are\nsmall scale and manually generated or generated without a rich ontology, making\nthe corresponding graphs sparse. Furthermore, these datasets contain many\nunlinked entities between their KG and text pairs. EventNarrative consists of\napproximately 230,000 graphs and their corresponding natural language text, 6\ntimes larger than the current largest parallel dataset. It makes use of a rich\nontology, all of the KGs entities are linked to the text, and our manual\nannotations confirm a high data quality. Our aim is two-fold: help break new\nground in event-centric research where data is lacking, and to give researchers\na well-defined, large-scale dataset in order to better evaluate existing and\nfuture knowledge graph-to-text models. We also evaluate two types of baseline\non EventNarrative: a graph-to-text specific model and two state-of-the-art\nlanguage models, which previous work has shown to be adaptable to the knowledge\ngraph-to-text domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colas_A/0/1/0/all/0/1\">Anthony Colas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghian_A/0/1/0/all/0/1\">Ali Sadeghian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daisy Zhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EmpBot: A T5-based Empathetic Chatbot focusing on Sentiments. (arXiv:2111.00310v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00310","description":"<p>In this paper, we introduce EmpBot: an end-to-end empathetic chatbot.\nEmpathetic conversational agents should not only understand what is being\ndiscussed, but also acknowledge the implied feelings of the conversation\npartner and respond appropriately. To this end, we propose a method based on a\ntransformer pretrained language model (T5). Specifically, during finetuning we\npropose to use three objectives: response language modeling, sentiment\nunderstanding, and empathy forcing. The first objective is crucial for\ngenerating relevant and coherent responses, while the next ones are significant\nfor acknowledging the sentimental state of the conversational partner and for\nfavoring empathetic responses. We evaluate our model on the EmpatheticDialogues\ndataset using both automated metrics and human evaluation. The inclusion of the\nsentiment understanding and empathy forcing auxiliary losses favor empathetic\nresponses, as human evaluation results indicate, comparing with the current\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaranis_E/0/1/0/all/0/1\">Emmanouil Zaranis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paraskevopoulos_G/0/1/0/all/0/1\">Georgios Paraskevopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsamanis_A/0/1/0/all/0/1\">Athanasios Katsamanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potamianos_A/0/1/0/all/0/1\">Alexandros Potamianos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdvCodeMix: Adversarial Attack on Code-Mixed Data. (arXiv:2111.00350v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00350","description":"<p>Research on adversarial attacks are becoming widely popular in the recent\nyears. One of the unexplored areas where prior research is lacking is the\neffect of adversarial attacks on code-mixed data. Therefore, in the present\nwork, we have explained the first generalized framework on text perturbation to\nattack code-mixed classification models in a black-box setting. We rely on\nvarious perturbation techniques that preserve the semantic structures of the\nsentences and also obscure the attacks from the perception of a human user. The\npresent methodology leverages the importance of a token to decide where to\nattack by employing various perturbation strategies. We test our strategies on\nvarious sentiment classification models trained on Bengali-English and\nHindi-English code-mixed datasets, and reduce their F1-scores by nearly 51 %\nand 53 % respectively, which can be further reduced if a larger number of\ntokens are perturbed in a given sentence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sourya Dipta Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basak_A/0/1/0/all/0/1\">Ayan Basak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandal_S/0/1/0/all/0/1\">Soumil Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dipankar Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientWord-Net: An Open Source Hotword Detection Engine based on One-shot Learning. (arXiv:2111.00379v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00379","description":"<p>Voice assistants like Siri, Google Assistant, Alexa etc. are used widely\nacross the globe for home automation, these require the use of special phrases\nalso known as hotwords to wake it up and perform an action like \"Hey Alexa!\",\n\"Ok Google!\" and \"Hey Siri!\" etc. These hotwords are detected with lightweight\nreal-time engines whose purpose is to detect the hotwords uttered by the user.\nThis paper presents the design and implementation of a hotword detection engine\nbased on one-shot learning which detects the hotword uttered by the user in\nreal-time with just one or few training samples of the hotword. This approach\nis efficient when compared to existing implementations because the process of\nadding a new hotword in the existing systems requires enormous amounts of\npositive and negative training samples and the model needs to retrain for every\nhotword. This makes the existing implementations inefficient in terms of\ncomputation and cost. The architecture proposed in this paper has achieved an\naccuracy of 94.51%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+R_C/0/1/0/all/0/1\">Chidhambararajan R</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangaur_A/0/1/0/all/0/1\">Aman Rangaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethuraman_S/0/1/0/all/0/1\">Sibi Chakkaravarthy Sethuraman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FANS: Fusing ASR and NLU for on-device SLU. (arXiv:2111.00400v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00400","description":"<p>Spoken language understanding (SLU) systems translate voice input commands to\nsemantics which are encoded as an intent and pairs of slot tags and values.\nMost current SLU systems deploy a cascade of two neural models where the first\none maps the input audio to a transcript (ASR) and the second predicts the\nintent and slots from the transcript (NLU). In this paper, we introduce FANS, a\nnew end-to-end SLU model that fuses an ASR audio encoder to a multi-task NLU\ndecoder to infer the intent, slot tags, and slot values directly from a given\ninput audio, obviating the need for transcription. FANS consists of a shared\naudio encoder and three decoders, two of which are seq-to-seq decoders that\npredict non null slot tags and slot values in parallel and in an\nauto-regressive manner. FANS neural encoder and decoders architectures are\nflexible which allows us to leverage different combinations of LSTM,\nself-attention, and attenders. Our experiments show compared to the\nstate-of-the-art end-to-end SLU models, FANS reduces ICER and IRER errors\nrelatively by 30 % and 7 %, respectively, when tested on an in-house SLU\ndataset and by 0.86 % and 2 % absolute when tested on a public SLU dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Radfar_M/0/1/0/all/0/1\">Martin Radfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1\">Athanasios Mouchtaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunzmann_S/0/1/0/all/0/1\">Siegfried Kunzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastrow_A/0/1/0/all/0/1\">Ariya Rastrow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Emotion Recognition Using Quaternion Convolutional Neural Networks. (arXiv:2111.00404v1 [cs.SD])","link":"http://arxiv.org/abs/2111.00404","description":"<p>Although speech recognition has become a widespread technology, inferring\nemotion from speech signals still remains a challenge. To address this problem,\nthis paper proposes a quaternion convolutional neural network (QCNN) based\nspeech emotion recognition (SER) model in which Mel-spectrogram features of\nspeech signals are encoded in an RGB quaternion domain. We show that our QCNN\nbased SER model outperforms other real-valued methods in the Ryerson\nAudio-Visual Database of Emotional Speech and Song (RAVDESS, 8-classes)\ndataset, achieving, to the best of our knowledge, state-of-the-art results. The\nQCNN also achieves comparable results with the state-of-the-art methods in the\nInteractive Emotional Dyadic Motion Capture (IEMOCAP 4-classes) and Berlin\nEMO-DB (7-classes) datasets. Specifically, the model achieves an accuracy of\n77.87\\%, 70.46\\%, and 88.78\\% for the RAVDESS, IEMOCAP, and EMO-DB datasets,\nrespectively. In addition, our results show that the quaternion unit structure\nis better able to encode internal dependencies to reduce its model size\nsignificantly compared to other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muppidi_A/0/1/0/all/0/1\">Aneesh Muppidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radfar_M/0/1/0/all/0/1\">Martin Radfar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Deep Residual Reasoning for Temporal Moment Localization. (arXiv:2111.00417v1 [cs.MM])","link":"http://arxiv.org/abs/2111.00417","description":"<p>Temporal Moment Localization (TML) in untrimmed videos is a challenging task\nin the field of multimedia, which aims at localizing the start and end points\nof the activity in the video, described by a sentence query. Existing methods\nmainly focus on mining the correlation between video and sentence\nrepresentations or investigating the fusion manner of the two modalities. These\nworks mainly understand the video and sentence coarsely, ignoring the fact that\na sentence can be understood from various semantics, and the dominant words\naffecting the moment localization in the semantics are the action and object\nreference. Toward this end, we propose a Hierarchical Deep Residual Reasoning\n(HDRR) model, which decomposes the video and sentence into multi-level\nrepresentations with different semantics to achieve a finer-grained\nlocalization. Furthermore, considering that videos with different resolution\nand sentences with different length have different difficulty in understanding,\nwe design the simple yet effective Res-BiGRUs for feature fusion, which is able\nto grasp the useful information in a self-adapting manner. Extensive\nexperiments conducted on Charades-STA and ActivityNet-Captions datasets\ndemonstrate the superiority of our HDRR model compared with other\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianjing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xuemeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiran Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSC-IITISM at FinCausal 2021: Combining POS tagging with Attention-based Contextual Representations for Identifying Causal Relationships in Financial Documents. (arXiv:2111.00490v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00490","description":"<p>Causality detection draws plenty of attention in the field of Natural\nLanguage Processing and linguistics research. It has essential applications in\ninformation retrieval, event prediction, question answering, financial\nanalysis, and market research. In this study, we explore several methods to\nidentify and extract cause-effect pairs in financial documents using\ntransformers. For this purpose, we propose an approach that combines POS\ntagging with the BIO scheme, which can be integrated with modern transformer\nmodels to address this challenge of identifying causality in a given text. Our\nbest methodology achieves an F1-Score of 0.9551, and an Exact Match Score of\n0.8777 on the blind test in the FinCausal-2021 Shared Task at the FinCausal\n2021 Workshop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haldar_G/0/1/0/all/0/1\">Gunjan Haldar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Aman Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Pradyumna Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visualization: the missing factor in Simultaneous Speech Translation. (arXiv:2111.00514v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00514","description":"<p>Simultaneous speech translation (SimulST) is the task in which output\ngeneration has to be performed on partial, incremental speech input. In recent\nyears, SimulST has become popular due to the spread of cross-lingual\napplication scenarios, like international live conferences and streaming\nlectures, in which on-the-fly speech translation can facilitate users' access\nto audio-visual content. In this paper, we analyze the characteristics of the\nSimulST systems developed so far, discussing their strengths and weaknesses. We\nthen concentrate on the evaluation framework required to properly assess\nsystems' effectiveness. To this end, we raise the need for a broader\nperformance analysis, also including the user experience standpoint. SimulST\nsystems, indeed, should be evaluated not only in terms of quality/latency\nmeasures, but also via task-oriented metrics accounting, for instance, for the\nvisualization strategy adopted. In light of this, we highlight which are the\ngoals achieved by the community and what is still missing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FinEAS: Financial Embedding Analysis of Sentiment. (arXiv:2111.00526v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00526","description":"<p>We introduce a new language representation model in finance called Financial\nEmbedding Analysis of Sentiment (FinEAS). In financial markets, news and\ninvestor sentiment are significant drivers of security prices. Thus, leveraging\nthe capabilities of modern NLP approaches for financial sentiment analysis is a\ncrucial component in identifying patterns and trends that are useful for market\nparticipants and regulators. In recent years, methods that use transfer\nlearning from large Transformer-based language models like BERT, have achieved\nstate-of-the-art results in text classification tasks, including sentiment\nanalysis using labelled datasets. Researchers have quickly adopted these\napproaches to financial texts, but best practices in this domain are not\nwell-established. In this work, we propose a new model for financial sentiment\nanalysis based on supervised fine-tuned sentence embeddings from a standard\nBERT model. We demonstrate our approach achieves significant improvements in\ncomparison to vanilla BERT, LSTM, and FinBERT, a financial domain specific\nBERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_M/0/1/0/all/0/1\">Miquel Noguer i Alonso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolm_P/0/1/0/all/0/1\">Petter Kolm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Reasoning via Template Filling. (arXiv:2111.00539v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00539","description":"<p>In this paper, we explore the ability of sequence to sequence models to\nperform cross-domain reasoning. Towards this, we present a\nprompt-template-filling approach to enable sequence to sequence models to\nperform cross-domain reasoning. We also present a case-study with commonsense\nand health and well-being domains, where we study how prompt-template-filling\nenables pretrained sequence to sequence models across domains. Our experiments\nacross several pretrained encoder-decoder models show that cross-domain\nreasoning is challenging for current models. We also show an in-depth error\nanalysis and avenues for future research for reasoning across domains\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_D/0/1/0/all/0/1\">Dheeraj Rajagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khetan_V/0/1/0/all/0/1\">Vivek Khetan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sacaleanu_B/0/1/0/all/0/1\">Bogdan Sacaleanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gershman_A/0/1/0/all/0/1\">Anatole Gershman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fano_A/0/1/0/all/0/1\">Andrew Fano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quality Estimation Using Round-trip Translation with Sentence Embeddings. (arXiv:2111.00554v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00554","description":"<p>Estimating the quality of machine translation systems has been an ongoing\nchallenge for researchers in this field. Many previous attempts at using\nround-trip translation as a measure of quality have failed, and there is much\ndisagreement as to whether it can be a viable method of quality estimation. In\nthis paper, we revisit round-trip translation, proposing a system which aims to\nsolve the previous pitfalls found with the approach. Our method makes use of\nrecent advances in language representation learning to more accurately gauge\nthe similarity between the original and round-trip translated sentences.\nExperiments show that while our approach does not reach the performance of\ncurrent state of the art methods, it may still be an effective approach for\nsome language pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Crone_N/0/1/0/all/0/1\">Nathan Crone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Power_A/0/1/0/all/0/1\">Adam Power</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weldon_J/0/1/0/all/0/1\">John Weldon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revealing and Protecting Labels in Distributed Training. (arXiv:2111.00556v1 [cs.LG])","link":"http://arxiv.org/abs/2111.00556","description":"<p>Distributed learning paradigms such as federated learning often involve\ntransmission of model updates, or gradients, over a network, thereby avoiding\ntransmission of private data. However, it is possible for sensitive information\nabout the training data to be revealed from such gradients. Prior works have\ndemonstrated that labels can be revealed analytically from the last layer of\ncertain models (e.g., ResNet), or they can be reconstructed jointly with model\ninputs by using Gradients Matching [Zhu et al'19] with additional knowledge\nabout the current state of the model. In this work, we propose a method to\ndiscover the set of labels of training samples from only the gradient of the\nlast layer and the id to label mapping. Our method is applicable to a wide\nvariety of model architectures across multiple domains. We demonstrate the\neffectiveness of our method for model training in two domains - image\nclassification, and automatic speech recognition. Furthermore, we show that\nexisting reconstruction techniques improve their efficacy when used in\nconjunction with our method. Conversely, we demonstrate that gradient\nquantization and sparsification can significantly reduce the success of the\nattack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Trung Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_O/0/1/0/all/0/1\">Om Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramaswamy_S/0/1/0/all/0/1\">Swaroop Ramaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathews_R/0/1/0/all/0/1\">Rajiv Mathews</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_P/0/1/0/all/0/1\">Peter Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Approach to Inference-Driven Dialogue Management within a Social Chatbot. (arXiv:2111.00570v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00570","description":"<p>We present a chatbot implementing a novel dialogue management approach based\non logical inference. Instead of framing conversation a sequence of response\ngeneration tasks, we model conversation as a collaborative inference process in\nwhich speakers share information to synthesize new knowledge in real time. Our\nchatbot pipeline accomplishes this modelling in three broad stages. The first\nstage translates user utterances into a symbolic predicate representation. The\nsecond stage then uses this structured representation in conjunction with a\nlarger knowledge base to synthesize new predicates using efficient graph\nmatching. In the third and final stage, our bot selects a small subset of\npredicates and translates them into an English response. This approach lends\nitself to understanding latent semantics of user inputs, flexible initiative\ntaking, and responses that are novel and coherent with the dialogue context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Finch_S/0/1/0/all/0/1\">Sarah E. Finch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finch_J/0/1/0/all/0/1\">James D. Finch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huryn_D/0/1/0/all/0/1\">Daniil Huryn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutsell_W/0/1/0/all/0/1\">William Hutsell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Han He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Went Wrong? Explaining Overall Dialogue Quality through Utterance-Level Impacts. (arXiv:2111.00572v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00572","description":"<p>Improving user experience of a dialogue system often requires intensive\ndeveloper effort to read conversation logs, run statistical analyses, and\nintuit the relative importance of system shortcomings. This paper presents a\nnovel approach to automated analysis of conversation logs that learns the\nrelationship between user-system interactions and overall dialogue quality.\nUnlike prior work on utterance-level quality prediction, our approach learns\nthe impact of each interaction from the overall user rating without\nutterance-level annotation, allowing resultant model conclusions to be derived\non the basis of empirical evidence and at low cost. Our model identifies\ninteractions that have a strong correlation with the overall dialogue quality\nin a chatbot setting. Experiments show that the automated analysis from our\nmodel agrees with expert judgments, making this work the first to show that\nsuch weakly-supervised learning of utterance-level quality prediction is highly\nachievable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Finch_J/0/1/0/all/0/1\">James D. Finch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finch_S/0/1/0/all/0/1\">Sarah E. Finch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Classification for Task-based Source Code Related Questions. (arXiv:2111.00580v1 [cs.SE])","link":"http://arxiv.org/abs/2111.00580","description":"<p>There is a key demand to automatically generate code for small tasks for\ndevelopers. Websites such as StackOverflow provide a simplistic way by offering\nsolutions in small snippets which provide a complete answer to whatever task\nquestion the developer wants to code. Natural Language Processing and\nparticularly Question-Answering Systems are very helpful in resolving and\nworking on these tasks. In this paper, we develop a two-fold deep learning\nmodel: Seq2Seq and a binary classifier that takes in the intent (which is in\nnatural language) and code snippets in Python. We train both the intent and the\ncode utterances in the Seq2Seq model, where we decided to compare the effect of\nthe hidden layer embedding from the encoder for representing the intent and\nsimilarly, using the decoder's hidden layer embeddings for the code sequence.\nThen we combine both these embeddings and then train a simple binary neural\nnetwork classifier model for predicting if the intent is correctly answered by\nthe predicted code sequence from the seq2seq model. We find that the hidden\nstate layer's embeddings perform slightly better than regular standard\nembeddings from a constructed vocabulary. We experimented with our tests on the\nCoNaLa dataset in addition to the StaQC database consisting of simple task-code\nsnippet-based pairs. We empirically establish that using additional pre-trained\nembeddings for code snippets in Python is less context-based in comparison to\nusing hidden state context vectors from seq2seq models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vijayaraghavan_S/0/1/0/all/0/1\">Sairamvinay Vijayaraghavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jinxiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomassi_D/0/1/0/all/0/1\">David Tomassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punj_S/0/1/0/all/0/1\">Siddhartha Punj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_J/0/1/0/all/0/1\">Jailan Sabet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimum Description Length Recurrent Neural Networks. (arXiv:2111.00600v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00600","description":"<p>We train neural networks to optimize a Minimum Description Length score,\ni.e., to balance between the complexity of the network and its accuracy at a\ntask. We show that networks trained with this objective function master tasks\ninvolving memory challenges such as counting, including cases that go beyond\ncontext-free languages. These learners master grammars for, e.g., $a^nb^n$,\n$a^nb^nc^n$, $a^nb^{2n}$, and $a^nb^mc^{n+m}$, and they perform addition. They\ndo so with 100% accuracy, sometimes also with 100% confidence. The networks are\nalso small and their inner workings are transparent. We thus provide formal\nproofs that their perfect accuracy holds not only on a given test set, but for\nany input sequence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_N/0/1/0/all/0/1\">Nur Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geyer_M/0/1/0/all/0/1\">Michal Geyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chemla_E/0/1/0/all/0/1\">Emmanuel Chemla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katzir_R/0/1/0/all/0/1\">Roni Katzir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Investigation of Commonsense Understanding in Large Language Models. (arXiv:2111.00607v1 [cs.CL])","link":"http://arxiv.org/abs/2111.00607","description":"<p>Large language models have shown impressive performance on many natural\nlanguage processing (NLP) tasks in a zero-shot setting. We ask whether these\nmodels exhibit commonsense understanding -- a critical component of NLP\napplications -- by evaluating models against four commonsense benchmarks. We\nfind that the impressive zero-shot performance of large language models is\nmostly due to existence of dataset bias in our benchmarks. We also show that\nthe zero-shot performance is sensitive to the choice of hyper-parameters and\nsimilarity of the benchmark to the pre-training datasets. Moreover, we did not\nobserve substantial improvements when evaluating models in a few-shot setting.\nFinally, in contrast to previous work, we find that leveraging explicit\ncommonsense knowledge does not yield substantial improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lorraine Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuncoro_A/0/1/0/all/0/1\">Adhi Kuncoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+dAutume_C/0/1/0/all/0/1\">Cyprien de Masson d&#x27;Autume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blunsom_P/0/1/0/all/0/1\">Phil Blunsom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Unreasonable Effectiveness of Machine Learning in Moldavian versus Romanian Dialect Identification. (arXiv:2007.15700v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.15700","description":"<p>Motivated by the seemingly high accuracy levels of machine learning models in\nMoldavian versus Romanian dialect identification and the increasing research\ninterest on this topic, we provide a follow-up on the Moldavian versus Romanian\nCross-Dialect Topic Identification (MRC) shared task of the VarDial 2019\nEvaluation Campaign. The shared task included two sub-task types: one that\nconsisted in discriminating between the Moldavian and Romanian dialects and one\nthat consisted in classifying documents by topic across the two dialects of\nRomanian. Participants achieved impressive scores, e.g. the top model for\nMoldavian versus Romanian dialect identification obtained a macro F1 score of\n0.895. We conduct a subjective evaluation by human annotators, showing that\nhumans attain much lower accuracy rates compared to machine learning (ML)\nmodels. Hence, it remains unclear why the methods proposed by participants\nattain such high accuracy rates. Our goal is to understand (i) why the proposed\nmethods work so well (by visualizing the discriminative features) and (ii) to\nwhat extent these methods can keep their high accuracy levels, e.g. when we\nshorten the text samples to single sentences or when we use tweets at inference\ntime. A secondary goal of our work is to propose an improved ML model using\nensemble learning. Our experiments show that ML models can accurately identify\nthe dialects, even at the sentence level and across different domains (news\narticles versus tweets). We also analyze the most discriminative features of\nthe best performing models, providing some explanations behind the decisions\ntaken by these models. Interestingly, we learn new dialectal patterns\npreviously unknown to us or to our human annotators. Furthermore, we conduct\nexperiments showing that the machine learning performance on the MRC shared\ntask can be improved through an ensemble based on stacking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaman_M/0/1/0/all/0/1\">Mihaela G&#x103;man</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Looking for Clues of Language in Multilingual BERT to Improve Cross-lingual Generalization. (arXiv:2010.10041v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.10041","description":"<p>Token embeddings in multilingual BERT (m-BERT) contain both language and\nsemantic information. We find that the representation of a language can be\nobtained by simply averaging the embeddings of the tokens of the language.\nGiven this language representation, we control the output languages of\nmultilingual BERT by manipulating the token embeddings, thus achieving\nunsupervised token translation. We further propose a computationally cheap but\neffective approach to improve the cross-lingual ability of m-BERT based on this\nobservation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chi-Liang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_T/0/1/0/all/0/1\">Tsung-Yuan Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Sung Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chung-Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Multiple Choices Question Answering: Start Learning from Basic Knowledge. (arXiv:2010.11003v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.11003","description":"<p>In this paper, we study the possibility of almost unsupervised Multiple\nChoices Question Answering (MCQA). Starting from very basic knowledge, MCQA\nmodel knows that some choices have higher probabilities of being correct than\nthe others. The information, though very noisy, guides the training of an MCQA\nmodel. The proposed method is shown to outperform the baseline approaches on\nRACE and even comparable with some supervised learning approaches on MC500.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chi-Liang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Text Style Transfer: A Survey. (arXiv:2011.00416v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.00416","description":"<p>Text style transfer is an important task in natural language generation,\nwhich aims to control certain attributes in the generated text, such as\npoliteness, emotion, humor, and many others. It has a long history in the field\nof natural language processing, and recently has re-gained significant\nattention thanks to the promising performance brought by deep neural models. In\nthis paper, we present a systematic survey of the research on neural text style\ntransfer, spanning over 100 representative articles since the first neural text\nstyle transfer work in 2017. We discuss the task formulation, existing datasets\nand subtasks, evaluation, as well as the rich methodologies in the presence of\nparallel and non-parallel data. We also provide discussions on a variety of\nimportant topics regarding the future development of this task. Our curated\npaper list is at https://github.com/zhijing-jin/Text_Style_Transfer_Survey\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vechtomova_O/0/1/0/all/0/1\">Olga Vechtomova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Portuguese Semantic Role Labeling with Transformers and Transfer Learning. (arXiv:2101.01213v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.01213","description":"<p>The Natural Language Processing task of determining \"Who did what to whom\" is\ncalled Semantic Role Labeling. For English, recent methods based on Transformer\nmodels have allowed for major improvements in this task over the previous state\nof the art. However, for low resource languages, like Portuguese, currently\navailable semantic role labeling models are hindered by scarce training data.\nIn this paper, we explore a model architecture with only a pre-trained\nTransformer-based model, a linear layer, softmax and Viterbi decoding. We\nsubstantially improve the state-of-the-art performance in Portuguese by over 15\nF1. Additionally, we improve semantic role labeling results in Portuguese\ncorpora by exploiting cross-lingual transfer learning using multilingual\npre-trained models, and transfer learning from dependency parsing in\nPortuguese, evaluating the various proposed approaches empirically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_S/0/1/0/all/0/1\">Sofia Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loureiro_D/0/1/0/all/0/1\">Daniel Loureiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorge_A/0/1/0/all/0/1\">Al&#xed;pio Jorge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Extraction of Causal Relations from Natural Language Text. (arXiv:2101.06426v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2101.06426","description":"<p>As an essential component of human cognition, cause-effect relations appear\nfrequently in text, and curating cause-effect relations from text helps in\nbuilding causal networks for predictive tasks. Existing causality extraction\ntechniques include knowledge-based, statistical machine learning(ML)-based, and\ndeep learning-based approaches. Each method has its advantages and weaknesses.\nFor example, knowledge-based methods are understandable but require extensive\nmanual domain knowledge and have poor cross-domain applicability. Statistical\nmachine learning methods are more automated because of natural language\nprocessing (NLP) toolkits. However, feature engineering is labor-intensive, and\ntoolkits may lead to error propagation. In the past few years, deep learning\ntechniques attract substantial attention from NLP researchers because of its'\npowerful representation learning ability and the rapid increase in\ncomputational resources. Their limitations include high computational costs and\na lack of adequate annotated training data. In this paper, we conduct a\ncomprehensive survey of causality extraction. We initially introduce primary\nforms existing in the causality extraction: explicit intra-sentential\ncausality, implicit causality, and inter-sentential causality. Next, we list\nbenchmark datasets and modeling assessment methods for causal relation\nextraction. Then, we present a structured overview of the three techniques with\ntheir representative systems. Lastly, we highlight existing open challenges\nwith their potential directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1\">Josiah Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"First Target and Opinion then Polarity: Enhancing Target-opinion Correlation for Aspect Sentiment Triplet Extraction. (arXiv:2102.08549v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.08549","description":"<p>Aspect Sentiment Triplet Extraction (ASTE) aims to extract triplets from a\nsentence, including target entities, associated sentiment polarities, and\nopinion spans which rationalize the polarities. Existing methods are short on\nbuilding correlation between target-opinion pairs, and neglect the mutual\ninterference among different sentiment triplets. To address these issues, we\nutilize a two-stage framework to enhance the correlation between targets and\nopinions: at stage one, we extract targets and opinions through sequence\ntagging; then we append a group of artificial tags named Perceivable Pair,\nwhich indicate the span of a specific target-opinion tuple, to the input\nsentence to obtain closer correlated target-opinion pair representation.\nMeanwhile, we reduce the negative interference between triplets by restricting\ntokens' attention field. Finally, the polarity is identified according to the\nrepresentation of the Perceivable Pair. We conduct experiments on four\ndatasets, and the experimental results show the effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lianzhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sujian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaodong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhicong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Houfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELLA: Exploration through Learned Language Abstraction. (arXiv:2103.05825v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.05825","description":"<p>Building agents capable of understanding language instructions is critical to\neffective and robust human-AI collaboration. Recent work focuses on training\nthese agents via reinforcement learning in environments with synthetic\nlanguage; however, instructions often define long-horizon, sparse-reward tasks,\nand learning policies requires many episodes of experience. We introduce ELLA:\nExploration through Learned Language Abstraction, a reward shaping approach\ngeared towards boosting sample efficiency in sparse reward environments by\ncorrelating high-level instructions with simpler low-level constituents. ELLA\nhas two key elements: 1) A termination classifier that identifies when agents\ncomplete low-level instructions, and 2) A relevance classifier that correlates\nlow-level instructions with success on high-level tasks. We learn the\ntermination classifier offline from pairs of instructions and terminal states.\nNotably, in departure from prior work in language and abstraction, we learn the\nrelevance classifier online, without relying on an explicit decomposition of\nhigh-level instructions to low-level instructions. On a suite of complex BabyAI\nenvironments with varying instruction complexities and reward sparsity, ELLA\nshows gains in sample efficiency relative to language-based shaping and\ntraditional RL methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirchandani_S/0/1/0/all/0/1\">Suvir Mirchandani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karamcheti_S/0/1/0/all/0/1\">Siddharth Karamcheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1\">Dorsa Sadigh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HBert + BiasCorp -- Fighting Racism on the Web. (arXiv:2104.02242v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.02242","description":"<p>Subtle and overt racism is still present both in physical and online\ncommunities today and has impacted many lives in different segments of the\nsociety. In this short piece of work, we present how we're tackling this\nsocietal issue with Natural Language Processing. We are releasing BiasCorp, a\ndataset containing 139,090 comments and news segment from three specific\nsources - Fox News, BreitbartNews and YouTube. The first batch (45,000 manually\nannotated) is ready for publication. We are currently in the final phase of\nmanually labeling the remaining dataset using Amazon Mechanical Turk. BERT has\nbeen used widely in several downstream tasks. In this work, we present hBERT,\nwhere we modify certain layers of the pretrained BERT model with the new\nHopfield Layer. hBert generalizes well across different distributions with the\nadded advantage of a reduced model complexity. We are also releasing a\nJavaScript library and a Chrome Extension Application, to help developers make\nuse of our trained model in web applications (say chat application) and for\nusers to identify and report racially biased contents on the web respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Onabola_O/0/1/0/all/0/1\">Olawale Onabola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhuang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akera_B/0/1/0/all/0/1\">Benjamin Akera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibraheem_A/0/1/0/all/0/1\">Abdulrahman Ibraheem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jia Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dianbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Achieving Model Robustness through Discrete Adversarial Training. (arXiv:2104.05062v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.05062","description":"<p>Discrete adversarial attacks are symbolic perturbations to a language input\nthat preserve the output label but lead to a prediction error. While such\nattacks have been extensively explored for the purpose of evaluating model\nrobustness, their utility for improving robustness has been limited to offline\naugmentation only. Concretely, given a trained model, attacks are used to\ngenerate perturbed (adversarial) examples, and the model is re-trained exactly\nonce. In this work, we address this gap and leverage discrete attacks for\nonline augmentation, where adversarial examples are generated at every training\nstep, adapting to the changing nature of the model. We propose (i) a new\ndiscrete attack, based on best-first search, and (ii) random sampling attacks\nthat unlike prior work are not based on expensive search-based procedures.\nSurprisingly, we find that random sampling leads to impressive gains in\nrobustness, outperforming the commonly-used offline augmentation, while leading\nto a speedup at training time of ~10x. Furthermore, online augmentation with\nsearch-based attacks justifies the higher training cost, significantly\nimproving robustness on three datasets. Last, we show that our new attack\nsubstantially improves robustness compared to prior methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivgi_M/0/1/0/all/0/1\">Maor Ivgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing small BERTs trained for German NER. (arXiv:2104.11559v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.11559","description":"<p>Currently, the most widespread neural network architecture for training\nlanguage models is the so called BERT which led to improvements in various\nNatural Language Processing (NLP) tasks. In general, the larger the number of\nparameters in a BERT model, the better the results obtained in these NLP tasks.\nUnfortunately, the memory consumption and the training duration drastically\nincreases with the size of these models. In this article, we investigate\nvarious training techniques of smaller BERT models: We combine different\nmethods from other BERT variants like ALBERT, RoBERTa, and relative positional\nencoding. In addition, we propose two new fine-tuning modifications leading to\nbetter performance: Class-Start-End tagging and a modified form of Linear Chain\nConditional Random Fields. Furthermore, we introduce Whole-Word Attention which\nreduces BERTs memory usage and leads to a small increase in performance\ncompared to classical Multi-Head-Attention. We evaluate these techniques on\nfive public German Named Entity Recognition (NER) tasks of which two are\nintroduced by this article.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1\">Jochen Z&#xf6;llner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sperfeld_K/0/1/0/all/0/1\">Konrad Sperfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wick_C/0/1/0/all/0/1\">Christoph Wick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labahn_R/0/1/0/all/0/1\">Roger Labahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PREDICT: Persian Reverse Dictionary. (arXiv:2105.00309v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.00309","description":"<p>Finding the appropriate words to convey concepts (i.e., lexical access) is\nessential for effective communication. Reverse dictionaries fulfill this need\nby helping individuals to find the word(s) which could relate to a specific\nconcept or idea. To the best of our knowledge, this resource has not been\navailable for the Persian language. In this paper, we compare four different\narchitectures for implementing a Persian reverse dictionary (PREDICT).\n</p>\n<p>We evaluate our models using (phrase,word) tuples extracted from the only\nPersian dictionaries available online, namely Amid, Moein, and Dehkhoda where\nthe phrase describes the word. Given the phrase, a model suggests the most\nrelevant word(s) in terms of the ability to convey the concept. The model is\nconsidered to perform well if the correct word is one of its top suggestions.\n</p>\n<p>Our experiments show that a model consisting of Long Short-Term Memory (LSTM)\nunits enhanced by an additive attention mechanism is enough to produce\nsuggestions comparable to (or in some cases better than) the word in the\noriginal dictionary. The study also reveals that the model sometimes produces\nthe synonyms of the word as its output which led us to introduce a new metric\nfor the evaluation of reverse dictionaries called Synonym Accuracy accounting\nfor the percentage of times the event of producing the word or a synonym of it\noccurs. The assessment of the best model using this new metric also indicates\nthat at least 62% of the times, it produces an accurate result within the top\n100 suggestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malekzadeh_A/0/1/0/all/0/1\">Arman Malekzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gheibi_A/0/1/0/all/0/1\">Amin Gheibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohades_A/0/1/0/all/0/1\">Ali Mohades</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual and crosslingual speech recognition using phonological-vector based phone embeddings. (arXiv:2107.05038v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.05038","description":"<p>The use of phonological features (PFs) potentially allows language-specific\nphones to remain linked in training, which is highly desirable for information\nsharing for multilingual and crosslingual speech recognition methods for\nlow-resourced languages. A drawback suffered by previous methods in using\nphonological features is that the acoustic-to-PF extraction in a bottom-up way\nis itself difficult. In this paper, we propose to join phonology driven phone\nembedding (top-down) and deep neural network (DNN) based acoustic feature\nextraction (bottom-up) to calculate phone probabilities. The new method is\ncalled JoinAP (Joining of Acoustics and Phonology). Remarkably, no inversion\nfrom acoustics to phonological features is required for speech recognition. For\neach phone in the IPA (International Phonetic Alphabet) table, we encode its\nphonological features to a phonological-vector, and then apply linear or\nnonlinear transformation of the phonological-vector to obtain the phone\nembedding. A series of multilingual and crosslingual (both zero-shot and\nfew-shot) speech recognition experiments are conducted on the CommonVoice\ndataset (German, French, Spanish and Italian) and the AISHLL-1 dataset\n(Mandarin), and demonstrate the superiority of JoinAP with nonlinear phone\nembeddings over both JoinAP with linear phone embeddings and the traditional\nmethod with flat phone embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chengrui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_K/0/1/0/all/0/1\">Keyu An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huahuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-to-Sequence Learning with Latent Neural Grammars. (arXiv:2109.01135v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01135","description":"<p>Sequence-to-sequence learning with neural networks has become the de facto\nstandard for sequence prediction tasks. This approach typically models the\nlocal distribution over the next word with a powerful neural network that can\ncondition on arbitrary context. While flexible and performant, these models\noften require large datasets for training and can fail spectacularly on\nbenchmarks designed to test for compositional generalization. This work\nexplores an alternative, hierarchical approach to sequence-to-sequence learning\nwith quasi-synchronous grammars, where each node in the target tree is\ntransduced by a node in the source tree. Both the source and target trees are\ntreated as latent and induced during training. We develop a neural\nparameterization of the grammar which enables parameter sharing over the\ncombinatorial space of derivation rules without the need for manual feature\nengineering. We apply this latent neural grammar to various domains -- a\ndiagnostic language navigation task designed to test for compositional\ngeneralization (SCAN), style transfer, and small-scale machine translation --\nand find that it performs respectably compared to standard baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncovering the Limits of Text-based Emotion Detection. (arXiv:2109.01900v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01900","description":"<p>Identifying emotions from text is crucial for a variety of real world tasks.\nWe consider the two largest now-available corpora for emotion classification:\nGoEmotions, with 58k messages labelled by readers, and Vent, with 33M\nwriter-labelled messages. We design a benchmark and evaluate several feature\nspaces and learning algorithms, including two simple yet novel models on top of\nBERT that outperform previous strong baselines on GoEmotions. Through an\nexperiment with human participants, we also analyze the differences between how\nwriters express emotions and how readers perceive them. Our results suggest\nthat emotions expressed by writers are harder to identify than emotions that\nreaders perceive. We share a public web interface for researchers to explore\nour models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Gonzalez_N/0/1/0/all/0/1\">Nurudin Alvarez-Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaltenbrunner_A/0/1/0/all/0/1\">Andreas Kaltenbrunner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_V/0/1/0/all/0/1\">Vicen&#xe7; G&#xf3;mez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaker-Oriented Latent Structures for Dialogue-Based Relation Extraction. (arXiv:2109.05182v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05182","description":"<p>Dialogue-based relation extraction (DiaRE) aims to detect the structural\ninformation from unstructured utterances in dialogues. Existing relation\nextraction models may be unsatisfactory under such a conversational setting,\ndue to the entangled logic and information sparsity issues in utterances\ninvolving multiple speakers. To this end, we introduce SOLS, a novel model\nwhich can explicitly induce speaker-oriented latent structures for better\nDiaRE. Specifically, we learn latent structures to capture the relationships\namong tokens beyond the utterance boundaries, alleviating the entangled logic\nissue. During the learning process, our speaker-specific regularization method\nprogressively highlights speaker-related key clues and erases the irrelevant\nones, alleviating the information sparsity issue. Experiments on three public\ndatasets demonstrate the effectiveness of our proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nan_G/0/1/0/all/0/1\">Guoshun Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Guoqing Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_S/0/1/0/all/0/1\">Sicong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Variational Graph Autoencoders for Unsupervised Cross-domain Prerequisite Chains. (arXiv:2109.08722v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.08722","description":"<p>Prerequisite chain learning helps people acquire new knowledge efficiently.\nWhile people may quickly determine learning paths over concepts in a domain,\nfinding such paths in other domains can be challenging. We introduce\nDomain-Adversarial Variational Graph Autoencoders (DAVGAE) to solve this\ncross-domain prerequisite chain learning task efficiently. Our novel model\nconsists of a variational graph autoencoder (VGAE) and a domain discriminator.\nThe VGAE is trained to predict concept relations through link prediction, while\nthe domain discriminator takes both source and target domain data as input and\nis trained to predict domain labels. Most importantly, this method only needs\nsimple homogeneous graphs as input, compared with the current state-of-the-art\nmodel. We evaluate our model on the LectureBankCD dataset, and results show\nthat our model outperforms recent graph-based benchmarks while using only 1/10\nof graph scale and 1/3 computation time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_V/0/1/0/all/0/1\">Vanessa Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsolved Problems in ML Safety. (arXiv:2109.13916v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.13916","description":"<p>Machine learning (ML) systems are rapidly increasing in size, are acquiring\nnew capabilities, and are increasingly deployed in high-stakes settings. As\nwith other powerful technologies, safety for ML should be a leading research\npriority. In response to emerging safety challenges in ML, such as those\nintroduced by recent large-scale models, we provide a new roadmap for ML Safety\nand refine the technical problems that the field needs to address. We present\nfour problems ready for research, namely withstanding hazards (\"Robustness\"),\nidentifying hazards (\"Monitoring\"), steering ML systems (\"Alignment\"), and\nreducing hazards in deployment (\"External Safety\"). Throughout, we clarify each\nproblem's motivation and provide concrete research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulman_J/0/1/0/all/0/1\">John Schulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Explainability of BERT-Based Evaluation Metrics by Disentangling along Linguistic Factors. (arXiv:2110.04399v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04399","description":"<p>Evaluation metrics are a key ingredient for progress of text generation\nsystems. In recent years, several BERT-based evaluation metrics have been\nproposed (including BERTScore, MoverScore, BLEURT, etc.) which correlate much\nbetter with human assessment of text generation quality than BLEU or ROUGE,\ninvented two decades ago. However, little is known what these metrics, which\nare based on black-box language model representations, actually capture (it is\ntypically assumed they model semantic similarity). In this work, we use a\nsimple regression based global explainability technique to disentangle metric\nscores along linguistic factors, including semantics, syntax, morphology, and\nlexical overlap. We show that the different metrics capture all aspects to some\ndegree, but that they are all substantially sensitive to lexical overlap, just\nlike BLEU and ROUGE. This exposes limitations of these novelly proposed\nmetrics, which we also highlight in an adversarial test scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaster_M/0/1/0/all/0/1\">Marvin Kaster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Hate Speech Detection in Code Mixed Text using Transformer Based Approaches. (arXiv:2110.09338v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.09338","description":"<p>In the recent past, social media platforms have helped people in connecting\nand communicating to a wider audience. But this has also led to a drastic\nincrease in cyberbullying. It is essential to detect and curb hate speech to\nkeep the sanity of social media platforms. Also, code mixed text containing\nmore than one language is frequently used on these platforms. We, therefore,\npropose automated techniques for hate speech detection in code mixed text from\nscraped Twitter. We specifically focus on code mixed English-Hindi text and\ntransformer-based approaches. While regular approaches analyze the text\nindependently, we also make use of content text in the form of parent tweets.\nWe try to evaluate the performances of multilingual BERT and Indic-BERT in\nsingle-encoder and dual-encoder settings. The first approach is to concatenate\nthe target text and context text using a separator token and get a single\nrepresentation from the BERT model. The second approach encodes the two texts\nindependently using a dual BERT encoder and the corresponding representations\nare averaged. We show that the dual-encoder approach using independent\nrepresentations yields better performance. We also employ simple ensemble\nmethods to further improve the performance. Using these methods we report the\nbest F1 score of 73.07% on the HASOC 2021 ICHCL code mixed data set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_R/0/1/0/all/0/1\">Ravindra Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NormFormer: Improved Transformer Pretraining with Extra Normalization. (arXiv:2110.09456v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.09456","description":"<p>During pretraining, the Pre-LayerNorm transformer suffers from a gradient\nmagnitude mismatch: gradients at early layers are much larger than at later\nlayers. These issues can be alleviated by our proposed NormFormer architecture,\nwhich adds three normalization operations to each layer: a Layer Norm after\nself attention, head-wise scaling of self-attention outputs, and a Layer Norm\nafter the first fully connected layer. The extra operations incur negligible\ncompute cost (+0.4% parameter increase), but improve pretraining perplexity and\ndownstream task performance for both causal and masked language models ranging\nfrom 125 Million to 2.7 Billion parameters. For example, adding NormFormer on\ntop of our strongest 1.3B parameter baseline can reach equal perplexity 24%\nfaster, or converge 0.27 perplexity better in the same compute budget. This\nmodel reaches GPT3-Large (1.3B) zero shot performance 60% faster. For masked\nlanguage modeling, NormFormer improves fine-tuned GLUE performance by 1.9% on\naverage. Code to train NormFormer models is available in fairseq\nhttps://github.com/pytorch/fairseq/tree/main/examples/normformer .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shleifer_S/0/1/0/all/0/1\">Sam Shleifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_M/0/1/0/all/0/1\">Myle Ott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hate and Offensive Speech Detection in Hindi and Marathi. (arXiv:2110.12200v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.12200","description":"<p>Sentiment analysis is the most basic NLP task to determine the polarity of\ntext data. There has been a significant amount of work in the area of\nmultilingual text as well. Still hate and offensive speech detection faces a\nchallenge due to inadequate availability of data, especially for Indian\nlanguages like Hindi and Marathi. In this work, we consider hate and offensive\nspeech detection in Hindi and Marathi texts. The problem is formulated as a\ntext classification task using the state of the art deep learning approaches.\nWe explore different deep learning architectures like CNN, LSTM, and variations\nof BERT like multilingual BERT, IndicBERT, and monolingual RoBERTa. The basic\nmodels based on CNN and LSTM are augmented with fast text word embeddings. We\nuse the HASOC 2021 Hindi and Marathi hate speech datasets to compare these\nalgorithms. The Marathi dataset consists of binary labels and the Hindi dataset\nconsists of binary as well as more-fine grained labels. We show that the\ntransformer-based models perform the best and even the basic models along with\nFastText embeddings give a competitive performance. Moreover, with normal\nhyper-parameter tuning, the basic models perform better than BERT-based models\non the fine-grained Hindi dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Velankar_A/0/1/0/all/0/1\">Abhishek Velankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_H/0/1/0/all/0/1\">Hrushikesh Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gore_A/0/1/0/all/0/1\">Amol Gore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salunke_S/0/1/0/all/0/1\">Shubham Salunke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic Entity Representation Model for Reasoning over Knowledge Graphs. (arXiv:2110.13522v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.13522","description":"<p>Logical reasoning over Knowledge Graphs (KGs) is a fundamental technique that\ncan provide efficient querying mechanism over large and incomplete databases.\nCurrent approaches employ spatial geometries such as boxes to learn query\nrepresentations that encompass the answer entities and model the logical\noperations of projection and intersection. However, their geometry is\nrestrictive and leads to non-smooth strict boundaries, which further results in\nambiguous answer entities. Furthermore, previous works propose transformation\ntricks to handle unions which results in non-closure and, thus, cannot be\nchained in a stream. In this paper, we propose a Probabilistic Entity\nRepresentation Model (PERM) to encode entities as a Multivariate Gaussian\ndensity with mean and covariance parameters to capture its semantic position\nand smooth decision boundary, respectively. Additionally, we also define the\nclosed logical operations of projection, intersection, and union that can be\naggregated using an end-to-end objective function. On the logical query\nreasoning problem, we demonstrate that the proposed PERM significantly\noutperforms the state-of-the-art methods on various public benchmark KG\ndatasets on standard evaluation metrics. We also evaluate PERM's competence on\na COVID-19 drug-repurposing case study and show that our proposed work is able\nto recommend drugs with substantially better F1 than current methods. Finally,\nwe demonstrate the working of our PERM's query answering process through a\nlow-dimensional visualization of the Gaussian representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_N/0/1/0/all/0/1\">Nurendra Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_N/0/1/0/all/0/1\">Nikhil Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katariya_S/0/1/0/all/0/1\">Sumeet Katariya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subbian_K/0/1/0/all/0/1\">Karthik Subbian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chandan K. Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Vagueness Detection with Deep Learning to Identify Fake News. (arXiv:2110.14780v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.14780","description":"<p>In this paper, we combine two independent detection methods for identifying\nfake news: the algorithm VAGO uses semantic rules combined with NLP techniques\nto measure vagueness and subjectivity in texts, while the classifier FAKE-CLF\nrelies on Convolutional Neural Network classification and supervised deep\nlearning to classify texts as biased or legitimate. We compare the results of\nthe two methods on four corpora. We find a positive correlation between the\nvagueness and subjectivity measures obtained by VAGO, and the classification of\ntext as biased by FAKE-CLF. The comparison yields mutual benefits: VAGO helps\nexplain the results of FAKE-CLF. Conversely FAKE-CLF helps us corroborate and\nexpand VAGO's database. The use of two complementary techniques (rule-based vs\ndata-driven) proves a fruitful approach for the challenging problem of\nidentifying fake news.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guelorget_P/0/1/0/all/0/1\">Paul Gu&#xe9;lorget</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Icard_B/0/1/0/all/0/1\">Benjamin Icard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadek_G/0/1/0/all/0/1\">Guillaume Gadek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gahbiche_S/0/1/0/all/0/1\">Souhir Gahbiche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatepaille_S/0/1/0/all/0/1\">Sylvain Gatepaille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atemezing_G/0/1/0/all/0/1\">Ghislain Atemezing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egre_P/0/1/0/all/0/1\">Paul &#xc9;gr&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-11-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Adaptive Hierarchical Similarity Metric Learning with Noisy Labels. (arXiv:2111.00006v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00006","description":"<p>Deep Metric Learning (DML) plays a critical role in various machine learning\ntasks. However, most existing deep metric learning methods with binary\nsimilarity are sensitive to noisy labels, which are widely present in\nreal-world data. Since these noisy labels often cause severe performance\ndegradation, it is crucial to enhance the robustness and generalization ability\nof DML. In this paper, we propose an Adaptive Hierarchical Similarity Metric\nLearning method. It considers two noise-insensitive information, \\textit{i.e.},\nclass-wise divergence and sample-wise consistency. Specifically, class-wise\ndivergence can effectively excavate richer similarity information beyond binary\nin modeling by taking advantage of Hyperbolic metric learning, while\nsample-wise consistency can further improve the generalization ability of the\nmodel using contrastive augmentation. More importantly, we design an adaptive\nstrategy to integrate this information in a unified view. It is noteworthy that\nthe new method can be extended to any pair-based metric loss. Extensive\nexperimental results on benchmark datasets demonstrate that our method achieves\nstate-of-the-art performance compared with current deep metric learning\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jiexi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Lei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Cheng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Agnostic Few-Shot Learning For Document Intelligence. (arXiv:2111.00007v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00007","description":"<p>Few-shot learning aims to generalize to novel classes with only a few samples\nwith class labels. Research in few-shot learning has borrowed techniques from\ntransfer learning, metric learning, meta-learning, and Bayesian methods. These\nmethods also aim to train models from limited training samples, and while\nencouraging performance has been achieved, they often fail to generalize to\nnovel domains. Many of the existing meta-learning methods rely on training data\nfor which the base classes are sampled from the same domain as the novel\nclasses used for meta-testing. However, in many applications in the industry,\nsuch as document classification, collecting large samples of data for\nmeta-learning is infeasible or impossible. While research in the field of the\ncross-domain few-shot learning exists, it is mostly limited to computer vision.\nTo our knowledge, no work yet exists that examines the use of few-shot learning\nfor classification of semi-structured documents (scans of paper documents)\ngenerated as part of a business workflow (forms, letters, bills, etc.). Here\nthe domain shift is significant, going from natural images to the\nsemi-structured documents of interest. In this work, we address the problem of\nfew-shot document image classification under domain shift. We evaluate our work\nby extensive comparisons with existing methods. Experimental results\ndemonstrate that the proposed method shows consistent improvements on the\nfew-shot classification performance under domain shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mandivarapu_J/0/1/0/all/0/1\">Jaya Krishna Mandivarapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+bunch_E/0/1/0/all/0/1\">Eric bunch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+fung_G/0/1/0/all/0/1\">Glenn fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On-device Real-time Hand Gesture Recognition. (arXiv:2111.00038v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00038","description":"<p>We present an on-device real-time hand gesture recognition (HGR) system,\nwhich detects a set of predefined static gestures from a single RGB camera. The\nsystem consists of two parts: a hand skeleton tracker and a gesture classifier.\nWe use MediaPipe Hands as the basis of the hand skeleton tracker, improve the\nkeypoint accuracy, and add the estimation of 3D keypoints in a world metric\nspace. We create two different gesture classifiers, one based on heuristics and\nthe other using neural networks (NN).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sung_G/0/1/0/all/0/1\">George Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokal_K/0/1/0/all/0/1\">Kanstantsin Sokal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uboweja_E/0/1/0/all/0/1\">Esha Uboweja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazarevsky_V/0/1/0/all/0/1\">Valentin Bazarevsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baccash_J/0/1/0/all/0/1\">Jonathan Baccash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazavan_E/0/1/0/all/0/1\">Eduard Gabriel Bazavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chuo-Ling Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grundmann_M/0/1/0/all/0/1\">Matthias Grundmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CvS: Classification via Segmentation For Small Datasets. (arXiv:2111.00042v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00042","description":"<p>Deep learning models have shown promising results in a wide range of computer\nvision applications across various domains. The success of deep learning\nmethods relies heavily on the availability of a large amount of data. Deep\nneural networks are prone to overfitting when data is scarce. This problem\nbecomes even more severe for neural network with classification head with\naccess to only a few data points. However, acquiring large-scale datasets is\nvery challenging, laborious, or even infeasible in some domains. Hence,\ndeveloping classifiers that are able to perform well in small data regimes is\ncrucial for applications with limited data. This paper presents CvS, a\ncost-effective classifier for small datasets that derives the classification\nlabels from predicting the segmentation maps. We employ the label propagation\nmethod to achieve a fully segmented dataset with only a handful of manually\nsegmented data. We evaluate the effectiveness of our framework on diverse\nproblems showing that CvS is able to achieve much higher classification results\ncompared to previous methods when given only a handful of examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mojab_N/0/1/0/all/0/1\">Nooshin Mojab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hallak_J/0/1/0/all/0/1\">Joelle A. Hallak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_D/0/1/0/all/0/1\">Darvin Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Data Weighting via Class-level Gradient Manipulation. (arXiv:2111.00056v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00056","description":"<p>Label noise and class imbalance are two major issues coexisting in real-world\ndatasets. To alleviate the two issues, state-of-the-art methods reweight each\ninstance by leveraging a small amount of clean and unbiased data. Yet, these\nmethods overlook class-level information within each instance, which can be\nfurther utilized to improve performance. To this end, in this paper, we propose\nGeneralized Data Weighting (GDW) to simultaneously mitigate label noise and\nclass imbalance by manipulating gradients at the class level. To be specific,\nGDW unrolls the loss gradient to class-level gradients by the chain rule and\nreweights the flow of each gradient separately. In this way, GDW achieves\nremarkable performance improvement on both issues. Aside from the performance\ngain, GDW efficiently obtains class-level weights without introducing any extra\ncomputational cost compared with instance weighting methods. Specifically, GDW\nperforms a gradient descent step on class-level weights, which only relies on\nintermediate gradients. Extensive experiments in various settings verify the\neffectiveness of GDW. For example, GDW outperforms state-of-the-art methods by\n$2.56\\%$ under the $60\\%$ uniform noise setting in CIFAR10. Our code is\navailable at https://github.com/GGchen1997/GDW-NIPS2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Can Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_E/0/1/0/all/0/1\">Erqun Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polyline Based Generative Navigable Space Segmentation for Autonomous Visual Navigation. (arXiv:2111.00063v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00063","description":"<p>Detecting navigable space is a fundamental capability for mobile robots\nnavigating in unknown or unmapped environments. In this work, we treat the\nvisual navigable space segmentation as a scene decomposition problem and\npropose Polyline Segmentation Variational AutoEncoder Networks (PSV-Nets), a\nrepresentation-learning-based framework to enable robots to learn the navigable\nspace segmentation in an unsupervised manner. Current segmentation techniques\nheavily rely on supervised learning strategies which demand a large amount of\npixel-level annotated images. In contrast, the proposed framework leverages a\ngenerative model - Variational AutoEncoder (VAE) and an AutoEncoder (AE) to\nlearn a polyline representation that compactly outlines the desired navigable\nspace boundary in an unsupervised way. We also propose a visual receding\nhorizon planning method that uses the learned navigable space and a Scaled\nEuclidean Distance Field (SEDF) to achieve autonomous navigation without an\nexplicit map. Through extensive experiments, we have validated that the\nproposed PSV-Nets can learn the visual navigable space with high accuracy, even\nwithout any single label. We also show that the prediction of the PSV-Nets can\nbe further improved with a small number of labels (if available) and can\nsignificantly outperform the state-of-the-art fully supervised-learning-based\nsegmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhengming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1\">David Crandall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lantao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepDoseNet: A Deep Learning model for 3D Dose Prediction in Radiation Therapy. (arXiv:2111.00077v1 [physics.med-ph])","link":"http://arxiv.org/abs/2111.00077","description":"<p>The DeepDoseNet 3D dose prediction model based on ResNet and Dilated DenseNet\nis proposed. The 340 head-and-neck datasets from the 2020 AAPM OpenKBP\nchallenge were utilized, with 200 for training, 40 for validation, and 100 for\ntesting. Structures include 56Gy, 63Gy, 70Gy PTVs, and brainstem, spinal cord,\nright parotid, left parotid, larynx, esophagus, and mandible OARs. Mean squared\nerror (MSE) loss, mean absolute error (MAE) loss, and MAE plus dose-volume\nhistogram (DVH) based loss functions were investigated. Each model's\nperformance was compared using a 3D dose score, $\\bar{S_{D}}$, (mean absolute\ndifference between ground truth and predicted 3D dose distributions) and a DVH\nscore, $\\bar{S_{DVH}}$ (mean absolute difference between ground truth and\npredicted dose-volume metrics).Furthermore, DVH metrics Mean[Gy] and D0.1cc\n[Gy] for OARs and D99%, D95%, D1% for PTVs were computed. DeepDoseNet with the\nMAE plus DVH-based loss function had the best dose score performance of the\nOpenKBP entries. MAE+DVH model had the lowest prediction error (P&lt;0.0001,\nWilcoxon test) on validation and test datasets (validation:\n$\\bar{S_{D}}$=2.3Gy, $\\bar{S_{DVH}}$=1.9Gy; test: $\\bar{S_{D}}$=2.0Gy,\n$\\bar{S_{DVH}}$=1.6Gy) followed by the MAE model (validation:\n$\\bar{S_{D}}$=3.6Gy, $\\bar{S_{DVH}}$=2.4Gy; test: $\\bar{S_{D}}$=3.5Gy,\n$\\bar{S_{DVH}}$=2.3Gy). The MSE model had the highest prediction error\n(validation: $\\bar{S_{D}}$=3.7Gy, $\\bar{S_{DVH}}$=3.2Gy; test:\n$\\bar{S_{D}}$=3.6Gy, $\\bar{S_{DVH}}$=3.0Gy). No significant difference was\nfound among models in terms of Mean [Gy], but the MAE+DVH model significantly\noutperformed the MAE and MSE models in terms of D0.1cc[Gy], particularly for\nmandible and parotids on both validation (P&lt;0.01) and test (P&lt;0.0001) datasets.\nMAE+DVH outperformed (P&lt;0.0001) in terms of D99%, D95%, D1% for targets.\nMAE+DVH reduced $\\bar{S_{D}}$ by ~60% and $\\bar{S_{DVH}}$ by ~70%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Soomro_M/0/1/0/all/0/1\">Mumtaz Hussain Soomro</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Alves_V/0/1/0/all/0/1\">Victor Gabriel Leandro Alves</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nourzadeh_H/0/1/0/all/0/1\">Hamidreza Nourzadeh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Siebers_J/0/1/0/all/0/1\">Jeffrey V. Siebers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Deterministic Uncertainty for Semantic Segmentation. (arXiv:2111.00079v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00079","description":"<p>We extend Deep Deterministic Uncertainty (DDU), a method for uncertainty\nestimation using feature space densities, to semantic segmentation. DDU enables\nquantifying and disentangling epistemic and aleatoric uncertainty in a single\nforward pass through the model. We study the similarity of feature\nrepresentations of pixels at different locations for the same class and\nconclude that it is feasible to apply DDU location independently, which leads\nto a significant reduction in memory consumption compared to pixel dependent\nDDU. Using the DeepLab-v3+ architecture on Pascal VOC 2012, we show that DDU\nimproves upon MC Dropout and Deep Ensembles while being significantly faster to\ncompute.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mukhoti_J/0/1/0/all/0/1\">Jishnu Mukhoti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amersfoort_J/0/1/0/all/0/1\">Joost van Amersfoort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fetal MRI by robust deep generative prior reconstruction and diffeomorphic registration: application to gestational age prediction. (arXiv:2111.00102v1 [eess.IV])","link":"http://arxiv.org/abs/2111.00102","description":"<p>Magnetic resonance imaging of whole fetal body and placenta is limited by\ndifferent sources of motion affecting the womb. Usual scanning techniques\nemploy single-shot multi-slice sequences where anatomical information in\ndifferent slices may be subject to different deformations, contrast variations\nor artifacts. Volumetric reconstruction formulations have been proposed to\ncorrect for these factors, but they must accommodate a non-homogeneous and\nnon-isotropic sampling, so regularization becomes necessary. Thus, in this\npaper we propose a deep generative prior for robust volumetric reconstructions\nintegrated with a diffeomorphic volume to slice registration method.\nExperiments are performed to validate our contributions and compare with a\nstate of the art method in a cohort of $72$ fetal datasets in the range of\n$20-36$ weeks gestational age. Results suggest improved image resolution and\nmore accurate prediction of gestational age at scan when comparing to a state\nof the art reconstruction method. In addition, gestational age prediction\nresults from our volumetric reconstructions compare favourably with existing\nbrain-based approaches, with boosted accuracy when integrating information of\norgans other than the brain. Namely, a mean absolute error of $0.618$ weeks\n($R^2=0.958$) is achieved when combining fetal brain and trunk information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cordero_Grande_L/0/1/0/all/0/1\">Lucilio Cordero-Grande</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ortuno_Fisac_J/0/1/0/all/0/1\">Juan Enrique Ortu&#xf1;o-Fisac</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uus_A/0/1/0/all/0/1\">Alena Uus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deprez_M/0/1/0/all/0/1\">Maria Deprez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Santos_A/0/1/0/all/0/1\">Andr&#xe9;s Santos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hajnal_J/0/1/0/all/0/1\">Joseph V. Hajnal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ledesma_Carbayo_M/0/1/0/all/0/1\">Mar&#xed;a Jes&#xfa;s Ledesma-Carbayo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FC2T2: The Fast Continuous Convolutional Taylor Transform with Applications in Vision and Graphics. (arXiv:2111.00110v1 [cs.LG])","link":"http://arxiv.org/abs/2111.00110","description":"<p>Series expansions have been a cornerstone of applied mathematics and\nengineering for centuries. In this paper, we revisit the Taylor series\nexpansion from a modern Machine Learning perspective. Specifically, we\nintroduce the Fast Continuous Convolutional Taylor Transform (FC2T2), a variant\nof the Fast Multipole Method (FMM), that allows for the efficient approximation\nof low dimensional convolutional operators in continuous space. We build upon\nthe FMM which is an approximate algorithm that reduces the computational\ncomplexity of N-body problems from O(NM) to O(N+M) and finds application in\ne.g. particle simulations. As an intermediary step, the FMM produces a series\nexpansion for every cell on a grid and we introduce algorithms that act\ndirectly upon this representation. These algorithms analytically but\napproximately compute the quantities required for the forward and backward pass\nof the backpropagation algorithm and can therefore be employed as (implicit)\nlayers in Neural Networks. Specifically, we introduce a root-implicit layer\nthat outputs surface normals and object distances as well as an\nintegral-implicit layer that outputs a rendering of a radiance field given a 3D\npose. In the context of Machine Learning, $N$ and $M$ can be understood as the\nnumber of model parameters and model evaluations respectively which entails\nthat, for applications that require repeated function evaluations which are\nprevalent in Computer Vision and Graphics, unlike regular Neural Networks, the\ntechniques introduce in this paper scale gracefully with parameters. For some\napplications, this results in a 200x reduction in FLOPs compared to\nstate-of-the-art approaches at a reasonable or non-existent loss in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lange_H/0/1/0/all/0/1\">Henning Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutz_J/0/1/0/all/0/1\">J. Nathan Kutz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of jujube fruit based on several pricing factors using machine learning methods. (arXiv:2111.00112v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00112","description":"<p>Jujube is a fruit mainly cultivated in India, China and Iran and has many\nhealth benefits. It is sold both fresh and dried. There are several factors in\njujube pricing such as weight, wrinkles and defections. Some jujube farmers\nsell their product all at once, without any proper sorting or classification,\nfor an average price. Our studies and experiences show that their profit can\nincrease significantly if their product is sold after the sorting process.\nThere are some traditional sorting methods for dried jujube fruit but they are\ncostly, time consuming and can be inaccurate due to human error. Nowadays,\ncomputer vision combined with machine learning methods, is used increasingly in\nfood industry for sorting and classification purposes and solve many of the\ntraditional sorting methods' problems. In this paper we are proposing a\ncomputer vision-based method for grading jujube fruits using machine learning\ntechniques which will take most of the important pricing factors into account\nand can be used to increase the profit of farmers. In this method we first\nacquire several images from different samples and then extract their visual\nfeatures such as color features, shape and size features, texture features,\ndefection and wrinkle features and then we select the most useful features\nusing feature selection algorithms like PCA and CFS. A feature vector is\nobtained for each sample and we use these vectors to train our classifiers to\nbe able to specify the corresponding pre-defined group for each of the samples.\nWe used different classifiers and training methods in order to obtain the best\nresult and by using decision tree we could reach 98.8% accuracy of the\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zakeri_A/0/1/0/all/0/1\">Abdollah Zakeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedayati_R/0/1/0/all/0/1\">Ruhollah Hedayati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khedmati_M/0/1/0/all/0/1\">Mohammad Khedmati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taghipour_Gorjikolaie_M/0/1/0/all/0/1\">Mehran Taghipour-Gorjikolaie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Explanations for Convolutional Neural Networks via Latent Traversal. (arXiv:2111.00116v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00116","description":"<p>Lack of explainability in artificial intelligence, specifically deep neural\nnetworks, remains a bottleneck for implementing models in practice. Popular\ntechniques such as Gradient-weighted Class Activation Mapping (Grad-CAM)\nprovide a coarse map of salient features in an image, which rarely tells the\nwhole story of what a convolutional neural network (CNN) learned. Using\nCOVID-19 chest X-rays, we present a method for interpreting what a CNN has\nlearned by utilizing Generative Adversarial Networks (GANs). Our GAN framework\ndisentangles lung structure from COVID-19 features. Using this GAN, we can\nvisualize the transition of a pair of COVID negative lungs in a chest\nradiograph to a COVID positive pair by interpolating in the latent space of the\nGAN, which provides fine-grained visualization of how the CNN responds to\nvarying features within the lungs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dravid_A/0/1/0/all/0/1\">Amil Dravid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsaggelos_A/0/1/0/all/0/1\">Aggelos K. Katsaggelos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Longitudinal Analysis of Mask and No-Mask on Child Face Recognition. (arXiv:2111.00121v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00121","description":"<p>Face is one of the most widely employed traits for person recognition, even\nin many large-scale applications. Despite technological advancements in face\nrecognition systems, they still face obstacles caused by pose, expression,\nocclusion, and aging variations. Owing to the COVID-19 pandemic, contactless\nidentity verification has become exceedingly vital. To constrain the pandemic,\npeople have started using face mask. Recently, few studies have been conducted\non the effect of face mask on adult face recognition systems. However, the\nimpact of aging with face mask on child subject recognition has not been\nadequately explored. Thus, the main objective of this study is analyzing the\nchild longitudinal impact together with face mask and other covariates on face\nrecognition systems. Specifically, we performed a comparative investigation of\nthree top performing publicly available face matchers and a post-COVID-19\ncommercial-off-the-shelf (COTS) system under child cross-age verification and\nidentification settings using our generated synthetic mask and no-mask samples.\nFurthermore, we investigated the longitudinal consequence of eyeglasses with\nmask and no-mask. The study exploited no-mask longitudinal child face dataset\n(i.e., extended Indian Child Longitudinal Face Dataset) that contains $26,258$\nface images of $7,473$ subjects in the age group of $[2, 18]$ over an average\ntime span of $3.35$ years. Experimental results showed that problem of face\nmask on automated face recognition is compounded by aging variate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chandaliya_P/0/1/0/all/0/1\">Praveen Kumar Chandaliya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_Z/0/1/0/all/0/1\">Zahid Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nain_N/0/1/0/all/0/1\">Neeta Nain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Atlantic Multidecadal Variability. (arXiv:2111.00124v1 [cs.LG])","link":"http://arxiv.org/abs/2111.00124","description":"<p>Atlantic Multidecadal Variability (AMV) describes variations of North\nAtlantic sea surface temperature with a typical cycle of between 60 and 70\nyears. AMV strongly impacts local climate over North America and Europe,\ntherefore prediction of AMV, especially the extreme values, is of great\nsocietal utility for understanding and responding to regional climate change.\nThis work tests multiple machine learning models to improve the state of AMV\nprediction from maps of sea surface temperature, salinity, and sea level\npressure in the North Atlantic region. We use data from the Community Earth\nSystem Model 1 Large Ensemble Project, a state-of-the-art climate model with\n3,440 years of data. Our results demonstrate that all of the models we use\noutperform the traditional persistence forecast baseline. Predicting the AMV is\nimportant for identifying future extreme temperatures and precipitation, as\nwell as hurricane activity, in Europe and North America up to 25 years in\nadvance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Glenn Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beveridge_M/0/1/0/all/0/1\">Matthew Beveridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Young-Oh Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Three approaches to facilitate DNN generalization to objects in out-of-distribution orientations and illuminations: late-stopping, tuning batch normalization and invariance loss. (arXiv:2111.00131v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00131","description":"<p>The training data distribution is often biased towards objects in certain\norientations and illumination conditions. While humans have a remarkable\ncapability of recognizing objects in out-of-distribution (OoD) orientations and\nilluminations, Deep Neural Networks (DNNs) severely suffer in this case, even\nwhen large amounts of training examples are available. In this paper, we\ninvestigate three different approaches to improve DNNs in recognizing objects\nin OoD orientations and illuminations. Namely, these are (i) training much\nlonger after convergence of the in-distribution (InD) validation accuracy,\ni.e., late-stopping, (ii) tuning the momentum parameter of the batch\nnormalization layers, and (iii) enforcing invariance of the neural activity in\nan intermediate layer to orientation and illumination conditions. Each of these\napproaches substantially improves the DNN's OoD accuracy (more than 20% in some\ncases). We report results in four datasets: two datasets are modified from the\nMNIST and iLab datasets, and the other two are novel (one of 3D rendered cars\nand another of objects taken from various controlled orientations and\nillumination conditions). These datasets allow to study the effects of\ndifferent amounts of bias and are challenging as DNNs perform poorly in OoD\nconditions. Finally, we demonstrate that even though the three approaches focus\non different aspects of DNNs, they all tend to lead to the same underlying\nneural mechanism to enable OoD accuracy gains -- individual neurons in the\nintermediate layers become more selective to a category and also invariant to\nOoD orientations and illuminations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sakai_A/0/1/0/all/0/1\">Akira Sakai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunagawa_T/0/1/0/all/0/1\">Taro Sunagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1\">Spandan Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_K/0/1/0/all/0/1\">Kanata Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katoh_T/0/1/0/all/0/1\">Takashi Katoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobashi_H/0/1/0/all/0/1\">Hiromichi Kobashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_P/0/1/0/all/0/1\">Pawan Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIB-R++: Learning to Predict Lighting and Material with a Hybrid Differentiable Renderer. (arXiv:2111.00140v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00140","description":"<p>We consider the challenging problem of predicting intrinsic object properties\nfrom a single image by exploiting differentiable renderers. Many previous\nlearning-based approaches for inverse graphics adopt rasterization-based\nrenderers and assume naive lighting and material models, which often fail to\naccount for non-Lambertian, specular reflections commonly observed in the wild.\nIn this work, we propose DIBR++, a hybrid differentiable renderer which\nsupports these photorealistic effects by combining rasterization and\nray-tracing, taking the advantage of their respective strengths -- speed and\nrealism. Our renderer incorporates environmental lighting and spatially-varying\nmaterial models to efficiently approximate light transport, either through\ndirect estimation or via spherical basis functions. Compared to more advanced\nphysics-based differentiable renderers leveraging path tracing, DIBR++ is\nhighly performant due to its compact and expressive shading model, which\nenables easy integration with learning frameworks for geometry, reflectance and\nlighting prediction from a single image without requiring any ground-truth. We\nexperimentally demonstrate that our approach achieves superior material and\nlighting disentanglement on synthetic and real data compared to existing\nrasterization-based approaches and showcase several artistic applications\nincluding material editing and relighting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenzheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litalien_J/0/1/0/all/0/1\">Joey Litalien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_C/0/1/0/all/0/1\">Clement Fuji Tsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khamis_S/0/1/0/all/0/1\">Sameh Khamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litany_O/0/1/0/all/0/1\">Or Litany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HIERMATCH: Leveraging Label Hierarchies for Improving Semi-Supervised Learning. (arXiv:2111.00164v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00164","description":"<p>Semi-supervised learning approaches have emerged as an active area of\nresearch to combat the challenge of obtaining large amounts of annotated data.\nTowards the goal of improving the performance of semi-supervised learning\nmethods, we propose a novel framework, HIERMATCH, a semi-supervised approach\nthat leverages hierarchical information to reduce labeling costs and performs\nas well as a vanilla semi-supervised learning method. Hierarchical information\nis often available as prior knowledge in the form of coarse labels (e.g.,\nwoodpeckers) for images with fine-grained labels (e.g., downy woodpeckers or\ngolden-fronted woodpeckers). However, the use of supervision using coarse\ncategory labels to improve semi-supervised techniques has not been explored. In\nthe absence of fine-grained labels, HIERMATCH exploits the label hierarchy and\nuses coarse class labels as a weak supervisory signal. Additionally, HIERMATCH\nis a generic-approach to improve any semisupervised learning framework, we\ndemonstrate this using our results on recent state-of-the-art techniques\nMixMatch and FixMatch. We evaluate the efficacy of HIERMATCH on two benchmark\ndatasets, namely CIFAR-100 and NABirds. HIERMATCH can reduce the usage of\nfine-grained labels by 50% on CIFAR-100 with only a marginal drop of 0.59% in\ntop-1 accuracy as compared to MixMatch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Ashima Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagga_S/0/1/0/all/0/1\">Shaurya Bagga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_Y/0/1/0/all/0/1\">Yashvardhan Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_S/0/1/0/all/0/1\">Saket Anand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iris Recognition Based on SIFT Features. (arXiv:2111.00176v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00176","description":"<p>Biometric methods based on iris images are believed to allow very high\naccuracy, and there has been an explosion of interest in iris biometrics in\nrecent years. In this paper, we use the Scale Invariant Feature Transformation\n(SIFT) for recognition using iris images. Contrarily to traditional iris\nrecognition systems, the SIFT approach does not rely on the transformation of\nthe iris pattern to polar coordinates or on highly accurate segmentation,\nallowing less constrained image acquisition conditions. We extract\ncharacteristic SIFT feature points in scale space and perform matching based on\nthe texture information around the feature points using the SIFT operator.\nExperiments are done using the BioSec multimodal database, which includes 3,200\niris images from 200 individuals acquired in two different sessions. We\ncontribute with the analysis of the influence of different SIFT parameters on\nthe recognition performance. We also show the complementarity between the SIFT\napproach and a popular matching approach based on transformation to polar\ncoordinates and Log-Gabor wavelets. The combination of the two approaches\nachieves significantly better performance than either of the individual\nschemes, with a performance improvement of 24% in the Equal Error Rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1\">Fernando Alonso-Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tome_Gonzalez_P/0/1/0/all/0/1\">Pedro Tome-Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Albacete_V/0/1/0/all/0/1\">Virginia Ruiz-Albacete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1\">Javier Ortega-Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct attacks using fake images in iris verification. (arXiv:2111.00178v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00178","description":"<p>In this contribution, the vulnerabilities of iris-based recognition systems\nto direct attacks are studied. A database of fake iris images has been created\nfrom real iris of the BioSec baseline database. Iris images are printed using a\ncommercial printer and then, presented at the iris sensor. We use for our\nexperiments a publicly available iris recognition system, which some\nmodifications to improve the iris segmentation step. Based on results achieved\non different operational scenarios, we show that the system is vulnerable to\ndirect attacks, pointing out the importance of having countermeasures against\nthis type of fraudulent actions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Albacete_V/0/1/0/all/0/1\">Virginia Ruiz-Albacete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tome_Gonzalez_P/0/1/0/all/0/1\">Pedro Tome-Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1\">Fernando Alonso-Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galbally_J/0/1/0/all/0/1\">Javier Galbally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1\">Javier Ortega-Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometry-Aware Hierarchical Bayesian Learning on Manifolds. (arXiv:2111.00184v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00184","description":"<p>Bayesian learning with Gaussian processes demonstrates encouraging regression\nand classification performances in solving computer vision tasks. However,\nBayesian methods on 3D manifold-valued vision data, such as meshes and point\nclouds, are seldom studied. One of the primary challenges is how to effectively\nand efficiently aggregate geometric features from the irregular inputs. In this\npaper, we propose a hierarchical Bayesian learning model to address this\nchallenge. We initially introduce a kernel with the properties of\ngeometry-awareness and intra-kernel convolution. This enables geometrically\nreasonable inferences on manifolds without using any specific hand-crafted\nfeature descriptors. Then, we use a Gaussian process regression to organize the\ninputs and finally implement a hierarchical Bayesian network for the feature\naggregation. Furthermore, we incorporate the feature learning of neural\nnetworks with the feature aggregation of Bayesian models to investigate the\nfeasibility of jointly learning on manifolds. Experimental results not only\nshow that our method outperforms existing Bayesian methods on manifolds but\nalso demonstrate the prospect of coupling neural networks with Bayesian\nnetworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yonghui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yalin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging SE(3) Equivariance for Self-Supervised Category-Level Object Pose Estimation. (arXiv:2111.00190v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00190","description":"<p>Category-level object pose estimation aims to find 6D object poses of\npreviously unseen object instances from known categories without access to\nobject CAD models. To reduce the huge amount of pose annotations needed for\ncategory-level learning, we propose for the first time a self-supervised\nlearning framework to estimate category-level 6D object pose from single 3D\npoint clouds.During training, our method assumes no ground-truth pose\nannotations, no CAD models, and no multi-view supervision. The key to our\nmethod is to disentangle shape and pose through an invariant shape\nreconstruction module and an equivariant pose estimation module, empowered by\nSE(3) equivariant point cloud networks.The invariant shape reconstruction\nmodule learns to perform aligned reconstructions, yielding a category-level\nreference frame without using any annotations. In addition, the equivariant\npose estimation module achieves category-level pose estimation accuracy that is\ncomparable to some fully supervised methods. Extensive experiments demonstrate\nthe effectiveness of our approach on both complete and partial depth point\nclouds from the ModelNet40 benchmark, and on real depth point clouds from the\nNOCS-REAL 275 dataset. The project page with code and visualizations can be\nfound at: https://dragonlong.github.io/equi-pose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaolong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yijia Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbott_A/0/1/0/all/0/1\">A. Lynn Abbott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M2MRF: Many-to-Many Reassembly of Features for Tiny Lesion Segmentation in Fundus Images. (arXiv:2111.00193v1 [eess.IV])","link":"http://arxiv.org/abs/2111.00193","description":"<p>Feature reassembly is an essential component in modern CNNs-based\nsegmentation approaches, which includes feature downsampling and upsampling\noperators. Existing feature reassembly operators reassemble multiple features\nfrom a small predefined region into one for each target location independently.\nThis may result in loss of spatial information, which could vanish activations\nof tiny lesions particularly when they cluster together. In this paper, we\npropose a many-to-many reassembly of features (M2MRF). It reassembles features\nin a dimension-reduced feature space and simultaneously aggregates multiple\nfeatures inside a large predefined region into multiple target features. In\nthis way, long range spatial dependencies are captured to maintain activations\non tiny lesions, particularly when multiple lesions coexist. Experimental\nresults on two lesion segmentation benchmarks, i.e. DDR and IDRiD, show that\nour M2MRF outperforms existing feature reassembly operators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Haotian Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_Y/0/1/0/all/0/1\">Yixiong Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Review of Recent Few-Shot Object Detection Algorithms. (arXiv:2111.00201v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00201","description":"<p>Few-shot object detection, learning to adapt to the novel classes with a few\nlabeled data, is an imperative and long-lasting problem due to the inherent\nlong-tail distribution of real-world data and the urgent demands to cut costs\nof data collection and annotation. Recently, some studies have explored how to\nuse implicit cues in extra datasets without target-domain supervision to help\nfew-shot detectors refine robust task notions. This survey provides a\ncomprehensive overview from current classic and latest achievements for\nfew-shot object detection to future research expectations from manifold\nperspectives. In particular, we first propose a data-based taxonomy of the\ntraining data and the form of corresponding supervision which are accessed\nduring the training stage. Following this taxonomy, we present a significant\nreview of the formal definition, main challenges, benchmark datasets,\nevaluation metrics, and learning strategies. In addition, we present a detailed\ninvestigation of how to interplay the object detection methods to develop this\nissue systematically. Finally, we conclude with the current status of few-shot\nobject detection, along with potential research directions for this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiaxu_L/0/1/0/all/0/1\">Leng Jiaxu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taiyue_C/0/1/0/all/0/1\">Chen Taiyue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xinbo_G/0/1/0/all/0/1\">Gao Xinbo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yongtao_Y/0/1/0/all/0/1\">Yu Yongtao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1\">Gao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_W/0/1/0/all/0/1\">Wang Yue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imitating Arbitrary Talking Style for Realistic Audio-DrivenTalking Face Synthesis. (arXiv:2111.00203v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00203","description":"<p>People talk with diversified styles. For one piece of speech, different\ntalking styles exhibit significant differences in the facial and head pose\nmovements. For example, the \"excited\" style usually talks with the mouth wide\nopen, while the \"solemn\" style is more standardized and seldomly exhibits\nexaggerated motions. Due to such huge differences between different styles, it\nis necessary to incorporate the talking style into audio-driven talking face\nsynthesis framework. In this paper, we propose to inject style into the talking\nface synthesis framework through imitating arbitrary talking style of the\nparticular reference video. Specifically, we systematically investigate talking\nstyles with our collected \\textit{Ted-HD} dataset and construct style codes as\nseveral statistics of 3D morphable model~(3DMM) parameters. Afterwards, we\ndevise a latent-style-fusion~(LSF) model to synthesize stylized talking faces\nby imitating talking styles from the style codes. We emphasize the following\nnovel characteristics of our framework: (1) It doesn't require any annotation\nof the style, the talking style is learned in an unsupervised manner from\ntalking videos in the wild. (2) It can imitate arbitrary styles from arbitrary\nvideos, and the style codes can also be interpolated to generate new styles.\nExtensive experiments demonstrate that the proposed framework has the ability\nto synthesize more natural and expressive talking styles compared with baseline\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haozhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jia Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yishun Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chao Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Q/0/1/0/all/0/1\">Qingshan Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PatchFormer: A Versatile 3D Transformer Based on Patch Attention. (arXiv:2111.00207v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00207","description":"<p>The 3D vision community is witnesses a modeling shift from CNNs to\nTransformers, where pure Transformer architectures have attained top accuracy\non the major 3D learning benchmarks. However, existing 3D Transformers need to\ngenerate a large attention map, which has quadratic complexity (both in space\nand time) with respect to input size. To solve this shortcoming, we introduce\npatch-attention to adaptively learn a much smaller set of bases upon which the\nattention maps are computed. By a weighted summation upon these bases,\npatch-attention not only captures the global shape context but also achieves\nlinear complexity to input size. In addition, we propose a lightweight\nMulti-scale Attention (MSA) block to build attentions among features of\ndifferent scales, providing the model with multi-scale features. Based on these\nproposed modules, we construct our neural architecture called PatchFormer.\nExtensive experiments demonstrate that our network achieves strong accuracy on\ngeneral 3D recognition tasks with 7.3x speed-up than previous 3D Transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1\">Haocheng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xinyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zizhao Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mastering Atari Games with Limited Data. (arXiv:2111.00210v1 [cs.LG])","link":"http://arxiv.org/abs/2111.00210","description":"<p>Reinforcement learning has achieved great success in many applications.\nHowever, sample efficiency remains a key challenge, with prominent methods\nrequiring millions (or even billions) of environment steps to train. Recently,\nthere has been significant progress in sample efficient image-based RL\nalgorithms; however, consistent human-level performance on the Atari game\nbenchmark remains an elusive goal. We propose a sample efficient model-based\nvisual RL algorithm built on MuZero, which we name EfficientZero. Our method\nachieves 190.4% mean human performance and 116.0% median performance on the\nAtari 100k benchmark with only two hours of real-time game experience and\noutperforms the state SAC in some tasks on the DMControl 100k benchmark. This\nis the first time an algorithm achieves super-human performance on Atari games\nwith such little data. EfficientZero's performance is also close to DQN's\nperformance at 200 million frames while we consume 500 times less data.\nEfficientZero's low sample complexity and high performance can bring RL closer\nto real-world applicability. We implement our algorithm in an\neasy-to-understand manner and it is available at\nhttps://github.com/YeWR/EfficientZero. We hope it will accelerate the research\nof MCTS-based RL algorithms in the wider community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Weirui Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaohuai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurutach_T/0/1/0/all/0/1\">Thanard Kurutach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unpaired Learning for High Dynamic Range Image Tone Mapping. (arXiv:2111.00219v1 [eess.IV])","link":"http://arxiv.org/abs/2111.00219","description":"<p>High dynamic range (HDR) photography is becoming increasingly popular and\navailable by DSLR and mobile-phone cameras. While deep neural networks (DNN)\nhave greatly impacted other domains of image manipulation, their use for HDR\ntone-mapping is limited due to the lack of a definite notion of ground-truth\nsolution, which is needed for producing training data.\n</p>\n<p>In this paper we describe a new tone-mapping approach guided by the distinct\ngoal of producing low dynamic range (LDR) renditions that best reproduce the\nvisual characteristics of native LDR images. This goal enables the use of an\nunpaired adversarial training based on unrelated sets of HDR and LDR images,\nboth of which are widely available and easy to acquire.\n</p>\n<p>In order to achieve an effective training under this minimal requirements, we\nintroduce the following new steps and components: (i) a range-normalizing\npre-process which estimates and applies a different level of curve-based\ncompression, (ii) a loss that preserves the input content while allowing the\nnetwork to achieve its goal, and (iii) the use of a more concise discriminator\nnetwork, designed to promote the reproduction of low-level attributes native\nLDR possess.\n</p>\n<p>Evaluation of the resulting network demonstrates its ability to produce\nphoto-realistic artifact-free tone-mapped images, and state-of-the-art\nperformance on different image fidelity indices and visual distances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vinker_Y/0/1/0/all/0/1\">Yael Vinker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huberman_Spiegelglas_I/0/1/0/all/0/1\">Inbar Huberman-Spiegelglas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fattal_R/0/1/0/all/0/1\">Raanan Fattal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Spatio-Temporal Identity Verification Method for Person-Action Instance Search in Movies. (arXiv:2111.00228v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00228","description":"<p>As one of the challenging problems in video search, Person-Action Instance\nSearch (INS) aims to retrieve shots with specific person carrying out specific\naction from massive video shots. Existing methods mainly include two steps:\nFirst, two individual INS branches, i.e., person INS and action INS, are\nseparately conducted to compute the initial person and action ranking scores;\nSecond, both scores are directly fused to generate the final ranking list.\nHowever, direct aggregation of two individual INS scores cannot guarantee the\nidentity consistency between person and action. For example, a shot with \"Pat\nis standing\" and \"Ian is sitting on couch\" may be erroneously understood as\n\"Pat is sitting on couch\" or \"Ian is standing\". To address the above identity\ninconsistency problem (IIP), we study a spatio-temporal identity verification\nmethod. Specifically, in the spatial dimension, we propose an identity\nconsistency verification scheme to optimize the direct fusion score of person\nINS and action INS. The motivation originates from an observation that face\ndetection results usually locate in the identity-consistent action bounding\nboxes. Moreover, in the temporal dimension, considering the complex filming\ncondition, we propose an inter-frame detection extension operation to\ninterpolate missing face/action detection results in successive video frames.\nThe proposed method is evaluated on the large scale TRECVID INS dataset, and\nthe experimental results show that our method can effectively mitigate the IIP\nand surpass the existing second places in both TRECVID 2019 and 2020 INS tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingyao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yanrui Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Baojin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two Heads are Better than One: Geometric-Latent Attention for Point Cloud Classification and Segmentation. (arXiv:2111.00231v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00231","description":"<p>We present an innovative two-headed attention layer that combines geometric\nand latent features to segment a 3D scene into semantically meaningful subsets.\nEach head combines local and global information, using either the geometric or\nlatent features, of a neighborhood of points and uses this information to learn\nbetter local relationships. This Geometric-Latent attention layer (Ge-Latto) is\ncombined with a sub-sampling strategy to capture global features. Our method is\ninvariant to permutation thanks to the use of shared-MLP layers, and it can\nalso be used with point clouds with varying densities because the local\nattention layer does not depend on the neighbor order. Our proposal is simple\nyet robust, which allows it to achieve competitive results in the ShapeNetPart\nand ModelNet40 datasets, and the state-of-the-art when segmenting the complex\ndataset S3DIS, with 69.2% IoU on Area 5, and 89.7% overall accuracy using\nK-fold cross-validation on the 6 areas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cuevas_Velasquez_H/0/1/0/all/0/1\">Hanz Cuevas-Velasquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_A/0/1/0/all/0/1\">Antonio Javier Gallego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_R/0/1/0/all/0/1\">Robert B. Fisher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MFNet: Multi-class Few-shot Segmentation Network with Pixel-wise Metric Learning. (arXiv:2111.00232v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00232","description":"<p>In visual recognition tasks, few-shot learning requires the ability to learn\nobject categories with few support examples. Its recent resurgence in light of\nthe deep learning development is mainly in image classification. This work\nfocuses on few-shot semantic segmentation, which is still a largely unexplored\nfield. A few recent advances are often restricted to single-class few-shot\nsegmentation. In this paper, we first present a novel multi-way encoding and\ndecoding architecture which effectively fuses multi-scale query information and\nmulti-class support information into one query-support embedding; multi-class\nsegmentation is directly decoded upon this embedding. In order for better\nfeature fusion, a multi-level attention mechanism is proposed within the\narchitecture, which includes the attention for support feature modulation and\nattention for multi-scale combination. Last, to enhance the embedding space\nlearning, an additional pixel-wise metric learning module is devised with\ntriplet loss formulated on the pixel-level embedding of the input image.\nExtensive experiments on standard benchmarks PASCAL-5^i and COCO-20^i show\nclear benefits of our method over the state of the art in few-shot\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Miaojing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modality Fusion Transformer for Multispectral Object Detection. (arXiv:2111.00273v1 [eess.IV])","link":"http://arxiv.org/abs/2111.00273","description":"<p>Multispectral image pairs can provide the combined information, making object\ndetection applications more reliable and robust in the open world. To fully\nexploit the different modalities, we present a simple yet effective\ncross-modality feature fusion approach, named Cross-Modality Fusion Transformer\n(CFT) in this paper. Unlike prior CNNs-based works, guided by the transformer\nscheme, our network learns long-range dependencies and integrates global\ncontextual information in the feature extraction stage. More importantly, by\nleveraging the self attention of the transformer, the network can naturally\ncarry out simultaneous intra-modality and inter-modality fusion, and robustly\ncapture the latent interactions between RGB and Thermal domains, thereby\nsignificantly improving the performance of multispectral object detection.\nExtensive experiments and ablation studies on multiple datasets demonstrate\nthat our approach is effective and achieves state-of-the-art detection\nperformance. Our code and models will be released soon at\nhttps://github.com/DocF/multispectral-object-detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Qingyun_F/0/1/0/all/0/1\">Fang Qingyun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dapeng_H/0/1/0/all/0/1\">Han Dapeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhaokui_W/0/1/0/all/0/1\">Wang Zhaokui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Get Fooled for the Right Reason: Improving Adversarial Robustness through a Teacher-guided Curriculum Learning Approach. (arXiv:2111.00295v1 [cs.LG])","link":"http://arxiv.org/abs/2111.00295","description":"<p>Current SOTA adversarially robust models are mostly based on adversarial\ntraining (AT) and differ only by some regularizers either at inner maximization\nor outer minimization steps. Being repetitive in nature during the inner\nmaximization step, they take a huge time to train. We propose a non-iterative\nmethod that enforces the following ideas during training. Attribution maps are\nmore aligned to the actual object in the image for adversarially robust models\ncompared to naturally trained models. Also, the allowed set of pixels to\nperturb an image (that changes model decision) should be restricted to the\nobject pixels only, which reduces the attack strength by limiting the attack\nspace. Our method achieves significant performance gains with a little extra\neffort (10-20%) over existing AT models and outperforms all other methods in\nterms of adversarial as well as natural accuracy. We have performed extensive\nexperimentation with CIFAR-10, CIFAR-100, and TinyImageNet datasets and\nreported results against many popular strong adversarial attacks to prove the\neffectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Anindya Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Anirban Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gali_S/0/1/0/all/0/1\">Sowrya Gali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A fast accurate fine-grain object detection model based on YOLOv4 deep neural network. (arXiv:2111.00298v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00298","description":"<p>Early identification and prevention of various plant diseases in commercial\nfarms and orchards is a key feature of precision agriculture technology. This\npaper presents a high-performance real-time fine-grain object detection\nframework that addresses several obstacles in plant disease detection that\nhinder the performance of traditional methods, such as, dense distribution,\nirregular morphology, multi-scale object classes, textural similarity, etc. The\nproposed model is built on an improved version of the You Only Look Once\n(YOLOv4) algorithm. The modified network architecture maximizes both detection\naccuracy and speed by including the DenseNet in the back-bone to optimize\nfeature transfer and reuse, two new residual blocks in the backbone and neck\nenhance feature extraction and reduce computing cost; the Spatial Pyramid\nPooling (SPP) enhances receptive field, and a modified Path Aggregation Network\n(PANet) preserves fine-grain localized information and improve feature fusion.\nAdditionally, the use of the Hard-Swish function as the primary activation\nimproved the model's accuracy due to better nonlinear feature extraction. The\nproposed model is tested in detecting four different diseases in tomato plants\nunder various challenging environments. The model outperforms the existing\nstate-of-the-art detection models in detection accuracy and speed. At a\ndetection rate of 70.19 FPS, the proposed model obtained a precision value of\n$90.33 \\%$, F1-score of $93.64 \\%$, and a mean average precision ($mAP$) value\nof $96.29 \\%$. Current work provides an effective and efficient method for\ndetecting different plant diseases in complex scenarios that can be extended to\ndifferent fruit and crop detection, generic disease detection, and various\nautomated agricultural detection processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Arunabha M. Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bose_R/0/1/0/all/0/1\">Rikhi Bose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhaduri_J/0/1/0/all/0/1\">Jayabrata Bhaduri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3DP3: 3D Scene Perception via Probabilistic Programming. (arXiv:2111.00312v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00312","description":"<p>We present 3DP3, a framework for inverse graphics that uses inference in a\nstructured generative model of objects, scenes, and images. 3DP3 uses (i) voxel\nmodels to represent the 3D shape of objects, (ii) hierarchical scene graphs to\ndecompose scenes into objects and the contacts between them, and (iii) depth\nimage likelihoods based on real-time graphics. Given an observed RGB-D image,\n3DP3's inference algorithm infers the underlying latent 3D scene, including the\nobject poses and a parsimonious joint parametrization of these poses, using\nfast bottom-up pose proposals, novel involutive MCMC updates of the scene graph\nstructure, and, optionally, neural object detectors and pose estimators. We\nshow that 3DP3 enables scene understanding that is aware of 3D shape,\nocclusion, and contact structure. Our results demonstrate that 3DP3 is more\naccurate at 6DoF object pose estimation from real images than deep learning\nbaselines and shows better generalization to challenging scenes with novel\nviewpoints, contact, and partial observability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gothoskar_N/0/1/0/all/0/1\">Nishad Gothoskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cusumano_Towner_M/0/1/0/all/0/1\">Marco Cusumano-Towner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zinberg_B/0/1/0/all/0/1\">Ben Zinberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghavamizadeh_M/0/1/0/all/0/1\">Matin Ghavamizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollok_F/0/1/0/all/0/1\">Falk Pollok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrett_A/0/1/0/all/0/1\">Austin Garrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutfreund_D/0/1/0/all/0/1\">Dan Gutfreund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansinghka_V/0/1/0/all/0/1\">Vikash K. Mansinghka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Functional Neural Networks for Parametric Image Restoration Problems. (arXiv:2111.00361v1 [eess.IV])","link":"http://arxiv.org/abs/2111.00361","description":"<p>Almost every single image restoration problem has a closely related\nparameter, such as the scale factor in super-resolution, the noise level in\nimage denoising, and the quality factor in JPEG deblocking. Although recent\nstudies on image restoration problems have achieved great success due to the\ndevelopment of deep neural networks, they handle the parameter involved in an\nunsophisticated way. Most previous researchers either treat problems with\ndifferent parameter levels as independent tasks, and train a specific model for\neach parameter level; or simply ignore the parameter, and train a single model\nfor all parameter levels. The two popular approaches have their own\nshortcomings. The former is inefficient in computing and the latter is\nineffective in performance. In this work, we propose a novel system called\nfunctional neural network (FuncNet) to solve a parametric image restoration\nproblem with a single model. Unlike a plain neural network, the smallest\nconceptual element of our FuncNet is no longer a floating-point variable, but a\nfunction of the parameter of the problem. This feature makes it both efficient\nand effective for a parametric problem. We apply FuncNet to super-resolution,\nimage denoising, and JPEG deblocking. The experimental results show the\nsuperiority of our FuncNet on all three parametric image restoration tasks over\nthe state of the arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Luo_F/0/1/0/all/0/1\">Fangzhou Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xiaolin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1\">Yanhui Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Attention Network for Heart Rate and Respiratory Rate Estimation. (arXiv:2111.00390v1 [eess.IV])","link":"http://arxiv.org/abs/2111.00390","description":"<p>Heart rate and respiratory rate measurement is a vital step for diagnosing\nmany diseases. Non-contact camera based physiological measurement is more\naccessible and convenient in Telehealth nowadays than contact instruments such\nas fingertip oximeters since non-contact methods reduce risk of infection.\nHowever, remote physiological signal measurement is challenging due to\nenvironment illumination variations, head motion, facial expression, etc. It's\nalso desirable to have a unified network which could estimate both heart rate\nand respiratory rate to reduce system complexity and latency. We propose a\nconvolutional neural network which leverages spatial attention and channel\nattention, which we call it dual attention network (DAN) to jointly estimate\nheart rate and respiratory rate with camera video as input. Extensive\nexperiments demonstrate that our proposed system significantly improves heart\nrate and respiratory rate measurement accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ren_Y/0/1/0/all/0/1\">Yuzhuo Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Syrnyk_B/0/1/0/all/0/1\">Braeden Syrnyk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Avadhanam_N/0/1/0/all/0/1\">Niranjan Avadhanam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A robust single-pixel particle image velocimetry based on fully convolutional networks with cross-correlation embedded. (arXiv:2111.00395v1 [physics.flu-dyn])","link":"http://arxiv.org/abs/2111.00395","description":"<p>Particle image velocimetry (PIV) is essential in experimental fluid dynamics.\nIn the current work, we propose a new velocity field estimation paradigm, which\nachieves a synergetic combination of the deep learning method and the\ntraditional cross-correlation method. Specifically, the deep learning method is\nused to optimize and correct a coarse velocity guess to achieve a\nsuper-resolution calculation. And the cross-correlation method provides the\ninitial velocity field based on a coarse correlation with a large interrogation\nwindow. As a reference, the coarse velocity guess helps with improving the\nrobustness of the proposed algorithm. This fully convolutional network with\nembedded cross-correlation is named as CC-FCN. CC-FCN has two types of input\nlayers, one is for the particle images, and the other is for the initial\nvelocity field calculated using cross-correlation with a coarse resolution.\nFirstly, two pyramidal modules extract features of particle images and initial\nvelocity field respectively. Then the fusion module appropriately fuses these\nfeatures. Finally, CC-FCN achieves the super-resolution calculation through a\nseries of deconvolution layers to obtain the single-pixel velocity field. As\nthe supervised learning strategy is considered, synthetic data sets including\nground-truth fluid motions are generated to train the network parameters.\nSynthetic and real experimental PIV data sets are used to test the trained\nneural network in terms of accuracy, precision, spatial resolution and\nrobustness. The test results show that these attributes of CC-FCN are further\nimproved compared with those of other tested PIV algorithms. The proposed model\ncould therefore provide competitive and robust estimations for PIV experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Gao_Q/0/1/0/all/0/1\">Qi Gao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lin_H/0/1/0/all/0/1\">Hongtao Lin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tu_H/0/1/0/all/0/1\">Han Tu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhu_H/0/1/0/all/0/1\">Haoran Zhu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wei_R/0/1/0/all/0/1\">Runjie Wei</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_G/0/1/0/all/0/1\">Guoping Zhang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Shao_X/0/1/0/all/0/1\">Xueming Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Approach to Image Tilt Correction with Self-Attention MobileNet for Smartphones. (arXiv:2111.00398v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00398","description":"<p>The main contributions of our work are two-fold. First, we present a\nSelf-Attention MobileNet, called SA-MobileNet Network that can model long-range\ndependencies between the image features instead of processing the local region\nas done by standard convolutional kernels. SA-MobileNet contains self-attention\nmodules integrated with the inverted bottleneck blocks of the MobileNetV3 model\nwhich results in modeling of both channel-wise attention and spatial attention\nof the image features and at the same time introduce a novel self-attention\narchitecture for low-resource devices. Secondly, we propose a novel training\npipeline for the task of image tilt detection. We treat this problem in a\nmulti-label scenario where we predict multiple angles for a tilted input image\nin a narrow interval of range 1-2 degrees, depending on the dataset used. This\nprocess induces an implicit correlation between labels without any\ncomputational overhead of the second or higher-order methods in multi-label\nlearning. With the combination of our novel approach and the architecture, we\npresent state-of-the-art results on detecting the image tilt angle on mobile\ndevices as compared to the MobileNetV3 model. Finally, we establish that\nSA-MobileNet is more accurate than MobileNetV3 on SUN397, NYU-V1, and ADE20K\ndatasets by 6.42%, 10.51%, and 9.09% points respectively, and faster by at\nleast 4 milliseconds on Snapdragon 750 Octa-core.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Siddhant Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_D/0/1/0/all/0/1\">Debi Prasanna Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thota_S/0/1/0/all/0/1\">Siva Prasad Thota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moharana_S/0/1/0/all/0/1\">Sukumar Moharana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PANet: Perspective-Aware Network with Dynamic Receptive Fields and Self-Distilling Supervision for Crowd Counting. (arXiv:2111.00406v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00406","description":"<p>Crowd counting aims to learn the crowd density distributions and estimate the\nnumber of objects (e.g. persons) in images. The perspective effect, which\nsignificantly influences the distribution of data points, plays an important\nrole in crowd counting. In this paper, we propose a novel perspective-aware\napproach called PANet to address the perspective problem. Based on the\nobservation that the size of the objects varies greatly in one image due to the\nperspective effect, we propose the dynamic receptive fields (DRF) framework.\nThe framework is able to adjust the receptive field by the dilated convolution\nparameters according to the input image, which helps the model to extract more\ndiscriminative features for each local region. Different from most previous\nworks which use Gaussian kernels to generate the density map as the supervised\ninformation, we propose the self-distilling supervision (SDS) training method.\nThe ground-truth density maps are refined from the first training stage and the\nperspective information is distilled to the model in the second stage. The\nexperimental results on ShanghaiTech Part_A and Part_B, UCF_QNRF, and UCF_CC_50\ndatasets demonstrate that our proposed PANet outperforms the state-of-the-art\nmethods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoshuang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiru Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yu Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1\">Fei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1\">Mingyuan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xiansheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongtao Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Deep Residual Reasoning for Temporal Moment Localization. (arXiv:2111.00417v1 [cs.MM])","link":"http://arxiv.org/abs/2111.00417","description":"<p>Temporal Moment Localization (TML) in untrimmed videos is a challenging task\nin the field of multimedia, which aims at localizing the start and end points\nof the activity in the video, described by a sentence query. Existing methods\nmainly focus on mining the correlation between video and sentence\nrepresentations or investigating the fusion manner of the two modalities. These\nworks mainly understand the video and sentence coarsely, ignoring the fact that\na sentence can be understood from various semantics, and the dominant words\naffecting the moment localization in the semantics are the action and object\nreference. Toward this end, we propose a Hierarchical Deep Residual Reasoning\n(HDRR) model, which decomposes the video and sentence into multi-level\nrepresentations with different semantics to achieve a finer-grained\nlocalization. Furthermore, considering that videos with different resolution\nand sentences with different length have different difficulty in understanding,\nwe design the simple yet effective Res-BiGRUs for feature fusion, which is able\nto grasp the useful information in a self-adapting manner. Extensive\nexperiments conducted on Charades-STA and ActivityNet-Captions datasets\ndemonstrate the superiority of our HDRR model compared with other\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianjing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xuemeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiran Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Loop closure detection using local 3D deep descriptors. (arXiv:2111.00440v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00440","description":"<p>We present a simple yet effective method to address loop closure detection in\nsimultaneous localisation and mapping using local 3D deep descriptors (L3Ds).\nL3Ds are emerging compact representations of patches extracted from point\nclouds that are learned from data using a deep learning algorithm. We propose a\nnovel overlap measure for loop detection by computing the metric error between\npoints that correspond to mutually-nearest-neighbour descriptors after\nregistering the loop candidate point cloud by its estimated relative pose. This\nnovel approach enables us to accurately detect loops and estimate six\ndegrees-of-freedom poses in the case of small overlaps. We compare our\nL3D-based loop closure approach with recent approaches on LiDAR data and\nachieve state-of-the-art loop closure detection accuracy. Additionally, we\nembed our loop closure approach in RESLAM, a recent edge-based SLAM system, and\nperform the evaluation on real-world RGBD-TUM and synthetic ICL datasets. Our\napproach enables RESLAM to achieve a better localisation accuracy compared to\nits original loop closure strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Youjie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poiesi_F/0/1/0/all/0/1\">Fabio Poiesi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Q/0/1/0/all/0/1\">Qi Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yi Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gaussian Kernel Mixture Network for Single Image Defocus Deblurring. (arXiv:2111.00454v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00454","description":"<p>Defocus blur is one kind of blur effects often seen in images, which is\nchallenging to remove due to its spatially variant amount. This paper presents\nan end-to-end deep learning approach for removing defocus blur from a single\nimage, so as to have an all-in-focus image for consequent vision tasks. First,\na pixel-wise Gaussian kernel mixture (GKM) model is proposed for representing\nspatially variant defocus blur kernels in an efficient linear parametric form,\nwith higher accuracy than existing models. Then, a deep neural network called\nGKMNet is developed by unrolling a fixed-point iteration of the GKM-based\ndeblurring. The GKMNet is built on a lightweight scale-recurrent architecture,\nwith a scale-recurrent attention module for estimating the mixing coefficients\nin GKM for defocus deblurring. Extensive experiments show that the GKMNet not\nonly noticeably outperforms existing defocus deblurring methods, but also has\nits advantages in terms of model complexity and computational efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quan_Y/0/1/0/all/0/1\">Yuhui Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zicong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Hui Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IGCN: Image-to-graph Convolutional Network for 2D/3D Deformable Registration. (arXiv:2111.00484v1 [eess.IV])","link":"http://arxiv.org/abs/2111.00484","description":"<p>Organ shape reconstruction based on a single-projection image during\ntreatment has wide clinical scope, e.g., in image-guided radiotherapy and\nsurgical guidance. We propose an image-to-graph convolutional network that\nachieves deformable registration of a 3D organ mesh for a single-viewpoint 2D\nprojection image. This framework enables simultaneous training of two types of\ntransformation: from the 2D projection image to a displacement map, and from\nthe sampled per-vertex feature to a 3D displacement that satisfies the\ngeometrical constraint of the mesh structure. Assuming application to radiation\ntherapy, the 2D/3D deformable registration performance is verified for multiple\nabdominal organs that have not been targeted to date, i.e., the liver, stomach,\nduodenum, and kidney, and for pancreatic cancer. The experimental results show\nshape prediction considering relationships among multiple organs can be used to\npredict respiratory motion and deformation from digitally reconstructed\nradiographs with clinically acceptable accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nakao_M/0/1/0/all/0/1\">Megumi Nakao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nakamura_M/0/1/0/all/0/1\">Mitsuhiro Nakamura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matsuda_T/0/1/0/all/0/1\">Tetsuya Matsuda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Image Compression with Separate Hyperprior Decoders. (arXiv:2111.00485v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00485","description":"<p>Learned image compression techniques have achieved considerable development\nin recent years. In this paper, we find that the performance bottleneck lies in\nthe use of a single hyperprior decoder, in which case the ternary Gaussian\nmodel collapses to a binary one. To solve this, we propose to use three\nhyperprior decoders to separate the decoding process of the mixed parameters in\ndiscrete Gaussian mixture likelihoods, achieving more accurate parameters\nestimation. Experimental results demonstrate the proposed method optimized by\nMS-SSIM achieves on average 3.36% BD-rate reduction compared with\nstate-of-the-art approach. The contribution of the proposed method to the\ncoding time and FLOPs is negligible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zan_Z/0/1/0/all/0/1\">Zhao Zan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Heming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiaoyang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yibo Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smart(Sampling)Augment: Optimal and Efficient Data Augmentation for Semantic Segmentation. (arXiv:2111.00487v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00487","description":"<p>Data augmentation methods enrich datasets with augmented data to improve the\nperformance of neural networks. Recently, automated data augmentation methods\nhave emerged, which automatically design augmentation strategies. Existing work\nfocuses on image classification and object detection, whereas we provide the\nfirst study on semantic image segmentation and introduce two new approaches:\n\\textit{SmartAugment} and \\textit{SmartSamplingAugment}. SmartAugment uses\nBayesian Optimization to search over a rich space of augmentation strategies\nand achieves a new state-of-the-art performance in all semantic segmentation\ntasks we consider. SmartSamplingAugment, a simple parameter-free approach with\na fixed augmentation strategy competes in performance with the existing\nresource-intensive approaches and outperforms cheap state-of-the-art data\naugmentation methods. Further, we analyze the impact, interaction, and\nimportance of data augmentation hyperparameters and perform ablation studies,\nwhich confirm our design choices behind SmartAugment and SmartSamplingAugment.\nLastly, we will provide our source code for reproducibility and to facilitate\nfurther research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Negassi_M/0/1/0/all/0/1\">Misgana Negassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_D/0/1/0/all/0/1\">Diane Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiterer_A/0/1/0/all/0/1\">Alexander Reiterer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DPNET: Dual-Path Network for Efficient Object Detectioj with Lightweight Self-Attention. (arXiv:2111.00500v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00500","description":"<p>Object detection often costs a considerable amount of computation to get\nsatisfied performance, which is unfriendly to be deployed in edge devices. To\naddress the trade-off between computational cost and detection accuracy, this\npaper presents a dual path network, named DPNet, for efficient object detection\nwith lightweight self-attention. In backbone, a single input/output lightweight\nself-attention module (LSAM) is designed to encode global interactions between\ndifferent positions. LSAM is also extended into a multiple-inputs version in\nfeature pyramid network (FPN), which is employed to capture cross-resolution\ndependencies in two paths. Extensive experiments on the COCO dataset\ndemonstrate that our method achieves state-of-the-art detection results. More\nspecifically, DPNet obtains 29.0% AP on COCO test-dev, with only 1.14 GFLOPs\nand 2.27M model size for a 320x320 image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Huimin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Quan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1\">Yinghao Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaofu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latecki_L/0/1/0/all/0/1\">Longin Jan Latecki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully convolutional Siamese neural networks for buildings damage assessment from satellite images. (arXiv:2111.00508v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00508","description":"<p>Damage assessment after natural disasters is needed to distribute aid and\nforces to recovery from damage dealt optimally. This process involves acquiring\nsatellite imagery for the region of interest, localization of buildings, and\nclassification of the amount of damage caused by nature or urban factors to\nbuildings. In case of natural disasters, this means processing many square\nkilometers of the area to judge whether a particular building had suffered from\nthe damaging factors.\n</p>\n<p>In this work, we develop a computational approach for an automated comparison\nof the same region's satellite images before and after the disaster, and\nclassify different levels of damage in buildings. Our solution is based on\nSiamese neural networks with encoder-decoder architecture. We include an\nextensive ablation study and compare different encoders, decoders, loss\nfunctions, augmentations, and several methods to combine two images. The\nsolution achieved one of the best results in the Computer Vision for Building\nDamage Assessment competition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khvedchenya_E/0/1/0/all/0/1\">Eugene Khvedchenya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabruseva_T/0/1/0/all/0/1\">Tatiana Gabruseva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRBANET: A Lightweight Dual-Resolution Network for Semantic Segmentation with Boundary Auxiliary. (arXiv:2111.00509v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00509","description":"<p>Due to the powerful ability to encode image details and semantics, many\nlightweight dual-resolution networks have been proposed in recent years.\nHowever, most of them ignore the benefit of boundary information. This paper\nintroduces a lightweight dual-resolution network, called DRBANet, aiming to\nrefine semantic segmentation results with the aid of boundary information.\nDRBANet adopts dual parallel architecture, including: high resolution branch\n(HRB) and low resolution branch (LRB). Specifically, HRB mainly consists of a\nset of Efficient Inverted Bottleneck Modules (EIBMs), which learn feature\nrepresentations with larger receptive fields. LRB is composed of a series of\nEIBMs and an Extremely Lightweight Pyramid Pooling Module (ELPPM), where ELPPM\nis utilized to capture multi-scale context through hierarchical residual\nconnections. Finally, a boundary supervision head is designed to capture object\nboundaries in HRB. Extensive experiments on Cityscapes and CamVid datasets\ndemonstrate that our method achieves promising trade-off between segmentation\naccuracy and running efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Quan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chenfeng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaofu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latecki_L/0/1/0/all/0/1\">Longin Jan Latecki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrating the Dice loss to handle neural network overconfidence for biomedical image segmentation. (arXiv:2111.00528v1 [eess.IV])","link":"http://arxiv.org/abs/2111.00528","description":"<p>The Dice similarity coefficient (DSC) is both a widely used metric and loss\nfunction for biomedical image segmentation due to its robustness to class\nimbalance. However, it is well known that the DSC loss is poorly calibrated,\nresulting in overconfident predictions that cannot be usefully interpreted in\nbiomedical and clinical practice. Performance is often the only metric used to\nevaluate segmentations produced by deep neural networks, and calibration is\noften neglected. However, calibration is important for translation into\nbiomedical and clinical practice, providing crucial contextual information to\nmodel predictions for interpretation by scientists and clinicians. In this\nstudy, we identify poor calibration as an emerging challenge of deep learning\nbased biomedical image segmentation. We provide a simple yet effective\nextension of the DSC loss, named the DSC++ loss, that selectively modulates the\npenalty associated with overconfident, incorrect predictions. As a standalone\nloss function, the DSC++ loss achieves significantly improved calibration over\nthe conventional DSC loss across five well-validated open-source biomedical\nimaging datasets. Similarly, we observe significantly improved when integrating\nthe DSC++ loss into four DSC-based loss functions. Finally, we use softmax\nthresholding to illustrate that well calibrated outputs enable tailoring of\nprecision-recall bias, an important post-processing technique to adapt the\nmodel predictions to suit the biomedical or clinical task. The DSC++ loss\novercomes the major limitation of the DSC, providing a suitable loss function\nfor training deep learning segmentation models for use in biomedical and\nclinical practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yeung_M/0/1/0/all/0/1\">Michael Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rundo_L/0/1/0/all/0/1\">Leonardo Rundo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nan_Y/0/1/0/all/0/1\">Yang Nan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sala_E/0/1/0/all/0/1\">Evis Sala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Debiased and Disentangled Representations for Semantic Segmentation. (arXiv:2111.00531v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00531","description":"<p>Deep neural networks are susceptible to learn biased models with entangled\nfeature representations, which may lead to subpar performances on various\ndownstream tasks. This is particularly true for under-represented classes,\nwhere a lack of diversity in the data exacerbates the tendency. This limitation\nhas been addressed mostly in classification tasks, but there is little study on\nadditional challenges that may appear in more complex dense prediction problems\nincluding semantic segmentation. To this end, we propose a model-agnostic and\nstochastic training scheme for semantic segmentation, which facilitates the\nlearning of debiased and disentangled representations. For each class, we first\nextract class-specific information from the highly entangled feature map. Then,\ninformation related to a randomly sampled class is suppressed by a feature\nselection process in the feature space. By randomly eliminating certain class\ninformation in each training iteration, we effectively reduce feature\ndependencies among classes, and the model is able to learn more debiased and\ndisentangled feature representations. Models trained with our approach\ndemonstrate strong results on multiple semantic segmentation benchmarks, with\nespecially notable performance gains on under-represented classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_S/0/1/0/all/0/1\">Sanghyeok Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bohyung Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Boundary Uncertainty into loss functions for biomedical image segmentation. (arXiv:2111.00533v1 [eess.IV])","link":"http://arxiv.org/abs/2111.00533","description":"<p>Manual segmentation is used as the gold-standard for evaluating neural\nnetworks on automated image segmentation tasks. Due to considerable\nheterogeneity in shapes, colours and textures, demarcating object boundaries is\nparticularly difficult in biomedical images, resulting in significant inter and\nintra-rater variability. Approaches, such as soft labelling and distance\npenalty term, apply a global transformation to the ground truth, redefining the\nloss function with respect to uncertainty. However, global operations are\ncomputationally expensive, and neither approach accurately reflects the\nuncertainty underlying manual annotation. In this paper, we propose the\nBoundary Uncertainty, which uses morphological operations to restrict soft\nlabelling to object boundaries, providing an appropriate representation of\nuncertainty in ground truth labels, and may be adapted to enable robust model\ntraining where systematic manual segmentation errors are present. We\nincorporate Boundary Uncertainty with the Dice loss, achieving consistently\nimproved performance across three well-validated biomedical imaging datasets\ncompared to soft labelling and distance-weighted penalty. Boundary Uncertainty\nnot only more accurately reflects the segmentation process, but it is also\nefficient, robust to segmentation errors and exhibits better generalisation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yeung_M/0/1/0/all/0/1\">Michael Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sala_E/0/1/0/all/0/1\">Evis Sala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rundo_L/0/1/0/all/0/1\">Leonardo Rundo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focal Attention Networks: optimising attention for biomedical image segmentation. (arXiv:2111.00534v1 [eess.IV])","link":"http://arxiv.org/abs/2111.00534","description":"<p>In recent years, there has been increasing interest to incorporate attention\ninto deep learning architectures for biomedical image segmentation. The modular\ndesign of attention mechanisms enables flexible integration into convolutional\nneural network architectures, such as the U-Net. Whether attention is\nappropriate to use, what type of attention to use, and where in the network to\nincorporate attention modules, are all important considerations that are\ncurrently overlooked. In this paper, we investigate the role of the Focal\nparameter in modulating attention, revealing a link between attention in loss\nfunctions and networks. By incorporating a Focal distance penalty term, we\nextend the Unified Focal loss framework to include boundary-based losses.\nFurthermore, we develop a simple and interpretable, dataset and model-specific\nheuristic to integrate the Focal parameter into the Squeeze-and-Excitation\nblock and Attention Gate, achieving optimal performance with fewer number of\nattention modules on three well-validated biomedical imaging datasets,\nsuggesting judicious use of attention modules results in better performance and\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yeung_M/0/1/0/all/0/1\">Michael Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rundo_L/0/1/0/all/0/1\">Leonardo Rundo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sala_E/0/1/0/all/0/1\">Evis Sala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Face to Gait: Weakly-Supervised Learning of Gender Information from Walking Patterns. (arXiv:2111.00538v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00538","description":"<p>Obtaining demographics information from video is valuable for a range of\nreal-world applications. While approaches that leverage facial features for\ngender inference are very successful in restrained environments, they do not\nwork in most real-world scenarios when the subject is not facing the camera,\nhas the face obstructed or the face is not clear due to distance from the\ncamera or poor resolution. We propose a weakly-supervised method for learning\ngender information of people based on their manner of walking. We make use of\nstate-of-the art facial analysis models to automatically annotate front-view\nwalking sequences and generalise to unseen angles by leveraging gait-based\nlabel propagation. Our results show on par or higher performance with facial\nanalysis models with an F1 score of 91% and the ability to successfully\ngeneralise to scenarios in which facial analysis is unfeasible due to subjects\nnot facing the camera or having the face obstructed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Catruna_A/0/1/0/all/0/1\">Andy Catruna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radoi_I/0/1/0/all/0/1\">Ion Emilian Radoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Detect Open Carry and Concealed Object with 77GHz Radar. (arXiv:2111.00551v1 [eess.SP])","link":"http://arxiv.org/abs/2111.00551","description":"<p>Detecting harmful carried objects plays a key role in intelligent\nsurveillance systems and has widespread applications, for example, in airport\nsecurity. In this paper, we focus on the relatively unexplored area of using\nlow-cost 77GHz mmWave radar for the carried objects detection problem. The\nproposed system is capable of real-time detecting three classes of objects -\nlaptop, phone, and knife - under open carry and concealed cases where objects\nare hidden with clothes or bags. This capability is achieved by initial signal\nprocessing for localization and generating range-azimuth-elevation image cubes,\nfollowed by a deep learning-based prediction network and a multi-shot\npost-processing module for detecting objects. Extensive experiments for\nvalidating the system performance on detecting open carry and concealed objects\nhave been presented with a self-built radar-camera testbed and dataset.\nAdditionally, the influence of different input, factors, and parameters on\nsystem performance is analyzed, providing an intuitive understanding of the\nsystem. This system would be the very first baseline for other future works\naiming to detect carried objects using 77GHz radar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_X/0/1/0/all/0/1\">Xiangyu Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roy_S/0/1/0/all/0/1\">Sumit Roy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xing_G/0/1/0/all/0/1\">Guanbin Xing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alansari_A/0/1/0/all/0/1\">Ali Alansari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_Y/0/1/0/all/0/1\">Youchen Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TorchXRayVision: A library of chest X-ray datasets and models. (arXiv:2111.00595v1 [eess.IV])","link":"http://arxiv.org/abs/2111.00595","description":"<p>TorchXRayVision is an open source software library for working with chest\nX-ray datasets and deep learning models. It provides a common interface and\ncommon pre-processing chain for a wide set of publicly available chest X-ray\ndatasets. In addition, a number of classification and representation learning\nmodels with different architectures, trained on different data combinations,\nare available through the library to serve as baselines or feature extractors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cohen_J/0/1/0/all/0/1\">Joseph Paul Cohen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Viviano_J/0/1/0/all/0/1\">Joseph D. Viviano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bertin_P/0/1/0/all/0/1\">Paul Bertin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Morrison_P/0/1/0/all/0/1\">Paul Morrison</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Torabian_P/0/1/0/all/0/1\">Parsa Torabian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guarrera_M/0/1/0/all/0/1\">Matteo Guarrera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P Lungren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chaudhari_A/0/1/0/all/0/1\">Akshay Chaudhari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brooks_R/0/1/0/all/0/1\">Rupert Brooks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hashir_M/0/1/0/all/0/1\">Mohammad Hashir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bertrand_H/0/1/0/all/0/1\">Hadrien Bertrand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognizing Families In the Wild (RFIW): The 5th Edition. (arXiv:2111.00598v1 [cs.CV])","link":"http://arxiv.org/abs/2111.00598","description":"<p>Recognizing Families In the Wild (RFIW), held as a data challenge in\nconjunction with the 16th IEEE International Conference on Automatic Face and\nGesture Recognition (FG), is a large-scale, multi-track visual kinship\nrecognition evaluation. This is our fifth edition of RFIW, for which we\ncontinue the effort to attract scholars, bring together professionals, publish\nnew work, and discuss prospects. In this paper, we summarize submissions for\nthe three tasks of this year's RFIW: specifically, we review the results for\nkinship verification, tri-subject verification, and family member search and\nretrieval. We take a look at the RFIW problem, as well as share current efforts\nand make recommendations for promising future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1\">Joseph P. Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Can Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1\">Ming Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turk_M/0/1/0/all/0/1\">Matthew A. Turk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Attack Generation Empowered by Min-Max Optimization. (arXiv:1906.03563v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1906.03563","description":"<p>The worst-case training principle that minimizes the maximal adversarial\nloss, also known as adversarial training (AT), has shown to be a\nstate-of-the-art approach for enhancing adversarial robustness. Nevertheless,\nmin-max optimization beyond the purpose of AT has not been rigorously explored\nin the adversarial context. In this paper, we show how a general framework of\nmin-max optimization over multiple domains can be leveraged to advance the\ndesign of different types of adversarial attacks. In particular, given a set of\nrisk sources, minimizing the worst-case attack loss can be reformulated as a\nmin-max problem by introducing domain weights that are maximized over the\nprobability simplex of the domain set. We showcase this unified framework in\nthree attack generation problems -- attacking model ensembles, devising\nuniversal perturbation under multiple inputs, and crafting attacks resilient to\ndata transformations. Extensive experiments demonstrate that our approach leads\nto substantial attack improvement over the existing heuristic strategies as\nwell as robustness improvement over state-of-the-art defense methods trained to\nbe robust against multiple perturbation types. Furthermore, we find that the\nself-adjusted domain weights learned from our min-max framework can provide a\nholistic tool to explain the difficulty level of attack across domains. Code is\navailable at https://github.com/wangjksjtu/minmax-adv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingkang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiacen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fardad_M/0/1/0/all/0/1\">Makan Fardad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Image-generation Enhanced Adaptation for Object Detection in Thermal images. (arXiv:2002.06770v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2002.06770","description":"<p>Object detection in thermal images is an important computer vision task and\nhas many applications such as unmanned vehicles, robotics, surveillance and\nnight vision. Deep learning based detectors have achieved major progress, which\nusually need large amount of labelled training data. However, labelled data for\nobject detection in thermal images is scarce and expensive to collect. How to\ntake advantage of the large number labelled visible images and adapt them into\nthermal image domain, is expected to solve. This paper proposes an unsupervised\nimage-generation enhanced adaptation method for object detection in thermal\nimages. To reduce the gap between visible domain and thermal domain, the\nproposed method manages to generate simulated fake thermal images that are\nsimilar to the target images, and preserves the annotation information of the\nvisible source domain. The image generation includes a CycleGAN based\nimage-to-image translation and an intensity inversion transformation. Generated\nfake thermal images are used as renewed source domain. And then the\noff-the-shelf Domain Adaptive Faster RCNN is utilized to reduce the gap between\ngenerated intermediate domain and the thermal target domain. Experiments\ndemonstrate the effectiveness and superiority of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fuyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wanyi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ENSEI: Efficient Secure Inference via Frequency-Domain Homomorphic Convolution for Privacy-Preserving Visual Recognition. (arXiv:2003.05328v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2003.05328","description":"<p>In this work, we propose ENSEI, a secure inference (SI) framework based on\nthe frequency-domain secure convolution (FDSC) protocol for the efficient\nexecution of privacy-preserving visual recognition. Our observation is that,\nunder the combination of homomorphic encryption and secret sharing, homomorphic\nconvolution can be obliviously carried out in the frequency domain,\nsignificantly simplifying the related computations. We provide protocol designs\nand parameter derivations for number-theoretic transform (NTT) based FDSC. In\nthe experiment, we thoroughly study the accuracy-efficiency trade-offs between\ntime- and frequency-domain homomorphic convolution. With ENSEI, compared to the\nbest known works, we achieve 5--11x online time reduction, up to 33x setup time\nreduction, and up to 10x reduction in the overall inference time. A further 33%\nof bandwidth reductions can be obtained on binary neural networks with only 1%\nof accuracy degradation on the CIFAR-10 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1\">Song Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianchen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hiromoto_M/0/1/0/all/0/1\">Masayuki Hiromoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_T/0/1/0/all/0/1\">Takashi Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teacher-Class Network: A Neural Network Compression Mechanism. (arXiv:2004.03281v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2004.03281","description":"<p>To reduce the overwhelming size of Deep Neural Networks (DNN) teacher-student\nmethodology tries to transfer knowledge from a complex teacher network to a\nsimple student network. We instead propose a novel method called the\nteacher-class network consisting of a single teacher and multiple student\nnetworks (i.e. class of students). Instead of transferring knowledge to one\nstudent only, the proposed method transfers a chunk of knowledge to each\nstudent. Our students are not trained for problem-specific logits, they are\ntrained to mimic knowledge (dense representation) learned by the teacher\nnetwork thus the combined knowledge learned by the class of students can be\nused to solve other problems as well. The proposed teacher-class architecture\nis evaluated on several benchmark datasets such as MNIST, Fashion MNIST, IMDB\nMovie Reviews, CAMVid, CIFAR-10 and ImageNet on multiple tasks including image\nclassification, sentiment classification and segmentation. Our approach\noutperforms the state of-the-art single student approach in terms of accuracy\nas well as computational cost while achieving 10-30 times reduction in\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malik_S/0/1/0/all/0/1\">Shaiq Munir Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haider_M/0/1/0/all/0/1\">Muhammad Umair Haider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tharani_M/0/1/0/all/0/1\">Mohbat Tharani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasheed_M/0/1/0/all/0/1\">Musab Rasheed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taj_M/0/1/0/all/0/1\">Murtaza Taj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Mid-Level Patterns for Cross-Domain Few-Shot Recognition. (arXiv:2008.03128v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.03128","description":"<p>Existing few-shot learning (FSL) methods usually assume base classes and\nnovel classes are from the same domain (in-domain setting). However, in\npractice, it may be infeasible to collect sufficient training samples for some\nspecial domains to construct base classes. To solve this problem, cross-domain\nFSL (CDFSL) is proposed very recently to transfer knowledge from general-domain\nbase classes to special-domain novel classes. Existing CDFSL works mostly focus\non transferring between near domains, while rarely consider transferring\nbetween distant domains, which is in practical need as any novel classes could\nappear in real-world applications, and is even more challenging. In this paper,\nwe study a challenging subset of CDFSL where the novel classes are in distant\ndomains from base classes, by revisiting the mid-level features, which are more\ntransferable yet under-explored in main stream FSL work. To boost the\ndiscriminability of mid-level features, we propose a residual-prediction task\nto encourage mid-level features to learn discriminative information of each\nsample. Notably, such mechanism also benefits the in-domain FSL and CDFSL in\nnear domains. Therefore, we provide two types of features for both cross- and\nin-domain FSL respectively, under the same training framework. Experiments\nunder both settings on six public datasets, including two challenging medical\ndatasets, validate the our rationale and demonstrate state-of-the-art\nperformance. Code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yixiong Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanghang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">JianPeng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moura_J/0/1/0/all/0/1\">Jos&#xe9; M. F. Moura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Encoding Robustness to Image Style via Adversarial Feature Perturbations. (arXiv:2009.08965v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.08965","description":"<p>Adversarial training is the industry standard for producing models that are\nrobust to small adversarial perturbations. However, machine learning\npractitioners need models that are robust to other kinds of changes that occur\nnaturally, such as changes in the style or illumination of input images. Such\nchanges in input distribution have been effectively modeled as shifts in the\nmean and variance of deep image features. We adapt adversarial training by\ndirectly perturbing feature statistics, rather than image pixels, to produce\nmodels that are robust to various unseen distributional shifts. We explore the\nrelationship between these perturbations and distributional shifts by\nvisualizing adversarial features. Our proposed method, Adversarial Batch\nNormalization (AdvBN), is a single network layer that generates worst-case\nfeature perturbations during training. By fine-tuning neural networks on\nadversarial feature distributions, we observe improved robustness of networks\nto various unseen distributional shifts, including style variations and image\ncorruptions. In addition, we show that our proposed adversarial feature\nperturbation can be complementary to existing image space data augmentation\nmethods, leading to improved performance. The source code and pre-trained\nmodels are released at \\url{https://github.com/azshue/AdvBN}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_M/0/1/0/all/0/1\">Manli Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Translation for Medical Image Generation -- Ischemic Stroke Lesions. (arXiv:2010.02745v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2010.02745","description":"<p>Deep learning based disease detection and segmentation algorithms promise to\nimprove many clinical processes. However, such algorithms require vast amounts\nof annotated training data, which are typically not available in the medical\ncontext due to data privacy, legal obstructions, and non-uniform data\nacquisition protocols. Synthetic databases with annotated pathologies could\nprovide the required amounts of training data. We demonstrate with the example\nof ischemic stroke that an improvement in lesion segmentation is feasible using\ndeep learning based augmentation. To this end, we train different\nimage-to-image translation models to synthesize magnetic resonance images of\nbrain volumes with and without stroke lesions from semantic segmentation maps.\nIn addition, we train a generative adversarial network to generate synthetic\nlesion masks. Subsequently, we combine these two components to build a large\ndatabase of synthetic stroke images. The performance of the various models is\nevaluated using a U-Net which is trained to segment stroke lesions on a\nclinical test set. We report a Dice score of $\\mathbf{72.8}$%\n[$\\mathbf{70.8\\pm1.0}$%] for the model with the best performance, which\noutperforms the model trained on the clinical images alone $\\mathbf{67.3}$%\n[$\\mathbf{63.2\\pm1.9}$%], and is close to the human inter-reader Dice score of\n$\\mathbf{76.9}$%. Moreover, we show that for a small database of only 10 or 50\nclinical cases, synthetic data augmentation yields significant improvement\ncompared to a setting where no synthetic data is used. To the best of our\nknowledge, this presents the first comparative analysis of synthetic data\naugmentation based on image-to-image translation, and first application to\nischemic stroke.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Platscher_M/0/1/0/all/0/1\">Moritz Platscher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zopes_J/0/1/0/all/0/1\">Jonathan Zopes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Federau_C/0/1/0/all/0/1\">Christian Federau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NCP-VAE: Variational Autoencoders with Noise Contrastive Priors. (arXiv:2010.02917v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.02917","description":"<p>Variational autoencoders (VAEs) are one of the powerful likelihood-based\ngenerative models with applications in various domains. However, they struggle\nto generate high-quality images, especially when samples are obtained from the\nprior without any tempering. One explanation for VAEs' poor generative quality\nis the prior hole problem: the prior distribution fails to match the aggregate\napproximate posterior. Due to this mismatch, there exist areas in the latent\nspace with high density under the prior that do not correspond to any encoded\nimage. Samples from those areas are decoded to corrupted images. To tackle this\nissue, we propose an energy-based prior defined by the product of a base prior\ndistribution and a reweighting factor, designed to bring the base closer to the\naggregate posterior. We train the reweighting factor by noise contrastive\nestimation, and we generalize it to hierarchical VAEs with many latent variable\ngroups. Our experiments confirm that the proposed noise contrastive priors\nimprove the generative performance of state-of-the-art VAEs by a large margin\non the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aneja_J/0/1/0/all/0/1\">Jyoti Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander Schwing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahdat_A/0/1/0/all/0/1\">Arash Vahdat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invariant Representation Learning for Infant Pose Estimation with Small Data. (arXiv:2010.06100v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.06100","description":"<p>Infant motion analysis is a topic with critical importance in early childhood\ndevelopment studies. However, while the applications of human pose estimation\nhave become more and more broad, models trained on large-scale adult pose\ndatasets are barely successful in estimating infant poses due to the\nsignificant differences in their body ratio and the versatility of their poses.\nMoreover, the privacy and security considerations hinder the availability of\nadequate infant pose data required for training of a robust model from scratch.\nTo address this problem, this paper presents (1) building and publicly\nreleasing a hybrid synthetic and real infant pose (SyRIP) dataset with small\nyet diverse real infant images as well as generated synthetic infant poses and\n(2) a multi-stage invariant representation learning strategy that could\ntransfer the knowledge from the adjacent domains of adult poses and synthetic\ninfant images into our fine-tuned domain-adapted infant pose (FiDIP) estimation\nmodel. In our ablation study, with identical network structure, models trained\non SyRIP dataset show noticeable improvement over the ones trained on the only\nother public infant pose datasets. Integrated with pose estimation backbone\nnetworks with varying complexity, FiDIP performs consistently better than the\nfine-tuned versions of those models. One of our best infant pose estimation\nperformers on the state-of-the-art DarkPose model shows mean average precision\n(mAP) of 93.6.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaofei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_N/0/1/0/all/0/1\">Nihang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuangjun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostadabbas_S/0/1/0/all/0/1\">Sarah Ostadabbas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pose And Joint-Aware Action Recognition. (arXiv:2010.08164v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.08164","description":"<p>Recent progress on action recognition has mainly focused on RGB and optical\nflow features. In this paper, we approach the problem of joint-based action\nrecognition. Unlike other modalities, constellation of joints and their motion\ngenerate models with succinct human motion information for activity\nrecognition. We present a new model for joint-based action recognition, which\nfirst extracts motion features from each joint separately through a shared\nmotion encoder before performing collective reasoning. Our joint selector\nmodule re-weights the joint information to select the most discriminative\njoints for the task. We also propose a novel joint-contrastive loss that pulls\ntogether groups of joint features which convey the same action. We strengthen\nthe joint-based representations by using a geometry-aware data augmentation\ntechnique which jitters pose heatmaps while retaining the dynamics of the\naction. We show large improvements over the current state-of-the-art\njoint-based approaches on JHMDB, HMDB, Charades, AVA action recognition\ndatasets. A late fusion with RGB and Flow-based approaches yields additional\nimprovements. Our model also outperforms the existing baseline on Mimetics, a\ndataset with out-of-context actions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Anshul Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shlok Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1\">Ankan Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun-Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RobustBench: a standardized adversarial robustness benchmark. (arXiv:2010.09670v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.09670","description":"<p>As a research community, we are still lacking a systematic understanding of\nthe progress on adversarial robustness which often makes it hard to identify\nthe most promising ideas in training robust models. A key challenge in\nbenchmarking robustness is that its evaluation is often error-prone leading to\nrobustness overestimation. Our goal is to establish a standardized benchmark of\nadversarial robustness, which as accurately as possible reflects the robustness\nof the considered models within a reasonable computational budget. To this end,\nwe start by considering the image classification task and introduce\nrestrictions (possibly loosened in the future) on the allowed models. We\nevaluate adversarial robustness with AutoAttack, an ensemble of white- and\nblack-box attacks, which was recently shown in a large-scale study to improve\nalmost all robustness evaluations compared to the original publications. To\nprevent overadaptation of new defenses to AutoAttack, we welcome external\nevaluations based on adaptive attacks, especially where AutoAttack flags a\npotential overestimation of robustness. Our leaderboard, hosted at\nhttps://robustbench.github.io/, contains evaluations of 120+ models and aims at\nreflecting the current state of the art in image classification on a set of\nwell-defined tasks in $\\ell_\\infty$- and $\\ell_2$-threat models and on common\ncorruptions, with possible extensions in the future. Additionally, we\nopen-source the library https://github.com/RobustBench/robustbench that\nprovides unified access to 80+ robust models to facilitate their downstream\napplications. Finally, based on the collected models, we analyze the impact of\nrobustness on the performance on distribution shifts, calibration,\nout-of-distribution detection, fairness, privacy leakage, smoothness, and\ntransferability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Croce_F/0/1/0/all/0/1\">Francesco Croce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andriushchenko_M/0/1/0/all/0/1\">Maksym Andriushchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sehwag_V/0/1/0/all/0/1\">Vikash Sehwag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debenedetti_E/0/1/0/all/0/1\">Edoardo Debenedetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flammarion_N/0/1/0/all/0/1\">Nicolas Flammarion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_M/0/1/0/all/0/1\">Mung Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1\">Prateek Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1\">Matthias Hein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Distinguishing Normal versus Abnormal Chest Radiographs and Generalization to Unseen Diseases. (arXiv:2010.11375v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2010.11375","description":"<p>Chest radiography (CXR) is the most widely-used thoracic clinical imaging\nmodality and is crucial for guiding the management of cardiothoracic\nconditions. The detection of specific CXR findings has been the main focus of\nseveral artificial intelligence (AI) systems. However, the wide range of\npossible CXR abnormalities makes it impractical to build specific systems to\ndetect every possible condition. In this work, we developed and evaluated an AI\nsystem to classify CXRs as normal or abnormal. For development, we used a\nde-identified dataset of 248,445 patients from a multi-city hospital network in\nIndia. To assess generalizability, we evaluated our system using 6\ninternational datasets from India, China, and the United States. Of these\ndatasets, 4 focused on diseases that the AI was not trained to detect: 2\ndatasets with tuberculosis and 2 datasets with coronavirus disease 2019. Our\nresults suggest that the AI system generalizes to new patient populations and\nabnormalities. In a simulated workflow where the AI system prioritized abnormal\ncases, the turnaround time for abnormal cases reduced by 7-28%. These results\nrepresent an important step towards evaluating whether AI can be safely used to\nflag cases in a general setting where previously unseen abnormalities exist.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nabulsi_Z/0/1/0/all/0/1\">Zaid Nabulsi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sellergren_A/0/1/0/all/0/1\">Andrew Sellergren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jamshy_S/0/1/0/all/0/1\">Shahar Jamshy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lau_C/0/1/0/all/0/1\">Charles Lau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Santos_E/0/1/0/all/0/1\">Edward Santos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiraly_A/0/1/0/all/0/1\">Atilla P. Kiraly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_W/0/1/0/all/0/1\">Wenxing Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pilgrim_R/0/1/0/all/0/1\">Rory Pilgrim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kazemzadeh_S/0/1/0/all/0/1\">Sahar Kazemzadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jin Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalidindi_S/0/1/0/all/0/1\">Sreenivasa Raju Kalidindi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Etemadi_M/0/1/0/all/0/1\">Mozziyar Etemadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garcia_Vicente_F/0/1/0/all/0/1\">Florencia Garcia-Vicente</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Melnick_D/0/1/0/all/0/1\">David Melnick</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Corrado_G/0/1/0/all/0/1\">Greg S. Corrado</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1\">Lily Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eswaran_K/0/1/0/all/0/1\">Krish Eswaran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tse_D/0/1/0/all/0/1\">Daniel Tse</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beladia_N/0/1/0/all/0/1\">Neeral Beladia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_P/0/1/0/all/0/1\">Po-Hsuan Cameron Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shetty_S/0/1/0/all/0/1\">Shravya Shetty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi Scale Identity-Preserving Image-to-Image Translation Network for Low-Resolution Face Recognition. (arXiv:2010.12249v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.12249","description":"<p>State-of-the-art deep neural network models have reached near perfect face\nrecognition accuracy rates on controlled high-resolution face images. However,\ntheir performance is drastically degraded when they are tested with very\nlow-resolution face images. This is particularly critical in surveillance\nsystems, where a low-resolution probe image is to be matched with\nhigh-resolution gallery images. super-resolution techniques aim at producing\nhigh-resolution face images from low-resolution counterparts. While they are\ncapable of reconstructing images that are visually appealing, the\nidentity-related information is not preserved. Here, we propose an\nidentity-preserving end-to-end image-to-image translation deep neural network\nwhich is capable of super-resolving very low-resolution faces to their\nhigh-resolution counterparts while preserving identity-related information. We\nachieved this by training a very deep convolutional encoder-decoder network\nwith a symmetric contracting path between corresponding layers. This network\nwas trained with a combination of a reconstruction and an identity-preserving\nloss, on multi-scale low-resolution conditions. Extensive quantitative\nevaluations of our proposed model demonstrated that it outperforms competing\nsuper-resolution and low-resolution face recognition methods on natural and\nartificial low-resolution face data sets and even unseen identities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khazaie_V/0/1/0/all/0/1\">Vahid Reza Khazaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayat_N/0/1/0/all/0/1\">Nicky Bayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohsenzadeh_Y/0/1/0/all/0/1\">Yalda Mohsenzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Casting a BAIT for Offline and Online Source-free Domain Adaptation. (arXiv:2010.12427v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.12427","description":"<p>We address the source-free domain adaptation (SFDA) problem, where only the\nsource model is available during adaptation to the target domain. We consider\ntwo settings: the offline setting where all target data can be visited multiple\ntimes (epochs) to arrive at a prediction for each target sample, and the online\nsetting where the target data needs to be directly classified upon arrival.\nInspired by diverse classifier based domain adaptation methods, in this paper\nwe introduce a second classifier, but with another classifier head fixed. When\nadapting to the target domain, the additional classifier initialized from\nsource classifier is expected to find misclassified features. Next, when\nupdating the feature extractor, those features will be pushed towards the right\nside of the source decision boundary, thus achieving source-free domain\nadaptation. Experimental results show that the proposed method achieves\ncompetitive results for offline SFDA on several benchmark datasets compared\nwith existing DA and SFDA methods, and our method surpasses by a large margin\nother SFDA methods under online source-free domain adaptation setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shiqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaxing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herranz_L/0/1/0/all/0/1\">Luis Herranz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1\">Shangling Jui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Contextual Prediction for Learned Image Compression. (arXiv:2011.09704v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.09704","description":"<p>Over the past several years, we have witnessed impressive progress in the\nfield of learned image compression. Recent learned image codecs are commonly\nbased on autoencoders, that first encode an image into low-dimensional latent\nrepresentations and then decode them for reconstruction purposes. To capture\nspatial dependencies in the latent space, prior works exploit hyperprior and\nspatial context model to build an entropy model, which estimates the bit-rate\nfor end-to-end rate-distortion optimization. However, such an entropy model is\nsuboptimal from two aspects: (1) It fails to capture spatially global\ncorrelations among the latents. (2) Cross-channel relationships of the latents\nare still underexplored. In this paper, we propose the concept of separate\nentropy coding to leverage a serial decoding process for causal contextual\nentropy prediction in the latent space. A causal context model is proposed that\nseparates the latents across channels and makes use of cross-channel\nrelationships to generate highly informative contexts. Furthermore, we propose\na causal global prediction model, which is able to find global reference points\nfor accurate predictions of unknown points. Both these two models facilitate\nentropy estimation without the transmission of overhead. In addition, we\nfurther adopt a new separate attention module to build more powerful transform\nnetworks. Experimental results demonstrate that our full image compression\nmodel outperforms standard VVC/H.266 codec on Kodak dataset in terms of both\nPSNR and MS-SSIM, yielding the state-of-the-art rate-distortion performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zongyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Runsen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotation-Efficient Untrimmed Video Action Recognition. (arXiv:2011.14478v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.14478","description":"<p>Deep learning has achieved great success in recognizing video actions, but\nthe collection and annotation of training data are still quite laborious, which\nmainly lies in two aspects: (1) the amount of required annotated data is large;\n(2) temporally annotating the location of each action is time-consuming. Works\nsuch as few-shot learning or untrimmed video recognition have been proposed to\nhandle either one aspect or the other. However, very few existing works can\nhandle both issues simultaneously. In this paper, we target a new problem,\nAnnotation-Efficient Video Recognition, to reduce the requirement of\nannotations for both large amount of samples and the action location. Such\nproblem is challenging due to two aspects: (1) the untrimmed videos only have\nweak supervision; (2) video segments not relevant to current actions of\ninterests (background, BG) could contain actions of interests (foreground, FG)\nin novel classes, which is a widely existing phenomenon but has rarely been\nstudied in few-shot untrimmed video recognition. To achieve this goal, by\nanalyzing the property of BG, we categorize BG into informative BG (IBG) and\nnon-informative BG (NBG), and we propose (1) an open-set detection based method\nto find the NBG and FG, (2) a contrastive learning method to learn IBG and\ndistinguish NBG in a self-supervised way, and (3) a self-weighting mechanism\nfor the better distinguishing of IBG and FG. Extensive experiments on\nActivityNet v1.2 and ActivityNet v1.3 verify the rationale and effectiveness of\nthe proposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yixiong Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanghang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moura_J/0/1/0/all/0/1\">Jos&#xe9; M. F. Moura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative label cleaning for transductive and semi-supervised few-shot learning. (arXiv:2012.07962v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.07962","description":"<p>Few-shot learning amounts to learning representations and acquiring knowledge\nsuch that novel tasks may be solved with both supervision and data being\nlimited. Improved performance is possible by transductive inference, where the\nentire test set is available concurrently, and semi-supervised learning, where\nmore unlabeled data is available. Focusing on these two settings, we introduce\na new algorithm that leverages the manifold structure of the labeled and\nunlabeled data distribution to predict pseudo-labels, while balancing over\nclasses and using the loss value distribution of a limited-capacity classifier\nto select the cleanest labels, iteratively improving the quality of\npseudo-labels. Our solution surpasses or matches the state of the art results\non four benchmark datasets, namely miniImageNet, tieredImageNet, CUB and\nCIFAR-FS, while being robust over feature space pre-processing and the quantity\nof available data. The publicly available source code can be found in\nhttps://github.com/MichalisLazarou/iLPC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lazarou_M/0/1/0/all/0/1\">Michalis Lazarou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stathaki_T/0/1/0/all/0/1\">Tania Stathaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1\">Yannis Avrithis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Momentum-Contrastive Pre-Training for Robust Feature Extraction. (arXiv:2012.13154v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.13154","description":"<p>Recently proposed adversarial self-supervised learning methods usually\nrequire big batches and long training epochs to extract robust features, which\nis not friendly in practical application. In this paper, we present a novel\nadversarial momentum-contrastive learning approach that leverages two memory\nbanks to track the invariant features across different mini-batches. These\nmemory banks can be efficiently incorporated into each iteration and help the\nnetwork to learn more robust feature representations with smaller batches and\nfar fewer epochs. Furthermore, after fine-tuning on the classification tasks,\nthe proposed approach can meet or exceed the performance of some\nstate-of-the-art supervised baselines on real world datasets. Our code is\navailable at \\url{https://github.com/MTandHJ/amoc}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real Masks and Spoof Faces: On the Masked Face Presentation Attack Detection. (arXiv:2103.01546v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.01546","description":"<p>Face masks have become one of the main methods for reducing the transmission\nof COVID-19. This makes face recognition (FR) a challenging task because masks\nhide several discriminative features of faces. Moreover, face presentation\nattack detection (PAD) is crucial to ensure the security of FR systems. In\ncontrast to the growing number of masked FR studies, the impact of face masked\nattacks on PAD has not been explored. Therefore, we present novel attacks with\nreal face masks placed on presentations and attacks with subjects wearing masks\nto reflect the current real-world situation. Furthermore, this study\ninvestigates the effect of masked attacks on PAD performance by using seven\nstate-of-the-art PAD algorithms under different experimental settings. We also\nevaluate the vulnerability of FR systems to masked attacks. The experiments\nshow that real masked attacks pose a serious threat to the operation and\nsecurity of FR systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meiling Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Touchless Palmprint Recognition based on 3D Gabor Template and Block Feature Refinement. (arXiv:2103.02167v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.02167","description":"<p>With the growing demand for hand hygiene and convenience of use, palmprint\nrecognition with touchless manner made a great development recently, providing\nan effective solution for person identification. Despite many efforts that have\nbeen devoted to this area, it is still uncertain about the discriminative\nability of the contactless palmprint, especially for large-scale datasets. To\ntackle the problem, in this paper, we build a large-scale touchless palmprint\ndataset containing 2334 palms from 1167 individuals. To our best knowledge, it\nis the largest contactless palmprint image benchmark ever collected with regard\nto the number of individuals and palms. Besides, we propose a novel deep\nlearning framework for touchless palmprint recognition named 3DCPN (3D\nConvolution Palmprint recognition Network) which leverages 3D convolution to\ndynamically integrate multiple Gabor features. In 3DCPN, a novel variant of\nGabor filter is embedded into the first layer for enhancement of curve feature\nextraction. With a well-designed ensemble scheme,low-level 3D features are then\nconvolved to extract high-level features. Finally on the top, we set a\nregion-based loss function to strengthen the discriminative ability of both\nglobal and local descriptors. To demonstrate the superiority of our method,\nextensive experiments are conducted on our dataset and other popular databases\nTongJi and IITD, where the results show the proposed 3DCPN achieves\nstate-of-the-art or comparable performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoqun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Dandan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Importance of Sampling in Training GCNs: Tighter Analysis and Variance Reduction. (arXiv:2103.02696v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.02696","description":"<p>Graph Convolutional Networks (GCNs) have achieved impressive empirical\nadvancement across a wide variety of semi-supervised node classification tasks.\nDespite their great success, training GCNs on large graphs suffers from\ncomputational and memory issues. A potential path to circumvent these obstacles\nis sampling-based methods, where at each layer a subset of nodes is sampled.\nAlthough recent studies have empirically demonstrated the effectiveness of\nsampling-based methods, these works lack theoretical convergence guarantees\nunder realistic settings and cannot fully leverage the information of evolving\nparameters during optimization. In this paper, we describe and analyze a\ngeneral doubly variance reduction schema that can accelerate any sampling\nmethod under the memory budget. The motivating impetus for the proposed schema\nis a careful analysis of the variance of sampling methods where it is shown\nthat the induced variance can be decomposed into node embedding approximation\nvariance (zeroth-order variance) during forward propagation and\nlayerwise-gradient variance (first-order variance) during backward propagation.\nWe theoretically analyze the convergence of the proposed schema and show that\nit enjoys an $\\mathcal{O}(1/T)$ convergence rate. We complement our theoretical\nresults by integrating the proposed schema in different sampling methods and\napplying them to different large real-world graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cong_W/0/1/0/all/0/1\">Weilin Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1\">Morteza Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_M/0/1/0/all/0/1\">Mehrdad Mahdavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised 3D Representation Learning of Dressed Humans from Social Media Videos. (arXiv:2103.03319v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.03319","description":"<p>A key challenge of learning a visual representation for the 3D high fidelity\ngeometry of dressed humans lies in the limited availability of the ground truth\ndata (e.g., 3D scanned models), which results in the performance degradation of\n3D human reconstruction when applying to real-world imagery. We address this\nchallenge by leveraging a new data resource: a number of social media dance\nvideos that span diverse appearance, clothing styles, performances, and\nidentities. Each video depicts dynamic movements of the body and clothes of a\nsingle person while lacking the 3D ground truth geometry. To learn a visual\nrepresentation from these videos, we present a new self-supervised learning\nmethod to use the local transformation that warps the predicted local geometry\nof the person from an image to that of another image at a different time\ninstant. This allows self-supervision by enforcing a temporal coherence over\nthe predictions. In addition, we jointly learn the depths along with the\nsurface normals that are highly responsive to local texture, wrinkle, and shade\nby maximizing their geometric consistency. Our method is end-to-end trainable,\nresulting in high fidelity depth estimation that predicts fine geometry\nfaithful to the input real image. We demonstrate that our method outperforms\nthe state-of-the-art human depth estimation and human shape recovery approaches\non both real and rendered images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jafarian_Y/0/1/0/all/0/1\">Yasamin Jafarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyun Soo Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Morphological and Histogram based Text Line Segmentation in the OCR Context. (arXiv:2103.08922v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.08922","description":"<p>Text line segmentation is one of the pre-stages of modern optical character\nrecognition systems. The algorithmic approach proposed by this paper has been\ndesigned for this exact purpose. Its main characteristic is the combination of\ntwo different techniques, morphological image operations and horizontal\nhistogram projections. The method was developed to be applied on a historic\ndata collection that commonly features quality issues, such as degraded paper,\nblurred text, or presence of noise. For that reason, the segmenter in question\ncould be of particular interest for cultural institutions, that want access to\nrobust line bounding boxes for a given historic document. Because of the\npromising segmentation results that are joined by low computational cost, the\nalgorithm was incorporated into the OCR pipeline of the National Library of\nLuxembourg, in the context of the initiative of reprocessing their historic\nnewspaper collection. The general contribution of this paper is to outline the\napproach and to evaluate the gains in terms of accuracy and speed, comparing it\nto the segmentation algorithm bundled with the used open source OCR software.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1\">Pit Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Handling Missing Observations with an RNN-based Prediction-Update Cycle. (arXiv:2103.11747v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11747","description":"<p>In tasks such as tracking, time-series data inevitably carry missing\nobservations. While traditional tracking approaches can handle missing\nobservations, recurrent neural networks (RNNs) are designed to receive input\ndata in every step. Furthermore, current solutions for RNNs, like omitting the\nmissing data or data imputation, are not sufficient to account for the\nresulting increased uncertainty. Towards this end, this paper introduces an\nRNN-based approach that provides a full temporal filtering cycle for motion\nstate estimation. The Kalman filter inspired approach, enables to deal with\nmissing observations and outliers. For providing a full temporal filtering\ncycle, a basic RNN is extended to take observations and the associated belief\nabout its accuracy into account for updating the current state. An RNN\nprediction model, which generates a parametrized distribution to capture the\npredicted states, is combined with an RNN update model, which relies on the\nprediction model output and the current observation. By providing the model\nwith masking information, binary-encoded missing events, the model can overcome\nlimitations of standard techniques for dealing with missing input values. The\nmodel abilities are demonstrated on synthetic data reflecting prototypical\npedestrian tracking scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1\">Stefan Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hug_R/0/1/0/all/0/1\">Ronny Hug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubner_W/0/1/0/all/0/1\">Wolfgang H&#xfc;bner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1\">Michael Arens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_B/0/1/0/all/0/1\">Brendan T. Morris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch Craft: Video Denoising by Deep Modeling and Patch Matching. (arXiv:2103.13767v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.13767","description":"<p>The non-local self-similarity property of natural images has been exploited\nextensively for solving various image processing problems. When it comes to\nvideo sequences, harnessing this force is even more beneficial due to the\ntemporal redundancy. In the context of image and video denoising, many\nclassically-oriented algorithms employ self-similarity, splitting the data into\noverlapping patches, gathering groups of similar ones and processing these\ntogether somehow. With the emergence of convolutional neural networks (CNN),\nthe patch-based framework has been abandoned. Most CNN denoisers operate on the\nwhole image, leveraging non-local relations only implicitly by using a large\nreceptive field. This work proposes a novel approach for leveraging\nself-similarity in the context of video denoising, while still relying on a\nregular convolutional architecture. We introduce a concept of patch-craft\nframes - artificial frames that are similar to the real ones, built by tiling\nmatched patches. Our algorithm augments video sequences with patch-craft frames\nand feeds them to a CNN. We demonstrate the substantial boost in denoising\nperformance obtained with the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaksman_G/0/1/0/all/0/1\">Gregory Vaksman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1\">Michael Elad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViViT: A Video Vision Transformer. (arXiv:2103.15691v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15691","description":"<p>We present pure-transformer based models for video classification, drawing\nupon the recent success of such models in image classification. Our model\nextracts spatio-temporal tokens from the input video, which are then encoded by\na series of transformer layers. In order to handle the long sequences of tokens\nencountered in video, we propose several, efficient variants of our model which\nfactorise the spatial- and temporal-dimensions of the input. Although\ntransformer-based models are known to only be effective when large training\ndatasets are available, we show how we can effectively regularise the model\nduring training and leverage pretrained image models to be able to train on\ncomparatively small datasets. We conduct thorough ablation studies, and achieve\nstate-of-the-art results on multiple video classification benchmarks including\nKinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in\nTime, outperforming prior methods based on deep 3D convolutional networks. To\nfacilitate further research, we release code at\nhttps://github.com/google-research/scenic/tree/main/scenic/projects/vivit\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heigold_G/0/1/0/all/0/1\">Georg Heigold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1\">Mario Lu&#x10d;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effect of Radiology Report Labeler Quality on Deep Learning Models for Chest X-Ray Interpretation. (arXiv:2104.00793v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.00793","description":"<p>Although deep learning models for chest X-ray interpretation are commonly\ntrained on labels generated by automatic radiology report labelers, the impact\nof improvements in report labeling on the performance of chest X-ray\nclassification models has not been systematically investigated. We first\ncompare the CheXpert, CheXbert, and VisualCheXbert labelers on the task of\nextracting accurate chest X-ray image labels from radiology reports, reporting\nthat the VisualCheXbert labeler outperforms the CheXpert and CheXbert labelers.\nNext, after training image classification models using labels generated from\nthe different radiology report labelers on one of the largest datasets of chest\nX-rays, we show that an image classification model trained on labels from the\nVisualCheXbert labeler outperforms image classification models trained on\nlabels from the CheXpert and CheXbert labelers. Our work suggests that recent\nimprovements in radiology report labeling can translate to the development of\nhigher performing chest X-ray classification models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smit_A/0/1/0/all/0/1\">Akshay Smit</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distributional Robustness Loss for Long-tail Learning. (arXiv:2104.03066v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.03066","description":"<p>Real-world data is often unbalanced and long-tailed, but deep models struggle\nto recognize rare classes in the presence of frequent classes. To address\nunbalanced data, most studies try balancing the data, the loss, or the\nclassifier to reduce classification bias towards head classes. Far less\nattention has been given to the latent representations learned with unbalanced\ndata. We show that the feature extractor part of deep networks suffers greatly\nfrom this bias. We propose a new loss based on robustness theory, which\nencourages the model to learn high-quality representations for both head and\ntail classes. While the general form of the robustness loss may be hard to\ncompute, we further derive an easy-to-compute upper bound that can be minimized\nefficiently. This procedure reduces representation bias towards head classes in\nthe feature space and achieves new SOTA results on CIFAR100-LT, ImageNet-LT,\nand iNaturalist long-tail benchmarks. We find that training with robustness\nincreases recognition accuracy of tail classes while largely maintaining the\naccuracy of head classes. The new robustness loss can be combined with various\nclassifier balancing techniques and can be applied to representations at\nseveral layers of the deep model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1\">Dvir Samuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1\">Gal Chechik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Walsh-Hadamard Transform and Smooth-Thresholding Based Binary Layers in Deep Neural Networks. (arXiv:2104.07085v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07085","description":"<p>In this paper, we propose a novel layer based on fast Walsh-Hadamard\ntransform (WHT) and smooth-thresholding to replace $1\\times 1$ convolution\nlayers in deep neural networks. In the WHT domain, we denoise the transform\ndomain coefficients using the new smooth-thresholding non-linearity, a smoothed\nversion of the well-known soft-thresholding operator. We also introduce a\nfamily of multiplication-free operators from the basic 2$\\times$2 Hadamard\ntransform to implement $3\\times 3$ depthwise separable convolution layers.\nUsing these two types of layers, we replace the bottleneck layers in\nMobileNet-V2 to reduce the network's number of parameters with a slight loss in\naccuracy. For example, by replacing the final third bottleneck layers, we\nreduce the number of parameters from 2.270M to 540K. This reduces the accuracy\nfrom 95.21\\% to 92.98\\% on the CIFAR-10 dataset. Our approach significantly\nimproves the speed of data processing. The fast Walsh-Hadamard transform has a\ncomputational complexity of $O(m\\log_2 m)$. As a result, it is computationally\nmore efficient than the $1\\times1$ convolution layer. The fast Walsh-Hadamard\nlayer processes a tensor in $\\mathbb{R}^{10\\times32\\times32\\times1024}$ about 2\ntimes faster than $1\\times1$ convolution layer on NVIDIA Jetson Nano computer\nboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hongyi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabawi_D/0/1/0/all/0/1\">Diaa Dabawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cetin_A/0/1/0/all/0/1\">Ahmet Enis Cetin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning for detecting pulmonary tuberculosis via chest radiography: an international study across 10 countries. (arXiv:2105.07540v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.07540","description":"<p>Tuberculosis (TB) is a top-10 cause of death worldwide. Though the WHO\nrecommends chest radiographs (CXRs) for TB screening, the limited availability\nof CXR interpretation is a barrier. We trained a deep learning system (DLS) to\ndetect active pulmonary TB using CXRs from 9 countries across Africa, Asia, and\nEurope, and utilized large-scale CXR pretraining, attention pooling, and noisy\nstudent semi-supervised learning. Evaluation was on (1) a combined test set\nspanning China, India, US, and Zambia, and (2) an independent mining population\nin South Africa. Given WHO targets of 90% sensitivity and 70% specificity, the\nDLS's operating point was prespecified to favor sensitivity over specificity.\nOn the combined test set, the DLS's ROC curve was above all 9 India-based\nradiologists, with an AUC of 0.90 (95%CI 0.87-0.92). The DLS's sensitivity\n(88%) was higher than the India-based radiologists (75% mean sensitivity),\np&lt;0.001 for superiority; and its specificity (79%) was non-inferior to the\nradiologists (84% mean specificity), p=0.004. Similar trends were observed\nwithin HIV positive and sputum smear positive sub-groups, and in the South\nAfrica test set. We found that 5 US-based radiologists (where TB isn't endemic)\nwere more sensitive and less specific than the India-based radiologists (where\nTB is endemic). The DLS also remained non-inferior to the US-based\nradiologists. In simulations, using the DLS as a prioritization tool for\nconfirmatory testing reduced the cost per positive case detected by 40-80%\ncompared to using confirmatory testing alone. To conclude, our DLS generalized\nto 5 countries, and merits prospective evaluation to assist cost-effective\nscreening efforts in radiologist-limited settings. Operating point flexibility\nmay permit customization of the DLS to account for site-specific factors such\nas TB prevalence, demographics, clinical resources, and customary practice\npatterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kazemzadeh_S/0/1/0/all/0/1\">Sahar Kazemzadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jin Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jamshy_S/0/1/0/all/0/1\">Shahar Jamshy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pilgrim_R/0/1/0/all/0/1\">Rory Pilgrim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nabulsi_Z/0/1/0/all/0/1\">Zaid Nabulsi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Christina Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beladia_N/0/1/0/all/0/1\">Neeral Beladia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lau_C/0/1/0/all/0/1\">Charles Lau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McKinney_S/0/1/0/all/0/1\">Scott Mayer McKinney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hughes_T/0/1/0/all/0/1\">Thad Hughes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiraly_A/0/1/0/all/0/1\">Atilla Kiraly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalidindi_S/0/1/0/all/0/1\">Sreenivasa Raju Kalidindi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muyoyeta_M/0/1/0/all/0/1\">Monde Muyoyeta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malemela_J/0/1/0/all/0/1\">Jameson Malemela</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shih_T/0/1/0/all/0/1\">Ting Shih</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Corrado_G/0/1/0/all/0/1\">Greg S. Corrado</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1\">Lily Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chou_K/0/1/0/all/0/1\">Katherine Chou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_P/0/1/0/all/0/1\">Po-Hsuan Cameron Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eswaran_K/0/1/0/all/0/1\">Krish Eswaran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tse_D/0/1/0/all/0/1\">Daniel Tse</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shetty_S/0/1/0/all/0/1\">Shravya Shetty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prabhakara_S/0/1/0/all/0/1\">Shruthi Prabhakara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2106.00908v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00908","description":"<p>Multiple instance learning (MIL) is a powerful tool to solve the weakly\nsupervised classification in whole slide image (WSI) based pathology diagnosis.\nHowever, the current MIL methods are usually based on independent and identical\ndistribution hypothesis, thus neglect the correlation among different\ninstances. To address this problem, we proposed a new framework, called\ncorrelated MIL, and provided a proof for convergence. Based on this framework,\nwe devised a Transformer based MIL (TransMIL), which explored both\nmorphological and spatial information. The proposed TransMIL can effectively\ndeal with unbalanced/balanced and binary/multiple classification with great\nvisualization and interpretability. We conducted various experiments for three\ndifferent computational pathology problems and achieved better performance and\nfaster convergence compared with state-of-the-art methods. The test AUC for the\nbinary tumor classification can be up to 93.09% over CAMELYON16 dataset. And\nthe AUC over the cancer subtypes classification can be up to 96.03% and 98.82%\nover TCGA-NSCLC dataset and TCGA-RCC dataset, respectively. Implementation is\navailable at: https://github.com/szc19990412/TransMIL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhuchen Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_H/0/1/0/all/0/1\">Hao Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongbing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Associating Objects with Transformers for Video Object Segmentation. (arXiv:2106.02638v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02638","description":"<p>This paper investigates how to realize better and more efficient embedding\nlearning to tackle the semi-supervised video object segmentation under\nchallenging multi-object scenarios. The state-of-the-art methods learn to\ndecode features with a single positive object and thus have to match and\nsegment each target separately under multi-object scenarios, consuming multiple\ntimes computing resources. To solve the problem, we propose an Associating\nObjects with Transformers (AOT) approach to match and decode multiple objects\nuniformly. In detail, AOT employs an identification mechanism to associate\nmultiple targets into the same high-dimensional embedding space. Thus, we can\nsimultaneously process multiple objects' matching and segmentation decoding as\nefficiently as processing a single object. For sufficiently modeling\nmulti-object association, a Long Short-Term Transformer is designed for\nconstructing hierarchical matching and propagation. We conduct extensive\nexperiments on both multi-object and single-object benchmarks to examine AOT\nvariant networks with different complexities. Particularly, our R50-AOT-L\noutperforms all the state-of-the-art competitors on three popular benchmarks,\ni.e., YouTube-VOS (84.1% J&amp;F), DAVIS 2017 (84.9%), and DAVIS 2016 (91.1%),\nwhile keeping more than $3\\times$ faster multi-object run-time. Meanwhile, our\nAOT-T can maintain real-time multi-object speed on the above benchmarks. Based\non AOT, we ranked 1st in the 3rd Large-scale VOS Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DISCO: accurate Discrete Scale Convolutions. (arXiv:2106.02733v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02733","description":"<p>Scale is often seen as a given, disturbing factor in many vision tasks. When\ndoing so it is one of the factors why we need more data during learning. In\nrecent work scale equivariance was added to convolutional neural networks. It\nwas shown to be effective for a range of tasks. We aim for accurate\nscale-equivariant convolutional neural networks (SE-CNNs) applicable for\nproblems where high granularity of scale and small kernel sizes are required.\nCurrent SE-CNNs rely on weight sharing and kernel rescaling, the latter of\nwhich is accurate for integer scales only. To reach accurate scale\nequivariance, we derive general constraints under which scale-convolution\nremains equivariant to discrete rescaling. We find the exact solution for all\ncases where it exists, and compute the approximation for the rest. The discrete\nscale-convolution pays off, as demonstrated in a new state-of-the-art\nclassification on MNIST-scale and on STL-10 in the supervised learning setting.\nWith the same SE scheme, we also improve the computational effort of a\nscale-equivariant Siamese tracker on OTB-13.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sosnovik_I/0/1/0/all/0/1\">Ivan Sosnovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moskalev_A/0/1/0/all/0/1\">Artem Moskalev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smeulders_A/0/1/0/all/0/1\">Arnold Smeulders</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence. (arXiv:2106.03743v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.03743","description":"<p>We investigate the reasons for the performance degradation incurred with\nbatch-independent normalization. We find that the prototypical techniques of\nlayer normalization and instance normalization both induce the appearance of\nfailure modes in the neural network's pre-activations: (i) layer normalization\ninduces a collapse towards channel-wise constant functions; (ii) instance\nnormalization induces a lack of variability in instance statistics, symptomatic\nof an alteration of the expressivity. To alleviate failure mode (i) without\naggravating failure mode (ii), we introduce the technique \"Proxy Normalization\"\nthat normalizes post-activations using a proxy distribution. When combined with\nlayer normalization or group normalization, this batch-independent\nnormalization emulates batch normalization's behavior and consistently matches\nor exceeds its performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Labatie_A/0/1/0/all/0/1\">Antoine Labatie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masters_D/0/1/0/all/0/1\">Dominic Masters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eaton_Rosen_Z/0/1/0/all/0/1\">Zach Eaton-Rosen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1\">Carlo Luschi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethink Transfer Learning in Medical Image Classification. (arXiv:2106.05152v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.05152","description":"<p>Transfer learning (TL) with deep convolutional neural networks (DCNNs) is\ncrucial for modern medical image classification (MIC). However, the current\npractice of finetuning the entire pretrained model is puzzling, as most MIC\ntasks rely only on low- to mid-level features that are learned by up to mid\nlayers of DCNNs. To resolve the puzzle, we perform careful empirical\ncomparisons of several existing deep and shallow models, and propose a novel\ntruncated TL method that consistently leads to comparable or superior\nperformance and compact models on two MIC tasks. Our results highlight the\nimportance of transferring the right level of pretrained visual features\ncommensurate with the intrinsic complexity of the task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1\">Le Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_H/0/1/0/all/0/1\">Hengyue Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_G/0/1/0/all/0/1\">Gaoxiang Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1\">Taihui Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Ju Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Facet Clustering Variational Autoencoders. (arXiv:2106.05241v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2106.05241","description":"<p>Work in deep clustering focuses on finding a single partition of data.\nHowever, high-dimensional data, such as images, typically feature multiple\ninteresting characteristics one could cluster over. For example, images of\nobjects against a background could be clustered over the shape of the object\nand separately by the colour of the background. In this paper, we introduce\nMulti-Facet Clustering Variational Autoencoders (MFCVAE), a novel class of\nvariational autoencoders with a hierarchy of latent variables, each with a\nMixture-of-Gaussians prior, that learns multiple clusterings simultaneously,\nand is trained fully unsupervised and end-to-end. MFCVAE uses a\nprogressively-trained ladder architecture which leads to highly stable\nperformance. We provide novel theoretical results for optimising the ELBO\nanalytically with respect to the categorical variational posterior\ndistribution, correcting earlier influential theoretical work. On image\nbenchmarks, we demonstrate that our approach separates out and clusters over\ndifferent aspects of the data in a disentangled manner. We also show other\nadvantages of our model: the compositionality of its latent space and that it\nprovides controlled generation of samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Falck_F/0/1/0/all/0/1\">Fabian Falck</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1\">Haoting Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Willetts_M/0/1/0/all/0/1\">Matthew Willetts</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nicholson_G/0/1/0/all/0/1\">George Nicholson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yau_C/0/1/0/all/0/1\">Christopher Yau</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Holmes_C/0/1/0/all/0/1\">Chris Holmes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physics-Aware Downsampling with Deep Learning for Scalable Flood Modeling. (arXiv:2106.07218v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.07218","description":"<p>Background: Floods are the most common natural disaster in the world,\naffecting the lives of hundreds of millions. Flood forecasting is therefore a\nvitally important endeavor, typically achieved using physical water flow\nsimulations, which rely on accurate terrain elevation maps. However, such\nsimulations, based on solving partial differential equations, are\ncomputationally prohibitive on a large scale. This scalability issue is\ncommonly alleviated using a coarse grid representation of the elevation map,\nthough this representation may distort crucial terrain details, leading to\nsignificant inaccuracies in the simulation. Contributions: We train a deep\nneural network to perform physics-informed downsampling of the terrain map: we\noptimize the coarse grid representation of the terrain maps, so that the flood\nprediction will match the fine grid solution. For the learning process to\nsucceed, we configure a dataset specifically for this task. We demonstrate that\nwith this method, it is possible to achieve a significant reduction in\ncomputational cost, while maintaining an accurate solution. A reference\nimplementation accompanies the paper as well as documentation and code for\ndataset reproduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giladi_N/0/1/0/all/0/1\">Niv Giladi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Haim_Z/0/1/0/all/0/1\">Zvika Ben-Haim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nevo_S/0/1/0/all/0/1\">Sella Nevo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1\">Yossi Matias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soudry_D/0/1/0/all/0/1\">Daniel Soudry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Distillation Network for Cross-Domain Few-Shot Recognition with Unlabeled Data. (arXiv:2106.07807v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.07807","description":"<p>Most existing works in few-shot learning rely on meta-learning the network on\na large base dataset which is typically from the same domain as the target\ndataset. We tackle the problem of cross-domain few-shot learning where there is\na large shift between the base and target domain. The problem of cross-domain\nfew-shot recognition with unlabeled target data is largely unaddressed in the\nliterature. STARTUP was the first method that tackles this problem using\nself-training. However, it uses a fixed teacher pretrained on a labeled base\ndataset to create soft labels for the unlabeled target samples. As the base\ndataset and unlabeled dataset are from different domains, projecting the target\nimages in the class-domain of the base dataset with a fixed pretrained model\nmight be sub-optimal. We propose a simple dynamic distillation-based approach\nto facilitate unlabeled images from the novel/base dataset. We impose\nconsistency regularization by calculating predictions from the weakly-augmented\nversions of the unlabeled images from a teacher network and matching it with\nthe strongly augmented versions of the same images from a student network. The\nparameters of the teacher network are updated as exponential moving average of\nthe parameters of the student network. We show that the proposed network learns\nrepresentation that can be easily adapted to the target domain even though it\nhas not been trained with target-specific classes during the pretraining phase.\nOur model outperforms the current state-of-the art method by 4.4% for 1-shot\nand 3.6% for 5-shot classification in the BSCD-FSL benchmark, and also shows\ncompetitive performance on traditional in-domain few-shot learning task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_A/0/1/0/all/0/1\">Ashraful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun-Fu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1\">Leonid Karlinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radke_R/0/1/0/all/0/1\">Richard J. Radke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Language Navigation with Random Environmental Mixup. (arXiv:2106.07876v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.07876","description":"<p>Vision-language Navigation (VLN) tasks require an agent to navigate\nstep-by-step while perceiving the visual observations and comprehending a\nnatural language instruction. Large data bias, which is caused by the disparity\nratio between the small data scale and large navigation space, makes the VLN\ntask challenging. Previous works have proposed various data augmentation\nmethods to reduce data bias. However, these works do not explicitly reduce the\ndata bias across different house scenes. Therefore, the agent would overfit to\nthe seen scenes and achieve poor navigation performance in the unseen scenes.\nTo tackle this problem, we propose the Random Environmental Mixup (REM) method,\nwhich generates cross-connected house scenes as augmented data via mixuping\nenvironment. Specifically, we first select key viewpoints according to the room\nconnection graph for each scene. Then, we cross-connect the key views of\ndifferent scenes to construct augmented scenes. Finally, we generate augmented\ninstruction-path pairs in the cross-connected scenes. The experimental results\non benchmark datasets demonstrate that our augmentation data via REM help the\nagent reduce its performance gap between the seen and unseen environment and\nimprove the overall performance, making our model the best existing approach on\nthe standard VLN benchmark. The code have released:\nhttps://github.com/LCFractal/VLNREM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengda Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yi-Dong Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v3 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2106.08208","description":"<p>Adaptive gradient methods have shown excellent performances for solving many\nmachine learning problems. Although multiple adaptive methods were recently\nstudied, they mainly focus on either empirical or theoretical aspects and also\nonly work for specific problems by using some specific adaptive learning rates.\nIt is desired to design a universal framework for practical algorithms of\nadaptive gradients with theoretical guarantee to solve general problems. To\nfill this gap, we propose a faster and universal framework of adaptive\ngradients (\\emph{i.e.}, SUPER-ADAM) by introducing a universal adaptive matrix\nthat includes most existing adaptive gradient forms. Moreover, our framework\ncan flexibly integrate the momentum and variance reduced techniques. In\nparticular, our novel framework provides the convergence analysis support for\nadaptive gradient methods under the nonconvex setting. In theoretical analysis,\nwe prove that our SUPER-ADAM algorithm can achieve the best known complexity of\n$\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point of\nnonconvex optimization, which matches the lower bound for stochastic smooth\nnonconvex optimization. In numerical experiments, we employ various deep\nlearning tasks to validate that our algorithm consistently outperforms the\nexisting adaptive algorithms. Code is available at\nhttps://github.com/LIJUNYI95/SuperAdam\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-Net: Towards Unified Image Segmentation. (arXiv:2106.14855v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.14855","description":"<p>Semantic, instance, and panoptic segmentations have been addressed using\ndifferent and specialized frameworks despite their underlying connections. This\npaper presents a unified, simple, and effective framework for these essentially\nsimilar tasks. The framework, named K-Net, segments both instances and semantic\ncategories consistently by a group of learnable kernels, where each kernel is\nresponsible for generating a mask for either a potential instance or a stuff\nclass. To remedy the difficulties of distinguishing various instances, we\npropose a kernel update strategy that enables each kernel dynamic and\nconditional on its meaningful group in the input image. K-Net can be trained in\nan end-to-end manner with bipartite matching, and its training and inference\nare naturally NMS-free and box-free. Without bells and whistles, K-Net\nsurpasses all previous published state-of-the-art single-model results of\npanoptic segmentation on MS COCO test-dev split and semantic segmentation on\nADE20K val split with 55.2% PQ and 54.3% mIoU, respectively. Its instance\nsegmentation performance is also on par with Cascade Mask R-CNN on MS COCO with\n60%-90% faster inference speeds. Code and models will be released at\nhttps://github.com/ZwwWayne/K-Net/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiangmiao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Synthetic Training Data for Deep Learning-Based UAV Trajectory Prediction. (arXiv:2107.00422v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00422","description":"<p>Deep learning-based models, such as recurrent neural networks (RNNs), have\nbeen applied to various sequence learning tasks with great success. Following\nthis, these models are increasingly replacing classic approaches in object\ntracking applications for motion prediction. On the one hand, these models can\ncapture complex object dynamics with less modeling required, but on the other\nhand, they depend on a large amount of training data for parameter tuning.\nTowards this end, we present an approach for generating synthetic trajectory\ndata of unmanned-aerial-vehicles (UAVs) in image space. Since UAVs, or rather\nquadrotors are dynamical systems, they can not follow arbitrary trajectories.\nWith the prerequisite that UAV trajectories fulfill a smoothness criterion\ncorresponding to a minimal change of higher-order motion, methods for planning\naggressive quadrotors flights can be utilized to generate optimal trajectories\nthrough a sequence of 3D waypoints. By projecting these maneuver trajectories,\nwhich are suitable for controlling quadrotors, to image space, a versatile\ntrajectory data set is realized. To demonstrate the applicability of the\nsynthetic trajectory data, we show that an RNN-based prediction model solely\ntrained on the generated data can outperform classic reference models on a\nreal-world UAV tracking dataset. The evaluation is done on the publicly\navailable ANTI-UAV dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1\">Stefan Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hug_R/0/1/0/all/0/1\">Ronny Hug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubner_W/0/1/0/all/0/1\">Wolfgang H&#xfc;bner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1\">Michael Arens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_B/0/1/0/all/0/1\">Brendan T. Morris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Per-Pixel Classification is Not All You Need for Semantic Segmentation. (arXiv:2107.06278v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06278","description":"<p>Modern approaches typically formulate semantic segmentation as a per-pixel\nclassification task, while instance-level segmentation is handled with an\nalternative mask classification. Our key insight: mask classification is\nsufficiently general to solve both semantic- and instance-level segmentation\ntasks in a unified manner using the exact same model, loss, and training\nprocedure. Following this observation, we propose MaskFormer, a simple mask\nclassification model which predicts a set of binary masks, each associated with\na single global class label prediction. Overall, the proposed mask\nclassification-based method simplifies the landscape of effective approaches to\nsemantic and panoptic segmentation tasks and shows excellent empirical results.\nIn particular, we observe that MaskFormer outperforms per-pixel classification\nbaselines when the number of classes is large. Our mask classification-based\nmethod outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K)\nand panoptic segmentation (52.7 PQ on COCO) models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Bowen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander G. Schwing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirillov_A/0/1/0/all/0/1\">Alexander Kirillov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Passive Attention in Artificial Neural Networks Predicts Human Visual Selectivity. (arXiv:2107.07013v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.07013","description":"<p>Developments in machine learning interpretability techniques over the past\ndecade have provided new tools to observe the image regions that are most\ninformative for classification and localization in artificial neural networks\n(ANNs). Are the same regions similarly informative to human observers? Using\ndata from 79 new experiments and 7,810 participants, we show that passive\nattention techniques reveal a significant overlap with human visual selectivity\nestimates derived from 6 distinct behavioral tasks including visual\ndiscrimination, spatial localization, recognizability, free-viewing,\ncued-object search, and saliency search fixations. We find that input\nvisualizations derived from relatively simple ANN architectures probed using\nguided backpropagation methods are the best predictors of a shared component in\nthe joint variability of the human measures. We validate these correlational\nresults with causal manipulations using recognition experiments. We show that\nimages masked with ANN attention maps were easier for humans to classify than\ncontrol masks in a speeded recognition experiment. Similarly, we find that\nrecognition performance in the same ANN models was likewise influenced by\nmasking input images using human visual selectivity maps. This work contributes\na new approach to evaluating the biological and psychological validity of\nleading ANNs as models of human vision: by examining their similarities and\ndifferences in terms of their visual selectivity to the information contained\nin images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Langlois_T/0/1/0/all/0/1\">Thomas A. Langlois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">H. Charles Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grant_E/0/1/0/all/0/1\">Erin Grant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1\">Ishita Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1\">Nori Jacoby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rectifying the Shortcut Learning of Background for Few-Shot Learning. (arXiv:2107.07746v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.07746","description":"<p>The category gap between training and evaluation has been characterised as\none of the main obstacles to the success of Few-Shot Learning (FSL). In this\npaper, we for the first time empirically identify image background, common in\nrealistic images, as a shortcut knowledge helpful for in-class classification\nbut ungeneralizable beyond training categories in FSL. A novel framework,\nCOSOC, is designed to tackle this problem by extracting foreground objects in\nimages at both training and evaluation without any extra supervision. Extensive\nexperiments carried on inductive FSL tasks demonstrate the effectiveness of our\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Longhui Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Liangjian Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinrong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Box-Aware Feature Enhancement for Single Object Tracking on Point Clouds. (arXiv:2108.04728v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04728","description":"<p>Current 3D single object tracking approaches track the target based on a\nfeature comparison between the target template and the search area. However,\ndue to the common occlusion in LiDAR scans, it is non-trivial to conduct\naccurate feature comparisons on severe sparse and incomplete shapes. In this\nwork, we exploit the ground truth bounding box given in the first frame as a\nstrong cue to enhance the feature description of the target object, enabling a\nmore accurate feature comparison in a simple yet effective way. In particular,\nwe first propose the BoxCloud, an informative and robust representation, to\ndepict an object using the point-to-box relation. We further design an\nefficient box-aware feature fusion module, which leverages the aforementioned\nBoxCloud for reliable feature matching and embedding. Integrating the proposed\ngeneral components into an existing model P2B, we construct a superior\nbox-aware tracker (BAT). Experiments confirm that our proposed BAT outperforms\nthe previous state-of-the-art by a large margin on both KITTI and NuScenes\nbenchmarks, achieving a 15.2% improvement in terms of precision while running\n~20% faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chaoda Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiantao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weibing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Neural Network (CNN) vs Vision Transformer (ViT) for Digital Holography. (arXiv:2108.09147v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09147","description":"<p>In Digital Holography (DH), it is crucial to extract the object distance from\na hologram in order to reconstruct its amplitude and phase. This step is called\nauto-focusing and it is conventionally solved by first reconstructing a stack\nof images and then by sharpening each reconstructed image using a focus metric\nsuch as entropy or variance. The distance corresponding to the sharpest image\nis considered the focal position. This approach, while effective, is\ncomputationally demanding and time-consuming. In this paper, the determination\nof the distance is performed by Deep Learning (DL). Two deep learning (DL)\narchitectures are compared: Convolutional Neural Network (CNN) and Vision\nTransformer (ViT). ViT and CNN are used to cope with the problem of\nauto-focusing as a classification problem. Compared to a first attempt [11] in\nwhich the distance between two consecutive classes was 100$\\mu$m, our proposal\nallows us to drastically reduce this distance to 1$\\mu$m. Moreover, ViT reaches\nsimilar accuracy and is more robust than CNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cuenat_S/0/1/0/all/0/1\">St&#xe9;phane Cuenat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couturier_R/0/1/0/all/0/1\">Rapha&#xeb;l Couturier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AP-10K: A Benchmark for Animal Pose Estimation in the Wild. (arXiv:2108.12617v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12617","description":"<p>Accurate animal pose estimation is an essential step towards understanding\nanimal behavior, and can potentially benefit many downstream applications, such\nas wildlife conservation. Previous works only focus on specific animals while\nignoring the diversity of animal species, limiting the generalization ability.\nIn this paper, we propose AP-10K, the first large-scale benchmark for mammal\nanimal pose estimation, to facilitate the research in animal pose estimation.\nAP-10K consists of 10,015 images collected and filtered from 23 animal families\nand 54 species following the taxonomic rank and high-quality keypoint\nannotations labeled and checked manually. Based on AP-10K, we benchmark\nrepresentative pose estimation models on the following three tracks: (1)\nsupervised learning for animal pose estimation, (2) cross-domain transfer\nlearning from human pose estimation to animal pose estimation, and (3) intra-\nand inter-family domain generalization for unseen animals. The experimental\nresults provide sound empirical evidence on the superiority of learning from\ndiverse animals species in terms of both accuracy and generalization ability.\nIt opens new directions for facilitating future research in animal pose\nestimation. AP-10k is publicly available at\nhttps://github.com/AlexTheBad/AP10K.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yufei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1\">Ziyu Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mesh convolutional neural networks for wall shear stress estimation in 3D artery models. (arXiv:2109.04797v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.04797","description":"<p>Computational fluid dynamics (CFD) is a valuable tool for personalised,\nnon-invasive evaluation of hemodynamics in arteries, but its complexity and\ntime-consuming nature prohibit large-scale use in practice. Recently, the use\nof deep learning for rapid estimation of CFD parameters like wall shear stress\n(WSS) on surface meshes has been investigated. However, existing approaches\ntypically depend on a hand-crafted re-parametrisation of the surface mesh to\nmatch convolutional neural network architectures. In this work, we propose to\ninstead use mesh convolutional neural networks that directly operate on the\nsame finite-element surface mesh as used in CFD. We train and evaluate our\nmethod on two datasets of synthetic coronary artery models with and without\nbifurcation, using a ground truth obtained from CFD simulation. We show that\nour flexible deep learning model can accurately predict 3D WSS vectors on this\nsurface mesh. Our method processes new meshes in less than 5 [s], consistently\nachieves a normalised mean absolute error of $\\leq$ 1.6 [%], and peaks at 90.5\n[%] median approximation accuracy over the held-out test set, comparing\nfavourably to previously published work. This demonstrates the feasibility of\nCFD surrogate modelling using mesh convolutional neural networks for\nhemodynamic parameter estimation in artery models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suk_J/0/1/0/all/0/1\">Julian Suk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haan_P/0/1/0/all/0/1\">Pim de Haan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippe_P/0/1/0/all/0/1\">Phillip Lippe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brune_C/0/1/0/all/0/1\">Christoph Brune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolterink_J/0/1/0/all/0/1\">Jelmer M. Wolterink</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSOR: A Scalable Statistical Filter for Removing Falling Snow from LiDAR Point Clouds in Severe Winter Weather. (arXiv:2109.07078v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07078","description":"<p>For autonomous vehicles to viably replace human drivers they must contend\nwith inclement weather. Falling rain and snow introduce noise in LiDAR returns\nresulting in both false positive and false negative object detections. In this\narticle we introduce the Winter Adverse Driving dataSet (WADS) collected in the\nsnow belt region of Michigan's Upper Peninsula. WADS is the first multi-modal\ndataset featuring dense point-wise labeled sequential LiDAR scans collected in\nsevere winter weather; weather that would cause an experienced driver to alter\ntheir driving behavior. We have labelled and will make available over 7 GB or\n3.6 billion labelled LiDAR points out of over 26 TB of total LiDAR and camera\ndata collected. We also present the Dynamic Statistical Outlier Removal (DSOR)\nfilter, a statistical PCL-based filter capable or removing snow with a higher\nrecall than the state of the art snow de-noising filter while being 28\\%\nfaster. Further, the DSOR filter is shown to have a lower time complexity\ncompared to the state of the art resulting in an improved scalability.\n</p>\n<p>Our labeled dataset and DSOR filter will be made available at\nhttps://bitbucket.org/autonomymtu/dsor_filter\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurup_A/0/1/0/all/0/1\">Akhil Kurup</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bos_J/0/1/0/all/0/1\">Jeremy Bos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learnable Multi-level Frequency Decomposition and Hierarchical Attention Mechanism for Generalized Face Presentation Attack Detection. (arXiv:2109.07950v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07950","description":"<p>With the increased deployment of face recognition systems in our daily lives,\nface presentation attack detection (PAD) is attracting a lot of attention and\nplaying a key role in securing face recognition systems. Despite the great\nperformance achieved by the hand-crafted and deep learning based methods in\nintra-dataset evaluations, the performance drops when dealing with unseen\nscenarios. In this work, we propose a dual-stream convolution neural networks\n(CNNs) framework. One stream adapts four learnable frequency filters to learn\nfeatures in the frequency domain, which are less influenced variations in\nsensors/illuminations. The other stream leverage the RGB images to complement\nthe features of the frequency domain. Moreover, we propose a hierarchical\nattention module integration to join the information from the two streams at\ndifferent stages by considering the nature of deep features in different layers\nof the CNN. The proposed method is evaluated in the intra-dataset and\ncross-dataset setups and the results demonstrates that our proposed approach\nenhances the generalizability in most experimental setups in comparison to\nstate-of-the-art, including the methods designed explicitly for domain\nadaption/shift problem. We successfully prove the design of our proposed PAD\nsolution in a step-wise ablation study that involves our proposed learnable\nfrequency decomposition, our hierarchical attention module design, and the used\nloss function. Training codes and pre-trained models are publicly released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meiling Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auditing AI models for Verified Deployment under Semantic Specifications. (arXiv:2109.12456v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.12456","description":"<p>Auditing trained deep learning (DL) models prior to deployment is vital for\npreventing unintended consequences. One of the biggest challenges in auditing\nis the lack of human-interpretable specifications for the DL models that are\ndirectly useful to the auditor. We address this challenge through a sequence of\nsemantically-aligned unit tests, where each unit test verifies whether a\npredefined specification (e.g., accuracy over 95%) is satisfied with respect to\ncontrolled and semantically aligned variations in the input space (e.g., in\nface recognition, the angle relative to the camera). We enable such unit tests\nthrough variations in a semantically-interpretable latent space of a generative\nmodel. Further, we conduct certified training for the DL model through a shared\nlatent space representation with the generative model. With evaluations on four\ndifferent datasets, covering images of chest X-rays, human faces, ImageNet\nclasses, and towers, we show how AuditAI allows us to obtain controlled\nvariations for certified training. Thus, our framework, AuditAI, bridges the\ngap between semantically-aligned formal verification and scalability. A blog\npost accompanying the paper is at this link\nhttps://developer.nvidia.com/blog/nvidia-research-auditing-ai-models-for-verified-deployment-under-semantic-specifications\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bharadhwaj_H/0/1/0/all/0/1\">Homanga Bharadhwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">De-An Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Animesh Garg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsolved Problems in ML Safety. (arXiv:2109.13916v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.13916","description":"<p>Machine learning (ML) systems are rapidly increasing in size, are acquiring\nnew capabilities, and are increasingly deployed in high-stakes settings. As\nwith other powerful technologies, safety for ML should be a leading research\npriority. In response to emerging safety challenges in ML, such as those\nintroduced by recent large-scale models, we provide a new roadmap for ML Safety\nand refine the technical problems that the field needs to address. We present\nfour problems ready for research, namely withstanding hazards (\"Robustness\"),\nidentifying hazards (\"Monitoring\"), steering ML systems (\"Alignment\"), and\nreducing hazards in deployment (\"External Safety\"). Throughout, we clarify each\nproblem's motivation and provide concrete research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulman_J/0/1/0/all/0/1\">John Schulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Velocity Mapping Cardiac MRI Coupled with Automated Left Ventricle Segmentation. (arXiv:2110.01304v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.01304","description":"<p>Temporal patterns of cardiac motion provide important information for cardiac\ndisease diagnosis. This pattern could be obtained by three-directional CINE\nmulti-slice left ventricular myocardial velocity mapping (3Dir MVM), which is a\ncardiac MR technique providing magnitude and phase information of the\nmyocardial motion simultaneously. However, long acquisition time limits the\nusage of this technique by causing breathing artifacts, while shortening the\ntime causes low temporal resolution and may provide an inaccurate assessment of\ncardiac motion. In this study, we proposed a frame synthesis algorithm to\nincrease the temporal resolution of 3Dir MVM data. Our algorithm is featured by\n1) three attention-based encoders which accept magnitude images, phase images,\nand myocardium segmentation masks respectively as inputs; 2) three decoders\nthat output the interpolated frames and corresponding myocardium segmentation\nresults; and 3) loss functions highlighting myocardium pixels. Our algorithm\ncan not only increase the temporal resolution 3Dir MVMs, but can also generates\nthe myocardium segmentation results at the same time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xing_X/0/1/0/all/0/1\">Xiaodan Xing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yinzhe Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Firmin_D/0/1/0/all/0/1\">David Firmin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gatehouse_P/0/1/0/all/0/1\">Peter Gatehouse</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Out-of-the-Box Frameworks for Contrastive Unpaired Image Translation for Vestibular Schwannoma and Cochlea Segmentation: An approach for the crossMoDA Challenge. (arXiv:2110.01607v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.01607","description":"<p>The purpose of this study is to apply and evaluate out-of-the-box deep\nlearning frameworks for the crossMoDA challenge. We use the CUT model for\ndomain adaptation from contrast-enhanced T1 MR to high-resolution T2 MR. As\ndata augmentation, we generated additional images with vestibular schwannomas\nwith lower signal intensity. For the segmentation task, we use the nnU-Net\nframework. Our final submission achieved mean Dice scores of 0.8299 in the\nvalidation phase and 0.8253 in the test phase. Our method ranked 3rd in the\ncrossMoDA challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Choi_J/0/1/0/all/0/1\">Jae Won Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data. (arXiv:2110.03374v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03374","description":"<p>Unsupervised domain adaptation aims to align a labeled source domain and an\nunlabeled target domain, but it requires to access the source data which often\nraises concerns in data privacy, data portability and data transmission\nefficiency. We study unsupervised model adaptation (UMA), or called\nUnsupervised Domain Adaptation without Source Data, an alternative setting that\naims to adapt source-trained models towards target distributions without\naccessing source data. To this end, we design an innovative historical\ncontrastive learning (HCL) technique that exploits historical source hypothesis\nto make up for the absence of source data in UMA. HCL addresses the UMA\nchallenge from two perspectives. First, it introduces historical contrastive\ninstance discrimination (HCID) that learns from target samples by contrasting\ntheir embeddings which are generated by the currently adapted model and the\nhistorical models. With the historical models, HCID encourages UMA to learn\ninstance-discriminative target representations while preserving the source\nhypothesis. Second, it introduces historical contrastive category\ndiscrimination (HCCD) that pseudo-labels target samples to learn\ncategory-discriminative target representations. Specifically, HCCD re-weights\npseudo labels according to their prediction consistency across the current and\nhistorical models. Extensive experiments show that HCL outperforms and\nstate-of-the-art methods consistently across a variety of visual tasks and\nsetups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Dayan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"6D-ViT: Category-Level 6D Object Pose Estimation via Transformer-based Instance Representation Learning. (arXiv:2110.04792v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04792","description":"<p>This paper presents 6D-ViT, a transformer-based instance representation\nlearning network, which is suitable for highly accurate category-level object\npose estimation on RGB-D images. Specifically, a novel two-stream\nencoder-decoder framework is dedicated to exploring complex and powerful\ninstance representations from RGB images, point clouds and categorical shape\npriors. For this purpose, the whole framework consists of two main branches,\nnamed Pixelformer and Pointformer. The Pixelformer contains a pyramid\ntransformer encoder with an all-MLP decoder to extract pixelwise appearance\nrepresentations from RGB images, while the Pointformer relies on a cascaded\ntransformer encoder and an all-MLP decoder to acquire the pointwise geometric\ncharacteristics from point clouds. Then, dense instance representations (i.e.,\ncorrespondence matrix, deformation field) are obtained from a multi-source\naggregation network with shape priors, appearance and geometric information as\ninput. Finally, the instance 6D pose is computed by leveraging the\ncorrespondence among dense representations, shape priors, and the instance\npoint clouds. Extensive experiments on both synthetic and real-world datasets\ndemonstrate that the proposed 3D instance representation learning framework\nachieves state-of-the-art performance on both datasets, and significantly\noutperforms all existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Lu Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhangjin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1\">Naijie Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMIU: Dataset for Visual Intent Understanding in Multimodal Assistants. (arXiv:2110.06416v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06416","description":"<p>In multimodal assistant, where vision is also one of the input modalities,\nthe identification of user intent becomes a challenging task as visual input\ncan influence the outcome. Current digital assistants take spoken input and try\nto determine the user intent from conversational or device context. So, a\ndataset, which includes visual input (i.e. images or videos for the\ncorresponding questions targeted for multimodal assistant use cases, is not\nreadily available. The research in visual question answering (VQA) and visual\nquestion generation (VQG) is a great step forward. However, they do not capture\nquestions that a visually-abled person would ask multimodal assistants.\nMoreover, many times questions do not seek information from external knowledge.\nIn this paper, we provide a new dataset, MMIU (MultiModal Intent\nUnderstanding), that contains questions and corresponding intents provided by\nhuman annotators while looking at images. We, then, use this dataset for intent\nclassification task in multimodal digital assistant. We also experiment with\nvarious approaches for combining vision and language features including the use\nof multimodal transformer for classification of image-question pairs into 14\nintents. We provide the benchmark results and discuss the role of visual and\ntext features for the intent classification task on our dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Alkesh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1\">Joel Ruben Antony Moniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_R/0/1/0/all/0/1\">Roman Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzou_N/0/1/0/all/0/1\">Nick Tzou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotek_H/0/1/0/all/0/1\">Hadas Kotek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renkens_V/0/1/0/all/0/1\">Vincent Renkens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Deep Neural Networks with Joint Quantization and Pruning of Weights and Activations. (arXiv:2110.08271v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.08271","description":"<p>Quantization and pruning are core techniques used to reduce the inference\ncosts of deep neural networks. State-of-the-art quantization techniques are\ncurrently applied to both the weights and activations; however, pruning is most\noften applied to only the weights of the network. In this work, we jointly\napply novel uniform quantization and unstructured pruning methods to both the\nweights and activations of deep neural networks during training. Using our\nmethods, we empirically evaluate the currently accepted prune-then-quantize\nparadigm across a wide range of computer vision tasks and observe a\nnon-commutative nature when applied to both the weights and activations of deep\nneural networks. Informed by these observations, we articulate the\nnon-commutativity hypothesis: for a given deep neural network being trained for\na specific task, there exists an exact training schedule in which quantization\nand pruning can be introduced to optimize network performance. We identify that\nthis optimal ordering not only exists, but also varies across discriminative\nand generative tasks. Using the optimal training schedule within our training\nframework, we demonstrate increased performance per memory footprint over\nexisting solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colbert_I/0/1/0/all/0/1\">Ian Colbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutz_Delgado_K/0/1/0/all/0/1\">Ken Kreutz-Delgado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srinjoy Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salt and pepper noise removal method based on stationary Framelet transform with non-convex sparsity regularization. (arXiv:2110.09113v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.09113","description":"<p>Salt and pepper noise removal is a common inverse problem in image\nprocessing. Traditional denoising methods have two limitations. First, noise\ncharacteristics are often not described accurately. For example, the noise\nlocation information is often ignored and the sparsity of the salt and pepper\nnoise is often described by L1 norm, which cannot illustrate the sparse\nvariables clearly. Second, conventional methods separate the contaminated image\ninto a recovered image and a noise part, thus resulting in recovering an image\nwith unsatisfied smooth parts and detail parts. In this study, we introduce a\nnoise detection strategy to determine the position of the noise, and a\nnon-convex sparsity regularization depicted by Lp quasi-norm is employed to\ndescribe the sparsity of the noise, thereby addressing the first limitation.\nThe morphological component analysis framework with stationary Framelet\ntransform is adopted to decompose the processed image into cartoon, texture,\nand noise parts to resolve the second limitation. Then, the alternating\ndirection method of multipliers (ADMM) is employed to solve the proposed model.\nFinally, experiments are conducted to verify the proposed method and compare it\nwith some current state-of-the-art denoising methods. The experimental results\nshow that the proposed method can remove salt and pepper noise while preserving\nthe details of the processed image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yingpin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yuming Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lingzhi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Huiying Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1\">Jianhua Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_C/0/1/0/all/0/1\">Chaoqun Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanping Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning in High Dimension Always Amounts to Extrapolation. (arXiv:2110.09485v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.09485","description":"<p>The notion of interpolation and extrapolation is fundamental in various\nfields from deep learning to function approximation. Interpolation occurs for a\nsample $x$ whenever this sample falls inside or on the boundary of the given\ndataset's convex hull. Extrapolation occurs when $x$ falls outside of that\nconvex hull. One fundamental (mis)conception is that state-of-the-art\nalgorithms work so well because of their ability to correctly interpolate\ntraining data. A second (mis)conception is that interpolation happens\nthroughout tasks and datasets, in fact, many intuitions and theories rely on\nthat assumption. We empirically and theoretically argue against those two\npoints and demonstrate that on any high-dimensional ($&gt;$100) dataset,\ninterpolation almost surely never happens. Those results challenge the validity\nof our current interpolation/extrapolation definition as an indicator of\ngeneralization performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1\">Randall Balestriero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pesenti_J/0/1/0/all/0/1\">Jerome Pesenti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOS: A Low Latency and Lightweight Framework for Face Detection, Landmark Localization, and Head Pose Estimation. (arXiv:2110.10953v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.10953","description":"<p>With the emergence of service robots and surveillance cameras, dynamic face\nrecognition (DFR) in wild has received much attention in recent years. Face\ndetection and head pose estimation are two important steps for DFR. Very often,\nthe pose is estimated after the face detection. However, such sequential\ncomputations lead to higher latency. In this paper, we propose a low latency\nand lightweight network for simultaneous face detection, landmark localization\nand head pose estimation. Inspired by the observation that it is more\nchallenging to locate the facial landmarks for faces with large angles, a pose\nloss is proposed to constrain the learning. Moreover, we also propose an\nuncertainty multi-task loss to learn the weights of individual tasks\nautomatically. Another challenge is that robots often use low computational\nunits like ARM based computing core and we often need to use lightweight\nnetworks instead of the heavy ones, which lead to performance drop especially\nfor small and hard faces. In this paper, we propose online feedback sampling to\naugment the training samples across different scales, which increases the\ndiversity of training data automatically. Through validation in commonly used\nWIDER FACE, AFLW and AFLW2000 datasets, the results show that the proposed\nmethod achieves the state-of-the-art performance in low computational\nresources. The code and data will be available at\nhttps://github.com/lyp-deeplearning/MOS-Multi-Task-Face-Detect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yepeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zaiwang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shenghua Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yusheng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effect of Wearing a Face Mask on Face Image Quality. (arXiv:2110.11283v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11283","description":"<p>Due to the COVID-19 situation, face masks have become a main part of our\ndaily life. Wearing mouth-and-nose protection has been made a mandate in many\npublic places, to prevent the spread of the COVID-19 virus. However, face masks\naffect the performance of face recognition, since a large area of the face is\ncovered. The effect of wearing a face mask on the different components of the\nface recognition system in a collaborative environment is a problem that is\nstill to be fully studied. This work studies, for the first time, the effect of\nwearing a face mask on face image quality by utilising state-of-the-art face\nimage quality assessment methods of different natures. This aims at providing\nbetter understanding on the effect of face masks on the operation of face\nrecognition as a whole system. In addition, we further studied the effect of\nsimulated masks on face image utility in comparison to real face masks. We\ndiscuss the correlation between the mask effect on face image quality and that\non the face verification performance by automatic systems and human experts,\nindicating a consistent trend between both factors. The evaluation is conducted\non the database containing (1) no-masked faces, (2) real face masks, and (3)\nsimulated face masks, by synthetically generating digital facial masks on\nno-masked faces. Finally, a visual interpretation of the face areas\ncontributing to the quality score of a selected set of quality assessment\nmethods is provided to give a deeper insight into the difference of network\ndecisions in masked and non-masked faces, among other variations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1\">Biying Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parametric Variational Linear Units (PVLUs) in Deep Convolutional Networks. (arXiv:2110.12246v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12246","description":"<p>The Rectified Linear Unit is currently a state-of-the-art activation function\nin deep convolutional neural networks. To combat ReLU's dying neuron problem,\nwe propose the Parametric Variational Linear Unit (PVLU), which adds a\nsinusoidal function with trainable coefficients to ReLU. Along with introducing\nnonlinearity and non-zero gradients across the entire real domain, PVLU allows\nfor increased model generalization and robustness when implemented in the\ncontext of transfer learning. On a simple, non-transfer sequential CNN, PVLU\nallowed for a relative error decrease of 16.3% and 11.3% (without and with data\naugmentation) on CIFAR-10. PVLU is also tested on transfer learning problems.\nThe VGG-16 and VGG-19 models experience relative error reductions of 9.5% and\n10.7% on CIFAR-10, respectively, after the substitution of ReLU with PVLU. When\ntraining on Gaussian-filtered CIFAR-10 images, similar improvements are noted\nfor the VGG models. Most notably, PVLU fine tuning allows for relative error\nreductions up to and exceeding 10% on near state-of-the-art ResNet models for\nboth CIFAR-10 and CIFAR-100.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aarush Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_S/0/1/0/all/0/1\">Shikhar Ahuja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logsig-RNN: a novel network for robust and efficient skeleton-based action recognition. (arXiv:2110.13008v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13008","description":"<p>This paper contributes to the challenge of skeleton-based human action\nrecognition in videos. The key step is to develop a generic network\narchitecture to extract discriminative features for the spatio-temporal\nskeleton data. In this paper, we propose a novel module, namely Logsig-RNN,\nwhich is the combination of the log-signature layer and recurrent type neural\nnetworks (RNNs). The former one comes from the mathematically principled\ntechnology of signatures and log-signatures as representations for streamed\ndata, which can manage high sample rate streams, non-uniform sampling and time\nseries of variable length. It serves as an enhancement of the recurrent layer,\nwhich can be conveniently plugged into neural networks. Besides we propose two\npath transformation layers to significantly reduce path dimension while\nretaining the essential information fed into the Logsig-RNN module. Finally,\nnumerical results demonstrate that replacing the RNN module by the Logsig-RNN\nmodule in SOTA networks consistently improves the performance on both Chalearn\ngesture data and NTU RGB+D 120 action data in terms of accuracy and robustness.\nIn particular, we achieve the state-of-the-art accuracy on Chalearn2013 gesture\ndata by combining simple path transformation layers with the Logsig-RNN. Codes\nare available at https://github.com/steveliao93/GCN_LogsigRNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shujian Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyons_T/0/1/0/all/0/1\">Terry Lyons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Weixin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlegel_K/0/1/0/all/0/1\">Kevin Schlegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_H/0/1/0/all/0/1\">Hao Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BI-GCN: Boundary-Aware Input-Dependent Graph Convolution Network for Biomedical Image Segmentation. (arXiv:2110.14775v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.14775","description":"<p>Segmentation is an essential operation of image processing. The convolution\noperation suffers from a limited receptive field, while global modelling is\nfundamental to segmentation tasks. In this paper, we apply graph convolution\ninto the segmentation task and propose an improved \\textit{Laplacian}.\nDifferent from existing methods, our \\textit{Laplacian} is data-dependent, and\nwe introduce two attention diagonal matrices to learn a better vertex\nrelationship. In addition, it takes advantage of both region and boundary\ninformation when performing graph-based information propagation. Specifically,\nwe model and reason about the boundary-aware region-wise correlations of\ndifferent classes through learning graph representations, which is capable of\nmanipulating long range semantic reasoning across various regions with the\nspatial enhancement along the object's boundary. Our model is well-suited to\nobtain global semantic region information while also accommodates local spatial\nboundary characteristics simultaneously. Experiments on two types of\nchallenging datasets demonstrate that our method outperforms the\nstate-of-the-art approaches on the segmentation of polyps in colonoscopy images\nand of the optic disc and optic cup in colour fundus images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yanda Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongrun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dongxu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yitian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xuesheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaowei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yalin Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis. (arXiv:2110.15678v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.15678","description":"<p>The advancement of generative radiance fields has pushed the boundary of\n3D-aware image synthesis. Motivated by the observation that a 3D object should\nlook realistic from multiple viewpoints, these methods introduce a multi-view\nconstraint as regularization to learn valid 3D radiance fields from 2D images.\nDespite the progress, they often fall short of capturing accurate 3D shapes due\nto the shape-color ambiguity, limiting their applicability in downstream tasks.\nIn this work, we address this ambiguity by proposing a novel shading-guided\ngenerative implicit model that is able to learn a starkly improved shape\nrepresentation. Our key insight is that an accurate 3D shape should also yield\na realistic rendering under different lighting conditions. This multi-lighting\nconstraint is realized by modeling illumination explicitly and performing\nshading with various lighting conditions. Gradients are derived by feeding the\nsynthesized images to a discriminator. To compensate for the additional\ncomputational burden of calculating surface normals, we further devise an\nefficient volume rendering strategy via surface tracking, reducing the training\nand inference time by 24% and 48%, respectively. Our experiments on multiple\ndatasets show that the proposed approach achieves photorealistic 3D-aware image\nsynthesis while capturing accurate underlying 3D shapes. We demonstrate\nimproved performance of our approach on 3D shape reconstruction against\nexisting methods, and show its applicability on image relighting. Our code will\nbe released at https://github.com/XingangPan/ShadeGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xingang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xudong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generational Frameshifts in Technology: Computer Science and Neurosurgery, The VR Use Case. (arXiv:2110.15719v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2110.15719","description":"<p>We are at a unique moment in history where there is a confluence of\ntechnologies which will synergistically come together to transform the practice\nof neurosurgery. These technological transformations will be all-encompassing,\nincluding improved tools and methods for intraoperative performance of\nneurosurgery, scalable solutions for asynchronous neurosurgical training and\nsimulation, as well as broad aggregation of operative data allowing fundamental\nchanges in quality assessment, billing, outcome measures, and dissemination of\nsurgical best practices. The ability to perform surgery more safely and more\nefficiently while capturing the operative details and parsing each component of\nthe operation will open an entirely new epoch advancing our field and all\nsurgical specialties. The digitization of all components within the operating\nroom will allow us to leverage the various fields within computer and\ncomputational science to obtain new insights that will improve care and\ndelivery of the highest quality neurosurgery regardless of location. The\ndemocratization of neurosurgery is at hand and will be driven by our\ndevelopment, extraction, and adoption of these tools of the modern world.\nVirtual reality provides a good example of how consumer-facing technologies are\nfinding a clear role in industry and medicine and serves as a notable example\nof the confluence of various computer science technologies creating a novel\nparadigm for scaling human ability and interactions. The authors describe the\ntechnology ecosystem that has come and highlight a myriad of computational and\ndata sciences that will be necessary to enable the operating room of the near\nfuture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Browd_S/0/1/0/all/0/1\">Samuel R. Browd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Maya Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_C/0/1/0/all/0/1\">Chetan Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple Sclerosis Lesions Identification/Segmentation in Magnetic Resonance Imaging using Ensemble CNN and Uncertainty Classification. (arXiv:2108.11791v2 [eess.IV] CROSS LISTED)","link":"http://arxiv.org/abs/2108.11791","description":"<p>To date, several automated strategies for identification/segmentation of\nMultiple Sclerosis (MS) lesions with the use of Magnetic Resonance Imaging\n(MRI) have been presented but they are either outperformed by human experts or\nperform differently from them. This is mainly due to the ambiguity originated\nby MRI instabilities, peculiar variability of MS and unspecific nature of MRI\nwith respect to MS. Physicians partially manage the uncertainty generated by\nambiguity relying on their personal radiological/clinical/anatomical background\nand experience. We present an automated framework based on three pivotal\nconcepts to better emulate human reasoning: 1. the modelling of uncertainty; 2.\nthe proposal of two, separately trained, CNN, one optimized with respect to\nlesions themselves and the other to the environment surrounding lesions,\nrespectively repeated for axial, coronal and sagittal directions; 3. the\ndefinition of an ensemble classifier to merge the information collected by all\nCNN. The proposed framework is trained, validated and tested on the 2016 MSSEG\nbenchmark public data set from a single imaging modality, the FLuid-Attenuated\nInversion Recovery (FLAIR). The comparison, made with the consensus (the\nground-truth) between 7 human raters and with each of the 7 human raters,\nproves that there is no significant difference between the automated and the\nhuman raters. The results of our framework concerning the uncertainty are also\nreported, even if a comparison with the raters is impossible because they don't\nrecognize this class.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Placidi_G/0/1/0/all/0/1\">Giuseppe Placidi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cinque_L/0/1/0/all/0/1\">Luigi Cinque</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mignosi_F/0/1/0/all/0/1\">Filippo Mignosi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Polsinelli_M/0/1/0/all/0/1\">Matteo Polsinelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-11-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}