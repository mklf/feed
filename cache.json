{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-02-01T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Schema-Free Dependency Parsing via Sequence Generation. (arXiv:2201.12407v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12407","description":"<p>Dependency parsing aims to extract syntactic dependency structure or semantic\ndependency structure for sentences. Existing methods suffer the drawbacks of\nlacking universality or highly relying on the auxiliary decoder. To remedy\nthese drawbacks, we propose to achieve universal and schema-free Dependency\nParsing (DP) via Sequence Generation (SG) DPSG by utilizing only the\npre-trained language model (PLM) without any auxiliary structures or parsing\nalgorithms. We first explore different serialization designing strategies for\nconverting parsing structures into sequences. Then we design dependency units\nand concatenate these units into the sequence for DPSG. Thanks to the high\nflexibility of the sequence generation, our DPSG can achieve both syntactic DP\nand semantic DP using a single model. By concatenating the prefix to indicate\nthe specific schema with the sequence, our DPSG can even accomplish\nmulti-schemata parsing. The effectiveness of our DPSG is demonstrated by the\nexperiments on widely used DP benchmarks, i.e., PTB, CODT, SDP15, and\nSemEval16. DPSG achieves comparable results with the first-tier methods on all\nthe benchmarks and even the state-of-the-art (SOTA) performance in CODT and\nSemEval16. This paper demonstrates our DPSG has the potential to be a new\nparsing paradigm. We will release our codes upon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Boda Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zijun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiaxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shulin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Binghao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Si Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Approach to Entity-Centric Context Tracking in Social Conversations. (arXiv:2201.12409v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12409","description":"<p>In human-human conversations, Context Tracking deals with identifying\nimportant entities and keeping track of their properties and relationships.\nThis is a challenging problem that encompasses several subtasks such as slot\ntagging, coreference resolution, resolving plural mentions and entity linking.\nWe approach this problem as an end-to-end modeling task where the\nconversational context is represented by an entity repository containing the\nentity references mentioned so far, their properties and the relationships\nbetween them. The repository is updated turn-by-turn, thus making training and\ninference computationally efficient even for long conversations. This paper\nlays the groundwork for an investigation of this framework in two ways. First,\nwe release Contrack, a large scale human-human conversation corpus for context\ntracking with people and location annotations. It contains over 7000\nconversations with an average of 11.8 turns, 5.8 entities and 15.2 references\nper conversation. Second, we open-source a neural network architecture for\ncontext tracking. Finally we compare this network to state-of-the-art\napproaches for the subtasks it subsumes and report results on the involved\ntradeoffs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruckert_U/0/1/0/all/0/1\">Ulrich R&#xfc;ckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkara_S/0/1/0/all/0/1\">Srinivas Sunkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Abhinav Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1\">Sushant Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khaitan_P/0/1/0/all/0/1\">Pranav Khaitan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval. (arXiv:2201.12431v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12431","description":"<p>Retrieval-based language models (R-LM) model the probability of natural\nlanguage text by combining a standard language model (LM) with examples\nretrieved from an external datastore at test time. While effective, a major\nbottleneck of using these models in practice is the computationally costly\ndatastore search, which can be performed as frequently as every time step. In\nthis paper, we present RetoMaton -- retrieval automaton -- which approximates\nthe datastore search, based on (1) clustering of entries into \"states\", and (2)\nstate transitions from previous entries. This effectively results in a weighted\nfinite automaton built on top of the datastore, instead of representing the\ndatastore as a flat list. The creation of the automaton is unsupervised, and a\nRetoMaton can be constructed from any text collection: either the original\ntraining corpus or from another domain. Traversing this automaton at inference\ntime, in parallel to the LM inference, reduces its perplexity, or alternatively\nsaves up to 83% of the nearest neighbor searches over kNN-LM (Khandelwal et\nal., 2020), without hurting perplexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1\">Uri Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Frank F. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Sudipta Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey. (arXiv:2201.12438v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12438","description":"<p>While commonsense knowledge acquisition and reasoning has traditionally been\na core research topic in the knowledge representation and reasoning community,\nrecent years have seen a surge of interest in the natural language processing\ncommunity in developing pre-trained models and testing their ability to address\na variety of newly designed commonsense knowledge reasoning and generation\ntasks. This paper presents a survey of these tasks, discusses the strengths and\nweaknesses of state-of-the-art pre-trained models for commonsense reasoning and\ngeneration as revealed by these tasks, and reflects on future research\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhargava_P/0/1/0/all/0/1\">Prajjwal Bhargava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_V/0/1/0/all/0/1\">Vincent Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScaLA: Accelerating Adaptation of Pre-Trained Transformer-Based Language Models via Efficient Large-Batch Adversarial Noise. (arXiv:2201.12469v1 [cs.LG])","link":"http://arxiv.org/abs/2201.12469","description":"<p>In recent years, large pre-trained Transformer-based language models have led\nto dramatic improvements in many natural language understanding tasks. To train\nthese models with increasing sizes, many neural network practitioners attempt\nto increase the batch sizes in order to leverage multiple GPUs to improve\ntraining speed. However, increasing the batch size often makes the optimization\nmore difficult, leading to slow convergence or poor generalization that can\nrequire orders of magnitude more training time to achieve the same model\nquality. In this paper, we explore the steepness of the loss landscape of\nlarge-batch optimization for adapting pre-trained Transformer-based language\nmodels to domain-specific tasks and find that it tends to be highly complex and\nirregular, posing challenges to generalization on downstream tasks.\n</p>\n<p>To tackle this challenge, we propose ScaLA, a novel and efficient method to\naccelerate the adaptation speed of pre-trained transformer networks. Different\nfrom prior methods, we take a sequential game-theoretic approach by adding\nlightweight adversarial noise into large-batch optimization, which\nsignificantly improves adaptation speed while preserving model generalization.\nExperiment results show that ScaLA attains 2.7--9.8$\\times$ adaptation speedups\nover the baseline for GLUE on BERT-base and RoBERTa-large, while achieving\ncomparable and sometimes higher accuracy than the state-of-the-art large-batch\noptimization methods. Finally, we also address the theoretical aspect of\nlarge-batch optimization with adversarial noise and provide a theoretical\nconvergence rate analysis for ScaLA using techniques for analyzing non-convex\nsaddle-point problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minjia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naresh_N/0/1/0/all/0/1\">Niranjan Uma Naresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Transliteration Help Multilingual Language Modeling?. (arXiv:2201.12501v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12501","description":"<p>As there is a scarcity of large representative corpora for most languages, it\nis important for Multilingual Language Models (MLLM) to extract the most out of\nexisting corpora. In this regard, script diversity presents a challenge to\nMLLMs by reducing lexical overlap among closely related languages. Therefore,\ntransliterating closely related languages that use different writing scripts to\na common script may improve the downstream task performance of MLLMs. In this\npaper, we pretrain two ALBERT models to empirically measure the effect of\ntransliteration on MLLMs. We specifically focus on the Indo-Aryan language\nfamily, which has the highest script diversity in the world. Afterward, we\nevaluate our models on the IndicGLUE benchmark. We perform Mann-Whitney U test\nto rigorously verify whether the effect of transliteration is significant or\nnot. We find that transliteration benefits the low-resource languages without\nnegatively affecting the comparatively high-resource languages. We also measure\nthe cross-lingual representation similarity (CLRS) of the models using centered\nkernel alignment (CKA) on parallel sentences of eight languages from the\nFLORES-101 dataset. We find that the hidden representations of the\ntransliteration-based model have higher and more stable CLRS scores. Our code\nis available at Github (github.com/ibraheem-moosa/XLM-Indic) and Hugging Face\nHub (huggingface.co/ibraheemmoosa/xlmindic-base-multiscript and\nhuggingface.co/ibraheemmoosa/xlmindic-base-uniscript).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moosa_I/0/1/0/all/0/1\">Ibraheem Muhammad Moosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhter_M/0/1/0/all/0/1\">Mahmud Elahi Akhter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habib_A/0/1/0/all/0/1\">Ashfia Binte Habib</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Summarization with Customized Granularities. (arXiv:2201.12502v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12502","description":"<p>Text summarization is a personalized and customized task, i.e., for one\ndocument, users often have different preferences for the summary. As a key\naspect of customization in summarization, granularity is used to measure the\nsemantic coverage between summary and source document. Coarse-grained summaries\ncan only contain the most central event in the original text, while\nfine-grained summaries cover more sub-events and corresponding details.\nHowever, previous studies mostly develop systems in the single-granularity\nscenario. And models that can generate summaries with customizable semantic\ncoverage still remain an under-explored topic. In this paper, we propose the\nfirst unsupervised multi-granularity summarization framework, GranuSum. We take\nevents as the basic semantic units of the source documents and propose to rank\nthese events by their salience. We also develop a model to summarize input\ndocuments with given events as anchors and hints. By inputting different\nnumbers of events, GranuSum is capable of producing multi-granular summaries in\nan unsupervised manner. Meanwhile, to evaluate multi-granularity summarization\nmodels, we annotate a new benchmark GranuDUC, in which we write multiple\nsummaries of different granularities for each document cluster. Experimental\nresults confirm the substantial superiority of GranuSum on multi-granularity\nsummarization over several baseline systems. Furthermore, by experimenting on\nconventional unsupervised abstractive summarization tasks, we find that\nGranuSum, by exploiting the event information, can also achieve new\nstate-of-the-art results under this scenario, outperforming strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Suyu Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yizhu Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoDistil: Few-shot Task-agnostic Neural Architecture Search for Distilling Large Language Models. (arXiv:2201.12507v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12507","description":"<p>Knowledge distillation (KD) methods compress large models into smaller\nstudents with manually-designed student architectures given pre-specified\ncomputational cost. This requires several trials to find a viable student, and\nfurther repeating the process for each student or computational budget change.\nWe use Neural Architecture Search (NAS) to automatically distill several\ncompressed students with variable cost from a large model. Current works train\na single SuperLM consisting of millions of subnetworks with weight-sharing,\nresulting in interference between subnetworks of different sizes. Our framework\nAutoDistil addresses above challenges with the following steps: (a)\nIncorporates inductive bias and heuristics to partition Transformer search\nspace into K compact sub-spaces (K=3 for typical student sizes of base, small\nand tiny); (b) Trains one SuperLM for each sub-space using task-agnostic\nobjective (e.g., self-attention distillation) with weight-sharing of students;\n(c) Lightweight search for the optimal student without re-training. Fully\ntask-agnostic training and search allow students to be reused for fine-tuning\non any downstream task. Experiments on GLUE benchmark against state-of-the-art\nKD and NAS methods demonstrate AutoDistil to outperform leading compression\ntechniques with upto 2.7x reduction in computational cost and negligible loss\nin task performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dongkuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_D/0/1/0/all/0/1\">Debadeepta Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Commonsense Knowledge into Story Ending Generation via Heterogeneous Graph Networks. (arXiv:2201.12538v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12538","description":"<p>Story ending generation is an interesting and challenging task, which aims to\ngenerate a coherent and reasonable ending given a story context. The key\nchallenges of the task lie in how to comprehend the story context sufficiently\nand handle the implicit knowledge behind story clues effectively, which are\nstill under-explored by previous work. In this paper, we propose a Story\nHeterogeneous Graph Network (SHGN) to explicitly model both the information of\nstory context at different granularity levels and the multi-grained interactive\nrelations among them. In detail, we consider commonsense knowledge, words and\nsentences as three types of nodes. To aggregate non-local information, a global\nnode is also introduced. Given this heterogeneous graph network, the node\nrepresentations are updated through graph propagation, which adequately\nutilizes commonsense knowledge to facilitate story comprehension. Moreover, we\ndesign two auxiliary tasks to implicitly capture the sentiment trend and key\nevents lie in the context. The auxiliary tasks are jointly optimized with the\nprimary story ending generation task in a multi-task learning strategy.\nExtensive experiments on the ROCStories Corpus show that the developed model\nachieves new state-of-the-art performances. Human study further demonstrates\nthat our model generates more reasonable story endings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_B/0/1/0/all/0/1\">Beiqi Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jianfeng Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pengpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">An Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lei Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Continual Learning for Spoken Keyword Spotting. (arXiv:2201.12546v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12546","description":"<p>Catastrophic forgetting is a thorny challenge when updating keyword spotting\n(KWS) models after deployment. To tackle such challenges, we propose a\nprogressive continual learning strategy for small-footprint spoken keyword\nspotting (PCL-KWS). Specifically, the proposed PCL-KWS framework introduces a\nnetwork instantiator to generate the task-specific sub-networks for remembering\npreviously learned keywords. As a result, the PCL-KWS approach incrementally\nlearns new keywords without forgetting prior knowledge. Besides, the\nkeyword-aware network scaling mechanism of PCL-KWS constrains the growth of\nmodel parameters while achieving high performance. Experimental results show\nthat after learning five new tasks sequentially, our proposed PCL-KWS approach\narchives the new state-of-the-art performance of 92.8% average accuracy for all\nthe tasks on Google Speech Command dataset compared with other baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yizheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_N/0/1/0/all/0/1\">Nana Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Information-Based Approach to Unsupervised Domain-Adaptive Aspect-Based Sentiment Analysis. (arXiv:2201.12549v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12549","description":"<p>Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask which aims to extract the aspects from sentences and identify their\ncorresponding sentiments. Aspect term extraction (ATE) is the crucial step for\nABSA. Due to the expensive annotation for aspect terms, we often lack labeled\ntarget domain data for fine-tuning. To address this problem, many approaches\nhave been proposed recently to transfer common knowledge in an unsupervised\nway, but such methods have too many modules and require expensive multi-stage\npreprocessing. In this paper, we propose a simple but effective technique based\non mutual information maximization, which can serve as an additional component\nto enhance any kind of model for cross-domain ABSA and ATE. Furthermore, we\nprovide some analysis of this approach. Experiment results show that our\nproposed method outperforms the state-of-the-art methods for cross-domain ABSA\nby 4.32% Micro-F1 on average over 10 different domain pairs. Apart from that,\nour method can be extended to other sequence labeling tasks, such as named\nentity recognition (NER).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Le Processus Powered Dirichlet-Hawkes comme A Priori Flexible pour Clustering Temporel de Textes. (arXiv:2201.12568v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12568","description":"<p>The textual content of a document and its publication date are intertwined.\nFor example, the publication of a news article on a topic is influenced by\nprevious publications on similar issues, according to underlying temporal\ndynamics. However, it can be challenging to retrieve meaningful information\nwhen textual information conveys little. Furthermore, the textual content of a\ndocument is not always correlated to its temporal dynamics. We develop a method\nto create clusters of textual documents according to both their content and\npublication time, the Powered Dirichlet-Hawkes process (PDHP). PDHP yields\nsignificantly better results than state-of-the-art models when temporal\ninformation or textual content is weakly informative. PDHP also alleviates the\nhypothesis that textual content and temporal dynamics are perfectly correlated.\nWe demonstrate that PDHP generalizes previous work --such as DHP and UP.\nFinally, we illustrate a possible application using a real-world dataset from\nReddit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poux_Medard_G/0/1/0/all/0/1\">Ga&#xeb;l Poux-M&#xe9;dard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velcin_J/0/1/0/all/0/1\">Julien Velcin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loudcher_S/0/1/0/all/0/1\">Sabine Loudcher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hand Gesture Recognition of Dumb Person Using one Against All Neural Network. (arXiv:2201.12622v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12622","description":"<p>We propose a new technique for recognition of dumb person hand gesture in\nreal world environment. In this technique, the hand image containing the\ngesture is preprocessed and then hand region is segmented by convergent the RGB\ncolor image to L.a.b color space. Only few statistical features are used to\nclassify the segmented image to different classes. Artificial Neural Network is\ntrained in sequential manner using one against all. When the system gets\ntrained, it becomes capable of recognition of each class in parallel manner.\nThe result of proposed technique is much better than existing techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Muhammad Asim Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Sajjad Ahmed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep CNN Architecture with Novel Pooling Layer Applied to Two Sudanese Arabic Sentiment Datasets. (arXiv:2201.12664v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12664","description":"<p>Arabic sentiment analysis has become an important research field in recent\nyears. Initially, work focused on Modern Standard Arabic (MSA), which is the\nmost widely-used form. Since then, work has been carried out on several\ndifferent dialects, including Egyptian, Levantine and Moroccan. Moreover, a\nnumber of datasets have been created to support such work. However, up until\nnow, less work has been carried out on Sudanese Arabic, a dialect which has 32\nmillion speakers. In this paper, two new publicly available datasets are\nintroduced, the 2-Class Sudanese Sentiment Dataset (SudSenti2) and the 3-Class\nSudanese Sentiment Dataset (SudSenti3). Furthermore, a CNN architecture, SCM,\nis proposed, comprising five CNN layers together with a novel pooling layer,\nMMA, to extract the best features. This SCM+MMA model is applied to SudSenti2\nand SudSenti3 with accuracies of 92.75% and 84.39%. Next, the model is compared\nto other deep learning classifiers and shown to be superior on these new\ndatasets. Finally, the proposed model is applied to the existing Saudi\nSentiment Dataset and to the MSA Hotel Arabic Review Dataset with accuracies\n85.55% and 90.01%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mhamed_M/0/1/0/all/0/1\">Mustafa Mhamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutcliffe_R/0/1/0/all/0/1\">Richard Sutcliffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xia Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almekhlafi_E/0/1/0/all/0/1\">Eiad Almekhlafi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Retta_E/0/1/0/all/0/1\">Ephrem A. Retta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models. (arXiv:2201.12675v1 [cs.LG])","link":"http://arxiv.org/abs/2201.12675","description":"<p>A central tenet of Federated learning (FL), which trains models without\ncentralizing user data, is privacy. However, previous work has shown that the\ngradient updates used in FL can leak user information. While the most\nindustrial uses of FL are for text applications (e.g. keystroke prediction),\nnearly all attacks on FL privacy have focused on simple image classifiers. We\npropose a novel attack that reveals private user text by deploying malicious\nparameter vectors, and which succeeds even with mini-batches, multiple users,\nand long sequences. Unlike previous attacks on FL, the attack exploits\ncharacteristics of both the Transformer architecture and the token embedding,\nseparately extracting tokens and positional embeddings to retrieve\nhigh-fidelity text. This work suggests that FL on text, which has historically\nbeen resistant to privacy attacks, is far more vulnerable than previously\nthought.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fowl_L/0/1/0/all/0/1\">Liam Fowl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reich_S/0/1/0/all/0/1\">Steven Reich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czaja_W/0/1/0/all/0/1\">Wojtek Czaja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VC-GPT: Visual Conditioned GPT for End-to-End Generative Vision-and-Language Pre-training. (arXiv:2201.12723v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12723","description":"<p>Vision-and-language pre-training models (VLMs) have achieved tremendous\nsuccess in the cross-modal area, but most of them require millions of parallel\nimage-caption data for pre-training. Collating such data is expensive and\nlabor-intensive. In this work, we focus on reducing such need for generative\nvision-and-language pre-training (G-VLP) by taking advantage of the visual\npre-trained model (CLIP-ViT) as encoder and language pre-trained model (GPT2)\nas decoder. Unfortunately, GPT2 lacks a necessary cross-attention module, which\nhinders the direct connection of CLIP-ViT and GPT2. To remedy such defects, we\nconduct extensive experiments to empirically investigate how to design and\npre-train our model. Based on our experimental results, we propose a novel\nG-VLP framework, Visual Conditioned GPT (VC-GPT), and pre-train it with a\nsmall-scale parallel image-caption corpus (Visual Genome, only 110k distinct\nimages). Evaluating on the image captioning downstream tasks (MSCOCO and\nFlickr30k Captioning), VC-GPT achieves either the best or the second-best\nperformance across all evaluation metrics over the previous works which consume\naround 30 times more parallel data during pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yadong Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Part of Speech Tagging (POST) of a Low-resource Language using another Language (Developing a POS-Tagged Lexicon for Kurdish (Sorani) using a Tagged Persian (Farsi) Corpus). (arXiv:2201.12793v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12793","description":"<p>Tagged corpora play a crucial role in a wide range of Natural Language\nProcessing. The Part of Speech Tagging (POST) is essential in developing tagged\ncorpora. It is time-and-effort-consuming and costly, and therefore, it could be\nmore affordable if it is automated. The Kurdish language currently lacks\npublicly available tagged corpora of proper sizes. Tagging the publicly\navailable Kurdish corpora can leverage the capability of those resources to a\nhigher level than what raw or segmented corpora can provide. Developing\nPOS-tagged lexicons can assist the mentioned task. We use a tagged corpus\n(Bijankhan corpus) in Persian (Farsi) as a close language to Kurdish to develop\na POS-tagged lexicon. This paper presents the approach of leveraging the\nresource of a close language to Kurdish to enrich its resources. A partial\ndataset of the results is publicly available for non-commercial use under CC\nBY-NC-SA 4.0 license at https://kurdishblark.github.io/. We plan to make the\nwhole tagged corpus available after further investigation on the outcome. The\ndataset can help in developing POS-tagged lexicons for other Kurdish dialects\nand automated Kurdish corpora tagging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1\">Hossein Hassani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Co-Regularized Adversarial Learning for Multi-Domain Text Classification. (arXiv:2201.12796v1 [cs.LG])","link":"http://arxiv.org/abs/2201.12796","description":"<p>Multi-domain text classification (MDTC) aims to leverage all available\nresources from multiple domains to learn a predictive model that can generalize\nwell on these domains. Recently, many MDTC methods adopt adversarial learning,\nshared-private paradigm, and entropy minimization to yield state-of-the-art\nresults. However, these approaches face three issues: (1) Minimizing domain\ndivergence can not fully guarantee the success of domain alignment; (2)\nAligning marginal feature distributions can not fully guarantee the\ndiscriminability of the learned features; (3) Standard entropy minimization may\nmake the predictions on unlabeled data over-confident, deteriorating the\ndiscriminability of the learned features. In order to address the above issues,\nwe propose a co-regularized adversarial learning (CRAL) mechanism for MDTC.\nThis approach constructs two diverse shared latent spaces, performs domain\nalignment in each of them, and punishes the disagreements of these two\nalignments with respect to the predictions on unlabeled data. Moreover, virtual\nadversarial training (VAT) with entropy minimization is incorporated to impose\nconsistency regularization to the CRAL method. Experiments show that our model\noutperforms state-of-the-art methods on two MDTC benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inkpen_D/0/1/0/all/0/1\">Diana Inkpen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Roby_A/0/1/0/all/0/1\">Ahmed El-Roby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognition of Implicit Geographic Movement in Text. (arXiv:2201.12799v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12799","description":"<p>Analyzing the geographic movement of humans, animals, and other phenomena is\na growing field of research. This research has benefited urban planning,\nlogistics, animal migration understanding, and much more. Typically, the\nmovement is captured as precise geographic coordinates and time stamps with\nGlobal Positioning Systems (GPS). Although some research uses computational\ntechniques to take advantage of implicit movement in descriptions of route\ndirections, hiking paths, and historical exploration routes, innovation would\naccelerate with a large and diverse corpus. We created a corpus of sentences\nlabeled as describing geographic movement or not and including the type of\nentity moving. Creating this corpus proved difficult without any comparable\ncorpora to start with, high human labeling costs, and since movement can at\ntimes be interpreted differently. To overcome these challenges, we developed an\niterative process employing hand labeling, crowd voting for confirmation, and\nmachine learning to predict more labels. By merging advances in word embeddings\nwith traditional machine learning models and model ensembling, prediction\naccuracy is at an acceptable level to produce a large silver-standard corpus\ndespite the small gold-standard corpus training set. Our corpus will likely\nbenefit computational processing of geography in text and spatial cognition, in\naddition to detection of movement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pezanowski_S/0/1/0/all/0/1\">Scott Pezanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1\">Prasenjit Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving End-to-End Contextual Speech Recognition with Fine-grained Contextual Knowledge Selection. (arXiv:2201.12806v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12806","description":"<p>Nowadays, most methods in end-to-end contextual speech recognition bias the\nrecognition process towards contextual knowledge. Since all-neural contextual\nbiasing methods rely on phrase-level contextual modeling and attention-based\nrelevance modeling, they may encounter confusion between similar\ncontext-specific phrases, which hurts predictions at the token level. In this\nwork, we focus on mitigating confusion problems with fine-grained contextual\nknowledge selection (FineCoS). In FineCoS, we introduce fine-grained knowledge\nto reduce the uncertainty of token predictions. Specifically, we first apply\nphrase selection to narrow the range of phrase candidates, and then conduct\ntoken attention on the tokens in the selected phrase candidates. Moreover, we\nre-normalize the attention weights of most relevant phrases in inference to\nobtain more focused phrase-level contextual representations, and inject\nposition information to better discriminate phrases or tokens. On LibriSpeech\nand an in-house 160,000-hour dataset, we explore the proposed methods based on\na controllable all-neural biasing method, collaborative decoding (ColDec). The\nproposed methods provide at most 6.1% relative word error rate reduction on\nLibriSpeech and 16.4% relative character error rate reduction on the in-house\ndataset over ColDec.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Minglun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Linhao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhenlin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Meng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TTS-Portuguese Corpus: a corpus for speech synthesis in Brazilian Portuguese. (arXiv:2005.05144v4 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2005.05144","description":"<p>Speech provides a natural way for human-computer interaction. In particular,\nspeech synthesis systems are popular in different applications, such as\npersonal assistants, GPS applications, screen readers and accessibility tools.\nHowever, not all languages are on the same level when in terms of resources and\nsystems for speech synthesis. This work consists of creating publicly available\nresources for Brazilian Portuguese in the form of a novel dataset along with\ndeep learning models for end-to-end speech synthesis. Such dataset has 10.5\nhours from a single speaker, from which a Tacotron 2 model with the RTISI-LA\nvocoder presented the best performance, achieving a 4.03 MOS value. The\nobtained results are comparable to related works covering English language and\nthe state-of-the-art in Portuguese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Casanova_E/0/1/0/all/0/1\">Edresson Casanova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Junior_A/0/1/0/all/0/1\">Arnaldo Candido Junior</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shulby_C/0/1/0/all/0/1\">Christopher Shulby</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_F/0/1/0/all/0/1\">Frederico Santos de Oliveira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teixeira_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Teixeira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ponti_M/0/1/0/all/0/1\">Moacir Antonelli Ponti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aluisio_S/0/1/0/all/0/1\">Sandra Maria Aluisio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Boundary Regression Model for Nested Named Entity Recognition. (arXiv:2011.14330v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.14330","description":"<p>Recognizing named entities (NEs) is commonly conducted as a classification\nproblem that predicts a class tag for a word or a NE candidate in a sentence.\nIn shallow structures, categorized features are weighted to support the\nprediction. Recent developments in neural networks have adopted deep structures\nthat map categorized features into continuous representations. This approach\nunfolds a dense space saturated with high-order abstract semantic information,\nwhere the prediction is based on distributed feature representations. In this\npaper, positions of NEs in a sentence are represented as continuous values.\nThen, a regression operation is introduced to regress boundaries of NEs in a\nsentence. Based on boundary regression, we design a boundary regression model\nto support nested NE recognition. It is a multiobjective learning framework,\nwhich simultaneously predicts the classification score of a NE candidate and\nrefine its spatial location in a sentence. It has the advantage to resolve\nnested NEs and support boundary regression for locating NEs in a sntence. By\nsharing parameters for predicting and locating, this model enables more potent\nnonlinear function approximators to enhance model discriminability. Experiments\ndemonstrate state-of-the-art performance for nested NE recognition\\footnote{Our\ncodes to implement the BR model are available at:\n\\url{https://github.com/wuyuefei3/BR}.}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanping Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lefei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinghua Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ruizhang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Liyuan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Junhui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qing_Y/0/1/0/all/0/1\">Yongbin Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Ping Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integer-only Zero-shot Quantization for Efficient Speech Recognition. (arXiv:2103.16827v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2103.16827","description":"<p>End-to-end neural network models achieve improved performance on various\nautomatic speech recognition (ASR) tasks. However, these models perform poorly\non edge hardware due to large memory and computation requirements. While\nquantizing model weights and/or activations to low-precision can be a promising\nsolution, previous research on quantizing ASR models is limited. In particular,\nthe previous approaches use floating-point arithmetic during inference and thus\nthey cannot fully exploit efficient integer processing units. Moreover, they\nrequire training and/or validation data during quantization, which may not be\navailable due to security or privacy concerns. To address these limitations, we\npropose an integer-only, zero-shot quantization scheme for ASR models. In\nparticular, we generate synthetic data whose runtime statistics resemble the\nreal data, and we use it to calibrate models during quantization. We apply our\nmethod to quantize QuartzNet, Jasper, and Conformer and show negligible WER\ndegradation as compared to the full-precision baseline models, even without\nusing any data. Moreover, we achieve up to 2.35x speedup on a T4 GPU and 4x\ncompression rate, with a modest WER degradation of &lt;1% with INT8 quantization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_N/0/1/0/all/0/1\">Nicholas Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_P/0/1/0/all/0/1\">Patrick Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nrusimha_A/0/1/0/all/0/1\">Aniruddha Nrusimha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhai_B/0/1/0/all/0/1\">Bohan Zhai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_T/0/1/0/all/0/1\">Tianren Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Pre-trained Convolutions Better than Pre-trained Transformers?. (arXiv:2105.03322v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.03322","description":"<p>In the era of pre-trained language models, Transformers are the de facto\nchoice of model architectures. While recent research has shown promise in\nentirely convolutional, or CNN, architectures, they have not been explored\nusing the pre-train-fine-tune paradigm. In the context of language models, are\nconvolutional models competitive to Transformers when pre-trained? This paper\ninvestigates this research question and presents several interesting findings.\nAcross an extensive set of experiments on 8 datasets/tasks, we find that\nCNN-based pre-trained models are competitive and outperform their Transformer\ncounterpart in certain scenarios, albeit with caveats. Overall, the findings\noutlined in this paper suggest that conflating pre-training and architectural\nadvances is misguided and that both advances should be considered\nindependently. We believe our research paves the way for a healthy amount of\noptimism in alternative architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aribandi_V/0/1/0/all/0/1\">Vamsi Aribandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence Similarity Based on Contexts. (arXiv:2105.07623v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.07623","description":"<p>Existing methods to measure sentence similarity are faced with two\nchallenges: (1) labeled datasets are usually limited in size, making them\ninsufficient to train supervised neural models; (2) there is a training-test\ngap for unsupervised language modeling (LM) based models to compute semantic\nscores between sentences, since sentence-level semantics are not explicitly\nmodeled at training. This results in inferior performances in this task. In\nthis work, we propose a new framework to address these two issues. The proposed\nframework is based on the core idea that the meaning of a sentence should be\ndefined by its contexts, and that sentence similarity can be measured by\ncomparing the probabilities of generating two sentences given the same context.\nThe proposed framework is able to generate high-quality, large-scale dataset\nwith semantic similarity scores between two sentences in an unsupervised\nmanner, with which the train-test gap can be largely bridged. Extensive\nexperiments show that the proposed framework achieves significant performance\nboosts over existing baselines under both the supervised and unsupervised\nsettings across different datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1\">Xiang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chun Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World. (arXiv:2106.00188v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.00188","description":"<p>We propose PIGLeT: a model that learns physical commonsense knowledge through\ninteraction, and then uses this knowledge to ground language. We factorize\nPIGLeT into a physical dynamics model, and a separate language model. Our\ndynamics model learns not just what objects are but also what they do: glass\ncups break when thrown, plastic ones don't. We then use it as the interface to\nour language model, giving us a unified model of linguistic form and grounded\nmeaning. PIGLeT can read a sentence, simulate neurally what might happen next,\nand then communicate that result through a literal symbolic representation, or\nnatural language.\n</p>\n<p>Experimental results show that our model effectively learns world dynamics,\nalong with how to communicate them. It is able to correctly forecast \"what\nhappens next\" given an English sentence over 80% of the time, outperforming a\n100x larger, text-to-text approach by over 10%. Likewise, its natural language\nsummaries of physical interactions are also judged by humans as more accurate\nthan LM alternatives. We present comprehensive analysis showing room for future\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discovery. (arXiv:2108.05669v3 [cs.DL] UPDATED)","link":"http://arxiv.org/abs/2108.05669","description":"<p>Isolated silos of scientific research and the growing challenge of\ninformation overload limit awareness across the literature and hinder\ninnovation. Algorithmic curation and recommendation, which often prioritize\nrelevance, can further reinforce these informational \"filter bubbles.\" In\nresponse, we describe Bridger, a system for facilitating discovery of scholars\nand their work. We construct a faceted representation of authors with\ninformation gleaned from their papers and inferred author personas, and use it\nto develop an approach that locates commonalities and contrasts between\nscientists to balance relevance and novelty. In studies with computer science\nresearchers, this approach helps users discover authors considered useful for\ngenerating novel research directions. We also demonstrate an approach for\ndisplaying information about authors, boosting the ability to understand the\nwork of new, unfamiliar scholars. Our analysis reveals that Bridger connects\nauthors who have different citation profiles and publish in different venues,\nraising the prospect of bridging diverse scientific communities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Portenoy_J/0/1/0/all/0/1\">Jason Portenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radensky_M/0/1/0/all/0/1\">Marissa Radensky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_J/0/1/0/all/0/1\">Jevin West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pairwise Supervised Contrastive Learning of Sentence Representations. (arXiv:2109.05424v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05424","description":"<p>Many recent successes in sentence representation learning have been achieved\nby simply fine-tuning on the Natural Language Inference (NLI) datasets with\ntriplet loss or siamese loss. Nevertheless, they share a common weakness:\nsentences in a contradiction pair are not necessarily from different semantic\ncategories. Therefore, optimizing the semantic entailment and contradiction\nreasoning objective alone is inadequate to capture the high-level semantic\nstructure. The drawback is compounded by the fact that the vanilla siamese or\ntriplet losses only learn from individual sentence pairs or triplets, which\noften suffer from bad local optima. In this paper, we propose PairSupCon, an\ninstance discrimination based approach aiming to bridge semantic entailment and\ncontradiction understanding with high-level categorical concept encoding. We\nevaluate PairSupCon on various downstream tasks that involve understanding\nsentence semantics at different granularities. We outperform the previous\nstate-of-the-art method with $10\\%$--$13\\%$ averaged improvement on eight\nclustering tasks, and $5\\%$--$6\\%$ averaged improvement on seven semantic\ntextual similarity (STS) tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dejiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nallapati_R/0/1/0/all/0/1\">Ramesh Nallapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_A/0/1/0/all/0/1\">Andrew O. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers. (arXiv:2109.10686v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10686","description":"<p>There remain many open questions pertaining to the scaling behaviour of\nTransformer architectures. These scaling decisions and findings can be\ncritical, as training runs often come with an associated computational cost\nwhich have both financial and/or environmental impact. The goal of this paper\nis to present scaling insights from pretraining and finetuning Transformers.\nWhile Kaplan et al. presents a comprehensive study of the scaling behaviour of\nTransformer language models, the scope is only on the upstream (pretraining)\nloss. Therefore, it is still unclear if these set of findings transfer to\ndownstream task within the context of the pretrain-finetune paradigm. The key\nfindings of this paper are as follows: (1) we show that aside from only the\nmodel size, model shape matters for downstream fine-tuning, (2) scaling\nprotocols operate differently at different compute regions, (3) widely adopted\nT5-base and T5-large sizes are Pareto-inefficient. To this end, we present\nimproved scaling protocols whereby our redesigned models achieve similar\ndownstream fine-tuning quality while having 50\\% fewer parameters and training\n40\\% faster compared to the widely adopted T5-base model. We publicly release\nover 100 pretrained checkpoints of different T5 configurations to facilitate\nfuture research and analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jinfeng Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abnar_S/0/1/0/all/0/1\">Samira Abnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaswani_A/0/1/0/all/0/1\">Ashish Vaswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking BERT: Understanding its Vulnerabilities for Named Entity Recognition through Adversarial Attack. (arXiv:2109.11308v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11308","description":"<p>Both generic and domain-specific BERT models are widely used for natural\nlanguage processing (NLP) tasks. In this paper we investigate the vulnerability\nof BERT models to variation in input data for Named Entity Recognition (NER)\nthrough adversarial attack. Experimental results show that BERT models are\nvulnerable to variation in the entity context with 20.2 to 45.0% of entities\npredicted completely wrong and another 29.3 to 53.3% of entities predicted\nwrong partially. BERT models seem most vulnerable to changes in the local\ncontext of entities and often a single change is sufficient to fool the model.\nThe domain-specific BERT model trained from scratch (SciBERT) is more\nvulnerable than the original BERT model or the domain-specific model that\nretains the BERT vocabulary (BioBERT). We also find that BERT models are\nparticularly vulnerable to emergent entities. Our results chart the\nvulnerabilities of BERT models for NER and emphasize the importance of further\nresearch into uncovering and reducing these weaknesses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dirkson_A/0/1/0/all/0/1\">Anne Dirkson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verberne_S/0/1/0/all/0/1\">Suzan Verberne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraaij_W/0/1/0/all/0/1\">Wessel Kraaij</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FacTeR-Check: Semi-automated fact-checking through Semantic Similarity and Natural Language Inference. (arXiv:2110.14532v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.14532","description":"<p>Our society produces and shares overwhelming amounts of information through\nOnline Social Networks (OSNs). Within this environment, misinformation and\ndisinformation have proliferated, becoming a public safety concern in most\ncountries. Allowing the public and professionals to efficiently find reliable\nevidences about the factual veracity of a claim is a crucial step to mitigate\nthis harmful spread. To this end, we propose FacTeR-Check, a multilingual\narchitecture for semi-automated fact-checking that can be used for either\napplications designed for the general public and by fact-checking\norganisations. FacTeR-Check enables retrieving fact-checked information,\nunchecked claims verification and tracking dangerous information over social\nmedia. This architectures involves several modules developed to evaluate\nsemantic similarity, to calculate natural language inference and to retrieve\ninformation from Online Social Networks. The union of all these components\nbuilds a semi-automated fact-checking tool able of verifying new claims, to\nextract related evidence, and to track the evolution of a hoax on a OSN. While\nindividual modules are validated on related benchmarks (mainly MSTS and SICK),\nthe complete architecture is validated using a new dataset called NLI19-SP that\nis publicly released with COVID-19 related hoaxes and tweets from Spanish\nsocial media. Our results show state-of-the-art performance on the individual\nbenchmarks, as well as producing a useful analysis of the evolution over time\nof 61 different hoaxes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1\">Alejandro Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huertas_Tato_J/0/1/0/all/0/1\">Javier Huertas-Tato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huertas_Garcia_A/0/1/0/all/0/1\">&#xc1;lvaro Huertas-Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villar_Rodriguez_G/0/1/0/all/0/1\">Guillermo Villar-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_D/0/1/0/all/0/1\">David Camacho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning. (arXiv:2111.10952v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.10952","description":"<p>Despite the recent success of multi-task learning and transfer learning for\nnatural language processing (NLP), few works have systematically studied the\neffect of scaling up the number of tasks during pre-training. Towards this\ngoal, this paper introduces ExMix (Extreme Mixture): a massive collection of\n107 supervised NLP tasks across diverse domains and task-families. Using ExMix,\nwe study the effect of multi-task pre-training at the largest scale to date,\nand analyze co-training transfer amongst common families of tasks. Through this\nanalysis, we show that manually curating an ideal set of tasks for multi-task\npre-training is not straightforward, and that multi-task scaling can vastly\nimprove models on its own. Finally, we propose ExT5: a model pre-trained using\na multi-task objective of self-supervised span denoising and supervised ExMix.\nVia extensive experiments, we show that ExT5 outperforms strong T5 baselines on\nSuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of\nExMix. ExT5 also significantly improves sample efficiency while pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aribandi_V/0/1/0/all/0/1\">Vamsi Aribandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jinfeng Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huaixiu Steven Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sanket Vaibhav Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_H/0/1/0/all/0/1\">Honglei Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jianmo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1\">Kai Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-aware Video-language Pre-training for Retrieval. (arXiv:2112.00656v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00656","description":"<p>Recently, by introducing large-scale dataset and strong transformer network,\nvideo-language pre-training has shown great success especially for retrieval.\nYet, existing video-language transformer models do not explicitly fine-grained\nsemantic align. In this work, we present Object-aware Transformers, an\nobject-centric approach that extends video-language transformer to incorporate\nobject representations. The key idea is to leverage the bounding boxes and\nobject tags to guide the training process. We evaluate our model on three\nstandard sub-tasks of video-text matching on four widely used benchmarks. We\nalso provide deep analysis and detailed ablation about the proposed method. We\nshow clear improvement in performance across all tasks and datasets considered,\ndemonstrating the value of a model that incorporates object representations\ninto a video-language architecture. The code will be released at\n\\url{https://github.com/FingerRec/OA-Transformer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alex Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_G/0/1/0/all/0/1\">Guanyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_X/0/1/0/all/0/1\">Xiaohu Qie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone. (arXiv:2112.02418v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2112.02418","description":"<p>YourTTS brings the power of a multilingual approach to the task of zero-shot\nmulti-speaker TTS. Our method builds upon the VITS model and adds several novel\nmodifications for zero-shot multi-speaker and multilingual training. We\nachieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and\nresults comparable to SOTA in zero-shot voice conversion on the VCTK dataset.\nAdditionally, our approach achieves promising results in a target language with\na single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS\nand zero-shot voice conversion systems in low-resource languages. Finally, it\nis possible to fine-tune the YourTTS model with less than 1 minute of speech\nand achieve state-of-the-art results in voice similarity and with reasonable\nquality. This is important to allow synthesis for speakers with a very\ndifferent voice or recording characteristics from those seen during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casanova_E/0/1/0/all/0/1\">Edresson Casanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_J/0/1/0/all/0/1\">Julian Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shulby_C/0/1/0/all/0/1\">Christopher Shulby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_A/0/1/0/all/0/1\">Arnaldo Candido Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golge_E/0/1/0/all/0/1\">Eren G&#xf6;lge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_M/0/1/0/all/0/1\">Moacir Antonelli Ponti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training. (arXiv:2201.10207v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2201.10207","description":"<p>We introduce a new approach for speech pre-training named SPIRAL which works\nby learning denoising representation of perturbed data in a teacher-student\nframework. Specifically, given a speech utterance, we first feed the utterance\nto a teacher network to obtain corresponding representation. Then the same\nutterance is perturbed and fed to a student network. The student network is\ntrained to output representation resembling that of the teacher. At the same\ntime, the teacher network is updated as moving average of student's weights\nover training steps. In order to prevent representation collapse, we apply an\nin-utterance contrastive loss as pre-training objective and impose position\nrandomization on the input to the teacher. SPIRAL achieves competitive or\nbetter results compared to state-of-the-art speech pre-training method wav2vec\n2.0, with significant reduction of training cost (80% for Base model, 65% for\nLarge model). Furthermore, we address the problem of noise-robustness that is\ncritical to real-world speech applications. We propose multi-condition\npre-training by perturbing the student's input with various types of additive\nnoise. We demonstrate that multi-condition pre-trained SPIRAL models are more\nrobust to noisy speech (9.0% - 13.3% relative word error rate reduction on real\nnoisy test data), compared to applying multi-condition training solely in the\nfine-tuning stage. The code will be released after publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">Wenyong Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenhe Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeung_Y/0/1/0/all/0/1\">Yu Ting Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural-FST Class Language Model for End-to-End Speech Recognition. (arXiv:2201.11867v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11867","description":"<p>We propose Neural-FST Class Language Model (NFCLM) for end-to-end speech\nrecognition, a novel method that combines neural network language models\n(NNLMs) and finite state transducers (FSTs) in a mathematically consistent\nframework. Our method utilizes a background NNLM which models generic\nbackground text together with a collection of domain-specific entities modeled\nas individual FSTs. Each output token is generated by a mixture of these\ncomponents; the mixture weights are estimated with a separately trained neural\ndecider. We show that NFCLM significantly outperforms NNLM by 15.8% relative in\nterms of Word Error Rate. NFCLM achieves similar performance as traditional\nNNLM and FST shallow fusion while being less prone to overbiasing and 12 times\nmore compact, making it more suitable for on-device usage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bruguier_A/0/1/0/all/0/1\">Antoine Bruguier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dangna Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Eun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_F/0/1/0/all/0/1\">Fuchun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1\">Michael L. Seltzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model. (arXiv:2201.11990v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11990","description":"<p>Pretrained general-purpose language models can achieve state-of-the-art\naccuracies in various natural language processing domains by adapting to\ndownstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of\ntheir success, the size of these models has increased rapidly, requiring\nhigh-performance hardware, software, and algorithmic techniques to enable\ntraining such large models. As the result of a joint effort between Microsoft\nand NVIDIA, we present details on the training of the largest monolithic\ntransformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530\nbillion parameters. In this paper, we first focus on the infrastructure as well\nas the 3D parallelism methodology used to train this model using DeepSpeed and\nMegatron. Next, we detail the training process, the design of our training\ncorpus, and our data curation techniques, which we believe is a key ingredient\nto the success of the model. Finally, we discuss various evaluation results, as\nwell as other interesting observations and new properties exhibited by MT-NLG.\nWe demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning\naccuracies on several NLP benchmarks and establishes new state-of-the-art\nresults. We believe that our contributions will help further the development of\nlarge-scale training infrastructures, large-scale language models, and natural\nlanguage generations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1\">Shaden Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1\">Mostofa Patwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norick_B/0/1/0/all/0/1\">Brandon Norick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeGresley_P/0/1/0/all/0/1\">Patrick LeGresley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajbhandari_S/0/1/0/all/0/1\">Samyam Rajbhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casper_J/0/1/0/all/0/1\">Jared Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhumoye_S/0/1/0/all/0/1\">Shrimai Prabhumoye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zerveas_G/0/1/0/all/0/1\">George Zerveas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korthikanti_V/0/1/0/all/0/1\">Vijay Korthikanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1\">Elton Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Child_R/0/1/0/all/0/1\">Rewon Child</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aminabadi_R/0/1/0/all/0/1\">Reza Yazdani Aminabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernauer_J/0/1/0/all/0/1\">Julie Bernauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houston_M/0/1/0/all/0/1\">Michael Houston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwary_S/0/1/0/all/0/1\">Saurabh Tiwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-31T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"DiriNet: A network to estimate the spatial and spectral degradation functions. (arXiv:2201.12346v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12346","description":"<p>The spatial and spectral degradation functions are critical to hyper- and\nmulti-spectral image fusion. However, few work has been payed on the estimation\nof the degradation functions. To learn the spatial response function and the\npoint spread function from the image pairs to be fused, we propose a Dirichlet\nnetwork, where both functions are properly constrained. Specifically, the\nspatial response function is constrained with positivity, while the Dirichlet\ndistribution along with a total variation is imposed on the point spread\nfunction. To the best of our knowledge, the neural netwrok and the Dirichlet\nregularization are exclusively investigated, for the first time, to estimate\nthe degradation functions. Both image degradation and fusion experiments\ndemonstrate the effectiveness and superiority of the proposed Dirichlet\nnetwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Ting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-rank features based double transformation matrices learning for image classification. (arXiv:2201.12351v1 [cs.LG])","link":"http://arxiv.org/abs/2201.12351","description":"<p>Linear regression is a supervised method that has been widely used in\nclassification tasks. In order to apply linear regression to classification\ntasks, a technique for relaxing regression targets was proposed. However,\nmethods based on this technique ignore the pressure on a single transformation\nmatrix due to the complex information contained in the data. A single\ntransformation matrix in this case is too strict to provide a flexible\nprojection, thus it is necessary to adopt relaxation on transformation matrix.\nThis paper proposes a double transformation matrices learning method based on\nlatent low-rank feature extraction. The core idea is to use double\ntransformation matrices for relaxation, and jointly projecting the learned\nprincipal and salient features from two directions into the label space, which\ncan share the pressure of a single transformation matrix. Firstly, the low-rank\nfeatures are learned by the latent low rank representation (LatLRR) method\nwhich processes the original data from two directions. In this process, sparse\nnoise is also separated, which alleviates its interference on projection\nlearning to some extent. Then, two transformation matrices are introduced to\nprocess the two features separately, and the information useful for the\nclassification is extracted. Finally, the two transformation matrices can be\neasily obtained by alternate optimization methods. Through such processing,\neven when a large amount of redundant information is contained in samples, our\nmethod can also obtain projection results that are easy to classify.\nExperiments on multiple data sets demonstrate the effectiveness of our approach\nfor classification, especially for complex scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yu-Hong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven's Progressive Matrices. (arXiv:2201.12382v1 [cs.AI])","link":"http://arxiv.org/abs/2201.12382","description":"<p>Abstract visual reasoning (AVR) domain encompasses problems solving which\nrequires the ability to reason about relations among entities present in a\ngiven scene. While humans, generally, solve AVR tasks in a ``natural'' way,\neven without prior experience, this type of problems has proven difficult for\ncurrent machine learning systems. The paper summarises recent progress in\napplying deep learning methods to solving AVR problems, as a proxy for studying\nmachine intelligence. We focus on the most common type of AVR tasks -- the\nRaven's Progressive Matrices (RPMs) -- and provide a comprehensive review of\nthe learning methods and deep neural models applied to solve RPMs, as well as,\nthe RPM benchmark sets. Performance analysis of the state-of-the-art approaches\nto solving RPMs leads to formulation of certain insights and remarks on the\ncurrent and future trends in this area. We conclude the paper by demonstrating\nhow real-world problems can benefit from the discoveries of RPM studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malkinski_M/0/1/0/all/0/1\">Miko&#x142;aj Ma&#x142;ki&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandziuk_J/0/1/0/all/0/1\">Jacek Ma&#x144;dziuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Developing a Machine-Learning Algorithm to Diagnose Age-Related Macular Degeneration. (arXiv:2201.12384v1 [eess.IV])","link":"http://arxiv.org/abs/2201.12384","description":"<p>Today, more than 12 million people over the age of 40 suffer from ocular\ndiseases. Most commonly, older patients are susceptible to age related macular\ndegeneration, an eye disease that causes blurring of the central vision due to\nthe deterioration of the retina. The former can only be detected through\ncomplex and expensive imaging software, markedly a visual field test; this\nleaves a significant population with untreated eye disease and holds them at\nrisk for complete vision loss. The use of machine learning algorithms has been\nproposed for treating eye disease. However, the development of these models is\nlimited by a lack of understanding regarding appropriate model and training\nparameters to maximize model performance. In our study, we address these points\nby generating 6 models, each with a learning rate of 1 * 10^n where n is 0, -1,\n-2, ... -6, and calculated a f1 score for each of the models. Our analysis\nshows that sample imbalance is a key challenge in training of machine learning\nmodels and can result in deceptive improvements in training cost which does not\ntranslate to true improvements in model predictive performance. Considering the\nwide ranging impact of the disease and its adverse effects, we developed a\nmachine learning algorithm to treat the same. We trained our model on varying\neye disease datasets consisting of over 5000 patients, and the pictures of\ntheir infected eyes. In the future, we hope this model is used extensively,\nespecially in areas that are under-resourced, to better diagnose eye disease\nand improve well being for humanity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dua_A/0/1/0/all/0/1\">Ananya Dua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Minh_P/0/1/0/all/0/1\">Pham Hung Minh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fahmid_S/0/1/0/all/0/1\">Sajid Fahmid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1\">Shikhar Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_S/0/1/0/all/0/1\">Sophia Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moyo_V/0/1/0/all/0/1\">Vanessa Moyo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_Y/0/1/0/all/0/1\">Yanran Elisa Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A deep Q-learning method for optimizing visual search strategies in backgrounds of dynamic noise. (arXiv:2201.12385v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12385","description":"<p>Humans process visual information with varying resolution (foveated visual\nsystem) and explore images by orienting through eye movements the\nhigh-resolution fovea to points of interest. The Bayesian ideal searcher (IS)\nthat employs complete knowledge of task-relevant information optimizes eye\nmovement strategy and achieves the optimal search performance. The IS can be\nemployed as an important tool to evaluate the optimality of human eye\nmovements, and potentially provide guidance to improve human observer visual\nsearch strategies. Najemnik and Geisler (2005) derived an IS for backgrounds of\nspatial 1/f noise. The corresponding template responses follow Gaussian\ndistributions and the optimal search strategy can be analytically determined.\nHowever, the computation of the IS can be intractable when considering more\nrealistic and complex backgrounds such as medical images. Modern reinforcement\nlearning methods, successfully applied to obtain optimal policy for a variety\nof tasks, do not require complete knowledge of the background generating\nfunctions and can be potentially applied to anatomical backgrounds. An\nimportant first step is to validate the optimality of the reinforcement\nlearning method. In this study, we investigate the ability of a reinforcement\nlearning method that employs Q-network to approximate the IS. We demonstrate\nthat the search strategy corresponding to the Q-network is consistent with the\nIS search strategy. The findings show the potential of the reinforcement\nlearning with Q-network approach to estimate optimal eye movement planning with\nreal anatomical backgrounds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weimin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_M/0/1/0/all/0/1\">Miguel P. Eckstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Unsupervised Domain Adaptation for Multi-modal Cardiac Image Segmentation. (arXiv:2201.12386v1 [eess.IV])","link":"http://arxiv.org/abs/2201.12386","description":"<p>Unsupervised domain adaptation (UDA) methods intend to reduce the gap between\nsource and target domains by using unlabeled target domain and labeled source\ndomain data, however, in the medical domain, target domain data may not always\nbe easily available, and acquiring new samples is generally time-consuming.\nThis restricts the development of UDA methods for new domains. In this paper,\nwe explore the potential of UDA in a more challenging while realistic scenario\nwhere only one unlabeled target patient sample is available. We call it\nFew-shot Unsupervised Domain adaptation (FUDA). We first generate target-style\nimages from source images and explore diverse target styles from a single\ntarget patient with Random Adaptive Instance Normalization (RAIN). Then, a\nsegmentation network is trained in a supervised manner with the generated\ntarget images. Our experiments demonstrate that FUDA improves the segmentation\nperformance by 0.33 of Dice score on the target domain compared with the\nbaseline, and it also gives 0.28 of Dice score improvement in a more rigorous\none-shot setting. Our code is available at\n\\url{https://github.com/MingxuanGu/Few-shot-UDA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gu_M/0/1/0/all/0/1\">Mingxuan Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vesal_S/0/1/0/all/0/1\">Sulaiman Vesal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kosti_R/0/1/0/all/0/1\">Ronak Kosti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DoubleU-Net++: Architecture with Exploit Multiscale Features for Vertebrae Segmentation. (arXiv:2201.12389v1 [eess.IV])","link":"http://arxiv.org/abs/2201.12389","description":"<p>Accurate segmentation of the vertebra is an important prerequisite in various\nmedical applications (E.g. tele surgery) to assist surgeons. Following the\nsuccessful development of deep neural networks, recent studies have focused on\nthe essential rule of vertebral segmentation. Prior works contain a large\nnumber of parameters, and their segmentation is restricted to only one view.\nInspired by DoubleU-Net, we propose a novel model named DoubleU-Net++ in which\nDensNet as feature extractor, special attention module from Convolutional Block\nAttention on Module (CBAM) and, Pyramid Squeeze Attention (PSA) module are\nemployed to improve extracted features. We evaluate our proposed model on three\ndifferent views (sagittal, coronal, and axial) of VerSe2020 and xVertSeg\ndatasets. Compared with state-of-the-art studies, our architecture is trained\nfaster and achieves higher precision, recall, and F1-score as evaluation\n(imporoved by 4-6%) and the result of above 94% for sagittal view and above 94%\nfor both coronal view and above 93% axial view were gained for VerSe2020\ndataset, respectively. Also, for xVertSeg dataset, we achieved precision,\nrecall,and F1-score of above 97% for sagittal view, above 93% for coronal view\n,and above 96% for axial view.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jahangard_S/0/1/0/all/0/1\">Simindokht Jahangard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bonyani_M/0/1/0/all/0/1\">Mahdi Bonyani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syfer: Neural Obfuscation for Private Data Release. (arXiv:2201.12406v1 [cs.LG])","link":"http://arxiv.org/abs/2201.12406","description":"<p>Balancing privacy and predictive utility remains a central challenge for\nmachine learning in healthcare. In this paper, we develop Syfer, a neural\nobfuscation method to protect against re-identification attacks. Syfer composes\ntrained layers with random neural networks to encode the original data (e.g.\nX-rays) while maintaining the ability to predict diagnoses from the encoded\ndata. The randomness in the encoder acts as the private key for the data owner.\nWe quantify privacy as the number of attacker guesses required to re-identify a\nsingle image (guesswork). We propose a contrastive learning algorithm to\nestimate guesswork. We show empirically that differentially private methods,\nsuch as DP-Image, obtain privacy at a significant loss of utility. In contrast,\nSyfer achieves strong privacy while preserving utility. For example, X-ray\nclassifiers built with DP-image, Syfer, and original data achieve average AUCs\nof 0.53, 0.78, and 0.86, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yala_A/0/1/0/all/0/1\">Adam Yala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quach_V/0/1/0/all/0/1\">Victor Quach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esfahanizadeh_H/0/1/0/all/0/1\">Homa Esfahanizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DOliveira_R/0/1/0/all/0/1\">Rafael G. L. D&#x27;Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duffy_K/0/1/0/all/0/1\">Ken R. Duffy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medard_M/0/1/0/all/0/1\">Muriel M&#xe9;dard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1\">Tommi S. Jaakkola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoordX: Accelerating Implicit Neural Representation with a Split MLP Architecture. (arXiv:2201.12425v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12425","description":"<p>Implicit neural representations with multi-layer perceptrons (MLPs) have\nrecently gained prominence for a wide variety of tasks such as novel view\nsynthesis and 3D object representation and rendering. However, a significant\nchallenge with these representations is that both training and inference with\nan MLP over a large number of input coordinates to learn and represent an\nimage, video, or 3D object, require large amounts of computation and incur long\nprocessing times. In this work, we aim to accelerate inference and training of\ncoordinate-based MLPs for implicit neural representations by proposing a new\nsplit MLP architecture, CoordX. With CoordX, the initial layers are split to\nlearn each dimension of the input coordinates separately. The intermediate\nfeatures are then fused by the last layers to generate the learned signal at\nthe corresponding coordinate point. This significantly reduces the amount of\ncomputation required and leads to large speedups in training and inference,\nwhile achieving similar accuracy as the baseline MLP. This approach thus aims\nat first learning functions that are a decomposition of the original signal and\nthen fusing them to generate the learned signal. Our proposed architecture can\nbe generally used for many implicit neural representation tasks with no\nadditional memory overheads. We demonstrate a speedup of up to 2.92x compared\nto the baseline model for image, video, and 3D shape representation and\nrendering tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1\">Ruofan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijaykumar_N/0/1/0/all/0/1\">Nandita Vijaykumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Focused Few-Shot Object Detection for Robot Manipulation. (arXiv:2201.12437v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12437","description":"<p>This paper addresses the problem of mobile robot manipulation of novel\nobjects via detection. Our approach uses vision and control as complementary\nfunctions that learn from real-world tasks. We develop a manipulation method\nbased solely on detection then introduce task-focused few-shot object detection\nto learn new objects and settings. The current paradigm for few-shot object\ndetection uses existing annotated examples. In contrast, we extend this\nparadigm by using active data collection and annotation selection that improves\nperformance for specific downstream tasks (e.g., depth estimation and\ngrasping). In experiments for our interactive approach to few-shot learning, we\ntrain a robot to manipulate objects directly from detection (ClickBot).\nClickBot learns visual servo control from a single click of annotation, grasps\nnovel objects in clutter and other settings, and achieves state-of-the-art\nresults on an existing visual servo control and depth estimation benchmark.\nFinally, we establish a task-focused few-shot object detection benchmark to\nsupport future research: https://github.com/griffbr/TFOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Griffin_B/0/1/0/all/0/1\">Brent Griffin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters. (arXiv:2201.12467v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12467","description":"<p>The growing public concerns on data privacy in face recognition can be\ngreatly addressed by the federated learning (FL) paradigm. However,\nconventional FL methods perform poorly due to the uniqueness of the task:\nbroadcasting class centers among clients is crucial for recognition\nperformances but leads to privacy leakage. To resolve the privacy-utility\nparadox, this work proposes PrivacyFace, a framework largely improves the\nfederated learning face recognition via communicating auxiliary and\nprivacy-agnostic information among clients. PrivacyFace mainly consists of two\ncomponents: First, a practical Differentially Private Local Clustering (DPLC)\nmechanism is proposed to distill sanitized clusters from local class centers.\nSecond, a consensus-aware recognition loss subsequently encourages global\nconsensuses among clients, which ergo results in more discriminative features.\nThe proposed framework is mathematically proved to be differentially private,\nintroducing a lightweight overhead as well as yielding prominent performance\nboosts (\\textit{e.g.}, +9.63\\% and +10.26\\% for TAR@FAR=1e-4 on IJB-B and IJB-C\nrespectively). Extensive experiments and ablation studies on a large-scale\ndataset have demonstrated the efficacy and practicability of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qiang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Feng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hainan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1\">Tianshu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guochao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuanqing Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstruction of Power Lines from Point Clouds. (arXiv:2201.12499v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12499","description":"<p>This paper proposes a novel solution for constructing line features modeling\neach catenary curve present within a series of points representing multiple\ncatenary curves. The solution can be applied to extract power lines from lidar\npoint clouds, which can then be used in downstream applications like creating\ndigital twin geospatial models and evaluating the encroachment of vegetation.\nThis paper offers an example of how the results obtained by the proposed\nsolution could be used to assess vegetation growth near transmission power\nlines based on freely available lidar data for the City of Utrecht, Netherlands\n[1].\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gribov_A/0/1/0/all/0/1\">Alexander Gribov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duri_K/0/1/0/all/0/1\">Khalid Duri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"2D+3D facial expression recognition via embedded tensor manifold regularization. (arXiv:2201.12506v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12506","description":"<p>In this paper, a novel approach via embedded tensor manifold regularization\nfor 2D+3D facial expression recognition (FERETMR) is proposed. Firstly, 3D\ntensors are constructed from 2D face images and 3D face shape models to keep\nthe structural information and correlations. To maintain the local structure\n(geometric information) of 3D tensor samples in the low-dimensional tensors\nspace during the dimensionality reduction, the $\\ell_0$-norm of the core\ntensors and a tensor manifold regularization scheme embedded on core tensors\nare adopted via a low-rank truncated Tucker decomposition on the generated\ntensors. As a result, the obtained factor matrices will be used for facial\nexpression classification prediction. To make the resulting tensor optimization\nmore tractable, $\\ell_1$-norm surrogate is employed to relax $\\ell_0$-norm and\nhence the resulting tensor optimization problem has a nonsmooth objective\nfunction due to the $\\ell_1$-norm and orthogonal constraints from the\northogonal Tucker decomposition. To efficiently tackle this tensor optimization\nproblem, we establish the first-order optimality condition in terms of\nstationary points, and then design a block coordinate descent (BCD) algorithm\nwith convergence analysis and the computational complexity. Numerical results\non BU-3DFE database and Bosphorus databases demonstrate the effectiveness of\nour proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yunfang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Q/0/1/0/all/0/1\">Qiuqi Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziyan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_G/0/1/0/all/0/1\">Gaoyun An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spherical Convolution empowered FoV Prediction in 360-degree Video Multicast with Limited FoV Feedback. (arXiv:2201.12525v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12525","description":"<p>Field of view (FoV) prediction is critical in 360-degree video multicast,\nwhich is a key component of the emerging Virtual Reality (VR) and Augmented\nReality (AR) applications. Most of the current prediction methods combining\nsaliency detection and FoV information neither take into account that the\ndistortion of projected 360-degree videos can invalidate the weight sharing of\ntraditional convolutional networks, nor do they adequately consider the\ndifficulty of obtaining complete multi-user FoV information, which degrades the\nprediction performance. This paper proposes a spherical convolution-empowered\nFoV prediction method, which is a multi-source prediction framework combining\nsalient features extracted from 360-degree video with limited FoV feedback\ninformation. A spherical convolution neural network (CNN) is used instead of a\ntraditional two-dimensional CNN to eliminate the problem of weight sharing\nfailure caused by video projection distortion. Specifically, salient\nspatial-temporal features are extracted through a spherical convolution-based\nsaliency detection model, after which the limited feedback FoV information is\nrepresented as a time-series model based on a spherical convolution-empowered\ngated recurrent unit network. Finally, the extracted salient video features are\ncombined to predict future user FoVs. The experimental results show that the\nperformance of the proposed method is better than other prediction methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Ling Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiyue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale-Invariant Adversarial Attack for Evaluating and Enhancing Adversarial Defenses. (arXiv:2201.12527v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12527","description":"<p>Efficient and effective attacks are crucial for reliable evaluation of\ndefenses, and also for developing robust models. Projected Gradient Descent\n(PGD) attack has been demonstrated to be one of the most successful adversarial\nattacks. However, the effect of the standard PGD attack can be easily weakened\nby rescaling the logits, while the original decision of every input will not be\nchanged. To mitigate this issue, in this paper, we propose Scale-Invariant\nAdversarial Attack (SI-PGD), which utilizes the angle between the features in\nthe penultimate layer and the weights in the softmax layer to guide the\ngeneration of adversaries. The cosine angle matrix is used to learn angularly\ndiscriminative representation and will not be changed with the rescaling of\nlogits, thus making SI-PGD attack to be stable and effective. We evaluate our\nattack against multiple defenses and show improved performance when compared\nwith existing attacks. Further, we propose Scale-Invariant (SI) adversarial\ndefense mechanism based on the cosine angle matrix, which can be embedded into\nthe popular adversarial defenses. The experimental results show the defense\nmethod with our SI mechanism achieves state-of-the-art performance among\nmulti-step and single-step defenses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengting Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongnian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Daoqiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SupWMA: Consistent and Efficient Tractography Parcellation of Superficial White Matter with Deep Learning. (arXiv:2201.12528v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12528","description":"<p>White matter parcellation classifies tractography streamlines into clusters\nor anatomically meaningful tracts to enable quantification and visualization.\nMost parcellation methods focus on the deep white matter (DWM), while fewer\nmethods address the superficial white matter (SWM) due to its complexity. We\npropose a deep-learning-based framework, Superficial White Matter Analysis\n(SupWMA), that performs an efficient and consistent parcellation of 198 SWM\nclusters from whole-brain tractography. A point-cloud-based network is modified\nfor our SWM parcellation task, and supervised contrastive learning enables more\ndiscriminative representations between plausible streamlines and outliers. We\nperform evaluation on a large tractography dataset with ground truth labels and\non three independently acquired testing datasets from individuals across ages\nand health conditions. Compared to several state-of-the-art methods, SupWMA\nobtains a highly consistent and accurate SWM parcellation result. In addition,\nthe computational speed of SupWMA is much faster than other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1\">Tengfei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuqian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makris_N/0/1/0/all/0/1\">Nikos Makris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathi_Y/0/1/0/all/0/1\">Yogesh Rathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonnell_L/0/1/0/all/0/1\">Lauren J. O&#x27;Donnell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Light field Rectification based on relative pose estimation. (arXiv:2201.12533v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12533","description":"<p>Hand-held light field (LF) cameras have unique advantages in computer vision\nsuch as 3D scene reconstruction and depth estimation. However, the related\napplications are limited by the ultra-small baseline, e.g., leading to the\nextremely low depth resolution in reconstruction. To solve this problem, we\npropose to rectify LF to obtain a large baseline. Specifically, the proposed\nmethod aligns two LFs captured by two hand-held LF cameras with a random\nrelative pose, and extracts the corresponding row-aligned sub-aperture images\n(SAIs) to obtain an LF with a large baseline. For an accurate rectification, a\nmethod for pose estimation is also proposed, where the relative rotation and\ntranslation between the two LF cameras are estimated. The proposed pose\nestimation minimizes the degree of freedom (DoF) in the LF-point-LF-point\ncorrespondence model and explicitly solves this model in a linear way. The\nproposed pose estimation outperforms the state-of-the-art algorithms by\nproviding more accurate results to support rectification. The significantly\nimproved depth resolution in 3D reconstruction demonstrates the effectiveness\nof the proposed LF rectification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huo_X/0/1/0/all/0/1\">Xiao Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Dongyang Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Saiping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fuzheng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Differentiable Matrix Square Root and Inverse Square Root. (arXiv:2201.12543v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12543","description":"<p>Computing the matrix square root and its inverse in a differentiable manner\nis important in a variety of computer vision tasks. Previous methods either\nadopt the Singular Value Decomposition (SVD) to explicitly factorize the matrix\nor use the Newton-Schulz iteration (NS iteration) to derive the approximate\nsolution. However, both methods are not computationally efficient enough in\neither the forward pass or the backward pass. In this paper, we propose two\nmore efficient variants to compute the differentiable matrix square root and\nthe inverse square root. For the forward propagation, one method is to use\nMatrix Taylor Polynomial (MTP), and the other method is to use Matrix Pad\\'e\nApproximants (MPA). The backward gradient is computed by iteratively solving\nthe continuous-time Lyapunov equation using the matrix sign function. A series\nof numerical tests show that both methods yield considerable speed-up compared\nwith the SVD or the NS iteration. Moreover, we validate the effectiveness of\nour methods in several real-world applications, including de-correlated batch\nnormalization, second-order vision transformer, global covariance pooling for\nlarge-scale and fine-grained recognition, attentive covariance pooling for\nvideo recognition, and neural style transfer. The experimental results\ndemonstrate that our methods can also achieve competitive and even slightly\nbetter performances. The Pytorch implementation is available at\n\\href{https://github.com/KingJamesSong/FastDifferentiableMatSqrt}{https://github.com/KingJamesSong/FastDifferentiableMatSqrt}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The KFIoU Loss for Rotated Object Detection. (arXiv:2201.12558v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12558","description":"<p>Differing from the well-developed horizontal object detection area whereby\nthe computing-friendly IoU based loss is readily adopted and well fits with the\ndetection metrics. In contrast, rotation detectors often involve a more\ncomplicated loss based on SkewIoU which is unfriendly to gradient-based\ntraining. In this paper, we argue that one effective alternative is to devise\nan approximate loss who can achieve trend-level alignment with SkewIoU loss\ninstead of the strict value-level identity. Specifically, we model the objects\nas Gaussian distribution and adopt Kalman filter to inherently mimic the\nmechanism of SkewIoU by its definition, and show its alignment with the SkewIoU\nat trend-level. This is in contrast to recent Gaussian modeling based rotation\ndetectors e.g. GWD, KLD that involves a human-specified distribution distance\nmetric which requires additional hyperparameter tuning. The resulting new loss\ncalled KFIoU is easier to implement and works better compared with exact\nSkewIoU, thanks to its full differentiability and ability to handle the\nnon-overlapping cases. We further extend our technique to the 3-D case which\nalso suffers from the same issues as 2-D detection. Extensive results on\nvarious public datasets (2-D/3-D, aerial/text/face images) with different base\ndetectors show the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gefan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jitui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wentao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Balanced Batch Normalization for Exemplar-based Class-Incremental Learning. (arXiv:2201.12559v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12559","description":"<p>Batch Normalization (BN) is an essential layer for training neural network\nmodels in various computer vision tasks. It has been widely used in continual\nlearning scenarios with little discussion, but we find that BN should be\ncarefully applied, particularly for the exemplar memory based class incremental\nlearning (CIL). We first analyze that the empirical mean and variance obtained\nfor normalization in a BN layer become highly biased toward the current task.\nTo tackle its significant problems in training and test phases, we propose\nTask-Balanced Batch Normalization (TBBN). Given each mini-batch imbalanced\nbetween the current and previous tasks, TBBN first reshapes and repeats the\nbatch, calculating near task-balanced mean and variance. Second, we show that\nwhen the affine transformation parameters of BN are learned from a reshaped\nfeature map, they become less-biased toward the current task. Based on our\nextensive CIL experiments with CIFAR-100 and ImageNet-100 datasets, we\ndemonstrate that our TBBN is easily applicable to most of existing\nexemplar-based CIL algorithms, improving their performance by decreasing the\nforgetting on the previous tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sungmin Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Soonwon Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Moontae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale-arbitrary Invertible Image Downscaling. (arXiv:2201.12576v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12576","description":"<p>Downscaling is indispensable when distributing high-resolution (HR) images\nover the Internet to fit the displays of various resolutions, while upscaling\nis also necessary when users want to see details of the distributed images.\nRecent invertible image downscaling methods jointly model these two problems\nand achieve significant improvements. However, they only consider fixed integer\nscale factors that cannot meet the requirement of conveniently fitting the\ndisplays of various resolutions in real-world applications. In this paper, we\npropose a scale-Arbitrary Invertible image Downscaling Network (AIDN), to\nnatively downscale HR images with arbitrary scale factors for fitting various\ntarget resolutions. Meanwhile, the HR information is embedded in the downscaled\nlow-resolution (LR) counterparts in a nearly imperceptible form such that our\nAIDN can also restore the original HR images solely from the LR images. The key\nto supporting arbitrary scale factors is our proposed Conditional Resampling\nModule (CRM) that conditions the downscaling/upscaling kernels and sampling\nlocations on both scale factors and image content. Extensive experimental\nresults demonstrate that our AIDN achieves top performance for invertible\ndownscaling with both arbitrary integer and non-integer scale factors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1\">Jinbo Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1\">Tien-Tsin Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Matrix-Encoding Method for Privacy-Preserving Neural Networks (Inference). (arXiv:2201.12577v1 [cs.CR])","link":"http://arxiv.org/abs/2201.12577","description":"<p>In this work, we present $\\texttt{Volley Revolver}$, a novel matrix-encoding\nmethod that is particularly convenient for privacy-preserving neural networks\nto make predictions, and use it to implement a CNN for handwritten image\nclassification. Based on this encoding method, we develop several additional\noperations for putting into practice the secure matrix multiplication over\nencrypted data matrices. For two matrices $A$ and $B$ to perform multiplication\n$A \\times B$, the main idea is, in a simple version, to encrypt matrix $A$ and\nthe transposition of the matrix $B$ into two ciphertexts respectively. Along\nwith the additional operations, the homomorphic matrix multiplication $A \\times\nB$ can be calculated over encrypted data matrices efficiently. For the\nconvolution operation in CNN, on the basis of the $\\texttt{Volley Revolver}$\nencoding method, we develop a feasible and efficient evaluation strategy for\nperforming the convolution operation. We in advance span each convolution\nkernel of CNN to a matrix space of the same size as the input image so as to\ngenerate several ciphertexts, each of which is later used together with the\ninput image for calculating some part of the final convolution result. We\naccumulate all these part results of convolution operation and thus obtain the\nfinal convolution result.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_J/0/1/0/all/0/1\">John Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedMed-ATL: Misaligned Unpaired Brain Image Synthesis via Affine Transform Loss. (arXiv:2201.12589v1 [eess.IV])","link":"http://arxiv.org/abs/2201.12589","description":"<p>The existence of completely aligned and paired multi-modal neuroimaging data\nhas proved its effectiveness in the diagnosis of brain diseases. However,\ncollecting the full set of well-aligned and paired data is impractical or even\nluxurious, since the practical difficulties may include high cost, long time\nacquisition, image corruption, and privacy issues. Previously, the misaligned\nunpaired neuroimaging data (termed as MUD) are generally treated as noisy\nlabel. However, such a noisy label-based method could not work very well when\nmisaligned data occurs distortions severely, for example, different angles of\nrotation. In this paper, we propose a novel federated self-supervised learning\n(FedMed) for brain image synthesis. An affine transform loss (ATL) was\nformulated to make use of severely distorted images without violating privacy\nlegislation for the hospital. We then introduce a new data augmentation\nprocedure for self-supervised training and fed it into three auxiliary heads,\nnamely auxiliary rotation, auxiliary translation, and auxiliary scaling heads.\nThe proposed method demonstrates advanced performance in both the quality of\nsynthesized results under a severely misaligned and unpaired data setting, and\nbetter stability than other GAN-based algorithms. The proposed method also\nreduces the demand for deformable registration while encouraging to realize the\nusage of those misaligned and unpaired data. Experimental results verify the\noutstanding ability of our learning paradigm compared to other state-of-the-art\napproaches. Our code is available on the website:\nhttps://github.com/FedMed-Meta/FedMed-ATL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xie_G/0/1/0/all/0/1\">Guoyang Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jinbao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yawen Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_Y/0/1/0/all/0/1\">Yaochu Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exact Decomposition of Joint Low Rankness and Local Smoothness Plus Sparse Matrices. (arXiv:2201.12592v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12592","description":"<p>It is known that the decomposition in low-rank and sparse matrices\n(\\textbf{L+S} for short) can be achieved by several Robust PCA techniques.\nBesides the low rankness, the local smoothness (\\textbf{LSS}) is a vitally\nessential prior for many real-world matrix data such as hyperspectral images\nand surveillance videos, which makes such matrices have low-rankness and local\nsmoothness properties at the same time. This poses an interesting question: Can\nwe make a matrix decomposition in terms of \\textbf{L\\&amp;LSS +S } form exactly? To\naddress this issue, we propose in this paper a new RPCA model based on\nthree-dimensional correlated total variation regularization (3DCTV-RPCA for\nshort) by fully exploiting and encoding the prior expression underlying such\njoint low-rank and local smoothness matrices. Specifically, using a\nmodification of Golfing scheme, we prove that under some mild assumptions, the\nproposed 3DCTV-RPCA model can decompose both components exactly, which should\nbe the first theoretical guarantee among all such related methods combining low\nrankness and local smoothness. In addition, by utilizing Fast Fourier Transform\n(FFT), we propose an efficient ADMM algorithm with a solid convergence\nguarantee for solving the resulting optimization problem. Finally, a series of\nexperiments on both simulations and real applications are carried out to\ndemonstrate the general validity of the proposed 3DCTV-RPCA model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jiangjun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MVP: Multi-Stage Vision-Language Pre-Training via Multi-Level Semantic Alignment. (arXiv:2201.12596v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12596","description":"<p>In this paper, we propose a Multi-stage Vision-language Pre-training (MVP)\nframework to learn cross-modality representation via multi-level semantic\nalignment. We introduce concepts in both modalities to construct two-level\nsemantic representations for language and vision. Based on the multi-level\ninput, we train the cross-modality model in two stages, namely, uni-modal\nlearning and cross-modal learning. The former stage enforces within-modality\ninteractions to learn multi-level semantics for each single modality. The\nlatter stage enforces interactions across modalities via both coarse-grain and\nfine-grain semantic alignment tasks. Image-text matching and masked language\nmodeling are then used to further optimize the pre-training model. Our model\ngenerates the-state-of-the-art results on several vision and language tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zejun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhihao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1\">Huaixiao Tou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-assisted image compression. (arXiv:2201.12599v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12599","description":"<p>Conventional image compression methods typically aim at pixel-level\nconsistency while ignoring the performance of downstream AI tasks.To solve this\nproblem, this paper proposes a Semantic-Assisted Image Compression method\n(SAIC), which can maintain semantic-level consistency to enable high\nperformance of downstream AI tasks.To this end, we train the compression\nnetwork using semantic-level loss function. In particular, semantic-level loss\nis measured using gradient-based semantic weights mechanism (GSW). GSW directly\nconsider downstream AI tasks' perceptual results. Then, this paper proposes a\nsemantic-level distortion evaluation metric to quantify the amount of semantic\ninformation retained during the compression process. Experimental results show\nthat the proposed SAIC method can retain more semantic-level information and\nachieve better performance of downstream AI tasks compared to the traditional\ndeep learning-based method and the advanced perceptual method at the same\ncompression ratio.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qizheng Sun</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Caili Guo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiujiu Chen</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xijun Xue</a> (2) ((1) bupt.edu.cn, (2) chinatelecom.cn )"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System. (arXiv:2201.12604v1 [cs.LG])","link":"http://arxiv.org/abs/2201.12604","description":"<p>Humans excel at continually learning from an ever-changing environment\nwhereas it remains a challenge for deep neural networks which exhibit\ncatastrophic forgetting. The complementary learning system (CLS) theory\nsuggests that the interplay between rapid instance-based learning and slow\nstructured learning in the brain is crucial for accumulating and retaining\nknowledge. Here, we propose CLS-ER, a novel dual memory experience replay (ER)\nmethod which maintains short-term and long-term semantic memories that interact\nwith the episodic memory. Our method employs an effective replay mechanism\nwhereby new knowledge is acquired while aligning the decision boundaries with\nthe semantic memories. CLS-ER does not utilize the task boundaries or make any\nassumption about the distribution of the data which makes it versatile and\nsuited for \"general continual learning\". Our approach achieves state-of-the-art\nperformance on standard benchmarks as well as more realistic general continual\nlearning settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarfraz_F/0/1/0/all/0/1\">Fahad Sarfraz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hand Gesture Recognition of Dumb Person Using one Against All Neural Network. (arXiv:2201.12622v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12622","description":"<p>We propose a new technique for recognition of dumb person hand gesture in\nreal world environment. In this technique, the hand image containing the\ngesture is preprocessed and then hand region is segmented by convergent the RGB\ncolor image to L.a.b color space. Only few statistical features are used to\nclassify the segmented image to different classes. Artificial Neural Network is\ntrained in sequential manner using one against all. When the system gets\ntrained, it becomes capable of recognition of each class in parallel manner.\nThe result of proposed technique is much better than existing techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Muhammad Asim Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Sajjad Ahmed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADC-Net: An Open-Source Deep Learning Network for Automated Dispersion Compensation in Optical Coherence Tomography. (arXiv:2201.12625v1 [eess.IV])","link":"http://arxiv.org/abs/2201.12625","description":"<p>Chromatic dispersion is a common problem to degrade the system resolution in\noptical coherence tomography (OCT). This study is to develop a deep learning\nnetwork for automated dispersion compensation (ADC-Net) in OCT. The ADC-Net is\nbased on a redesigned UNet architecture which employs an encoder-decoder\npipeline. The input section encompasses partially compensated OCT B-scans with\nindividual retinal layers optimized. Corresponding output is a fully\ncompensated OCT B-scans with all retinal layers optimized. Two numeric\nparameters, i.e., peak signal to noise ratio (PSNR) and structural similarity\nindex metric computed at multiple scales (MS-SSIM), were used for objective\nassessment of the ADC-Net performance. Comparative analysis of training models,\nincluding single, three, five, seven and nine input channels were implemented.\nThe five-input channels implementation was observed as the optimal mode for\nADC-Net training to achieve robust dispersion compensation in OCT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ahmed_S/0/1/0/all/0/1\">Shaiban Ahmed</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Le_D/0/1/0/all/0/1\">David Le</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Son_T/0/1/0/all/0/1\">Taeyoon Son</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Adejumo_T/0/1/0/all/0/1\">Tobiloba Adejumo</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Yao_X/0/1/0/all/0/1\">Xincheng Yao</a> (1,2) (1) <a href=\"http://arxiv.org/find/eess/1/au:+Engineering_D/0/1/0/all/0/1\">Department of Biomedical Engineering</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chicago_U/0/1/0/all/0/1\">University of Illinois at Chicago</a> (2) <a href=\"http://arxiv.org/find/eess/1/au:+Ophthalmology_D/0/1/0/all/0/1\">Department of Ophthalmology</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Science_V/0/1/0/all/0/1\">Visual Science</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chicago_U/0/1/0/all/0/1\">University of Illinois at Chicago</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Cross-dataset Generalization of Pedestrian Crossing Predictors. (arXiv:2201.12626v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12626","description":"<p>Pedestrian crossing prediction has been a topic of active research, resulting\nin many new algorithmic solutions. While measuring the overall progress of\nthose solutions over time tends to be more and more established due to the new\npublicly available benchmark and standardized evaluation procedures, knowing\nhow well existing predictors react to unseen data remains an unanswered\nquestion. This evaluation is imperative as serviceable crossing behavior\npredictors should be set to work in various scenarii without compromising\npedestrian safety due to misprediction. To this end, we conduct a study based\non direct cross-dataset evaluation. Our experiments show that current\nstate-of-the-art pedestrian behavior predictors generalize poorly in\ncross-dataset evaluation scenarii, regardless of their robustness during a\ndirect training-test set evaluation setting. In the light of what we observe,\nwe argue that the future of pedestrian crossing prediction, e.g. reliable and\ngeneralizable implementations, should not be about tailoring models, trained\nwith very little available data, and tested in a classical train-test scenario\nwith the will to infer anything about their behavior in real life. It should be\nabout evaluating models in a cross-dataset setting while considering their\nuncertainty estimates under domain shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gesnouin_J/0/1/0/all/0/1\">Joseph Gesnouin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechberti_S/0/1/0/all/0/1\">Steve Pechberti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanciulescu_B/0/1/0/all/0/1\">Bogdan Stanciulescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1\">Fabien Moutarde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Classification using Graph Neural Network and Multiscale Wavelet Superpixels. (arXiv:2201.12633v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12633","description":"<p>Prior studies using graph neural networks (GNNs) for image classification\nhave focused on graphs generated from a regular grid of pixels or similar-sized\nsuperpixels. In the latter, a single target number of superpixels is defined\nfor an entire dataset irrespective of differences across images and their\nintrinsic multiscale structure. On the contrary, this study investigates image\nclassification using graphs generated from an image-specific number of\nmultiscale superpixels. We propose WaveMesh, a new wavelet-based superpixeling\nalgorithm, where the number and sizes of superpixels in an image are\nsystematically computed based on its content. WaveMesh superpixel graphs are\nstructurally different from similar-sized superpixel graphs. We use SplineCNN,\na state-of-the-art network for image graph classification, to compare WaveMesh\nand similar-sized superpixels. Using SplineCNN, we perform extensive\nexperiments on three benchmark datasets under three local-pooling settings: 1)\nno pooling, 2) GraclusPool, and 3) WavePool, a novel spatially heterogeneous\npooling scheme tailored to WaveMesh superpixels. Our experiments demonstrate\nthat SplineCNN learns from multiscale WaveMesh superpixels on-par with\nsimilar-sized superpixels. In all WaveMesh experiments, GraclusPool performs\npoorer than no pooling / WavePool, indicating that poor choice of pooling can\nresult in inferior performance while learning from multiscale superpixels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1\">Varun Vasudevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bassenne_M/0/1/0/all/0/1\">Maxime Bassenne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Tauhidul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Lei Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self Semi Supervised Neural Architecture Search for Semantic Segmentation. (arXiv:2201.12646v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12646","description":"<p>In this paper, we propose a Neural Architecture Search strategy based on self\nsupervision and semi-supervised learning for the task of semantic segmentation.\nOur approach builds an optimized neural network (NN) model for this task by\njointly solving a jigsaw pretext task discovered with self-supervised learning\nover unlabeled training data, and, exploiting the structure of the unlabeled\ndata with semi-supervised learning. The search of the architecture of the NN\nmodel is performed by dynamic routing using a gradient descent algorithm.\nExperiments on the Cityscapes and PASCAL VOC 2012 datasets demonstrate that the\ndiscovered neural network is more efficient than a state-of-the-art\nhand-crafted NN model with four times less floating operations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pauletto_L/0/1/0/all/0/1\">Lo&#xef;c Pauletto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_M/0/1/0/all/0/1\">Massih-Reza Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winckler_N/0/1/0/all/0/1\">Nicolas Winckler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Learning for Estimation of Pendubot Angular Position Using Deep Neural Networks. (arXiv:2201.12649v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12649","description":"<p>In this paper, a machine learning based approach is introduced to estimate\nPendubot angular position from its captured images. Initially, a baseline\nalgorithm is introduced to estimate the angle using conventional image\nprocessing technique. The baseline algorithm performs well for the cases that\nthe Pendubot is not moving fast. However, when moving quickly due to a free\nfall, the Pendubot appears as a blurred object in the captured image in a way\nthat the baseline algorithm fails to estimate the angle. Consequently, a Deep\nNeural Network (DNN) based algorithm is introduced to cope with this challenge.\nThe approach relies on the concept of transfer learning to allow the training\nof the DNN on a very small fine-tuning dataset. The base algorithm is used to\ncreate the ground truth labels of the fine-tuning dataset. Experimental results\non the held-out evaluation set show that the proposed approach achieves a\nmedian absolute error of 0.02 and 0.06 degrees for the sharp and blurry images\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khanagha_S/0/1/0/all/0/1\">Sina Khanagha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Stochastic Bundle Method for Interpolating Networks. (arXiv:2201.12678v1 [cs.LG])","link":"http://arxiv.org/abs/2201.12678","description":"<p>We propose a novel method for training deep neural networks that are capable\nof interpolation, that is, driving the empirical loss to zero. At each\niteration, our method constructs a stochastic approximation of the learning\nobjective. The approximation, known as a bundle, is a pointwise maximum of\nlinear functions. Our bundle contains a constant function that lower bounds the\nempirical loss. This enables us to compute an automatic adaptive learning rate,\nthereby providing an accurate solution. In addition, our bundle includes linear\napproximations computed at the current iterate and other linear estimates of\nthe DNN parameters. The use of these additional approximations makes our method\nsignificantly more robust to its hyperparameters. Based on its desirable\nempirical properties, we term our method Bundle Optimisation for Robust and\nAccurate Training (BORAT). In order to operationalise BORAT, we design a novel\nalgorithm for optimising the bundle approximation efficiently at each\niteration. We establish the theoretical convergence of BORAT in both convex and\nnon-convex settings. Using standard publicly available data sets, we provide a\nthorough comparison of BORAT to other single hyperparameter optimisation\nalgorithms. Our experiments demonstrate BORAT matches the state-of-the-art\ngeneralisation performance for these methods and is the most robust.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paren_A/0/1/0/all/0/1\">Alasdair Paren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrada_L/0/1/0/all/0/1\">Leonard Berrada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poudel_R/0/1/0/all/0/1\">Rudra P. K. Poudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">M. Pawan Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Contrastive Learning is Provably (almost) Principal Component Analysis. (arXiv:2201.12680v1 [cs.LG])","link":"http://arxiv.org/abs/2201.12680","description":"<p>We show that Contrastive Learning (CL) under a family of loss functions\n(including InfoNCE) has a game-theoretical formulation, where the \\emph{max\nplayer} finds representation to maximize contrastiveness, and the \\emph{min\nplayer} puts weights on pairs of samples with similar representation. We show\nthat the max player who does \\emph{representation learning} reduces to\nPrincipal Component Analysis for deep linear network, and almost all local\nminima are global, recovering optimal PCA solutions. Experiments show that the\nformulation yields comparable (or better) performance on CIFAR10 and STL-10\nwhen extending beyond InfoNCE, yielding novel contrastive losses. Furthermore,\nwe extend our theoretical analysis to 2-layer ReLU networks, showing its\ndifference from linear ones, and proving that feature composition is preferred\nover picking single dominant feature under strong augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Built Environment Features for Planning Research with Computer Vision: A Review and Discussion of State-of-the-Art Approaches. (arXiv:2201.12693v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12693","description":"<p>This is an extended abstract for a presentation at The 17th International\nConference on CUPUM - Computational Urban Planning and Urban Management in June\n2021. This study presents an interdisciplinary synthesis of the\nstate-of-the-art approaches in computer vision technologies to extract built\nenvironment features that could improve the robustness of empirical research in\nplanning. We discussed the findings from the review of studies in both planning\nand computer science.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Meiqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_H/0/1/0/all/0/1\">Hao Sheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Robust Framework for Deep Learning Approaches to Facial Emotion Recognition and Evaluation. (arXiv:2201.12705v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12705","description":"<p>Facial emotion recognition is a vast and complex problem space within the\ndomain of computer vision and thus requires a universally accepted baseline\nmethod with which to evaluate proposed models. While test datasets have served\nthis purpose in the academic sphere real world application and testing of such\nmodels lacks any real comparison. Therefore we propose a framework in which\nmodels developed for FER can be compared and contrasted against one another in\na constant standardized fashion. A lightweight convolutional neural network is\ntrained on the AffectNet dataset a large variable dataset for facial emotion\nrecognition and a web application is developed and deployed with our proposed\nframework as a proof of concept. The CNN is embedded into our application and\nis capable of instant real time facial emotion recognition. When tested on the\nAffectNet test set this model achieves high accuracy for emotion classification\nof eight different emotions. Using our framework the validity of this model and\nothers can be properly tested by evaluating a model efficacy not only based on\nits accuracy on a sample test dataset, but also on in the wild experiments.\nAdditionally, our application is built with the ability to save and store any\nimage captured or uploaded to it for emotion recognition, allowing for the\ncuration of more quality and diverse facial emotion recognition datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_N/0/1/0/all/0/1\">Nyle Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_R/0/1/0/all/0/1\">Rushit Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_T/0/1/0/all/0/1\">Tyler Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reither_T/0/1/0/all/0/1\">Thomas Reither</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_D/0/1/0/all/0/1\">Dylan Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanson_M/0/1/0/all/0/1\">Mitchell Hanson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor Recovery Based on Tensor Equivalent Minimax-Concave Penalty. (arXiv:2201.12709v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12709","description":"<p>Tensor recovery is an important problem in computer vision and machine\nlearning. It usually uses the convex relaxation of tensor rank and $l_{0}$\nnorm, i.e., the nuclear norm and $l_{1}$ norm respectively, to solve the\nproblem. It is well known that convex approximations produce biased estimators.\nIn order to overcome this problem, a corresponding non-convex regularizer has\nbeen proposed to solve it. Inspired by matrix equivalent Minimax-Concave\nPenalty (EMCP), we propose and prove theorems of tensor equivalent\nMinimax-Concave Penalty (TEMCP). The tensor equivalent MCP (TEMCP) as a\nnon-convex regularizer and the equivalent weighted tensor $\\gamma$ norm (EWTGN)\nwhich can represent the low-rank part are obtained. Both of them can realize\nweight adaptive. At the same time, we propose two corresponding adaptive models\nfor two classical tensor recovery problems, low-rank tensor completion (LRTC)\nand tensor robust principal component analysis (TRPCA), and the optimization\nalgorithm is based on alternating direction multiplier (ADMM). This novel\niterative adaptive algorithm can produce more accurate tensor recovery effect.\nFor the tensor completion model, multispectral image (MSI), magnetic resonance\nimaging (MRI) and color video (CV) data sets are considered, while for the\ntensor robust principal component analysis model, hyperspectral image (HSI)\ndenoising under gaussian noise plus salt and pepper noise is considered. The\nproposed algorithm is superior to the state-of-arts method, and the algorithm\nis guaranteed to meet the reduction and convergence through experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hongtao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yajing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yinlin Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Win the Lottery Ticket via Fourier Analysis: Frequencies Guided Network Pruning. (arXiv:2201.12712v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12712","description":"<p>With the remarkable success of deep learning recently, efficient network\ncompression algorithms are urgently demanded for releasing the potential\ncomputational power of edge devices, such as smartphones or tablets. However,\noptimal network pruning is a non-trivial task which mathematically is an\nNP-hard problem. Previous researchers explain training a pruned network as\nbuying a lottery ticket. In this paper, we investigate the Magnitude-Based\nPruning (MBP) scheme and analyze it from a novel perspective through Fourier\nanalysis on the deep learning model to guide model designation. Besides\nexplaining the generalization ability of MBP using Fourier transform, we also\npropose a novel two-stage pruning approach, where one stage is to obtain the\ntopological structure of the pruned network and the other stage is to retrain\nthe pruned network to recover the capacity using knowledge distillation from\nlower to higher on the frequency domain. Extensive experiments on CIFAR-10 and\nCIFAR-100 demonstrate the superiority of our novel Fourier analysis based MBP\ncompared to other traditional MBP algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1\">Yuzhang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_B/0/1/0/all/0/1\">Bin Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1\">Ziliang Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Only Demonstrate Once: Category-Level Manipulation from Single Visual Demonstration. (arXiv:2201.12716v1 [cs.RO])","link":"http://arxiv.org/abs/2201.12716","description":"<p>Promising results have been achieved recently in category-level manipulation\nthat generalizes across object instances. Nevertheless, it often requires\nexpensive real-world data collection and manual specification of semantic\nkeypoints for each object category and task. Additionally, coarse keypoint\npredictions and ignoring intermediate action sequences hinder adoption in\ncomplex manipulation tasks beyond pick-and-place. This work proposes a novel,\ncategory-level manipulation framework that leverages an object-centric,\ncategory-level representation and model-free 6 DoF motion tracking. The\ncanonical object representation is learned solely in simulation and then used\nto parse a category-level, task trajectory from a single demonstration video.\nThe demonstration is reprojected to a target trajectory tailored to a novel\nobject via the canonical representation. During execution, the manipulation\nhorizon is decomposed into long-range, collision-free motion and last-inch\nmanipulation. For the latter part, a category-level behavior cloning (CatBC)\nmethod leverages motion tracking to perform closed-loop control. CatBC follows\nthe target trajectory, projected from the demonstration and anchored to a\ndynamically selected category-level coordinate frame. The frame is\nautomatically selected along the manipulation horizon by a local attention\nmechanism. This framework allows to teach different manipulation strategies by\nsolely providing a single demonstration, without complicated manual\nprogramming. Extensive experiments demonstrate its efficacy in a range of\nchallenging industrial tasks in high-precision assembly, which involve learning\ncomplex, long-horizon policies. The process exhibits robustness against\nuncertainty due to dynamics as well as generalization across object instances\nand scene configurations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bowen Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_W/0/1/0/all/0/1\">Wenzhao Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekris_K/0/1/0/all/0/1\">Kostas Bekris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaal_S/0/1/0/all/0/1\">Stefan Schaal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VC-GPT: Visual Conditioned GPT for End-to-End Generative Vision-and-Language Pre-training. (arXiv:2201.12723v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12723","description":"<p>Vision-and-language pre-training models (VLMs) have achieved tremendous\nsuccess in the cross-modal area, but most of them require millions of parallel\nimage-caption data for pre-training. Collating such data is expensive and\nlabor-intensive. In this work, we focus on reducing such need for generative\nvision-and-language pre-training (G-VLP) by taking advantage of the visual\npre-trained model (CLIP-ViT) as encoder and language pre-trained model (GPT2)\nas decoder. Unfortunately, GPT2 lacks a necessary cross-attention module, which\nhinders the direct connection of CLIP-ViT and GPT2. To remedy such defects, we\nconduct extensive experiments to empirically investigate how to design and\npre-train our model. Based on our experimental results, we propose a novel\nG-VLP framework, Visual Conditioned GPT (VC-GPT), and pre-train it with a\nsmall-scale parallel image-caption corpus (Visual Genome, only 110k distinct\nimages). Evaluating on the image captioning downstream tasks (MSCOCO and\nFlickr30k Captioning), VC-GPT achieves either the best or the second-best\nperformance across all evaluation metrics over the previous works which consume\naround 30 times more parallel data during pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yadong Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Architecture Ranker. (arXiv:2201.12725v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12725","description":"<p>Architecture ranking has recently been advocated to design an efficient and\neffective performance predictor for Neural Architecture Search (NAS). The\nprevious contrastive method solves the ranking problem by comparing pairs of\narchitectures and predicting their relative performance, which may suffer\ngeneralization issues due to local pair-wise comparison. Inspired by the\nquality stratification phenomenon in the search space, we propose a predictor,\nnamely Neural Architecture Ranker (NAR), from a new and global perspective by\nexploiting the quality distribution of the whole search space. The NAR learns\nthe similar characteristics of the same quality tier (i.e., level) and\ndistinguishes among different individuals by first matching architectures with\nthe representation of tiers, and then classifying and scoring them. It can\ncapture the features of different quality tiers and thus generalize its ranking\nability to the entire search space. Besides, distributions of different quality\ntiers are also beneficial to guide the sampling procedure, which is free of\ntraining a search algorithm and thus simplifies the NAS pipeline. The proposed\nNAR achieves better performance than the state-of-the-art methods on two widely\naccepted datasets. On NAS-Bench-101, it finds the architectures with top\n0.01$\\unicode{x2030}$ performance among the search space and stably focuses on\nthe top architectures. On NAS-Bench-201, it identifies the optimal\narchitectures on CIFAR-10, CIFAR-100 and, ImageNet-16-120. We expand and\nrelease these two datasets covering detailed cell computational information to\nboost the study of NAS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Bicheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shibo He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1\">Peng Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-based Facial Micro-Expression Analysis: A Survey of Datasets, Features and Algorithms. (arXiv:2201.12728v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12728","description":"<p>Unlike the conventional facial expressions, micro-expressions are involuntary\nand transient facial expressions capable of revealing the genuine emotions that\npeople attempt to hide. Therefore, they can provide important information in a\nbroad range of applications such as lie detection, criminal detection, etc.\nSince micro-expressions are transient and of low intensity, however, their\ndetection and recognition is difficult and relies heavily on expert\nexperiences. Due to its intrinsic particularity and complexity, video-based\nmicro-expression analysis is attractive but challenging, and has recently\nbecome an active area of research. Although there have been numerous\ndevelopments in this area, thus far there has been no comprehensive survey that\nprovides researchers with a systematic overview of these developments with a\nunified evaluation. Accordingly, in this survey paper, we first highlight the\nkey differences between macro- and micro-expressions, then use these\ndifferences to guide our research survey of video-based micro-expression\nanalysis in a cascaded structure, encompassing the neuropsychological basis,\ndatasets, features, spotting algorithms, recognition algorithms, applications\nand evaluation of state-of-the-art approaches. For each aspect, the basic\ntechniques, advanced developments and major challenges are addressed and\ndiscussed. Furthermore, after considering the limitations of existing\nmicro-expression datasets, we present and release a new dataset - called\nmicro-and-macro expression warehouse (MMEW) - containing more video samples and\nmore labeled emotion types. We then perform a unified comparison of\nrepresentative methods on CAS(ME)2 for spotting, and on MMEW and SAMM for\nrecognition, respectively. Finally, some potential future research directions\nare explored and outlined.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ben_X/0/1/0/all/0/1\">Xianye Ben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Su-Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kpalma_K/0/1/0/all/0/1\">Kidiyo Kpalma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_W/0/1/0/all/0/1\">Weixiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong-Jin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TPC: Transformation-Specific Smoothing for Point Cloud Models. (arXiv:2201.12733v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12733","description":"<p>Point cloud models with neural network architectures have achieved great\nsuccess and have been widely used in safety-critical applications, such as\nLidar-based recognition systems in autonomous vehicles. However, such models\nare shown vulnerable against adversarial attacks which aim to apply stealthy\nsemantic transformations such as rotation and tapering to mislead model\npredictions. In this paper, we propose a transformation-specific smoothing\nframework TPC, which provides tight and scalable robustness guarantees for\npoint cloud models against semantic transformation attacks. We first categorize\ncommon 3D transformations into three categories: additive (e.g., shearing),\ncomposable (e.g., rotation), and indirectly composable (e.g., tapering), and we\npresent generic robustness certification strategies for all categories\nrespectively. We then specify unique certification protocols for a range of\nspecific semantic transformations and their compositions. Extensive experiments\non several common 3D transformations show that TPC significantly outperforms\nthe state of the art. For example, our framework boosts the certified accuracy\nagainst twisting transformation along z-axis (within 20$^\\circ$) from 20.3$\\%$\nto 83.8$\\%$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wenda Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RIM-Net: Recursive Implicit Fields for Unsupervised Learning of Hierarchical Shape Structures. (arXiv:2201.12763v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12763","description":"<p>We introduce RIM-Net, a neural network which learns recursive implicit fields\nfor unsupervised inference of hierarchical shape structures. Our network\nrecursively decomposes an input 3D shape into two parts, resulting in a binary\ntree hierarchy. Each level of the tree corresponds to an assembly of shape\nparts, represented as implicit functions, to reconstruct the input shape. At\neach node of the tree, simultaneous feature decoding and shape decomposition\nare carried out by their respective feature and part decoders, with weight\nsharing across the same hierarchy level. As an implicit field decoder, the part\ndecoder is designed to decompose a sub-shape, via a two-way branched\nreconstruction, where each branch predicts a set of parameters defining a\nGaussian to serve as a local point distribution for shape reconstruction. With\nreconstruction losses accounted for at each hierarchy level and a decomposition\nloss at each node, our network training does not require any ground-truth\nsegmentations, let alone hierarchies. Through extensive experiments and\ncomparisons to state-of-the-art alternatives, we demonstrate the quality,\nconsistency, and interpretability of hierarchical structural inference by\nRIM-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1\">Chengjie Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Corruption and Adversarial Robustness by Enhancing Weak Subnets. (arXiv:2201.12765v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12765","description":"<p>Deep neural networks have achieved great success in many computer vision\ntasks. However, deep networks have been shown to be very susceptible to\ncorrupted or adversarial images, which often result in significant performance\ndrops. In this paper, we observe that weak subnetwork (subnet) performance is\ncorrelated with a lack of robustness against corruptions and adversarial\nattacks. Based on that observation, we propose a novel robust training method\nwhich explicitly identifies and enhances weak subnets (EWS) during training to\nimprove robustness. Specifically, we develop a search algorithm to find\nparticularly weak subnets and propose to explicitly strengthen them via\nknowledge distillation from the full network. We show that our EWS greatly\nimproves the robustness against corrupted images as well as the accuracy on\nclean data. Being complementary to many state-of-the-art data augmentation\napproaches, EWS consistently improves corruption robustness on top of many of\nthese approaches. Moreover, EWS is also able to boost the adversarial\nrobustness when combined with popular adversarial training methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stutz_D/0/1/0/all/0/1\">David Stutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MVP-Net: Multiple View Pointwise Semantic Segmentation of Large-Scale Point Clouds. (arXiv:2201.12769v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12769","description":"<p>Semantic segmentation of 3D point cloud is an essential task for autonomous\ndriving environment perception. The pipeline of most pointwise point cloud\nsemantic segmentation methods includes points sampling, neighbor searching,\nfeature aggregation, and classification. Neighbor searching method like\nK-nearest neighbors algorithm, KNN, has been widely applied. However, the\ncomplexity of KNN is always a bottleneck of efficiency. In this paper, we\npropose an end-to-end neural architecture, Multiple View Pointwise Net,\nMVP-Net, to efficiently and directly infer large-scale outdoor point cloud\nwithout KNN or any complex pre/postprocessing. Instead, assumption-based\nsorting and multi-rotation of point cloud methods are introduced to point\nfeature aggregation and receptive field expanding. Numerical experiments show\nthat the proposed MVP-Net is 11 times faster than the most efficient pointwise\nsemantic segmentation method RandLA-Net and achieves the same accuracy on the\nlarge-scale benchmark SemanticKITTI dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chuanyu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Nuo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Han Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shengguang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Moving Vehicle Detection from Audio-Visual Cues. (arXiv:2201.12771v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12771","description":"<p>Robust detection of moving vehicles is a critical task for any autonomously\noperating outdoor robot or self-driving vehicle. Most modern approaches for\nsolving this task rely on training image-based detectors using large-scale\nvehicle detection datasets such as nuScenes or the Waymo Open Dataset.\nProviding manual annotations is an expensive and laborious exercise that does\nnot scale well in practice. To tackle this problem, we propose a\nself-supervised approach that leverages audio-visual cues to detect moving\nvehicles in videos. Our approach employs contrastive learning for localizing\nvehicles in images from corresponding pairs of images and recorded audio. In\nextensive experiments carried out with a real-world dataset, we demonstrate\nthat our approach provides accurate detections of moving vehicles and does not\nrequire manual annotations. We furthermore show that our model can be used as a\nteacher to supervise an audio-only detection model. This student model is\ninvariant to illumination changes and thus effectively bridges the domain gap\ninherent to models leveraging exclusively vision as the predominant modality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zurn_J/0/1/0/all/0/1\">Jannik Z&#xfc;rn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical Noise Simulation for RGB Images. (arXiv:2201.12773v1 [eess.IV])","link":"http://arxiv.org/abs/2201.12773","description":"<p>This document describes a noise generator that simulates realistic noise\nfound in smartphone cameras. The generator simulates Poissonian-Gaussian noise\nwhose parameters have been estimated on the Smartphone Image Denoising Dataset\n(SIDD). The generator is available online, and is currently being used in\ncompressed-domain denoising exploration experiments in JPEG AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Alvar_S/0/1/0/all/0/1\">Saeed Ranjbar Alvar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bajic_I/0/1/0/all/0/1\">Ivan V. Baji&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransBTSV2: Wider Instead of Deeper Transformer for Medical Image Segmentation. (arXiv:2201.12785v1 [eess.IV])","link":"http://arxiv.org/abs/2201.12785","description":"<p>Transformer, benefiting from global (long-range) information modeling using\nself-attention mechanism, has been successful in natural language processing\nand computer vision recently. Convolutional Neural Networks, capable of\ncapturing local features, are unable to model explicit long-distance\ndependencies from global feature space. However, both local and global features\nare crucial for dense prediction tasks, especially for 3D medical image\nsegmentation. In this paper, we exploit Transformer in 3D CNN for 3D medical\nimage volumetric segmentation and propose a novel network named TransBTSV2\nbased on the encoder-decoder structure. Different from our original TransBTS,\nthe proposed TransBTSV2 is not limited to brain tumor segmentation (BTS) but\nfocuses on general medical image segmentation, providing a strong and efficient\n3D baseline for volumetric segmentation of medical images. As a hybrid\nCNN-Transformer architecture, TransBTSV2 can achieve accurate segmentation of\nmedical images without any pre-training. With the proposed insight to redesign\nthe internal structure of Transformer and the introduced Deformable Bottleneck\nModule, a highly efficient architecture is achieved with superior performance.\nExtensive experimental results on four medical image datasets (BraTS 2019,\nBraTS 2020, LiTS 2017 and KiTS 2019) demonstrate that TransBTSV2 achieves\ncomparable or better results as compared to the state-of-the-art methods for\nthe segmentation of brain tumor, liver tumor as well as kidney tumor. Code is\navailable at https://github.com/Wenxuan-1119/TransBTS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiangyun Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1\">Tianxiang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zha_S/0/1/0/all/0/1\">Sen Zha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SelfRecon: Self Reconstruction Your Digital Avatar from Monocular Video. (arXiv:2201.12792v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12792","description":"<p>We propose SelfRecon, a clothed human body reconstruction method that\ncombines implicit and explicit representations to recover space-time coherent\ngeometries from a monocular self-rotating human video. Explicit methods require\na predefined template mesh for a given sequence, while the template is hard to\nacquire for a specific subject. Meanwhile, the fixed topology limits the\nreconstruction accuracy and clothing types. Implicit methods support arbitrary\ntopology and have high quality due to continuous geometric representation.\nHowever, it is difficult to integrate multi-frame information to produce a\nconsistent registration sequence for downstream applications. We propose to\ncombine the advantages of both representations. We utilize differential mask\nloss of the explicit mesh to obtain the coherent overall shape, while the\ndetails on the implicit surface are refined with the differentiable neural\nrendering. Meanwhile, the explicit mesh is updated periodically to adjust its\ntopology changes, and a consistency loss is designed to match both\nrepresentations closely. Compared with existing methods, SelfRecon can produce\nhigh-fidelity surfaces for arbitrary clothed humans with self-supervised\noptimization. Extensive experimental results demonstrate its effectiveness on\nreal captured monocular videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Boyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Similarity and Generalization: From Noise to Corruption. (arXiv:2201.12803v1 [cs.LG])","link":"http://arxiv.org/abs/2201.12803","description":"<p>Contrastive learning aims to extract distinctive features from data by\nfinding an embedding representation where similar samples are close to each\nother, and different ones are far apart. We study generalization in contrastive\nlearning, focusing on its simplest representative: Siamese Neural Networks\n(SNNs). We show that Double Descent also appears in SNNs and is exacerbated by\nnoise. We point out that SNNs can be affected by two distinct sources of noise:\nPair Label Noise (PLN) and Single Label Noise (SLN). The effect of SLN is\nasymmetric, but it preserves similarity relations, while PLN is symmetric but\nbreaks transitivity. We show that the dataset topology crucially affects\ngeneralization. While sparse datasets show the same performances under SLN and\nPLN for an equal amount of noise, SLN outperforms PLN in the overparametrized\nregion in dense datasets. Indeed, in this regime, PLN similarity violation\nbecomes macroscopical, corrupting the dataset to the point where complete\noverfitting cannot be achieved. We call this phenomenon Density-Induced Break\nof Similarity (DIBS). We also probe the equivalence between online optimization\nand offline generalization for similarity tasks. We observe that an\nonline/offline correspondence in similarity learning can be affected by both\nthe network architecture and label noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_N/0/1/0/all/0/1\">Nayara Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guidetti_V/0/1/0/all/0/1\">Veronica Guidetti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Segmentation of Left Ventricle in Cardiac Magnetic Resonance Images. (arXiv:2201.12805v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12805","description":"<p>Segmentation of the left ventricle in cardiac magnetic resonance imaging MRI\nscans enables cardiologists to calculate the volume of the left ventricle and\nsubsequently its ejection fraction. The ejection fraction is a measurement that\nexpresses the percentage of blood leaving the heart with each contraction.\nCardiologists often use ejection fraction to determine one's cardiac function.\nWe propose multiscale template matching technique for detection and an\nelliptical active disc for automated segmentation of the left ventricle in MR\nimages. The elliptical active disc optimizes the local energy function with\nrespect to its five free parameters which define the disc. Gradient descent is\nused to minimize the energy function along with Green's theorem to optimize the\ncomputation expenses. We report validations on 320 scans containing 5,273\nannotated slices which are publicly available through the Multi-Centre,\nMulti-Vendor, and Multi-Disease Cardiac Segmentation (M&amp;Ms) Challenge. We\nachieved successful localization of the left ventricle in 89.63% of the cases\nand a Dice coefficient of 0.873 on diastole slices and 0.770 on systole slices.\nThe proposed technique is based on traditional image processing techniques with\na performance on par with the deep learning techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chhabra_G/0/1/0/all/0/1\">Garvit Chhabra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gagan_J/0/1/0/all/0/1\">J. H. Gagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_J/0/1/0/all/0/1\">J. R. Harish Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning from Demonstrations. (arXiv:2201.12813v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12813","description":"<p>This paper presents a framework for learning visual representations from\nunlabeled video demonstrations captured from multiple viewpoints. We show that\nthese representations are applicable for imitating several robotic tasks,\nincluding pick and place. We optimize a recently proposed self-supervised\nlearning algorithm by applying contrastive learning to enhance task-relevant\ninformation while suppressing irrelevant information in the feature embeddings.\nWe validate the proposed method on the publicly available Multi-View Pouring\nand a custom Pick and Place data sets and compare it with the TCN triplet\nbaseline. We evaluate the learned representations using three metrics:\nviewpoint alignment, stage classification and reinforcement learning, and in\nall cases the results improve when compared to state-of-the-art approaches,\nwith the added benefit of reduced number of training iterations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Correia_A/0/1/0/all/0/1\">Andr&#xe9; Correia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexandre_L/0/1/0/all/0/1\">Lu&#xed;s A. Alexandre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Gradient-driven Criteria in Network Sparsity: Gradient is All You Need. (arXiv:2201.12826v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12826","description":"<p>Network sparsity receives popularity mostly due to its capability to reduce\nthe network complexity. Extensive studies excavate gradient-driven sparsity.\nTypically, these methods are constructed upon premise of weight independence,\nwhich however, is contrary to the fact that weights are mutually influenced.\nThus, their performance remains to be improved. In this paper, we propose to\nfurther optimize gradient-driven sparsity (OptG) by solving this independence\nparadox. Our motive comes from the recent advances on supermask training which\nshows that sparse subnetworks can be located in a randomly initialized network\nby simply updating mask values without modifying any weight. We prove that\nsupermask training is to accumulate the weight gradients and can partly solve\nthe independence paradox. Consequently, OptG integrates supermask training into\ngradient-driven sparsity, and a specialized mask optimizer is designed to solve\nthe independence paradox. Experiments show that OptG can well surpass many\nexisting state-of-the-art competitors. Our code is available at\n\\url{https://github.com/zyxxmu/OptG}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mengzhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zihan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comprehensive Saliency Fusion for Object Co-segmentation. (arXiv:2201.12828v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12828","description":"<p>Object co-segmentation has drawn significant attention in recent years,\nthanks to its clarity on the expected foreground, the shared object in a group\nof images. Saliency fusion has been one of the promising ways to carry it out.\nHowever, prior works either fuse saliency maps of the same image or saliency\nmaps of different images to extract the expected foregrounds. Also, they rely\non hand-crafted saliency extraction and correspondence processes in most cases.\nThis paper revisits the problem and proposes fusing saliency maps of both the\nsame image and different images. It also leverages advances in deep learning\nfor the saliency extraction and correspondence processes. Hence, we call it\ncomprehensive saliency fusion. Our experiments reveal that our approach\nachieves much-improved object co-segmentation results compared to prior works\non important benchmark datasets such as iCoseg, MSRC, and Internet Images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chhabra_H/0/1/0/all/0/1\">Harshit Singh Chhabra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jerripothula_K/0/1/0/all/0/1\">Koteswar Rao Jerripothula</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Infrared and visible image fusion using Latent Low-Rank Representation. (arXiv:1804.08992v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1804.08992","description":"<p>Infrared and visible image fusion is an important problem in the field of\nimage fusion which has been applied widely in many fields. To better preserve\nthe useful information from source images, in this paper, we propose a novel\nimage fusion method based on latent low-rank representation(LatLRR) which is\nsimple and effective. Firstly, the source images are decomposed into low-rank\nparts(global structure) and salient parts(local structure) by LatLRR. Then, the\nlow-rank parts are fused by weighted-average strategy to preserve more contour\ninformation. Then, the salient parts are simply fused by sum strategy which is\na efficient operation in this fusion framework. Finally, the fused image is\nobtained by combining the fused low-rank part and the fused salient part.\nCompared with other fusion methods experimentally, the proposed method has\nbetter fusion performance than state-of-the-art fusion methods in both\nsubjective and objective evaluation. The Code of our fusion method is available\nat https://github.com/hli1221/imagefusion\\_Infrared\\_visible\\_latlrr\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-focus Noisy Image Fusion using Low-Rank Representation. (arXiv:1804.09325v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1804.09325","description":"<p>Multi-focus noisy image fusion represents an important task in the field of\nimage fusion which generates a single, clear and focused image from all source\nimages. In this paper, we propose a novel multi-focus noisy image fusion method\nbased on low-rank representation (LRR) which is a powerful tool in\nrepresentation learning. A multi-scale transform framework is adopted in which\nsource images are decomposed into low frequency and high frequency\ncoefficients, respectively. For low frequency coefficients, the fused low\nfrequency coefficients are determined by a spatial frequency strategy, while\nthe high frequency coefficients are fused by the LRR-based fusion strategy.\nFinally, the fused image is reconstructed by inverse multi-scale transforms\nwith fused coefficients. Experimental results demonstrate that the proposed\nalgorithm offers state-of-the-art performance even when the source images\ncontain noise. The Code of our fusion method is available at\nhttps://github.com/hli1221/imagefusion_noisy_lrr\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_T/0/1/0/all/0/1\">Tariq Durrani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optical Flow Techniques for Facial Expression Analysis -- a Practical Evaluation Study. (arXiv:1904.11592v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1904.11592","description":"<p>Optical flow techniques are becoming increasingly performant and robust when\nestimating motion in a scene, but their performance has yet to be proven in the\narea of facial expression recognition. In this work, a variety of optical flow\napproaches are evaluated across multiple facial expression datasets, so as to\nprovide a consistent performance evaluation. The aim of this work is not to\npropose a new expression recognition technique, but to understand better the\nadequacy of existing state-of-the art optical flow for encoding facial motion\nin the context of facial expression recognition. Our evaluations highlight the\nfact that motion approximation methods used to overcome motion discontinuities\nhave a significant impact when optical flows are used to characterize facial\nexpressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Allaert_B/0/1/0/all/0/1\">Benjamin Allaert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ward_I/0/1/0/all/0/1\">Isaac Ronald Ward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilasco_I/0/1/0/all/0/1\">Ioan Marius Bilasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Djeraba_C/0/1/0/all/0/1\">Chaabane Djeraba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-focus Image Fusion Based on Similarity Characteristics. (arXiv:1912.07959v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1912.07959","description":"<p>A novel multi-focus image fusion algorithm performed in spatial domain based\non similarity characteristics is proposed incorporating with region\nsegmentation. In this paper, a new similarity measure is developed based on the\nstructural similarity (SSIM) index, which is more suitable for multi-focus\nimage segmentation. Firstly, the SSNSIM map is calculated between two input\nimages. Then we segment the SSNSIM map using watershed method, and merge the\nsmall homogeneous regions with fuzzy c-means clustering algorithm (FCM). For\nthree source images, a joint region segmentation method based on segmentation\nof two images is used to obtain the final segmentation result. Finally, the\ncorresponding segmented regions of the source images are fused according to\ntheir average gradient. The performance of the image fusion method is evaluated\nby several criteria including spatial frequency, average gradient, entropy,\nedge retention etc. The evaluation results indicate that the proposed method is\neffective and has good visual perception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya-Qiong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved dual channel pulse coupled neural network and its application to multi-focus image fusion. (arXiv:2002.01102v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2002.01102","description":"<p>This paper presents an improved dual channel pulse coupled neural network\n(IDC-PCNN) model for image fusion. The model can overcome some defects of\nstandard PCNN model. In this fusion scheme, the multiplication rule is replaced\nby addition rule in the information fusion pool of dual channel PCNN (DC-PCNN)\nmodel. Meanwhile the sum of modified Laplacian (SML) measure is adopted, which\nis better than other focus measures. This method not only inherits the good\ncharacteristics of the standard PCNN model but also enhances the computing\nefficiency and fusion quality. The performance of the proposed method is\nevaluated by using four criteria including average cross entropy, root mean\nsquare error, peak value signal to noise ratio and structure similarity index.\nComparative studies show that the proposed fusion algorithm outperforms the\nstandard PCNN method and the DC-PCNN method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Huai-Shui Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepEMD: Differentiable Earth Mover's Distance for Few-Shot Learning. (arXiv:2003.06777v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.06777","description":"<p>In this work, we develop methods for few-shot image classification from a new\nperspective of optimal matching between image regions. We employ the Earth\nMover's Distance (EMD) as a metric to compute a structural distance between\ndense image representations to determine image relevance. The EMD generates the\noptimal matching flows between structural elements that have the minimum\nmatching cost, which is used to calculate the image distance for\nclassification. To generate the important weights of elements in the EMD\nformulation, we design a cross-reference mechanism, which can effectively\nalleviate the adverse impact caused by the cluttered background and large\nintra-class appearance variations. To implement k-shot classification, we\npropose to learn a structured fully connected layer that can directly classify\ndense image representations with the EMD. Based on the implicit function\ntheorem, the EMD can be inserted as a layer into the network for end-to-end\ntraining. Our extensive experiments validate the effectiveness of our algorithm\nwhich outperforms state-of-the-art methods by a significant margin on five\nwidely used few-shot classification benchmarks, namely, miniImageNet,\ntieredImageNet, Fewshot-CIFAR100 (FC100), Caltech-UCSD Birds-200-2011 (CUB),\nand CIFAR-FewShot (CIFAR-FS). We also demonstrate the effectiveness of our\nmethod on the image retrieval task in our experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yujun Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Increasing-Margin Adversarial (IMA) Training to Improve Adversarial Robustness of Neural Networks. (arXiv:2005.09147v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.09147","description":"<p>Convolutional neural network (CNN) has surpassed traditional methods for\nmedical image classification. However, CNN is vulnerable to adversarial attacks\nwhich may lead to disastrous consequences in medical applications. Although\nadversarial noises are usually generated by attack algorithms,\nwhite-noise-induced adversarial samples can exist, and therefore the threats\nare real. In this study, we propose a novel training method, named IMA, to\nimprove the robust-ness of CNN against adversarial noises. During training, the\nIMA method increases the margins of training samples in the input space, i.e.,\nmoving CNN decision boundaries far away from the training samples to improve\nrobustness. The IMA method is evaluated on publicly available datasets under\nstrong 100-PGD white-box adversarial attacks, and the results show that the\nproposed method significantly improved CNN classification and segmentation\naccuracy on noisy data while keeping a high accuracy on clean data. We hope our\napproach may facilitate the development of robust applications in medical\nfield.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Linhai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Liang Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can we Generalize and Distribute Private Representation Learning?. (arXiv:2010.01792v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.01792","description":"<p>We study the problem of learning representations that are private yet\ninformative, i.e., provide information about intended \"ally\" targets while\nhiding sensitive \"adversary\" attributes. We propose Exclusion-Inclusion\nGenerative Adversarial Network (EIGAN), a generalized private representation\nlearning (PRL) architecture that accounts for multiple ally and adversary\nattributes unlike existing PRL solutions. While centrally-aggregated dataset is\na prerequisite for most PRL techniques, data in real-world is often siloed\nacross multiple distributed nodes unwilling to share the raw data because of\nprivacy concerns. We address this practical constraint by developing D-EIGAN,\nthe first distributed PRL method that learns representations at each node\nwithout transmitting the source data. We theoretically analyze the behavior of\nadversaries under the optimal EIGAN and D-EIGAN encoders and the impact of\ndependencies among ally and adversary tasks on the optimization objective. Our\nexperiments on various datasets demonstrate the advantages of EIGAN in terms of\nperformance, robustness, and scalability. In particular, EIGAN outperforms the\nprevious state-of-the-art by a significant accuracy margin (47% improvement),\nand D-EIGAN's performance is consistently on par with EIGAN under different\nnetwork settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azam_S/0/1/0/all/0/1\">Sheikh Shams Azam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taejin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseinalipour_S/0/1/0/all/0/1\">Seyyedali Hosseinalipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joe_Wong_C/0/1/0/all/0/1\">Carlee Joe-Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1\">Saurabh Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1\">Christopher Brinton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FSOCO: The Formula Student Objects in Context Dataset. (arXiv:2012.07139v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.07139","description":"<p>This paper presents the FSOCO dataset, a collaborative dataset for\nvision-based cone detection systems in Formula Student Driverless competitions.\nIt contains human annotated ground truth labels for both bounding boxes and\ninstance-wise segmentation masks. The data buy-in philosophy of FSOCO asks\nstudent teams to contribute to the database first before being granted access\nensuring continuous growth. By providing clear labeling guidelines and tools\nfor a sophisticated raw image selection, new annotations are guaranteed to meet\nthe desired quality. The effectiveness of the approach is shown by comparing\nprediction results of a network trained on FSOCO and its unregulated\npredecessor. The FSOCO dataset can be found at fsoco-dataset.com.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vodisch_N/0/1/0/all/0/1\">Niclas V&#xf6;disch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodel_D/0/1/0/all/0/1\">David Dodel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schotz_M/0/1/0/all/0/1\">Michael Sch&#xf6;tz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Disentanglement of Structured Representations. (arXiv:2101.04041v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.04041","description":"<p>We introduce the first metric for evaluating disentanglement at individual\nhierarchy levels of a structured latent representation. Applied to\nobject-centric generative models, this offers a systematic, unified approach to\nevaluating (i) object separation between latent slots (ii) disentanglement of\nobject properties inside individual slots (iii) disentanglement of intrinsic\nand extrinsic object properties. We theoretically show that for structured\nrepresentations, our framework gives stronger guarantees of selecting a good\nmodel than previous disentanglement metrics. Experimentally, we demonstrate\nthat viewing object compositionality as a disentanglement problem addresses\nseveral issues with prior visual metrics of object separation. As a core\ntechnical component, we present the first representation probing algorithm\nhandling slot permutation invariance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dang_Nhu_R/0/1/0/all/0/1\">Rapha&#xeb;l Dang-Nhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Automated Machine Learning from Transfer Learning. (arXiv:2103.00241v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.00241","description":"<p>In this paper, we propose a neural architecture search framework based on a\nsimilarity measure between some baseline tasks and a target task. We first\ndefine the notion of the task similarity based on the log-determinant of the\nFisher Information matrix. Next, we compute the task similarity from each of\nthe baseline tasks to the target task. By utilizing the relation between a\ntarget and a set of learned baseline tasks, the search space of architectures\nfor the target task can be significantly reduced, making the discovery of the\nbest candidates in the set of possible architectures tractable and efficient,\nin terms of GPU days. This method eliminates the requirement for training the\nnetworks from scratch for a given target task as well as introducing the bias\nin the initialization of the search space from the human domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1\">Cat P. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltani_M/0/1/0/all/0/1\">Mohammadreza Soltani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravier_R/0/1/0/all/0/1\">Robert Ravier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1\">Vahid Tarokh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simplicial Complex Representation Learning. (arXiv:2103.04046v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.04046","description":"<p>Simplicial complexes form an important class of topological spaces that are\nfrequently used in many application areas such as computer-aided design,\ncomputer graphics, and simulation. Representation learning on graphs, which are\njust 1-d simplicial complexes, has witnessed a great attention in recent years.\nHowever, there has not been enough effort to extend representation learning to\nhigher dimensional simplicial objects due to the additional complexity these\nobjects hold, especially when it comes to entire-simplicial complex\nrepresentation learning. In this work, we propose a method for simplicial\ncomplex-level representation learning that embeds a simplicial complex to a\nuniversal embedding space in a way that complex-to-complex proximity is\npreserved. Our method uses our novel geometric message passing schemes to learn\nan entire simplicial complex representation in an end-to-end fashion. We\ndemonstrate the proposed model on publicly available mesh dataset. To the best\nof our knowledge, this work presents the first method for learning simplicial\ncomplex-level representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1\">Mustafa Hajij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1\">Ghada Zamzmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papamarkou_T/0/1/0/all/0/1\">Theodore Papamarkou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maroulas_V/0/1/0/all/0/1\">Vasileios Maroulas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xuanting Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation. (arXiv:2103.17235v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.17235","description":"<p>The increase of available large clinical and experimental datasets has\ncontributed to a substantial amount of important contributions in the area of\nbiomedical image analysis. Image segmentation, which is crucial for any\nquantitative analysis, has especially attracted attention. Recent hardware\nadvancement has led to the success of deep learning approaches. However,\nalthough deep learning models are being trained on large datasets, existing\nmethods do not use the information from different learning epochs effectively.\nIn this work, we leverage the information of each training epoch to prune the\nprediction maps of the subsequent epochs. We propose a novel architecture\ncalled feedback attention network (FANet) that unifies the previous epoch mask\nwith the feature map of the current training epoch. The previous epoch mask is\nthen used to provide hard attention to the learned feature maps at different\nconvolutional layers. The network also allows to rectify the predictions in an\niterative fashion during the test time. We show that our proposed feedback\nattention model provides a substantial improvement on most segmentation metrics\ntested on seven publicly available biomedical imaging datasets demonstrating\nthe effectiveness of FANet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomar_N/0/1/0/all/0/1\">Nikhil Kumar Tomar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1\">Debesh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansen_H/0/1/0/all/0/1\">H&#xe5;vard D. Johansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansen_D/0/1/0/all/0/1\">Dag Johansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rittscher_J/0/1/0/all/0/1\">Jens Rittscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1\">Sharib Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bootstrapping Semantic Segmentation with Regional Contrast. (arXiv:2104.04465v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04465","description":"<p>We present ReCo, a contrastive learning framework designed at a regional\nlevel to assist learning in semantic segmentation. ReCo performs\nsemi-supervised or supervised pixel-level contrastive learning on a sparse set\nof hard negative pixels, with minimal additional memory footprint. ReCo is easy\nto implement, being built on top of off-the-shelf segmentation networks, and\nconsistently improves performance in both semi-supervised and supervised\nsemantic segmentation methods, achieving smoother segmentation boundaries and\nfaster convergence. The strongest effect is in semi-supervised learning with\nvery few labels. With ReCo, we achieve high-quality semantic segmentation\nmodels, requiring only 5 examples of each semantic class. Code is available at\nhttps://github.com/lorenmt/reco.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shikun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhi_S/0/1/0/all/0/1\">Shuaifeng Zhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1\">Edward Johns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Andrew J. Davison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSRF-Net: A Multi-Scale Residual Fusion Network for Biomedical Image Segmentation. (arXiv:2105.07451v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.07451","description":"<p>Methods based on convolutional neural networks have improved the performance\nof biomedical image segmentation. However, most of these methods cannot\nefficiently segment objects of variable sizes and train on small and biased\ndatasets, which are common for biomedical use cases. While methods exist that\nincorporate multi-scale fusion approaches to address the challenges arising\nwith variable sizes, they usually use complex models that are more suitable for\ngeneral semantic segmentation problems. In this paper, we propose a novel\narchitecture called Multi-Scale Residual Fusion Network (MSRF-Net), which is\nspecially designed for medical image segmentation. The proposed MSRF-Net is\nable to exchange multi-scale features of varying receptive fields using a\nDual-Scale Dense Fusion (DSDF) block. Our DSDF block can exchange information\nrigorously across two different resolution scales, and our MSRF sub-network\nuses multiple DSDF blocks in sequence to perform multi-scale fusion. This\nallows the preservation of resolution, improved information flow and\npropagation of both high- and low-level features to obtain accurate\nsegmentation maps. The proposed MSRF-Net allows to capture object variabilities\nand provides improved results on different biomedical datasets. Extensive\nexperiments on MSRF-Net demonstrate that the proposed method outperforms the\ncutting-edge medical image segmentation methods on four publicly available\ndatasets. We achieve the dice coefficient of 0.9217, 0.9420, and 0.9224, 0.8824\non Kvasir-SEG, CVC-ClinicDB, 2018 Data Science Bowl dataset, and ISIC-2018 skin\nlesion segmentation challenge dataset respectively. We further conducted\ngeneralizability tests and achieved a dice coefficient of 0.7921 and 0.7575 on\nCVC-ClinicDB and Kvasir-SEG, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Srivastava_A/0/1/0/all/0/1\">Abhishek Srivastava</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jha_D/0/1/0/all/0/1\">Debesh Jha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chanda_S/0/1/0/all/0/1\">Sukalpa Chanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pal_U/0/1/0/all/0/1\">Umapada Pal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Johansen_H/0/1/0/all/0/1\">H&#xe5;vard D. Johansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Johansen_D/0/1/0/all/0/1\">Dag Johansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Sharib Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UncertaintyFuseNet: Robust Uncertainty-aware Hierarchical Feature Fusion Model with Ensemble Monte Carlo Dropout for COVID-19 Detection. (arXiv:2105.08590v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.08590","description":"<p>The COVID-19 (Coronavirus disease 2019) pandemic has become a major global\nthreat to human health and well-being. Thus, the development of computer-aided\ndetection (CAD) systems that are capable to accurately distinguish COVID-19\nfrom other diseases using chest computed tomography (CT) and X-ray data is of\nimmediate priority. Such automatic systems are usually based on traditional\nmachine learning or deep learning methods. Differently from most of existing\nstudies, which used either CT scan or X-ray images in COVID-19-case\nclassification, we present a simple but efficient deep learning feature fusion\nmodel, called UncertaintyFuseNet, which is able to classify accurately large\ndatasets of both of these types of images. We argue that the uncertainty of the\nmodel's predictions should be taken into account in the learning process, even\nthough most of existing studies have overlooked it. We quantify the prediction\nuncertainty in our feature fusion model using effective Ensemble MC Dropout\n(EMCD) technique. A comprehensive simulation study has been conducted to\ncompare the results of our new model to the existing approaches, evaluating the\nperformance of competing models in terms of Precision, Recall, F-Measure,\nAccuracy and ROC curves. The obtained results prove the efficiency of our model\nwhich provided the prediction accuracy of 99.08\\% and 96.35\\% for the\nconsidered CT scan and X-ray datasets, respectively. Moreover, our\nUncertaintyFuseNet model was generally robust to noise and performed well with\npreviously unseen data. The source code of our implementation is freely\navailable at:\nhttps://github.com/moloud1987/UncertaintyFuseNet-for-COVID-19-Classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Abdar_M/0/1/0/all/0/1\">Moloud Abdar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salari_S/0/1/0/all/0/1\">Soorena Salari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qahremani_S/0/1/0/all/0/1\">Sina Qahremani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lam_H/0/1/0/all/0/1\">Hak-Keung Lam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karray_F/0/1/0/all/0/1\">Fakhri Karray</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hussain_S/0/1/0/all/0/1\">Sadiq Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Acharya_U/0/1/0/all/0/1\">U. Rajendra Acharya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Makarenkov_V/0/1/0/all/0/1\">Vladimir Makarenkov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProtoRes: Proto-Residual Network for Pose Authoring via Learned Inverse Kinematics. (arXiv:2106.01981v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.01981","description":"<p>Our work focuses on the development of a learnable neural representation of\nhuman pose for advanced AI assisted animation tooling. Specifically, we tackle\nthe problem of constructing a full static human pose based on sparse and\nvariable user inputs (e.g. locations and/or orientations of a subset of body\njoints). To solve this problem, we propose a novel neural architecture that\ncombines residual connections with prototype encoding of a partially specified\npose to create a new complete pose from the learned latent space. We show that\nour architecture outperforms a baseline based on Transformer, both in terms of\naccuracy and computational efficiency. Additionally, we develop a user\ninterface to integrate our neural model in Unity, a real-time 3D development\nplatform. Furthermore, we introduce two new datasets representing the static\nhuman pose modeling problem, based on high-quality human motion capture data,\nwhich will be released publicly along with model code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oreshkin_B/0/1/0/all/0/1\">Boris N. Oreshkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bocquelet_F/0/1/0/all/0/1\">Florent Bocquelet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harvey_F/0/1/0/all/0/1\">F&#xe9;lix G. Harvey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raitt_B/0/1/0/all/0/1\">Bay Raitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laflamme_D/0/1/0/all/0/1\">Dominic Laflamme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BR-NPA: A Non-Parametric High-Resolution Attention Model to improve the Interpretability of Attention. (arXiv:2106.02566v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02566","description":"<p>The prevalence of employing attention mechanisms has brought along concerns\non the interpretability of attention distributions. Although it provides\ninsights about how a model is operating, utilizing attention as the explanation\nof model predictions is still highly dubious. The community is still seeking\nmore interpretable strategies for better identifying local active regions that\ncontribute the most to the final decision. To improve the interpretability of\nexisting attention models, we propose a novel Bilinear Representative\nNon-Parametric Attention (BR-NPA) strategy that captures the task-relevant\nhuman-interpretable information. The target model is first distilled to have\nhigher-resolution intermediate feature maps. From which, representative\nfeatures are then grouped based on local pairwise feature similarity, to\nproduce finer-grained, more precise attention maps highlighting task-relevant\nparts of the input. The obtained attention maps are ranked according to the\nactivity level of the compound feature, which provides information regarding\nthe important level of the highlighted regions. The proposed model can be\neasily adapted in a wide variety of modern deep models, where classification is\ninvolved. Extensive quantitative and qualitative experiments showcase more\ncomprehensive and accurate visual explanations compared to state-of-the-art\nattention models and visualizations methods across multiple tasks including\nfine-grained image classification, few-shot classification, and person\nre-identification, without compromising the classification accuracy. The\nproposed visualization model sheds imperative light on how neural networks `pay\ntheir attention' differently in different tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_T/0/1/0/all/0/1\">Tristan Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_S/0/1/0/all/0/1\">Suiyi Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freour_T/0/1/0/all/0/1\">Thomas Fr&#xe9;our</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouchere_H/0/1/0/all/0/1\">Harold Mouch&#xe8;re</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Implicit 3D Shapes from Single Images with Spatial Patterns. (arXiv:2106.03087v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03087","description":"<p>Neural implicit functions have achieved impressive results for reconstructing\n3D shapes from single images. However, the image features for describing 3D\npoint samplings of implicit functions are less effective when significant\nvariations of occlusions, views, and appearances exist from the image. To\nbetter encode image features, we study a geometry-aware convolutional kernel to\nleverage geometric relationships of point samplings by the proposed\n\\emph{spatial pattern}, i.e., a structured point set. Specifically, the kernel\noperates at 2D projections of 3D points from the spatial pattern. Supported by\nthe spatial pattern, the 2D kernel encodes geometric information that is\ncrucial for 3D reconstruction tasks, while traditional ones mainly consider\nappearance information. Furthermore, to enable the network to discover more\nadaptive spatial patterns for further capturing non-local contextual\ninformation, the kernel is devised to be deformable manipulated by a spatial\npattern generator. Experimental results on both synthetic and real datasets\ndemonstrate the superiority of the proposed method. Pre-trained models, codes,\nand data are available at https://github.com/yixin26/SVR-SP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yixin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunzhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoquan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-scale Unsupervised Semantic Segmentation. (arXiv:2106.03149v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03149","description":"<p>Powered by the ImageNet dataset, unsupervised learning on large-scale data\nhas made significant advances for classification tasks. There are two major\nchallenges to allowing such an attractive learning modality for segmentation\ntasks: i) a large-scale benchmark for assessing algorithms is missing; ii)\nunsupervised category/shape representation learning is difficult. We propose a\nnew problem of large-scale unsupervised semantic segmentation (LUSS) with a\nnewly created benchmark dataset to track the research progress. Based on the\nImageNet dataset, we propose the ImageNet-S dataset with 1.2 million training\nimages and 50k high-quality semantic segmentation annotations for evaluation.\nOur benchmark has a high data diversity and a clear task objective. We also\npresent a simple yet effective method that works surprisingly well for LUSS. In\naddition, we benchmark related un/weakly/fully supervised methods accordingly,\nidentifying the challenges and possible directions of LUSS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shanghua Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhong-Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-learning with implicit gradients in a few-shot setting for medical image segmentation. (arXiv:2106.03223v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03223","description":"<p>Widely used traditional supervised deep learning methods require a large\nnumber of training samples but often fail to generalize on unseen datasets.\nTherefore, a more general application of any trained model is quite limited for\nmedical imaging for clinical practice. Using separately trained models for each\nunique lesion category or a unique patient population will require sufficiently\nlarge curated datasets, which is not practical to use in a real-world clinical\nset-up. Few-shot learning approaches can not only minimize the need for an\nenormous number of reliable ground truth labels that are labour-intensive and\nexpensive but can also be used to model on a dataset coming from a new\npopulation. To this end, we propose to exploit an optimization-based implicit\nmodel agnostic meta-learning (iMAML) algorithm under few-shot settings for\nmedical image segmentation. Our approach can leverage the learned weights from\ndiverse but small training samples to perform analysis on unseen datasets with\nhigh accuracy. We show that, unlike classical few-shot learning approaches, our\nmethod improves generalization capability. To our knowledge, this is the first\nwork that exploits iMAML for medical image segmentation and explores the\nstrength of the model on scenarios such as meta-training on unique and mixed\ninstances of lesion datasets. Our quantitative results on publicly available\nskin and polyp datasets show that the proposed method outperforms the naive\nsupervised baseline model and two recent few-shot segmentation approaches by\nlarge margins. In addition, our iMAML approach shows an improvement of 2%-4% in\ndice score compared to its counterpart MAML for most experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khadga_R/0/1/0/all/0/1\">Rabindra Khadga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1\">Debesh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hicks_S/0/1/0/all/0/1\">Steven Hicks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thambawita_V/0/1/0/all/0/1\">Vajira Thambawita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1\">Sharib Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entropy-based Logic Explanations of Neural Networks. (arXiv:2106.06804v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2106.06804","description":"<p>Explainable artificial intelligence has rapidly emerged since lawmakers have\nstarted requiring interpretable models for safety-critical domains.\nConcept-based neural networks have arisen as explainable-by-design methods as\nthey leverage human-understandable symbols (i.e. concepts) to predict class\nmemberships. However, most of these approaches focus on the identification of\nthe most relevant concepts but do not provide concise, formal explanations of\nhow such concepts are leveraged by the classifier to make predictions. In this\npaper, we propose a novel end-to-end differentiable approach enabling the\nextraction of logic explanations from neural networks using the formalism of\nFirst-Order Logic. The method relies on an entropy-based criterion which\nautomatically identifies the most relevant concepts. We consider four different\ncase studies to demonstrate that: (i) this entropy-based criterion enables the\ndistillation of concise logic explanations in safety-critical domains from\nclinical data to computer vision; (ii) the proposed approach outperforms\nstate-of-the-art white-box models in terms of classification accuracy and\nmatches black box performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1\">Pietro Barbiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1\">Gabriele Ciravegna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1\">Francesco Giannini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1\">Marco Gori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melacci_S/0/1/0/all/0/1\">Stefano Melacci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The 2021 Image Similarity Dataset and Challenge. (arXiv:2106.09672v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09672","description":"<p>This paper introduces a new benchmark for large-scale image similarity\ndetection. This benchmark is used for the Image Similarity Challenge at\nNeurIPS'21 (ISC2021). The goal is to determine whether a query image is a\nmodified copy of any image in a reference corpus of size 1~million. The\nbenchmark features a variety of image transformations such as automated\ntransformations, hand-crafted image edits and machine-learning based\nmanipulations. This mimics real-life cases appearing in social media, for\nexample for integrity-related problems dealing with misinformation and\nobjectionable content. The strength of the image manipulations, and therefore\nthe difficulty of the benchmark, is calibrated according to the performance of\na set of baseline approaches. Both the query and reference set contain a\nmajority of \"distractor\" images that do not match, which corresponds to a\nreal-life needle-in-haystack setting, and the evaluation metric reflects that.\nWe expect the DISC21 benchmark to promote image copy detection as an important\nand challenging computer vision task and refresh the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1\">Matthijs Douze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_G/0/1/0/all/0/1\">Giorgos Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pizzi_E/0/1/0/all/0/1\">Ed Pizzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papakipos_Z/0/1/0/all/0/1\">Zo&#xeb; Papakipos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_L/0/1/0/all/0/1\">Lowik Chanussot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radenovic_F/0/1/0/all/0/1\">Filip Radenovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenicek_T/0/1/0/all/0/1\">Tomas Jenicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maximov_M/0/1/0/all/0/1\">Maxim Maximov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taix&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1\">Ismail Elezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chum_O/0/1/0/all/0/1\">Ond&#x159;ej Chum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1\">Cristian Canton Ferrer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global and Local Contrastive Self-Supervised Learning for Semantic Segmentation of HR Remote Sensing Images. (arXiv:2106.10605v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10605","description":"<p>Supervised learning for semantic segmentation requires a large number of\nlabeled samples, which is difficult to obtain in the field of remote sensing.\nSelf-supervised learning (SSL), can be used to solve such problems by\npre-training a general model with a large number of unlabeled images and then\nfine-tuning it on a downstream task with very few labeled samples. Contrastive\nlearning is a typical method of SSL that can learn general invariant features.\nHowever, most existing contrastive learning methods are designed for\nclassification tasks to obtain an image-level representation, which may be\nsuboptimal for semantic segmentation tasks requiring pixel-level\ndiscrimination. Therefore, we propose a global style and local matching\ncontrastive learning network (GLCNet) for remote sensing image semantic\nsegmentation. Specifically, 1) the global style contrastive learning module is\nused to better learn an image-level representation, as we consider that style\nfeatures can better represent the overall image features. 2) The local features\nmatching contrastive learning module is designed to learn representations of\nlocal regions, which is beneficial for semantic segmentation. The experimental\nresults show that our method mostly outperforms SOTA self-supervised methods\nand the ImageNet pre-training method. Specifically, with 1\\% annotation from\nthe original dataset, our approach improves Kappa by 6\\% on the ISPRS Potsdam\ndataset relative to the existing baseline. Moreover, our method outperforms\nsupervised learning methods when there are some differences between the\ndatasets of upstream tasks and downstream tasks. Since SSL could directly learn\nthe essential characteristics of data from unlabeled data, which is easy to\nobtain in the remote sensing field, this may be of great significance for tasks\nsuch as global mapping. The source code is available at\nhttps://github.com/GeoX-Lab/G-RSIM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruoyun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haozhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chao Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Steerable 3D Spherical Neurons. (arXiv:2106.13863v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13863","description":"<p>Emerging from low-level vision theory, steerable filters found their\ncounterpart in prior work on steerable convolutional neural networks\nequivariant to rigid transformations. In our work, we propose a steerable\nfeed-forward learning-based approach that consists of neurons with spherical\ndecision surfaces and operates on point clouds. Such spherical neurons are\nobtained by conformal embedding of Euclidean space and have recently been\nrevisited in the context of learning representations of point sets. Focusing on\n3D geometry, we exploit the isometry property of spherical neurons and derive a\n3D steerability constraint. After training spherical neurons to classify point\nclouds in a canonical orientation, we use a tetrahedron basis to quadruplicate\nthe neurons and construct rotation-equivariant spherical filter banks. We then\napply the derived constraint to interpolate the filter bank outputs and, thus,\nobtain a rotation-invariant network. Finally, we use a synthetic point set and\nreal-world 3D skeleton data to verify our theoretical findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1\">Pavlo Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1\">M&#xe5;rten Wadenb&#xe4;ck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Split, embed and merge: An accurate table structure recognizer. (arXiv:2107.05214v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05214","description":"<p>Table structure recognition is an essential part for making machines\nunderstand tables. Its main task is to recognize the internal structure of a\ntable. However, due to the complexity and diversity in their structure and\nstyle, it is very difficult to parse the tabular data into the structured\nformat which machines can understand easily, especially for complex tables. In\nthis paper, we introduce Split, Embed and Merge (SEM), an accurate table\nstructure recognizer. Our model takes table images as input and can correctly\nrecognize the structure of tables, whether they are simple or a complex tables.\nSEM is mainly composed of three parts, splitter, embedder and merger. In the\nfirst stage, we apply the splitter to predict the potential regions of the\ntable row (column) separators, and obtain the fine grid structure of the table.\nIn the second stage, by taking a full consideration of the textual information\nin the table, we fuse the output features for each table grid from both vision\nand language modalities. Moreover, we achieve a higher precision in our\nexperiments through adding additional semantic features. Finally, we process\nthe merging of these basic table grids in a self-regression manner. The\ncorrespondent merging results is learned through the attention mechanism. In\nour experiments, SEM achieves an average F1-Measure of 97.11% on the SciTSR\ndataset which outperforms other methods by a large margin. We also won the\nfirst place in the complex table and third place in all tables in ICDAR 2021\nCompetition on Scientific Literature Parsing, Task-B. Extensive experiments on\nother publicly available datasets demonstrate that our model achieves\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenrong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianshu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jun Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LASOR: Learning Accurate 3D Human Pose and Shape Via Synthetic Occlusion-Aware Data and Neural Mesh Rendering. (arXiv:2108.00351v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00351","description":"<p>A key challenge in the task of human pose and shape estimation is occlusion,\nincluding self-occlusions, object-human occlusions, and inter-person\nocclusions. The lack of diverse and accurate pose and shape training data\nbecomes a major bottleneck, especially for scenes with occlusions in the wild.\nIn this paper, we focus on the estimation of human pose and shape in the case\nof inter-person occlusions, while also handling object-human occlusions and\nself-occlusion. We propose a novel framework that synthesizes occlusion-aware\nsilhouette and 2D keypoints data and directly regress to the SMPL pose and\nshape parameters. A neural 3D mesh renderer is exploited to enable silhouette\nsupervision on the fly, which contributes to great improvements in shape\nestimation. In addition, keypoints-and-silhouette-driven training data in\npanoramic viewpoints are synthesized to compensate for the lack of viewpoint\ndiversity in any existing dataset. Experimental results show that we are among\nthe state-of-the-art on the 3DPW and 3DPW-Crowd datasets in terms of pose\nestimation accuracy. The proposed method evidently outperforms Mesh\nTransformer, 3DCrowdNet and ROMP in terms of shape estimation. Top performance\nis also achieved on SSP-3D in terms of shape prediction accuracy. Demo and code\nwill be available at https://igame-lab.github.io/LASOR/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaibing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1\">Renshu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Maoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toyoura_M/0/1/0/all/0/1\">Masahiro Toyoura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Gang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pro-UIGAN: Progressive Face Hallucination from Occluded Thumbnails. (arXiv:2108.00602v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00602","description":"<p>In this paper, we study the task of hallucinating an authentic\nhigh-resolution (HR) face from an occluded thumbnail. We propose a multi-stage\nProgressive Upsampling and Inpainting Generative Adversarial Network, dubbed\nPro-UIGAN, which exploits facial geometry priors to replenish and upsample (8*)\nthe occluded and tiny faces (16*16 pixels). Pro-UIGAN iteratively (1) estimates\nfacial geometry priors for low-resolution (LR) faces and (2) acquires\nnon-occluded HR face images under the guidance of the estimated priors. Our\nmulti-stage hallucination network super-resolves and inpaints occluded LR faces\nin a coarse-to-fine manner, thus reducing unwanted blurriness and artifacts\nsignificantly. Specifically, we design a novel cross-modal transformer module\nfor facial priors estimation, in which an input face and its landmark features\nare formulated as queries and keys, respectively. Such a design encourages\njoint feature learning across the input facial and landmark features, and deep\nfeature correspondences will be discovered by attention. Thus, facial\nappearance features and facial geometry priors are learned in a mutual\npromotion manner. Extensive experiments demonstrate that our Pro-UIGAN achieves\nvisually pleasing HR faces, reaching superior performance in downstream tasks,\ni.e., face alignment, face parsing, face recognition and expression\nclassification, compared with other state-of-the-art (SotA) methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaobo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BIGRoC: Boosting Image Generation via a Robust Classifier. (arXiv:2108.03702v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03702","description":"<p>The interest of the machine learning community in image synthesis has grown\nsignificantly in recent years, with the introduction of a wide range of deep\ngenerative models and means for training them. In this work, we propose a\ngeneral model-agnostic technique for improving the image quality and the\ndistribution fidelity of generated images, obtained by any generative model.\nOur method, termed BIGRoC (Boosting Image Generation via a Robust Classifier),\nis based on a post-processing procedure via the guidance of a given robust\nclassifier and without a need for additional training of the generative model.\nGiven a synthesized image, we propose to update it through projected gradient\nsteps over the robust classifier, in an attempt to refine its recognition. We\ndemonstrate this post-processing algorithm on various image synthesis methods\nand show a significant improvement of the generated images, both quantitatively\nand qualitatively, on CIFAR-10 and ImageNet. Specifically, BIGRoC improves the\nimage synthesis state of the art on ImageNet 128x128 by 14.81%, attaining an\nFID score of 2.53 and on 256x256 by 13.29%, achieving an FID of 3.98.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganz_R/0/1/0/all/0/1\">Roy Ganz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1\">Michael Elad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FT-TDR: Frequency-guided Transformer and Top-Down Refinement Network for Blind Face Inpainting. (arXiv:2108.04424v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04424","description":"<p>Blind face inpainting refers to the task of reconstructing visual contents\nwithout explicitly indicating the corrupted regions in a face image.\nInherently, this task faces two challenges: (1) how to detect various mask\npatterns of different shapes and contents; (2) how to restore visually\nplausible and pleasing contents in the masked regions. In this paper, we\npropose a novel two-stage blind face inpainting method named Frequency-guided\nTransformer and Top-Down Refinement Network (FT-TDR) to tackle these\nchallenges. Specifically, we first use a transformer-based network to detect\nthe corrupted regions to be inpainted as masks by modeling the relation among\ndifferent patches. We also exploit the frequency modality as complementary\ninformation for improved detection results and capture the local contextual\nincoherence to enhance boundary consistency. Then a top-down refinement network\nis proposed to hierarchically restore features at different levels and generate\ncontents that are semantically consistent with the unmasked face regions.\nExtensive experiments demonstrate that our method outperforms current\nstate-of-the-art blind and non-blind face inpainting methods qualitatively and\nquantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoxiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Relative Confidence and Label-Free Model Selection for Convolutional Neural Networks. (arXiv:2108.11845v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11845","description":"<p>This letter is concerned with image classification with deep convolutional\nneural networks (CNNs). The focus is on the following question: given a set of\ncandidate CNN models, how to select the right one with the best generalization\nproperty for the current task? Present model selection methods require access\nto a batch of labeled data for computing a pre-specified performance metric,\nsuch as the cross-entropy loss, the classification error rate, the negative\nlog-likelihood. In many practical cases, labels are not available in time as\nlabeling itself is a time-consuming and expensive task. To this end, this\nletter presents an approach to CNN model selection using only unlabeled data.\nThis method is developed based on a principle termed consistent relative\nconfidence. The effectiveness and efficiency of the proposed method are\ndemonstrated by experiments using benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers. (arXiv:2109.10686v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10686","description":"<p>There remain many open questions pertaining to the scaling behaviour of\nTransformer architectures. These scaling decisions and findings can be\ncritical, as training runs often come with an associated computational cost\nwhich have both financial and/or environmental impact. The goal of this paper\nis to present scaling insights from pretraining and finetuning Transformers.\nWhile Kaplan et al. presents a comprehensive study of the scaling behaviour of\nTransformer language models, the scope is only on the upstream (pretraining)\nloss. Therefore, it is still unclear if these set of findings transfer to\ndownstream task within the context of the pretrain-finetune paradigm. The key\nfindings of this paper are as follows: (1) we show that aside from only the\nmodel size, model shape matters for downstream fine-tuning, (2) scaling\nprotocols operate differently at different compute regions, (3) widely adopted\nT5-base and T5-large sizes are Pareto-inefficient. To this end, we present\nimproved scaling protocols whereby our redesigned models achieve similar\ndownstream fine-tuning quality while having 50\\% fewer parameters and training\n40\\% faster compared to the widely adopted T5-base model. We publicly release\nover 100 pretrained checkpoints of different T5 configurations to facilitate\nfuture research and analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jinfeng Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abnar_S/0/1/0/all/0/1\">Samira Abnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaswani_A/0/1/0/all/0/1\">Ashish Vaswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frequency Disentangled Residual Network. (arXiv:2109.12556v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12556","description":"<p>Residual networks (ResNets) have been utilized for various computer vision\nand image processing applications. The residual connection improves the\ntraining of the network with better gradient flow. A residual block consists of\nfew convolutional layers having trainable parameters, which leads to\noverfitting. Moreover, the present residual networks are not able to utilize\nthe high and low frequency information suitably, which also challenges the\ngeneralization capability of the network. In this paper, a frequency\ndisentangled residual network (FDResNet) is proposed to tackle these issues.\nSpecifically, FDResNet includes separate connections in the residual block for\nlow and high frequency components, respectively. Basically, the proposed model\ndisentangles the low and high frequency components to increase the\ngeneralization ability. Moreover, the computation of low and high frequency\ncomponents using fixed filters further avoids the overfitting. The proposed\nmodel is tested on benchmark CIFAR10/100, Caltech and TinyImageNet datasets for\nimage classification. The performance of the proposed model is also tested in\nimage retrieval framework. It is noticed that the proposed model outperforms\nits counterpart residual model. The effect of kernel size and standard\ndeviation is also evaluated. The impact of the frequency disentangling is also\nanalyzed using saliency map.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satya Rajendra Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yedla_R/0/1/0/all/0/1\">Roshan Reddy Yedla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanodiya_R/0/1/0/all/0/1\">Rakesh Sanodiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wei-Ta Chu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spike-inspired Rank Coding for Fast and Accurate Recurrent Neural Networks. (arXiv:2110.02865v2 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2110.02865","description":"<p>Biological spiking neural networks (SNNs) can temporally encode information\nin their outputs, e.g. in the rank order in which neurons fire, whereas\nartificial neural networks (ANNs) conventionally do not. As a result, models of\nSNNs for neuromorphic computing are regarded as potentially more rapid and\nefficient than ANNs when dealing with temporal input. On the other hand, ANNs\nare simpler to train, and usually achieve superior performance. Here we show\nthat temporal coding such as rank coding (RC) inspired by SNNs can also be\napplied to conventional ANNs such as LSTMs, and leads to computational savings\nand speedups. In our RC for ANNs, we apply backpropagation through time using\nthe standard real-valued activations, but only from a strategically early time\nstep of each sequential input example, decided by a threshold-crossing event.\nLearning then incorporates naturally also _when_ to produce an output, without\nother changes to the model or the algorithm. Both the forward and the backward\ntraining pass can be significantly shortened by skipping the remaining input\nsequence after that first event. RC-training also significantly reduces\ntime-to-insight during inference, with a minimal decrease in accuracy. The\ndesired speed-accuracy trade-off is tunable by varying the threshold or a\nregularization parameter that rewards output entropy. We demonstrate these in\ntwo toy problems of sequence classification, and in a temporally-encoded MNIST\ndataset where our RC model achieves 99.19% accuracy after the first input\ntime-step, outperforming the state of the art in temporal coding with SNNs, as\nwell as in spoken-word classification of Google Speech Commands, outperforming\nnon-RC-trained early inference with LSTMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeffares_A/0/1/0/all/0/1\">Alan Jeffares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qinghai Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moraitis_T/0/1/0/all/0/1\">Timoleon Moraitis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A transformer-based deep learning approach for classifying brain metastases into primary organ sites using clinical whole brain MRI. (arXiv:2110.03588v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.03588","description":"<p>Treatment decisions for brain metastatic disease rely on knowledge of the\nprimary organ site, and currently made with biopsy and histology. Here we\ndevelop a novel deep learning approach for accurate non-invasive digital\nhistology with whole-brain MRI data. Our IRB-approved single-site retrospective\nstudy was comprised of patients (n=1,399) referred for MRI treatment-planning\nand gamma knife radiosurgery over 21 years. Contrast-enhanced T1-weighted and\nT2-weighted Fluid-Attenuated Inversion Recovery brain MRI exams (n=1,582) were\npreprocessed and input to the proposed deep learning workflow for tumor\nsegmentation, modality transfer, and primary site classification into one of\nfive classes (lung, breast, melanoma, renal, and others). Ten-fold\ncross-validation generated overall AUC of 0.947 (95%CI:0.938,0.955), lung class\nAUC of 0.899 (95%CI:0.884,0.915), breast class AUC of 0.990\n(95%CI:0.983,0.997), melanoma class AUC of 0.882 (95%CI:0.858,0.906), renal\nclass AUC of 0.870 (95%CI:0.823,0.918), and other class AUC of 0.885\n(95%CI:0.843,0.949). These data establish that whole-brain imaging features are\ndiscriminative to allow accurate diagnosis of the primary organ site of\nmalignancy. Our end-to-end deep radiomic approach has great potential for\nclassifying metastatic tumor types from whole-brain MRI images. Further\nrefinement may offer an invaluable clinical tool to expedite primary cancer\nsite identification for precision treatment and improved outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Namjoshi_S/0/1/0/all/0/1\">Sanjeev V. Namjoshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McTyre_E/0/1/0/all/0/1\">Emory McTyre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Topaloglu_U/0/1/0/all/0/1\">Umit Topaloglu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barcus_R/0/1/0/all/0/1\">Richard Barcus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_M/0/1/0/all/0/1\">Michael D. Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cramer_C/0/1/0/all/0/1\">Christina K. Cramer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Debinski_W/0/1/0/all/0/1\">Waldemar Debinski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gurcan_M/0/1/0/all/0/1\">Metin N. Gurcan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lesser_G/0/1/0/all/0/1\">Glenn J. Lesser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_H/0/1/0/all/0/1\">Hui-Kuan Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munden_R/0/1/0/all/0/1\">Reginald F. Munden</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pasche_B/0/1/0/all/0/1\">Boris C. Pasche</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sai_K/0/1/0/all/0/1\">Kiran Kumar Solingapuram Sai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strowd_R/0/1/0/all/0/1\">Roy E. Strowd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tatter_S/0/1/0/all/0/1\">Stephen B. Tatter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watabe_K/0/1/0/all/0/1\">Kounosuke Watabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Whitlow_C/0/1/0/all/0/1\">Christopher T. Whitlow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZARTS: On Zero-order Optimization for Neural Architecture Search. (arXiv:2110.04743v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04743","description":"<p>Differentiable architecture search (DARTS) has been a popular one-shot\nparadigm for NAS due to its high efficiency. It introduces trainable\narchitecture parameters to represent the importance of candidate operations and\nproposes first/second-order approximation to estimate their gradients, making\nit possible to solve NAS by gradient descent algorithm. However, our in-depth\nempirical results show that the approximation will often distort the loss\nlandscape, leading to the biased objective to optimize and in turn inaccurate\ngradient estimation for architecture parameters. This work turns to zero-order\noptimization and proposes a novel NAS scheme, called ZARTS, to search without\nenforcing the above approximation. Specifically, three representative\nzero-order optimization methods are introduced: RS, MGS, and GLD, among which\nMGS performs best by balancing the accuracy and speed. Moreover, we explore the\nconnections between RS/MGS and gradient descent algorithm and show that our\nZARTS can be seen as a robust gradient-free counterpart to DARTS. Extensive\nexperiments on multiple datasets and search spaces show the remarkable\nperformance of our method. In particular, results on 12 benchmarks verify the\noutstanding robustness of ZARTS, where the performance of DARTS collapses due\nto its known instability issue. Also, we search on the search space of DARTS to\ncompare with peer methods, and our discovered architecture achieves 97.54%\naccuracy on CIFAR-10 and 75.7% top-1 accuracy on ImageNet, which are\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoxing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wenxuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jianlin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Is Graph: Structured Graph Module for Video Action Recognition. (arXiv:2110.05904v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05904","description":"<p>In the field of action recognition, video clips are always treated as ordered\nframes for subsequent processing. To achieve spatio-temporal perception,\nexisting approaches propose to embed adjacent temporal interaction in the\nconvolutional layer. The global semantic information can therefore be obtained\nby stacking multiple local layers hierarchically. However, such global temporal\naccumulation can only reflect the high-level semantics in deep layers,\nneglecting the potential low-level holistic clues in shallow layers. In this\npaper, we first propose to transform a video sequence into a graph to obtain\ndirect long-term dependencies among temporal frames. To preserve sequential\ninformation during transformation, we devise a structured graph module (SGM),\nachieving fine-grained temporal interactions throughout the entire network. In\nparticular, SGM divides the neighbors of each node into several temporal\nregions so as to extract global structural information with diverse sequential\nflows. Extensive experiments are performed on standard benchmark datasets,\ni.e., Something-Something V1 &amp; V2, Diving48, Kinetics-400, UCF101, and HMDB51.\nThe reported performance and analysis demonstrate that SGM can achieve\noutstanding precision with less computational complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongchang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling. (arXiv:2110.08263v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.08263","description":"<p>The recently proposed FixMatch achieved state-of-the-art results on most\nsemi-supervised learning (SSL) benchmarks. However, like other modern SSL\nalgorithms, FixMatch uses a pre-defined constant threshold for all classes to\nselect unlabeled data that contribute to the training, thus failing to consider\ndifferent learning status and learning difficulties of different classes. To\naddress this issue, we propose Curriculum Pseudo Labeling (CPL), a curriculum\nlearning approach to leverage unlabeled data according to the model's learning\nstatus. The core of CPL is to flexibly adjust thresholds for different classes\nat each time step to let pass informative unlabeled data and their pseudo\nlabels. CPL does not introduce additional parameters or computations (forward\nor backward propagation). We apply CPL to FixMatch and call our improved\nalgorithm FlexMatch. FlexMatch achieves state-of-the-art performance on a\nvariety of SSL benchmarks, with especially strong performances when the labeled\ndata are extremely limited or when the task is challenging. For example,\nFlexMatch achieves 13.96% and 18.96% error rate reduction over FixMatch on\nCIFAR-100 and STL-10 datasets respectively, when there are only 4 labels per\nclass. CPL also significantly boosts the convergence speed, e.g., FlexMatch can\nuse only 1/5 training time of FixMatch to achieve even better performance.\nFurthermore, we show that CPL can be easily adapted to other SSL algorithms and\nremarkably improve their performances. We open-source our code at\nhttps://github.com/TorchSSL/TorchSSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1\">Wenxin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okumura_M/0/1/0/all/0/1\">Manabu Okumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinozaki_T/0/1/0/all/0/1\">Takahiro Shinozaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PlaneRecNet: Multi-Task Learning with Cross-Task Consistency for Piece-Wise Plane Detection and Reconstruction from a Single RGB Image. (arXiv:2110.11219v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11219","description":"<p>Piece-wise 3D planar reconstruction provides holistic scene understanding of\nman-made environments, especially for indoor scenarios. Most recent approaches\nfocused on improving the segmentation and reconstruction results by introducing\nadvanced network architectures but overlooked the dual characteristics of\npiece-wise planes as objects and geometric models. Different from other\nexisting approaches, we start from enforcing cross-task consistency for our\nmulti-task convolutional neural network, PlaneRecNet, which integrates a\nsingle-stage instance segmentation network for piece-wise planar segmentation\nand a depth decoder to reconstruct the scene from a single RGB image. To\nachieve this, we introduce several novel loss functions (geometric constraint)\nthat jointly improve the accuracy of piece-wise planar segmentation and depth\nestimation. Meanwhile, a novel Plane Prior Attention module is used to guide\ndepth estimation with the awareness of plane instances. Exhaustive experiments\nare conducted in this work to validate the effectiveness and efficiency of our\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yaxu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_F/0/1/0/all/0/1\">Fangwen Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambach_J/0/1/0/all/0/1\">Jason Rambach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagani_A/0/1/0/all/0/1\">Alain Pagani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Unlearning via Class-Discriminative Pruning. (arXiv:2110.11794v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11794","description":"<p>We explore the problem of selectively forgetting categories from trained CNN\nclassification models in the federated learning (FL). Given that the data used\nfor training cannot be accessed globally in FL, our insights probe deep into\nthe internal influence of each channel. Through the visualization of feature\nmaps activated by different channels, we observe that different channels have a\nvarying contribution to different categories in image classification. Inspired\nby this, we propose a method for scrubbing the model clean of information about\nparticular categories. The method does not require retraining from scratch, nor\nglobal access to the data used for training. Instead, we introduce the concept\nof Term Frequency Inverse Document Frequency (TF-IDF) to quantize the class\ndiscrimination of channels. Channels with high TF-IDF scores have more\ndiscrimination on the target categories and thus need to be pruned to unlearn.\nThe channel pruning is followed by a fine-tuning process to recover the\nperformance of the pruned model. Evaluated on CIFAR10 dataset, our method\naccelerates the speed of unlearning by 8.9x for the ResNet model, and 7.9x for\nthe VGG model under no degradation in accuracy, compared to retraining from\nscratch. For CIFAR100 dataset, the speedups are 9.9x and 8.4x, respectively. We\nenvision this work as a complementary block for FL towards compliance with\nlegal and ethical criteria.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Song Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Heng Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards the Generalization of Contrastive Self-Supervised Learning. (arXiv:2111.00743v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.00743","description":"<p>Recently, self-supervised learning has attracted great attention, since it\nonly requires unlabeled data for training. Contrastive learning is a popular\napproach for self-supervised learning and achieves promising empirical\nperformance. However, the theoretical understanding of its generalization\nability is still limited. To this end, we define a kind of\n$(\\sigma,\\delta)$-measure to mathematically quantify the data augmentation, and\nthen provide an upper bound of the downstream classification error based on the\nmeasure. We show that the generalization ability of contrastive self-supervised\nlearning depends on three key factors: alignment of positive samples,\ndivergence of class centers, and concentration of augmented data. The first two\nfactors can be optimized by contrastive algorithms, while the third one is\npriorly determined by pre-defined data augmentation. With the above theoretical\nfindings, we further study two canonical contrastive losses, InfoNCE and\ncross-correlation loss, and prove that both of them are able to obtain the\nembedding space satisfying the aforementioned factors. Finally, we conduct\nvarious experiments on the real-world dataset, and show that our theoretical\ninferences on the relationship between the data augmentation and the\ngeneralization of contrastive self-supervised learning agree with the empirical\nobservations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weiran Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_M/0/1/0/all/0/1\">Mingyang Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xuyang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering and Explaining the Representation Bottleneck of DNNs. (arXiv:2111.06236v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.06236","description":"<p>This paper explores the bottleneck of feature representations of deep neural\nnetworks (DNNs), from the perspective of the complexity of interactions between\ninput variables encoded in DNNs. To this end, we focus on the multi-order\ninteraction between input variables, where the order represents the complexity\nof interactions. We discover that a DNN is more likely to encode both too\nsimple interactions and too complex interactions, but usually fails to learn\ninteractions of intermediate complexity. Such a phenomenon is widely shared by\ndifferent DNNs for different tasks. This phenomenon indicates a cognition gap\nbetween DNNs and human beings, and we call it a representation bottleneck. We\ntheoretically prove the underlying reason for the representation bottleneck.\nFurthermore, we propose a loss to encourage/penalize the learning of\ninteractions of specific complexities, and analyze the representation\ncapacities of interactions of different complexities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Huiqi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1\">Qihan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable multiple abnormality classification of chest CT volumes with deep learning. (arXiv:2111.12215v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.12215","description":"<p>Understanding model predictions is critical in healthcare, to facilitate\nrapid verification of model correctness and to guard against use of models that\nexploit confounding variables. We introduce the challenging new task of\nexplainable multiple abnormality classification in volumetric medical images,\nin which a model must indicate the regions used to predict each abnormality. To\nsolve this task, we propose a multiple instance learning convolutional neural\nnetwork, AxialNet, that allows identification of top slices for each\nabnormality. Next we incorporate HiResCAM, an attention mechanism, to identify\nsub-slice regions. We prove that for AxialNet, HiResCAM explanations are\nguaranteed to reflect the locations the model used, unlike Grad-CAM which\nsometimes highlights irrelevant locations. Armed with a model that produces\nfaithful explanations, we then aim to improve the model's learning through a\nnovel mask loss that leverages HiResCAM and 3D allowed regions to encourage the\nmodel to predict abnormalities based only on the organs in which those\nabnormalities appear. The 3D allowed regions are obtained automatically through\na new approach, PARTITION, that combines location information extracted from\nradiology reports with organ segmentation maps obtained through morphological\nimage processing. Overall, we propose the first model for explainable\nmulti-abnormality prediction in volumetric medical images, and then use the\nmask loss to achieve a 33% improvement in organ localization of multiple\nabnormalities in the RAD-ChestCT data set of 36,316 scans, representing the\nstate of the art. This work advances the clinical applicability of multiple\nabnormality modeling in chest CT volumes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Draelos_R/0/1/0/all/0/1\">Rachel Lea Draelos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-aware Video-language Pre-training for Retrieval. (arXiv:2112.00656v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00656","description":"<p>Recently, by introducing large-scale dataset and strong transformer network,\nvideo-language pre-training has shown great success especially for retrieval.\nYet, existing video-language transformer models do not explicitly fine-grained\nsemantic align. In this work, we present Object-aware Transformers, an\nobject-centric approach that extends video-language transformer to incorporate\nobject representations. The key idea is to leverage the bounding boxes and\nobject tags to guide the training process. We evaluate our model on three\nstandard sub-tasks of video-text matching on four widely used benchmarks. We\nalso provide deep analysis and detailed ablation about the proposed method. We\nshow clear improvement in performance across all tasks and datasets considered,\ndemonstrating the value of a model that incorporates object representations\ninto a video-language architecture. The code will be released at\n\\url{https://github.com/FingerRec/OA-Transformer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alex Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_G/0/1/0/all/0/1\">Guanyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_X/0/1/0/all/0/1\">Xiaohu Qie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sample Prior Guided Robust Model Learning to Suppress Noisy Labels. (arXiv:2112.01197v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01197","description":"<p>Imperfect labels are ubiquitous in real-world datasets and seriously harm the\nmodel performance. Several recent effective methods for handling noisy labels\nhave two key steps: 1) dividing samples into cleanly labeled and wrongly\nlabeled sets by training loss, 2) using semi-supervised methods to generate\npseudo-labels for samples in the wrongly labeled set. However, current methods\nalways hurt the informative hard samples due to the similar loss distribution\nbetween the hard samples and the noisy ones. In this paper, we proposed PGDF\n(Prior Guided Denoising Framework), a novel framework to learn a deep model to\nsuppress noise by generating the samples' prior knowledge, which is integrated\ninto both dividing samples step and semi-supervised step. Our framework can\nsave more informative hard clean samples into the cleanly labeled set. Besides,\nour framework also promotes the quality of pseudo-labels during the\nsemi-supervised step by suppressing the noise in the current pseudo-labels\ngenerating scheme. To further enhance the hard samples, we reweight the samples\nin the cleanly labeled set during training. We evaluated our method using\nsynthetic datasets based on CIFAR-10 and CIFAR-100, as well as on the\nreal-world datasets WebVision and Clothing1M. The results demonstrate\nsubstantial improvements over state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenkai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chuang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Contrastive Learning through the Lens of Neighborhood Component Analysis: an Integrated Framework. (arXiv:2112.04468v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.04468","description":"<p>As a seminal tool in self-supervised representation learning, contrastive\nlearning has gained unprecedented attention in recent years. In essence,\ncontrastive learning aims to leverage pairs of positive and negative samples\nfor representation learning, which relates to exploiting neighborhood\ninformation in a feature space. By investigating the connection between\ncontrastive learning and neighborhood component analysis (NCA), we provide a\nnovel stochastic nearest neighbor viewpoint of contrastive learning and\nsubsequently propose a series of contrastive losses that outperform the\nexisting ones. Under our proposed framework, we show a new methodology to\ndesign integrated contrastive losses that could simultaneously achieve good\naccuracy and robustness on downstream tasks. With the integrated framework, we\nachieve up to 6\\% improvement on the standard accuracy and 17\\% improvement on\nthe robust accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ko_C/0/1/0/all/0/1\">Ching-Yun Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohapatra_J/0/1/0/all/0/1\">Jeet Mohapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniel_L/0/1/0/all/0/1\">Luca Daniel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_L/0/1/0/all/0/1\">Lily Weng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curvature-guided dynamic scale networks for Multi-view Stereo. (arXiv:2112.05999v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05999","description":"<p>Multi-view stereo (MVS) is a crucial task for precise 3D reconstruction. Most\nrecent studies tried to improve the performance of matching cost volume in MVS\nby designing aggregated 3D cost volumes and their regularization. This paper\nfocuses on learning a robust feature extraction network to enhance the\nperformance of matching costs without heavy computation in the other steps. In\nparticular, we present a dynamic scale feature extraction network, namely,\nCDSFNet. It is composed of multiple novel convolution layers, each of which can\nselect a proper patch scale for each pixel guided by the normal curvature of\nthe image surface. As a result, CDFSNet can estimate the optimal patch scales\nto learn discriminative features for accurate matching computation between\nreference and source images. By combining the robust extracted features with an\nappropriate cost formulation strategy, our resulting MVS architecture can\nestimate depth maps more precisely. Extensive experiments showed that the\nproposed method outperforms other state-of-the-art methods on complex outdoor\nscenes. It significantly improves the completeness of reconstructed models. As\na result, the method can process higher resolution inputs within faster\nrun-time and lower memory than other MVS methods. Our source code is available\nat url{https://github.com/TruongKhang/cds-mvsnet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giang_K/0/1/0/all/0/1\">Khang Truong Giang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Soohwan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_S/0/1/0/all/0/1\">Sungho Jo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"360{\\deg} Optical Flow using Tangent Images. (arXiv:2112.14331v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.14331","description":"<p>Omnidirectional 360{\\deg} images have found many promising and exciting\napplications in computer vision, robotics and other fields, thanks to their\nincreasing affordability, portability and their 360{\\deg} field of view. The\nmost common format for storing, processing and visualising 360{\\deg} images is\nequirectangular projection (ERP). However, the distortion introduced by the\nnonlinear mapping from 360{\\deg} image to ERP image is still a barrier that\nholds back ERP images from being used as easily as conventional perspective\nimages. This is especially relevant when estimating 360{\\deg} optical flow, as\nthe distortions need to be mitigated appropriately. In this paper, we propose a\n360{\\deg} optical flow method based on tangent images. Our method leverages\ngnomonic projection to locally convert ERP images to perspective images, and\nuniformly samples the ERP image by projection to a cubemap and regular\nicosahedron vertices, to incrementally refine the estimated 360{\\deg} flow\nfields even in the presence of large rotations. Our experiments demonstrate the\nbenefits of our proposed method both quantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingze Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richardt_C/0/1/0/all/0/1\">Christian Richardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Res2NetFuse: A Fusion Method for Infrared and Visible Images. (arXiv:2112.14540v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.14540","description":"<p>This paper presents a novel Res2Net-based fusion framework for infrared and\nvisible images. The proposed fusion model has three parts: an encoder, a fusion\nlayer and a decoder, respectively. The Res2Net-based encoder is used to extract\nmulti-scale features of source images, the paper introducing a new training\nstrategy for training a Res2Net-based encoder that uses only a single image.\nThen, a new fusion strategy is developed based on the attention model. Finally,\nthe fused image is reconstructed by the decoder. The proposed approach is also\nanalyzed in detail. Experiments show that our method achieves state-of-the-art\nfusion performance in objective and subjective assessment by comparing with the\nexisting methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palade_V/0/1/0/all/0/1\">Vasile Palade</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When less is more: Simplifying inputs aids neural network understanding. (arXiv:2201.05610v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.05610","description":"<p>How do neural network image classifiers respond to simpler and simpler\ninputs? And what do such responses reveal about the learning process? To answer\nthese questions, we need a clear measure of input simplicity (or inversely,\ncomplexity), an optimization objective that correlates with simplification, and\na framework to incorporate such objective into training and inference. Lastly\nwe need a variety of testbeds to experiment and evaluate the impact of such\nsimplification on learning. In this work, we measure simplicity with the\nencoding bit size given by a pretrained generative model, and minimize the bit\nsize to simplify inputs in training and inference. We investigate the effect of\nsuch simplification in several scenarios: conventional training, dataset\ncondensation and post-hoc explanations. In all settings, inputs are simplified\nalong with the original classification task, and we investigate the trade-off\nbetween input simplicity and task performance. For images with injected\ndistractors, such simplification naturally removes superfluous information. For\ndataset condensation, we find that inputs can be simplified with almost no\naccuracy degradation. When used in post-hoc explanation, our learning-based\nsimplification approach offers a valuable new tool to explore the basis of\nnetwork decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schirrmeister_R/0/1/0/all/0/1\">Robin Tibor Schirrmeister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rosanne Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ball_T/0/1/0/all/0/1\">Tonio Ball</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Specificity in Mammography Using Cross-correlation between Wavelet and Fourier Transform. (arXiv:2201.08385v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.08385","description":"<p>Breast cancer is in the most common malignant tumor in women. It accounted\nfor 30% of new malignant tumor cases. Although the incidence of breast cancer\nremains high around the world, the mortality rate has been continuously\nreduced. This is mainly due to recent developments in molecular biology\ntechnology and improved level of comprehensive diagnosis and standard\ntreatment. Early detection by mammography is an integral part of that. The most\ncommon breast abnormalities that may indicate breast cancer are masses and\ncalcifications. Previous detection approaches usually obtain relatively high\nsensitivity but unsatisfactory specificity. We will investigate an approach\nthat applies the discrete wavelet transform and Fourier transform to parse the\nimages and extracts statistical features that characterize an image's content,\nsuch as the mean intensity and the skewness of the intensity. A naive Bayesian\nclassifier uses these features to classify the images. We expect to achieve an\noptimal high specificity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Liuhua Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Aggregation for Adaptive RGBT Tracking. (arXiv:2201.08949v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08949","description":"<p>Visual object tracking with RGB and thermal infrared (TIR) spectra available,\nshorted in RGBT tracking, is a novel and challenging research topic which draws\nincreasing attention nowadays. In this paper, we propose an RGBT tracker which\ntakes spatio-temporal clues into account for robust appearance model learning,\nand simultaneously, constructs an adaptive fusion sub-network for cross-modal\ninteractions. Unlike most existing RGBT trackers that implement object tracking\ntasks with only spatial information included, temporal information is further\nconsidered in this method. Specifically, different from traditional Siamese\ntrackers, which only obtain one search image during the process of picking up\ntemplate-search image pairs, an extra search sample adjacent to the original\none is selected to predict the temporal transformation, resulting in improved\nrobustness of tracking performance.As for multi-modal tracking, constrained to\nthe limited RGBT datasets, the adaptive fusion sub-network is appended to our\nmethod at the decision level to reflect the complementary characteristics\ncontained in two modalities. To design a thermal infrared assisted RGB tracker,\nthe outputs of the classification head from the TIR modality are taken into\nconsideration before the residual connection from the RGB modality. Extensive\nexperimental results on three challenging datasets, i.e. VOT-RGBT2019, GTOT and\nRGBT210, verify the effectiveness of our method. Code will be shared at\n\\textcolor{blue}{\\emph{https://github.com/Zhangyong-Tang/TAAT}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhangyong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Representation Learning with Self-Supervised Attention for Low-Label High-data Regime. (arXiv:2201.08951v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08951","description":"<p>Self-supervision has shown outstanding results for natural language\nprocessing, and more recently, for image recognition. Simultaneously, vision\ntransformers and its variants have emerged as a promising and scalable\nalternative to convolutions on various computer vision tasks. In this paper, we\nare the first to question if self-supervised vision transformers (SSL-ViTs) can\nbe adapted to two important computer vision tasks in the low-label, high-data\nregime: few-shot image classification and zero-shot image retrieval. The\nmotivation is to reduce the number of manual annotations required to train a\nvisual embedder, and to produce generalizable and semantically meaningful\nembeddings. For few-shot image classification we train SSL-ViTs without any\nsupervision, on external data, and use this trained embedder to adapt quickly\nto novel classes with limited number of labels. For zero-shot image retrieval,\nwe use SSL-ViTs pre-trained on a large dataset without any labels and fine-tune\nthem with several metric learning objectives. Our self-supervised attention\nrepresentations outperforms the state-of-the-art on several public benchmarks\nfor both tasks, namely miniImageNet and CUB200 for few-shot image\nclassification by up-to 6%-10%, and Stanford Online Products, Cars196 and\nCUB200 for zero-shot image retrieval by up-to 4%-11%. Code is available at\n\\url{https://github.com/AutoVision-cloud/SSL-ViT-lowlabel-highdata}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Prarthana Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaonan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fehervari_I/0/1/0/all/0/1\">Istv&#xe1;n Feh&#xe9;rv&#xe1;ri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jason Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Object Tracking on Multi-modal RGB-D Videos: A Review. (arXiv:2201.09207v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09207","description":"<p>The development of visual object tracking has continued for decades. Recent\nyears, as the wide accessibility of the low-cost RGBD sensors, the task of\nvisual object tracking on RGB-D videos has drawn much attention. Compared to\nconventional RGB-only tracking, the RGB-D videos can provide more information\nthat facilitates objecting tracking in some complicated scenarios. The goal of\nthis review is to summarize the relative knowledge of the research filed of\nRGB-D tracking. To be specific, we will generalize the related RGB-D tracking\nbenchmarking datasets as well as the corresponding performance measurements.\nBesides, the existing RGB-D tracking methods are summarized in the paper.\nMoreover, we discuss the possible future direction in the field of RGB-D\ntracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xue-Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face recognition via compact second order image gradient orientations. (arXiv:2201.09246v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09246","description":"<p>Conventional subspace learning approaches based on image gradient\norientations only employ the first-order gradient information. However, recent\nresearches on human vision system (HVS) uncover that the neural image is a\nlandscape or a surface whose geometric properties can be captured through the\nsecond order gradient information. The second order image gradient orientations\n(SOIGO) can mitigate the adverse effect of noises in face images. To reduce the\nredundancy of SOIGO, we propose compact SOIGO (CSOIGO) by applying linear\ncomplex principal component analysis (PCA) in SOIGO. Combined with\ncollaborative representation based classification (CRC) algorithm, the\nclassification performance of CSOIGO is further enhanced. CSOIGO is evaluated\nunder real-world disguise, synthesized occlusion and mixed variations.\nExperimental results indicate that the proposed method is superior to its\ncompeting approaches with few training samples, and even outperforms some\nprevailing deep neural network based approaches. The source code of CSOIGO is\navailable at https://github.com/yinhefeng/SOIGO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">He-Feng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiaoning Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey for Deep RGBT Tracking. (arXiv:2201.09296v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09296","description":"<p>Visual object tracking with the visible (RGB) and thermal infrared (TIR)\nelectromagnetic waves, shorted in RGBT tracking, recently draws increasing\nattention in the tracking community. Considering the rapid development of deep\nlearning, a survey for the recent deep neural network based RGBT trackers is\npresented in this paper. Firstly, we give brief introduction for the RGBT\ntrackers concluded into this category. Then, a comparison among the existing\nRGBT trackers on several challenging benchmarks is given statistically.\nSpecifically, MDNet and Siamese architectures are the two mainstream frameworks\nin the RGBT community, especially the former. Trackers based on MDNet achieve\nhigher performance while Siamese-based trackers satisfy the real-time\nrequirement. In summary, since the large-scale dataset LasHeR is published, the\nintegration of end-to-end framework, e.g., Siamese and Transformer, should be\nfurther considered to fulfil the real-time as well as more robust performance.\nFurthermore, the mathematical meaning should be more considered during\ndesigning the network. This survey can be treated as a look-up-table for\nresearchers who are concerned about RGBT tracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhangyong Tang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a> (1) ((1) Jiangnan University, China)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hybrid Quantum-Classical Algorithm for Robust Fitting. (arXiv:2201.10110v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10110","description":"<p>Fitting geometric models onto outlier contaminated data is provably\nintractable. Many computer vision systems rely on random sampling heuristics to\nsolve robust fitting, which do not provide optimality guarantees and error\nbounds. It is therefore critical to develop novel approaches that can bridge\nthe gap between exact solutions that are costly, and fast heuristics that offer\nno quality assurances. In this paper, we propose a hybrid quantum-classical\nalgorithm for robust fitting. Our core contribution is a novel robust fitting\nformulation that solves a sequence of integer programs and terminates with a\nglobal solution or an error bound. The combinatorial subproblems are amenable\nto a quantum annealer, which helps to tighten the bound efficiently. While our\nusage of quantum computing does not surmount the fundamental intractability of\nrobust fitting, by providing error bounds our algorithm is a practical\nimprovement over randomised heuristics. Moreover, our work represents a\nconcrete application of quantum computing in computer vision. We present\nresults obtained using an actual quantum computer (D-Wave Advantage) and via\nsimulation. Source code: https://github.com/dadung/HQC-robust-fitting\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doan_A/0/1/0/all/0/1\">Anh-Dzung Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasdelli_M/0/1/0/all/0/1\">Michele Sasdelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suter_D/0/1/0/all/0/1\">David Suter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Tat-Jun Chin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSNet: A Deep Multi-scale Submanifold Network for Visual Classification. (arXiv:2201.10145v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10145","description":"<p>The Symmetric Positive Definite (SPD) matrix has received wide attention as a\ntool for visual data representation in computer vision. Although there are many\ndifferent attempts to develop effective deep architectures for data processing\non the Riemannian manifold of SPD matrices, a very few solutions explicitly\nmine the local geometrical information in deep SPD feature representations.\nWhile CNNs have demonstrated the potential of hierarchical local pattern\nextraction even for SPD represented data, we argue that it is of utmost\nimportance to ensure the preservation of local geometric information in the SPD\nnetworks. Accordingly, in this work we propose an SPD network designed with\nthis objective in mind. In particular, we propose an architecture, referred to\nas MSNet, which fuses geometrical multi-scale information. We first analyse the\nconvolution operator commonly used for mapping the local information in\nEuclidean deep networks from the perspective of a higher level of abstraction\nafforded by the Category Theory. Based on this analysis, we postulate a\nsubmanifold selection principle to guide the design of our MSNet. In\nparticular, we use it to design a submanifold fusion block to take advantage of\nthe rich local geometry encoded in the network layers. The experiments\ninvolving multiple visual tasks show that our algorithm outperforms most\nRiemannian SOTA competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiwu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1\">Josef Kittler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Image Fusion Method based on Feature Mutual Mapping. (arXiv:2201.10152v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10152","description":"<p>Deep learning-based image fusion approaches have obtained wide attention in\nrecent years, achieving promising performance in terms of visual perception.\nHowever, the fusion module in the current deep learning-based methods suffers\nfrom two limitations, \\textit{i.e.}, manually designed fusion function, and\ninput-independent network learning. In this paper, we propose an unsupervised\nadaptive image fusion method to address the above issues. We propose a feature\nmutual mapping fusion module and dual-branch multi-scale autoencoder. More\nspecifically, we construct a global map to measure the connections of pixels\nbetween the input source images. % The found mapping relationship guides the\nimage fusion. Besides, we design a dual-branch multi-scale network through\nsampling transformation to extract discriminative image features. We further\nenrich feature representations of different scales through feature aggregation\nin the decoding process. Finally, we propose a modified loss function to train\nthe network with efficient convergence property. Through sufficient training on\ninfrared and visible image data sets, our method also shows excellent\ngeneralized performance in multi-focus and medical image fusion. Our method\nachieves superior performance in both visual perception and objective\nevaluation. Experiments prove that the performance of our proposed method on a\nvariety of image fusion tasks surpasses other state-of-the-art methods, proving\nthe effectiveness and versatility of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1\">Dongyu Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guoyang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains. (arXiv:2201.11528v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.11528","description":"<p>Adversarial examples have posed a severe threat to deep neural networks due\nto their transferable nature. Currently, various works have paid great efforts\nto enhance the cross-model transferability, which mostly assume the substitute\nmodel is trained in the same domain as the target model. However, in reality,\nthe relevant information of the deployed model is unlikely to leak. Hence, it\nis vital to build a more practical black-box threat model to overcome this\nlimitation and evaluate the vulnerability of deployed models. In this paper,\nwith only the knowledge of the ImageNet domain, we propose a Beyond ImageNet\nAttack (BIA) to investigate the transferability towards black-box domains\n(unknown classification tasks). Specifically, we leverage a generative model to\nlearn the adversarial function for disrupting low-level features of input\nimages. Based on this framework, we further propose two variants to narrow the\ngap between the source and target domains from the data and model perspectives,\nrespectively. Extensive experiments on coarse-grained and fine-grained domains\ndemonstrate the effectiveness of our proposed methods. Notably, our methods\noutperform state-of-the-art approaches by up to 7.71\\% (towards coarse-grained\ndomains) and 25.91\\% (towards fine-grained domains) on average. Our code is\navailable at \\url{https://github.com/qilong-zhang/Beyond-ImageNet-Attack}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaodan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Checklist: Towards Testable Error Analysis of Image Models to Help System Designers Interrogate Model Capabilities. (arXiv:2201.11674v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.11674","description":"<p>Using large pre-trained models for image recognition tasks is becoming\nincreasingly common owing to the well acknowledged success of recent models\nlike vision transformers and other CNN-based models like VGG and Resnet. The\nhigh accuracy of these models on benchmark tasks has translated into their\npractical use across many domains including safety-critical applications like\nautonomous driving and medical diagnostics. Despite their widespread use, image\nmodels have been shown to be fragile to changes in the operating environment,\nbringing their robustness into question. There is an urgent need for methods\nthat systematically characterise and quantify the capabilities of these models\nto help designers understand and provide guarantees about their safety and\nrobustness. In this paper, we propose Vision Checklist, a framework aimed at\ninterrogating the capabilities of a model in order to produce a report that can\nbe used by a system designer for robustness evaluations. This framework\nproposes a set of perturbation operations that can be applied on the underlying\ndata to generate test samples of different types. The perturbations reflect\npotential changes in operating environments, and interrogate various properties\nranging from the strictly quantitative to more qualitative. Our framework is\nevaluated on multiple datasets like Tinyimagenet, CIFAR10, CIFAR100 and\nCamelyon17 and for models like ViT and Resnet. Our Vision Checklist proposes a\nspecific set of evaluations that can be integrated into the previously proposed\nconcept of a model card. Robustness evaluations like our checklist will be\ncrucial in future safety evaluations of visual perception modules, and be\nuseful for a wide range of stakeholders including designers, deployers, and\nregulators involved in the certification of these systems. Source code of\nVision Checklist would be open for public use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xin Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Legastelois_B/0/1/0/all/0/1\">Benedicte Legastelois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesh_B/0/1/0/all/0/1\">Bhargavi Ganesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_A/0/1/0/all/0/1\">Ajitha Rajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chockler_H/0/1/0/all/0/1\">Hana Chockler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belle_V/0/1/0/all/0/1\">Vaishak Belle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_S/0/1/0/all/0/1\">Stuart Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthy_S/0/1/0/all/0/1\">Subramanian Ramamoorthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural JPEG: End-to-End Image Compression Leveraging a Standard JPEG Encoder-Decoder. (arXiv:2201.11795v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.11795","description":"<p>Recent advances in deep learning have led to superhuman performance across a\nvariety of applications. Recently, these methods have been successfully\nemployed to improve the rate-distortion performance in the task of image\ncompression. However, current methods either use additional post-processing\nblocks on the decoder end to improve compression or propose an end-to-end\ncompression scheme based on heuristics. For the majority of these, the trained\ndeep neural networks (DNNs) are not compatible with standard encoders and would\nbe difficult to deply on personal computers and cellphones. In light of this,\nwe propose a system that learns to improve the encoding performance by\nenhancing its internal neural representations on both the encoder and decoder\nends, an approach we call Neural JPEG. We propose frequency domain pre-editing\nand post-editing methods to optimize the distribution of the DCT coefficients\nat both encoder and decoder ends in order to improve the standard compression\n(JPEG) method. Moreover, we design and integrate a scheme for jointly learning\nquantization tables within this hybrid neural compression framework.Experiments\ndemonstrate that our approach successfully improves the rate-distortion\nperformance over JPEG across various quality metrics, such as PSNR and MS-SSIM,\nand generates visually appealing images with better color retention quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mali_A/0/1/0/all/0/1\">Ankur Mali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ororbia_A/0/1/0/all/0/1\">Alexander Ororbia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kifer_D/0/1/0/all/0/1\">Daniel Kifer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Giles_L/0/1/0/all/0/1\">Lee Giles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer-aided Recognition and Assessment of a Porous Bioelastomer on Ultrasound Images for Regenerative Medicine Applications. (arXiv:2201.11987v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.11987","description":"<p>Biodegradable elastic scaffolds have attracted more and more attention in the\nfield of soft tissue repair and tissue engineering. These scaffolds made of\nporous bioelastomers support tissue ingrowth along with their own degradation.\nIt is necessary to develop a computer-aided analyzing method based on\nultrasound images to identify the degradation performance of the scaffold, not\nonly to obviate the need to do destructive testing, but also to monitor the\nscaffold's degradation and tissue ingrowth over time. It is difficult using a\nsingle traditional image processing algorithm to extract continuous and\naccurate contour of a porous bioelastomer. This paper proposes a joint\nalgorithm for the bioelastomer's contour detection and a texture feature\nextraction method for monitoring the degradation behavior of the bioelastomer.\nMean-shift clustering method is used to obtain the bioelastomer's and native\ntissue's clustering feature information. Then the OTSU image binarization\nmethod automatically selects the optimal threshold value to convert the\ngrayscale ultrasound image into a binary image. The Canny edge detector is used\nto extract the complete bioelastomer's contour. The first-order and\nsecond-order statistical features of texture are extracted. The proposed joint\nalgorithm not only achieves the ideal extraction of the bioelastomer's contours\nin ultrasound images, but also gives valuable feedback of the degradation\nbehavior of the bioelastomer at the implant site based on the changes of\ntexture characteristics and contour area. The preliminary results of this study\nsuggest that the proposed computer-aided image processing techniques have\nvalues and potentials in the non-invasive analysis of tissue scaffolds in vivo\nbased on ultrasound images and may help tissue engineers evaluate the tissue\nscaffold's degradation and cellular ingrowth progress and improve the scaffold\ndesigns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Dun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_K/0/1/0/all/0/1\">Kaixuan Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanying Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jia Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dreglea_A/0/1/0/all/0/1\">Aliona Dreglea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jiao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Single-shot Depth Estimation using Perceptual Reconstruction. (arXiv:2201.12170v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12170","description":"<p>Real-time estimation of actual object depth is a module that is essential to\nperforming various autonomous system tasks such as 3D reconstruction, scene\nunderstanding and condition assessment of machinery parts. During the last\ndecade of machine learning, extensive deployment of deep learning methods to\ncomputer vision tasks has yielded approaches that succeed in achieving\nrealistic depth synthesis out of a simple RGB modality. While most of these\nmodels are based on paired depth data or availability of video sequences and\nstereo images, methods for single-view depth synthesis in a fully unsupervised\nsetting have hardly been explored. This study presents the most recent advances\nin the field of generative neural networks, leveraging them to perform fully\nunsupervised single-shot depth synthesis. Two generators for RGB-to-depth and\ndepth-to-RGB transfer are implemented and simultaneously optimized using the\nWasserstein-1 distance and a novel perceptual reconstruction term. To ensure\nthat the proposed method is plausible, we comprehensively evaluate the models\nusing industrial surface depth data as well as the Texas 3D Face Recognition\nDatabase and the SURREAL dataset that records body depth. The success observed\nin this study suggests the great potential for unsupervised single-shot depth\nestimation in real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Angermann_C/0/1/0/all/0/1\">Christoph Angermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwab_M/0/1/0/all/0/1\">Matthias Schwab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haltmeier_M/0/1/0/all/0/1\">Markus Haltmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laubichler_C/0/1/0/all/0/1\">Christian Laubichler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsson_S/0/1/0/all/0/1\">Steinbj&#xf6;rn J&#xf3;nsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Learning Music Composition and Dance Choreography. (arXiv:2201.11999v1 [cs.SD] CROSS LISTED)","link":"http://arxiv.org/abs/2201.11999","description":"<p>Music and dance have always co-existed as pillars of human activities,\ncontributing immensely to the cultural, social, and entertainment functions in\nvirtually all societies. Notwithstanding the gradual systematization of music\nand dance into two independent disciplines, their intimate connection is\nundeniable and one art-form often appears incomplete without the other. Recent\nresearch works have studied generative models for dance sequences conditioned\non music. The dual task of composing music for given dances, however, has been\nlargely overlooked. In this paper, we propose a novel extension, where we\njointly model both tasks in a dual learning approach. To leverage the duality\nof the two modalities, we introduce an optimal transport objective to align\nfeature embeddings, as well as a cycle consistency loss to foster overall\nconsistency. Experimental results demonstrate that our dual learning framework\nimproves individual task performance, delivering generated music compositions\nand dance choreographs that are realistic and faithful to the conditioned\ninputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-31T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}