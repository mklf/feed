{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-24T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Recommendations for Systematic Research on Emergent Language. (arXiv:2206.11302v1 [cs.MA])","link":"http://arxiv.org/abs/2206.11302","description":"<p>Emergent language is unique among fields within the discipline of machine\nlearning for its open-endedness, not obviously presenting well-defined problems\nto be solved. As a result, the current research in the field has largely been\nexploratory: focusing on establishing new problems, techniques, and phenomena.\nYet after these problems have been established, subsequent progress requires\nresearch which can measurably demonstrate how it improves on prior approaches.\nThis type of research is what we call systematic research; in this paper, we\nillustrate this mode of research specifically for emergent language. We first\nidentify the overarching goals of emergent language research, categorizing them\nas either science or engineering. Using this distinction, we present core\nmethodological elements of science and engineering, analyze their role in\ncurrent emergent language research, and recommend how to apply these elements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boldt_B/0/1/0/all/0/1\">Brendon Boldt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1\">David Mortensen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GODEL: Large-Scale Pre-Training for Goal-Directed Dialog. (arXiv:2206.11309v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11309","description":"<p>We introduce GODEL (Grounded Open Dialogue Language Model), a large\npre-trained language model for dialog. In contrast with earlier models such as\nDialoGPT, GODEL leverages a new phase of grounded pre-training designed to\nbetter support adapting GODEL to a wide range of downstream dialog tasks that\nrequire information external to the current conversation (e.g., a database or\ndocument) to produce good responses. Experiments against an array of benchmarks\nthat encompass task-oriented dialog, conversational QA, and grounded\nopen-domain dialog show that GODEL outperforms state-of-the-art pre-trained\ndialog models in few-shot fine-tuning setups, in terms of both human and\nautomatic evaluation. A novel feature of our evaluation methodology is the\nintroduction of a notion of utility that assesses the usefulness of responses\n(extrinsic evaluation) in addition to their communicative features (intrinsic\nevaluation). We show that extrinsic evaluation offers improved inter-annotator\nagreement and correlation with automated metrics. Code and data processing\nscripts are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockett_C/0/1/0/all/0/1\">Chris Brockett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liden_L/0/1/0/all/0/1\">Lars Liden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nouri_E/0/1/0/all/0/1\">Elnaz Nouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolan_B/0/1/0/all/0/1\">Bill Dolan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DP-Parse: Finding Word Boundaries from Raw Speech with an Instance Lexicon. (arXiv:2206.11332v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11332","description":"<p>Finding word boundaries in continuous speech is challenging as there is\nlittle or no equivalent of a 'space' delimiter between words. Popular Bayesian\nnon-parametric models for text segmentation use a Dirichlet process to jointly\nsegment sentences and build a lexicon of word types. We introduce DP-Parse,\nwhich uses similar principles but only relies on an instance lexicon of word\ntokens, avoiding the clustering errors that arise with a lexicon of word types.\nOn the Zero Resource Speech Benchmark 2017, our model sets a new speech\nsegmentation state-of-the-art in 5 languages. The algorithm monotonically\nimproves with better input representations, achieving yet higher scores when\nfed with weakly supervised inputs. Despite lacking a type lexicon, DP-Parse can\nbe pipelined to a language model and learn semantic and syntactic\nrepresentations as assessed by a new spoken word embedding benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Algayres_R/0/1/0/all/0/1\">Robin Algayres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricoul_T/0/1/0/all/0/1\">Tristan Ricoul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karadayi_J/0/1/0/all/0/1\">Julien Karadayi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurencon_H/0/1/0/all/0/1\">Hugo Lauren&#xe7;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiem_S/0/1/0/all/0/1\">Salah Zaiem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Injection: Parameterization of Fixed Inputs. (arXiv:2206.11349v1 [cs.LG])","link":"http://arxiv.org/abs/2206.11349","description":"<p>Recent works have shown that attaching prompts to the input is effective at\nconditioning Language Models (LM) to perform specific tasks. However, prompts\nare always included in the input text during inference, thus incurring\nsubstantial computational and memory overhead. Also, there is currently no\nstraightforward method of utilizing prompts that are longer than the maximum\ninput length of the LMs without incurring additional costs during inference. We\npropose Prompt Injection (PI), a novel formulation of injecting the prompt into\nthe parameters of an LM to be an efficient alternative to attaching fixed\nprompts to the input. We show that in scenarios with long fixed prompts, PI can\nbe up to 280 times more efficient in terms of total FLOPs than previous\napproaches. We further explore methodologies for PI and show promising results\nin persona-dependent conversation, semantic parsing, and zero-shot learning\nwith task instructions. Through these explorations, we show that PI can be a\npromising direction for conditioning language models, especially in scenarios\nwith long and fixed prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunbi Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1\">Yongrae Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joel Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large Language Models. (arXiv:2206.11484v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11484","description":"<p>This paper presents exploratory work on whether and to what extent biases\nagainst queer and trans people are encoded in large language models (LLMs) such\nas BERT. We also propose a method for reducing these biases in downstream\ntasks: finetuning the models on data written by and/or about queer people. To\nmeasure anti-queer bias, we introduce a new benchmark dataset, WinoQueer,\nmodeled after other bias-detection benchmarks but addressing homophobic and\ntransphobic biases. We found that BERT shows significant homophobic bias, but\nthis bias can be mostly mitigated by finetuning BERT on a natural language\ncorpus written by members of the LGBTQ+ community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Felkner_V/0/1/0/all/0/1\">Virginia K. Felkner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Ho-Chun Herbert Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_E/0/1/0/all/0/1\">Eugene Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining Error Templates for Grammatical Error Correction. (arXiv:2206.11569v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11569","description":"<p>Some grammatical error correction (GEC) systems incorporate hand-crafted\nrules and achieve positive results. However, manually defining rules is\ntime-consuming and laborious. In view of this, we propose a method to mine\nerror templates for GEC automatically. An error template is a regular\nexpression aiming at identifying text errors. We use the web crawler to acquire\nsuch error templates from the Internet. For each template, we further select\nthe corresponding corrective action by using the language model perplexity as a\ncriterion. We have accumulated 1,119 error templates for Chinese GEC based on\nthis method. Experimental results on the newly proposed CTC-2021 Chinese GEC\nbenchmark show that combing our error templates can effectively improve the\nperformance of a strong GEC system, especially on two error types with very\nlittle training data. Our error templates are available at\n\\url{https://github.com/HillZhang1999/gec_error_template}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haochen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1\">Zuyi Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenghua Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Cross-lingual Consumer Health Vocabulary with Word-Embedding from Comparable User Generated Content. (arXiv:2206.11612v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11612","description":"<p>The online health community (OHC) is the primary channel for laypeople to\nshare health information. To analyze the health consumer-generated content\n(HCGC) from the OHCs, identifying the colloquial medical expressions used by\nlaypeople is a critical challenge. The open-access and collaborative consumer\nhealth vocabulary (OAC CHV) is the controlled vocabulary for addressing such a\nchallenge. Nevertheless, OAC CHV is only available in English, limiting the\napplicability to other languages. This research aims to propose a cross-lingual\nautomatic term recognition framework for extending the English OAC CHV into a\ncross-lingual one. Our framework requires an English HCGC corpus and a\nnon-English (i.e., Chinese in this study) HCGC corpus as inputs. Two\nmonolingual word vector spaces are determined using skip-gram algorithm so that\neach space encodes common word associations from laypeople within a language.\nBased on isometry assumption, the framework align two monolingual spaces into a\nbilingual word vector space, where we employ cosine similarity as a metric for\nidentifying semantically similar words across languages. In the experiments,\nour framework demonstrates that it can effectively retrieve similar medical\nterms, including colloquial expressions, across languages and further\nfacilitate compilation of cross-lingual CHV.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chia-Hsuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Christopher C. Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Theory-Grounded Measurement of U.S. Social Stereotypes in English Language Models. (arXiv:2206.11684v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11684","description":"<p>NLP models trained on text have been shown to reproduce human stereotypes,\nwhich can magnify harms to marginalized groups when systems are deployed at\nscale. We adapt the Agency-Belief-Communion (ABC) stereotype model of Koch et\nal. (2016) from social psychology as a framework for the systematic study and\ndiscovery of stereotypic group-trait associations in language models (LMs). We\nintroduce the sensitivity test (SeT) for measuring stereotypical associations\nfrom language models. To evaluate SeT and other measures using the ABC model,\nwe collect group-trait judgments from U.S.-based subjects to compare with\nEnglish LM stereotypes. Finally, we extend this framework to measure LM\nstereotyping of intersectional identities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Trista Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sotnikova_A/0/1/0/all/0/1\">Anna Sotnikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daume_H/0/1/0/all/0/1\">Hal Daum&#xe9; III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudinger_R/0/1/0/all/0/1\">Rachel Rudinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Linda Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery. (arXiv:2206.11706v1 [eess.AS])","link":"http://arxiv.org/abs/2206.11706","description":"<p>Latent Dirichlet allocation (LDA) is widely used for unsupervised topic\nmodelling on sets of documents. No temporal information is used in the model.\nHowever, there is often a relationship between the corresponding topics of\nconsecutive tokens. In this paper, we present an extension to LDA that uses a\nMarkov chain to model temporal information. We use this new model for acoustic\nunit discovery from speech. As input tokens, the model takes a discretised\nencoding of speech from a vector quantised (VQ) neural network with 512 codes.\nThe goal is then to map these 512 VQ codes to 50 phone-like units (topics) in\norder to more closely resemble true phones. In contrast to the base LDA, which\nonly considers how VQ codes co-occur within utterances (documents), the Markov\nchain LDA additionally captures how consecutive codes follow one another. This\nextension leads to an increase in cluster quality and phone segmentation\nresults compared to the base LDA. Compared to a recent vector quantised neural\nnetwork approach that also learns 50 units, the extended LDA model performs\nbetter in phone segmentation but worse in mutual information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Merwe_W/0/1/0/all/0/1\">Werner van der Merwe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Preez_J/0/1/0/all/0/1\">Johan du Preez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models. (arXiv:2206.11719v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11719","description":"<p>The objective of pre-trained language models is to learn contextual\nrepresentations of textual data. Pre-trained language models have become\nmainstream in natural language processing and code modeling. Using probes, a\ntechnique to study the linguistic properties of hidden vector spaces, previous\nworks have shown that these pre-trained language models encode simple\nlinguistic properties in their hidden representations. However, none of the\nprevious work assessed whether these models encode the whole grammatical\nstructure of a programming language. In this paper, we prove the existence of a\n\\textit{syntactic subspace}, lying in the hidden representations of pre-trained\nlanguage models, which contain the syntactic information of the programming\nlanguage. We show that this subspace can be extracted from the models'\nrepresentations and define a novel probing method, the AST-Probe, that enables\nrecovering the whole abstract syntax tree (AST) of an input code snippet. In\nour experimentations, we show that this syntactic subspace exists in five\nstate-of-the-art pre-trained language models. In addition, we highlight that\nthe middle layers of the models are the ones that encode most of the AST\ninformation. Finally, we estimate the optimal size of this syntactic subspace\nand show that its dimension is substantially lower than those of the models'\nrepresentation spaces. This suggests that pre-trained language models use a\nsmall part of their representation spaces to encode syntactic information of\nthe programming languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lopez_J/0/1/0/all/0/1\">Jos&#xe9; Antonio Hern&#xe1;ndez L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weyssow_M/0/1/0/all/0/1\">Martin Weyssow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuadrado_J/0/1/0/all/0/1\">Jes&#xfa;s S&#xe1;nchez Cuadrado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahraoui_H/0/1/0/all/0/1\">Houari Sahraoui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chat, Shift and Perform: Bridging the Gap between Task-oriented and Non-task-oriented Dialog Systems. (arXiv:2206.11813v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11813","description":"<p>We propose CASPER (ChAt, Shift and PERform), a novel dialog system consisting\nof three types of dialog models: chatter, shifter, and performer. Shifter,\nwhich is designed for topic switching, enables a seamless flow of dialog from\nopen-domain chat- to task-oriented dialog. In a user study, CASPER gave a\nbetter impression in terms of naturalness of response, lack of forced topic\nswitching, and satisfaction compared with a baseline dialog system trained in\nan end-to-end manner. In an ablation study, we found that naturalness of\nresponse, dialog satisfaction, and task-elicitation rate improved compared with\nwhen shifter was removed from CASPER, indicating that topic shift with shifter\nsupports the introduction of natural task-oriented dialog.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoshino_T/0/1/0/all/0/1\">Teppei Yoshino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukuchi_Y/0/1/0/all/0/1\">Yosuke Fukuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumori_S/0/1/0/all/0/1\">Shoya Matsumori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imai_M/0/1/0/all/0/1\">Michita Imai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Always Keep your Target in Mind: Studying Semantics and Improving Performance of Neural Lexical Substitution. (arXiv:2206.11815v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11815","description":"<p>Lexical substitution, i.e. generation of plausible words that can replace a\nparticular target word in a given context, is an extremely powerful technology\nthat can be used as a backbone of various NLP applications, including word\nsense induction and disambiguation, lexical relation extraction, data\naugmentation, etc. In this paper, we present a large-scale comparative study of\nlexical substitution methods employing both rather old and most recent language\nand masked language models (LMs and MLMs), such as context2vec, ELMo, BERT,\nRoBERTa, XLNet. We show that already competitive results achieved by SOTA\nLMs/MLMs can be further substantially improved if information about the target\nword is injected properly. Several existing and new target word injection\nmethods are compared for each LM/MLM using both intrinsic evaluation on lexical\nsubstitution datasets and extrinsic evaluation on word sense induction (WSI)\ndatasets. On two WSI datasets we obtain new SOTA results. Besides, we analyze\nthe types of semantic relations between target words and their substitutes\ngenerated by different models or given by annotators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arefyev_N/0/1/0/all/0/1\">Nikolay Arefyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheludko_B/0/1/0/all/0/1\">Boris Sheludko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Podolskiy_A/0/1/0/all/0/1\">Alexander Podolskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panchenko_A/0/1/0/all/0/1\">Alexander Panchenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Obj2Sub: Unsupervised Conversion of Objective to Subjective Questions. (arXiv:2206.11848v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11848","description":"<p>Exams are conducted to test the learner's understanding of the subject. To\nprevent the learners from guessing or exchanging solutions, the mode of tests\nadministered must have sufficient subjective questions that can gauge whether\nthe learner has understood the concept by mandating a detailed answer. Hence,\nin this paper, we propose a novel hybrid unsupervised approach leveraging\nrule-based methods and pre-trained dense retrievers for the novel task of\nautomatically converting the objective questions to subjective questions. We\nobserve that our approach outperforms the existing data-driven approaches by\n36.45% as measured by Recall@k and Precision@k.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chhabra_A/0/1/0/all/0/1\">Aarish Chhabra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_N/0/1/0/all/0/1\">Nandini Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_V/0/1/0/all/0/1\">Venktesh V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohania_M/0/1/0/all/0/1\">Mukesh Mohania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwivedi_D/0/1/0/all/0/1\">Deep Dwivedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-based Virtual Adversarial Training for Text Classification with Noisy Labels. (arXiv:2206.11851v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11851","description":"<p>Deep neural networks (DNNs) have a high capacity to completely memorize noisy\nlabels given sufficient training time, and its memorization, unfortunately,\nleads to performance degradation. Recently, virtual adversarial training (VAT)\nattracts attention as it could further improve the generalization of DNNs in\nsemi-supervised learning. The driving force behind VAT is to prevent the models\nfrom overfitting data points by enforcing consistency between the inputs and\nthe perturbed inputs. This strategy could be helpful in learning from noisy\nlabels if it prevents neural models from learning noisy samples while\nencouraging the models to generalize clean samples. In this paper, we propose\ncontext-based virtual adversarial training (ConVAT) to prevent a text\nclassifier from overfitting to noisy labels. Unlike the previous works, the\nproposed method performs the adversarial training at the context level rather\nthan the inputs. It makes the classifier not only learn its label but also its\ncontextual neighbors, which alleviates the learning from noisy labels by\npreserving contextual semantics on each data point. We conduct extensive\nexperiments on four text classification datasets with two types of label\nnoises. Comprehensive experimental results clearly show that the proposed\nmethod works quite well even with extremely noisy settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Do-Myoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yeachan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_C/0/1/0/all/0/1\">Chang-gyun Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HYU at SemEval-2022 Task 2: Effective Idiomaticity Detection with Consideration at Different Levels of Contextualization. (arXiv:2206.11854v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11854","description":"<p>We propose a unified framework that enables us to consider various aspects of\ncontextualization at different levels to better identify the idiomaticity of\nmulti-word expressions. Through extensive experiments, we demonstrate that our\napproach based on the inter- and inner-sentence context of a target MWE is\neffective in improving the performance of related models. We also share our\nexperience in detail on the task of SemEval-2022 Tasks 2 such that future work\non the same task can be benefited from this.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joung_Y/0/1/0/all/0/1\">Youngju Joung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taeuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Transliterated Words for Finding Similarity in Inter-Language News Articles using Machine Learning. (arXiv:2206.11860v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11860","description":"<p>Finding similarities between two inter-language news articles is a\nchallenging problem of Natural Language Processing (NLP). It is difficult to\nfind similar news articles in a different language other than the native\nlanguage of user, there is a need for a Machine Learning based automatic system\nto find the similarity between two inter-language news articles. In this\narticle, we propose a Machine Learning model with the combination of English\nUrdu word transliteration which will show whether the English news article is\nsimilar to the Urdu news article or not. The existing approaches to find\nsimilarities has a major drawback when the archives contain articles of\nlow-resourced languages like Urdu along with English news article. The existing\napproaches to find similarities has drawback when the archives contain\nlow-resourced languages like Urdu along with English news articles. We used\nlexicon to link Urdu and English news articles. As Urdu language processing\napplications like machine translation, text to speech, etc are unable to handle\nEnglish text at the same time so this research proposed technique to find\nsimilarities in English and Urdu news articles based on transliteration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naeem_S/0/1/0/all/0/1\">Sameea Naeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_D/0/1/0/all/0/1\">Dr. Arif ur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haider_S/0/1/0/all/0/1\">Syed Mujtaba Haider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mughal_A/0/1/0/all/0/1\">Abdul Basit Mughal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Generation of Programming Exercises and Code Explanations with Large Language Models. (arXiv:2206.11861v1 [cs.SE])","link":"http://arxiv.org/abs/2206.11861","description":"<p>OpenAI Codex is a recent large language model from the GPT-3 family for\ntranslating code into natural language and vice versa. Recent explorations of\nCodex have highlighted that given typical introductory programming exercise\nproblem statements as input, the model can generate code solutions well above\nthe level of an average student. In this article, we explore the natural\nlanguage generation capabilities of Codex in two different phases of the life\nof a programming exercise; automatically creating programming exercises\n(including sample solutions and test cases) and explanations of written code,\nassessing these qualitatively and quantitatively. We find the majority of this\nautomatically generated content both novel and sensible, and in many cases\nready to use as is. We further find that influencing the content of the created\nprogramming exercises is remarkably easy with minor modifications to the input.\nOur analysis suggests that there is significant value in massive generative\nmachine learning models as a tool for instructors, although some oversight\nmight be needed to ensure the quality of the generated content before it is\ndelivered to students. We further discuss the implications of OpenAI Codex and\nsimilar tools for introductory programming education and highlight future\nresearch streams that have the potential to improve the quality of the\neducational experience for both teachers and students alike.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarsa_S/0/1/0/all/0/1\">Sami Sarsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1\">Paul Denny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellas_A/0/1/0/all/0/1\">Arto Hellas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leinonen_J/0/1/0/all/0/1\">Juho Leinonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Urdu News Article Recommendation Model using Natural Language Processing Techniques. (arXiv:2206.11862v1 [cs.IR])","link":"http://arxiv.org/abs/2206.11862","description":"<p>There are several online newspapers in urdu but for the users it is difficult\nto find the content they are looking for because these most of them contain\nirrelevant data and most users did not get what they want to retrieve. Our\nproposed framework will help to predict Urdu news in the interests of users and\nreduce the users searching time for news. For this purpose, NLP techniques are\nused for pre-processing, and then TF-IDF with cosine similarity is used for\ngaining the highest similarity and recommended news on user preferences.\nMoreover, the BERT language model is also used for similarity, and by using the\nBERT model similarity increases as compared to TF-IDF so the approach works\nbetter with the BERT language model and recommends news to the user on their\ninterest. The news is recommended when the similarity of the articles is above\n60 percent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abbas_S/0/1/0/all/0/1\">Syed Zain Abbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_D/0/1/0/all/0/1\">Dr. Arif ur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mughal_A/0/1/0/all/0/1\">Abdul Basit Mughal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haider_S/0/1/0/all/0/1\">Syed Mujtaba Haider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CHEF: A Pilot Chinese Dataset for Evidence-Based Fact-Checking. (arXiv:2206.11863v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11863","description":"<p>The explosion of misinformation spreading in the media ecosystem urges for\nautomated fact-checking. While misinformation spans both geographic and\nlinguistic boundaries, most work in the field has focused on English. Datasets\nand tools available in other languages, such as Chinese, are limited. In order\nto bridge this gap, we construct CHEF, the first CHinese Evidence-based\nFact-checking dataset of 10K real-world claims. The dataset covers multiple\ndomains, ranging from politics to public health, and provides annotated\nevidence retrieved from the Internet. Further, we develop established baselines\nand a novel approach that is able to model the evidence retrieval as a latent\nvariable, allowing jointly training with the veracity prediction model in an\nend-to-end fashion. Extensive experiments show that CHEF will provide a\nchallenging testbed for the development of fact-checking systems designed to\nretrieve and reason over non-English claims.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhijiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guanyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Romantic-Computing. (arXiv:2206.11864v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11864","description":"<p>In this paper we compare various text generation models' ability to write\npoetry in the style of early English Romanticism. These models include:\nCharacter-Level Recurrent Neural Networks with Long Short-Term Memory, Hugging\nFace's GPT-2, OpenAI's GPT-3, and EleutherAI's GPT-NEO. Quality was measured\nbased syllable count and coherence with the automatic evaluation metric GRUEN.\nCharacter-Level Recurrent Neural Networks performed far worse compared to\ntransformer models. And, as parameter-size increased, the quality of\ntransformer models' poems improved. These models are typically not compared in\na creative context, and we are happy to contribute.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Horishny_E/0/1/0/all/0/1\">Elizabeth Horishny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BOS at LSCDiscovery: Lexical Substitution for Interpretable Lexical Semantic Change Detection. (arXiv:2206.11865v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11865","description":"<p>We propose a solution for the LSCDiscovery shared task on Lexical Semantic\nChange Detection in Spanish. Our approach is based on generating lexical\nsubstitutes that describe old and new senses of a given word. This approach\nachieves the second best result in sense loss and sense gain detection\nsubtasks. By observing those substitutes that are specific for only one time\nperiod, one can understand which senses were obtained or lost. This allows\nproviding more detailed information about semantic change to the user and makes\nour method interpretable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kudisov_A/0/1/0/all/0/1\">Artem Kudisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arefyev_N/0/1/0/all/0/1\">Nikolay Arefyev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Policy Framework for Deep Learning-Based Fake News Detection. (arXiv:2206.11866v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11866","description":"<p>Connectivity plays an ever-increasing role in modern society, with people all\naround the world having easy access to rapidly disseminated information.\nHowever, a more interconnected society enables the spread of intentionally\nfalse information. To mitigate the negative impacts of fake news, it is\nessential to improve detection methodologies. This work introduces Multi-Policy\nStatement Checker (MPSC), a framework that automates fake news detection by\nusing deep learning techniques to analyze a statement itself and its related\nnews articles, predicting whether it is seemingly credible or suspicious. The\nproposed framework was evaluated using four merged datasets containing real and\nfake news. Long-Short Term Memory (LSTM), Gated Recurrent Unit (GRU) and\nBidirectional Encoder Representations from Transformers (BERT) models were\ntrained to utilize both lexical and syntactic features, and their performance\nwas evaluated. The obtained results demonstrate that a multi-policy analysis\nreliably identifies suspicious statements, which can be advantageous for fake\nnews detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vitorino_J/0/1/0/all/0/1\">Jo&#xe3;o Vitorino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dias_T/0/1/0/all/0/1\">Tiago Dias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_T/0/1/0/all/0/1\">Tiago Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_N/0/1/0/all/0/1\">Nuno Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Praca_I/0/1/0/all/0/1\">Isabel Pra&#xe7;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lifelong Learning Natural Language Processing Approach for Multilingual Data Classification. (arXiv:2206.11867v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11867","description":"<p>The abundance of information in digital media, which in today's world is the\nmain source of knowledge about current events for the masses, makes it possible\nto spread disinformation on a larger scale than ever before. Consequently,\nthere is a need to develop novel fake news detection approaches capable of\nadapting to changing factual contexts and generalizing previously or\nconcurrently acquired knowledge. To deal with this problem, we propose a\nlifelong learning-inspired approach, which allows for fake news detection in\nmultiple languages and the mutual transfer of knowledge acquired in each of\nthem. Both classical feature extractors, such as Term frequency-inverse\ndocument frequency or Latent Dirichlet Allocation, and integrated deep NLP\n(Natural Language Processing) BERT (Bidirectional Encoder Representations from\nTransformers) models paired with MLP (Multilayer Perceptron) classifier, were\nemployed. The results of experiments conducted on two datasets dedicated to the\nfake news classification task (in English and Spanish, respectively), supported\nby statistical analysis, confirmed that utilization of additional languages\ncould improve performance for traditional methods. Also, in some cases\nsupplementing the deep learning method with classical ones can positively\nimpact obtained results. The ability of models to generalize the knowledge\nacquired between the analyzed languages was also observed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kozal_J/0/1/0/all/0/1\">J&#x119;drzej Kozal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Les_M/0/1/0/all/0/1\">Micha&#x142; Le&#x15b;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zyblewski_P/0/1/0/all/0/1\">Pawe&#x142; Zyblewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ksieniewicz_P/0/1/0/all/0/1\">Pawe&#x142; Ksieniewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wozniak_M/0/1/0/all/0/1\">Micha&#x142; Wo&#x17a;niak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Offline RL for Natural Language Generation with Implicit Language Q Learning. (arXiv:2206.11871v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11871","description":"<p>Large language models distill broad knowledge from text corpora. However,\nthey can be inconsistent when it comes to completing user specified tasks. This\nissue can be addressed by finetuning such models via supervised learning on\ncurated datasets, or via reinforcement learning. In this work, we propose a\nnovel offline RL motivated method, implicit language Q-learning (ILQL),\ndesigned for use on language models, that combines both the flexible utility\noptimization framework of traditional RL algorithms with supervised learning's\nability to leverage existing data and its simplicity and stability. Our method,\nbased on dynamic programming, employs a blend of value conservatism alongside\nan implicit dataset support constraint in learning value functions, which are\nthen used to guide language model generations towards maximizing utility. In\naddition to empirically validating ILQL, we present a detailed empirical\nanalysis of situations where offline RL can be useful in natural language\ngeneration settings, demonstrating how it can be a more effective utility\noptimizer than prior approaches for end-to-end dialogue, and how it can\neffectively optimize high variance reward functions based on subjective\njudgement, such as whether to label a comment as an example of toxic speech or\nnot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Snell_C/0/1/0/all/0/1\">Charlie Snell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostrikov_I/0/1/0/all/0/1\">Ilya Kostrikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mengjiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modulating Bottom-Up and Top-Down Visual Processing via Language-Conditional Filters. (arXiv:2003.12739v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.12739","description":"<p>How to best integrate linguistic and perceptual processing in multi-modal\ntasks that involve language and vision is an important open problem. In this\nwork, we argue that the common practice of using language in a top-down manner,\nto direct visual attention over high-level visual features, may not be optimal.\nWe hypothesize that the use of language to also condition the bottom-up\nprocessing from pixels to high-level features can provide benefits to the\noverall performance. To support our claim, we propose a U-Net-based model and\nperform experiments on two language-vision dense-prediction tasks: referring\nexpression segmentation and language-guided image colorization. We compare\nresults where either one or both of the top-down and bottom-up visual branches\nare conditioned on language. Our experiments reveal that using language to\ncontrol the filters for bottom-up visual processing in addition to top-down\nattention leads to better results on both tasks and achieves competitive\nperformance. Our linguistic analysis suggests that bottom-up conditioning\nimproves segmentation of objects especially when input text refers to low-level\nvisual concepts. Code is available at https://github.com/ilkerkesen/bvpr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kesen_I/0/1/0/all/0/1\">&#x130;lker Kesen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Can_O/0/1/0/all/0/1\">Ozan Arkan Can</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1\">Erkut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1\">Aykut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuret_D/0/1/0/all/0/1\">Deniz Yuret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base. (arXiv:2007.03875v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.03875","description":"<p>Complex question answering over knowledge base (Complex KBQA) is challenging\nbecause it requires various compositional reasoning capabilities, such as\nmulti-hop inference, attribute comparison, set operation. Existing benchmarks\nhave some shortcomings that limit the development of Complex KBQA: 1) they only\nprovide QA pairs without explicit reasoning processes; 2) questions are poor in\ndiversity or scale. To this end, we introduce KQA Pro, a dataset for Complex\nKBQA including ~120K diverse natural language questions. We introduce a\ncompositional and interpretable programming language KoPL to represent the\nreasoning process of complex questions. For each question, we provide the\ncorresponding KoPL program and SPARQL query, so that KQA Pro serves for both\nKBQA and semantic parsing tasks. Experimental results show that SOTA KBQA\nmethods cannot achieve promising results on KQA Pro as on current datasets,\nwhich suggests that KQA Pro is challenging and Complex KBQA requires further\nresearch efforts. We also treat KQA Pro as a diagnostic dataset for testing\nmultiple reasoning skills, conduct a thorough evaluation of existing models and\ndiscuss further directions for Complex KBQA. Our codes and datasets can be\nobtained from https://github.com/shijx12/KQAPro_Baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shulin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiaxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liangming Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Lunyiu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yutong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface Form Competition: Why the Highest Probability Answer Isn't Always Right. (arXiv:2104.08315v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08315","description":"<p>Large language models have shown promising results in zero-shot settings\n(Brown et al.,2020; Radford et al., 2019). For example, they can perform\nmultiple choice tasks simply by conditioning on a question and selecting the\nanswer with the highest probability.\n</p>\n<p>However, ranking by string probability can be problematic due to surface form\ncompetition-wherein different surface forms compete for probability mass, even\nif they represent the same underlying concept, e.g. \"computer\" and \"PC.\" Since\nprobability mass is finite, this lowers the probability of the correct answer,\ndue to competition from other strings that are valid answers (but not one of\nthe multiple choice options).\n</p>\n<p>We introduce Domain Conditional Pointwise Mutual Information, an alternative\nscoring function that directly compensates for surface form competition by\nsimply reweighing each option according to a term that is proportional to its a\npriori likelihood within the context of the specific zero-shot task. It\nachieves consistent gains in zero-shot performance over both calibrated (Zhao\net al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models\nover a variety of multiple choice datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Exploratory Study on Utilising the Web of Linked Data for Product Data Mining. (arXiv:2109.01411v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01411","description":"<p>The Linked Open Data practice has led to a significant growth of structured\ndata on the Web in the last decade. Such structured data describe real-world\nentities in a machine-readable way, and have created an unprecedented\nopportunity for research in the field of Natural Language Processing. However,\nthere is a lack of studies on how such data can be used, for what kind of\ntasks, and to what extent they can be useful for these tasks. This work focuses\non the e-commerce domain to explore methods of utilising such structured data\nto create language resources that may be used for product classification and\nlinking. We process billions of structured data points in the form of RDF\nn-quads, to create multi-million words of product-related corpora that are\nlater used in three different ways for creating of language resources: training\nword embedding models, continued pre-training of BERT-like language models, and\ntraining Machine Translation models that are used as a proxy to generate\nproduct-related keywords. Our evaluation on an extensive set of benchmarks\nshows word embeddings to be the most reliable and consistent method to improve\nthe accuracy on both tasks (with up to 6.9 percentage points in macro-average\nF1 on some datasets). The other two methods however, are not as useful. Our\nanalysis shows that this could be due to a number of reasons, including the\nbiased domain representation in the structured data and lack of vocabulary\ncoverage. We share our datasets and discuss how our lessons learned could be\ntaken forward to inform future research in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-based automatic personality prediction: A bibliographic review. (arXiv:2110.01186v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01186","description":"<p>Personality detection is an old topic in psychology and Automatic Personality\nPrediction (or Perception) (APP) is the automated (computationally) forecasting\nof the personality on different types of human generated/exchanged contents\n(such as text, speech, image, video). The principal objective of this study is\nto offer a shallow (overall) review of natural language processing approaches\non APP since 2010. With the advent of deep learning and following it\ntransfer-learning and pre-trained model in NLP, APP research area has been a\nhot topic, so in this review, methods are categorized into three; pre-trained\nindependent, pre-trained model based, multimodal approaches. Also, to achieve a\ncomprehensive comparison, reported results are informed by datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_A/0/1/0/all/0/1\">Ali-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1\">Majid Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikzad_Khasmakhi_N/0/1/0/all/0/1\">Narjes Nikzad-Khasmakhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgari_Chenaghlu_M/0/1/0/all/0/1\">Meysam Asgari-Chenaghlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akan_T/0/1/0/all/0/1\">Taymaz Akan</a> (Rahkar-Farshi), <a href=\"http://arxiv.org/find/cs/1/au:+Ranjbar_Khadivi_M/0/1/0/all/0/1\">Mehrdad Ranjbar-Khadivi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafarni_Moattar_E/0/1/0/all/0/1\">Elnaz Zafarni-Moattar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahanbakhsh_Naghadeh_Z/0/1/0/all/0/1\">Zoleikha Jahanbakhsh-Naghadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-horizon Robot Manipulation Tasks. (arXiv:2112.03227v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2112.03227","description":"<p>General-purpose robots coexisting with humans in their environment must learn\nto relate human language to their perceptions and actions to be useful in a\nrange of daily tasks. Moreover, they need to acquire a diverse repertoire of\ngeneral-purpose skills that allow composing long-horizon tasks by following\nunconstrained language instructions. In this paper, we present CALVIN\n(Composing Actions from Language and Vision), an open-source simulated\nbenchmark to learn long-horizon language-conditioned tasks. Our aim is to make\nit possible to develop agents that can solve many robotic manipulation tasks\nover a long horizon, from onboard sensors, and specified only via human\nlanguage. CALVIN tasks are more complex in terms of sequence length, action\nspace, and language than existing vision-and-language task datasets and\nsupports flexible specification of sensor suites. We evaluate the agents in\nzero-shot to novel language instructions and to novel environments and objects.\nWe show that a baseline model based on multi-context imitation learning\nperforms poorly on CALVIN, suggesting that there is significant room for\ndeveloping innovative agents that learn to relate human language to their world\nmodels with this benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mees_O/0/1/0/all/0/1\">Oier Mees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermann_L/0/1/0/all/0/1\">Lukas Hermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosete_Beas_E/0/1/0/all/0/1\">Erick Rosete-Beas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Refined Commonsense Knowledge from Large-Scale Web Contents. (arXiv:2112.04596v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.04596","description":"<p>Commonsense knowledge (CSK) about concepts and their properties is helpful\nfor AI applications. Prior works, such as ConceptNet, have compiled large CSK\ncollections. However, they are restricted in their expressiveness to\nsubject-predicate-object (SPO) triples with simple concepts for S and strings\nfor P and O. This paper presents a method called ASCENT++ to automatically\nbuild a large-scale knowledge base (KB) of CSK assertions, with refined\nexpressiveness and both better precision and recall than prior works. ASCENT++\ngoes beyond SPO triples by capturing composite concepts with subgroups and\naspects, and by refining assertions with semantic facets. The latter is\nessential to express the temporal and spatial validity of assertions and\nfurther qualifiers. Furthermore, ASCENT++ combines open information extraction\n(OpenIE) with judicious cleaning and ranking by typicality and saliency scores.\nFor high coverage, our method taps into the large-scale crawl C4 with broad web\ncontents. The evaluation with human judgments shows the superior quality of the\nASCENT++ KB, and an extrinsic evaluation for QA-support tasks underlines the\nbenefits of ASCENT++. A web interface, data, and code can be accessed at\nhttps://ascentpp.mpi-inf.mpg.de/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan-Phong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1\">Simon Razniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_J/0/1/0/all/0/1\">Julien Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing language context confusion for end-to-end code-switching automatic speech recognition. (arXiv:2201.12155v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.12155","description":"<p>Code-switching deals with alternative languages in communication process.\nTraining end-to-end (E2E) automatic speech recognition (ASR) systems for\ncode-switching is especially challenging as code-switching training data are\nalways insufficient to combat the increased multilingual context confusion due\nto the presence of more than one language. We propose a language-related\nattention mechanism to reduce multilingual context confusion for the E2E\ncode-switching ASR model based on the Equivalence Constraint (EC) Theory. The\nlinguistics theory requires that any monolingual fragment that occurs in the\ncode-switching sentence must occur in one of the monolingual sentences. The\ntheory establishes a bridge between monolingual data and code-switching data.\nWe leverage this linguistics theory to design the code-switching E2E ASR model.\nThe proposed model efficiently transfers language knowledge from rich\nmonolingual data to improve the performance of the code-switching ASR model. We\nevaluate our model on ASRU 2019 Mandarin-English code-switching challenge\ndataset. Compared to the baseline model, our proposed model achieves a 17.12%\nrelative error reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jiangyan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhengkun Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1\">Jianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_Y/0/1/0/all/0/1\">Yu Ting Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Liqun Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JARVix at SemEval-2022 Task 2: It Takes One to Know One? Idiomaticity Detection using Zero and One-Shot Learning. (arXiv:2202.02394v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02394","description":"<p>Large Language Models have been successful in a wide variety of Natural\nLanguage Processing tasks by capturing the compositionality of the text\nrepresentations. In spite of their great success, these vector representations\nfail to capture meaning of idiomatic multi-word expressions (MWEs). In this\npaper, we focus on the detection of idiomatic expressions by using binary\nclassification. We use a dataset consisting of the literal and idiomatic usage\nof MWEs in English and Portuguese. Thereafter, we perform the classification in\ntwo different settings: zero shot and one shot, to determine if a given\nsentence contains an idiom or not. N shot classification for this task is\ndefined by N number of common idioms between the training and testing sets. In\nthis paper, we train multiple Large Language Models in both the settings and\nachieve an F1 score (macro) of 0.73 for the zero shot setting and an F1 score\n(macro) of 0.85 for the one shot setting. An implementation of our work can be\nfound at\nhttps://github.com/ashwinpathak20/Idiomaticity_Detection_Using_Few_Shot_Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jakhotiya_Y/0/1/0/all/0/1\">Yash Jakhotiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vaibhav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_A/0/1/0/all/0/1\">Ashwin Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Raj Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Short Math Answer Grading via In-context Meta-learning. (arXiv:2205.15219v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.15219","description":"<p>Automatic short answer grading is an important research direction in the\nexploration of how to use artificial intelligence (AI)-based tools to improve\neducation. Current state-of-the-art approaches use neural language models to\ncreate vectorized representations of students responses, followed by\nclassifiers to predict the score. However, these approaches have several key\nlimitations, including i) they use pre-trained language models that are not\nwell-adapted to educational subject domains and/or student-generated text and\nii) they almost always train one model per question, ignoring the linkage\nacross a question and result in a significant model storage problem due to the\nsize of advanced language models. In this paper, we study the problem of\nautomatic short answer grading for students' responses to math questions and\npropose a novel framework for this task. First, we use MathBERT, a variant of\nthe popular language model BERT adapted to mathematical content, as our base\nmodel and fine-tune it for the downstream task of student response grading.\nSecond, we use an in-context learning approach that provides scoring examples\nas input to the language model to provide additional context information and\npromote generalization to previously unseen questions. We evaluate our\nframework on a real-world dataset of student responses to open-ended math\nquestions and show that our framework (often significantly) outperforms\nexisting approaches, especially for new questions that are not seen during\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengxue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_S/0/1/0/all/0/1\">Sami Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_N/0/1/0/all/0/1\">Neil Heffernan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_A/0/1/0/all/0/1\">Andrew Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval. (arXiv:2206.02873v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2206.02873","description":"<p>Recent work has shown that small distilled language models are strong\ncompetitors to models that are orders of magnitude larger and slower in a wide\nrange of information retrieval tasks. This has made distilled and dense models,\ndue to latency constraints, the go-to choice for deployment in real-world\nretrieval applications. In this work, we question this practice by showing that\nthe number of parameters and early query-document interaction play a\nsignificant role in the generalization ability of retrieval models. Our\nexperiments show that increasing model size results in marginal gains on\nin-domain test sets, but much larger gains in new domains never seen during\nfine-tuning. Furthermore, we show that rerankers largely outperform dense ones\nof similar size in several tasks. Our largest reranker reaches the state of the\nart in 12 of the 18 datasets of the Benchmark-IR (BEIR) and surpasses the\nprevious state of the art by 3 average points. Finally, we confirm that\nin-domain effectiveness is not a good indicator of zero-shot effectiveness.\nCode is available at\nhttps://github.com/guilhermemr04/scaling-zero-shot-retrieval.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1\">Guilherme Moraes Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonifacio_L/0/1/0/all/0/1\">Luiz Bonifacio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeronymo_V/0/1/0/all/0/1\">Vitor Jeronymo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abonizio_H/0/1/0/all/0/1\">Hugo Abonizio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fadaee_M/0/1/0/all/0/1\">Marzieh Fadaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GEMv2: Multilingual NLG Benchmarking in a Single Line of Code. (arXiv:2206.11249v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.11249","description":"<p>Evaluation in machine learning is usually informed by past choices, for\nexample which datasets or metrics to use. This standardization enables the\ncomparison on equal footing using leaderboards, but the evaluation choices\nbecome sub-optimal as better alternatives arise. This problem is especially\npertinent in natural language generation which requires ever-improving suites\nof datasets, metrics, and human evaluation to make definitive claims. To make\nfollowing best model evaluation practices easier, we introduce GEMv2. The new\nversion of the Generation, Evaluation, and Metrics Benchmark introduces a\nmodular infrastructure for dataset, model, and metric developers to benefit\nfrom each others work. GEMv2 supports 40 documented datasets in 51 languages.\nModels for all datasets can be evaluated online and our interactive data card\ncreation and rendering tools make it easier to add new datasets to the living\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Abhik Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahendiran_A/0/1/0/all/0/1\">Abinaya Mahendiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alex Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1\">Alexandros Papangelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMillan_Major_A/0/1/0/all/0/1\">Angelina McMillan-Major</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shvets_A/0/1/0/all/0/1\">Anna Shvets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_A/0/1/0/all/0/1\">Ashish Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bingsheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chaobin You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomson_C/0/1/0/all/0/1\">Craig Thomson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garbacea_C/0/1/0/all/0/1\">Cristina Garbacea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1\">Daniel Deutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkatzia_D/0/1/0/all/0/1\">Dimitra Gkatzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_E/0/1/0/all/0/1\">Elizabeth Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladhak_F/0/1/0/all/0/1\">Faisal Ladhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginter_F/0/1/0/all/0/1\">Filip Ginter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1\">Hendrik Strobelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1\">Hiroaki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novikova_J/0/1/0/all/0/1\">Jekaterina Novikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanerva_J/0/1/0/all/0/1\">Jenna Kanerva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chim_J/0/1/0/all/0/1\">Jenny Chim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiawei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clive_J/0/1/0/all/0/1\">Jordan Clive</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1\">Joshua Maynez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juraska_J/0/1/0/all/0/1\">Juraj Juraska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhole_K/0/1/0/all/0/1\">Kaustubh Dhole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1\">Khyathi Raghavi Chandu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Beltrachini_L/0/1/0/all/0/1\">Laura Perez-Beltrachini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leonardo F. R. Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tunstall_L/0/1/0/all/0/1\">Lewis Tunstall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pushkarna_M/0/1/0/all/0/1\">Mahima Pushkarna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creutz_M/0/1/0/all/0/1\">Mathias Creutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1\">Michael White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1\">Mihir Sanjay Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eddine_M/0/1/0/all/0/1\">Moussa Kamal Eddine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daheim_N/0/1/0/all/0/1\">Nico Daheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramani_N/0/1/0/all/0/1\">Nishant Subramani</a>, et al. (28 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Few-shot Long-Tailed Bird Audio Recognition. (arXiv:2206.11260v1 [cs.SD])","link":"http://arxiv.org/abs/2206.11260","description":"<p>It is easier to hear birds than see them. However, they still play an\nessential role in nature and are excellent indicators of deteriorating\nenvironmental quality and pollution. Recent advances in Machine Learning and\nConvolutional Neural Networks allow us to process continuous audio data to\ndetect and classify bird sounds. This technology can assist researchers in\nmonitoring bird populations' status and trends and ecosystems' biodiversity.\n</p>\n<p>We propose a sound detection and classification pipeline to analyze complex\nsoundscape recordings and identify birdcalls in the background. Our method\nlearns from weak labels and few data and acoustically recognizes the bird\nspecies. Our solution achieved 18th place of 807 teams at the BirdCLEF 2022\nChallenge hosted on Kaggle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1\">Marcos V. Conde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_U/0/1/0/all/0/1\">Ui-Jin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Doubly Reparameterized Importance Weighted Structure Learning for Scene Graph Generation. (arXiv:2206.11352v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11352","description":"<p>As a structured prediction task, scene graph generation, given an input\nimage, aims to explicitly model objects and their relationships by constructing\na visually-grounded scene graph. In the current literature, such task is\nuniversally solved via a message passing neural network based mean field\nvariational Bayesian methodology. The classical loose evidence lower bound is\ngenerally chosen as the variational inference objective, which could induce\noversimplified variational approximation and thus underestimate the underlying\ncomplex posterior. In this paper, we propose a novel doubly reparameterized\nimportance weighted structure learning method, which employs a tighter\nimportance weighted lower bound as the variational inference objective. It is\ncomputed from multiple samples drawn from a reparameterizable Gumbel-Softmax\nsampler and the resulting constrained variational inference task is solved by a\ngeneric entropic mirror descent algorithm. The resulting doubly reparameterized\ngradient estimator reduces the variance of the corresponding derivatives with a\nbeneficial impact on learning. The proposed method achieves the\nstate-of-the-art performance on various popular scene graph generation\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bober_M/0/1/0/all/0/1\">Miroslaw Bober</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1\">Josef Kittler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monocular Spherical Depth Estimation with Explicitly Connected Weak Layout Cues. (arXiv:2206.11358v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11358","description":"<p>Spherical cameras capture scenes in a holistic manner and have been used for\nroom layout estimation. Recently, with the availability of appropriate\ndatasets, there has also been progress in depth estimation from a single\nomnidirectional image. While these two tasks are complementary, few works have\nbeen able to explore them in parallel to advance indoor geometric perception,\nand those that have done so either relied on synthetic data, or used small\nscale datasets, as few options are available that include both layout\nannotations and dense depth maps in real scenes. This is partly due to the\nnecessity of manual annotations for room layouts. In this work, we move beyond\nthis limitation and generate a 360 geometric vision (360V) dataset that\nincludes multiple modalities, multi-view stereo data and automatically\ngenerated weak layout cues. We also explore an explicit coupling between the\ntwo tasks to integrate them into a singleshot trained model. We rely on\ndepth-based layout reconstruction and layout-based depth attention,\ndemonstrating increased performance across both tasks. By using single 360\ncameras to scan rooms, the opportunity for facile and quick building-scale 3D\nscanning arises.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zioulis_N/0/1/0/all/0/1\">Nikolaos Zioulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_F/0/1/0/all/0/1\">Federico Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarpalas_D/0/1/0/all/0/1\">Dimitrios Zarpalas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daras_P/0/1/0/all/0/1\">Petros Daras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Online Skeleton Extraction and Gesture Recognition on Pepper. (arXiv:2206.11376v1 [cs.RO])","link":"http://arxiv.org/abs/2206.11376","description":"<p>We present a multi-stage pipeline for simple gesture recognition. The novelty\nof our approach is the association of different technologies, resulting in the\nfirst real-time system as of now to conjointly extract skeletons and recognise\ngesture on a Pepper robot. For this task, Pepper has been augmented with an\nembedded GPU for running deep CNNs and a fish-eye camera to capture whole scene\ninteraction. We show in this article that real-case scenarios are challenging,\nand the state-of-the-art approaches hardly deal with unknown human gestures. We\npresent here a way to handle such cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lefrant_A/0/1/0/all/0/1\">Axel Lefrant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montanier_J/0/1/0/all/0/1\">Jean-Marc Montanier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The ArtBench Dataset: Benchmarking Generative Models with Artworks. (arXiv:2206.11404v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11404","description":"<p>We introduce ArtBench-10, the first class-balanced, high-quality, cleanly\nannotated, and standardized dataset for benchmarking artwork generation. It\ncomprises 60,000 images of artwork from 10 distinctive artistic styles, with\n5,000 training images and 1,000 testing images per style. ArtBench-10 has\nseveral advantages over previous artwork datasets. Firstly, it is\nclass-balanced while most previous artwork datasets suffer from the long tail\nclass distributions. Secondly, the images are of high quality with clean\nannotations. Thirdly, ArtBench-10 is created with standardized data collection,\nannotation, filtering, and preprocessing procedures. We provide three versions\nof the dataset with different resolutions ($32\\times32$, $256\\times256$, and\noriginal image size), formatted in a way that is easy to be incorporated by\npopular machine learning frameworks. We also conduct extensive benchmarking\nexperiments using representative image synthesis models with ArtBench-10 and\npresent in-depth analysis. The dataset is available at\nhttps://github.com/liaopeiyuan/artbench under a Fair Use license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_P/0/1/0/all/0/1\">Peiyuan Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiuyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xihui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LidarMutliNet: Unifying LiDAR Semantic Segmentation, 3D Object Detection, and Panoptic Segmentation in a Single Multi-task Network. (arXiv:2206.11428v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11428","description":"<p>This technical report presents the 1st place winning solution for the Waymo\nOpen Dataset 3D semantic segmentation challenge 2022. Our network, termed\nLidarMultiNet, unifies the major LiDAR perception tasks such as 3D semantic\nsegmentation, object detection, and panoptic segmentation in a single\nframework. At the core of LidarMultiNet is a strong 3D voxel-based\nencoder-decoder network with a novel Global Context Pooling (GCP) module\nextracting global contextual features from a LiDAR frame to complement its\nlocal features. An optional second stage is proposed to refine the first-stage\nsegmentation or generate accurate panoptic segmentation results. Our solution\nachieves a mIoU of 71.13 and is the best for most of the 22 classes on the\nWaymo 3D semantic segmentation test set, outperforming all the other 3D\nsemantic segmentation methods on the official leaderboard. We demonstrate for\nthe first time that major LiDAR perception tasks can be unified in a single\nstrong network that can be trained end-to-end.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1\">Dongqiangzi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weijia Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zixiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yufei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Panqu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foroosh_H/0/1/0/all/0/1\">Hassan Foroosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-based Stability Quantification. (arXiv:2206.11443v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11443","description":"<p>Quantitative evaluation of human stability using foot pressure/force\nmeasurement hardware and motion capture (mocap) technology is expensive, time\nconsuming, and restricted to the laboratory (lab-based). We propose a novel\nimage-based method to estimate three key components for stability computation:\nCenter of Mass (CoM), Base of Support (BoS), and Center of Pressure (CoP).\nFurthermore, we quantitatively validate our image-based methods for computing\ntwo classic stability measures against the ones generated directly from\nlab-based sensory output (ground truth) using a publicly available\nmulti-modality (mocap, foot pressure, 2-view videos), ten-subject human motion\ndataset. Using leave-one-subject-out cross validation, our experimental results\nshow: 1) our CoM estimation method (CoMNet) consistently outperforms\nstate-of-the-art inertial sensor-based CoM estimation techniques; 2) our\nimage-based method combined with insole foot-pressure alone produces consistent\nand statistically significant correlation with ground truth stability measures\n(CoMtoCoP R=0.79 P&lt;0.001, CoMtoBoS R=0.75 P&lt;0.001); 3) our fully image-based\nstability metric estimation produces consistent, positive, and statistically\nsignificant correlation on the two stability metrics (CoMtoCoP R=0.31 P&lt;0.001,\nCoMtoBoS R=0.22 P&lt;0.001). Our study provides promising quantitative evidence\nfor stability computations and monitoring in natural environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scott_J/0/1/0/all/0/1\">Jesse Scott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Challis_J/0/1/0/all/0/1\">John Challis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_R/0/1/0/all/0/1\">Robert T. Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanxi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weighted Concordance Index Loss-based Multimodal Survival Modeling for Radiation Encephalopathy Assessment in Nasopharyngeal Carcinoma Radiotherapy. (arXiv:2206.11458v1 [eess.IV])","link":"http://arxiv.org/abs/2206.11458","description":"<p>Radiation encephalopathy (REP) is the most common complication for\nnasopharyngeal carcinoma (NPC) radiotherapy. It is highly desirable to assist\nclinicians in optimizing the NPC radiotherapy regimen to reduce\nradiotherapy-induced temporal lobe injury (RTLI) according to the probability\nof REP onset. To the best of our knowledge, it is the first exploration of\npredicting radiotherapy-induced REP by jointly exploiting image and non-image\ndata in NPC radiotherapy regimen. We cast REP prediction as a survival analysis\ntask and evaluate the predictive accuracy in terms of the concordance index\n(CI). We design a deep multimodal survival network (MSN) with two feature\nextractors to learn discriminative features from multimodal data. One feature\nextractor imposes feature selection on non-image data, and the other learns\nvisual features from images. Because the priorly balanced CI (BCI) loss\nfunction directly maximizing the CI is sensitive to uneven sampling per batch.\nHence, we propose a novel weighted CI (WCI) loss function to leverage all REP\nsamples effectively by assigning their different weights with a dual average\noperation. We further introduce a temperature hyper-parameter for our WCI to\nsharpen the risk difference of sample pairs to help model convergence. We\nextensively evaluate our WCI on a private dataset to demonstrate its\nfavourability against its counterparts. The experimental results also show\nmultimodal data of NPC radiotherapy can bring more gains for REP risk\nprediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fang_J/0/1/0/all/0/1\">Jiansheng Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_A/0/1/0/all/0/1\">Anwei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+OuYang_P/0/1/0/all/0/1\">Pu-Yun OuYang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiajian Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jingwen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Hongbo Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_F/0/1/0/all/0/1\">Fang-Yun Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explore Spatio-temporal Aggregation for Insubstantial Object Detection: Benchmark Dataset and Baseline. (arXiv:2206.11459v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11459","description":"<p>We endeavor on a rarely explored task named Insubstantial Object Detection\n(IOD), which aims to localize the object with following characteristics: (1)\namorphous shape with indistinct boundary; (2) similarity to surroundings; (3)\nabsence in color. Accordingly, it is far more challenging to distinguish\ninsubstantial objects in a single static frame and the collaborative\nrepresentation of spatial and temporal information is crucial. Thus, we\nconstruct an IOD-Video dataset comprised of 600 videos (141,017 frames)\ncovering various distances, sizes, visibility, and scenes captured by different\nspectral ranges. In addition, we develop a spatio-temporal aggregation\nframework for IOD, in which different backbones are deployed and a\nspatio-temporal aggregation loss (STAloss) is elaborately designed to leverage\nthe consistency along the time axis. Experiments conducted on IOD-Video dataset\ndemonstrate that spatio-temporal aggregation can significantly improve the\nperformance of IOD. We hope our work will attract further researches into this\nvaluable yet challenging task. The code will be available at:\n\\url{https://github.com/CalayZhou/IOD-Video}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kailai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Linsen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Q/0/1/0/all/0/1\">Qiu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xun Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Better User Studies in Computer Graphics and Vision. (arXiv:2206.11461v1 [cs.GR])","link":"http://arxiv.org/abs/2206.11461","description":"<p>Online crowdsourcing platforms make it easy to perform evaluations of\nalgorithm outputs with surveys that ask questions like \"which image is better,\nA or B?\") The proliferation of these \"user studies\" in vision and graphics\nresearch papers has led to an increase of hastily conducted studies that are\nsloppy and uninformative at best, and potentially harmful and misleading. We\nargue that more attention needs to be paid to both the design and reporting of\nuser studies in computer vision and graphics papers. In an attempt to improve\npractitioners' knowledge and increase the trustworthiness and replicability of\nuser studies, we provide an overview of methodologies from user experience\nresearch (UXR), human-computer interaction (HCI), and related fields. We\ndiscuss foundational user research methods (e.g., needfinding) that are\npresently underutilized in computer vision and graphics research, but can\nprovide valuable guidance for research projects. We provide further pointers to\nthe literature for readers interested in exploring other UXR methodologies.\nFinally, we describe broader open issues and recommendations for the research\ncommunity. We encourage authors and reviewers alike to recognize that not every\nresearch contribution requires a user study, and that having no study at all is\nbetter than having a carelessly conducted one.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bylinskii_Z/0/1/0/all/0/1\">Zoya Bylinskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herman_L/0/1/0/all/0/1\">Laura Herman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertzmann_A/0/1/0/all/0/1\">Aaron Hertzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutka_S/0/1/0/all/0/1\">Stefanie Hutka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yile Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ICME 2022 Few-shot LOGO detection top 9 solution. (arXiv:2206.11462v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11462","description":"<p>ICME-2022 few-shot logo detection competition is held in May, 2022.\nParticipants are required to develop a single model to detect logos by handling\ntiny logo instances, similar brands, and adversarial images at the same time,\nwith limited annotations. Our team achieved rank 16 and 11 in the first and\nsecond round of the competition respectively, with a final rank of 9th. This\ntechnical report summarized our major techniques used in this competitions, and\npotential improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tong_K/0/1/0/all/0/1\">Ka Ho Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_K/0/1/0/all/0/1\">Ka Wai Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaochuan Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complementary datasets to COCO for object detection. (arXiv:2206.11473v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11473","description":"<p>For nearly a decade, the COCO dataset has been the central test bed of\nresearch in object detection. According to the recent benchmarks, however, it\nseems that performance on this dataset has started to saturate. One possible\nreason can be that perhaps it is not large enough for training deep models. To\naddress this limitation, here we introduce two complementary datasets to COCO:\ni) COCO_OI, composed of images from COCO and OpenImages (from their 80 classes\nin common) with 1,418,978 training bounding boxes over 380,111 images, and\n41,893 validation bounding boxes over 18,299 images, and ii) ObjectNet_D\ncontaining objects in daily life situations (originally created for object\nrecognition known as ObjectNet; 29 categories in common with COCO). The latter\ncan be used to test the generalization ability of object detectors. We evaluate\nsome models on these datasets and pinpoint the source of errors. We encourage\nthe community to utilize these datasets for training and testing object\ndetection models. Code and data is available at\nhttps://github.com/aliborji/COCO_OI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entropy-driven Sampling and Training Scheme for Conditional Diffusion Generation. (arXiv:2206.11474v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11474","description":"<p>Denoising Diffusion Probabilistic Model (DDPM) is able to make flexible\nconditional image generation from prior noise to real data, by introducing an\nindependent noise-aware classifier to provide conditional gradient guidance at\neach time step of denoising process. However, due to the ability of classifier\nto easily discriminate an incompletely generated image only with high-level\nstructure, the gradient, which is a kind of class information guidance, tends\nto vanish early, leading to the collapse from conditional generation process\ninto the unconditional process. To address this problem, we propose two simple\nbut effective approaches from two perspectives. For sampling procedure, we\nintroduce the entropy of predicted distribution as the measure of guidance\nvanishing level and propose an entropy-aware scaling method to adaptively\nrecover the conditional semantic guidance. % for each generated sample. For\ntraining stage, we propose the entropy-aware optimization objectives to\nalleviate the overconfident prediction for noisy data.On ImageNet1000 256x256,\nwith our proposed sampling scheme and trained classifier, the pretrained\nconditional and unconditional DDPM model can achieve 10.89% (4.59 to 4.09) and\n43.5% (12 to 6.78) FID improvement respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guangcong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Taiping Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shoudong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Scene Deblurring Base on Continuous Cross-Layer Attention Transmission. (arXiv:2206.11476v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11476","description":"<p>The deep convolutional neural networks (CNNs) using attention mechanism have\nachieved great success for dynamic scene deblurring. In most of these networks,\nonly the features refined by the attention maps can be passed to the next layer\nand the attention maps of different layers are separated from each other, which\ndoes not make full use of the attention information from different layers in\nthe CNN. To address this problem, we introduce a new continuous cross-layer\nattention transmission (CCLAT) mechanism that can exploit hierarchical\nattention information from all the convolutional layers. Based on the CCLAT\nmechanism, we use a very simple attention module to construct a novel residual\ndense attention fusion block (RDAFB). In RDAFB, the attention maps inferred\nfrom the outputs of the preceding RDAFB and each layer are directly connected\nto the subsequent ones, leading to a CRLAT mechanism. Taking RDAFB as the\nbuilding block, we design an effective architecture for dynamic scene\ndeblurring named RDAFNet. The experiments on benchmark datasets show that the\nproposed model outperforms the state-of-the-art deblurring approaches, and\ndemonstrate the effectiveness of CCLAT mechanism. The source code is available\non: https://github.com/xjmz6/RDAFNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xia Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_J/0/1/0/all/0/1\">Junxiong Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">ZeZheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">JiangGuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1\">Hanyu Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Algorithm for Exact Concave Hull Extraction. (arXiv:2206.11481v1 [cs.CG])","link":"http://arxiv.org/abs/2206.11481","description":"<p>Region extraction is necessary in a wide range of applications, from object\ndetection in autonomous driving to analysis of subcellular morphology in cell\nbiology. There exist two main approaches: convex hull extraction, for which\nexact and efficient algorithms exist and concave hulls, which are better at\ncapturing real-world shapes but do not have a single solution. Especially in\nthe context of a uniform grid, concave hull algorithms are largely approximate,\nsacrificing region integrity for spatial and temporal efficiency. In this\nstudy, we present a novel algorithm that can provide vertex-minimized concave\nhulls with maximal (i.e. pixel-perfect) resolution and is tunable for\nspeed-efficiency tradeoffs. Our method provides advantages in multiple\ndownstream applications including data compression, retrieval, visualization,\nand analysis. To demonstrate the practical utility of our approach, we focus on\nimage compression. We demonstrate significant improvements through\ncontext-dependent compression on disparate regions within a single image\n(entropy encoding for noisy and predictive encoding for the structured\nregions). We show that these improvements range from biomedical images to\nnatural images. Beyond image compression, our algorithm can be applied more\nbroadly to aid in a wide range of practical applications for data retrieval,\nvisualization, and analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+VanHorn_K/0/1/0/all/0/1\">Kevin Christopher VanHorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobanoglu_M/0/1/0/all/0/1\">Murat Can &#xc7;obano&#x11f;lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Pre-Training for Federated Learning. (arXiv:2206.11488v1 [cs.LG])","link":"http://arxiv.org/abs/2206.11488","description":"<p>In most of the literature on federated learning (FL), neural networks are\ninitialized with random weights. In this paper, we present an empirical study\non the effect of pre-training on FL. Specifically, we aim to investigate if\npre-training can alleviate the drastic accuracy drop when clients'\ndecentralized data are non-IID. We focus on FedAvg, the fundamental and most\nwidely used FL algorithm. We found that pre-training does largely close the gap\nbetween FedAvg and centralized learning under non-IID data, but this does not\ncome from alleviating the well-known model drifting problem in FedAvg's local\ntraining. Instead, how pre-training helps FedAvg is by making FedAvg's global\naggregation more stable. When pre-training using real data is not feasible for\nFL, we propose a novel approach to pre-train with synthetic data. On various\nimage datasets (including one for segmentation), our approach with synthetic\npre-training leads to a notable gain, essentially a critical step toward\nscaling up federated learning for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong-You Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_C/0/1/0/all/0/1\">Cheng-Hao Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Han-Wei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Refactor Action and Co-occurrence Features for Temporal Action Localization. (arXiv:2206.11493v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11493","description":"<p>The main challenge of Temporal Action Localization is to retrieve subtle\nhuman actions from various co-occurring ingredients, e.g., context and\nbackground, in an untrimmed video. While prior approaches have achieved\nsubstantial progress through devising advanced action detectors, they still\nsuffer from these co-occurring ingredients which often dominate the actual\naction content in videos. In this paper, we explore two orthogonal but\ncomplementary aspects of a video snippet, i.e., the action features and the\nco-occurrence features. Especially, we develop a novel auxiliary task by\ndecoupling these two types of features within a video snippet and recombining\nthem to generate a new feature representation with more salient action\ninformation for accurate action localization. We term our method RefactorNet,\nwhich first explicitly factorizes the action content and regularizes its\nco-occurrence features, and then synthesizes a new action-dominated video\nrepresentation. Extensive experimental results and ablation studies on THUMOS14\nand ActivityNet v1.3 demonstrate that our new representation, combined with a\nsimple action detector, can significantly improve the action localization\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_K/0/1/0/all/0/1\">Kun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sanping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wei Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Structure from Motion for UAV Images via Weighted Connected Dominating Set. (arXiv:2206.11499v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11499","description":"<p>Incremental Structure from Motion (ISfM) has been widely used for UAV image\norientation. Its efficiency, however, decreases dramatically due to the\nsequential constraint. Although the divide-and-conquer strategy has been\nutilized for efficiency improvement, cluster merging becomes difficult or\ndepends on seriously designed overlap structures. This paper proposes an\nalgorithm to extract the global model for cluster merging and designs a\nparallel SfM solution to achieve efficient and accurate UAV image orientation.\nFirst, based on vocabulary tree retrieval, match pairs are selected to\nconstruct an undirected weighted match graph, whose edge weights are calculated\nby considering both the number and distribution of feature matches. Second, an\nalgorithm, termed weighted connected dominating set (WCDS), is designed to\nachieve the simplification of the match graph and build the global model, which\nincorporates the edge weight in the graph node selection and enables the\nsuccessful reconstruction of the global model. Third, the match graph is\nsimultaneously divided into compact and non-overlapped clusters. After the\nparallel reconstruction, cluster merging is conducted with the aid of common 3D\npoints between the global and cluster models. Finally, by using three UAV\ndatasets that are captured by classical oblique and recent optimized views\nphotogrammetry, the validation of the proposed solution is verified through\ncomprehensive analysis and comparison. The experimental results demonstrate\nthat the proposed parallel SfM can achieve 17.4 times efficiency improvement\nand comparative orientation accuracy. In absolute BA, the geo-referencing\naccuracy is approximately 2.0 and 3.0 times the GSD (Ground Sampling Distance)\nvalue in the horizontal and vertical directions, respectively. For parallel\nSfM, the proposed solution is a more reliable alternative.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">San Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wanshou Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A novel adversarial learning strategy for medical image classification. (arXiv:2206.11501v1 [eess.IV])","link":"http://arxiv.org/abs/2206.11501","description":"<p>Deep learning (DL) techniques have been extensively utilized for medical\nimage classification. Most DL-based classification networks are generally\nstructured hierarchically and optimized through the minimization of a single\nloss function measured at the end of the networks. However, such a single loss\ndesign could potentially lead to optimization of one specific value of interest\nbut fail to leverage informative features from intermediate layers that might\nbenefit classification performance and reduce the risk of overfitting.\nRecently, auxiliary convolutional neural networks (AuxCNNs) have been employed\non top of traditional classification networks to facilitate the training of\nintermediate layers to improve classification performance and robustness. In\nthis study, we proposed an adversarial learning-based AuxCNN to support the\ntraining of deep neural networks for medical image classification. Two main\ninnovations were adopted in our AuxCNN classification framework. First, the\nproposed AuxCNN architecture includes an image generator and an image\ndiscriminator for extracting more informative image features for medical image\nclassification, motivated by the concept of generative adversarial network\n(GAN) and its impressive ability in approximating target data distribution.\nSecond, a hybrid loss function is designed to guide the model training by\nincorporating different objectives of the classification network and AuxCNN to\nreduce overfitting. Comprehensive experimental studies demonstrated the\nsuperior classification performance of the proposed model. The effect of the\nnetwork-related factors on classification performance was investigated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fan_Z/0/1/0/all/0/1\">Zong Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaohui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gasienica_J/0/1/0/all/0/1\">Jacob A. Gasienica</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Potts_J/0/1/0/all/0/1\">Jennifer Potts</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ruan_S/0/1/0/all/0/1\">Su Ruan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thorstad_W/0/1/0/all/0/1\">Wade Thorstad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gay_H/0/1/0/all/0/1\">Hiram Gay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaowei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hua Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review of Published Machine Learning Natural Language Processing Applications for Protocolling Radiology Imaging. (arXiv:2206.11502v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11502","description":"<p>Machine learning (ML) is a subfield of Artificial intelligence (AI), and its\napplications in radiology are growing at an ever-accelerating rate. The most\nstudied ML application is the automated interpretation of images. However,\nnatural language processing (NLP), which can be combined with ML for text\ninterpretation tasks, also has many potential applications in radiology. One\nsuch application is automation of radiology protocolling, which involves\ninterpreting a clinical radiology referral and selecting the appropriate\nimaging technique. It is an essential task which ensures that the correct\nimaging is performed. However, the time that a radiologist must dedicate to\nprotocolling could otherwise be spent reporting, communicating with referrers,\nor teaching. To date, there have been few publications in which ML models were\ndeveloped that use clinical text to automate protocol selection. This article\nreviews the existing literature in this field. A systematic assessment of the\npublished models is performed with reference to best practices suggested by\nmachine learning convention. Progress towards implementing automated\nprotocolling in a clinical setting is discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raju_N/0/1/0/all/0/1\">Nihal Raju</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Woodburn_M/0/1/0/all/0/1\">Michael Woodburn</a> (1 and 5), <a href=\"http://arxiv.org/find/cs/1/au:+Kachel_S/0/1/0/all/0/1\">Stefan Kachel</a> (2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+OShaughnessy_J/0/1/0/all/0/1\">Jack O&#x27;Shaughnessy</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Sorace_L/0/1/0/all/0/1\">Laurence Sorace</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Natalie Yang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Lim_R/0/1/0/all/0/1\">Ruth P Lim</a> (2 and 4) ((1) Harvard University, Extension School, Cambridge, MA, USA, (2) Department of Radiology, The University of Melbourne, Parkville, (3) Department of Radiology, Columbia University in the City of New York, (4) Department of Surgery, Austin, The University of Melbourne, (5) Austin Hospital, Austin Health, Melbourne, Australia)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ICOS Protein Expression Segmentation: Can Transformer Networks Give Better Results?. (arXiv:2206.11520v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11520","description":"<p>Biomarkers identify a patients response to treatment. With the recent\nadvances in artificial intelligence based on the Transformer networks, there is\nonly limited research has been done to measure the performance on challenging\nhistopathology images. In this paper, we investigate the efficacy of the\nnumerous state-of-the-art Transformer networks for immune-checkpoint biomarker,\nInducible Tcell COStimulator (ICOS) protein cell segmentation in colon cancer\nfrom immunohistochemistry (IHC) slides. Extensive and comprehensive\nexperimental results confirm that MiSSFormer achieved the highest Dice score of\n74.85% than the rest evaluated Transformer and Efficient U-Net methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1\">Vivek Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reilly_P/0/1/0/all/0/1\">Paul O Reilly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_J/0/1/0/all/0/1\">Jacqueline James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tellez_M/0/1/0/all/0/1\">Manuel Salto Tellez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maxwell_P/0/1/0/all/0/1\">Perry Maxwell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Neuromorphic Vision-Based Measurement for Robust Relative Localization in Future Space Exploration Missions. (arXiv:2206.11541v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11541","description":"<p>Space exploration has witnessed revolutionary changes upon landing of the\nPerseverance Rover on the Martian surface and demonstrating the first flight\nbeyond Earth by the Mars helicopter, Ingenuity. During their mission on Mars,\nPerseverance Rover and Ingenuity collaboratively explore the Martian surface,\nwhere Ingenuity scouts terrain information for rover's safe traversability.\nHence, determining the relative poses between both the platforms is of\nparamount importance for the success of this mission. Driven by this necessity,\nthis work proposes a robust relative localization system based on a fusion of\nneuromorphic vision-based measurements (NVBMs) and inertial measurements. The\nemergence of neuromorphic vision triggered a paradigm shift in the computer\nvision community, due to its unique working principle delineated with\nasynchronous events triggered by variations of light intensities occurring in\nthe scene. This implies that observations cannot be acquired in static scenes\ndue to illumination invariance. To circumvent this limitation, high frequency\nactive landmarks are inserted in the scene to guarantee consistent event\nfiring. These landmarks are adopted as salient features to facilitate relative\nlocalization. A novel event-based landmark identification algorithm using\nGaussian Mixture Models (GMM) is developed for matching the landmarks\ncorrespondences formulating our NVBMs. The NVBMs are fused with inertial\nmeasurements in proposed state estimators, landmark tracking Kalman filter\n(LTKF) and translation decoupled Kalman filter (TDKF) for landmark tracking and\nrelative localization, respectively. The proposed system was tested in a\nvariety of experiments and has outperformed state-of-the-art approaches in\naccuracy and range.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salah_M/0/1/0/all/0/1\">Mohammed Salah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chehadah_M/0/1/0/all/0/1\">Mohammed Chehadah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Humais_M/0/1/0/all/0/1\">Muhammed Humais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahbah_M/0/1/0/all/0/1\">Mohammed Wahbah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayyad_A/0/1/0/all/0/1\">Abdulla Ayyad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azzam_R/0/1/0/all/0/1\">Rana Azzam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senevirante_L/0/1/0/all/0/1\">Lakmal Senevirante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zweiri_Y/0/1/0/all/0/1\">Yahya Zweiri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Towards the Largest Margins. (arXiv:2206.11589v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11589","description":"<p>One of the main challenges for feature representation in deep learning-based\nclassification is the design of appropriate loss functions that exhibit strong\ndiscriminative power. The classical softmax loss does not explicitly encourage\ndiscriminative learning of features. A popular direction of research is to\nincorporate margins in well-established losses in order to enforce extra\nintra-class compactness and inter-class separability, which, however, were\ndeveloped through heuristic means, as opposed to rigorous mathematical\nprinciples. In this work, we attempt to address this limitation by formulating\nthe principled optimization objective as learning towards the largest margins.\nSpecifically, we firstly define the class margin as the measure of inter-class\nseparability, and the sample margin as the measure of intra-class compactness.\nAccordingly, to encourage discriminative representation of features, the loss\nfunction should promote the largest possible margins for both classes and\nsamples. Furthermore, we derive a generalized margin softmax loss to draw\ngeneral conclusions for the existing margin-based losses. Not only does this\nprincipled framework offer new perspectives to understand and interpret\nexisting margin-based losses, but it also provides new insights that can guide\nthe design of new tools, including sample margin regularization and largest\nmargin softmax loss for the class-balanced case, and zero-centroid\nregularization for the class-imbalanced case. Experimental results demonstrate\nthe effectiveness of our strategy on a variety of tasks, including visual\nclassification, imbalanced classification, person re-identification, and face\nverification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_D/0/1/0/all/0/1\">Deming Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Learned Image Compression With Low Computational Cost. (arXiv:2206.11599v1 [eess.IV])","link":"http://arxiv.org/abs/2206.11599","description":"<p>Recently, learned image compression methods have developed rapidly and\nexhibited excellent rate-distortion performance when compared to traditional\nstandards, such as JPEG, JPEG2000 and BPG. However, the learning-based methods\nsuffer from high computational costs, which is not beneficial for deployment on\ndevices with limited resources. To this end, we propose shift-addition parallel\nmodules (SAPMs), including SAPM-E for the encoder and SAPM-D for the decoder,\nto largely reduce the energy consumption. To be specific, they can be taken as\nplug-and-play components to upgrade existing CNN-based architectures, where the\nshift branch is used to extract large-grained features as compared to\nsmall-grained features learned by the addition branch. Furthermore, we\nthoroughly analyze the probability distribution of latent representations and\npropose to use Laplace Mixture Likelihoods for more accurate entropy\nestimation. Experimental results demonstrate that the proposed methods can\nachieve comparable or even better performance on both PSNR and MS-SSIM metrics\nto that of the convolutional counterpart with an about 2x energy reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xin_Y/0/1/0/all/0/1\">Yao Xin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bao_Y/0/1/0/all/0/1\">Youneng Bao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_F/0/1/0/all/0/1\">Fanyang Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_Y/0/1/0/all/0/1\">Yongsheng Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_W/0/1/0/all/0/1\">Wen Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prototype-Anchored Learning for Learning with Imperfect Annotations. (arXiv:2206.11602v1 [cs.LG])","link":"http://arxiv.org/abs/2206.11602","description":"<p>The success of deep neural networks greatly relies on the availability of\nlarge amounts of high-quality annotated data, which however are difficult or\nexpensive to obtain. The resulting labels may be class imbalanced, noisy or\nhuman biased. It is challenging to learn unbiased classification models from\nimperfectly annotated datasets, on which we usually suffer from overfitting or\nunderfitting. In this work, we thoroughly investigate the popular softmax loss\nand margin-based loss, and offer a feasible approach to tighten the\ngeneralization error bound by maximizing the minimal sample margin. We further\nderive the optimality condition for this purpose, which indicates how the class\nprototypes should be anchored. Motivated by theoretical analysis, we propose a\nsimple yet effective method, namely prototype-anchored learning (PAL), which\ncan be easily incorporated into various learning-based classification schemes\nto handle imperfect annotation. We verify the effectiveness of PAL on\nclass-imbalanced learning and noise-tolerant learning by extensive experiments\non synthetic and real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_D/0/1/0/all/0/1\">Deming Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"1st Place Solutions for RxR-Habitat Vision-and-Language Navigation Competition (CVPR 2022). (arXiv:2206.11610v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11610","description":"<p>This report presents the methods of the winning entry of the RxR-Habitat\nCompetition in CVPR 2022. The competition addresses the problem of\nVision-and-Language Navigation in Continuous Environments (VLN-CE), which\nrequires an agent to follow step-by-step natural language instructions to reach\na target. We present a modular plan-and-control approach for the task. Our\nmodel consists of three modules: the candidate waypoints predictor (CWP), the\nhistory enhanced planner and the tryout controller. In each decision loop, CWP\nfirst predicts a set of candidate waypoints based on depth observations from\nmultiple views. It can reduce the complexity of the action space and facilitate\nplanning. Then, a history-enhanced planner is adopted to select one of the\ncandidate waypoints as the subgoal. The planner additionally encodes historical\nmemory to track the navigation progress, which is especially effective for\nlong-horizon navigation. Finally, we propose a non-parametric heuristic\ncontroller named tryout to execute low-level actions to reach the planned\nsubgoal. It is based on the trial-and-error mechanism which can help the agent\nto avoid obstacles and escape from getting stuck. All three modules work\nhierarchically until the agent stops. We further take several recent advances\nof Vision-and-Language Navigation (VLN) to improve the performance such as\npretraining based on large-scale synthetic in-domain dataset, environment-level\ndata augmentation and snapshot model ensemble. Our model won the RxR-Habitat\nCompetition 2022, with 48% and 90% relative improvements over existing methods\non NDTW and SR metrics respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1\">Dong An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yicong Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Waypoint Generation in Row-based Crops with Deep Learning and Contrastive Clustering. (arXiv:2206.11623v1 [cs.RO])","link":"http://arxiv.org/abs/2206.11623","description":"<p>The development of precision agriculture has gradually introduced automation\nin the agricultural process to support and rationalize all the activities\nrelated to field management. In particular, service robotics plays a\npredominant role in this evolution by deploying autonomous agents able to\nnavigate in fields while executing different tasks without the need for human\nintervention, such as monitoring, spraying and harvesting. In this context,\nglobal path planning is the first necessary step for every robotic mission and\nensures that the navigation is performed efficiently and with complete field\ncoverage. In this paper, we propose a learning-based approach to tackle\nwaypoint generation for planning a navigation path for row-based crops,\nstarting from a top-view map of the region-of-interest. We present a novel\nmethodology for waypoint clustering based on a contrastive loss, able to\nproject the points to a separable latent space. The proposed deep neural\nnetwork can simultaneously predict the waypoint position and cluster assignment\nwith two specialized heads in a single forward pass. The extensive\nexperimentation on simulated and real-world images demonstrates that the\nproposed approach effectively solves the waypoint generation problem for both\nstraight and curved row-based crops, overcoming the limitations of previous\nstate-of-the-art methodologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salvetti_F/0/1/0/all/0/1\">Francesco Salvetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angarano_S/0/1/0/all/0/1\">Simone Angarano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martini_M/0/1/0/all/0/1\">Mauro Martini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerrato_S/0/1/0/all/0/1\">Simone Cerrato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiaberge_M/0/1/0/all/0/1\">Marcello Chiaberge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Sensing and Measurements Reuse for Image Compressed Sensing. (arXiv:2206.11629v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11629","description":"<p>Recently, deep network-based image compressed sensing methods achieved high\nreconstruction quality and reduced computational overhead compared with\ntraditional methods. However, existing methods obtain measurements only from\npartial features in the network and use them only once for image\nreconstruction. They ignore there are low, mid, and high-level features in the\nnetwork\\cite{zeiler2014visualizing} and all of them are essential for\nhigh-quality reconstruction. Moreover, using measurements only once may not be\nenough for extracting richer information from measurements. To address these\nissues, we propose a novel Measurements Reuse Convolutional Compressed Sensing\nNetwork (MR-CCSNet) which employs Global Sensing Module (GSM) to collect all\nlevel features for achieving an efficient sensing and Measurements Reuse Block\n(MRB) to reuse measurements multiple times on multi-scale. Finally,\nexperimental results on three benchmark datasets show that our model can\nsignificantly outperform state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zi-En Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_F/0/1/0/all/0/1\">Feng Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_J/0/1/0/all/0/1\">Jia-Ni Quan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning To Generate Scene Graph from Head to Tail. (arXiv:2206.11653v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11653","description":"<p>Scene Graph Generation (SGG) represents objects and their interactions with a\ngraph structure. Recently, many works are devoted to solving the imbalanced\nproblem in SGG. However, underestimating the head predicates in the whole\ntraining process, they wreck the features of head predicates that provide\ngeneral features for tail ones. Besides, assigning excessive attention to the\ntail predicates leads to semantic deviation. Based on this, we propose a novel\nSGG framework, learning to generate scene graphs from Head to Tail (SGG-HT),\ncontaining Curriculum Re-weight Mechanism (CRM) and Semantic Context Module\n(SCM). CRM learns head/easy samples firstly for robust features of head\npredicates and then gradually focuses on tail/hard ones. SCM is proposed to\nrelieve semantic deviation by ensuring the semantic consistency between the\ngenerated scene graph and the ground truth in global and local representations.\nExperiments show that SGG-HT significantly alleviates the biased problem and\nchieves state-of-the-art performances on Visual Genome.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chaofan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1\">Xinyu Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_P/0/1/0/all/0/1\">Pengpeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Warped Convolution Networks for Homography Estimation. (arXiv:2206.11657v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11657","description":"<p>Homography transformation has an essential relationship with special linear\ngroup and the embedding Lie algebra structure. Although the Lie algebra\nrepresentation is elegant, few researchers have established the connection\nbetween homography estimation and algebra expression. In this paper, we propose\nWarped Convolution Networks (WCN) to effectively estimate the homography\ntransformation by SL(3) group and sl(3) algebra with group convolution. To this\nend, six commutative subgroups within SL(3) group are composed to form a\nhomography transformation. For each subgroup, a warping function is proposed to\nbridge the Lie algebra structure to its corresponding parameters in tomography.\nBy taking advantage of the warped convolution, homography estimation is\nformulated into several simple pseudo-translation regressions. By walking along\nthe Lie topology, our proposed WCN is able to learn the features that are\ninvariant to homography transformation. It can be easily plugged into other\npopular CNN-based methods. Extensive experiments on POT benchmark and\nMNIST-Proj dataset show that our proposed method is effective for both\nhomography estimation and classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xinrui Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Short-range forecasts of global precipitation using using deep learning-augmented numerical weather prediction. (arXiv:2206.11669v1 [physics.ao-ph])","link":"http://arxiv.org/abs/2206.11669","description":"<p>Precipitation governs Earth's hydroclimate, and its daily spatiotemporal\nfluctuations have major socioeconomic effects. Advances in Numerical weather\nprediction (NWP) have been measured by the improvement of forecasts for various\nphysical fields such as temperature and pressure; however, large biases exist\nin precipitation prediction. We augment the output of the well-known NWP model\nCFSv2 with deep learning to create a hybrid model that improves short-range\nglobal precipitation at 1-, 2-, and 3-day lead times. To hybridise, we address\nthe sphericity of the global data by using modified DLWP-CS architecture which\ntransforms all the fields to cubed-sphere projection. Dynamical model\nprecipitation and surface temperature outputs are fed into a modified DLWP-CS\n(UNET) to forecast ground truth precipitation. While CFSv2's average bias is +5\nto +7 mm/day over land, the multivariate deep learning model decreases it to\nwithin -1 to +1 mm/day. Hurricane Katrina in 2005, Hurricane Ivan in 2004,\nChina floods in 2010, India floods in 2005, and Myanmar storm Nargis in 2008\nare used to confirm the substantial enhancement in the skill for the hybrid\ndynamical-deep learning model. CFSv2 typically shows a moderate to large bias\nin the spatial pattern and overestimates the precipitation at short-range time\nscales. The proposed deep learning augmented NWP model can address these biases\nand vastly improve the spatial pattern and magnitude of predicted\nprecipitation. Deep learning enhanced CFSv2 reduces mean bias by 8x over\nimportant land regions for 1 day lead compared to CFSv2. The spatio-temporal\ndeep learning system opens pathways to further the precision and accuracy in\nglobal short-range precipitation forecasts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Singh_M/0/1/0/all/0/1\">Manmeet Singh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+B_V/0/1/0/all/0/1\">Vaisakh S B</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Acharya_N/0/1/0/all/0/1\">Nachiketa Acharya</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rao_S/0/1/0/all/0/1\">Suryachandra A Rao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kumar_B/0/1/0/all/0/1\">Bipin Kumar</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_Z/0/1/0/all/0/1\">Zong-Liang Yang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Niyogi_D/0/1/0/all/0/1\">Dev Niyogi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BlazePose GHUM Holistic: Real-time 3D Human Landmarks and Pose Estimation. (arXiv:2206.11678v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11678","description":"<p>We present BlazePose GHUM Holistic, a lightweight neural network pipeline for\n3D human body landmarks and pose estimation, specifically tailored to real-time\non-device inference. BlazePose GHUM Holistic enables motion capture from a\nsingle RGB image including avatar control, fitness tracking and AR/VR effects.\nOur main contributions include i) a novel method for 3D ground truth data\nacquisition, ii) updated 3D body tracking with additional hand landmarks and\niii) full body pose estimation from a monocular image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grishchenko_I/0/1/0/all/0/1\">Ivan Grishchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazarevsky_V/0/1/0/all/0/1\">Valentin Bazarevsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanfir_A/0/1/0/all/0/1\">Andrei Zanfir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazavan_E/0/1/0/all/0/1\">Eduard Gabriel Bazavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanfir_M/0/1/0/all/0/1\">Mihai Zanfir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yee_R/0/1/0/all/0/1\">Richard Yee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raveendran_K/0/1/0/all/0/1\">Karthik Raveendran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhdanovich_M/0/1/0/all/0/1\">Matsvei Zhdanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grundmann_M/0/1/0/all/0/1\">Matthias Grundmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1\">Cristian Sminchisescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NTIRE 2022 Challenge on Perceptual Image Quality Assessment. (arXiv:2206.11695v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11695","description":"<p>This paper reports on the NTIRE 2022 challenge on perceptual image quality\nassessment (IQA), held in conjunction with the New Trends in Image Restoration\nand Enhancement workshop (NTIRE) workshop at CVPR 2022. This challenge is held\nto address the emerging challenge of IQA by perceptual image processing\nalgorithms. The output images of these algorithms have completely different\ncharacteristics from traditional distortions and are included in the PIPAL\ndataset used in this challenge. This challenge is divided into two tracks, a\nfull-reference IQA track similar to the previous NTIRE IQA challenge and a new\ntrack that focuses on the no-reference IQA methods. The challenge has 192 and\n179 registered participants for two tracks. In the final testing stage, 7 and 8\nparticipating teams submitted their models and fact sheets. Almost all of them\nhave achieved better results than existing IQA methods, and the winning method\ncan demonstrate state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Haoming Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jimmy S. Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Training with Autoencoders for Visual Anomaly Detection. (arXiv:2206.11723v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11723","description":"<p>Deep convolutional autoencoders provide an effective tool for learning\nnon-linear dimensionality reduction in an unsupervised way. Recently, they have\nbeen used for the task of anomaly detection in the visual domain. By optimising\nfor the reconstruction error using anomaly-free examples, the common belief is\nthat a trained network will have difficulties to reconstruct anomalous parts\nduring the test phase. This is usually done by controlling the capacity of the\nnetwork by either reducing the size of the bottleneck layer or enforcing\nsparsity constraints on its activations. However, neither of these techniques\ndoes explicitly penalise reconstruction of anomalous signals often resulting in\na poor detection. We tackle this problem by adapting a self-supervised learning\nregime which allows to use discriminative information during training while\nregularising the model to focus on the data manifold by means of a modified\nreconstruction error resulting in an accurate detection. Unlike related\napproaches, the inference of the proposed method during training and prediction\nis very efficient processing the whole input image in one single step. Our\nexperiments on the MVTec Anomaly Detection dataset demonstrate high recognition\nand localisation performance of the proposed method. On the texture-subset, in\nparticular, our approach consistently outperforms a bunch of recent anomaly\ndetection methods by a big margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bauer_A/0/1/0/all/0/1\">Alexander Bauer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds. (arXiv:2206.11736v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11736","description":"<p>In order for artificial agents to perform useful tasks in changing\nenvironments, they must be able to both detect and adapt to novelty. However,\nvisual novelty detection research often only evaluates on repurposed datasets\nsuch as CIFAR-10 originally intended for object classification. This practice\nrestricts novelties to well-framed images of distinct object types. We suggest\nthat new benchmarks are needed to represent the challenges of navigating an\nopen world. Our new NovelCraft dataset contains multi-modal episodic data of\nthe images and symbolic world-states seen by an agent completing a pogo-stick\nassembly task within a video game world. In some episodes, we insert novel\nobjects that can impact gameplay. Novelty can vary in size, position, and\nocclusion within complex scenes. We benchmark state-of-the-art novelty\ndetection and generalized category discovery models with a focus on\ncomprehensive evaluation. Results suggest an opportunity for future research:\nmodels aware of task-specific costs of different types of mistakes could more\neffectively detect and adapt to novelty in open worlds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feeney_P/0/1/0/all/0/1\">Patrick Feeney</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1\">Sarah Schneider</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Lymperopoulos_P/0/1/0/all/0/1\">Panagiotis Lymperopoulos</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liping Liu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Scheutz_M/0/1/0/all/0/1\">Matthias Scheutz</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1\">Michael C. Hughes</a> (1) ((1) Dept. of Computer Science, Tufts University, (2) Center for Vision, Automation and Control, Austrian Institute of Technology)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evidence fusion with contextual discounting for multi-modality medical image segmentation. (arXiv:2206.11739v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11739","description":"<p>As information sources are usually imperfect, it is necessary to take into\naccount their reliability in multi-source information fusion tasks. In this\npaper, we propose a new deep framework allowing us to merge multi-MR image\nsegmentation results using the formalism of Dempster-Shafer theory while taking\ninto account the reliability of different modalities relative to different\nclasses. The framework is composed of an encoder-decoder feature extraction\nmodule, an evidential segmentation module that computes a belief function at\neach voxel for each modality, and a multi-modality evidence fusion module,\nwhich assigns a vector of discount rates to each modality evidence and combines\nthe discounted evidence using Dempster's rule. The whole framework is trained\nby minimizing a new loss function based on a discounted Dice index to increase\nsegmentation accuracy and reliability. The method was evaluated on the BraTs\n2021 database of 1251 patients with brain tumors. Quantitative and qualitative\nresults show that our method outperforms the state of the art, and implements\nan effective new idea for merging multi-information within deep neural\nnetworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Ling Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denoeux_T/0/1/0/all/0/1\">Thierry Denoeux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_P/0/1/0/all/0/1\">Pierre Vera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_S/0/1/0/all/0/1\">Su Ruan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptPose: Language Prompt Helps Animal Pose Estimation. (arXiv:2206.11752v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11752","description":"<p>Recently, animal pose estimation is attracting increasing interest from the\nacademia (e.g., wildlife and conservation biology) focusing on animal behavior\nunderstanding. However, currently animal pose estimation suffers from small\ndatasets and large data variances, making it difficult to obtain robust\nperformance. To tackle this problem, we propose that the rich knowledge about\nrelations between pose-related semantics learned by language models can be\nutilized to improve the animal pose estimation. Therefore, in this study, we\nintroduce a novel PromptPose framework to effectively apply language models for\nbetter understanding the animal poses based on prompt training. In PromptPose,\nwe propose that adapting the language knowledge to the visual animal poses is\nkey to achieve effective animal pose estimation. To this end, we first\nintroduce textual prompts to build connections between textual semantic\ndescriptions and supporting animal keypoint features. Moreover, we further\ndevise a pixel-level contrastive loss to build dense connections between\ntextual descriptions and local image features, as well as a semantic-level\ncontrastive loss to bridge the gap between global contrasts in language-image\ncross-modal pre-training and local contrasts in dense prediction. In practice,\nthe PromptPose has shown great benefits for improving animal pose estimation.\nBy conducting extensive experiments, we show that our PromptPose achieves\nsuperior performance under both supervised and few-shot settings, outperforming\nrepresentative methods by a large margin. The source code and models will be\nmade publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What makes you, you? Analyzing Recognition by Swapping Face Parts. (arXiv:2206.11759v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11759","description":"<p>Deep learning advanced face recognition to an unprecedented accuracy.\nHowever, understanding how local parts of the face affect the overall\nrecognition performance is still mostly unclear. Among others, face swap has\nbeen experimented to this end, but just for the entire face. In this paper, we\npropose to swap facial parts as a way to disentangle the recognition relevance\nof different face parts, like eyes, nose and mouth. In our method, swapping\nparts from a source face to a target one is performed by fitting a 3D prior,\nwhich establishes dense pixels correspondence between parts, while also\nhandling pose differences. Seamless cloning is then used to obtain smooth\ntransitions between the mapped source regions and the shape and skin tone of\nthe target face. We devised an experimental protocol that allowed us to draw\nsome preliminary conclusions when the swapped images are classified by deep\nnetworks, indicating a prominence of the eyes and eyebrows region. Code\navailable at https://github.com/clferrari/FacePartsSwap\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_C/0/1/0/all/0/1\">Claudio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serpentoni_M/0/1/0/all/0/1\">Matteo Serpentoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berretti_S/0/1/0/all/0/1\">Stefano Berretti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1\">Alberto Del Bimbo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FitGAN: Fit- and Shape-Realistic Generative Adversarial Networks for Fashion. (arXiv:2206.11768v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11768","description":"<p>Amidst the rapid growth of fashion e-commerce, remote fitting of fashion\narticles remains a complex and challenging problem and a main driver of\ncustomers' frustration. Despite the recent advances in 3D virtual try-on\nsolutions, such approaches still remain limited to a very narrow - if not only\na handful - selection of articles, and often for only one size of those fashion\nitems. Other state-of-the-art approaches that aim to support customers find\nwhat fits them online mostly require a high level of customer engagement and\nprivacy-sensitive data (such as height, weight, age, gender, belly shape,\netc.), or alternatively need images of customers' bodies in tight clothing.\nThey also often lack the ability to produce fit and shape aware visual guidance\nat scale, coming up short by simply advising which size to order that would\nbest match a customer's physical body attributes, without providing any\ninformation on how the garment may fit and look. Contributing towards taking a\nleap forward and surpassing the limitations of current approaches, we present\nFitGAN, a generative adversarial model that explicitly accounts for garments'\nentangled size and fit characteristics of online fashion at scale. Conditioned\non the fit and shape of the articles, our model learns disentangled item\nrepresentations and generates realistic images reflecting the true fit and\nshape properties of fashion articles. Through experiments on real world data at\nscale, we demonstrate how our approach is capable of synthesizing visually\nrealistic and diverse fits of fashion items and explore its ability to control\nfit and shape of images for thousands of online garments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pecenakova_S/0/1/0/all/0/1\">Sonia Pecenakova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karessli_N/0/1/0/all/0/1\">Nour Karessli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shirvany_R/0/1/0/all/0/1\">Reza Shirvany</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Surgical Instrument Segmentation: A Background Image Can Be All You Need. (arXiv:2206.11804v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11804","description":"<p>Data diversity and volume are crucial to the success of training deep\nlearning models, while in the medical imaging field, the difficulty and cost of\ndata collection and annotation are especially huge. Specifically in robotic\nsurgery, data scarcity and imbalance have heavily affected the model accuracy\nand limited the design and deployment of deep learning-based surgical\napplications such as surgical instrument segmentation. Considering this, in\nthis paper, we rethink the surgical instrument segmentation task and propose a\none-to-many data generation solution that gets rid of the complicated and\nexpensive process of data collection and annotation from robotic surgery. In\nour method, we only utilize a single surgical background tissue image and a few\nopen-source instrument images as the seed images and apply multiple\naugmentations and blending techniques to synthesize amounts of image\nvariations. In addition, we also introduce the chained augmentation mixing\nduring training to further enhance the data diversities. The proposed approach\nis evaluated on the real datasets of the EndoVis-2018 and EndoVis-2017 surgical\nscene segmentation. Our empirical analysis suggests that without the high cost\nof data collection and annotation, we can achieve decent surgical instrument\nsegmentation performance. Moreover, we also observe that our method can deal\nwith novel instrument prediction in the deployment domain. We hope our\ninspiring results would encourage researchers to emphasize data-centric methods\nto overcome demanding deep learning limitations besides data shortage, such as\nclass imbalance, domain adaptation, and incremental learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">An Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mobarakol Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengya Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongliang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unseen Object 6D Pose Estimation: A Benchmark and Baselines. (arXiv:2206.11808v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11808","description":"<p>Estimating the 6D pose for unseen objects is in great demand for many\nreal-world applications. However, current state-of-the-art pose estimation\nmethods can only handle objects that are previously trained. In this paper, we\npropose a new task that enables and facilitates algorithms to estimate the 6D\npose estimation of novel objects during testing. We collect a dataset with both\nreal and synthetic images and up to 48 unseen objects in the test set. In the\nmean while, we propose a new metric named Infimum ADD (IADD) which is an\ninvariant measurement for objects with different types of pose ambiguity. A\ntwo-stage baseline solution for this task is also provided. By training an\nend-to-end 3D correspondences network, our method finds corresponding points\nbetween an unseen object and a partial view RGBD image accurately and\nefficiently. It then calculates the 6D pose from the correspondences using an\nalgorithm robust to object symmetry. Extensive experiments show that our method\noutperforms several intuitive baselines and thus verify its effectiveness. All\nthe data, code and models will be made publicly available. Project page:\nwww.graspnet.net/unseen6d\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gou_M/0/1/0/all/0/1\">Minghao Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Haolin Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Hao-Shu Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Ping Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLOSA: Object detection based on 2D local feature superimposed self-attention. (arXiv:2206.11825v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11825","description":"<p>We analyzed the network structure of real-time object detection models and\nfound that the features in the feature concatenation stage are very rich.\nApplying an attention module here can effectively improve the detection\naccuracy of the model. However, the commonly used attention module or\nself-attention module shows poor performance in detection accuracy and\ninference efficiency. Therefore, we propose a novel self-attention module,\ncalled 2D local feature superimposed self-attention, for the feature\nconcatenation stage of the neck network. This self-attention module reflects\nglobal features through local features and local receptive fields. We also\npropose and optimize an efficient decoupled head and AB-OTA, and achieve SOTA\nresults. Average precisions of 49.0\\% (66.2 FPS), 46.1\\% (80.6 FPS), and 39.1\\%\n(100 FPS) were obtained for large, medium, and small-scale models built using\nour proposed improvements. Our models exceeded YOLOv5 by 0.8\\% -- 3.1\\% in\naverage precision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weisheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lin Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Clinically Assisted Colorectal Polyp Recognition via Structured Cross-modal Representation Consistency. (arXiv:2206.11826v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11826","description":"<p>The colorectal polyps classification is a critical clinical examination. To\nimprove the classification accuracy, most computer-aided diagnosis algorithms\nrecognize colorectal polyps by adopting Narrow-Band Imaging (NBI). However, the\nNBI usually suffers from missing utilization in real clinic scenarios since the\nacquisition of this specific image requires manual switching of the light mode\nwhen polyps have been detected by using White-Light (WL) images. To avoid the\nabove situation, we propose a novel method to directly achieve accurate\nwhite-light colonoscopy image classification by conducting structured\ncross-modal representation consistency. In practice, a pair of multi-modal\nimages, i.e. NBI and WL, are fed into a shared Transformer to extract\nhierarchical feature representations. Then a novel designed Spatial Attention\nModule (SAM) is adopted to calculate the similarities between the class token\nand patch tokens %from multi-levels for a specific modality image. By aligning\nthe class tokens and spatial attention maps of paired NBI and WL images at\ndifferent levels, the Transformer achieves the ability to keep both global and\nlocal representation consistency for the above two modalities. Extensive\nexperimental results illustrate the proposed method outperforms the recent\nstudies with a margin, realizing multi-modal prediction with a single\nTransformer while greatly improving the classification accuracy when only with\nWL images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Weijie Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Ye Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruimao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yiwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Li Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sample Condensation in Online Continual Learning. (arXiv:2206.11849v1 [cs.LG])","link":"http://arxiv.org/abs/2206.11849","description":"<p>Online Continual learning is a challenging learning scenario where the model\nmust learn from a non-stationary stream of data where each sample is seen only\nonce. The main challenge is to incrementally learn while avoiding catastrophic\nforgetting, namely the problem of forgetting previously acquired knowledge\nwhile learning from new data. A popular solution in these scenario is to use a\nsmall memory to retain old data and rehearse them over time. Unfortunately, due\nto the limited memory size, the quality of the memory will deteriorate over\ntime. In this paper we propose OLCGM, a novel replay-based continual learning\nstrategy that uses knowledge condensation techniques to continuously compress\nthe memory and achieve a better use of its limited size. The sample\ncondensation step compresses old samples, instead of removing them like other\nreplay strategies. As a result, the experiments show that, whenever the memory\nbudget is limited compared to the complexity of the data, OLCGM improves the\nfinal accuracy compared to state-of-the-art replay strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sangermano_M/0/1/0/all/0/1\">Mattia Sangermano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1\">Antonio Carta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cossu_A/0/1/0/all/0/1\">Andrea Cossu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1\">Davide Bacciu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Remote Sensing Change Detection (Segmentation) using Denoising Diffusion Probabilistic Models. (arXiv:2206.11892v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11892","description":"<p>Human civilization has an increasingly powerful influence on the earth\nsystem, and earth observations are an invaluable tool for assessing and\nmitigating the negative impacts. To this end, observing precisely defined\nchanges on Earth's surface is essential, and we propose an effective way to\nachieve this goal. Notably, our change detection (CD)/ segmentation method\nproposes a novel way to incorporate the millions of off-the-shelf, unlabeled,\nremote sensing images available through different earth observation programs\ninto the training process through denoising diffusion probabilistic models. We\nfirst leverage the information from these off-the-shelf, uncurated, and\nunlabeled remote sensing images by using a pre-trained denoising diffusion\nprobabilistic model and then employ the multi-scale feature representations\nfrom the diffusion model decoder to train a lightweight CD classifier to detect\nprecise changes. The experiments performed on four publically available CD\ndatasets show that the proposed approach achieves remarkably better results\nthan the state-of-the-art methods in F1, IoU, and overall accuracy. Code and\npre-trained models are available at: https://github.com/wgcban/ddpm-cd\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_N/0/1/0/all/0/1\">Nithin Gopalakrishnan Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaskViT: Masked Visual Pre-Training for Video Prediction. (arXiv:2206.11894v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11894","description":"<p>The ability to predict future visual observations conditioned on past\nobservations and motor commands can enable embodied agents to plan solutions to\na variety of tasks in complex environments. This work shows that we can create\ngood video prediction models by pre-training transformers via masked visual\nmodeling. Our approach, named MaskViT, is based on two simple design decisions.\nFirst, for memory and training efficiency, we use two types of window\nattention: spatial and spatiotemporal. Second, during training, we mask a\nvariable percentage of tokens instead of a fixed mask ratio. For inference,\nMaskViT generates all tokens via iterative refinement where we incrementally\ndecrease the masking ratio following a mask scheduling function. On several\ndatasets we demonstrate that MaskViT outperforms prior works in video\nprediction, is parameter efficient, and can generate high-resolution videos\n(256x256). Further, we demonstrate the benefits of inference speedup (up to\n512x) due to iterative decoding by using MaskViT for planning on a real robot.\nOur work suggests that we can endow embodied agents with powerful predictive\nmodels by leveraging the general framework of masked visual modeling with\nminimal domain knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Agrim Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Stephen Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Viewpoint-Agnostic Visual Representations by Recovering Tokens in 3D Space. (arXiv:2206.11895v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11895","description":"<p>Humans are remarkably flexible in understanding viewpoint changes due to\nvisual cortex supporting the perception of 3D structure. In contrast, most of\nthe computer vision models that learn visual representation from a pool of 2D\nimages often fail to generalize over novel camera viewpoints. Recently, the\nvision architectures have shifted towards convolution-free architectures,\nvisual Transformers, which operate on tokens derived from image patches.\nHowever, neither these Transformers nor 2D convolutional networks perform\nexplicit operations to learn viewpoint-agnostic representation for visual\nunderstanding. To this end, we propose a 3D Token Representation Layer (3DTRL)\nthat estimates the 3D positional information of the visual tokens and leverages\nit for learning viewpoint-agnostic representations. The key elements of 3DTRL\ninclude a pseudo-depth estimator and a learned camera matrix to impose\ngeometric transformations on the tokens. These enable 3DTRL to recover the 3D\npositional information of the tokens from 2D patches. In practice, 3DTRL is\neasily plugged-in into a Transformer. Our experiments demonstrate the\neffectiveness of 3DTRL in many vision tasks including image classification,\nmulti-view video alignment, and action recognition. The models with 3DTRL\noutperform their backbone Transformers in all the tasks with minimal added\ncomputation. Our project page is at\nhttps://www3.cs.stonybrook.edu/~jishang/3dtrl/3dtrl.html\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jinghuan Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srijan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael S. Ryoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EventNeRF: Neural Radiance Fields from a Single Colour Event Camera. (arXiv:2206.11896v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11896","description":"<p>Learning coordinate-based volumetric 3D scene representations such as neural\nradiance fields (NeRF) has been so far studied assuming RGB or RGB-D images as\ninputs. At the same time, it is known from the neuroscience literature that\nhuman visual system (HVS) is tailored to process asynchronous brightness\nchanges rather than synchronous RGB images, in order to build and continuously\nupdate mental 3D representations of the surroundings for navigation and\nsurvival. Visual sensors that were inspired by HVS principles are event\ncameras. Thus, events are sparse and asynchronous per-pixel brightness (or\ncolour channel) change signals. In contrast to existing works on neural 3D\nscene representation learning, this paper approaches the problem from a new\nperspective. We demonstrate that it is possible to learn NeRF suitable for\nnovel-view synthesis in the RGB space from asynchronous event streams. Our\nmodels achieve high visual accuracy of the rendered novel views of challenging\nscenes in the RGB space, even though they are trained with substantially fewer\ndata (i.e., event streams from a single event camera moving around the object)\nand more efficiently (due to the inherent sparsity of event streams) than the\nexisting NeRF models trained with RGB images. We will release our datasets and\nthe source code, see https://4dqv.mpi-inf.mpg.de/EventNeRF/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rudnev_V/0/1/0/all/0/1\">Viktor Rudnev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elgharib_M/0/1/0/all/0/1\">Mohamed Elgharib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modulating Bottom-Up and Top-Down Visual Processing via Language-Conditional Filters. (arXiv:2003.12739v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.12739","description":"<p>How to best integrate linguistic and perceptual processing in multi-modal\ntasks that involve language and vision is an important open problem. In this\nwork, we argue that the common practice of using language in a top-down manner,\nto direct visual attention over high-level visual features, may not be optimal.\nWe hypothesize that the use of language to also condition the bottom-up\nprocessing from pixels to high-level features can provide benefits to the\noverall performance. To support our claim, we propose a U-Net-based model and\nperform experiments on two language-vision dense-prediction tasks: referring\nexpression segmentation and language-guided image colorization. We compare\nresults where either one or both of the top-down and bottom-up visual branches\nare conditioned on language. Our experiments reveal that using language to\ncontrol the filters for bottom-up visual processing in addition to top-down\nattention leads to better results on both tasks and achieves competitive\nperformance. Our linguistic analysis suggests that bottom-up conditioning\nimproves segmentation of objects especially when input text refers to low-level\nvisual concepts. Code is available at https://github.com/ilkerkesen/bvpr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kesen_I/0/1/0/all/0/1\">&#x130;lker Kesen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Can_O/0/1/0/all/0/1\">Ozan Arkan Can</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1\">Erkut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1\">Aykut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuret_D/0/1/0/all/0/1\">Deniz Yuret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEAN: graph-based pruning for convolutional neural networks by extracting longest chains. (arXiv:2011.06923v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.06923","description":"<p>Neural network pruning techniques can substantially reduce the computational\ncost of applying convolutional neural networks (CNNs). Common pruning methods\ndetermine which convolutional filters to remove by ranking the filters\nindividually, i.e., without taking into account their interdependence. In this\npaper, we advocate the viewpoint that pruning should consider the\ninterdependence between series of consecutive operators. We propose the\nLongEst-chAiN (LEAN) method that prunes CNNs by using graph-based algorithms to\nselect relevant chains of convolutions. A CNN is interpreted as a graph, with\nthe operator norm of each operator as distance metric for the edges. LEAN\npruning iteratively extracts the highest value path from the graph to keep. In\nour experiments, we test LEAN pruning on several image-to-image tasks,\nincluding the well-known CamVid dataset, and a real-world X-ray CT dataset.\nResults indicate that LEAN pruning can result in networks with similar\naccuracy, while using 1.7-12x fewer convolutional filters than existing\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schoonhoven_R/0/1/0/all/0/1\">Richard Schoonhoven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendriksen_A/0/1/0/all/0/1\">Allard A. Hendriksen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelt_D/0/1/0/all/0/1\">Dani&#xeb;l M. Pelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batenburg_K/0/1/0/all/0/1\">K. Joost Batenburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end One-shot Human Parsing. (arXiv:2105.01241v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.01241","description":"<p>Previous human parsing models are limited to parsing humans into pre-defined\nclasses, which is inflexible for practical fashion applications that often have\nnew fashion item classes. In this paper, we define a novel one-shot human\nparsing (OSHP) task that requires parsing humans into an open set of classes\ndefined by any test example. During training, only base classes are exposed,\nwhich only overlap with part of the test-time classes. To address three main\nchallenges in OSHP, i.e., small sizes, testing bias, and similar parts, we\ndevise an End-to-end One-shot human Parsing Network (EOP-Net). Firstly, an\nend-to-end human parsing framework is proposed to parse the query image into\nboth coarse-grained and fine-grained human classes, which builds a strong\nembedding network with rich semantic information shared across different\ngranularities, facilitating identifying small-sized human classes. Then, we\npropose learning momentum-updated prototypes by gradually smoothing the\ntraining time static prototypes, which helps stabilize the training and learn\nrobust features. Moreover, we devise a dual metric learning scheme which\nencourages the network to enhance features' both representational capability\nand transferability. Therefore, our EOP-Net can learn representative features\nthat can quickly adapt to the novel classes and mitigate the testing bias\nissue. In addition, we employ a contrastive loss at the prototype level,\nthereby enforcing the distances among the classes in the fine-grained metric\nspace to discriminate similar parts. We tailor three existing popular human\nparsing benchmarks to the OSHP task. Experiments on the new benchmarks\ndemonstrate that EOP-Net outperforms representative one-shot segmentation\nmodels by large margins, which serves as a strong baseline for further research\non this new task. The source code is available at\nhttps://github.com/Charleshhy/One-shot-Human-Parsing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haoyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers. (arXiv:2106.10270v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10270","description":"<p>Vision Transformers (ViT) have been shown to attain highly competitive\nperformance for a wide range of vision applications, such as image\nclassification, object detection and semantic image segmentation. In comparison\nto convolutional neural networks, the Vision Transformer's weaker inductive\nbias is generally found to cause an increased reliance on model regularization\nor data augmentation (\"AugReg\" for short) when training on smaller training\ndatasets. We conduct a systematic empirical study in order to better understand\nthe interplay between the amount of training data, AugReg, model size and\ncompute budget. As one result of this study we find that the combination of\nincreased compute and AugReg can yield models with the same performance as\nmodels trained on an order of magnitude more training data: we train ViT models\nof various sizes on the public ImageNet-21k dataset which either match or\noutperform their counterparts trained on the larger, but not publicly available\nJFT-300M dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1\">Andreas Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1\">Alexander Kolesnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wightman_R/0/1/0/all/0/1\">Ross Wightman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uszkoreit_J/0/1/0/all/0/1\">Jakob Uszkoreit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1\">Lucas Beyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Energy-Based Generative Cooperative Saliency Prediction. (arXiv:2106.13389v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13389","description":"<p>Conventional saliency prediction models typically learn a deterministic\nmapping from an image to its saliency map, and thus fail to explain the\nsubjective nature of human attention. In this paper, to model the uncertainty\nof visual saliency, we study the saliency prediction problem from the\nperspective of generative models by learning a conditional probability\ndistribution over the saliency map given an input image, and treating the\nsaliency prediction as a sampling process from the learned distribution.\nSpecifically, we propose a generative cooperative saliency prediction\nframework, where a conditional latent variable model (LVM) and a conditional\nenergy-based model (EBM) are jointly trained to predict salient objects in a\ncooperative manner. The LVM serves as a fast but coarse predictor to\nefficiently produce an initial saliency map, which is then refined by the\niterative Langevin revision of the EBM that serves as a slow but fine\npredictor. Such a coarse-to-fine cooperative saliency prediction strategy\noffers the best of both worlds. Moreover, we propose a \"cooperative learning\nwhile recovering\" strategy and apply it to weakly supervised saliency\nprediction, where saliency annotations of training images are partially\nobserved. Lastly, we find that the learned energy function in the EBM can serve\nas a refinement module that can refine the results of other pre-trained\nsaliency prediction models. Experimental results show that our model can\nproduce a set of diverse and plausible saliency maps of an image, and obtain\nstate-of-the-art performance in both fully supervised and weakly supervised\nsaliency prediction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jianwen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zilong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Bias in Visual Datasets. (arXiv:2107.07919v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.07919","description":"<p>Computer Vision (CV) has achieved remarkable results, outperforming humans in\nseveral tasks. Nonetheless, it may result in significant discrimination if not\nhandled properly as CV systems highly depend on the data they are fed with and\ncan learn and amplify biases within such data. Thus, the problems of\nunderstanding and discovering biases are of utmost importance. Yet, there is no\ncomprehensive survey on bias in visual datasets. Hence, this work aims to: i)\ndescribe the biases that might manifest in visual datasets; ii) review the\nliterature on methods for bias discovery and quantification in visual datasets;\niii) discuss existing attempts to collect bias-aware visual datasets. A key\nconclusion of our study is that the problem of bias discovery and\nquantification in visual datasets is still open, and there is room for\nimprovement in terms of both methods and the range of biases that can be\naddressed. Moreover, there is no such thing as a bias-free dataset, so\nscientists and practitioners must become aware of the biases in their datasets\nand make them explicit. To this end, we propose a checklist to spot different\ntypes of bias during visual dataset collection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fabbrizzi_S/0/1/0/all/0/1\">Simone Fabbrizzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Symeon Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1\">Eirini Ntoutsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1\">Ioannis Kompatsiaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Stacking Ensemble Approach for Supervised Video Summarization. (arXiv:2109.12581v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12581","description":"<p>Video summarization methods are usually classified into shot-level or\nframe-level methods, which are individually used in a general way. This paper\ninvestigates the underlying complementarity between the frame-level and\nshot-level methods, and a stacking ensemble approach is proposed for supervised\nvideo summarization. Firstly, we build up a stacking model to predict both the\nkey frame probabilities and the temporal interest segments simultaneously. The\ntwo components are then combined via soft decision fusion to obtain the final\nscores of each frame in the video. A joint loss function is proposed here to\ntrain the model. The ablation experimental results show that the proposed\nmethod outperforms both the two corresponding individual method. Furthermore,\nextensive experiments and analysis on two benchmark datasets demonstrate the\neffectiveness of our method and its superior performance in comparison with the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_Y/0/1/0/all/0/1\">Yubo An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shenghui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoqiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keys to Accurate Feature Extraction Using Residual Spiking Neural Networks. (arXiv:2111.05955v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.05955","description":"<p>Spiking neural networks (SNNs) have become an interesting alternative to\nconventional artificial neural networks (ANN) thanks to their temporal\nprocessing capabilities and energy efficient implementations in neuromorphic\nhardware. However the challenges involved in training SNNs have limited their\nperformance in terms of accuracy and thus their applications. Improving\nlearning algorithms and neural architectures for a more accurate feature\nextraction is therefore one of the current priorities in SNN research. In this\npaper we present a study on the key components of modern spiking architectures.\nWe design a spiking version of the successful residual network architecture and\nprovide an in-depth study on the possible implementations of spiking residual\nconnections. This study shows how, depending on the use case, the optimal\nresidual connection implementation may vary. Additionally, we empirically\ncompare different techniques in image classification datasets taken from the\nbest performing networks. Our results provide a state of the art guide to SNN\ndesign, which allows to make informed choices when trying to build the optimal\nvisual feature extractor. Finally, our network outperforms previous SNN\narchitectures in CIFAR-10 (94.14%) and CIFAR-100 (74.65%) datasets and matches\nthe state of the art in DVS-CIFAR10 (72.98%), with less parameters than the\nprevious state of the art and without the need for ANN-SNN conversion. Code\navailable at https://github.com/VicenteAlex/Spiking_ResNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vicente_Sola_A/0/1/0/all/0/1\">Alex Vicente-Sola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manna_D/0/1/0/all/0/1\">Davide L. Manna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirkland_P/0/1/0/all/0/1\">Paul Kirkland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caterina_G/0/1/0/all/0/1\">Gaetano Di Caterina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bihl_T/0/1/0/all/0/1\">Trevor Bihl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model-Based Single Image Deep Dehazing. (arXiv:2111.10943v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10943","description":"<p>Model-based single image dehazing algorithms restore images with sharp edges\nand rich details at the expense of low PSNR values. Data-driven ones restore\nimages with high PSNR values but with low contrast, and even some remaining\nhaze. In this paper, a novel single image dehazing algorithm is introduced by\nfusing model-based and data-driven approaches. Both transmission map and\natmospheric light are initialized by the model-based methods, and refined by\ndeep learning approaches which form a neural augmentation. Haze-free images are\nrestored by using the transmission map and atmospheric light. Experimental\nresults indicate that the proposed algorithm can remove haze well from\nreal-world and synthetic hazy images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chaobing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_H/0/1/0/all/0/1\">Haiyan Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shiqian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-horizon Robot Manipulation Tasks. (arXiv:2112.03227v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2112.03227","description":"<p>General-purpose robots coexisting with humans in their environment must learn\nto relate human language to their perceptions and actions to be useful in a\nrange of daily tasks. Moreover, they need to acquire a diverse repertoire of\ngeneral-purpose skills that allow composing long-horizon tasks by following\nunconstrained language instructions. In this paper, we present CALVIN\n(Composing Actions from Language and Vision), an open-source simulated\nbenchmark to learn long-horizon language-conditioned tasks. Our aim is to make\nit possible to develop agents that can solve many robotic manipulation tasks\nover a long horizon, from onboard sensors, and specified only via human\nlanguage. CALVIN tasks are more complex in terms of sequence length, action\nspace, and language than existing vision-and-language task datasets and\nsupports flexible specification of sensor suites. We evaluate the agents in\nzero-shot to novel language instructions and to novel environments and objects.\nWe show that a baseline model based on multi-context imitation learning\nperforms poorly on CALVIN, suggesting that there is significant room for\ndeveloping innovative agents that learn to relate human language to their world\nmodels with this benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mees_O/0/1/0/all/0/1\">Oier Mees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermann_L/0/1/0/all/0/1\">Lukas Hermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosete_Beas_E/0/1/0/all/0/1\">Erick Rosete-Beas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PriFit: Learning to Fit Primitives Improves Few Shot Point Cloud Segmentation. (arXiv:2112.13942v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13942","description":"<p>We present PriFit, a semi-supervised approach for label-efficient learning of\n3D point cloud segmentation networks. PriFit combines geometric primitive\nfitting with point-based representation learning. Its key idea is to learn\npoint representations whose clustering reveals shape regions that can be\napproximated well by basic geometric primitives, such as cuboids and\nellipsoids. The learned point representations can then be re-used in existing\nnetwork architectures for 3D point cloud segmentation, and improves their\nperformance in the few-shot setting. According to our experiments on the widely\nused ShapeNet and PartNet benchmarks, PriFit outperforms several\nstate-of-the-art methods in this setting, suggesting that decomposability into\nprimitives is a useful prior for learning representations predictive of\nsemantic parts. We present a number of ablative experiments varying the choice\nof geometric primitives and downstream tasks to demonstrate the effectiveness\nof the method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1\">Gopal Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_B/0/1/0/all/0/1\">Bidya Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+RoyChowdhury_A/0/1/0/all/0/1\">Aruni RoyChowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadelha_M/0/1/0/all/0/1\">Matheus Gadelha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loizou_M/0/1/0/all/0/1\">Marios Loizou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Liangliang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Learned_Miller_E/0/1/0/all/0/1\">Erik Learned-Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhransu Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalogerakis_E/0/1/0/all/0/1\">Evangelos Kalogerakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Concatenation Volume for Accurate and Efficient Stereo Matching. (arXiv:2203.02146v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02146","description":"<p>Stereo matching is a fundamental building block for many vision and robotics\napplications. An informative and concise cost volume representation is vital\nfor stereo matching of high accuracy and efficiency. In this paper, we present\na novel cost volume construction method which generates attention weights from\ncorrelation clues to suppress redundant information and enhance\nmatching-related information in the concatenation volume. To generate reliable\nattention weights, we propose multi-level adaptive patch matching to improve\nthe distinctiveness of the matching cost at different disparities even for\ntextureless regions. The proposed cost volume is named attention concatenation\nvolume (ACV) which can be seamlessly embedded into most stereo matching\nnetworks, the resulting networks can use a more lightweight aggregation network\nand meanwhile achieve higher accuracy, e.g. using only 1/25 parameters of the\naggregation network can achieve higher accuracy for GwcNet. Furthermore, we\ndesign a highly accurate network (ACVNet) based on our ACV, which achieves\nstate-of-the-art performance on several benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Gangwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Junda Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Peng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Remote blood pressure measurement via spatiotemporal mapping of a short-time facial video. (arXiv:2203.03634v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.03634","description":"<p>Blood pressure (BP) monitoring is vital in daily healthcare, especially for\ncardiovascular diseases. However, BP values are mainly acquired through the\ncontact sensing method, which is inconvenient and unfriendly to continuous BP\nmeasurement. Hence, we propose an efficient end-to-end network to estimate the\nBP values from a facial video to achieve remote BP measurement in daily life.\nIn this study, we first derived a Spatial-temporal map of a short-time (~15s)\nfacial video. According to the Spatial-temporal map, we then regressed the BP\nranges by a designed blood pressure classifier and simultaneously calculated\nthe specific value by a blood pressure calculator in each BP range. In\naddition, we also developed an innovative oversampling training strategy to\nhandle the unbalanced data distribution problem. Finally, we trained the\nproposed network on a private dataset ASPD and tested it on the popular dataset\nMMSE-HR. As a result, the proposed network achieved a state-of-the-art MAE of\n12.35 mmHg and 9.5 mmHg on systolic and diastolic BP measurements, which is\nbetter than the recent works. It concludes that the proposed method has\nexcellent potential for camera-based BP monitoring in real-world scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_J/0/1/0/all/0/1\">Jialiang Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yuheng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_X/0/1/0/all/0/1\">Xiujuan Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VGQ-CNN: Moving Beyond Fixed Cameras and Top-Grasps for Grasp Quality Prediction. (arXiv:2203.04874v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2203.04874","description":"<p>We present the Versatile Grasp Quality Convolutional Neural Network\n(VGQ-CNN), a grasp quality prediction network for 6-DOF grasps. VGQ-CNN can be\nused when evaluating grasps for objects seen from a wide range of camera poses\nor mobile robots without the need to retrain the network. By defining the grasp\norientation explicitly as an input to the network, VGQ-CNN can evaluate 6-DOF\ngrasp poses, moving beyond the 4-DOF grasps used in most image-based grasp\nevaluation methods like GQ-CNN. To train VGQ-CNN, we generate the new Versatile\nGrasp dataset (VG-dset) containing 6-DOF grasps observed from a wide range of\ncamera poses. VGQ-CNN achieves a balanced accuracy of 82.1% on our test-split\nwhile generalising to a variety of camera poses. Meanwhile, it achieves\ncompetitive performance for overhead cameras and top-grasps with a balanced\naccuracy of 74.2% compared to GQ-CNN's 76.6%. We also propose a modified\nnetwork architecture, FAST-VGQ-CNN, that speeds up inference using a shared\nencoder architecture and can make 128 grasp quality predictions in 12ms on a\nCPU. Code and data are available at https://aucoroboticsmu.github.io/vgq-cnn/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Konrad_A/0/1/0/all/0/1\">A. Konrad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1\">J. McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villing_R/0/1/0/all/0/1\">R. Villing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-similarity based Hyperrelation Network for few-shot segmentation. (arXiv:2203.09550v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09550","description":"<p>Few-shot semantic segmentation aims at recognizing the object regions of\nunseen categories with only a few annotated examples as supervision. The key to\nfew-shot segmentation is to establish a robust semantic relationship between\nthe support and query images and to prevent overfitting. In this paper, we\npropose an effective Multi-similarity Hyperrelation Network (MSHNet) to tackle\nthe few-shot semantic segmentation problem. In MSHNet, we propose a new\nGenerative Prototype Similarity (GPS), which together with cosine similarity\ncan establish a strong semantic relation between the support and query images.\nThe locally generated prototype similarity based on global feature is logically\ncomplementary to the global cosine similarity based on local feature, and the\nrelationship between the query image and the supported image can be expressed\nmore comprehensively by using the two similarities simultaneously. In addition,\nwe propose a Symmetric Merging Block (SMB) in MSHNet to efficiently merge\nmulti-layer, multi-shot and multi-similarity hyperrelational features. MSHNet\nis built on the basis of similarity rather than specific category features,\nwhich can achieve more general unity and effectively reduce overfitting. On two\nbenchmark semantic segmentation datasets Pascal-5i and COCO-20i, MSHNet\nachieves new state-of-the-art performances on 1-shot and 5-shot semantic\nsegmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiangwen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhe Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaobing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Miao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xianghong Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Real Time Satellite Pose Estimation on Low Power Edge TPU. (arXiv:2204.03296v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03296","description":"<p>Pose estimation of an uncooperative space resident object is a key asset\ntowards autonomy in close proximity operations. In this context monocular\ncameras are a valuable solution because of their low system requirements.\nHowever, the associated image processing algorithms are either too\ncomputationally expensive for real time on-board implementation, or not enough\naccurate. In this paper we propose a pose estimation software exploiting neural\nnetwork architectures which can be scaled to different accuracy-latency\ntrade-offs. We designed our pipeline to be compatible with Edge Tensor\nProcessing Units to show how low power machine learning accelerators could\nenable Artificial Intelligence exploitation in space. The neural networks were\ntested both on the benchmark Spacecraft Pose Estimation Dataset, and on the\npurposely developed Cosmo Photorealistic Dataset, which depicts a COSMO-SkyMed\nsatellite in a variety of random poses and steerable solar panels orientations.\nThe lightest version of our architecture achieves state-of-the-art accuracy on\nboth datasets but at a fraction of networks complexity, running at 7.7 frames\nper second on a Coral Dev Board Mini consuming just 2.2W.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lotti_A/0/1/0/all/0/1\">Alessandro Lotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modenini_D/0/1/0/all/0/1\">Dario Modenini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tortora_P/0/1/0/all/0/1\">Paolo Tortora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saponara_M/0/1/0/all/0/1\">Massimiliano Saponara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perino_M/0/1/0/all/0/1\">Maria A. Perino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Diffusion Models. (arXiv:2204.03458v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03458","description":"<p>Generating temporally coherent high fidelity video is an important milestone\nin generative modeling research. We make progress towards this milestone by\nproposing a diffusion model for video generation that shows very promising\ninitial results. Our model is a natural extension of the standard image\ndiffusion architecture, and it enables jointly training from image and video\ndata, which we find to reduce the variance of minibatch gradients and speed up\noptimization. To generate long and higher resolution videos we introduce a new\nconditional sampling technique for spatial and temporal video extension that\nperforms better than previously proposed methods. We present the first results\non a large text-conditioned video generation task, as well as state-of-the-art\nresults on established benchmarks for video prediction and unconditional video\ngeneration. Supplementary material is available at\nhttps://video-diffusion.github.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1\">Alexey Gritsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning Ensemble Framework for Off-Nadir Geocentric Pose Prediction. (arXiv:2205.11230v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11230","description":"<p>Roughly 6,800 natural disasters occur worldwide annually, and this alarming\nnumber continues to grow due to the effects of climate change. Effective\nmethods to improve natural disaster response include performing change\ndetection, map alignment, and vision-aided navigation to allow for the\ntime-efficient delivery of life-saving aid. Current software functions\noptimally only on nadir images taken ninety degrees above ground level. The\ninability to generalize to oblique images increases the need to compute an\nimage's geocentric pose, which is its spatial orientation with respect to\ngravity. This Deep Learning investigation presents three convolutional models\nto predict geocentric pose using 5,923 nadir and oblique red, green, and blue\n(RGB) satellite images of cities worldwide. The first model is an autoencoder\nthat condenses the 256 x 256 x 3 images to 32 x 32 x 16 latent space\nrepresentations, demonstrating the ability to learn useful features from the\ndata. The second model is a U-Net Fully Convolutional Network with skip\nconnections used to predict each image's corresponding pixel-level elevation\nmask. This model achieves a median absolute deviation of 0.335 meters and an R2\nof 0.865 on test data. Afterward, the elevation masks are concatenated with the\nRGB images to form four-channel inputs fed into the third model, which predicts\neach image's rotation angle and scale, the components of its geocentric pose.\nThis Deep Convolutional Neural Network achieves an R2 of 0.943 on test data,\nsignificantly outperforming previous models designed by researchers. The\nhigh-accuracy software built in this study contributes to mapping and\nnavigation procedures to accelerate disaster relief and save human lives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Christopher Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_J/0/1/0/all/0/1\">Jai Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiti_M/0/1/0/all/0/1\">Milind Maiti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mesh-based Dynamics with Occlusion Reasoning for Cloth Manipulation. (arXiv:2206.02881v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2206.02881","description":"<p>Self-occlusion is challenging for cloth manipulation, as it makes it\ndifficult to estimate the full state of the cloth. Ideally, a robot trying to\nunfold a crumpled or folded cloth should be able to reason about the cloth's\noccluded regions. We leverage recent advances in pose estimation for cloth to\nbuild a system that uses explicit occlusion reasoning to unfold a crumpled\ncloth. Specifically, we first learn a model to reconstruct the mesh of the\ncloth. However, the model will likely have errors due to the complexities of\nthe cloth configurations and due to ambiguities from occlusions. Our main\ninsight is that we can further refine the predicted reconstruction by\nperforming test-time finetuning with self-supervised losses. The obtained\nreconstructed mesh allows us to use a mesh-based dynamics model for planning\nwhile reasoning about occlusions. We evaluate our system both on cloth\nflattening as well as on cloth canonicalization, in which the objective is to\nmanipulate the cloth into a canonical pose. Our experiments show that our\nmethod significantly outperforms prior methods that do not explicitly account\nfor occlusions or perform test-time optimization. Videos and visualizations can\nbe found on our\n$\\href{https://sites.google.com/view/occlusion-reason/home}{\\text{project\nwebsite}}.$\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zixuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xingyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1\">David Held</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dyna-DM: Dynamic Object-aware Self-supervised Monocular Depth Maps. (arXiv:2206.03799v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.03799","description":"<p>Self-supervised monocular depth estimation has been a subject of intense\nstudy in recent years, because of its applications in robotics and autonomous\ndriving. Much of the recent work focuses on improving depth estimation by\nincreasing architecture complexity. This paper shows that state-of-the-art\nperformance can also be achieved by improving the learning process rather than\nincreasing model complexity. More specifically, we propose (i) only using\ninvariant pose loss for the first few epochs during training, (ii) disregarding\nsmall potentially dynamic objects when training, and (iii) employing an\nappearance-based approach to separately estimate object pose for truly dynamic\nobjects. We demonstrate that these simplifications reduce GPU memory usage by\n29% and result in qualitatively and quantitatively improved depth maps. The\ncode is available at https://github.com/kieran514/Dyna-DM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saunders_K/0/1/0/all/0/1\">Kieran Saunders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogiatzis_G/0/1/0/all/0/1\">George Vogiatzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manso_L/0/1/0/all/0/1\">Luis J. Manso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CASS: Cross Architectural Self-Supervision for Medical Image Analysis. (arXiv:2206.04170v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.04170","description":"<p>Recent advances in Deep Learning and Computer Vision have alleviated many of\nthe bottlenecks, allowing algorithms to be label-free with better performance.\nSpecifically, Transformers provide a global perspective of the image, which\nConvolutional Neural Networks (CNN) lack by design. Here we present Cross\nArchitectural Self-Supervision, a novel self-supervised learning approach which\nleverages transformers and CNN simultaneously, while also being computationally\naccessible to general practitioners via easily available cloud services.\nCompared to existing state-of-the-art self-supervised learning approaches, we\nempirically show CASS trained CNNs, and Transformers gained an average of 8.5%\nwith 100% labelled data, 7.3% with 10% labelled data, and 11.5% with 1%\nlabelled data, across three diverse datasets. Notably, one of the employed\ndatasets included histopathology slides of an autoimmune disease, a topic\nunderrepresented in Medical Imaging and has minimal data. In addition, our\nfindings reveal that CASS is twice as efficient as other state-of-the-art\nmethods in terms of training time. The code is open source and is available on\nGitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Pranav Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sizikova_E/0/1/0/all/0/1\">Elena Sizikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cirrone_J/0/1/0/all/0/1\">Jacopo Cirrone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Self-supervised Learning Really Improve Reinforcement Learning from Pixels?. (arXiv:2206.05266v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.05266","description":"<p>We investigate whether self-supervised learning (SSL) can improve online\nreinforcement learning (RL) from pixels. We extend the contrastive\nreinforcement learning framework (e.g., CURL) that jointly optimizes SSL and RL\nlosses and conduct an extensive amount of experiments with various\nself-supervised losses. Our observations suggest that the existing SSL\nframework for RL fails to bring meaningful improvement over the baselines only\ntaking advantage of image augmentation when the same amount of data and\naugmentation is used. We further perform an evolutionary search to find the\noptimal combination of multiple self-supervised losses for RL, but find that\neven such a loss combination fails to meaningfully outperform the methods that\nonly utilize carefully designed image augmentations. Often, the use of\nself-supervised losses under the existing framework lowered RL performances. We\nevaluate the approach in multiple different environments including a real-world\nrobot environment and confirm that no single self-supervised loss or image\naugmentation method can dominate all environments and that the current\nframework for joint optimization of SSL and RL is limited. Finally, we\nempirically investigate the pretraining framework for SSL + RL and the\nproperties of representations learned with different approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jinghuan Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srijan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael S. Ryoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Polyp Segmentation with Multiple Kernel Dilated Convolution Network. (arXiv:2206.06264v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.06264","description":"<p>The detection and removal of precancerous polyps through colonoscopy is the\nprimary technique for the prevention of colorectal cancer worldwide. However,\nthe miss rate of colorectal polyp varies significantly among the endoscopists.\nIt is well known that a computer-aided diagnosis (CAD) system can assist\nendoscopists in detecting colon polyps and minimize the variation among\nendoscopists. In this study, we introduce a novel deep learning architecture,\nnamed MKDCNet, for automatic polyp segmentation robust to significant changes\nin polyp data distribution. MKDCNet is simply an encoder-decoder neural network\nthat uses the pre-trained ResNet50 as the encoder and novel multiple kernel\ndilated convolution (MKDC) block that expands the field of view to learn more\nrobust and heterogeneous representation. Extensive experiments on four publicly\navailable polyp datasets and cell nuclei dataset show that the proposed MKDCNet\noutperforms the state-of-the-art methods when trained and tested on the same\ndataset as well when tested on unseen polyp datasets from different\ndistributions. With rich results, we demonstrated the robustness of the\nproposed architecture. From an efficiency perspective, our algorithm can\nprocess at (approx 45) frames per second on RTX 3090 GPU. MKDCNet can be a\nstrong benchmark for building real-time systems for clinical colonoscopies. The\ncode of the proposed MKDCNet is available at\nhttps://github.com/nikhilroxtomar/MKDCNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tomar_N/0/1/0/all/0/1\">Nikhil Kumar Tomar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Srivastava_A/0/1/0/all/0/1\">Abhishek Srivastava</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bagci_U/0/1/0/all/0/1\">Ulas Bagci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jha_D/0/1/0/all/0/1\">Debesh Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Adversarial Attacks and Defenses in Vision Transformers trained with DINO. (arXiv:2206.06761v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.06761","description":"<p>This work conducts the first analysis on the robustness against adversarial\nattacks on self-supervised Vision Transformers trained using DINO. First, we\nevaluate whether features learned through self-supervision are more robust to\nadversarial attacks than those emerging from supervised learning. Then, we\npresent properties arising for attacks in the latent space. Finally, we\nevaluate whether three well-known defense strategies can increase adversarial\nrobustness in downstream tasks by only fine-tuning the classification head to\nprovide robustness even in view of limited compute resources. These defense\nstrategies are: Adversarial Training, Ensemble Adversarial Training and\nEnsemble of Specialized Networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rando_J/0/1/0/all/0/1\">Javier Rando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naimi_N/0/1/0/all/0/1\">Nasib Naimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumann_T/0/1/0/all/0/1\">Thomas Baumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathys_M/0/1/0/all/0/1\">Max Mathys</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Broken News: Making Newspapers Accessible to Print-Impaired. (arXiv:2206.10225v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.10225","description":"<p>Accessing daily news content still remains a big challenge for people with\nprint-impairment including blind and low-vision due to opacity of printed\ncontent and hindrance from online sources. In this paper, we present our\napproach for digitization of print newspaper into an accessible file format\nsuch as HTML. We use an ensemble of instance segmentation and detection\nframework for newspaper layout analysis and then OCR to recognize text elements\nsuch as headline and article text. Additionally, we propose EdgeMask loss\nfunction for Mask-RCNN framework to improve segmentation mask boundary and\nhence accuracy of downstream OCR task. Empirically, we show that our proposed\nloss function reduces the Word Error Rate (WER) of news article text by 32.5 %.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_V/0/1/0/all/0/1\">Vishal Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganu_T/0/1/0/all/0/1\">Tanuja Ganu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_S/0/1/0/all/0/1\">Saikat Guha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HealNet -- Self-Supervised Acute Wound Heal-Stage Classification. (arXiv:2206.10536v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.10536","description":"<p>Identifying, tracking, and predicting wound heal-stage progression is a\nfundamental task towards proper diagnosis, effective treatment, facilitating\nhealing, and reducing pain. Traditionally, a medical expert might observe a\nwound to determine the current healing state and recommend treatment. However,\nsourcing experts who can produce such a diagnosis solely from visual indicators\ncan be difficult, time-consuming and expensive. In addition, lesions may take\nseveral weeks to undergo the healing process, demanding resources to monitor\nand diagnose continually. Automating this task can be challenging; datasets\nthat follow wound progression from onset to maturation are small, rare, and\noften collected without computer vision in mind. To tackle these challenges, we\nintroduce a self-supervised learning scheme composed of (a) learning embeddings\nof wound's temporal dynamics, (b) clustering for automatic stage discovery, and\n(c) fine-tuned classification. The proposed self-supervised and flexible\nlearning framework is biologically inspired and trained on a small dataset with\nzero human labeling. The HealNet framework achieved high pre-text and\ndownstream classification accuracy; when evaluated on held-out test data,\nHealNet achieved 97.7% pre-text accuracy and 90.62% heal-stage classification\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carrion_H/0/1/0/all/0/1\">H&#xe9;ctor Carri&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafari_M/0/1/0/all/0/1\">Mohammad Jafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hsin-Ya Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isseroff_R/0/1/0/all/0/1\">Roslyn Rivkah Isseroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rolandi_M/0/1/0/all/0/1\">Marco Rolandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_M/0/1/0/all/0/1\">Marcella Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_N/0/1/0/all/0/1\">Narges Norouzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TiCo: Transformation Invariance and Covariance Contrast for Self-Supervised Visual Representation Learning. (arXiv:2206.10698v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.10698","description":"<p>We present Transformation Invariance and Covariance Contrast (TiCo) for\nself-supervised visual representation learning. Similar to other recent\nself-supervised learning methods, our method is based on maximizing the\nagreement among embeddings of different distorted versions of the same image,\nwhich pushes the encoder to produce transformation invariant representations.\nTo avoid the trivial solution where the encoder generates constant vectors, we\nregularize the covariance matrix of the embeddings from different images by\npenalizing low rank solutions. By jointly minimizing the transformation\ninvariance loss and covariance contrast loss, we get an encoder that is able to\nproduce useful representations for downstream tasks. We analyze our method and\nshow that it can be viewed as a variant of MoCo with an implicit memory bank of\nunlimited size at no extra memory cost. This makes our method perform better\nthan alternative methods when using small batch sizes. TiCo can also be seen as\na modification of Barlow Twins. By connecting the contrastive and\nredundancy-reduction methods together, TiCo gives us new insights into how\njoint embedding methods work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiachen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moraes_R/0/1/0/all/0/1\">Rafael M. Moraes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karakulak_S/0/1/0/all/0/1\">Serkan Karakulak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sobol_V/0/1/0/all/0/1\">Vlad Sobol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canziani_A/0/1/0/all/0/1\">Alfredo Canziani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Correct and Certify: A New Approach to Self-Supervised 3D-Object Perception. (arXiv:2206.11215v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.11215","description":"<p>We consider an object pose estimation and model fitting problem, where -\ngiven a partial point cloud of an object - the goal is to estimate the object\npose by fitting a CAD model to the sensor data. We solve this problem by\ncombining (i) a semantic keypoint-based pose estimation model, (ii) a novel\nself-supervised training approach, and (iii) a certification procedure, that\nnot only verifies whether the output produced by the model is correct or not,\nbut also flags uniqueness of the produced solution. The semantic keypoint\ndetector model is initially trained in simulation and does not perform well on\nreal-data due to the domain gap. Our self-supervised training procedure uses a\ncorrector and a certification module to improve the detector. The corrector\nmodule corrects the detected keypoints to compensate for the domain gap, and is\nimplemented as a declarative layer, for which we develop a simple\ndifferentiation rule. The certification module declares whether the corrected\noutput produced by the model is certifiable (i.e. correct) or not. At each\niteration, the approach optimizes over the loss induced only by the certifiable\ninput-output pairs. As training progresses, we see that the fraction of outputs\nthat are certifiable increases, eventually reaching near $100\\%$ in many cases.\nWe also introduce the notion of strong certifiability wherein the model can\ndetermine if the predicted object model fit is unique or not. The detected\nsemantic keypoints help us implement this in the forward pass. We conduct\nextensive experiments to evaluate the performance of the corrector, the\ncertification, and the proposed self-supervised training using the ShapeNet and\nYCB datasets, and show the proposed approach achieves performance comparable to\nfully supervised baselines while not requiring pose or keypoint supervision on\nreal data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Talak_R/0/1/0/all/0/1\">Rajat Talak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Lisa Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}