<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-05-13T01:30:00Z">05-13</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks. (arXiv:2205.05718v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05718">
<div class="article-summary-box-inner">
<span><p>Human language offers a powerful window into our thoughts -- we tell stories,
give explanations, and express our beliefs and goals through words. Abundant
evidence also suggests that language plays a developmental role in structuring
our learning. Here, we ask: how much of human-like thinking can be captured by
learning statistical patterns in language alone? We first contribute a new
challenge benchmark for comparing humans and distributional large language
models (LLMs). Our benchmark contains two problem-solving domains (planning and
explanation generation) and is designed to require generalization to new,
out-of-distribution problems expressed in language. We find that humans are far
more robust than LLMs on this benchmark. Next, we propose a hybrid
Parse-and-Solve model, which augments distributional LLMs with a structured
symbolic reasoning module. We find that this model shows more robust adaptation
to out-of-distribution planning problems, demonstrating the promise of hybrid
AI models for more human-like reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Some Grammatical Errors are Frequent, Others are Important. (arXiv:2205.05730v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05730">
<div class="article-summary-box-inner">
<span><p>In Grammatical Error Correction, systems are evaluated by the number of
errors they correct. However, no one has assessed whether all error types are
equally important. We provide and apply a method to quantify the importance of
different grammatical error types to humans. We show that some rare errors are
considered disturbing while other common ones are not. This affects possible
directions to improve both systems and their evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DISARM: Detecting the Victims Targeted by Harmful Memes. (arXiv:2205.05738v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05738">
<div class="article-summary-box-inner">
<span><p>Internet memes have emerged as an increasingly popular means of communication
on the Web. Although typically intended to elicit humour, they have been
increasingly used to spread hatred, trolling, and cyberbullying, as well as to
target specific individuals, communities, or society on political,
socio-cultural, and psychological grounds. While previous work has focused on
detecting harmful, hateful, and offensive memes, identifying whom they attack
remains a challenging and underexplored area. Here we aim to bridge this gap.
In particular, we create a dataset where we annotate each meme with its
victim(s) such as the name of the targeted person(s), organization(s), and
community(ies). We then propose DISARM (Detecting vIctimS targeted by hARmful
Memes), a framework that uses named entity recognition and person
identification to detect all entities a meme is referring to, and then,
incorporates a novel contextualized multimodal deep neural network to classify
whether the meme intends to harm these entities. We perform several systematic
experiments on three test setups, corresponding to entities that are (a) all
seen while training, (b) not seen as a harmful target on training, and (c) not
seen at all on training. The evaluation results show that DISARM significantly
outperforms ten unimodal and multimodal systems. Finally, we show that DISARM
is interpretable and comparatively more generalizable and that it can reduce
the relative error rate for harmful target identification by up to 9 points
absolute over several strong multimodal rivals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Retrieve Videos by Asking Questions. (arXiv:2205.05739v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05739">
<div class="article-summary-box-inner">
<span><p>The majority of traditional text-to-video retrieval systems operate in static
environments, i.e., there is no interaction between the user and the agent
beyond the initial textual query provided by the user. This can be suboptimal
if the initial query has ambiguities, which would lead to many falsely
retrieved videos. To overcome this limitation, we propose a novel framework for
Video Retrieval using Dialog (ViReD), which enables the user to interact with
an AI agent via multiple rounds of dialog. The key contribution of our
framework is a novel multimodal question generator that learns to ask questions
that maximize the subsequent video retrieval performance. Our multimodal
question generator uses (i) the video candidates retrieved during the last
round of interaction with the user and (ii) the text-based dialog history
documenting all previous interactions, to generate questions that incorporate
both visual and linguistic cues relevant to video retrieval. Furthermore, to
generate maximally informative questions, we propose an Information-Guided
Supervision (IGS), which guides the question generator to ask questions that
would boost subsequent video retrieval accuracy. We validate the effectiveness
of our interactive ViReD framework on the AVSD dataset, showing that our
interactive method performs significantly better than traditional
non-interactive video retrieval systems. Furthermore, we also demonstrate that
our proposed approach also generalizes to the real-world settings that involve
interactions with real humans, thus, demonstrating the robustness and
generality of our framework
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SubER: A Metric for Automatic Evaluation of Subtitle Quality. (arXiv:2205.05805v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05805">
<div class="article-summary-box-inner">
<span><p>This paper addresses the problem of evaluating the quality of automatically
generated subtitles, which includes not only the quality of the
machine-transcribed or translated speech, but also the quality of line
segmentation and subtitle timing. We propose SubER - a single novel metric
based on edit distance with shifts that takes all of these subtitle properties
into account. We compare it to existing metrics for evaluating transcription,
translation, and subtitle quality. A careful human evaluation in a post-editing
scenario shows that the new metric has a high correlation with the post-editing
effort and direct human assessment scores, outperforming baseline metrics
considering only the subtitle text, such as WER and BLEU, and existing methods
to integrate segmentation and timing features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AppTek's Submission to the IWSLT 2022 Isometric Spoken Language Translation Task. (arXiv:2205.05807v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05807">
<div class="article-summary-box-inner">
<span><p>To participate in the Isometric Spoken Language Translation Task of the IWSLT
2022 evaluation, constrained condition, AppTek developed neural
Transformer-based systems for English-to-German with various mechanisms of
length control, ranging from source-side and target-side pseudo-tokens to
encoding of remaining length in characters that replaces positional encoding.
We further increased translation length compliance by sentence-level selection
of length-compliant hypotheses from different system variants, as well as
rescoring of N-best candidates from a single system. Length-compliant
back-translated and forward-translated synthetic data, as well as other
parallel data variants derived from the original MuST-C training corpus were
important for a good quality/desired length trade-off. Our experimental results
show that length compliance levels above 90% can be reached while minimizing
losses in MT quality as measured in BERT and BLEU scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Vocabulary Extreme Classification Using Generative Models. (arXiv:2205.05812v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05812">
<div class="article-summary-box-inner">
<span><p>The extreme multi-label classification (XMC) task aims at tagging content
with a subset of labels from an extremely large label set. The label vocabulary
is typically defined in advance by domain experts and assumed to capture all
necessary tags. However in real world scenarios this label set, although large,
is often incomplete and experts frequently need to refine it. To develop
systems that simplify this process, we introduce the task of open vocabulary
XMC (OXMC): given a piece of content, predict a set of labels, some of which
may be outside of the known tag set. Hence, in addition to not having training
data for some labels - as is the case in zero-shot classification - models need
to invent some labels on-the-fly. We propose GROOV, a fine-tuned seq2seq model
for OXMC that generates the set of labels as a flat sequence and is trained
using a novel loss independent of predicted label order. We show the efficacy
of the approach, experimenting with popular XMC datasets for which GROOV is
able to predict meaningful labels outside the given vocabulary while performing
on par with state-of-the-art solutions for known labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NFLAT: Non-Flat-Lattice Transformer for Chinese Named Entity Recognition. (arXiv:2205.05832v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05832">
<div class="article-summary-box-inner">
<span><p>Recently, FLAT has achieved great success in Chinese Named Entity Recognition
(NER). This method achieves lexical enhancement by constructing a flat lattice,
which mitigates the difficulties posed by blurred word boundaries and the lack
of word semantics. To this end, FLAT uses the position information of the
starting and ending characters to connect the matching words. However, this
method is likely to match more words when dealing with long texts, resulting in
very long input sequences. Therefore, it increases the memory used by
self-attention and computational costs. To deal with this issue, we advocate a
novel lexical enhancement method, InterFormer, that effectively reduces the
amount of computational and memory costs by constructing the non-flat-lattice.
Furthermore, we implement a complete model, namely NFLAT, for the Chinese NER
task. NFLAT decouples lexicon fusion and context feature encoding. Compared
with FLAT, it reduces unnecessary attention calculations in "word-character"
and "word-word". This reduces the memory usage by about 50\% and can use more
extensive lexicons or higher batches for network training. The experimental
results obtained on several well-known benchmarks demonstrate the superiority
of the proposed method over the state-of-the-art character-word hybrid models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supplementary Material: Implementation and Experiments for GAU-based Model. (arXiv:2205.05842v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05842">
<div class="article-summary-box-inner">
<span><p>In February this year Google proposed a new Transformer variant called FLASH,
which has a faster speed, lower VRAM footprint and better performance. This is
achieved by designing a performant layer named GAU (Gated Attention Unit),
which combines the Attention layer and FFN. In this paper, some implementation
details are re-analyzed both theoretically and practically. We then propose a
novel GAU-based model and pre-train it model on a Chinese corpus. Results of
the CLUE benchmark show that our model achieves a dev average score of 75.02,
1% higher than RoFormerV1 and being 45% faster, which is also competitive with
RoFormerV2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">e-CARE: a New Dataset for Exploring Explainable Causal Reasoning. (arXiv:2205.05849v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05849">
<div class="article-summary-box-inner">
<span><p>Understanding causality has vital importance for various Natural Language
Processing (NLP) applications. Beyond the labeled instances, conceptual
explanations of the causality can provide deep understanding of the causal
facts to facilitate the causal reasoning process. However, such explanation
information still remains absent in existing causal reasoning resources. In
this paper, we fill this gap by presenting a human-annotated explainable CAusal
REasoning dataset (e-CARE), which contains over 21K causal reasoning questions,
together with natural language formed explanations of the causal questions.
Experimental results show that generating valid explanations for causal facts
still remains especially challenging for the state-of-the-art models, and the
explanation information can be helpful for promoting the accuracy and stability
of causal reasoning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Open Arabic Named Entity Recognition Tools. (arXiv:2205.05857v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05857">
<div class="article-summary-box-inner">
<span><p>The main objective of this paper is to compare and evaluate the performances
of three open Arabic NER tools: CAMeL, Hatmi, and Stanza. We collected a corpus
consisting of 30 articles written in MSA and manually annotated all the
entities of the person, organization, and location types at the article
(document) level. Our results suggest a similarity between Stanza and Hatmi
with the latter receiving the highest F1 score for the three entity types.
However, CAMeL achieved the highest precision values for names of people and
organizations. Following this, we implemented a "merge" method that combined
the results from the three tools and a "vote" method that tagged named entities
only when two of the three identified them as entities. Our results showed that
merging achieved the highest overall F1 scores. Moreover, merging had the
highest recall values while voting had the highest precision values for the
three entity types. This indicates that merging is more suitable when recall is
desired, while voting is optimal when precision is required. Finally, we
collected a corpus of 21,635 articles related to COVID-19 and applied the merge
and vote methods. Our analysis demonstrates the tradeoff between precision and
recall for the two methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaVAE: Exploring Adaptive GPT-2s in Variational Auto-Encoders for Language Modeling. (arXiv:2205.05862v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05862">
<div class="article-summary-box-inner">
<span><p>Variational Auto-Encoder (VAE) has become the de-facto learning paradigm in
achieving both representation learning and generation for natural language.
However, existing VAE-based language models either employ elementary RNNs,
which is not powerful to handle multi-tasks, or fine-tunes two pre-trained
language models (PLMs) for any downstream task, which requires huge energy
consumption. In this paper, we introduce the first VAE framework empowered with
adaptive GPT-2s (AdaVAE). Different from mentioned systems, we unify both the
encoder and decoder of VAE model using GPT-2s with adaptive parameter-efficient
components. Experiments from multiple dimensions validate that AdaVAE is
competent to better organize language in generation and representation
modeling, even with less than $15\%$ additionally activated parameters during
training. Our code is available at \url{https://github.com/ImKeTT/adavae}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Chit-Chats Enhanced Task-Oriented Dialogue Corpora for Fuse-Motive Conversation Systems. (arXiv:2205.05886v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05886">
<div class="article-summary-box-inner">
<span><p>The goal of building intelligent dialogue systems has largely been separately
pursued under two motives: task-oriented dialogue (TOD) systems, and
open-domain systems for chit-chat (CC). Although previous TOD dialogue systems
work well in the testing sets of benchmarks, they would lead to undesirable
failure when being exposed to natural scenarios in practice, where user
utterances can be of high motive-diversity that fusing both TOD and CC in
multi-turn interaction. Since an industrial TOD system should be able to
converse with the user between TOD and CC motives, constructing a fuse-motive
dialogue dataset that contains both TOD or CC is important. Most prior work
relies on crowd workers to collect and annotate large scale dataset and is
restricted to English language setting. Our work, on the contrary, addresses
this problem in a more effective way and releases a multi-turn dialogues
dataset called CCET (Chinese Chat-Enhanced-Task). Meanwhile, we also propose a
line of fuse-motive dialogues formalization approach, along with several
evaluation metrics for TOD sessions that are integrated by CC utterances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging the Gap between Reality and Ideality of Entity Matching: A Revisiting and Benchmark Re-Construction. (arXiv:2205.05889v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05889">
<div class="article-summary-box-inner">
<span><p>Entity matching (EM) is the most critical step for entity resolution (ER).
While current deep learningbased methods achieve very impressive performance on
standard EM benchmarks, their realworld application performance is much
frustrating. In this paper, we highlight that such the gap between reality and
ideality stems from the unreasonable benchmark construction process, which is
inconsistent with the nature of entity matching and therefore leads to biased
evaluations of current EM approaches. To this end, we build a new EM corpus and
re-construct EM benchmarks to challenge critical assumptions implicit in the
previous benchmark construction process by step-wisely changing the restricted
entities, balanced labels, and single-modal records in previous benchmarks into
open entities, imbalanced labels, and multimodal records in an open
environment. Experimental results demonstrate that the assumptions made in the
previous benchmark construction process are not coincidental with the open
environment, which conceal the main challenges of the task and therefore
significantly overestimate the current progress of entity matching. The
constructed benchmarks and code are publicly released
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Gender Stereotypes in Hindi and Marathi. (arXiv:2205.05901v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05901">
<div class="article-summary-box-inner">
<span><p>As the use of natural language processing increases in our day-to-day life,
the need to address gender bias inherent in these systems also amplifies. This
is because the inherent bias interferes with the semantic structure of the
output of these systems while performing tasks like machine translation. While
research is being done in English to quantify and mitigate bias, debiasing
methods in Indic Languages are either relatively nascent or absent for some
Indic languages altogether. Most Indic languages are gendered, i.e., each noun
is assigned a gender according to each language's grammar rules. As a
consequence, evaluation differs from what is done in English. This paper
evaluates the gender stereotypes in Hindi and Marathi languages. The
methodologies will differ from the ones in the English language because there
are masculine and feminine counterparts in the case of some words. We create a
dataset of neutral and gendered occupation words, emotion words and measure
bias with the help of Embedding Coherence Test (ECT) and Relative Norm Distance
(RND). We also attempt to mitigate this bias from the embeddings. Experiments
show that our proposed debiasing techniques reduce gender bias in these
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Inductive Bias in Transformers for Unsupervised Disentanglement of Syntax and Semantics with VAEs. (arXiv:2205.05943v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05943">
<div class="article-summary-box-inner">
<span><p>We propose a generative model for text generation, which exhibits
disentangled latent representations of syntax and semantics. Contrary to
previous work, this model does not need syntactic information such as
constituency parses, or semantic information such as paraphrase pairs. Our
model relies solely on the inductive bias found in attention-based
architectures such as Transformers.
</p>
<p>In the attention of Transformers, keys handle information selection while
values specify what information is conveyed. Our model, dubbed QKVAE, uses
Attention in its decoder to read latent variables where one latent variable
infers keys while another infers values. We run experiments on latent
representations and experiments on syntax/semantics transfer which show that
QKVAE displays clear signs of disentangled syntax and semantics. We also show
that our model displays competitive syntax transfer capabilities when compared
to supervised models and that comparable supervised models need a fairly large
amount of data (more than 50K samples) to outperform it on both syntactic and
semantic transfer. The code for our experiments is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Computational Acquisition Model for Multimodal Word Categorization. (arXiv:2205.05974v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05974">
<div class="article-summary-box-inner">
<span><p>Recent advances in self-supervised modeling of text and images open new
opportunities for computational models of child language acquisition, which is
believed to rely heavily on cross-modal signals. However, prior studies have
been limited by their reliance on vision models trained on large image datasets
annotated with a pre-defined set of depicted object categories. This is (a) not
faithful to the information children receive and (b) prohibits the evaluation
of such models with respect to category learning tasks, due to the pre-imposed
category structure. We address this gap, and present a cognitively-inspired,
multimodal acquisition model, trained from image-caption pairs on naturalistic
data using cross-modal self-supervision. We show that the model learns word
categories and object recognition abilities, and presents trends reminiscent of
those reported in the developmental literature. We make our code and trained
models public for future reference and use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AiSocrates: Towards Answering Ethical Quandary Questions. (arXiv:2205.05989v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05989">
<div class="article-summary-box-inner">
<span><p>Considerable advancements have been made in various NLP tasks based on the
impressive power of large pre-trained language models (LLMs). These results
have inspired efforts to understand the limits of LLMs so as to evaluate how
far we are from achieving human level general natural language understanding.
In this work, we challenge the capability of LLMs with the new task of Ethical
Quandary Generative Question Answering. Ethical quandary questions are more
challenging to address because multiple conflicting answers may exist to a
single quandary. We propose a system, AiSocrates, that provides an answer with
a deliberative exchange of different perspectives to an ethical quandary, in
the approach of Socratic philosophy, instead of providing a closed answer like
an oracle. AiSocrates searches for different ethical principles applicable to
the ethical quandary and generates an answer conditioned on the chosen
principles through prompt-based few-shot learning. We also address safety
concerns by providing a human controllability option in choosing ethical
principles. We show that AiSocrates generates promising answers to ethical
quandary questions with multiple perspectives, 6.92% more often than answers
written by human philosophers by one measure, but the system still needs
improvement to match the coherence of human philosophers fully. We argue that
AiSocrates is a promising step toward developing an NLP system that
incorporates human values explicitly by prompt instructions. We are releasing
the code for research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlling Formality in Low-Resource NMT with Domain Adaptation and Re-Ranking: SLT-CDT-UoS at IWSLT2022. (arXiv:2205.05990v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05990">
<div class="article-summary-box-inner">
<span><p>This paper describes the SLT-CDT-UoS group's submission to the first Special
Task on Formality Control for Spoken Language Translation, part of the IWSLT
2022 Evaluation Campaign. Our efforts were split between two fronts: data
engineering and altering the objective function for best hypothesis selection.
We used language-independent methods to extract formal and informal sentence
pairs from the provided corpora; using English as a pivot language, we
propagated formality annotations to languages treated as zero-shot in the task;
we also further improved formality controlling with a hypothesis re-ranking
approach. On the test sets for English-to-German and English-to-Spanish, we
achieved an average accuracy of .935 within the constrained setting and .995
within unconstrained setting. In a zero-shot setting for English-to-Russian and
English-to-Italian, we scored average accuracy of .590 for constrained setting
and .659 for unconstrained.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Falsesum: Generating Document-level NLI Examples for Recognizing Factual Inconsistency in Summarization. (arXiv:2205.06009v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06009">
<div class="article-summary-box-inner">
<span><p>Neural abstractive summarization models are prone to generate summaries which
are factually inconsistent with their source documents. Previous work has
introduced the task of recognizing such factual inconsistency as a downstream
application of natural language inference (NLI). However, state-of-the-art NLI
models perform poorly in this context due to their inability to generalize to
the target task. In this work, we show that NLI models can be effective for
this task when the training data is augmented with high-quality task-oriented
examples. We introduce Falsesum, a data generation pipeline leveraging a
controllable text generation model to perturb human-annotated summaries,
introducing varying types of factual inconsistencies. Unlike previously
introduced document-level NLI datasets, our generated dataset contains examples
that are diverse and inconsistent yet plausible. We show that models trained on
a Falsesum-augmented NLI dataset improve the state-of-the-art performance
across four benchmarks for detecting factual inconsistency in summarization.
</p>
<p>The code to obtain the dataset is available online at
https://github.com/joshbambrick/Falsesum
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DTW at Qur'an QA 2022: Utilising Transfer Learning with Transformers for Question Answering in a Low-resource Domain. (arXiv:2205.06025v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06025">
<div class="article-summary-box-inner">
<span><p>The task of machine reading comprehension (MRC) is a useful benchmark to
evaluate the natural language understanding of machines. It has gained
popularity in the natural language processing (NLP) field mainly due to the
large number of datasets released for many languages. However, the research in
MRC has been understudied in several domains, including religious texts. The
goal of the Qur'an QA 2022 shared task is to fill this gap by producing
state-of-the-art question answering and reading comprehension research on
Qur'an. This paper describes the DTW entry to the Quran QA 2022 shared task.
Our methodology uses transfer learning to take advantage of available Arabic
MRC data. We further improve the results using various ensemble learning
strategies. Our approach provided a partial Reciprocal Rank (pRR) score of 0.49
on the test set, proving its strong performance on the task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sampling with Attribute-Related Information for Controlling Language Models. (arXiv:2205.06036v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06036">
<div class="article-summary-box-inner">
<span><p>The dominant approaches for controlling language models are based on
fine-tuning large language models or prompt engineering. However, these methods
often require condition-specific data or considerable hand-crafting. We propose
a new simple guided decoding method, Gamma Sampling, which does not require
complex engineering and any extra data. Gamma Sampling introduces
attribute-related information (provided by humans or language models
themselves) into the sampling process to guide language models to generate
texts with desired attributes. Experiments on controlling topics and sentiments
of generated text show Gamma Sampling to be superior in diversity, attribute
relevance and overall quality of generated samples while maintaining a fast
generation speed. In addition, we successfully applied Gamma Sampling to
control other attributes of language such as relatedness and repetition, which
further demonstrates the versatility and effectiveness of this method. Gamma
Sampling is now available in the python package samplings via import gamma
sampling from samplings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimRelUz: Similarity and Relatedness scores as a Semantic Evaluation dataset for Uzbek language. (arXiv:2205.06072v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06072">
<div class="article-summary-box-inner">
<span><p>Semantic relatedness between words is one of the core concepts in natural
language processing, thus making semantic evaluation an important task. In this
paper, we present a semantic model evaluation dataset: SimRelUz - a collection
of similarity and relatedness scores of word pairs for the low-resource Uzbek
language. The dataset consists of more than a thousand pairs of words carefully
selected based on their morphological features, occurrence frequency, semantic
relation, as well as annotated by eleven native Uzbek speakers from different
age groups and gender. We also paid attention to the problem of dealing with
rare words and out-of-vocabulary words to thoroughly evaluate the robustness of
semantic models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Asking for Knowledge: Training RL Agents to Query External Knowledge Using Language. (arXiv:2205.06111v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06111">
<div class="article-summary-box-inner">
<span><p>To solve difficult tasks, humans ask questions to acquire knowledge from
external sources. In contrast, classical reinforcement learning agents lack
such an ability and often resort to exploratory behavior. This is exacerbated
as few present-day environments support querying for knowledge. In order to
study how agents can be taught to query external knowledge via language, we
first introduce two new environments: the grid-world-based Q-BabyAI and the
text-based Q-TextWorld. In addition to physical interactions, an agent can
query an external knowledge source specialized for these environments to gather
information. Second, we propose the "Asking for Knowledge" (AFK) agent, which
learns to generate language commands to query for meaningful knowledge that
helps solve the tasks. AFK leverages a non-parametric memory, a pointer
mechanism and an episodic exploration bonus to tackle (1) a large query
language space, (2) irrelevant information, (3) delayed reward for making
meaningful queries. Extensive experiments demonstrate that the AFK agent
outperforms recent baselines on the challenging Q-BabyAI and Q-TextWorld
environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Findings of the Shared Task on Offensive Span Identification from Code-Mixed Tamil-English Comments. (arXiv:2205.06118v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06118">
<div class="article-summary-box-inner">
<span><p>Offensive content moderation is vital in social media platforms to support
healthy online discussions. However, their prevalence in codemixed Dravidian
languages is limited to classifying whole comments without identifying part of
it contributing to offensiveness. Such limitation is primarily due to the lack
of annotated data for offensive spans. Accordingly, in this shared task, we
provide Tamil-English code-mixed social comments with offensive spans. This
paper outlines the dataset so released, methods, and results of the submitted
systems
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot Code-Mixed Offensive Span Identification through Rationale Extraction. (arXiv:2205.06119v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06119">
<div class="article-summary-box-inner">
<span><p>This paper investigates the effectiveness of sentence-level transformers for
zero-shot offensive span identification on a code-mixed Tamil dataset. More
specifically, we evaluate rationale extraction methods of Local Interpretable
Model Agnostic Explanations (LIME) \cite{DBLP:conf/kdd/Ribeiro0G16} and
Integrated Gradients (IG) \cite{DBLP:conf/icml/SundararajanTY17} for adapting
transformer based offensive language classification models for zero-shot
offensive span identification. To this end, we find that LIME and IG show
baseline $F_{1}$ of 26.35\% and 44.83\%, respectively. Besides, we study the
effect of data set size and training process on the overall accuracy of span
identification. As a result, we find both LIME and IG to show significant
improvement with Masked Data Augmentation and Multilabel Training, with $F_{1}$
of 50.23\% and 47.38\% respectively. \textit{Disclaimer : This paper contains
examples that may be considered profane, vulgar, or offensive. The examples do
not represent the views of the authors or their employers/graduate schools
towards any person(s), group(s), practice(s), or entity/entities. Instead they
are used to emphasize only the linguistic research challenges.}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Model, Multiple Modalities: A Sparsely Activated Approach for Text, Sound, Image, Video and Code. (arXiv:2205.06126v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06126">
<div class="article-summary-box-inner">
<span><p>People perceive the world with multiple senses (e.g., through hearing sounds,
reading words and seeing objects). However, most existing AI systems only
process an individual modality. This paper presents an approach that excels at
handling multiple modalities of information with a single model. In our
"{SkillNet}" model, different parts of the parameters are specialized for
processing different modalities. Unlike traditional dense models that always
activate all the model parameters, our model sparsely activates parts of the
parameters whose skills are relevant to the task. Such model design enables
SkillNet to learn skills in a more interpretable way. We develop our model for
five modalities including text, image, sound, video and code. Results show
that, SkillNet performs comparably to five modality-specific fine-tuned models.
Moreover, our model supports self-supervised pretraining with the same sparsely
activated way, resulting in better initialized parameters for different
modalities. We find that pretraining significantly improves the performance of
SkillNet on five modalities, on par with or even better than baselines with
modality-specific pretraining. On the task of Chinese text-to-image retrieval,
our final system achieves higher accuracy than existing leading systems
including Wukong{ViT-B} and Wenlan 2.0 while using less number of activated
parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models. (arXiv:2205.06130v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06130">
<div class="article-summary-box-inner">
<span><p>Massively Multilingual Transformer based Language Models have been observed
to be surprisingly effective on zero-shot transfer across languages, though the
performance varies from language to language depending on the pivot language(s)
used for fine-tuning. In this work, we build upon some of the existing
techniques for predicting the zero-shot performance on a task, by modeling it
as a multi-task learning problem. We jointly train predictive models for
different tasks which helps us build more accurate predictors for tasks where
we have test data in very few languages to measure the actual performance of
the model. Our approach also lends us the ability to perform a much more robust
feature selection and identify a common set of features that influence
zero-shot performance across a variety of tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fair NLP Models with Differentially Private Text Encoders. (arXiv:2205.06135v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06135">
<div class="article-summary-box-inner">
<span><p>Encoded text representations often capture sensitive attributes about
individuals (e.g., race or gender), which raise privacy concerns and can make
downstream models unfair to certain groups. In this work, we propose FEDERATE,
an approach that combines ideas from differential privacy and adversarial
training to learn private text representations which also induces fairer
models. We empirically evaluate the trade-off between the privacy of the
representations and the fairness and accuracy of the downstream model on four
NLP datasets. Our results show that FEDERATE consistently improves upon
previous methods, and thus suggest that privacy and fairness can positively
reinforce each other.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is the Computation of Abstract Sameness Relations Human-Like in Neural Language Models?. (arXiv:2205.06149v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06149">
<div class="article-summary-box-inner">
<span><p>In recent years, deep neural language models have made strong progress in
various NLP tasks. This work explores one facet of the question whether
state-of-the-art NLP models exhibit elementary mechanisms known from human
cognition. The exploration is focused on a relatively primitive mechanism for
which there is a lot of evidence from various psycholinguistic experiments with
infants. The computation of "abstract sameness relations" is assumed to play an
important role in human language acquisition and processing, especially in
learning more complex grammar rules. In order to investigate this mechanism in
BERT and other pre-trained language models (PLMs), the experiment designs from
studies with infants were taken as the starting point. On this basis, we
designed experimental settings in which each element from the original studies
was mapped to a component of language models. Even though the task in our
experiments was relatively simple, the results suggest that the cognitive
faculty of computing abstract sameness relations is stronger in infants than in
all investigated PLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TreeMix: Compositional Constituency-based Data Augmentation for Natural Language Understanding. (arXiv:2205.06153v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06153">
<div class="article-summary-box-inner">
<span><p>Data augmentation is an effective approach to tackle over-fitting. Many
previous works have proposed different data augmentations strategies for NLP,
such as noise injection, word replacement, back-translation etc. Though
effective, they missed one important characteristic of
language--compositionality, meaning of a complex expression is built from its
sub-parts. Motivated by this, we propose a compositional data augmentation
approach for natural language understanding called TreeMix. Specifically,
TreeMix leverages constituency parsing tree to decompose sentences into
constituent sub-structures and the Mixup data augmentation technique to
recombine them to generate new sentences. Compared with previous approaches,
TreeMix introduces greater diversity to the samples generated and encourages
models to learn compositionality of NLP data. Extensive experiments on text
classification and SCAN demonstrate that TreeMix outperforms current
state-of-the-art data augmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Prefix-Tuning for Generative Template-based Event Extraction. (arXiv:2205.06166v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06166">
<div class="article-summary-box-inner">
<span><p>We consider event extraction in a generative manner with template-based
conditional generation. Although there is a rising trend of casting the task of
event extraction as a sequence generation problem with prompts, these
generation-based methods have two significant challenges, including using
suboptimal prompts and static event type information. In this paper, we propose
a generative template-based event extraction method with dynamic prefix
(GTEE-DynPref) by integrating context information with type-specific prefixes
to learn a context-specific prefix for each context. Experimental results show
that our model achieves competitive results with the state-of-the-art
classification-based model OneIE on ACE 2005 and achieves the best performances
on ERE. Additionally, our model is proven to be portable to new types of events
effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using dependency parsing for few-shot learning in distributional semantics. (arXiv:2205.06168v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06168">
<div class="article-summary-box-inner">
<span><p>In this work, we explore the novel idea of employing dependency parsing
information in the context of few-shot learning, the task of learning the
meaning of a rare word based on a limited amount of context sentences. Firstly,
we use dependency-based word embedding models as background spaces for few-shot
learning. Secondly, we introduce two few-shot learning methods which enhance
the additive baseline model by using dependencies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Generalist Agent. (arXiv:2205.06175v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06175">
<div class="article-summary-box-inner">
<span><p>Inspired by progress in large-scale language modeling, we apply a similar
approach towards building a single generalist agent beyond the realm of text
outputs. The agent, which we refer to as Gato, works as a multi-modal,
multi-task, multi-embodiment generalist policy. The same network with the same
weights can play Atari, caption images, chat, stack blocks with a real robot
arm and much more, deciding based on its context whether to output text, joint
torques, button presses, or other tokens. In this report we describe the model
and the data, and document the current capabilities of Gato.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Meta Learning for Low Resource Speech Recognition. (arXiv:2205.06182v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06182">
<div class="article-summary-box-inner">
<span><p>We propose a new meta learning based framework for low resource speech
recognition that improves the previous model agnostic meta learning (MAML)
approach. The MAML is a simple yet powerful meta learning approach. However,
the MAML presents some core deficiencies such as training instabilities and
slower convergence speed. To address these issues, we adopt multi-step loss
(MSL). The MSL aims to calculate losses at every step of the inner loop of MAML
and then combines them with a weighted importance vector. The importance vector
ensures that the loss at the last step has more importance than the previous
steps. Our empirical evaluation shows that MSL significantly improves the
stability of the training procedure and it thus also improves the accuracy of
the overall system. Our proposed system outperforms MAML based low resource ASR
system on various languages in terms of character error rates and stable
training behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Human Psychometric Properties Using Computational Language Models. (arXiv:2205.06203v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06203">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models (LMs) continue to achieve state-of-the-art
performance on natural language processing (NLP) benchmarks, including tasks
designed to mimic human-inspired "commonsense" competencies. To better
understand the degree to which LMs can be said to have certain linguistic
reasoning skills, researchers are beginning to adapt the tools and concepts
from psychometrics. But to what extent can benefits flow in the other
direction? In other words, can LMs be of use in predicting the psychometric
properties of test items, when those items are given to human participants? If
so, the benefit for psychometric practitioners is enormous, as it can reduce
the need for multiple rounds of empirical testing. We gather responses from
numerous human participants and LMs (transformer- and non-transformer-based) on
a broad diagnostic test of linguistic competencies. We then use the human
responses to calculate standard psychometric properties of the items in the
diagnostic test, using the human responses and the LM responses separately. We
then determine how well these two sets of predictions correlate. We find that
transformer-based LMs predict the human psychometric data consistently well
across most categories, suggesting that they can be used to gather human-like
psychometric data without the need for extensive human trials.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CiteSum: Citation Text-guided Scientific Extreme Summarization and Low-resource Domain Adaptation. (arXiv:2205.06207v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06207">
<div class="article-summary-box-inner">
<span><p>Scientific extreme summarization (TLDR) aims to form ultra-short summaries of
scientific papers. Previous efforts on curating scientific TLDR datasets failed
to scale up due to the heavy human annotation and domain expertise required. In
this paper, we propose a simple yet effective approach to automatically
extracting TLDR summaries for scientific papers from their citation texts.
Based on the proposed approach, we create a new benchmark CiteSum without human
annotation, which is around 30 times larger than the previous human-curated
dataset SciTLDR. We conduct a comprehensive analysis of CiteSum, examining its
data characteristics and establishing strong baselines. We further demonstrate
the usefulness of CiteSum by adapting models pre-trained on CiteSum (named
CITES) to new tasks and domains with limited supervision. For scientific
extreme summarization, CITES outperforms most fully-supervised methods on
SciTLDR without any fine-tuning and obtains state-of-the-art results with only
128 examples. For news extreme summarization, CITES achieves significant gains
on XSum over its base model (not pre-trained on CiteSum), e.g., +7.2 ROUGE-1
zero-shot performance and state-of-the-art few-shot performance. For news
headline generation, CITES performs the best among unsupervised and zero-shot
methods on Gigaword.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What's in a Caption? Dataset-Specific Linguistic Diversity and Its Effect on Visual Description Models and Metrics. (arXiv:2205.06253v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06253">
<div class="article-summary-box-inner">
<span><p>While there have been significant gains in the field of automated video
description, the generalization performance of automated description models to
novel domains remains a major barrier to using these systems in the real world.
Most visual description methods are known to capture and exploit patterns in
the training data leading to evaluation metric increases, but what are those
patterns? In this work, we examine several popular visual description datasets,
and capture, analyze, and understand the dataset-specific linguistic patterns
that models exploit but do not generalize to new domains. At the token level,
sample level, and dataset level, we find that caption diversity is a major
driving factor behind the generation of generic and uninformative captions. We
further show that state-of-the-art models even outperform held-out ground truth
captions on modern metrics, and that this effect is an artifact of linguistic
diversity in datasets. Understanding this linguistic diversity is key to
building strong captioning models, we recommend several methods and approaches
for maintaining diversity in the collection of new data, and dealing with the
consequences of limited diversity when using current models and metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FETA: A Benchmark for Few-Sample Task Transfer in Open-Domain Dialogue. (arXiv:2205.06262v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06262">
<div class="article-summary-box-inner">
<span><p>Task transfer, transferring knowledge contained in related tasks, holds the
promise of reducing the quantity of labeled data required to fine-tune language
models. Dialogue understanding encompasses many diverse tasks, yet task
transfer has not been thoroughly studied in conversational AI. This work
explores conversational task transfer by introducing FETA: a benchmark for
few-sample task transfer in open-domain dialogue. FETA contains two underlying
sets of conversations upon which there are 10 and 7 tasks annotated, enabling
the study of intra-dataset task transfer; task transfer without domain
adaptation. We utilize three popular language models and three learning
algorithms to analyze the transferability between 132 source-target task pairs
and create a baseline for future work. We run experiments in the single- and
multi-source settings and report valuable findings, e.g., most performance
trends are model-specific, and span extraction and multiple-choice tasks
benefit the most from task transfer. In addition to task transfer, FETA can be
a valuable resource for future research into the efficiency and
generalizability of pre-training datasets and model architectures, as well as
for learning settings such as continual and multitask learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lifting the Curse of Multilinguality by Pre-training Modular Transformers. (arXiv:2205.06266v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06266">
<div class="article-summary-box-inner">
<span><p>Multilingual pre-trained models are known to suffer from the curse of
multilinguality, which causes per-language performance to drop as they cover
more languages. We address this issue by introducing language-specific modules,
which allows us to grow the total capacity of the model, while keeping the
total number of trainable parameters per language constant. In contrast with
prior work that learns language-specific components post-hoc, we pre-train the
modules of our Cross-lingual Modular (X-Mod) models from the start. Our
experiments on natural language inference, named entity recognition and
question answering show that our approach not only mitigates the negative
interference between languages, but also enables positive transfer, resulting
in improved monolingual and cross-lingual performance. Furthermore, our
approach enables adding languages post-hoc with no measurable drop in
performance, no longer limiting the model usage to the set of pre-trained
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The jsRealB Text Realizer: Organization and Use Cases -- Revised version. (arXiv:2012.15425v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15425">
<div class="article-summary-box-inner">
<span><p>This paper describes the design principles behind jsRealB (Version 4.0), a
surface realizer written JavaScript for English or French sentences from a
specification inspired by the constituent syntax formalism but for which a
dependency-based input notation is also available. jsRealB can be used either
within a web page or as a node.js module. We show that the seemingly simple
process of text realization involves many interesting implementation challenges
in order to take into account the specifics of each language. jsRealB has a
large coverage of English and French and has been used to develop realistic
data-to-text applications and to reproduce existing literary texts and
sentences from Universal Dependency annotations. Its source code and that of
its applications are available on GitHub. The port of this approach to Python
(pyrealb) is also presented.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">So Cloze yet so Far: N400 Amplitude is Better Predicted by Distributional Information than Human Predictability Judgements. (arXiv:2109.01226v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01226">
<div class="article-summary-box-inner">
<span><p>More predictable words are easier to process - they are read faster and
elicit smaller neural signals associated with processing difficulty, most
notably, the N400 component of the event-related brain potential. Thus, it has
been argued that prediction of upcoming words is a key component of language
comprehension, and that studying the amplitude of the N400 is a valuable way to
investigate the predictions we make. In this study, we investigate whether the
linguistic predictions of computational language models or humans better
reflect the way in which natural language stimuli modulate the amplitude of the
N400. One important difference in the linguistic predictions of humans versus
computational language models is that while language models base their
predictions exclusively on the preceding linguistic context, humans may rely on
other factors. We find that the predictions of three top-of-the-line
contemporary language models - GPT-3, RoBERTa, and ALBERT - match the N400 more
closely than human predictions. This suggests that the predictive processes
underlying the N400 may be more sensitive to the surface-level statistics of
language than previously thought.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Does Knowledge Graph Embedding Extrapolate to Unseen Data: A Semantic Evidence View. (arXiv:2109.11800v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11800">
<div class="article-summary-box-inner">
<span><p>Knowledge Graph Embedding (KGE) aims to learn representations for entities
and relations. Most KGE models have gained great success, especially on
extrapolation scenarios. Specifically, given an unseen triple (h, r, t), a
trained model can still correctly predict t from (h, r, ?), or h from (?, r,
t), such extrapolation ability is impressive. However, most existing KGE works
focus on the design of delicate triple modeling function, which mainly tells us
how to measure the plausibility of observed triples, but offers limited
explanation of why the methods can extrapolate to unseen data, and what are the
important factors to help KGE extrapolate. Therefore in this work, we attempt
to study the KGE extrapolation of two problems: 1. How does KGE extrapolate to
unseen data? 2. How to design the KGE model with better extrapolation ability?
For the problem 1, we first discuss the impact factors for extrapolation and
from relation, entity and triple level respectively, propose three Semantic
Evidences (SEs), which can be observed from train set and provide important
semantic information for extrapolation. Then we verify the effectiveness of SEs
through extensive experiments on several typical KGE methods. For the problem
2, to make better use of the three levels of SE, we propose a novel GNN-based
KGE model, called Semantic Evidence aware Graph Neural Network (SE-GNN). In
SE-GNN, each level of SE is modeled explicitly by the corresponding neighbor
pattern, and merged sufficiently by the multi-layer aggregation, which
contributes to obtaining more extrapolative knowledge representation. Finally,
through extensive experiments on FB15k-237 and WN18RR datasets, we show that
SE-GNN achieves state-of-the-art performance on Knowledge Graph Completion task
and performs a better extrapolation ability. Our code is available at
https://github.com/renli1024/SE-GNN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Universal Intrinsic Task Subspace via Prompt Tuning. (arXiv:2110.07867v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07867">
<div class="article-summary-box-inner">
<span><p>Why can pre-trained language models (PLMs) learn universal representations
and effectively adapt to broad NLP tasks differing a lot superficially? In this
work, we empirically find evidence indicating that the adaptations of PLMs to
various few-shot tasks can be reparameterized as optimizing only a few free
parameters in a unified low-dimensional intrinsic task subspace, which may help
us understand why PLMs could easily adapt to various NLP tasks with small-scale
data. To find such a subspace and examine its universality, we propose an
analysis pipeline called intrinsic prompt tuning (IPT). Specifically, we resort
to the recent success of prompt tuning and decompose the soft prompts of
multiple NLP tasks into the same low-dimensional nonlinear subspace, then we
learn to adapt the PLM to unseen data or tasks by only tuning parameters in
this subspace. In the experiments, we study diverse few-shot NLP tasks and
surprisingly find that in a 250-dimensional subspace found with 100 tasks, by
only tuning 250 free parameters, we can recover 97% and 83% of the full prompt
tuning performance for 100 seen tasks (using different training data) and 20
unseen tasks, respectively, showing great generalization ability of the found
intrinsic task subspace. Besides being an analysis tool, IPT could further
bring practical benefits, such as improving the prompt tuning stability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COPA-SSE: Semi-structured Explanations for Commonsense Reasoning. (arXiv:2201.06777v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06777">
<div class="article-summary-box-inner">
<span><p>We present Semi-Structured Explanations for COPA (COPA-SSE), a new
crowdsourced dataset of 9,747 semi-structured, English common sense
explanations for Choice of Plausible Alternatives (COPA) questions. The
explanations are formatted as a set of triple-like common sense statements with
ConceptNet relations but freely written concepts. This semi-structured format
strikes a balance between the high quality but low coverage of structured data
and the lower quality but high coverage of free-form crowdsourcing. Each
explanation also includes a set of human-given quality ratings. With their
familiar format, the explanations are geared towards commonsense reasoners
operating on knowledge graphs and serve as a starting point for ongoing work on
improving such systems. The dataset is available at
https://github.com/a-brassard/copa-sse.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmenting Pre-trained Language Models with QA-Memory for Open-Domain Question Answering. (arXiv:2204.04581v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04581">
<div class="article-summary-box-inner">
<span><p>Retrieval augmented language models have recently become the standard for
knowledge intensive tasks. Rather than relying purely on latent semantics
within the parameters of large neural models, these methods enlist a
semi-parametric memory to encode an index of knowledge for the model to
retrieve over. Most prior work has employed text passages as the unit of
knowledge, which has high coverage at the cost of interpretability,
controllability, and efficiency. The opposite properties arise in other methods
which have instead relied on knowledge base (KB) facts. At the same time, more
recent work has demonstrated the effectiveness of storing and retrieving from
an index of Q-A pairs derived from text \citep{lewis2021paq}. This approach
yields a high coverage knowledge representation that maintains KB-like
properties due to its representations being more atomic units of information.
In this work we push this line of research further by proposing a
question-answer augmented encoder-decoder model and accompanying pretraining
strategy. This yields an end-to-end system that not only outperforms prior QA
retrieval methods on single-hop QA tasks but also enables compositional
reasoning, as demonstrated by strong performance on two multi-hop QA datasets.
Together, these methods improve the ability to interpret and control the model
while narrowing the performance gap with passage retrieval systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective. (arXiv:2205.04733v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04733">
<div class="article-summary-box-inner">
<span><p>Neural retrievers based on dense representations combined with Approximate
Nearest Neighbors search have recently received a lot of attention, owing their
success to distillation and/or better sampling of examples for training --
while still relying on the same backbone architecture. In the meantime, sparse
representation learning fueled by traditional inverted indexing techniques has
seen a growing interest, inheriting from desirable IR priors such as explicit
lexical matching. While some architectural variants have been proposed, a
lesser effort has been put in the training of such models. In this work, we
build on SPLADE -- a sparse expansion-based retriever -- and show to which
extent it is able to benefit from the same training improvements as dense
models, by studying the effect of distillation, hard-negative mining as well as
the Pre-trained Language Model initialization. We furthermore study the link
between effectiveness and efficiency, on in-domain and zero-shot settings,
leading to state-of-the-art results in both scenarios for sufficiently
expressive models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Climate Awareness in NLP Research. (arXiv:2205.05071v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05071">
<div class="article-summary-box-inner">
<span><p>The climate impact of AI, and NLP research in particular, has become a
serious issue given the enormous amount of energy that is increasingly being
used for training and running computational models. Consequently, increasing
focus is placed on efficient NLP. However, this important initiative lacks
simple guidelines that would allow for systematic climate reporting of NLP
research. We argue that this deficiency is one of the reasons why very few
publications in NLP report key figures that would allow a more thorough
examination of environmental impact. As a remedy, we propose a climate
performance model card with the primary purpose of being practically usable
with only limited information about experiments and the underlying computer
hardware. We describe why this step is essential to increase awareness about
the environmental impact of NLP research and, thereby, paving the way for more
thorough discussions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers. (arXiv:2205.05055v2 [cs.AI] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05055">
<div class="article-summary-box-inner">
<span><p>Large transformer-based language models are able to perform few-shot learning
(also known as in-context learning), without having been explicitly trained for
it. We hypothesized that specific distributional properties of natural language
might drive this emergent phenomenon, as these characteristics might lead to a
kind of interpolation between few-shot meta-training (designed to elicit rapid
few-shot learning) and standard supervised training (designed to elicit gradual
in-weights learning). We also hypothesized that these distributional properties
could lead to emergent few-shot learning in domains outside of language.
Inspired by this idea, we ran a series of experiments on a standard image-based
few-shot dataset. We discovered that a number of data properties did indeed
promote the emergence of few-shot learning in transformer models. All of these
properties are present in natural language -- burstiness, long-tailedness, and
many-to-one or one-to-many label mappings. The data influenced whether models
were biased towards either few-shot learning vs. memorizing information in
their weights; models could generally perform well at only one or the other.
However, we discovered that an additional distributional property could allow
the two capabilities to co-exist in the same model -- a skewed, Zipfian
distribution over classes -- which occurs in language as well. Notably,
training data that could elicit few-shot learning in transformers were unable
to elicit few-shot learning in recurrent models. In sum, we find that few-shot
learning emerges only from applying the right architecture to the right data
distribution; neither component is sufficient on its own.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A Closer Look at Audio-Visual Multi-Person Speech Recognition and Active Speaker Selection. (arXiv:2205.05684v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05684">
<div class="article-summary-box-inner">
<span><p>Audio-visual automatic speech recognition is a promising approach to robust
ASR under noisy conditions. However, up until recently it had been
traditionally studied in isolation assuming the video of a single speaking face
matches the audio, and selecting the active speaker at inference time when
multiple people are on screen was put aside as a separate problem. As an
alternative, recent work has proposed to address the two problems
simultaneously with an attention mechanism, baking the speaker selection
problem directly into a fully differentiable model. One interesting finding was
that the attention indirectly learns the association between the audio and the
speaking face even though this correspondence is never explicitly provided at
training time. In the present work we further investigate this connection and
examine the interplay between the two problems. With experiments involving over
50 thousand hours of public YouTube videos as training data, we first evaluate
the accuracy of the attention layer on an active speaker selection task.
Secondly, we show under closer scrutiny that an end-to-end model performs at
least as well as a considerably larger two-step system that utilizes a hard
decision boundary under various noise conditions and number of parallel face
tracks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Class 3D Object Detection with Single-Class Supervision. (arXiv:2205.05703v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05703">
<div class="article-summary-box-inner">
<span><p>While multi-class 3D detectors are needed in many robotics applications,
training them with fully labeled datasets can be expensive in labeling cost. An
alternative approach is to have targeted single-class labels on disjoint data
samples. In this paper, we are interested in training a multi-class 3D object
detection model, while using these single-class labeled data. We begin by
detailing the unique stance of our "Single-Class Supervision" (SCS) setting
with respect to related concepts such as partial supervision and semi
supervision. Then, based on the case study of training the multi-class version
of Range Sparse Net (RSN), we adapt a spectrum of algorithms -- from supervised
learning to pseudo-labeling -- to fully exploit the properties of our SCS
setting, and perform extensive ablation studies to identify the most effective
algorithm and practice. Empirical experiments on the Waymo Open Dataset show
that proper training under SCS can approach or match full supervision training
while saving labeling costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse Video Generation from a Single Video. (arXiv:2205.05725v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05725">
<div class="article-summary-box-inner">
<span><p>GANs are able to perform generation and manipulation tasks, trained on a
single video. However, these single video GANs require unreasonable amount of
time to train on a single video, rendering them almost impractical. In this
paper we question the necessity of a GAN for generation from a single video,
and introduce a non-parametric baseline for a variety of generation and
manipulation tasks. We revive classical space-time patches-nearest-neighbors
approaches and adapt them to a scalable unconditional generative model, without
any learning. This simple baseline surprisingly outperforms single-video GANs
in visual quality and realism (confirmed by quantitative and qualitative
evaluations), and is disproportionately faster (runtime reduced from several
days to seconds). Our approach is easily scaled to Full-HD videos. We also use
the same framework to demonstrate video analogies and spatio-temporal
retargeting. These observations show that classical approaches significantly
outperform heavy deep learning machinery for these tasks. This sets a new
baseline for single-video generation and manipulation tasks, and no less
important -- makes diverse generation from a single video practically possible
for the first time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computational behavior recognition in child and adolescent psychiatry: A statistical and machine learning analysis plan. (arXiv:2205.05737v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05737">
<div class="article-summary-box-inner">
<span><p>Motivation: Behavioral observations are an important resource in the study
and evaluation of psychological phenomena, but it is costly, time-consuming,
and susceptible to bias. Thus, we aim to automate coding of human behavior for
use in psychotherapy and research with the help of artificial intelligence (AI)
tools. Here, we present an analysis plan. Methods: Videos of a gold-standard
semi-structured diagnostic interview of 25 youth with obsessive-compulsive
disorder (OCD) and 12 youth without a psychiatric diagnosis (no-OCD) will be
analyzed. Youth were between 8 and 17 years old. Features from the videos will
be extracted and used to compute ratings of behavior, which will be compared to
ratings of behavior produced by mental health professionals trained to use a
specific behavioral coding manual. We will test the effect of OCD diagnosis on
the computationally-derived behavior ratings using multivariate analysis of
variance (MANOVA). Using the generated features, a binary classification model
will be built and used to classify OCD/no-OCD classes. Discussion: Here, we
present a pre-defined plan for how data will be pre-processed, analyzed and
presented in the publication of results and their interpretation. A challenge
for the proposed study is that the AI approach will attempt to derive
behavioral ratings based solely on vision, whereas humans use visual,
paralinguistic and linguistic cues to rate behavior. Another challenge will be
using machine learning models for body and facial movement detection trained
primarily on adults and not on children. If the AI tools show promising
results, this pre-registered analysis plan may help reduce interpretation bias.
Trial registration: ClinicalTrials.gov - H-18010607
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DISARM: Detecting the Victims Targeted by Harmful Memes. (arXiv:2205.05738v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05738">
<div class="article-summary-box-inner">
<span><p>Internet memes have emerged as an increasingly popular means of communication
on the Web. Although typically intended to elicit humour, they have been
increasingly used to spread hatred, trolling, and cyberbullying, as well as to
target specific individuals, communities, or society on political,
socio-cultural, and psychological grounds. While previous work has focused on
detecting harmful, hateful, and offensive memes, identifying whom they attack
remains a challenging and underexplored area. Here we aim to bridge this gap.
In particular, we create a dataset where we annotate each meme with its
victim(s) such as the name of the targeted person(s), organization(s), and
community(ies). We then propose DISARM (Detecting vIctimS targeted by hARmful
Memes), a framework that uses named entity recognition and person
identification to detect all entities a meme is referring to, and then,
incorporates a novel contextualized multimodal deep neural network to classify
whether the meme intends to harm these entities. We perform several systematic
experiments on three test setups, corresponding to entities that are (a) all
seen while training, (b) not seen as a harmful target on training, and (c) not
seen at all on training. The evaluation results show that DISARM significantly
outperforms ten unimodal and multimodal systems. Finally, we show that DISARM
is interpretable and comparatively more generalizable and that it can reduce
the relative error rate for harmful target identification by up to 9 points
absolute over several strong multimodal rivals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Retrieve Videos by Asking Questions. (arXiv:2205.05739v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05739">
<div class="article-summary-box-inner">
<span><p>The majority of traditional text-to-video retrieval systems operate in static
environments, i.e., there is no interaction between the user and the agent
beyond the initial textual query provided by the user. This can be suboptimal
if the initial query has ambiguities, which would lead to many falsely
retrieved videos. To overcome this limitation, we propose a novel framework for
Video Retrieval using Dialog (ViReD), which enables the user to interact with
an AI agent via multiple rounds of dialog. The key contribution of our
framework is a novel multimodal question generator that learns to ask questions
that maximize the subsequent video retrieval performance. Our multimodal
question generator uses (i) the video candidates retrieved during the last
round of interaction with the user and (ii) the text-based dialog history
documenting all previous interactions, to generate questions that incorporate
both visual and linguistic cues relevant to video retrieval. Furthermore, to
generate maximally informative questions, we propose an Information-Guided
Supervision (IGS), which guides the question generator to ask questions that
would boost subsequent video retrieval accuracy. We validate the effectiveness
of our interactive ViReD framework on the AVSD dataset, showing that our
interactive method performs significantly better than traditional
non-interactive video retrieval systems. Furthermore, we also demonstrate that
our proposed approach also generalizes to the real-world settings that involve
interactions with real humans, thus, demonstrating the robustness and
generality of our framework
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface Representation for Point Clouds. (arXiv:2205.05740v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05740">
<div class="article-summary-box-inner">
<span><p>Most prior work represents the shapes of point clouds by coordinates.
However, it is insufficient to describe the local geometry directly. In this
paper, we present \textbf{RepSurf} (representative surfaces), a novel
representation of point clouds to \textbf{explicitly} depict the very local
structure. We explore two variants of RepSurf, Triangular RepSurf and Umbrella
RepSurf inspired by triangle meshes and umbrella curvature in computer
graphics. We compute the representations of RepSurf by predefined geometric
priors after surface reconstruction. RepSurf can be a plug-and-play module for
most point cloud models thanks to its free collaboration with irregular points.
Based on a simple baseline of PointNet++ (SSG version), Umbrella RepSurf
surpasses the previous state-of-the-art by a large margin for classification,
segmentation and detection on various benchmarks in terms of performance and
efficiency. With an increase of around \textbf{0.008M} number of parameters,
\textbf{0.04G} FLOPs, and \textbf{1.12ms} inference time, our method achieves
\textbf{94.7\%} (+0.5\%) on ModelNet40, and \textbf{84.6\%} (+1.8\%) on
ScanObjectNN for classification, while \textbf{74.3\%} (+0.8\%) mIoU on S3DIS
6-fold, and \textbf{70.0\%} (+1.6\%) mIoU on ScanNet for segmentation. For
detection, previous state-of-the-art detector with our RepSurf obtains
\textbf{71.2\%} (+2.1\%) mAP$\mathit{_{25}}$, \textbf{54.8\%} (+2.0\%)
mAP$\mathit{_{50}}$ on ScanNetV2, and \textbf{64.9\%} (+1.9\%)
mAP$\mathit{_{25}}$, \textbf{47.7\%} (+2.5\%) mAP$\mathit{_{50}}$ on SUN RGB-D.
Our lightweight Triangular RepSurf performs its excellence on these benchmarks
as well. The code is publicly available at
\url{https://github.com/hancyran/RepSurf}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEWS: Real-time Social Media Manipulation Detection and Analysis. (arXiv:2205.05783v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05783">
<div class="article-summary-box-inner">
<span><p>This article presents a beta-version of MEWS (Misinformation Early Warning
System). It describes the various aspects of the ingestion, manipulation
detection, and graphing algorithms employed to determine--in near
real-time--the relationships between social media images as they emerge and
spread on social media platforms. By combining these various technologies into
a single processing pipeline, MEWS can identify manipulated media items as they
arise and identify when these particular items begin trending on individual
social media platforms or even across multiple platforms. The emergence of a
novel manipulation followed by rapid diffusion of the manipulated content
suggests a disinformation campaign.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous wavelet transform of multiview images using wavelets based on voxel patterns. (arXiv:2205.05823v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05823">
<div class="article-summary-box-inner">
<span><p>We propose the multiview wavelets based on voxel patterns of autostereoscopic
multiview displays. Direct and inverse continuous wavelet transforms of binary
and gray-scale images were performed. The input to the inverse wavelet
transform was the array of wavelet coefficients of the direct transform. A
restored image reproduces the structure of the multiview image correctly. Also,
we modified the dimension of the parallax and the depth of 3D images. The
restored and modified images were displayed in 3D using lenticular plates. In
each case, the visual 3D picture corresponds to the applied modifications. The
results can be applied to the autostereoscopic 3D displays.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparseloop: An Analytical Approach To Sparse Tensor Accelerator Modeling. (arXiv:2205.05826v1 [cs.AR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05826">
<div class="article-summary-box-inner">
<span><p>In recent years, many accelerators have been proposed to efficiently process
sparse tensor algebra applications (e.g., sparse neural networks). However,
these proposals are single points in a large and diverse design space. The lack
of systematic description and modeling support for these sparse tensor
accelerators impedes hardware designers from efficient and effective design
space exploration. This paper first presents a unified taxonomy to
systematically describe the diverse sparse tensor accelerator design space.
Based on the proposed taxonomy, it then introduces Sparseloop, the first fast,
accurate, and flexible analytical modeling framework to enable early-stage
evaluation and exploration of sparse tensor accelerators. Sparseloop
comprehends a large set of architecture specifications, including various
dataflows and sparse acceleration features (e.g., elimination of zero-based
compute). Using these specifications, Sparseloop evaluates a design's
processing speed and energy efficiency while accounting for data movement and
compute incurred by the employed dataflow as well as the savings and overhead
introduced by the sparse acceleration features using stochastic tensor density
models. Across representative accelerators and workloads, Sparseloop achieves
over 2000 times faster modeling speed than cycle-level simulations, maintains
relative performance trends, and achieves 0.1% to 8% average error. With a case
study, we demonstrate Sparseloop's ability to help reveal important insights
for designing sparse tensor accelerators (e.g., it is important to co-design
orthogonal design aspects).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-domain Few-shot Meta-learning Using Stacking. (arXiv:2205.05831v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05831">
<div class="article-summary-box-inner">
<span><p>Cross-domain few-shot meta-learning (CDFSML) addresses learning problems
where knowledge needs to be transferred from several source domains into an
instance-scarce target domain with an explicitly different input distribution.
Recently published CDFSML methods generally construct a "universal model" that
combines knowledge of multiple source domains into one backbone feature
extractor. This enables efficient inference but necessitates re-computation of
the backbone whenever a new source domain is added. Moreover, state-of-the-art
methods derive their universal model from a collection of backbones -- normally
one for each source domain -- and the backbones may be constrained to have the
same architecture as the universal model. We propose a CDFSML method that is
inspired by the classic stacking approach to meta learning. It imposes no
constraints on the backbones' architecture or feature shape and does not incur
the computational overhead of (re-)computing a universal model. Given a
target-domain task, it fine-tunes each backbone independently, uses
cross-validation to extract meta training data from the task's instance-scarce
support set, and learns a simple linear meta classifier from this data. We
evaluate our stacking approach on the well-known Meta-Dataset benchmark,
targeting image classification with convolutional neural networks, and show
that it often yields substantially higher accuracy than competing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Uncertainty for Deep Interpretable Classification and Weakly-Supervised Segmentation of Histology Images. (arXiv:2205.05841v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05841">
<div class="article-summary-box-inner">
<span><p>Trained using only image class label, deep weakly supervised methods allow
image classification and ROI segmentation for interpretability. Despite their
success on natural images, they face several challenges over histology data
where ROI are visually similar to background making models vulnerable to high
pixel-wise false positives. These methods lack mechanisms for modeling
explicitly non-discriminative regions which raises false-positive rates. We
propose novel regularization terms, which enable the model to seek both
non-discriminative and discriminative regions, while discouraging unbalanced
segmentations and using only image class label. Our method is composed of two
networks: a localizer that yields segmentation mask, followed by a classifier.
The training loss pushes the localizer to build a segmentation mask that holds
most discrimiantive regions while simultaneously modeling background regions.
Comprehensive experiments over two histology datasets showed the merits of our
method in reducing false positives and accurately segmenting ROI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bi-level Alignment for Cross-Domain Crowd Counting. (arXiv:2205.05844v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05844">
<div class="article-summary-box-inner">
<span><p>Recently, crowd density estimation has received increasing attention. The
main challenge for this task is to achieve high-quality manual annotations on a
large amount of training data. To avoid reliance on such annotations, previous
works apply unsupervised domain adaptation (UDA) techniques by transferring
knowledge learned from easily accessible synthetic data to real-world datasets.
However, current state-of-the-art methods either rely on external data for
training an auxiliary task or apply an expensive coarse-to-fine estimation. In
this work, we aim to develop a new adversarial learning based method, which is
simple and efficient to apply. To reduce the domain gap between the synthetic
and real data, we design a bi-level alignment framework (BLA) consisting of (1)
task-driven data alignment and (2) fine-grained feature alignment. In contrast
to previous domain augmentation methods, we introduce AutoML to search for an
optimal transform on source, which well serves for the downstream task. On the
other hand, we do fine-grained alignment for foreground and background
separately to alleviate the alignment difficulty. We evaluate our approach on
five real-world crowd counting benchmarks, where we outperform existing
approaches by a large margin. Also, our approach is simple, easy to implement
and efficient to apply. The code is publicly available at
https://github.com/Yankeegsj/BLA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AFFIRM: Affinity Fusion-based Framework for Iteratively Random Motion correction of multi-slice fetal brain MRI. (arXiv:2205.05851v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05851">
<div class="article-summary-box-inner">
<span><p>Multi-slice magnetic resonance images of the fetal brain are usually
contaminated by severe and arbitrary fetal and maternal motion. Hence, stable
and robust motion correction is necessary to reconstruct high-resolution 3D
fetal brain volume for clinical diagnosis and quantitative analysis. However,
the conventional registration-based correction has a limited capture range and
is insufficient for detecting relatively large motions. Here, we present a
novel Affinity Fusion-based Framework for Iteratively Random Motion (AFFIRM)
correction of the multi-slice fetal brain MRI. It learns the sequential motion
from multiple stacks of slices and integrates the features between 2D slices
and reconstructed 3D volume using affinity fusion, which resembles the
iterations between slice-to-volume registration and volumetric reconstruction
in the regular pipeline. The method accurately estimates the motion regardless
of brain orientations and outperforms other state-of-the-art learning-based
methods on the simulated motion-corrupted data, with a 48.4% reduction of mean
absolute error for rotation and 61.3% for displacement. We then incorporated
AFFIRM into the multi-resolution slice-to-volume registration and tested it on
the real-world fetal MRI scans at different gestation stages. The results
indicated that adding AFFIRM to the conventional pipeline improved the success
rate of fetal brain super-resolution reconstruction from 77.2% to 91.9%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity-aware and Motion-aware Transformers for Language-driven Action Localization in Videos. (arXiv:2205.05854v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05854">
<div class="article-summary-box-inner">
<span><p>Language-driven action localization in videos is a challenging task that
involves not only visual-linguistic matching but also action boundary
prediction. Recent progress has been achieved through aligning language query
to video segments, but estimating precise boundaries is still under-explored.
In this paper, we propose entity-aware and motion-aware Transformers that
progressively localizes actions in videos by first coarsely locating clips with
entity queries and then finely predicting exact boundaries in a shrunken
temporal region with motion queries. The entity-aware Transformer incorporates
the textual entities into visual representation learning via cross-modal and
cross-frame attentions to facilitate attending action-related video clips. The
motion-aware Transformer captures fine-grained motion changes at multiple
temporal scales via integrating long short-term memory into the self-attention
module to further improve the precision of action boundary prediction.
Extensive experiments on the Charades-STA and TACoS datasets demonstrate that
our method achieves better performance than existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S3E-GNN: Sparse Spatial Scene Embedding with Graph Neural Networks for Camera Relocalization. (arXiv:2205.05861v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05861">
<div class="article-summary-box-inner">
<span><p>Camera relocalization is the key component of simultaneous localization and
mapping (SLAM) systems. This paper proposes a learning-based approach, named
Sparse Spatial Scene Embedding with Graph Neural Networks (S3E-GNN), as an
end-to-end framework for efficient and robust camera relocalization. S3E-GNN
consists of two modules. In the encoding module, a trained S3E network encodes
RGB images into embedding codes to implicitly represent spatial and semantic
embedding code. With embedding codes and the associated poses obtained from a
SLAM system, each image is represented as a graph node in a pose graph. In the
GNN query module, the pose graph is transformed to form a embedding-aggregated
reference graph for camera relocalization. We collect various scene datasets in
the challenging environments to perform experiments. Our results demonstrate
that S3E-GNN method outperforms the traditional Bag-of-words (BoW) for camera
relocalization due to learning-based embedding and GNN powered scene matching
mechanism.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">View Synthesis with Sculpted Neural Points. (arXiv:2205.05869v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05869">
<div class="article-summary-box-inner">
<span><p>We address the task of view synthesis, which can be posed as recovering a
rendering function that renders new views from a set of existing images. In
many recent works such as NeRF, this rendering function is parameterized using
implicit neural representations of scene geometry. Implicit neural
representations have achieved impressive visual quality but have drawbacks in
computational efficiency. In this work, we propose a new approach that performs
view synthesis using point clouds. It is the first point-based method to
achieve better visual quality than NeRF while being more than 100x faster in
rendering speed. Our approach builds on existing works on differentiable
point-based rendering but introduces a novel technique we call "Sculpted Neural
Points (SNP)", which significantly improves the robustness to errors and holes
in the reconstructed point cloud. Experiments show that on the task of view
synthesis, our sculpting technique closes the gap between point-based and
implicit representation-based methods. Code is available at
https://github.com/princeton-vl/SNP and supplementary video at
https://youtu.be/dBwCQP9uNws.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distinction Maximization Loss: Efficiently Improving Classification Accuracy, Uncertainty Estimation, and Out-of-Distribution Detection Simply Replacing the Loss and Calibrating. (arXiv:2205.05874v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05874">
<div class="article-summary-box-inner">
<span><p>Building robust deterministic deep neural networks is still a challenge. On
the one hand, some approaches improve out-of-distribution detection at the cost
of reducing classification accuracy in some situations. On the other hand, some
methods simultaneously increase classification accuracy, out-of-distribution
detection, and uncertainty estimation, but reduce inference efficiency, in
addition to training the same model many times to tune hyperparameters. In this
paper, we propose training deterministic deep neural networks using our DisMax
loss, which works as a drop-in replacement for the commonly used SoftMax loss
(i.e., the combination of the linear output layer, the SoftMax activation, and
the cross-entropy loss). Starting from the IsoMax+ loss, we created novel
logits that are based on the distance to all prototypes rather than just the
one associated with the correct class. We also propose a novel way to augment
images to construct what we call fractional probability regularization.
Moreover, we propose a new score to perform out-of-distribution detection and a
fast way to calibrate the network after training. Our experiments show that
DisMax usually outperforms all current approaches simultaneously in
classification accuracy, uncertainty estimation, inference efficiency, and
out-of-distribution detection, avoiding hyperparameter tuning and repetitive
model training. The code to replace the SoftMax loss with the DisMax loss and
reproduce the results in this paper is available at
https://github.com/dlmacedo/distinction-maximization-loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Decomposition and Bilinear Pooling Network for Blind Night-Time Image Quality Evaluation. (arXiv:2205.05880v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05880">
<div class="article-summary-box-inner">
<span><p>Blind image quality assessment (BIQA), which aims to accurately predict the
image quality without any pristine reference information, has been highly
concerned in the past decades. Especially, with the help of deep neural
networks, great progress has been achieved so far. However, it remains less
investigated on BIQA for night-time images (NTIs) which usually suffer from
complicated authentic distortions such as reduced visibility, low contrast,
additive noises, and color distortions. These diverse authentic degradations
particularly challenges the design of effective deep neural network for blind
NTI quality evaluation (NTIQE). In this paper, we propose a novel deep
decomposition and bilinear pooling network (DDB-Net) to better address this
issue. The DDB-Net contains three modules, i.e., an image decomposition module,
a feature encoding module, and a bilinear pooling module. The image
decomposition module is inspired by the Retinex theory and involves decoupling
the input NTI into an illumination layer component responsible for illumination
information and a reflectance layer component responsible for content
information. Then, the feature encoding module involves learning multi-scale
feature representations of degradations that are rooted in the two decoupled
components separately. Finally, by modeling illumination-related and
content-related degradations as two-factor variations, the two multi-scale
feature sets are bilinearly pooled and concatenated together to form a unified
representation for quality prediction. The superiority of the proposed DDB-Net
is well validated by extensive experiments on two publicly available night-time
image databases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-Supervised Action Detection Guided by Audio Narration. (arXiv:2205.05895v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05895">
<div class="article-summary-box-inner">
<span><p>Videos are more well-organized curated data sources for visual concept
learning than images. Unlike the 2-dimensional images which only involve the
spatial information, the additional temporal dimension bridges and synchronizes
multiple modalities. However, in most video detection benchmarks, these
additional modalities are not fully utilized. For example, EPIC Kitchens is the
largest dataset in first-person (egocentric) vision, yet it still relies on
crowdsourced information to refine the action boundaries to provide
instance-level action annotations.
</p>
<p>We explored how to eliminate the expensive annotations in video detection
data which provide refined boundaries. We propose a model to learn from the
narration supervision and utilize multimodal features, including RGB, motion
flow, and ambient sound. Our model learns to attend to the frames related to
the narration label while suppressing the irrelevant frames from being used.
Our experiments show that noisy audio narration suffices to learn a good action
detection model, thus reducing annotation expenses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo-Label Guided Multi-Contrast Generalization for Non-Contrast Organ-Aware Segmentation. (arXiv:2205.05898v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05898">
<div class="article-summary-box-inner">
<span><p>Non-contrast computed tomography (NCCT) is commonly acquired for lung cancer
screening, assessment of general abdominal pain or suspected renal stones,
trauma evaluation, and many other indications. However, the absence of contrast
limits distinguishing organ in-between boundaries. In this paper, we propose a
novel unsupervised approach that leverages pairwise contrast-enhanced CT (CECT)
context to compute non-contrast segmentation without ground-truth label. Unlike
generative adversarial approaches, we compute the pairwise morphological
context with CECT to provide teacher guidance instead of generating fake
anatomical context. Additionally, we further augment the intensity correlations
in 'organ-specific' settings and increase the sensitivity to organ-aware
boundary. We validate our approach on multi-organ segmentation with paired
non-contrast &amp; contrast-enhanced CT scans using five-fold cross-validation.
Full external validations are performed on an independent non-contrast cohort
for aorta segmentation. Compared with current abdominal organs segmentation
state-of-the-art in fully supervised setting, our proposed pipeline achieves a
significantly higher Dice by 3.98% (internal multi-organ annotated), and 8.00%
(external aorta annotated) for abdominal organs segmentation. The code and
pretrained models are publicly available at
https://github.com/MASILab/ContrastMix.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Infrared Invisible Clothing:Hiding from Infrared Detectors at Multiple Angles in Real World. (arXiv:2205.05909v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05909">
<div class="article-summary-box-inner">
<span><p>Thermal infrared imaging is widely used in body temperature measurement,
security monitoring, and so on, but its safety research attracted attention
only in recent years. We proposed the infrared adversarial clothing, which
could fool infrared pedestrian detectors at different angles. We simulated the
process from cloth to clothing in the digital world and then designed the
adversarial "QR code" pattern. The core of our method is to design a basic
pattern that can be expanded periodically, and make the pattern after random
cropping and deformation still have an adversarial effect, then we can process
the flat cloth with an adversarial pattern into any 3D clothes. The results
showed that the optimized "QR code" pattern lowered the Average Precision (AP)
of YOLOv3 by 87.7%, while the random "QR code" pattern and blank pattern
lowered the AP of YOLOv3 by 57.9% and 30.1%, respectively, in the digital
world. We then manufactured an adversarial shirt with a new material: aerogel.
Physical-world experiments showed that the adversarial "QR code" pattern
clothing lowered the AP of YOLOv3 by 64.6%, while the random "QR code" pattern
clothing and fully heat-insulated clothing lowered the AP of YOLOv3 by 28.3%
and 22.8%, respectively. We used the model ensemble technique to improve the
attack transferability to unseen models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building Facade Parsing R-CNN. (arXiv:2205.05912v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05912">
<div class="article-summary-box-inner">
<span><p>Building facade parsing, which predicts pixel-level labels for building
facades, has applications in computer vision perception for autonomous vehicle
(AV) driving. However, instead of a frontal view, an on-board camera of an AV
captures a deformed view of the facade of the buildings on both sides of the
road the AV is travelling on, due to the camera perspective. We propose Facade
R-CNN, which includes a transconv module, generalized bounding box detection,
and convex regularization, to perform parsing of deformed facade views.
Experiments demonstrate that Facade R-CNN achieves better performance than the
current state-of-the-art facade parsing models, which are primarily developed
for frontal views. We also publish a new building facade parsing dataset
derived from the Oxford RobotCar dataset, which we call the Oxford RobotCar
Facade dataset. This dataset contains 500 street-view images from the Oxford
RobotCar dataset augmented with accurate annotations of building facade
objects. The published dataset is available at
https://github.com/sijieaaa/Oxford-RobotCar-Facade
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Dense RGB-D SLAM using Learning-based Visual Odometry. (arXiv:2205.05916v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05916">
<div class="article-summary-box-inner">
<span><p>We propose a dense dynamic RGB-D SLAM pipeline based on a learning-based
visual odometry, TartanVO. TartanVO, like other direct methods rather than
feature-based, estimates camera pose through dense optical flow, which only
applies to static scenes and disregards dynamic objects. Due to the color
constancy assumption, optical flow is not able to differentiate between dynamic
and static pixels. Therefore, to reconstruct a static map through such direct
methods, our pipeline resolves dynamic/static segmentation by leveraging the
optical flow output, and only fuse static points into the map. Moreover, we
rerender the input frames such that the dynamic pixels are removed and
iteratively pass them back into the visual odometry to refine the pose
estimate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fall detection using multimodal data. (arXiv:2205.05918v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05918">
<div class="article-summary-box-inner">
<span><p>In recent years, the occurrence of falls has increased and has had
detrimental effects on older adults. Therefore, various machine learning
approaches and datasets have been introduced to construct an efficient fall
detection algorithm for the social community. This paper studies the fall
detection problem based on a large public dataset, namely the UP-Fall Detection
Dataset. This dataset was collected from a dozen of volunteers using different
sensors and two cameras. We propose several techniques to obtain valuable
features from these sensors and cameras and then construct suitable models for
the main problem. The experimental results show that our proposed methods can
bypass the state-of-the-art methods on this dataset in terms of accuracy,
precision, recall, and F1 score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Group R-CNN for Weakly Semi-supervised Object Detection with Points. (arXiv:2205.05920v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05920">
<div class="article-summary-box-inner">
<span><p>We study the problem of weakly semi-supervised object detection with points
(WSSOD-P), where the training data is combined by a small set of fully
annotated images with bounding boxes and a large set of weakly-labeled images
with only a single point annotated for each instance. The core of this task is
to train a point-to-box regressor on well-labeled images that can be used to
predict credible bounding boxes for each point annotation. We challenge the
prior belief that existing CNN-based detectors are not compatible with this
task. Based on the classic R-CNN architecture, we propose an effective
point-to-box regressor: Group R-CNN. Group R-CNN first uses instance-level
proposal grouping to generate a group of proposals for each point annotation
and thus can obtain a high recall rate. To better distinguish different
instances and improve precision, we propose instance-level proposal assignment
to replace the vanilla assignment strategy adopted in the original R-CNN
methods. As naive instance-level assignment brings converging difficulty, we
propose instance-aware representation learning which consists of instance-aware
feature enhancement and instance-aware parameter generation to overcome this
issue. Comprehensive experiments on the MS-COCO benchmark demonstrate the
effectiveness of our method. Specifically, Group R-CNN significantly
outperforms the prior method Point DETR by 3.9 mAP with 5% well-labeled images,
which is the most challenging scenario. The source code can be found at
https://github.com/jshilong/GroupRCNN
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ray Priors through Reprojection: Improving Neural Radiance Fields for Novel View Extrapolation. (arXiv:2205.05922v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05922">
<div class="article-summary-box-inner">
<span><p>Neural Radiance Fields (NeRF) have emerged as a potent paradigm for
representing scenes and synthesizing photo-realistic images. A main limitation
of conventional NeRFs is that they often fail to produce high-quality
renderings under novel viewpoints that are significantly different from the
training viewpoints. In this paper, instead of exploiting few-shot image
synthesis, we study the novel view extrapolation setting that (1) the training
images can well describe an object, and (2) there is a notable discrepancy
between the training and test viewpoints' distributions. We present RapNeRF
(RAy Priors) as a solution. Our insight is that the inherent appearances of a
3D surface's arbitrary visible projections should be consistent. We thus
propose a random ray casting policy that allows training unseen views using
seen views. Furthermore, we show that a ray atlas pre-computed from the
observed rays' viewing directions could further enhance the rendering quality
for extrapolated views. A main limitation is that RapNeRF would remove the
strong view-dependent effects because it leverages the multi-view consistency
property.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Single-shot Detector for Small Object Detection in Remote Sensing Images. (arXiv:2205.05927v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05927">
<div class="article-summary-box-inner">
<span><p>Small-object detection is a challenging problem. In the last few years, the
convolution neural networks methods have been achieved considerable progress.
However, the current detectors struggle with effective features extraction for
small-scale objects. To address this challenge, we propose image pyramid
single-shot detector (IPSSD). In IPSSD, single-shot detector is adopted
combined with an image pyramid network to extract semantically strong features
for generating candidate regions. The proposed network can enhance the
small-scale features from a feature pyramid network. We evaluated the
performance of the proposed model on two public datasets and the results show
the superior performance of our model compared to the other state-of-the-art
object detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimCPSR: Simple Contrastive Learning for Paper Submission Recommendation System. (arXiv:2205.05940v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05940">
<div class="article-summary-box-inner">
<span><p>The recommendation system plays a vital role in many areas, especially
academic fields, to support researchers in submitting and increasing the
acceptance of their work through the conference or journal selection process.
This study proposes a transformer-based model using transfer learning as an
efficient approach for the paper submission recommendation system. By combining
essential information (such as the title, the abstract, and the list of
keywords) with the aims and scopes of journals, the model can recommend the Top
K journals that maximize the acceptance of the paper. Our model had developed
through two states: (i) Fine-tuning the pre-trained language model (LM) with a
simple contrastive learning framework. We utilized a simple supervised
contrastive objective to fine-tune all parameters, encouraging the LM to learn
the document representation effectively. (ii) The fine-tuned LM was then
trained on different combinations of the features for the downstream task. This
study suggests a more advanced method for enhancing the efficiency of the paper
submission recommendation system compared to previous approaches when we
respectively achieve 0.5173, 0.8097, 0.8862, 0.9496 for Top 1, 3, 5, and 10
accuracies on the test set for combining the title, abstract, and keywords as
input features. Incorporating the journals' aims and scopes, our model shows an
exciting result by getting 0.5194, 0.8112, 0.8866, and 0.9496 respective to Top
1, 3, 5, and 10.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Economical Precise Manipulation and Auto Eye-Hand Coordination with Binocular Visual Reinforcement Learning. (arXiv:2205.05963v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05963">
<div class="article-summary-box-inner">
<span><p>Precision robotic manipulation tasks (insertion, screwing, precisely pick,
precisely place) are required in many scenarios. Previous methods achieved good
performance on such manipulation tasks. However, such methods typically require
tedious calibration or expensive sensors. 3D/RGB-D cameras and torque/force
sensors add to the cost of the robotic application and may not always be
economical. In this work, we aim to solve these but using only weak-calibrated
and low-cost webcams. We propose Binocular Alignment Learning (BAL), which
could automatically learn the eye-hand coordination and points alignment
capabilities to solve the four tasks. Our work focuses on working with unknown
eye-hand coordination and proposes different ways of performing eye-in-hand
camera calibration automatically. The algorithm was trained in simulation and
used a practical pipeline to achieve sim2real and test it on the real robot.
Our method achieves a competitively good result with minimal cost on the four
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FPSRS: A Fusion Approach for Paper Submission Recommendation System. (arXiv:2205.05965v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05965">
<div class="article-summary-box-inner">
<span><p>Recommender systems have been increasingly popular in entertainment and
consumption and are evident in academics, especially for applications that
suggest submitting scientific articles to scientists. However, because of the
various acceptance rates, impact factors, and rankings in different publishers,
searching for a proper venue or journal to submit a scientific work usually
takes a lot of time and effort. In this paper, we aim to present two newer
approaches extended from our paper [13] presented at the conference IAE/AIE
2021 by employing RNN structures besides using Conv1D. In addition, we also
introduce a new method, namely DistilBertAims, using DistillBert for two cases
of uppercase and lower-case words to vectorize features such as Title,
Abstract, and Keywords, and then use Conv1d to perform feature extraction.
Furthermore, we propose a new calculation method for similarity score for Aim &amp;
Scope with other features; this helps keep the weights of similarity score
calculation continuously updated and then continue to fit more data. The
experimental results show that the second approach could obtain a better
performance, which is 62.46% and 12.44% higher than the best of the previous
study [13] in terms of the Top 1 accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Target Aware Network Architecture Search and Compression for Efficient Knowledge Transfer. (arXiv:2205.05967v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05967">
<div class="article-summary-box-inner">
<span><p>Transfer Learning enables Convolutional Neural Networks (CNN) to acquire
knowledge from a source domain and transfer it to a target domain, where
collecting large-scale annotated examples is both time-consuming and expensive.
Conventionally, while transferring the knowledge learned from one task to
another task, the deeper layers of a pre-trained CNN are finetuned over the
target dataset. However, these layers that are originally designed for the
source task are over-parameterized for the target task. Thus, finetuning these
layers over the target dataset reduces the generalization ability of the CNN
due to high network complexity. To tackle this problem, we propose a two-stage
framework called TASCNet which enables efficient knowledge transfer. In the
first stage, the configuration of the deeper layers is learned automatically
and finetuned over the target dataset. Later, in the second stage, the
redundant filters are pruned from the fine-tuned CNN to decrease the network's
complexity for the target task while preserving the performance. This two-stage
mechanism finds a compact version of the pre-trained CNN with optimal structure
(number of filters in a convolutional layer, number of neurons in a dense
layer, and so on) from the hypothesis space. The efficacy of the proposed
method is evaluated using VGG-16, ResNet-50, and DenseNet-121 on CalTech-101,
CalTech-256, and Stanford Dogs datasets. The proposed TASCNet reduces the
computational complexity of pre-trained CNNs over the target task by reducing
both trainable parameters and FLOPs which enables resource-efficient knowledge
transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TaDeR: A New Task Dependency Recommendation for Project Management Platform. (arXiv:2205.05976v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05976">
<div class="article-summary-box-inner">
<span><p>Many startups and companies worldwide have been using project management
software and tools to monitor, track and manage their projects. For software
projects, the number of tasks from the beginning to the end is quite a large
number that sometimes takes a lot of time and effort to search and link the
current task to a group of previous ones for further references. This paper
proposes an efficient task dependency recommendation algorithm to suggest tasks
dependent on a given task that the user has just created. We present an
efficient feature engineering step and construct a deep neural network to this
aim. We performed extensive experiments on two different large projects
(MDLSITE from moodle.org and FLUME from apache.org) to find the best features
in 28 combinations of features and the best performance model using two
embedding methods (GloVe and FastText). We consider three types of models (GRU,
CNN, LSTM) using Accuracy@K, MRR@K, and Recall@K (where K = 1, 2, 3, and 5) and
baseline models using traditional methods: TF-IDF with various matching score
calculating such as cosine similarity, Euclidean distance, Manhattan distance,
and Chebyshev distance. After many experiments, the GloVe Embedding and CNN
model reached the best result in our dataset, so we chose this model as our
proposed method. In addition, adding the time filter in the post-processing
step can significantly improve the recommendation system's performance. The
experimental results show that our proposed method can reach 0.2335 in
Accuracy@1 and MRR@1 and 0.2011 in Recall@1 of dataset FLUME. With the MDLSITE
dataset, we obtained 0.1258 in Accuracy@1 and MRR@1 and 0.1141 in Recall@1. In
the top 5, our model reached 0.3040 in Accuracy@5, 0.2563 MRR@5, and 0.2651
Recall@5 in FLUME. In the MDLSITE dataset, our model got 0.5270 Accuracy@5,
0.2689 MRR@5, and 0.2651 Recall@5.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MPPNet: Multi-Frame Feature Intertwining with Proxy Points for 3D Temporal Object Detection. (arXiv:2205.05979v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05979">
<div class="article-summary-box-inner">
<span><p>Accurate and reliable 3D detection is vital for many applications including
autonomous driving vehicles and service robots. In this paper, we present a
flexible and high-performance 3D detection framework, named MPPNet, for 3D
temporal object detection with point cloud sequences. We propose a novel
three-hierarchy framework with proxy points for multi-frame feature encoding
and interactions to achieve better detection. The three hierarchies conduct
per-frame feature encoding, short-clip feature fusion, and whole-sequence
feature aggregation, respectively. To enable processing long-sequence point
clouds with reasonable computational resources, intra-group feature mixing and
inter-group feature attention are proposed to form the second and third feature
encoding hierarchies, which are recurrently applied for aggregating multi-frame
trajectory features. The proxy points not only act as consistent object
representations for each frame, but also serve as the courier to facilitate
feature interaction between frames. The experiments on largeWaymo Open dataset
show that our approach outperforms state-of-the-art methods with large margins
when applied to both short (e.g., 4-frame) and long (e.g., 16-frame) point
cloud sequences. Specifically, MPPNet achieves 74.21%, 74.62% and 73.31% for
vehicle, pedestrian and cyclist classes on the LEVEL 2 mAPH metric with
16-frame input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Teaching Independent Parts Separately"(TIPS-GAN) : Improving Accuracy and Stability in Unsupervised Adversarial 2D to 3D Human Pose Estimation. (arXiv:2205.05980v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05980">
<div class="article-summary-box-inner">
<span><p>We present TIPS-GAN, a new approach to improve the accuracy and stability in
unsupervised adversarial 2D to 3D human pose estimation. In our work we
demonstrate that the human kinematic skeleton should not be assumed as one
spatially dependent structure. In fact, we believe when a full 2D pose is
provided during training, there is an inherent bias learned where the 3D
coordinate of a keypoint is spatially codependent on the 2D locations of all
other keypoints. To investigate our theory we follow previous adversarial
approaches but trained two generators on spatially independent parts of the
kinematic skeleton, the torso and the legs. During our study we find that
improving self-consistency is key to lowering the evaluation error and
therefore introduce new consistency constraints within the standard adversarial
cycle. We then produced a final TIPS model via knowledge distillation which can
predict the 3D coordinates for the entire 2D pose with improved results.
Furthermore we help address the question left unanswered in prior adversarial
learning papers of how long to train for a truly unsupervised scenario. We show
that two independent generators training adversarially can hold a minimum error
against a discriminator for a longer period of time than that of a solo
generator which will diverge due to the adversarial network becoming unstable.
TIPS decreases the average error by 18\% when compared to that of a baseline
solo generator. TIPS improves upon other unsupervised approaches while also
performing strongly against supervised and weakly-supervised approaches during
evaluation on both the Human3.6M and MPI-INF-3DHP dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blueprint Separable Residual Network for Efficient Image Super-Resolution. (arXiv:2205.05996v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05996">
<div class="article-summary-box-inner">
<span><p>Recent advances in single image super-resolution (SISR) have achieved
extraordinary performance, but the computational cost is too heavy to apply in
edge devices. To alleviate this problem, many novel and effective solutions
have been proposed. Convolutional neural network (CNN) with the attention
mechanism has attracted increasing attention due to its efficiency and
effectiveness. However, there is still redundancy in the convolution operation.
In this paper, we propose Blueprint Separable Residual Network (BSRN)
containing two efficient designs. One is the usage of blueprint separable
convolution (BSConv), which takes place of the redundant convolution operation.
The other is to enhance the model ability by introducing more effective
attention modules. The experimental results show that BSRN achieves
state-of-the-art performance among existing efficient SR methods. Moreover, a
smaller variant of our model BSRN-S won the first place in model complexity
track of NTIRE 2022 Efficient SR Challenge. The code is available at
https://github.com/xiaom233/BSRN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accounting for the Sequential Nature of States to Learn Features for Reinforcement Learning. (arXiv:2205.06000v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06000">
<div class="article-summary-box-inner">
<span><p>In this work, we investigate the properties of data that cause popular
representation learning approaches to fail. In particular, we find that in
environments where states do not significantly overlap, variational
autoencoders (VAEs) fail to learn useful features. We demonstrate this failure
in a simple gridworld domain, and then provide a solution in the form of metric
learning. However, metric learning requires supervision in the form of a
distance function, which is absent in reinforcement learning. To overcome this,
we leverage the sequential nature of states in a replay buffer to approximate a
distance metric and provide a weak supervision signal, under the assumption
that temporally close states are also semantically similar. We modify a VAE
with triplet loss and demonstrate that this approach is able to learn useful
features for downstream tasks, without additional supervision, in environments
where standard VAEs fail.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D3T-GAN: Data-Dependent Domain Transfer GANs for Few-shot Image Generation. (arXiv:2205.06032v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06032">
<div class="article-summary-box-inner">
<span><p>As an important and challenging problem, few-shot image generation aims at
generating realistic images through training a GAN model given few samples. A
typical solution for few-shot generation is to transfer a well-trained GAN
model from a data-rich source domain to the data-deficient target domain. In
this paper, we propose a novel self-supervised transfer scheme termed D3T-GAN,
addressing the cross-domain GANs transfer in few-shot image generation.
Specifically, we design two individual strategies to transfer knowledge between
generators and discriminators, respectively. To transfer knowledge between
generators, we conduct a data-dependent transformation, which projects and
reconstructs the target samples into the source generator space. Then, we
perform knowledge transfer from transformed samples to generated samples. To
transfer knowledge between discriminators, we design a multi-level discriminant
knowledge distillation from the source discriminator to the target
discriminator on both the real and fake samples. Extensive experiments show
that our method improve the quality of generated images and achieves the
state-of-the-art FID scores on commonly used datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep morphological recognition of kidney stones using intra-operative endoscopic digital videos. (arXiv:2205.06093v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06093">
<div class="article-summary-box-inner">
<span><p>The collection and the analysis of kidney stone morphological criteria are
essential for an aetiological diagnosis of stone disease. However, in-situ
LASER-based fragmentation of urinary stones, which is now the most established
chirurgical intervention, may destroy the morphology of the targeted stone. In
the current study, we assess the performance and added value of processing
complete digital endoscopic video sequences for the automatic recognition of
stone morphological features during a standard-of-care intra-operative session.
To this end, a computer-aided video classifier was developed to predict in-situ
the morphology of stone using an intra-operative digital endoscopic video
acquired in a clinical setting.
</p>
<p>The proposed technique was evaluated on pure (i.e. include one morphology)
and mixed (i.e. include at least two morphologies) stones involving "Ia/Calcium
Oxalate Monohydrate (COM)", "IIb/ Calcium Oxalate Dihydrate (COD)" and
"IIIb/Uric Acid (UA)" morphologies. 71 digital endoscopic videos (50 exhibited
only one morphological type and 21 displayed two) were analyzed using the
proposed video classifier (56840 frames processed in total). Using the proposed
approach, diagnostic performances (averaged over both pure and mixed stone
types) were as follows: balanced accuracy=88%, sensitivity=80%,
specificity=95%, precision=78% and F1-score=78%.
</p>
<p>The obtained results demonstrate that AI applied on digital endoscopic video
sequences is a promising tool for collecting morphological information during
the time-course of the stone fragmentation process without resorting to any
human intervention for stone delineation or selection of good quality steady
frames. To this end, irrelevant image information must be removed from the
prediction process at both frame and pixel levels, which is now feasible thanks
to the use of AI-dedicated networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tensor-based Emotion Editing in the StyleGAN Latent Space. (arXiv:2205.06102v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06102">
<div class="article-summary-box-inner">
<span><p>In this paper, we use a tensor model based on the Higher-Order Singular Value
Decomposition (HOSVD) to discover semantic directions in Generative Adversarial
Networks. This is achieved by first embedding a structured facial expression
database into the latent space using the e4e encoder. Specifically, we discover
directions in latent space corresponding to the six prototypical emotions:
anger, disgust, fear, happiness, sadness, and surprise, as well as a direction
for yaw rotation. These latent space directions are employed to change the
expression or yaw rotation of real face images. We compare our found directions
to similar directions found by two other methods. The results show that the
visual quality of the resultant edits are on par with State-of-the-Art. It can
also be concluded that the tensor-based model is well suited for emotion and
yaw editing, i.e., that the emotion or yaw rotation of a novel face image can
be robustly changed without a significant effect on identity or other
attributes in the images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Model, Multiple Modalities: A Sparsely Activated Approach for Text, Sound, Image, Video and Code. (arXiv:2205.06126v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06126">
<div class="article-summary-box-inner">
<span><p>People perceive the world with multiple senses (e.g., through hearing sounds,
reading words and seeing objects). However, most existing AI systems only
process an individual modality. This paper presents an approach that excels at
handling multiple modalities of information with a single model. In our
"{SkillNet}" model, different parts of the parameters are specialized for
processing different modalities. Unlike traditional dense models that always
activate all the model parameters, our model sparsely activates parts of the
parameters whose skills are relevant to the task. Such model design enables
SkillNet to learn skills in a more interpretable way. We develop our model for
five modalities including text, image, sound, video and code. Results show
that, SkillNet performs comparably to five modality-specific fine-tuned models.
Moreover, our model supports self-supervised pretraining with the same sparsely
activated way, resulting in better initialized parameters for different
modalities. We find that pretraining significantly improves the performance of
SkillNet on five modalities, on par with or even better than baselines with
modality-specific pretraining. On the task of Chinese text-to-image retrieval,
our final system achieves higher accuracy than existing leading systems
including Wukong{ViT-B} and Wenlan 2.0 while using less number of activated
parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smooth-Reduce: Leveraging Patches for Improved Certified Robustness. (arXiv:2205.06154v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06154">
<div class="article-summary-box-inner">
<span><p>Randomized smoothing (RS) has been shown to be a fast, scalable technique for
certifying the robustness of deep neural network classifiers. However, methods
based on RS require augmenting data with large amounts of noise, which leads to
significant drops in accuracy. We propose a training-free, modified smoothing
approach, Smooth-Reduce, that leverages patching and aggregation to provide
improved classifier certificates. Our algorithm classifies overlapping patches
extracted from an input image, and aggregates the predicted logits to certify a
larger radius around the input. We study two aggregation schemes -- max and
mean -- and show that both approaches provide better certificates in terms of
certified accuracy, average certified radii and abstention rates as compared to
concurrent approaches. We also provide theoretical guarantees for such
certificates, and empirically show significant improvements over other
randomized smoothing methods that require expensive retraining. Further, we
extend our approach to videos and provide meaningful certificates for video
classifiers. A project page can be found at
https://nyu-dice-lab.github.io/SmoothReduce/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Localized Vision-Language Matching for Open-vocabulary Object Detection. (arXiv:2205.06160v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06160">
<div class="article-summary-box-inner">
<span><p>In this work, we propose an open-world object detection method that, based on
image-caption pairs, learns to detect novel object classes along with a given
set of known classes. It is a two-stage training approach that first uses a
location-guided image-caption matching technique to learn class labels for both
novel and known classes in a weakly-supervised manner and second specializes
the model for the object detection task using known class annotations. We show
that a simple language model fits better than a large contextualized language
model for detecting novel objects. Moreover, we introduce a
consistency-regularization technique to better exploit image-caption pair
information. Our method compares favorably to existing open-world detection
approaches while being data-efficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Deep Visual and Inertial Odometry with Adaptive Visual Modality Selection. (arXiv:2205.06187v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06187">
<div class="article-summary-box-inner">
<span><p>In recent years, deep learning-based approaches for visual-inertial odometry
(VIO) have shown remarkable performance outperforming traditional geometric
methods. Yet, all existing methods use both the visual and inertial
measurements for every pose estimation incurring potential computational
redundancy. While visual data processing is much more expensive than that for
the inertial measurement unit (IMU), it may not always contribute to improving
the pose estimation accuracy. In this paper, we propose an adaptive
deep-learning based VIO method that reduces computational redundancy by
opportunistically disabling the visual modality. Specifically, we train a
policy network that learns to deactivate the visual feature extractor on the
fly based on the current motion state and IMU readings. A Gumbel-Softmax trick
is adopted to train the policy network to make the decision process
differentiable for end-to-end system training. The learned strategy is
interpretable, and it shows scenario-dependent decision patterns for adaptive
complexity reduction. Experiment results show that our method achieves a
similar or even better performance than the full-modality baseline with up to
78.8% computational complexity reduction for KITTI dataset evaluation. Our code
will be shared in https://github.com/mingyuyng/Visual-Selective-VIO
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Segmentation with Topological Priors. (arXiv:2205.06197v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06197">
<div class="article-summary-box-inner">
<span><p>Solving segmentation tasks with topological priors proved to make fewer
errors in fine-scale structures. In this work, we use topological priors both
before and during the deep neural network training procedure. We compared the
results of the two approaches with simple segmentation on various accuracy
metrics and the Betti number error, which is directly related to topological
correctness, and discovered that incorporating topological information into the
classical UNet model performed significantly better. We conducted experiments
on the ISBI EM segmentation dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embodied vision for learning object representations. (arXiv:2205.06198v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06198">
<div class="article-summary-box-inner">
<span><p>Recent time-contrastive learning approaches manage to learn invariant object
representations without supervision. This is achieved by mapping successive
views of an object onto close-by internal representations. When considering
this learning approach as a model of the development of human object
recognition, it is important to consider what visual input a toddler would
typically observe while interacting with objects. First, human vision is highly
foveated, with high resolution only available in the central region of the
field of view. Second, objects may be seen against a blurry background due to
infants' limited depth of field. Third, during object manipulation a toddler
mostly observes close objects filling a large part of the field of view due to
their rather short arms. Here, we study how these effects impact the quality of
visual representations learnt through time-contrastive learning. To this end,
we let a visually embodied agent "play" with objects in different locations of
a near photo-realistic flat. During each play session the agent views an object
in multiple orientations before turning its body to view another object. The
resulting sequence of views feeds a time-contrastive learning algorithm. Our
results show that visual statistics mimicking those of a toddler improve object
recognition accuracy in both familiar and novel environments. We argue that
this effect is caused by the reduction of features extracted in the background,
a neural network bias for large features in the image and a greater similarity
between novel and familiar background regions. We conclude that the embodied
nature of visual learning may be crucial for understanding the development of
human object perception.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">F3A-GAN: Facial Flow for Face Animation with Generative Adversarial Networks. (arXiv:2205.06204v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06204">
<div class="article-summary-box-inner">
<span><p>Formulated as a conditional generation problem, face animation aims at
synthesizing continuous face images from a single source image driven by a set
of conditional face motion. Previous works mainly model the face motion as
conditions with 1D or 2D representation (e.g., action units, emotion codes,
landmark), which often leads to low-quality results in some complicated
scenarios such as continuous generation and largepose transformation. To tackle
this problem, the conditions are supposed to meet two requirements, i.e.,
motion information preserving and geometric continuity. To this end, we propose
a novel representation based on a 3D geometric flow, termed facial flow, to
represent the natural motion of the human face at any pose. Compared with other
previous conditions, the proposed facial flow well controls the continuous
changes to the face. After that, in order to utilize the facial flow for face
editing, we build a synthesis framework generating continuous images with
conditional facial flows. To fully take advantage of the motion information of
facial flows, a hierarchical conditional framework is designed to combine the
extracted multi-scale appearance features from images and motion features from
flows in a hierarchical manner. The framework then decodes multiple fused
features back to images progressively. Experimental results demonstrate the
effectiveness of our method compared to other state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delving into High-Quality Synthetic Face Occlusion Segmentation Datasets. (arXiv:2205.06218v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06218">
<div class="article-summary-box-inner">
<span><p>This paper performs comprehensive analysis on datasets for occlusion-aware
face segmentation, a task that is crucial for many downstream applications. The
collection and annotation of such datasets are time-consuming and
labor-intensive. Although some efforts have been made in synthetic data
generation, the naturalistic aspect of data remains less explored. In our
study, we propose two occlusion generation techniques, Naturalistic Occlusion
Generation (NatOcc), for producing high-quality naturalistic synthetic occluded
faces; and Random Occlusion Generation (RandOcc), a more general synthetic
occluded data generation method. We empirically show the effectiveness and
robustness of both methods, even for unseen occlusions. To facilitate model
evaluation, we present two high-resolution real-world occluded face datasets
with fine-grained annotations, RealOcc and RealOcc-Wild, featuring both careful
alignment preprocessing and an in-the-wild setting for robustness test. We
further conduct a comprehensive analysis on a newly introduced segmentation
benchmark, offering insights for future exploration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple Open-Vocabulary Object Detection with Vision Transformers. (arXiv:2205.06230v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06230">
<div class="article-summary-box-inner">
<span><p>Combining simple architectures with large-scale pre-training has led to
massive improvements in image classification. For object detection,
pre-training and scaling approaches are less well established, especially in
the long-tailed and open-vocabulary setting, where training data is relatively
scarce. In this paper, we propose a strong recipe for transferring image-text
models to open-vocabulary object detection. We use a standard Vision
Transformer architecture with minimal modifications, contrastive image-text
pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling
properties of this setup shows that increasing image-level pre-training and
model size yield consistent improvements on the downstream detection task. We
provide the adaptation strategies and regularizations needed to attain very
strong performance on zero-shot text-conditioned and one-shot image-conditioned
object detection. Code and models are available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Distillation for Multi-Target Domain Adaptation in Real-Time Person Re-Identification. (arXiv:2205.06237v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06237">
<div class="article-summary-box-inner">
<span><p>Despite the recent success of deep learning architectures, person
re-identification (ReID) remains a challenging problem in real-word
applications. Several unsupervised single-target domain adaptation (STDA)
methods have recently been proposed to limit the decline in ReID accuracy
caused by the domain shift that typically occurs between source and target
video data. Given the multimodal nature of person ReID data (due to variations
across camera viewpoints and capture conditions), training a common CNN
backbone to address domain shifts across multiple target domains, can provide
an efficient solution for real-time ReID applications. Although multi-target
domain adaptation (MTDA) has not been widely addressed in the ReID literature,
a straightforward approach consists in blending different target datasets, and
performing STDA on the mixture to train a common CNN. However, this approach
may lead to poor generalization, especially when blending a growing number of
distinct target domains to train a smaller CNN.
</p>
<p>To alleviate this problem, we introduce a new MTDA method based on knowledge
distillation (KD-ReID) that is suitable for real-time person ReID applications.
Our method adapts a common lightweight student backbone CNN over the target
domains by alternatively distilling from multiple specialized teacher CNNs,
each one adapted on data from a specific target domain. Extensive experiments
conducted on several challenging person ReID datasets indicate that our
approach outperforms state-of-art methods for MTDA, including blending methods,
particularly when training a compact CNN backbone like OSNet. Results suggest
that our flexible MTDA approach can be employed to design cost-effective ReID
systems for real-time video surveillance applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What's in a Caption? Dataset-Specific Linguistic Diversity and Its Effect on Visual Description Models and Metrics. (arXiv:2205.06253v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06253">
<div class="article-summary-box-inner">
<span><p>While there have been significant gains in the field of automated video
description, the generalization performance of automated description models to
novel domains remains a major barrier to using these systems in the real world.
Most visual description methods are known to capture and exploit patterns in
the training data leading to evaluation metric increases, but what are those
patterns? In this work, we examine several popular visual description datasets,
and capture, analyze, and understand the dataset-specific linguistic patterns
that models exploit but do not generalize to new domains. At the token level,
sample level, and dataset level, we find that caption diversity is a major
driving factor behind the generation of generic and uninformative captions. We
further show that state-of-the-art models even outperform held-out ground truth
captions on modern metrics, and that this effect is an artifact of linguistic
diversity in datasets. Understanding this linguistic diversity is key to
building strong captioning models, we recommend several methods and approaches
for maintaining diversity in the collection of new data, and dealing with the
consequences of limited diversity when using current models and metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Vertex Descent: A New Direction for 3D Human Model Fitting. (arXiv:2205.06254v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06254">
<div class="article-summary-box-inner">
<span><p>We propose a novel optimization-based paradigm for 3D human model fitting on
images and scans. In contrast to existing approaches that directly regress the
parameters of a low-dimensional statistical body model (e.g. SMPL) from input
images, we train an ensemble of per-vertex neural fields network. The network
predicts, in a distributed manner, the vertex descent direction towards the
ground truth, based on neural features extracted at the current vertex
projection. At inference, we employ this network, dubbed LVD, within a
gradient-descent optimization pipeline until its convergence, which typically
occurs in a fraction of a second even when initializing all vertices into a
single point. An exhaustive evaluation demonstrates that our approach is able
to capture the underlying body of clothed people with very different body
shapes, achieving a significant improvement compared to state-of-the-art. LVD
is also applicable to 3D model fitting of humans and hands, for which we show a
significant improvement to the SOTA with a much simpler and faster method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Moments from Near-Duplicate Photos. (arXiv:2205.06255v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06255">
<div class="article-summary-box-inner">
<span><p>We introduce 3D Moments, a new computational photography effect. As input we
take a pair of near-duplicate photos, i.e., photos of moving subjects from
similar viewpoints, common in people's photo collections. As output, we produce
a video that smoothly interpolates the scene motion from the first photo to the
second, while also producing camera motion with parallax that gives a
heightened sense of 3D. To achieve this effect, we represent the scene as a
pair of feature-based layered depth images augmented with scene flow. This
representation enables motion interpolation along with independent control of
the camera viewpoint. Our system produces photorealistic space-time videos with
motion parallax and scene dynamics, while plausibly recovering regions occluded
in the original views. We conduct extensive experiments demonstrating superior
performance over baselines on public datasets and in-the-wild photos. Project
page: https://3d-moments.github.io/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELODI: Ensemble Logit Difference Inhibition for Positive-Congruent Training. (arXiv:2205.06265v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06265">
<div class="article-summary-box-inner">
<span><p>Negative flips are errors introduced in a classification system when a legacy
model is replaced with a new one. Existing methods to reduce the negative flip
rate (NFR) either do so at the expense of overall accuracy using model
distillation, or use ensembles, which multiply inference cost prohibitively. We
present a method to train a classification system that achieves paragon
performance in both error rate and NFR, at the inference cost of a single
model. Our method introduces a generalized distillation objective, Logit
Difference Inhibition (LDI), that penalizes changes in the logits between the
new and old model, without forcing them to coincide as in ordinary
distillation. LDI affords the model flexibility to reduce error rate along with
NFR. The method uses a homogeneous ensemble as the reference model for LDI,
hence the name Ensemble LDI, or ELODI. The reference model can then be
substituted with a single model at inference time. The method leverages the
observation that negative flips are typically not close to the decision
boundary, but often exhibit large deviations in the distance among their
logits, which are reduced by ELODI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topologically-Aware Deformation Fields for Single-View 3D Reconstruction. (arXiv:2205.06267v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06267">
<div class="article-summary-box-inner">
<span><p>We present a new framework for learning 3D object shapes and dense
cross-object 3D correspondences from just an unaligned category-specific image
collection. The 3D shapes are generated implicitly as deformations to a
category-specific signed distance field and are learned in an unsupervised
manner solely from unaligned image collections without any 3D supervision.
Generally, image collections on the internet contain several intra-category
geometric and topological variations, for example, different chairs can have
different topologies, which makes the task of joint shape and correspondence
estimation much more challenging. Because of this, prior works either focus on
learning each 3D object shape individually without modeling cross-instance
correspondences or perform joint shape and correspondence estimation on
categories with minimal intra-category topological variations. We overcome
these restrictions by learning a topologically-aware implicit deformation field
that maps a 3D point in the object space to a higher dimensional point in the
category-specific canonical space. At inference time, given a single image, we
reconstruct the underlying 3D shape by first implicitly deforming each 3D point
in the object space to the learned category-specific canonical space using the
topologically-aware deformation field and then reconstructing the 3D shape as a
canonical signed distance field. Both canonical shape and deformation field are
learned end-to-end in an inverse-graphics fashion using a learned recurrent ray
marcher (SRN) as a differentiable rendering module. Our approach, dubbed TARS,
achieves state-of-the-art reconstruction fidelity on several datasets:
ShapeNet, Pascal3D+, CUB, and Pix3D chairs. Result videos and code at
https://shivamduggal4.github.io/tars-3D/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-negative Sparse and Collaborative Representation for Pattern Classification. (arXiv:1908.07956v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.07956">
<div class="article-summary-box-inner">
<span><p>Sparse representation (SR) and collaborative representation (CR) have been
successfully applied in many pattern classification tasks such as face
recognition. In this paper, we propose a novel Non-negative Sparse and
Collaborative Representation (NSCR) for pattern classification. The NSCR
representation of each test sample is obtained by seeking a non-negative sparse
and collaborative representation vector that represents the test sample as a
linear combination of training samples. We observe that the non-negativity can
make the SR and CR more discriminative and effective for pattern
classification. Based on the proposed NSCR, we propose a NSCR based classifier
for pattern classification. Extensive experiments on benchmark datasets
demonstrate that the proposed NSCR based classifier outperforms the previous SR
or CR based approach, as well as state-of-the-art deep approaches, on diverse
challenging pattern classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ResLT: Residual Learning for Long-tailed Recognition. (arXiv:2101.10633v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10633">
<div class="article-summary-box-inner">
<span><p>Deep learning algorithms face great challenges with long-tailed data
distribution which, however, is quite a common case in real-world scenarios.
Previous methods tackle the problem from either the aspect of input space
(re-sampling classes with different frequencies) or loss space (re-weighting
classes with different weights), suffering from heavy over-fitting to tail
classes or hard optimization during training. To alleviate these issues, we
propose a more fundamental perspective for long-tailed recognition, i.e., from
the aspect of parameter space, and aims to preserve specific capacity for
classes with low frequencies. From this perspective, the trivial solution
utilizes different branches for the head, medium, and tail classes
respectively, and then sums their outputs as the final results is not feasible.
Instead, we design the effective residual fusion mechanism -- with one main
branch optimized to recognize images from all classes, another two residual
branches are gradually fused and optimized to enhance images from medium+tail
classes and tail classes respectively. Then the branches are aggregated into
final results by additive shortcuts. We test our method on several benchmarks,
i.e., long-tailed version of CIFAR-10, CIFAR-100, Places, ImageNet, and
iNaturalist 2018. Experimental results manifest the effectiveness of our
method. Our code is available at https://github.com/jiequancui/ResLT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Model Compression based on the Training History. (arXiv:2102.00160v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00160">
<div class="article-summary-box-inner">
<span><p>Deep Convolutional Neural Networks (DCNNs) have shown promising performances
in several visual recognition problems which motivated the researchers to
propose popular architectures such as LeNet, AlexNet, VGGNet, ResNet, and many
more. These architectures come at a cost of high computational complexity and
parameter storage. To get rid of storage and computational complexity, deep
model compression methods have been evolved. We propose a "History Based Filter
Pruning (HBFP)" method that utilizes network training history for filter
pruning. Specifically, we prune the redundant filters by observing similar
patterns in the filter's L1-norms (absolute sum of weights) over the training
epochs. We iteratively prune the redundant filters of a CNN in three steps.
First, we train the model and select the filter pairs with redundant filters in
each pair. Next, we optimize the network to ensure an increased measure of
similarity between the filters in a pair. This optimization of the network
facilitates us to prune one filter from each pair based on its importance
without much information loss. Finally, we retrain the network to regain the
performance, which is dropped due to filter pruning. We test our approach on
popular architectures such as LeNet-5 on MNIST dataset; VGG-16, ResNet-56, and
ResNet-110 on CIFAR-10 dataset, and ResNet-50 on ImageNet. The proposed pruning
method outperforms the state-of-the-art in terms of FLOPs reduction
(floating-point operations) by 97.98%, 83.42%, 78.43%, 74.95%, and 75.45% for
LeNet-5, VGG-16, ResNet-56, ResNet-110, and ResNet-50, respectively, while
maintaining the less error rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How I failed machine learning in medical imaging -- shortcomings and recommendations. (arXiv:2103.10292v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10292">
<div class="article-summary-box-inner">
<span><p>Medical imaging is an important research field with many opportunities for
improving patients' health. However, there are a number of challenges that are
slowing down the progress of the field as a whole, such optimizing for
publication. In this paper we reviewed several problems related to choosing
datasets, methods, evaluation metrics, and publication strategies. With a
review of literature and our own analysis, we show that at every step,
potential biases can creep in. On a positive note, we also see that initiatives
to counteract these problems are already being started. Finally we provide a
broad range of recommendations on how to further these address problems in the
future. For reproducibility, data and code for our analyses are available on
\url{https://github.com/GaelVaroquaux/ml_med_imaging_failures}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Multi-Target Domain Adaptation for Object Detection with Efficient Domain Transfer. (arXiv:2104.06476v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06476">
<div class="article-summary-box-inner">
<span><p>Recent advances in unsupervised domain adaptation have significantly improved
the recognition accuracy of CNNs by alleviating the domain shift between
(labeled) source and (unlabeled) target data distributions. While the problem
of single-target domain adaptation (STDA) for object detection has recently
received much attention, multi-target domain adaptation (MTDA) remains largely
unexplored, despite its practical relevance in several real-world applications,
such as multi-camera video surveillance. Compared to the STDA problem that may
involve large domain shifts between complex source and target distributions,
MTDA faces additional challenges, most notably the computational requirements
and catastrophic forgetting of previously-learned targets, which can depend on
the order of target adaptations. STDA for detection can be applied to MTDA by
adapting one model per target, or one common model with a mixture of data from
target domains. However, these approaches are either costly or inaccurate. The
only state-of-art MTDA method specialized for detection learns targets
incrementally, one target at a time, and mitigates the loss of knowledge by
using a duplicated detection model for knowledge distillation, which is
computationally expensive and does not scale well to many domains. In this
paper, we introduce an efficient approach for incremental learning that
generalizes well to multiple target domains. Our MTDA approach is more suitable
for real-world applications since it allows updating the detection model
incrementally, without storing data from previous-learned target domains, nor
retraining when a new target domain becomes available. Our proposed method,
MTDA-DTM, achieved the highest level of detection accuracy compared against
state-of-the-art approaches on several MTDA detection benchmarks and Wildtrack,
a benchmark for multi-camera pedestrian detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-vocabulary Object Detection via Vision and Language Knowledge Distillation. (arXiv:2104.13921v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13921">
<div class="article-summary-box-inner">
<span><p>We aim at advancing open-vocabulary object detection, which detects objects
described by arbitrary text inputs. The fundamental challenge is the
availability of training data. It is costly to further scale up the number of
classes contained in existing object detection datasets. To overcome this
challenge, we propose ViLD, a training method via Vision and Language knowledge
Distillation. Our method distills the knowledge from a pretrained
open-vocabulary image classification model (teacher) into a two-stage detector
(student). Specifically, we use the teacher model to encode category texts and
image regions of object proposals. Then we train a student detector, whose
region embeddings of detected boxes are aligned with the text and image
embeddings inferred by the teacher. We benchmark on LVIS by holding out all
rare categories as novel categories that are not seen during training. ViLD
obtains 16.1 mask AP$_r$ with a ResNet-50 backbone, even outperforming the
supervised counterpart by 3.8. When trained with a stronger teacher model
ALIGN, ViLD achieves 26.3 AP$_r$. The model can directly transfer to other
datasets without finetuning, achieving 72.2 AP$_{50}$ on PASCAL VOC, 36.6 AP on
COCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous
state-of-the-art by 4.8 on novel AP and 11.4 on overall AP. Code and demo are
open-sourced at
https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v10 [math.OC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08208">
<div class="article-summary-box-inner">
<span><p>Adaptive gradient methods have shown excellent performances for solving many
machine learning problems. Although multiple adaptive gradient methods were
recently studied, they mainly focus on either empirical or theoretical aspects
and also only work for specific problems by using some specific adaptive
learning rates. Thus, it is desired to design a universal framework for
practical algorithms of adaptive gradients with theoretical guarantee to solve
general problems. To fill this gap, we propose a faster and universal framework
of adaptive gradients (i.e., SUPER-ADAM) by introducing a universal adaptive
matrix that includes most existing adaptive gradient forms. Moreover, our
framework can flexibly integrate the momentum and variance reduced techniques.
In particular, our novel framework provides the convergence analysis support
for adaptive gradient methods under the nonconvex setting. In theoretical
analysis, we prove that our SUPER-ADAM algorithm can achieve the best known
gradient (i.e., stochastic first-order oracle (SFO)) complexity of
$\tilde{O}(\epsilon^{-3})$ for finding an $\epsilon$-stationary point of
nonconvex optimization, which matches the lower bound for stochastic smooth
nonconvex optimization. In numerical experiments, we employ various deep
learning tasks to validate that our algorithm consistently outperforms the
existing adaptive algorithms. Code is available at
https://github.com/LIJUNYI95/SuperAdam
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balancing the Budget: Feature Selection and Tracking for Multi-Camera Visual-Inertial Odometry. (arXiv:2109.05975v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05975">
<div class="article-summary-box-inner">
<span><p>We present a multi-camera visual-inertial odometry system based on factor
graph optimization which estimates motion by using all cameras simultaneously
while retaining a fixed overall feature budget. We focus on motion tracking in
challenging environments, such as narrow corridors, dark spaces with aggressive
motions, and abrupt lighting changes. These scenarios cause traditional
monocular or stereo odometry to fail. While tracking motion with extra cameras
should theoretically prevent failures, it leads to additional complexity and
computational burden. To overcome these challenges, we introduce two novel
methods to improve multi-camera feature tracking. First, instead of tracking
features separately in each camera, we track features continuously as they move
from one camera to another. This increases accuracy and achieves a more compact
factor graph representation. Second, we select a fixed budget of tracked
features across the cameras to reduce back-end optimization time. We have found
that using a smaller set of informative features can maintain the same tracking
accuracy. Our proposed method was extensively tested using a
hardware-synchronized device consisting of an IMU and four cameras (a front
stereo pair and two lateral) in scenarios including: an underground mine, large
open spaces, and building interiors with narrow stairs and corridors. Compared
to stereo-only state-of-the-art visual-inertial odometry methods, our approach
reduces the drift rate, relative pose error, by up to 80% in translation and
39% in rotation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single-Stage Keypoint-Based Category-Level Object Pose Estimation from an RGB Image. (arXiv:2109.06161v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06161">
<div class="article-summary-box-inner">
<span><p>Prior work on 6-DoF object pose estimation has largely focused on
instance-level processing, in which a textured CAD model is available for each
object being detected. Category-level 6-DoF pose estimation represents an
important step toward developing robotic vision systems that operate in
unstructured, real-world scenarios. In this work, we propose a single-stage,
keypoint-based approach for category-level object pose estimation that operates
on unknown object instances within a known category using a single RGB image as
input. The proposed network performs 2D object detection, detects 2D keypoints,
estimates 6-DoF pose, and regresses relative bounding cuboid dimensions. These
quantities are estimated in a sequential fashion, leveraging the recent idea of
convGRU for propagating information from easier tasks to those that are more
difficult. We favor simplicity in our design choices: generic cuboid vertex
coordinates, single-stage network, and monocular RGB input. We conduct
extensive experiments on the challenging Objectron benchmark, outperforming
state-of-the-art methods on the 3D IoU metric (27.6% higher than the MobilePose
single-stage approach and 7.1% higher than the related two-stage approach).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instant Response Few-shot Object Detection with Meta Strategy and Explicit Localization Inference. (arXiv:2110.13377v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13377">
<div class="article-summary-box-inner">
<span><p>Aiming at recognizing and localizing the object of novel categories by a few
reference samples, few-shot object detection (FSOD) is a quite challenging
task. Previous works often depend on the fine-tuning process to transfer their
model to the novel category and rarely consider the defect of fine-tuning,
resulting in many application drawbacks. For example, these methods are far
from satisfying in the episode-changeable scenarios due to excessive
fine-tuning times, and their performance on low-quality (e.g., low-shot and
class-incomplete) support sets degrades severely. To this end, this paper
proposes an instant response few-shot object detector (IR-FSOD) that can
accurately and directly detect the objects of novel categories without the
fine-tuning process. To accomplish the objective, we carefully analyze the
defects of individual modules in the Faster R-CNN framework under the FSOD
setting and then extend it to IR-FSOD by improving these defects. Specifically,
we first propose two simple but effective meta-strategies for the box
classifier and RPN module to enable the object detection of novel categories
with instant response. Then, we introduce two explicit inferences into the
localization module to alleviate its over-fitting to the base categories,
including explicit localization score and semi-explicit box regression.
Extensive experiments show that the IR-FSOD framework not only achieves
few-shot object detection with the instant response but also reaches
state-of-the-art performance in precision and recall under various FSOD
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain Adaptive Teacher for Object Detection. (arXiv:2111.13216v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13216">
<div class="article-summary-box-inner">
<span><p>We address the task of domain adaptation in object detection, where there is
a domain gap between a domain with annotations (source) and a domain of
interest without annotations (target). As an effective semi-supervised learning
method, the teacher-student framework (a student model is supervised by the
pseudo labels from a teacher model) has also yielded a large accuracy gain in
cross-domain object detection. However, it suffers from the domain shift and
generates many low-quality pseudo labels (\textit{e.g.,} false positives),
which leads to sub-optimal performance. To mitigate this problem, we propose a
teacher-student framework named Adaptive Teacher (AT) which leverages domain
adversarial learning and weak-strong data augmentation to address the domain
gap. Specifically, we employ feature-level adversarial training in the student
model, allowing features derived from the source and target domains to share
similar distributions. This process ensures the student model produces
domain-invariant features. Furthermore, we apply weak-strong augmentation and
mutual learning between the teacher model (taking data from the target domain)
and the student model (taking data from both domains). This enables the teacher
model to learn the knowledge from the student model without being biased to the
source domain. We show that AT demonstrates superiority over existing
approaches and even Oracle (fully-supervised) models by a large margin. For
example, we achieve 50.9% (49.3%) mAP on Foggy Cityscape (Clipart1K), which is
9.2% (5.2%) and 8.2% (11.0%) higher than previous state-of-the-art and Oracle,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Camera LiDAR Inertial Extension to the Newer College Dataset. (arXiv:2112.08854v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08854">
<div class="article-summary-box-inner">
<span><p>We present a multi-camera LiDAR inertial dataset of 4.5 km walking distance
as an expansion of the Newer College Dataset. The global shutter multi-camera
device is hardware synchronized with both the IMU and LiDAR, which is more
accurate than the original dataset with software synchronization. This dataset
also provides six Degrees of Freedom (DoF) ground truth poses at LiDAR
frequency (10 Hz). Three data collections are described and an example use case
of multi-camera visual-inertial odometry is demonstrated. This expansion
dataset contains small and narrow passages, large scale open spaces, as well as
vegetated areas, to test localization and mapping systems. Furthermore, some
sequences present challenging situations such as abrupt lighting change,
textureless surfaces, and aggressive motion. The dataset is available at:
https://ori-drs.github. io/newer-college-dataset/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Segmentation of Head and Neck Tumor: How Powerful Transformers Are?. (arXiv:2201.06251v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06251">
<div class="article-summary-box-inner">
<span><p>Cancer is one of the leading causes of death worldwide, and head and neck
(H&amp;N) cancer is amongst the most prevalent types. Positron emission tomography
and computed tomography are used to detect, segment and quantify the tumor
region. Clinically, tumor segmentation is extensively time-consuming and prone
to error. Machine learning, and deep learning in particular, can assist to
automate this process, yielding results as accurate as the results of a
clinician. In this paper, we investigate a vision transformer-based method to
automatically delineate H&amp;N tumor, and compare its results to leading
convolutional neural network (CNN)-based models. We use multi-modal data from
CT and PET scans to perform the segmentation task. We show that a solution with
a transformer-based model has the potential to achieve comparable results to
CNN-based ones. With cross validation, the model achieves a mean dice
similarity coefficient (DSC) of 0.736, mean precision of 0.766 and mean recall
of 0.766. This is only 0.021 less than the 2020 competition winning model
(cross validated in-house) in terms of the DSC score. On the testing set, the
model performs similarly, with DSC of 0.736, precision of 0.773, and recall of
0.760, which is only 0.023 lower in DSC than the 2020 competition winning
model. This work shows that cancer segmentation via transformer-based models is
a promising research area to further explore.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MHSnet: Multi-head and Spatial Attention Network with False-Positive Reduction for Pulmonary Nodules Detection. (arXiv:2201.13392v6 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.13392">
<div class="article-summary-box-inner">
<span><p>The mortality of lung cancer has ranked high among cancers for many years.
Early detection of lung cancer is critical for disease prevention, cure, and
mortality rate reduction. However, existing detection methods on pulmonary
nodules introduce an excessive number of false positive proposals in order to
achieve high sensitivity, which is not practical in clinical situations. In
this paper, we propose the multi-head detection and spatial
squeeze-and-attention network, MHSnet, to detect pulmonary nodules, in order to
aid doctors in the early diagnosis of lung cancers. Specifically, we first
introduce multi-head detectors and skip connections to customize for the
variety of nodules in sizes, shapes and types and capture multi-scale features.
Then, we implement a spatial attention module to enable the network to focus on
different regions differently inspired by how experienced clinicians screen CT
images, which results in fewer false positive proposals. Lastly, we present a
lightweight but effective false positive reduction module with the Linear
Regression model to cut down the number of false positive proposals, without
any constraints on the front network. Extensive experimental results compared
with the state-of-the-art models have shown the superiority of the MHSnet in
terms of the average FROC, sensitivity and especially false discovery rate
(2.98% and 2.18% improvement in terms of average FROC and sensitivity, 5.62%
and 28.33% decrease in terms of false discovery rate and average candidates per
scan). The false positive reduction module significantly decreases the average
number of candidates generated per scan by 68.11% and the false discovery rate
by 13.48%, which is promising to reduce distracted proposals for the downstream
tasks based on the detection results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion Sickness Modeling with Visual Vertical Estimation and Its Application to Autonomous Personal Mobility Vehicles. (arXiv:2202.06299v4 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06299">
<div class="article-summary-box-inner">
<span><p>Passengers (drivers) of level 3-5 autonomous personal mobility vehicles
(APMV) and cars can perform non-driving tasks, such as reading books and
smartphones, while driving. It has been pointed out that such activities may
increase motion sickness. Many studies have been conducted to build
countermeasures, of which various computational motion sickness models have
been developed. Many of these are based on subjective vertical conflict (SVC)
theory, which describes vertical changes in direction sensed by human sensory
organs vs. those expected by the central nervous system. Such models are
expected to be applied to autonomous driving scenarios. However, no current
computational model can integrate visual vertical information with vestibular
sensations.
</p>
<p>We proposed a 6 DoF SVC-VV model which add a visually perceived vertical
block into a conventional six-degrees-of-freedom SVC model to predict VV
directions from image data simulating the visual input of a human. Hence, a
simple image-based VV estimation method is proposed.
</p>
<p>As the validation of the proposed model, this paper focuses on describing the
fact that the motion sickness increases as a passenger reads a book while using
an AMPV, assuming that visual vertical (VV) plays an important role. In the
static experiment, it is demonstrated that the estimated VV by the proposed
method accurately described the gravitational acceleration direction with a low
mean absolute deviation. In addition, the results of the driving experiment
using an APMV demonstrated that the proposed 6 DoF SVC-VV model could describe
that the increased motion sickness experienced when the VV and gravitational
acceleration directions were different.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reducing Overconfidence Predictions for Autonomous Driving Perception. (arXiv:2202.07825v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07825">
<div class="article-summary-box-inner">
<span><p>In state-of-the-art deep learning for object recognition, SoftMax and Sigmoid
functions are most commonly employed as the predictor outputs. Such layers
often produce overconfident predictions rather than proper probabilistic
scores, which can thus harm the decision-making of `critical' perception
systems applied in autonomous driving and robotics. Given this, the experiments
in this work propose a probabilistic approach based on distributions calculated
out of the Logit layer scores of pre-trained networks. We demonstrate that
Maximum Likelihood (ML) and Maximum a-Posteriori (MAP) functions are more
suitable for probabilistic interpretations than SoftMax and Sigmoid-based
predictions for object recognition. We explore distinct sensor modalities via
RGB images and LiDARs (RV: range-view) data from the KITTI and Lyft Level-5
datasets, where our approach shows promising performance compared to the usual
SoftMax and Sigmoid layers, with the benefit of enabling interpretable
probabilistic predictions. Another advantage of the approach introduced in this
paper is that the ML and MAP functions can be implemented in existing trained
networks, that is, the approach benefits from the output of the Logit layer of
pre-trained networks. Thus, there is no need to carry out a new training phase
since the ML and MAP functions are used in the test/prediction phase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Connections between Deep Equilibrium and Sparse Representation models with Application to Hyperspectral Imaging. (arXiv:2203.15901v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15901">
<div class="article-summary-box-inner">
<span><p>In this study, the problem of computing a sparse representation of
multi-dimensional visual data is considered. In general, such data e.g.,
hyperspectral images, color images or video data consists of signals that
exhibit strong local dependencies. A new computationally efficient sparse
coding optimization problem is derived by employing regularization terms that
are adapted to the properties of the signals of interest. Exploiting the merits
of the learnable regularization techniques, a neural network is employed to act
as structure prior and reveal the underlying signal dependencies. To solve the
optimization problem Deep unrolling and Deep equilibrium based algorithms are
developed, forming highly interpretable and concise deep-learning-based
architectures, that process the input dataset in a block-by-block fashion.
Extensive simulation results, in the context of hyperspectral image denoising,
are provided, which demonstrate that the proposed algorithms outperform
significantly other sparse coding approaches and exhibit superior performance
against recent state-of-the-art deep-learning-based denoising models. In a
wider perspective, our work provides a unique bridge between a classic
approach, that is the sparse representation theory, and modern representation
tools that are based on deep learning modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring hand use in the home after cervical spinal cord injury using egocentric video. (arXiv:2203.16996v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16996">
<div class="article-summary-box-inner">
<span><p>Background: Egocentric video has recently emerged as a potential solution for
monitoring hand function in individuals living with tetraplegia in the
community, especially for its ability to detect functional use in the home
environment. Objective: To develop and validate a wearable vision-based system
for measuring hand use in the home among individuals living with tetraplegia.
Methods: Several deep learning algorithms for detecting functional hand-object
interactions were developed and compared. The most accurate algorithm was used
to extract measures of hand function from 65 hours of unscripted video recorded
at home by 20 participants with tetraplegia. These measures were: the
percentage of interaction time over total recording time (Perc); the average
duration of individual interactions (Dur); the number of interactions per hour
(Num). To demonstrate the clinical validity of the technology, egocentric
measures were correlated with validated clinical assessments of hand function
and independence (Graded Redefined Assessment of Strength, Sensibility and
Prehension - GRASSP, Upper Extremity Motor Score - UEMS, and Spinal Cord
Independent Measure - SCIM). Results: Hand-object interactions were
automatically detected with a median F1-score of 0.80 (0.67-0.87). Our results
demonstrated that higher UEMS and better prehension were related to greater
time spent interacting, whereas higher SCIM and better hand sensation resulted
in a higher number of interactions performed during the egocentric video
recordings. Conclusions: For the first time, measures of hand function
automatically estimated in an unconstrained environment in individuals with
tetraplegia have been validated against internationally accepted measures of
hand function. Future work will necessitate a formal evaluation of the
reliability and responsiveness of the egocentric-based performance measures for
hand use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Constrained Dynamic Correlations in Spatiotemporal Graphs for Motion Prediction. (arXiv:2204.01297v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01297">
<div class="article-summary-box-inner">
<span><p>Human motion prediction is a challenging task due to the dynamic
spatiotemporal correlations in different motion sequences. How to efficiently
represent spatiotemporal correlations and model dynamic correlation variances
between different motion sequences is a challenge for spatiotemporal
representation in motion prediction. In this work, we propose Dynamic
SpatioTemporal Decompose Graph Convolution (DSTD-GC), which decomposes dynamic
spatiotemporal graph modeling with a combination of Dynamic Spatial Graph
Convolution (DS-GC) and Dynamic Temporal Graph Convolution (DT-GC). The dynamic
spatial/temporal correlations in DS-GC/DT-GC are efficiently represented by
Constrained Dynamic Correlation Modeling, which is inspired by the common
constraints in human motion like body connections and dynamic patterns from
different samples. The Constrained Dynamic Correlation Modeling represents the
spatial/temporal graph as a combination of a shared spatial/temporal
correlation and an unshared correlation extraction function. This
spatiotemporal representation is of square space complexity and only requires
28.6% parameters of the state-of-the-art sample-shared decomposition
representation. It also explicitly models sample-specific spatiotemporal
correlation variances. Moreover, we also mathematically reformulate graph
convolutions on spatiotemporal graphs into a unified form and find that DSTD-GC
relaxes certain constraints of other graph convolutions, which leads to a
stronger representation capability. Combining DSTD-GC with prior knowledge like
body connection and temporal context, we propose a powerful spatiotemporal
graph convolution network called DSTD-GCN. On the Human3.6M and CMU Mocap
datasets, DSTD-GCN outperforms state-of-the-art methods by 3.9% - 5.7% in
prediction accuracy with 55.0% - 96.9% parameter reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving ImageNet: a Unified Scheme for Training any Backbone to Top Results. (arXiv:2204.03475v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03475">
<div class="article-summary-box-inner">
<span><p>ImageNet serves as the primary dataset for evaluating the quality of
computer-vision models. The common practice today is training each architecture
with a tailor-made scheme, designed and tuned by an expert. In this paper, we
present a unified scheme for training any backbone on ImageNet. The scheme,
named USI (Unified Scheme for ImageNet), is based on knowledge distillation and
modern tricks. It requires no adjustments or hyper-parameters tuning between
different models, and is efficient in terms of training times. We test USI on a
wide variety of architectures, including CNNs, Transformers, Mobile-oriented
and MLP-only. On all models tested, USI outperforms previous state-of-the-art
results. Hence, we are able to transform training on ImageNet from an
expert-oriented task to an automatic seamless routine. Since USI accepts any
backbone and trains it to top results, it also enables to perform methodical
comparisons, and identify the most efficient backbones along the speed-accuracy
Pareto curve. Implementation is available
at:https://github.com/Alibaba-MIIL/Solving_ImageNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmentation-Consistent Probabilistic Lesion Counting. (arXiv:2204.05276v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05276">
<div class="article-summary-box-inner">
<span><p>Lesion counts are important indicators of disease severity, patient
prognosis, and treatment efficacy, yet counting as a task in medical imaging is
often overlooked in favor of segmentation. This work introduces a novel
continuously differentiable function that maps lesion segmentation predictions
to lesion count probability distributions in a consistent manner. The proposed
end-to-end approach--which consists of voxel clustering, lesion-level voxel
probability aggregation, and Poisson-binomial counting--is non-parametric and
thus offers a robust and consistent way to augment lesion segmentation models
with post hoc counting capabilities. Experiments on Gadolinium-enhancing lesion
counting demonstrate that our method outputs accurate and well-calibrated count
distributions that capture meaningful uncertainty information. They also reveal
that our model is suitable for multi-task learning of lesion segmentation, is
efficient in low data regimes, and is robust to adversarial attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monocular Depth Estimation Using Cues Inspired by Biological Vision Systems. (arXiv:2204.10384v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10384">
<div class="article-summary-box-inner">
<span><p>Monocular depth estimation (MDE) aims to transform an RGB image of a scene
into a pixelwise depth map from the same camera view. It is fundamentally
ill-posed due to missing information: any single image can have been taken from
many possible 3D scenes. Part of the MDE task is, therefore, to learn which
visual cues in the image can be used for depth estimation, and how. With
training data limited by cost of annotation or network capacity limited by
computational power, this is challenging. In this work we demonstrate that
explicitly injecting visual cue information into the model is beneficial for
depth estimation. Following research into biological vision systems, we focus
on semantic information and prior knowledge of object sizes and their
relations, to emulate the biological cues of relative size, familiar size, and
absolute size. We use state-of-the-art semantic and instance segmentation
models to provide external information, and exploit language embeddings to
encode relational information between classes. We also provide a prior on the
average real-world size of objects. This external information overcomes the
limitation in data availability, and ensures that the limited capacity of a
given network is focused on known-helpful cues, therefore improving
performance. We experimentally validate our hypothesis and evaluate the
proposed model on the widely used NYUD2 indoor depth estimation benchmark. The
results show improvements in depth prediction when the semantic information,
size prior and instance size are explicitly provided along with the RGB images,
and our method can be easily adapted to any depth estimation system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ultra-fast image categorization in vivo and in silico. (arXiv:2205.03635v2 [q-bio.NC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03635">
<div class="article-summary-box-inner">
<span><p>Humans are able to robustly categorize images and can, for instance, detect
the presence of an animal in a briefly flashed image in as little as 120 ms.
Initially inspired by neuroscience, deep-learning algorithms literally bloomed
up in the last decade such that the accuracy of machines is at present superior
to humans for visual recognition tasks. However, these artificial networks are
usually trained and evaluated on very specific tasks, for instance on the 1000
separate categories of ImageNet. In that regard, biological visual systems are
more flexible and efficient compared to artificial systems on generic
ecological tasks. In order to deepen this comparison, we re-trained the
standard VGG Convolutional Neural Network (CNN) on two independent tasks which
are ecologically relevant for humans: one task defined as detecting the
presence of an animal and the other as detecting the presence of an artifact.
We show that retraining the network achieves human-like performance level which
is reported in psychophysical tasks. We also compare the accuracy of the
detection on an image-by-image basis. This showed in particular that the two
models perform better when combining their outputs. Indeed, animals (e.g.
lions) tend to be less present in photographs containing artifacts (e.g.
buildings). These re-trained models could reproduce some unexpected behavioral
observations from humans psychophysics such as the robustness to rotations
(e.g. upside-down or slanted image) or to a grayscale transformation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-supervised segmentation of referring expressions. (arXiv:2205.04725v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04725">
<div class="article-summary-box-inner">
<span><p>Visual grounding localizes regions (boxes or segments) in the image
corresponding to given referring expressions. In this work we address image
segmentation from referring expressions, a problem that has so far only been
addressed in a fully-supervised setting. A fully-supervised setup, however,
requires pixel-wise supervision and is hard to scale given the expense of
manual annotation. We therefore introduce a new task of weakly-supervised image
segmentation from referring expressions and propose Text grounded semantic
SEGgmentation (TSEG) that learns segmentation masks directly from image-level
referring expressions without pixel-level annotations. Our transformer-based
method computes patch-text similarities and guides the classification objective
during training with a new multi-label patch assignment mechanism. The
resulting visual grounding model segments image regions corresponding to given
natural language expressions. Our approach TSEG demonstrates promising results
for weakly-supervised referring expression segmentation on the challenging
PhraseCut and RefCOCO datasets. TSEG also shows competitive performance when
evaluated in a zero-shot setting for semantic segmentation on Pascal VOC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Impact of Partial Occlusion on Pedestrian Detectability. (arXiv:2205.04812v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04812">
<div class="article-summary-box-inner">
<span><p>Robust detection of vulnerable road users is a safety critical requirement
for the deployment of autonomous vehicles in heterogeneous traffic. One of the
most complex outstanding challenges is that of partial occlusion where a target
object is only partially available to the sensor due to obstruction by another
foreground object. A number of leading pedestrian detection benchmarks provide
annotation for partial occlusion, however each benchmark varies greatly in
their definition of the occurrence and severity of occlusion. Recent research
demonstrates that a high degree of subjectivity is used to classify occlusion
level in these cases and occlusion is typically categorized into 2 to 3 broad
categories such as partially and heavily occluded. This can lead to inaccurate
or inconsistent reporting of pedestrian detection model performance depending
on which benchmark is used. This research introduces a novel, objective
benchmark for partially occluded pedestrian detection to facilitate the
objective characterization of pedestrian detection models. Characterization is
carried out on seven popular pedestrian detection models for a range of
occlusion levels from 0-99%. Results demonstrate that pedestrian detection
performance degrades, and the number of false negative detections increase as
pedestrian occlusion level increases. Of the seven popular pedestrian detection
routines characterized, CenterNet has the greatest overall performance,
followed by SSDlite. RetinaNet has the lowest overall detection performance
across the range of occlusion levels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face Detection on Mobile: Five Implementations and Analysis. (arXiv:2205.05572v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05572">
<div class="article-summary-box-inner">
<span><p>In many practical cases face detection on smartphones or other highly
portable devices is a necessity. Applications include mobile face access
control systems, driver status tracking, emotion recognition, etc. Mobile
devices have limited processing power and should have long-enough battery life
even with face detection application running. Thus, striking the right balance
between algorithm quality and complexity is crucial. In this work we adapt 5
algorithms to mobile. These algorithms are based on handcrafted or
neural-network-based features and include: Viola-Jones (Haar cascade), LBP,
HOG, MTCNN, BlazeFace. We analyze inference time of these algorithms on
different devices with different input image resolutions. We provide guidance,
which algorithms are the best fit for mobile face access control systems and
potentially other mobile applications. Interestingly, we note that cascaded
algorithms perform faster on scenes without faces, while BlazeFace is slower on
empty scenes. Exploiting this behavior might be useful in practice.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-05-14 23:07:46.723743748 UTC">2022-05-14 23:07:46 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>