<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-10-12T01:30:00Z">10-12</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">DPUV3INT8: A Compiler View to programmable FPGA Inference Engines. (arXiv:2110.04327v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04327">
<div class="article-summary-box-inner">
<span><p>We have a FPGA design, we make it fast, efficient, and tested for a few
important examples. Now we must infer a general solution to deploy in the data
center. Here, we describe the FPGA DPUV3INT8 design and our compiler effort.
The hand-tuned SW-HW solution for Resnet50\_v1 has (close to) 2 times better
images per second (throughput) than our best FPGA implementation; the compiler
generalizes the hand written techniques achieving about 1.5 times better
performance for the same example, the compiler generalizes the optimizations to
a model zoo of networks, and it achieves 80+\% HW efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering. (arXiv:2110.04330v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04330">
<div class="article-summary-box-inner">
<span><p>Current Open-Domain Question Answering (ODQA) model paradigm often contains a
retrieving module and a reading module. Given an input question, the reading
module predicts the answer from the relevant passages which are retrieved by
the retriever. The recent proposed Fusion-in-Decoder (FiD), which is built on
top of the pretrained generative model T5, achieves the state-of-the-art
performance in the reading module. Although being effective, it remains
constrained by inefficient attention on all retrieved passages which contain a
lot of noise. In this work, we propose a novel method KG-FiD, which filters
noisy passages by leveraging the structural relationship among the retrieved
passages with a knowledge graph. We initiate the passage node embedding from
the FiD encoder and then use graph neural network (GNN) to update the
representation for reranking. To improve the efficiency, we build the GNN on
top of the intermediate layer output of the FiD encoder and only pass a few top
reranked passages into the higher layers of encoder and decoder for answer
generation. We also apply the proposed GNN based reranking method to enhance
the passage retrieval results in the retrieving module. Extensive experiments
on common ODQA benchmark datasets (Natural Question and TriviaQA) demonstrate
that KG-FiD can improve vanilla FiD by up to 1.5% on answer exact match score
and achieve comparable performance with FiD with only 40% of computation cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Describe Solutions for Bug Reports Based on Developer Discussions. (arXiv:2110.04353v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04353">
<div class="article-summary-box-inner">
<span><p>When a software bug is reported, developers engage in a discussion to
collaboratively resolve it. While the solution is likely formulated within the
discussion, it is often buried in a large amount of text, making it difficult
to comprehend, which delays its implementation. To expedite bug resolution, we
propose generating a concise natural language description of the solution by
synthesizing relevant content within the discussion, which encompasses both
natural language and source code. Furthermore, to support generating an
informative description during an ongoing discussion, we propose a secondary
task of determining when sufficient context about the solution emerges in
real-time. We construct a dataset for these tasks with a novel technique for
obtaining noisy supervision from repository changes linked to bug reports. We
establish baselines for generating solution descriptions, and develop a
classifier which makes a prediction following each new utterance on whether or
not the necessary context for performing generation is available. Through
automated and human evaluation, we find these tasks to form an ideal testbed
for complex reasoning in long, bimodal dialogue context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Unified View of Parameter-Efficient Transfer Learning. (arXiv:2110.04366v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04366">
<div class="article-summary-box-inner">
<span><p>Fine-tuning large pre-trained language models on downstream tasks has become
the de-facto learning paradigm in NLP. However, conventional approaches
fine-tune all the parameters of the pre-trained model, which becomes
prohibitive as the model size and the number of tasks grow. Recent work has
proposed a variety of parameter-efficient transfer learning methods that only
fine-tune a small number of (extra) parameters to attain strong performance.
While effective, the critical ingredients for success and the connections among
the various methods are poorly understood. In this paper, we break down the
design of state-of-the-art parameter-efficient transfer learning methods and
present a unified framework that establishes connections between them.
Specifically, we re-frame them as modifications to specific hidden states in
pre-trained models, and define a set of design dimensions along which different
methods vary, such as the function to compute the modification and the position
to apply the modification. Through comprehensive empirical studies across
machine translation, text summarization, language understanding, and text
classification benchmarks, we utilize the unified view to identify important
design choices in previous methods. Furthermore, our unified framework enables
the transfer of design elements across different approaches, and as a result we
are able to instantiate new parameter-efficient fine-tuning methods that tune
less parameters than previous methods while being more effective, achieving
comparable results to fine-tuning all parameters on all four tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Few More Examples May Be Worth Billions of Parameters. (arXiv:2110.04374v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04374">
<div class="article-summary-box-inner">
<span><p>We investigate the dynamics of increasing the number of model parameters
versus the number of labeled examples across a wide variety of tasks. Our
exploration reveals that while scaling parameters consistently yields
performance improvements, the contribution of additional examples highly
depends on the task's format. Specifically, in open question answering tasks,
enlarging the training set does not improve performance. In contrast,
classification, extractive question answering, and multiple choice tasks
benefit so much from additional examples that collecting a few hundred examples
is often "worth" billions of parameters. We hypothesize that unlike open
question answering, which involves recalling specific information, solving
strategies for tasks with a more restricted output space transfer across
examples, and can therefore be learned with small amounts of labeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Summarization Systems across Gender, Age, and Race. (arXiv:2110.04384v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04384">
<div class="article-summary-box-inner">
<span><p>Summarization systems are ultimately evaluated by human annotators and
raters. Usually, annotators and raters do not reflect the demographics of end
users, but are recruited through student populations or crowdsourcing platforms
with skewed demographics. For two different evaluation scenarios -- evaluation
against gold summaries and system output ratings -- we show that summary
evaluation is sensitive to protected attributes. This can severely bias system
development and evaluation, leading us to build models that cater for some
groups rather than others.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Eval4NLP Shared Task on Explainable Quality Estimation: Overview and Results. (arXiv:2110.04392v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04392">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce the Eval4NLP-2021shared task on explainable
quality estimation. Given a source-translation pair, this shared task requires
not only to provide a sentence-level score indicating the overall quality of
the translation, but also to explain this score by identifying the words that
negatively impact translation quality. We present the data, annotation
guidelines and evaluation setup of the shared task, describe the six
participating systems, and analyze the results. To the best of our knowledge,
this is the first shared task on explainable NLP evaluation metrics. Datasets
and results are available at https://github.com/eval4nlp/SharedTask2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Explainability of BERT-Based Evaluation Metrics by Disentangling along Linguistic Factors. (arXiv:2110.04399v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04399">
<div class="article-summary-box-inner">
<span><p>Evaluation metrics are a key ingredient for progress of text generation
systems. In recent years, several BERT-based evaluation metrics have been
proposed (including BERTScore, MoverScore, BLEURT, etc.) which correlate much
better with human assessment of text generation quality than BLEU or ROUGE,
invented two decades ago. However, little is known what these metrics, which
are based on black-box language model representations, actually capture (it is
typically assumed they model semantic similarity). In this work, we \wei{use a
simple regression based global explainability technique to} disentangle metric
scores along linguistic factors, including semantics, syntax, morphology, and
lexical overlap. We show that the different metrics capture all aspects to some
degree, but that they are all substantially sensitive to lexical overlap, just
like BLEU and ROUGE. This exposes limitations of these novelly proposed
metrics, which we also highlight in an adversarial test scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HydraSum -- Disentangling Stylistic Features in Text Summarization using Multi-Decoder Models. (arXiv:2110.04400v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04400">
<div class="article-summary-box-inner">
<span><p>Existing abstractive summarization models lack explicit control mechanisms
that would allow users to influence the stylistic features of the model
outputs. This results in generating generic summaries that do not cater to the
users needs or preferences. To address this issue we introduce HydraSum, a new
summarization architecture that extends the single decoder framework of current
models, e.g. BART, to a mixture-of-experts version consisting of multiple
decoders. Our proposed model encourages each expert, i.e. decoder, to learn and
generate stylistically-distinct summaries along dimensions such as
abstractiveness, length, specificity, and others. At each time step, HydraSum
employs a gating mechanism that decides the contribution of each individual
decoder to the next token's output probability distribution. Through
experiments on three summarization datasets (CNN, Newsroom, XSum), we
demonstrate that this gating mechanism automatically learns to assign
contrasting summary styles to different HydraSum decoders under the standard
training objective without the need for additional supervision. We further show
that a guided version of the training process can explicitly govern which
summary style is partitioned between decoders, e.g. high abstractiveness vs.
low abstractiveness or high specificity vs. low specificity, and also increase
the stylistic-difference between individual decoders. Finally, our experiments
demonstrate that our decoder framework is highly flexible: during inference, we
can sample from individual decoders or mixtures of different subsets of the
decoders to yield a diverse set of summaries and enforce single- and
multi-style control over summary generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content. (arXiv:2110.04406v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04406">
<div class="article-summary-box-inner">
<span><p>Natural language descriptions sometimes accompany visualizations to better
communicate and contextualize their insights, and to improve their
accessibility for readers with disabilities. However, it is difficult to
evaluate the usefulness of these descriptions, and how effectively they improve
access to meaningful information, because we have little understanding of the
semantic content they convey, and how different readers receive this content.
In response, we introduce a conceptual model for the semantic content conveyed
by natural language descriptions of visualizations. Developed through a
grounded theory analysis of 2,147 sentences, our model spans four levels of
semantic content: enumerating visualization construction properties (e.g.,
marks and encodings); reporting statistical concepts and relations (e.g.,
extrema and correlations); identifying perceptual and cognitive phenomena
(e.g., complex trends and patterns); and elucidating domain-specific insights
(e.g., social and political context). To demonstrate how our model can be
applied to evaluate the effectiveness of visualization descriptions, we conduct
a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that
these reader groups differ significantly on which semantic content they rank as
most useful. Together, our model and findings suggest that access to meaningful
information is strongly reader-specific, and that research in automatic
visualization captioning should orient toward descriptions that more richly
communicate overall trends and statistics, sensitive to reader preferences. Our
work further opens a space of research on natural language as a data interface
coequal with visualization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Community Sensitive Norm Violations in Online Conversations. (arXiv:2110.04419v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04419">
<div class="article-summary-box-inner">
<span><p>Online platforms and communities establish their own norms that govern what
behavior is acceptable within the community. Substantial effort in NLP has
focused on identifying unacceptable behaviors and, recently, on forecasting
them before they occur. However, these efforts have largely focused on toxicity
as the sole form of community norm violation. Such focus has overlooked the
much larger set of rules that moderators enforce. Here, we introduce a new
dataset focusing on a more complete spectrum of community norms and their
violations in the local conversational and global community contexts. We
introduce a series of models that use this data to develop context- and
community-sensitive norm violation detection, showing that these changes give
high performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning. (arXiv:2110.04429v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04429">
<div class="article-summary-box-inner">
<span><p>Distantly supervised named entity recognition (DS-NER) efficiently reduces
labor costs but meanwhile intrinsically suffers from the label noise due to the
strong assumption of distant supervision. Typically, the wrongly labeled
instances comprise numbers of incomplete and inaccurate annotation noise, while
most prior denoising works are only concerned with one kind of noise and fail
to fully explore useful information in the whole training set. To address this
issue, we propose a robust learning paradigm named Self-Collaborative Denoising
Learning (SCDL), which jointly trains two teacher-student networks in a
mutually-beneficial manner to iteratively perform noisy label refinery. Each
network is designed to exploit reliable labels via self denoising, and two
networks communicate with each other to explore unreliable annotations by
collaborative denoising. Extensive experimental results on five real-world
datasets demonstrate that SCDL is superior to state-of-the-art DS-NER denoising
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-stage Visual Cues Enhancement Network for Referring Image Segmentation. (arXiv:2110.04435v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04435">
<div class="article-summary-box-inner">
<span><p>Referring Image Segmentation (RIS) aims at segmenting the target object from
an image referred by one given natural language expression. The diverse and
flexible expressions as well as complex visual contents in the images raise the
RIS model with higher demands for investigating fine-grained matching behaviors
between words in expressions and objects presented in images. However, such
matching behaviors are hard to be learned and captured when the visual cues of
referents (i.e. referred objects) are insufficient, as the referents with weak
visual cues tend to be easily confused by cluttered background at boundary or
even overwhelmed by salient objects in the image. And the insufficient visual
cues issue can not be handled by the cross-modal fusion mechanisms as done in
previous work. In this paper, we tackle this problem from a novel perspective
of enhancing the visual information for the referents by devising a Two-stage
Visual cues enhancement Network (TV-Net), where a novel Retrieval and
Enrichment Scheme (RES) and an Adaptive Multi-resolution feature Fusion (AMF)
module are proposed. Through the two-stage enhancement, our proposed TV-Net
enjoys better performances in learning fine-grained matching behaviors between
the natural language expression and image, especially when the visual
information of the referent is inadequate, thus produces better segmentation
results. Extensive experiments are conducted to validate the effectiveness of
the proposed method on the RIS task, with our proposed TV-Net surpassing the
state-of-the-art approaches on four benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language for Human-Robot Collaboration: Problems Beyond Language Grounding. (arXiv:2110.04441v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04441">
<div class="article-summary-box-inner">
<span><p>To enable robots to instruct humans in collaborations, we identify several
aspects of language processing that are not commonly studied in this context.
These include location, planning, and generation. We suggest evaluations for
each task, offer baselines for simple methods, and close by discussing
challenges and opportunities in studying language for collaboration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging recent advances in Pre-Trained Language Models forEye-Tracking Prediction. (arXiv:2110.04475v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04475">
<div class="article-summary-box-inner">
<span><p>Cognitively inspired Natural Language Pro-cessing uses human-derived
behavioral datalike eye-tracking data, which reflect the seman-tic
representations of language in the humanbrain to augment the neural nets to
solve arange of tasks spanning syntax and semanticswith the aim of teaching
machines about lan-guage processing mechanisms. In this paper,we use the ZuCo
1.0 and ZuCo 2.0 dataset con-taining the eye-gaze features to explore
differ-ent linguistic models to directly predict thesegaze features for each
word with respect to itssentence. We tried different neural networkmodels with
the words as inputs to predict thetargets. And after lots of experimentation
andfeature engineering finally devised a novel ar-chitecture consisting of
RoBERTa Token Clas-sifier with a dense layer on top for languagemodeling and a
stand-alone model consistingof dense layers followed by a transformer layerfor
the extra features we engineered. Finally,we took the mean of the outputs of
both thesemodels to make the final predictions. We eval-uated the models using
mean absolute error(MAE) and the R2 score for each target.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian Active Summarization. (arXiv:2110.04480v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04480">
<div class="article-summary-box-inner">
<span><p>Bayesian Active Learning has had significant impact to various NLP problems,
but nevertheless it's application to text summarization has been explored very
little. We introduce Bayesian Active Summarization (BAS), as a method of
combining active learning methods with state-of-the-art summarization models.
Our findings suggest that BAS achieves better and more robust performance,
compared to random selection, particularly for small and very small data
annotation budgets. Using BAS we showcase it is possible to leverage large
summarization models to effectively solve real-world problems with very limited
annotated data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Lifelong Learning of Multilingual Text-To-Speech Synthesis. (arXiv:2110.04482v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04482">
<div class="article-summary-box-inner">
<span><p>This work presents a lifelong learning approach to train a multilingual
Text-To-Speech (TTS) system, where each language was seen as an individual task
and was learned sequentially and continually. It does not require pooled data
from all languages altogether, and thus alleviates the storage and computation
burden. One of the challenges of lifelong learning methods is "catastrophic
forgetting": in TTS scenario it means that model performance quickly degrades
on previous languages when adapted to a new language. We approach this problem
via a data-replay-based lifelong learning method. We formulate the replay
process as a supervised learning problem, and propose a simple yet effective
dual-sampler framework to tackle the heavily language-imbalanced training
samples. Through objective and subjective evaluations, we show that this
supervised learning formulation outperforms other gradient-based and
regularization-based lifelong learning methods, achieving 43% Mel-Cepstral
Distortion reduction compared to a fine-tuning baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wav2vec-S: Semi-Supervised Pre-Training for Speech Recognition. (arXiv:2110.04484v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04484">
<div class="article-summary-box-inner">
<span><p>Self-supervised pre-training has dramatically improved the performance of
automatic speech recognition (ASR). However, most existing self-supervised
pre-training approaches are task-agnostic, i.e., could be applied to various
downstream tasks. And there is a gap between the task-agnostic pre-training and
the task-specific downstream fine-tuning, which may degrade the downstream
performance. In this work, we propose a novel pre-training paradigm called
wav2vec-S, where we use task-specific semi-supervised pre-training to bridge
this gap. Specifically, the semi-supervised pre-training is conducted on the
basis of self-supervised pre-training such as wav2vec 2.0. Experiments on ASR
show that compared to wav2vec 2.0, wav2vec-S only requires marginal increment
of pre-training time but could significantly improve ASR performance on
in-domain, cross-domain and cross-lingual datasets. The average relative WER
reductions are 26.3% and 6.3% for 1h and 10h fine-tuning, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PAMA-TTS: Progression-Aware Monotonic Attention for Stable Seq2Seq TTS With Accurate Phoneme Duration Control. (arXiv:2110.04486v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04486">
<div class="article-summary-box-inner">
<span><p>Sequence expansion between encoder and decoder is a critical challenge in
sequence-to-sequence TTS. Attention-based methods achieve great naturalness but
suffer from unstable issues like missing and repeating phonemes, not to mention
accurate duration control. Duration-informed methods, on the contrary, seem to
easily adjust phoneme duration but show obvious degradation in speech
naturalness. This paper proposes PAMA-TTS to address the problem. It takes the
advantage of both flexible attention and explicit duration models. Based on the
monotonic attention mechanism, PAMA-TTS also leverages token duration and
relative position of a frame, especially countdown information, i.e. in how
many future frames the present phoneme will end. They help the attention to
move forward along the token sequence in a soft but reliable control.
Experimental results prove that PAMA-TTS achieves the highest naturalness,
while has on-par or even better duration controllability than the
duration-informed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Isotropy Analysis in the Multilingual BERT Embedding Space. (arXiv:2110.04504v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04504">
<div class="article-summary-box-inner">
<span><p>Several studies have explored various advantages of multilingual pre-trained
models (e.g., multilingual BERT) in capturing shared linguistic knowledge.
However, their limitations have not been paid enough attention. In this paper,
we investigate the representation degeneration problem in multilingual
contextual word representations (CWRs) of BERT and show that the embedding
spaces of the selected languages suffer from anisotropy problem. Our
experimental results demonstrate that, similarly to their monolingual
counterparts, increasing the isotropy of multilingual embedding space can
significantly improve its representation power and performance. Our analysis
indicates that although the degenerated directions vary in different languages,
they encode similar linguistic knowledge, suggesting a shared linguistic space
among languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extending Multi-Text Sentence Fusion Resources via Pyramid Annotations. (arXiv:2110.04517v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04517">
<div class="article-summary-box-inner">
<span><p>NLP models that compare or consolidate information across multiple documents
often struggle when challenged with recognizing substantial information
redundancies across the texts. For example, in multi-document summarization it
is crucial to identify salient information across texts and then generate a
non-redundant summary, while facing repeated and usually differently-phrased
salient content. To facilitate researching such challenges, the sentence-level
task of \textit{sentence fusion} was proposed, yet previous datasets for this
task were very limited in their size and scope. In this paper, we revisit and
substantially extend previous dataset creation efforts. With careful
modifications, relabeling and employing complementing data sources, we were
able to triple the size of a notable earlier dataset. Moreover, we show that
our extended version uses more representative texts for multi-document tasks
and provides a larger and more diverse training set, which substantially
improves model training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DMRST: A Joint Framework for Document-Level Multilingual RST Discourse Segmentation and Parsing. (arXiv:2110.04518v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04518">
<div class="article-summary-box-inner">
<span><p>Text discourse parsing weighs importantly in understanding information flow
and argumentative structure in natural language, making it beneficial for
downstream tasks. While previous work significantly improves the performance of
RST discourse parsing, they are not readily applicable to practical use cases:
(1) EDU segmentation is not integrated into most existing tree parsing
frameworks, thus it is not straightforward to apply such models on newly-coming
data. (2) Most parsers cannot be used in multilingual scenarios, because they
are developed only in English. (3) Parsers trained from single-domain treebanks
do not generalize well on out-of-domain inputs. In this work, we propose a
document-level multilingual RST discourse parsing framework, which conducts EDU
segmentation and discourse tree parsing jointly. Moreover, we propose a
cross-translation augmentation strategy to enable the framework to support
multilingual parsing and improve its domain generality. Experimental results
show that our model achieves state-of-the-art performance on document-level
multilingual RST parsing in all sub-tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rumor Detection on Twitter with Claim-Guided Hierarchical Graph Attention Networks. (arXiv:2110.04522v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04522">
<div class="article-summary-box-inner">
<span><p>Rumors are rampant in the era of social media. Conversation structures
provide valuable clues to differentiate between real and fake claims. However,
existing rumor detection methods are either limited to the strict relation of
user responses or oversimplify the conversation structure. In this study, to
substantially reinforces the interaction of user opinions while alleviating the
negative impact imposed by irrelevant posts, we first represent the
conversation thread as an undirected interaction graph. We then present a
Claim-guided Hierarchical Graph Attention Network for rumor classification,
which enhances the representation learning for responsive posts considering the
entire social contexts and attends over the posts that can semantically infer
the target claim. Extensive experiments on three Twitter datasets demonstrate
that our rumor detection method achieves much better performance than
state-of-the-art methods and exhibits a superior capacity for detecting rumors
at early stages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Disentangled Arguments with Prompts: A Simple Event Extraction Framework that Works. (arXiv:2110.04525v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04525">
<div class="article-summary-box-inner">
<span><p>Event Extraction bridges the gap between text and event signals. Based on the
assumption of trigger-argument dependency, existing approaches have achieved
state-of-the-art performance with expert-designed templates or complicated
decoding constraints. In this paper, for the first time we introduce the
prompt-based learning strategy to the domain of Event Extraction, which
empowers the automatic exploitation of label semantics on both input and output
sides. To validate the effectiveness of the proposed generative method, we
conduct extensive experiments with 11 diverse baselines. Empirical results show
that, in terms of F1 score on Argument Extraction, our simple architecture is
stronger than any other generative counterpart and even competitive with
algorithms that require template engineering. Regarding the measure of recall,
it sets new overall records for both Argument and Trigger Extractions. We
hereby recommend this framework to the community, with the code publicly
available at https://git.io/GDAP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Multi-Party Dialogue Discourse Parsing via Domain Integration. (arXiv:2110.04526v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04526">
<div class="article-summary-box-inner">
<span><p>While multi-party conversations are often less structured than monologues and
documents, they are implicitly organized by semantic level correlations across
the interactive turns, and dialogue discourse analysis can be applied to
predict the dependency structure and relations between the elementary discourse
units, and provide feature-rich structural information for downstream tasks.
However, the existing corpora with dialogue discourse annotation are collected
from specific domains with limited sample sizes, rendering the performance of
data-driven approaches poor on incoming dialogues without any domain
adaptation. In this paper, we first introduce a Transformer-based parser, and
assess its cross-domain performance. We next adopt three methods to gain domain
integration from both data and language modeling perspectives to improve the
generalization capability. Empirical results show that the neural parser can
benefit from our proposed methods, and performs better on cross-domain dialogue
samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design. (arXiv:2110.04541v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04541">
<div class="article-summary-box-inner">
<span><p>Pretraining Neural Language Models (NLMs) over a large corpus involves
chunking the text into training examples, which are contiguous text segments of
sizes processable by the neural architecture. We highlight a bias introduced by
this common practice: we prove that the pretrained NLM can model much stronger
dependencies between text segments that appeared in the same training example,
than it can between text segments that appeared in different training examples.
This intuitive result has a twofold role. First, it formalizes the motivation
behind a broad line of recent successful NLM training heuristics, proposed for
the pretraining and fine-tuning stages, which do not necessarily appear related
at first glance. Second, our result clearly indicates further improvements to
be made in NLM pretraining for the benefit of Natural Language Understanding
tasks. As an example, we propose "kNN-Pretraining": we show that including
semantically related non-neighboring sentences in the same pretraining example
yields improved sentence representations and open domain question answering
abilities. This theoretically motivated degree of freedom for "pretraining
example design" indicates new training schemes for self-improving
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP-Adapter: Better Vision-Language Models with Feature Adapters. (arXiv:2110.04544v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04544">
<div class="article-summary-box-inner">
<span><p>Large-scale contrastive vision-language pre-training has shown significant
progress in visual representation learning. Unlike traditional visual systems
trained by a fixed set of discrete labels, a new paradigm was introduced in
\cite{radford2021learning} to directly learn to align images with raw texts in
an open-vocabulary setting. On downstream tasks, a carefully chosen text prompt
is employed to make zero-shot predictions.~To avoid non-trivial prompt
engineering, context optimization \cite{zhou2021coop} has been proposed to
learn continuous vectors as task-specific prompts with few-shot training
examples.~In this paper, we show that there is an alternative path to achieve
better vision-language models other than prompt tuning.~While prompt tuning is
for the textual inputs, we propose CLIP-Adapter to conduct fine-tuning with
feature adapters on either visual or language branch. Specifically,
CLIP-Adapter adopts an additional bottleneck layer to learn new features and
performs residual-style feature blending with the original pre-trained
features.~As a consequence, CLIP-Adapter is able to outperform context
optimization while maintains a simple design. Experiments and extensive
ablation studies on various visual classification tasks demonstrate the
effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Exploration of Self-Supervised Pretrained Representations for End-to-End Speech Recognition. (arXiv:2110.04590v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04590">
<div class="article-summary-box-inner">
<span><p>Self-supervised pretraining on speech data has achieved a lot of progress.
High-fidelity representation of the speech signal is learned from a lot of
untranscribed data and shows promising performance. Recently, there are several
works focusing on evaluating the quality of self-supervised pretrained
representations on various tasks without domain restriction, e.g. SUPERB.
However, such evaluations do not provide a comprehensive comparison among many
ASR benchmark corpora. In this paper, we focus on the general applications of
pretrained speech representations, on advanced end-to-end automatic speech
recognition (E2E-ASR) models. We select several pretrained speech
representations and present the experimental results on various open-source and
publicly available corpora for E2E-ASR. Without any modification of the
back-end model architectures or training strategy, some of the experiments with
pretrained representations, e.g., WSJ, WSJ0-2mix with HuBERT, reach or
outperform current state-of-the-art (SOTA) recognition performance. Moreover,
we further explore more scenarios for whether the pretraining representations
are effective, such as the cross-language or overlapped speech. The scripts,
configuratons and the trained models have been released in ESPnet to let the
community reproduce our experiments and improve them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalized Automatic Speech Recognition Trained on Small Disordered Speech Datasets. (arXiv:2110.04612v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04612">
<div class="article-summary-box-inner">
<span><p>This study investigates the performance of personalized automatic speech
recognition (ASR) for recognizing disordered speech using small amounts of
per-speaker adaptation data. We trained personalized models for 195 individuals
with different types and severities of speech impairment with training sets
ranging in size from &lt;1 minute to 18-20 minutes of speech data. Word error rate
(WER) thresholds were selected to determine Success Percentage (the percentage
of personalized models reaching the target WER) in different application
scenarios. For the home automation scenario, 79% of speakers reached the target
WER with 18-20 minutes of speech; but even with only 3-4 minutes of speech, 63%
of speakers reached the target WER. Further evaluation found similar
improvement on test sets with conversational and out-of-domain, unprompted
phrases. Our results demonstrate that with only a few minutes of recordings,
individuals with disordered speech could benefit from personalized ASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empathetic Response Generation through Graph-based Multi-hop Reasoning on Emotional Causality. (arXiv:2110.04614v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04614">
<div class="article-summary-box-inner">
<span><p>Empathetic response generation aims to comprehend the user emotion and then
respond to it appropriately. Most existing works merely focus on what the
emotion is and ignore how the emotion is evoked, thus weakening the capacity of
the model to understand the emotional experience of the user for generating
empathetic responses. To tackle this problem, we consider the emotional
causality, namely, what feelings the user expresses (i.e., emotion) and why the
user has such feelings (i.e., cause). Then, we propose a novel graph-based
model with multi-hop reasoning to model the emotional causality of the
empathetic conversation. Finally, we demonstrate the effectiveness of our model
on EMPATHETICDIALOGUES in comparison with several competitive models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Framework for Rationale Extraction for Deep QA models. (arXiv:2110.04620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04620">
<div class="article-summary-box-inner">
<span><p>As neural-network-based QA models become deeper and more complex, there is a
demand for robust frameworks which can access a model's rationale for its
prediction. Current techniques that provide insights on a model's working are
either dependent on adversarial datasets or are proposing models with explicit
explanation generation components. These techniques are time-consuming and
challenging to extend to existing models and new datasets. In this work, we use
`Integrated Gradients' to extract rationale for existing state-of-the-art
models in the task of Reading Comprehension based Question Answering (RCQA). On
detailed analysis and comparison with collected human rationales, we find that
though ~40-80% words of extracted rationale coincide with the human rationale
(precision), only 6-19% of human rationale is present in the extracted
rationale (recall).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Relation between Syntactic Divergence and Zero-Shot Performance. (arXiv:2110.04644v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04644">
<div class="article-summary-box-inner">
<span><p>We explore the link between the extent to which syntactic relations are
preserved in translation and the ease of correctly constructing a parse tree in
a zero-shot setting. While previous work suggests such a relation, it tends to
focus on the macro level and not on the level of individual edges-a gap we aim
to address. As a test case, we take the transfer of Universal Dependencies (UD)
parsing from English to a diverse set of languages and conduct two sets of
experiments. In one, we analyze zero-shot performance based on the extent to
which English source edges are preserved in translation. In another, we apply
three linguistically motivated transformations to UD, creating more
cross-lingually stable versions of it, and assess their zero-shot parsability.
In order to compare parsing performance across different schemes, we perform
extrinsic evaluation on the downstream task of cross-lingual relation
extraction (RE) using a subset of a popular English RE benchmark translated to
Russian and Korean. In both sets of experiments, our results suggest a strong
relation between cross-lingual stability and zero-shot parsing performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Follow Language Instructions with Compositional Policies. (arXiv:2110.04647v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04647">
<div class="article-summary-box-inner">
<span><p>We propose a framework that learns to execute natural language instructions
in an environment consisting of goal-reaching tasks that share components of
their task descriptions. Our approach leverages the compositionality of both
value functions and language, with the aim of reducing the sample complexity of
learning novel tasks. First, we train a reinforcement learning agent to learn
value functions that can be subsequently composed through a Boolean algebra to
solve novel tasks. Second, we fine-tune a seq2seq model pretrained on web-scale
corpora to map language to logical expressions that specify the required value
function compositions. Evaluating our agent in the BabyAI domain, we observe a
decrease of 86% in the number of training steps needed to learn a second task
after mastering a single task. Results from ablation studies further indicate
that it is the combination of compositional value functions and language
representations that allows the agent to quickly generalize to new tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangled Sequence to Sequence Learning for Compositional Generalization. (arXiv:2110.04655v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04655">
<div class="article-summary-box-inner">
<span><p>There is mounting evidence that existing neural network models, in particular
the very popular sequence-to-sequence architecture, struggle with compositional
generalization, i.e., the ability to systematically generalize to unseen
compositions of seen components. In this paper we demonstrate that one of the
reasons hindering compositional generalization relates to the representations
being entangled. We propose an extension to sequence-to-sequence models which
allows us to learn disentangled representations by adaptively re-encoding (at
each time step) the source input. Specifically, we condition the source
representations on the newly decoded target context which makes it easier for
the encoder to exploit specialized information for each prediction rather than
capturing all source information in a single forward pass. Experimental results
on semantic parsing and machine translation empirically show that our proposal
yields more disentangled representations and better generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Audio Captions Be Evaluated with Image Caption Metrics?. (arXiv:2110.04684v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04684">
<div class="article-summary-box-inner">
<span><p>Automated audio captioning aims at generating textual descriptions for an
audio clip. To evaluate the quality of generated audio captions, previous works
directly adopt image captioning metrics like SPICE and CIDEr, without
justifying their suitability in this new domain, which may mislead the
development of advanced models. This problem is still unstudied due to the lack
of human judgment datasets on caption quality. Therefore, we firstly construct
two evaluation benchmarks, AudioCaps-Eval and Clotho-Eval. They are established
with pairwise comparison instead of absolute rating to achieve better
inter-annotator agreement. Current metrics are found in poor correlation with
human annotations on these datasets. To overcome their limitations, we propose
a metric named FENSE, where we combine the strength of Sentence-BERT in
capturing similarity, and a novel Error Detector to penalize erroneous
sentences for robustness. On the newly established benchmarks, FENSE
outperforms current metrics by 14-25% accuracy. Code, data and web demo
available at: https://github.com/blmoistawinde/fense
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Channel End-to-End Neural Diarization with Distributed Microphones. (arXiv:2110.04694v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04694">
<div class="article-summary-box-inner">
<span><p>Recent progress on end-to-end neural diarization (EEND) has enabled
overlap-aware speaker diarization with a single neural network. This paper
proposes to enhance EEND by using multi-channel signals from distributed
microphones. We replace Transformer encoders in EEND with two types of encoders
that process a multi-channel input: spatio-temporal and co-attention encoders.
Both are independent of the number and geometry of microphones and suitable for
distributed microphone settings. We also propose a model adaptation method
using only single-channel recordings. With simulated and real-recorded
datasets, we demonstrated that the proposed method outperformed conventional
EEND when a multi-channel input was given while maintaining comparable
performance with a single-channel input. We also showed that the proposed
method performed well even when spatial information is inoperative given
multi-channel inputs, such as in hybrid meetings in which the utterances of
multiple remote participants are played back from the same loudspeaker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention in Natural Language Processing. (arXiv:1902.02181v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1902.02181">
<div class="article-summary-box-inner">
<span><p>Attention is an increasingly popular mechanism used in a wide range of neural
architectures. The mechanism itself has been realized in a variety of formats.
However, because of the fast-paced advances in this domain, a systematic
overview of attention is still missing. In this article, we define a unified
model for attention architectures in natural language processing, with a focus
on those designed to work with vector representations of the textual data. We
propose a taxonomy of attention models according to four dimensions: the
representation of the input, the compatibility function, the distribution
function, and the multiplicity of the input and/or output. We present the
examples of how prior information can be exploited in attention models and
discuss ongoing research efforts and open challenges in the area, providing the
first extensive categorization of the vast body of literature in this exciting
domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Relational Image Captioning via Multi-task Triple-Stream Networks. (arXiv:2010.03855v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03855">
<div class="article-summary-box-inner">
<span><p>We introduce dense relational captioning, a novel image captioning task which
aims to generate multiple captions with respect to relational information
between objects in a visual scene. Relational captioning provides explicit
descriptions for each relationship between object combinations. This framework
is advantageous in both diversity and amount of information, leading to a
comprehensive image understanding based on relationships, e.g., relational
proposal generation. For relational understanding between objects, the
part-of-speech (POS; i.e., subject-object-predicate categories) can be a
valuable prior information to guide the causal sequence of words in a caption.
We enforce our framework to learn not only to generate captions but also to
understand the POS of each word. To this end, we propose the multi-task
triple-stream network (MTTSNet) which consists of three recurrent units
responsible for each POS which is trained by jointly predicting the correct
captions and POS for each word. In addition, we found that the performance of
MTTSNet can be improved by modulating the object embeddings with an explicit
relational module. We demonstrate that our proposed model can generate more
diverse and richer captions, via extensive experimental analysis on large scale
datasets and several metrics. Then, we present applications of our framework to
holistic image captioning, scene graph generation, and retrieval tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MLQE-PE: A Multilingual Quality Estimation and Post-Editing Dataset. (arXiv:2010.04480v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.04480">
<div class="article-summary-box-inner">
<span><p>We present MLQE-PE, a new dataset for Machine Translation (MT) Quality
Estimation (QE) and Automatic Post-Editing (APE). The dataset contains eleven
language pairs, with human labels for up to 10,000 translations per language
pair in the following formats: sentence-level direct assessments and
post-editing effort, and word-level good/bad labels. It also contains the
post-edited sentences, as well as titles of the articles where the sentences
were extracted from, and the neural MT models used to translate the text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemMT: A Semantic-based Testing Approach for Machine Translation Systems. (arXiv:2012.01815v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01815">
<div class="article-summary-box-inner">
<span><p>Machine translation has wide applications in daily life. In mission-critical
applications such as translating official documents, incorrect translation can
have unpleasant or sometimes catastrophic consequences. This motivates recent
research on testing methodologies for machine translation systems. Existing
methodologies mostly rely on metamorphic relations designed at the textual
level (e.g., Levenshtein distance) or syntactic level (e.g., the distance
between grammar structures) to determine the correctness of translation
results. However, these metamorphic relations do not consider whether the
original and translated sentences have the same meaning (i.e., Semantic
similarity). Therefore, in this paper, we propose SemMT, an automatic testing
approach for machine translation systems based on semantic similarity checking.
SemMT applies round-trip translation and measures the semantic similarity
between the original and translated sentences. Our insight is that the
semantics expressed by the logic and numeric constraint in sentences can be
captured using regular expressions (or deterministic finite automata) where
efficient equivalence/similarity checking algorithms are available. Leveraging
the insight, we propose three semantic similarity metrics and implement them in
SemMT. The experiment result reveals SemMT can achieve higher effectiveness
compared with state-of-the-art works, achieving an increase of 21% and 23% on
accuracy and F-Score, respectively. We also explore potential improvements that
can be achieved when proper combinations of metrics are adopted. Finally, we
discuss a solution to locate the suspicious trip in round-trip translation,
which may shed lights on further exploration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Approximating How Single Head Attention Learns. (arXiv:2103.07601v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07601">
<div class="article-summary-box-inner">
<span><p>Why do models often attend to salient words, and how does this evolve
throughout training? We approximate model training as a two stage process:
early on in training when the attention weights are uniform, the model learns
to translate individual input word `i` to `o` if they co-occur frequently.
Later, the model learns to attend to `i` while the correct output is $o$
because it knows `i` translates to `o`. To formalize, we define a model
property, Knowledge to Translate Individual Words (KTIW) (e.g. knowing that `i`
translates to `o`), and claim that it drives the learning of the attention.
This claim is supported by the fact that before the attention mechanism is
learned, KTIW can be learned from word co-occurrence statistics, but not the
other way around. Particularly, we can construct a training distribution that
makes KTIW hard to learn, the learning of the attention fails, and the model
cannot even learn the simple task of copying the input words to the output. Our
approximation explains why models sometimes attend to salient words, and
inspires a toy example where a multi-head attention model can overcome the
above hard training distribution by improving learning dynamics rather than
expressiveness. We end by discussing the limitation of our approximation
framework and suggest future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LSTM Based Sentiment Analysis for Cryptocurrency Prediction. (arXiv:2103.14804v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14804">
<div class="article-summary-box-inner">
<span><p>Recent studies in big data analytics and natural language processing develop
automatic techniques in analyzing sentiment in the social media information. In
addition, the growing user base of social media and the high volume of posts
also provide valuable sentiment information to predict the price fluctuation of
the cryptocurrency. This research is directed to predicting the volatile price
movement of cryptocurrency by analyzing the sentiment in social media and
finding the correlation between them. While previous work has been developed to
analyze sentiment in English social media posts, we propose a method to
identify the sentiment of the Chinese social media posts from the most popular
Chinese social media platform Sina-Weibo. We develop the pipeline to capture
Weibo posts, describe the creation of the crypto-specific sentiment dictionary,
and propose a long short-term memory (LSTM) based recurrent neural network
along with the historical cryptocurrency price movement to predict the price
trend for future time frames. The conducted experiments demonstrate the
proposed approach outperforms the state of the art auto regressive based model
by 18.5% in precision and 15.4% in recall.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering. (arXiv:2104.06378v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06378">
<div class="article-summary-box-inner">
<span><p>The problem of answering questions using knowledge from pre-trained language
models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA
context (question and answer choice), methods need to (i) identify relevant
knowledge from large KGs, and (ii) perform joint reasoning over the QA context
and KG. In this work, we propose a new model, QA-GNN, which addresses the above
challenges through two key innovations: (i) relevance scoring, where we use LMs
to estimate the importance of KG nodes relative to the given QA context, and
(ii) joint reasoning, where we connect the QA context and KG to form a joint
graph, and mutually update their representations through graph neural networks.
We evaluate QA-GNN on the CommonsenseQA and OpenBookQA datasets, and show its
improvement over existing LM and LM+KG models, as well as its capability to
perform interpretable and structured reasoning, e.g., correctly handling
negation in questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph. (arXiv:2104.07302v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07302">
<div class="article-summary-box-inner">
<span><p>Multi-hop Question Answering (QA) is a challenging task because it requires
precise reasoning with entity relations at every step towards the answer. The
relations can be represented in terms of labels in knowledge graph (e.g.,
\textit{spouse}) or text in text corpus (e.g., \textit{they have been married
for 26 years}). Existing models usually infer the answer by predicting the
sequential relation path or aggregating the hidden graph features. The former
is hard to optimize, and the latter lacks interpretability. In this paper, we
propose TransferNet, an effective and transparent model for multi-hop QA, which
supports both label and text relations in a unified framework. TransferNet
jumps across entities at multiple steps. At each step, it attends to different
parts of the question, computes activated scores for relations, and then
transfer the previous entity scores along activated relations in a
differentiable way. We carry out extensive experiments on three datasets and
demonstrate that TransferNet surpasses the state-of-the-art models by a large
margin. In particular, on MetaQA, it achieves 100\% accuracy in 2-hop and 3-hop
questions. By qualitative analysis, we show that TransferNet has transparent
and interpretable intermediate results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation. (arXiv:2104.08200v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08200">
<div class="article-summary-box-inner">
<span><p>Natural language generation (NLG) benchmarks provide an important avenue to
measure progress and develop better NLG systems. Unfortunately, the lack of
publicly available NLG benchmarks for low-resource languages poses a
challenging barrier for building NLG systems that work well for languages with
limited amounts of data. Here we introduce IndoNLG, the first benchmark to
measure natural language generation (NLG) progress in three low-resource -- yet
widely spoken -- languages of Indonesia: Indonesian, Javanese, and Sundanese.
Altogether, these languages are spoken by more than 100 million native
speakers, and hence constitute an important use case of NLG systems today.
Concretely, IndoNLG covers six tasks: summarization, question answering,
chit-chat, and three different pairs of machine translation (MT) tasks. We
collate a clean pretraining corpus of Indonesian, Sundanese, and Javanese
datasets, Indo4B-Plus, which is used to pretrain our models: IndoBART and
IndoGPT. We show that IndoBART and IndoGPT achieve competitive performance on
all tasks -- despite using only one-fifth the parameters of a larger
multilingual model, mBART-LARGE (Liu et al., 2020). This finding emphasizes the
importance of pretraining on closely related, local languages to achieve more
efficient learning and faster inference for very low-resource languages like
Javanese and Sundanese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning. (arXiv:2104.08808v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08808">
<div class="article-summary-box-inner">
<span><p>The ability to continuously expand knowledge over time and utilize it to
rapidly generalize to new tasks is a key feature of human linguistic
intelligence. Existing models that pursue rapid generalization to new tasks
(e.g., few-shot learning methods), however, are mostly trained in a single shot
on fixed datasets, unable to dynamically expand their knowledge; while
continual learning algorithms are not specifically designed for rapid
generalization. We present a new learning setup, Continual Learning of Few-Shot
Learners (CLIF), to address the challenges of both learning settings in a
unified setup. CLIF assumes a model learns from a sequence of diverse NLP tasks
arriving sequentially, accumulating knowledge for improved generalization to
new tasks, while also retaining performance on the tasks learned earlier. We
examine how the generalization ability is affected in the continual learning
setup, evaluate a number of continual learning algorithms, and propose a novel
regularized adapter generation approach. We find that catastrophic forgetting
affects generalization ability to a less degree than performance on seen tasks;
while continual learning algorithms can still bring considerable benefit to the
generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoFormer: Enhanced Transformer with Rotary Position Embedding. (arXiv:2104.09864v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09864">
<div class="article-summary-box-inner">
<span><p>Position encoding in transformer architecture provides supervision for
dependency modeling between elements at different positions in the sequence. We
investigate various methods to encode positional information in
transformer-based language models and propose a novel implementation named
Rotary Position Embedding(RoPE). The proposed RoPE encodes absolute positional
information with rotation matrix and naturally incorporates explicit relative
position dependency in self-attention formulation. Notably, RoPE comes with
valuable properties such as flexibility of being expand to any sequence
lengths, decaying inter-token dependency with increasing relative distances,
and capability of equipping the linear self-attention with relative position
encoding. As a result, the enhanced transformer with rotary position embedding,
or RoFormer, achieves superior performance in tasks with long texts. We release
the theoretical analysis along with some preliminary experiment results on
Chinese data. The undergoing experiment for English benchmark will soon be
updated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Offensive Expressions of Opinion in Context. (arXiv:2104.12227v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12227">
<div class="article-summary-box-inner">
<span><p>Classic information extraction techniques consist in building questions and
answers about the facts. Indeed, it is still a challenge to subjective
information extraction systems to identify opinions and feelings in context. In
sentiment-based NLP tasks, there are few resources to information extraction,
above all offensive or hateful opinions in context. To fill this important gap,
this short paper provides a new cross-lingual and contextual offensive lexicon,
which consists of explicit and implicit offensive and swearing expressions of
opinion, which were annotated in two different classes: context dependent and
context-independent offensive. In addition, we provide markers to identify hate
speech. Annotation approach was evaluated at the expression-level and achieves
high human inter-annotator agreement. The provided offensive lexicon is
available in Portuguese and English languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Lexicon-Based Approach for Hate Speech and Offensive Language Detection. (arXiv:2104.12265v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12265">
<div class="article-summary-box-inner">
<span><p>This paper provides a new approach for offensive language and hate speech
detection on social media. Our approach incorporates an offensive lexicon
composed of implicit and explicit offensive and swearing expressions annotated
with binary classes: context-dependent and context-independent offensive. Due
to the severity of the hate speech and offensive comments in Brazil, and the
lack of research in Portuguese, Brazilian Portuguese is the language used to
validate the proposed method. Nevertheless, our proposal may be applied to any
other language or domain. Based on the obtained results, the proposed approach
showed high-performance overcoming the current baselines for European and
Brazilian Portuguese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WhatTheWikiFact: Fact-Checking Claims Against Wikipedia. (arXiv:2105.00826v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00826">
<div class="article-summary-box-inner">
<span><p>The rise of Internet has made it a major source of information.
Unfortunately, not all information online is true, and thus a number of
fact-checking initiatives have been launched, both manual and automatic, to
deal with the problem. Here, we present our contribution in this regard:
\emph{WhatTheWikiFact}, a system for automatic claim verification using
Wikipedia. The system can predict the veracity of an input claim, and it
further shows the evidence it has retrieved as part of the verification
process. It shows confidence scores and a list of relevant Wikipedia articles,
together with detailed information about each article, including the phrase
used to retrieve it, the most relevant sentences extracted from it and their
stance with respect to the input claim, as well as the associated
probabilities. The system supports several languages: Bulgarian, English, and
Russian.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Black or White but never neutral: How readers perceive identity from yellow or skin-toned emoji. (arXiv:2105.05887v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05887">
<div class="article-summary-box-inner">
<span><p>Research in sociology and linguistics shows that people use language not only
to express their own identity but to understand the identity of others. Recent
work established a connection between expression of identity and emoji usage on
social media, through use of emoji skin tone modifiers. Motivated by that
finding, this work asks if, as with language, readers are sensitive to such
acts of self-expression and use them to understand the identity of authors. In
behavioral experiments (n=488), where text and emoji content of social media
posts were carefully controlled before being presented to participants, we find
in the affirmative -- emoji are a salient signal of author identity. That
signal is distinct from, and complementary to, the one encoded in language.
Participant groups (based on self-identified ethnicity) showed no differences
in how they perceive this signal, except in the case of the default yellow
emoji. While both groups associate this with a White identity, the effect was
stronger in White participants. Our finding that emoji can index social
variables will have experimental applications for researchers but also
implications for designers: supposedly ``neutral`` defaults may be more
representative of some users than others.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Doc2Dict: Information Extraction as Text Generation. (arXiv:2105.07510v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07510">
<div class="article-summary-box-inner">
<span><p>Typically, information extraction (IE) requires a pipeline approach: first, a
sequence labeling model is trained on manually annotated documents to extract
relevant spans; then, when a new document arrives, a model predicts spans which
are then post-processed and standardized to convert the information into a
database entry. We replace this labor-intensive workflow with a transformer
language model trained on existing database records to directly generate
structured JSON. Our solution removes the workload associated with producing
token-level annotations and takes advantage of a data source which is generally
quite plentiful (e.g. database records). As long documents are common in
information extraction tasks, we use gradient checkpointing and chunked
encoding to apply our method to sequences of up to 32,000 tokens on a single
GPU. Our Doc2Dict approach is competitive with more complex, hand-engineered
pipelines and offers a simple but effective baseline for document-level
information extraction. We release our Doc2Dict model and code to reproduce our
experiments and facilitate future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable agent communication from scratch (with a generic visual processor emerging on the side). (arXiv:2106.04258v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04258">
<div class="article-summary-box-inner">
<span><p>As deep networks begin to be deployed as autonomous agents, the issue of how
they can communicate with each other becomes important. Here, we train two deep
nets from scratch to perform realistic referent identification through
unsupervised emergent communication. We show that the largely interpretable
emergent protocol allows the nets to successfully communicate even about object
types they did not see at training time. The visual representations induced as
a by-product of our training regime, moreover, show comparable quality, when
re-used as generic visual features, to a recent self-supervised learning model.
Our results provide concrete evidence of the viability of (interpretable)
emergent deep net communication in a more realistic scenario than previously
considered, as well as establishing an intriguing link between this field and
self-supervised visual learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounding Spatio-Temporal Language with Transformers. (arXiv:2106.08858v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08858">
<div class="article-summary-box-inner">
<span><p>Language is an interface to the outside world. In order for embodied agents
to use it, language must be grounded in other, sensorimotor modalities. While
there is an extended literature studying how machines can learn grounded
language, the topic of how to learn spatio-temporal linguistic concepts is
still largely uncharted. To make progress in this direction, we here introduce
a novel spatio-temporal language grounding task where the goal is to learn the
meaning of spatio-temporal descriptions of behavioral traces of an embodied
agent. This is achieved by training a truth function that predicts if a
description matches a given history of observations. The descriptions involve
time-extended predicates in past and present tense as well as spatio-temporal
references to objects in the scene. To study the role of architectural biases
in this task, we train several models including multimodal Transformer
architectures; the latter implement different attention computations between
words and objects across space and time. We test models on two classes of
generalization: 1) generalization to randomly held-out sentences; 2)
generalization to grammar primitives. We observe that maintaining object
identity in the attention computation of our Transformers is instrumental to
achieving good performance on generalization overall, and that summarizing
object traces in a single token has little influence on performance. We then
discuss how this opens new perspectives for language-guided autonomous embodied
agents. We also release our code under open-source license as well as
pretrained models and datasets to encourage the wider community to build upon
and extend our work in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Layer-wise Analysis of a Self-supervised Speech Representation Model. (arXiv:2107.04734v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.04734">
<div class="article-summary-box-inner">
<span><p>Recently proposed self-supervised learning approaches have been successful
for pre-training speech representation models. The utility of these learned
representations has been observed empirically, but not much has been studied
about the type or extent of information encoded in the pre-trained
representations themselves. Developing such insights can help understand the
capabilities and limits of these models and enable the research community to
more efficiently develop their usage for downstream applications. In this work,
we begin to fill this gap by examining one recent and successful pre-trained
model (wav2vec 2.0), via its intermediate representation vectors, using a suite
of analysis tools. We use the metrics of canonical correlation, mutual
information, and performance on simple downstream tasks with non-parametric
probes, in order to (i) query for acoustic and linguistic information content,
(ii) characterize the evolution of information across model layers, and (iii)
understand how fine-tuning the model for automatic speech recognition (ASR)
affects these observations. Our findings motivate modifying the fine-tuning
protocol for ASR, which produces improved word error rates in a low-resource
setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence Model with Self-Adaptive Sliding Window for Efficient Spoken Document Segmentation. (arXiv:2107.09278v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09278">
<div class="article-summary-box-inner">
<span><p>Transcripts generated by automatic speech recognition (ASR) systems for
spoken documents lack structural annotations such as paragraphs, significantly
reducing their readability. Automatically predicting paragraph segmentation for
spoken documents may both improve readability and downstream NLP performance
such as summarization and machine reading comprehension. We propose a sequence
model with self-adaptive sliding window for accurate and efficient paragraph
segmentation. We also propose an approach to exploit phonetic information,
which significantly improves robustness of spoken document segmentation to ASR
errors. Evaluations are conducted on the English Wiki-727K document
segmentation benchmark, a Chinese Wikipedia-based document segmentation dataset
we created, and an in-house Chinese spoken document dataset. Our proposed model
outperforms the state-of-the-art (SOTA) model based on the same BERT-Base,
increasing segmentation F1 on the English benchmark by 4.2 points and on
Chinese datasets by 4.3-10.1 points, while reducing inference time to less than
1/6 of inference time of the current SOTA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence-to-Sequence Learning with Latent Neural Grammars. (arXiv:2109.01135v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01135">
<div class="article-summary-box-inner">
<span><p>Sequence-to-sequence learning with neural networks has become the de facto
standard for sequence prediction tasks. This approach typically models the
local distribution over the next word with a powerful neural network that can
condition on arbitrary context. While flexible and performant, these models
often require large datasets for training and can fail spectacularly on
benchmarks designed to test for compositional generalization. This work
explores an alternative, hierarchical approach to sequence-to-sequence learning
with quasi-synchronous grammars, where each node in the target tree is
transduced by a node in the source tree. Both the source and target trees are
treated as latent and induced during training. We develop a neural
parameterization of the grammar which enables parameter sharing over the
combinatorial space of derivation rules without the need for manual feature
engineering. We apply this latent neural grammar to various domains -- a
diagnostic language navigation task designed to test for compositional
generalization (SCAN), style transfer, and small-scale machine translation --
and find that it performs respectably compared to standard baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization. (arXiv:2109.02401v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02401">
<div class="article-summary-box-inner">
<span><p>Multimodal abstractive summarization (MAS) models that summarize videos
(vision modality) and their corresponding transcripts (text modality) are able
to extract the essential information from massive multimodal data on the
Internet. Recently, large-scale generative pre-trained language models (GPLMs)
have been shown to be effective in text generation tasks. However, existing MAS
models cannot leverage GPLMs' powerful generation ability. To fill this
research gap, we aim to study two research questions: 1) how to inject visual
information into GPLMs without hurting their generation ability; and 2) where
is the optimal place in GPLMs to inject the visual information? In this paper,
we present a simple yet effective method to construct vision guided (VG) GPLMs
for the MAS task using attention-based add-on layers to incorporate visual
information while maintaining their original text generation ability. Results
show that our best model significantly surpasses the prior state-of-the-art
model by 5.7 ROUGE-1, 5.3 ROUGE-2, and 5.1 ROUGE-L scores on the How2 dataset,
and our visual guidance method contributes 83.6% of the overall improvement.
Furthermore, we conduct thorough ablation studies to analyze the effectiveness
of various modality fusion methods and fusion locations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding. (arXiv:2109.04947v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04947">
<div class="article-summary-box-inner">
<span><p>Large-scale, pre-trained language models (LMs) have achieved human-level
performance on a breadth of language understanding tasks. However, evaluations
only based on end task performance shed little light on machines' true ability
in language understanding and reasoning. In this paper, we highlight the
importance of evaluating the underlying reasoning process in addition to end
performance. Toward this goal, we introduce Tiered Reasoning for Intuitive
Physics (TRIP), a novel commonsense reasoning dataset with dense annotations
that enable multi-tiered evaluation of machines' reasoning process. Our
empirical results show that while large LMs can achieve high end performance,
they struggle to support their predictions with valid supporting evidence. The
TRIP dataset and our baseline results will motivate verifiable evaluation of
commonsense reasoning and facilitate future research toward developing better
language understanding and reasoning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pack Together: Entity and Relation Extraction with Levitated Marker. (arXiv:2109.06067v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06067">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) and Relation Extraction (RE) are the core
sub-tasks for information extraction. Many recent works formulate these two
tasks as the span (pair) classification problem, and thus focus on
investigating how to obtain a better span representation from the pre-trained
encoder. However, a major limitation of existing works is that they ignore the
dependencies between spans (pairs). In this work, we propose a novel span
representation approach, named Packed Levitated Markers, to consider the
dependencies between the spans (pairs) by strategically packing the markers in
the encoder. In particular, we propose a group packing strategy to enable our
model to process massive spans together to consider their dependencies with
limited resources. Furthermore, for those more complicated span pair
classification tasks, we design a subject-oriented packing strategy, which
packs each subject and all its objects into an instance to model the
dependencies between the same-subject span pairs. Our experiments show that our
model with packed levitated markers outperforms the sequence labeling model by
0.4%-1.9% F1 on three flat NER tasks, beats the token concat model on six NER
benchmarks, and obtains a 3.5%-3.6% strict relation F1 improvement with higher
speed over previous SOTA models on ACE04 and ACE05. Code and models are
publicly available at https://github.com/thunlp/PL-Marker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wav-BERT: Cooperative Acoustic and Linguistic Representation Learning for Low-Resource Speech Recognition. (arXiv:2109.09161v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09161">
<div class="article-summary-box-inner">
<span><p>Unifying acoustic and linguistic representation learning has become
increasingly crucial to transfer the knowledge learned on the abundance of
high-resource language data for low-resource speech recognition. Existing
approaches simply cascade pre-trained acoustic and language models to learn the
transfer from speech to text. However, how to solve the representation
discrepancy of speech and text is unexplored, which hinders the utilization of
acoustic and linguistic information. Moreover, previous works simply replace
the embedding layer of the pre-trained language model with the acoustic
features, which may cause the catastrophic forgetting problem. In this work, we
introduce Wav-BERT, a cooperative acoustic and linguistic representation
learning method to fuse and utilize the contextual information of speech and
text. Specifically, we unify a pre-trained acoustic model (wav2vec 2.0) and a
language model (BERT) into an end-to-end trainable framework. A Representation
Aggregation Module is designed to aggregate acoustic and linguistic
representation, and an Embedding Attention Module is introduced to incorporate
acoustic information into BERT, which can effectively facilitate the
cooperation of two pre-trained models and thus boost the representation
learning. Extensive experiments show that our Wav-BERT significantly
outperforms the existing approaches and achieves state-of-the-art performance
on low-resource speech recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Bias in NLP -- Application to Hate Speech Classification using transfer learning techniques. (arXiv:2109.09725v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09725">
<div class="article-summary-box-inner">
<span><p>In this paper, a BERT based neural network model is applied to the JIGSAW
data set in order to create a model identifying hateful and toxic comments
(strictly seperated from offensive language) in online social platforms
(English language), in this case Twitter. Three other neural network
architectures and a GPT-2 model are also applied on the provided data set in
order to compare these different models. The trained BERT model is then applied
on two different data sets to evaluate its generalisation power, namely on
another Twitter data set and the data set HASOC 2019 which includes Twitter and
also Facebook comments; we focus on the English HASOC 2019 data. In addition,
it can be shown that by fine-tuning the trained BERT model on these two data
sets by applying different transfer learning scenarios via retraining partial
or all layers the predictive scores improve compared to simply applying the
model pre-trained on the JIGSAW data set. With our results, we get precisions
from 64% to around 90% while still achieving acceptable recall values of at
least lower 60s%, proving that BERT is suitable for real use cases in social
platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WRENCH: A Comprehensive Benchmark for Weak Supervision. (arXiv:2109.11377v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11377">
<div class="article-summary-box-inner">
<span><p>Recent Weak Supervision (WS) approaches have had widespread success in easing
the bottleneck of labeling training data for machine learning by synthesizing
labels from multiple potentially noisy supervision sources. However, proper
measurement and analysis of these approaches remain a challenge. First,
datasets used in existing works are often private and/or custom, limiting
standardization. Second, WS datasets with the same name and base data often
vary in terms of the labels and weak supervision sources used, a significant
"hidden" source of evaluation variance. Finally, WS studies often diverge in
terms of the evaluation protocol and ablations used. To address these problems,
we introduce a benchmark platform, WRENCH, for thorough and standardized
evaluation of WS approaches. It consists of 22 varied real-world datasets for
classification and sequence tagging; a range of real, synthetic, and
procedurally-generated weak supervision sources; and a modular, extensible
framework for WS evaluation, including implementations for popular WS methods.
We use WRENCH to conduct extensive comparisons over more than 120 method
variants to demonstrate its efficacy as a benchmark platform. The code is
available at https://github.com/JieyuZ2/wrench.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine Translation. (arXiv:2109.12584v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12584">
<div class="article-summary-box-inner">
<span><p>In recent times, there has been definitive progress in the field of NLP, with
its applications growing as the utility of our language models increases with
advances in their performance. However, these models require a large amount of
computational power and data to train, consequently leading to large carbon
footprints. Therefore, it is imperative that we study the carbon efficiency and
look for alternatives to reduce the overall environmental impact of training
models, in particular large language models. In our work, we assess the
performance of models for machine translation, across multiple language pairs
to assess the difference in computational power required to train these models
for each of these language pairs and examine the various components of these
models to analyze aspects of our pipeline that can be optimized to reduce these
carbon emissions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Sentence-Level and Aspect-Level (Un)certainty in Science Communications. (arXiv:2109.14776v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14776">
<div class="article-summary-box-inner">
<span><p>Certainty and uncertainty are fundamental to science communication. Hedges
have widely been used as proxies for uncertainty. However, certainty is a
complex construct, with authors expressing not only the degree but the type and
aspects of uncertainty in order to give the reader a certain impression of what
is known. Here, we introduce a new study of certainty that models both the
level and the aspects of certainty in scientific findings. Using a new dataset
of 2167 annotated scientific findings, we demonstrate that hedges alone account
for only a partial explanation of certainty. We show that both the overall
certainty and individual aspects can be predicted with pre-trained language
models, providing a more complete picture of the author's intended
communication. Downstream analyses on 431K scientific findings from news and
scientific abstracts demonstrate that modeling sentence-level and aspect-level
certainty is meaningful for areas like science communication. Both the model
and datasets used in this paper are released at
https://blablablab.si.umich.edu/projects/certainty/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">#ContextMatters: Advantages and Limitations of Using Machine Learning to Support Women in Politics. (arXiv:2110.00116v2 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00116">
<div class="article-summary-box-inner">
<span><p>The United Nations identified gender equality as a Sustainable Development
Goal in 2015, recognizing the underrepresentation of women in politics as a
specific barrier to achieving gender equality. Political systems around the
world experience gender inequality across all levels of elected government as
fewer women run for office than men. This is due in part to online abuse,
particularly on social media platforms like Twitter, where women seeking or in
power tend to be targeted with more toxic maltreatment than their male
counterparts. In this paper, we present reflections on ParityBOT - the first
natural language processing-based intervention designed to affect online
discourse for women in politics for the better, at scale. Deployed across
elections in Canada, the United States and New Zealand, ParityBOT was used to
analyse and classify more than 12 million tweets directed at women candidates
and counter toxic tweets with supportive ones. From these elections we present
three case studies highlighting the current limitations of, and future research
and application opportunities for, using a natural language processing-based
system to detect online toxicity, specifically with regards to contextually
important microaggressions. We examine the rate of false negatives, where
ParityBOT failed to pick up on insults directed at specific high profile women,
which would be obvious to human users. We examine the unaddressed harms of
microaggressions and the potential of yet unseen damage they cause for women in
these communities, and for progress towards gender equality overall, in light
of these technological blindspots. This work concludes with a discussion on the
benefits of partnerships between nonprofit social groups and technology experts
to develop responsible, socially impactful approaches to addressing online
hate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HowSumm: A Multi-Document Summarization Dataset Derived from WikiHow Articles. (arXiv:2110.03179v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03179">
<div class="article-summary-box-inner">
<span><p>We present HowSumm, a novel large-scale dataset for the task of query-focused
multi-document summarization (qMDS), which targets the use-case of generating
actionable instructions from a set of sources. This use-case is different from
the use-cases covered in existing multi-document summarization (MDS) datasets
and is applicable to educational and industrial scenarios. We employed
automatic methods, and leveraged statistics from existing human-crafted qMDS
datasets, to create HowSumm from wikiHow website articles and the sources they
cite. We describe the creation of the dataset and discuss the unique features
that distinguish it from other summarization corpora. Automatic and human
evaluations of both extractive and abstractive summarization models on the
dataset reveal that there is room for improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisualTTS: TTS with Accurate Lip-Speech Synchronization for Automatic Voice Over. (arXiv:2110.03342v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03342">
<div class="article-summary-box-inner">
<span><p>In this paper, we formulate a novel task to synthesize speech in sync with a
silent pre-recorded video, denoted as automatic voice over (AVO). Unlike
traditional speech synthesis, AVO seeks to generate not only human-sounding
speech, but also perfect lip-speech synchronization. A natural solution to AVO
is to condition the speech rendering on the temporal progression of lip
sequence in the video. We propose a novel text-to-speech model that is
conditioned on visual input, named VisualTTS, for accurate lip-speech
synchronization. The proposed VisualTTS adopts two novel mechanisms that are 1)
textual-visual attention, and 2) visual fusion strategy during acoustic
decoding, which both contribute to forming accurate alignment between the input
text content and lip motion in input lip sequence. Experimental results show
that VisualTTS achieves accurate lip-speech synchronization and outperforms all
baseline systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Applying Phonological Features in Multilingual Text-To-Speech. (arXiv:2110.03609v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03609">
<div class="article-summary-box-inner">
<span><p>This study investigates whether phonological features can be applied in
text-to-speech systems to generate native and non-native speech in English and
Mandarin. We present a mapping of ARPABET/pinyin to SAMPA/SAMPA-SC and then to
phonological features. We tested whether this mapping could lead to the
successful generation of native, non-native, and code-switched speech in the
two languages. We ran two experiments, one with a small dataset and one with a
larger dataset. The results proved that phonological features could be used as
a feasible input system, although further investigation is needed to improve
model performance. The accented output generated by the TTS models also helps
with understanding human second language acquisition processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation of professions in entertainment media: Insights into frequency and sentiment trends through computational text analysis. (arXiv:2110.03873v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03873">
<div class="article-summary-box-inner">
<span><p>Societal ideas and trends dictate media narratives and cinematic depictions
which in turn influences people's beliefs and perceptions of the real world.
Media portrayal of culture, education, government, religion, and family affect
their function and evolution over time as people interpret and perceive these
representations and incorporate them into their beliefs and actions. It is
important to study media depictions of these social structures so that they do
not propagate or reinforce negative stereotypes, or discriminate against any
demographic section. In this work, we examine media representation of
professions and provide computational insights into their incidence, and
sentiment expressed, in entertainment media content. We create a searchable
taxonomy of professional groups and titles to facilitate their retrieval from
speaker-agnostic text passages like movie and television (TV) show subtitles.
We leverage this taxonomy and relevant natural language processing (NLP) models
to create a corpus of professional mentions in media content, spanning more
than 136,000 IMDb titles over seven decades (1950-2017). We analyze the
frequency and sentiment trends of different occupations, study the effect of
media attributes like genre, country of production, and title type on these
trends, and investigate if the incidence of professions in media subtitles
correlate with their real-world employment statistics. We observe increased
media mentions of STEM, arts, sports, and entertainment occupations in the
analyzed subtitles, and a decreased frequency of manual labor jobs and military
occupations. The sentiment expressed toward lawyers, police, and doctors is
becoming negative over time, whereas astronauts, musicians, singers, and
engineers are mentioned favorably. Professions that employ more people have
increased media frequency, supporting our hypothesis that media acts as a
mirror to society.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Meta-Segmentation Neural Network. (arXiv:2110.04297v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04297">
<div class="article-summary-box-inner">
<span><p>Though deep learning methods have shown great success in 3D point cloud part
segmentation, they generally rely on a large volume of labeled training data,
which makes the model suffer from unsatisfied generalization abilities to
unseen classes with limited data. To address this problem, we present a novel
meta-learning strategy that regards the 3D shape segmentation function as a
task. By training over a number of 3D part segmentation tasks, our method is
capable to learn the prior over the respective 3D segmentation function space
which leads to an optimal model that is rapidly adapting to new part
segmentation tasks. To implement our meta-learning strategy, we propose two
novel modules: meta part segmentation learner and part segmentation learner.
During the training process, the part segmentation learner is trained to
complete a specific part segmentation task in the few-shot scenario. In the
meantime, the meta part segmentation learner is trained to capture the prior
from multiple similar part segmentation tasks. Based on the learned information
of task distribution, our meta part segmentation learner is able to dynamically
update the part segmentation learner with optimal parameters which enable our
part segmentation learner to rapidly adapt and have great generalization
ability on new part segmentation tasks. We demonstrate that our model achieves
superior part segmentation performance with the few-shot setting on the widely
used dataset: ShapeNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal ImageNet: How to discover spurious features in Deep Learning?. (arXiv:2110.04301v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04301">
<div class="article-summary-box-inner">
<span><p>A key reason for the lack of reliability of deep neural networks in the real
world is their heavy reliance on {\it spurious} input features that are
causally unrelated to the true label. Focusing on image classifications, we
define causal attributes as the set of visual features that are always a part
of the object while spurious attributes are the ones that are likely to {\it
co-occur} with the object but not a part of it (e.g., attribute ``fingers" for
class ``band aid"). Traditional methods for discovering spurious features
either require extensive human annotations (thus, not scalable), or are useful
on specific models. In this work, we introduce a {\it scalable} framework to
discover a subset of spurious and causal visual attributes used in inferences
of a general model and localize them on a large number of images with minimal
human supervision. Our methodology is based on this key idea: to identify
spurious or causal \textit{visual attributes} used in model predictions, we
identify spurious or causal \textit{neural features} (penultimate layer neurons
of a robust model) via limited human supervision (e.g., using top 5 activating
images per feature). We then show that these neural feature annotations {\it
generalize} extremely well to many more images {\it without} any human
supervision. We use the activation maps for these neural features as the soft
masks to highlight spurious or causal visual attributes. Using this
methodology, we introduce the {\it Causal Imagenet} dataset containing causal
and spurious masks for a large set of samples from Imagenet. We assess the
performance of several popular Imagenet models and show that they rely heavily
on various spurious features in their predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVID-19 Face Mask Recognition with Advanced Face Cut Algorithm for Human Safety Measures. (arXiv:2110.04316v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04316">
<div class="article-summary-box-inner">
<span><p>In the last year, the outbreak of COVID-19 has deployed computer vision and
machine learning algorithms in various fields to enhance human life
interactions. COVID-19 is a highly contaminated disease that affects mainly the
respiratory organs of the human body. We must wear a mask in this situation as
the virus can be contaminated through the air and a non-masked person can be
affected. Our proposal deploys a computer vision and deep learning framework to
recognize face masks from images or videos. We have implemented a Boundary
dependent face cut recognition algorithm that can cut the face from the image
using 27 landmarks and then the preprocessed image can further be sent to the
deep learning ResNet50 model. The experimental result shows a significant
advancement of 3.4 percent compared to the YOLOV3 mask recognition architecture
in just 10 epochs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning a Self-Expressive Network for Subspace Clustering. (arXiv:2110.04318v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04318">
<div class="article-summary-box-inner">
<span><p>State-of-the-art subspace clustering methods are based on self-expressive
model, which represents each data point as a linear combination of other data
points. However, such methods are designed for a finite sample dataset and lack
the ability to generalize to out-of-sample data. Moreover, since the number of
self-expressive coefficients grows quadratically with the number of data
points, their ability to handle large-scale datasets is often limited. In this
paper, we propose a novel framework for subspace clustering, termed
Self-Expressive Network (SENet), which employs a properly designed neural
network to learn a self-expressive representation of the data. We show that our
SENet can not only learn the self-expressive coefficients with desired
properties on the training data, but also handle out-of-sample data. Besides,
we show that SENet can also be leveraged to perform subspace clustering on
large-scale datasets. Extensive experiments conducted on synthetic data and
real world benchmark data validate the effectiveness of the proposed method. In
particular, SENet yields highly competitive performance on MNIST, Fashion MNIST
and Extended MNIST and state-of-the-art performance on CIFAR-10. The code is
available at https://github.com/zhangsz1998/Self-Expressive-Network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Token Attacks on Vision Transformers. (arXiv:2110.04337v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04337">
<div class="article-summary-box-inner">
<span><p>Vision transformers rely on a patch token based self attention mechanism, in
contrast to convolutional networks. We investigate fundamental differences
between these two families of models, by designing a block sparsity based
adversarial token attack. We probe and analyze transformer as well as
convolutional models with token attacks of varying patch sizes. We infer that
transformer models are more sensitive to token attacks than convolutional
models, with ResNets outperforming Transformer models by up to $\sim30\%$ in
robust accuracy for single token attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantum pixel representations and compression for $N$-dimensional images. (arXiv:2110.04405v1 [quant-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04405">
<div class="article-summary-box-inner">
<span><p>We introduce a novel and uniform framework for quantum pixel representations
that overarches many of the most popular representations proposed in the recent
literature, such as (I)FRQI, (I)NEQR, MCRQI, and (I)NCQI. The proposed QPIXL
framework results in more efficient circuit implementations and significantly
reduces the gate complexity for all considered quantum pixel representations.
Our method only requires a linear number of gates in terms of the number of
pixels and does not use ancilla qubits. Furthermore, the circuits only consist
of Ry gates and CNOT gates making them practical in the NISQ era. Additionally,
we propose a circuit and image compression algorithm that is shown to be highly
effective, being able to reduce the necessary gates to prepare an FRQI state
for example scientific images by up to 90% without sacrificing image quality.
Our algorithms are made publicly available as part of QPIXL++, a Quantum Image
Pixel Library.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Pose-Aware Part Decomposition for 3D Articulated Objects. (arXiv:2110.04411v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04411">
<div class="article-summary-box-inner">
<span><p>Articulated objects exist widely in the real world. However, previous 3D
generative methods for unsupervised part decomposition are unsuitable for such
objects, because they assume a spatially fixed part location, resulting in
inconsistent part parsing. In this paper, we propose PPD (unsupervised
Pose-aware Part Decomposition) to address a novel setting that explicitly
targets man-made articulated objects with mechanical joints, considering the
part poses. We show that category-common prior learning for both part shapes
and poses facilitates the unsupervised learning of (1) part decomposition with
non-primitive-based implicit representation, and (2) part pose as joint
parameters under single-frame shape supervision. We evaluate our method on
synthetic and real datasets, and we show that it outperforms previous works in
consistent part parsing of the articulated objects based on comparable part
pose estimation performance to the supervised baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustness Evaluation of Transformer-based Form Field Extractors via Form Attacks. (arXiv:2110.04413v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04413">
<div class="article-summary-box-inner">
<span><p>We propose a novel framework to evaluate the robustness of transformer-based
form field extraction methods via form attacks. We introduce 14 novel form
transformations to evaluate the vulnerability of the state-of-the-art field
extractors against form attacks from both OCR level and form level, including
OCR location/order rearrangement, form background manipulation and form
field-value augmentation. We conduct robustness evaluation using real invoices
and receipts, and perform comprehensive research analysis. Experimental results
suggest that the evaluated models are very susceptible to form perturbations
such as the variation of field-values (~15% drop in F1 score), the
disarrangement of input text order(~15% drop in F1 score) and the disruption of
the neighboring words of field-values(~10% drop in F1 score). Guided by the
analysis, we make recommendations to improve the design of field extractors and
the process of data collection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Arabic Speech Emotion Recognition Employing Wav2vec2.0 and HuBERT Based on BAVED Dataset. (arXiv:2110.04425v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04425">
<div class="article-summary-box-inner">
<span><p>Recently, there have been tremendous research outcomes in the fields of
speech recognition and natural language processing. This is due to the
well-developed multi-layers deep learning paradigms such as wav2vec2.0,
Wav2vecU, WavBERT, and HuBERT that provide better representation learning and
high information capturing. Such paradigms run on hundreds of unlabeled data,
then fine-tuned on a small dataset for specific tasks. This paper introduces a
deep learning constructed emotional recognition model for Arabic speech
dialogues. The developed model employs the state of the art audio
representations include wav2vec2.0 and HuBERT. The experiment and performance
results of our model overcome the previous known outcomes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harnessing Unlabeled Data to Improve Generalization of Biometric Gender and Age Classifiers. (arXiv:2110.04427v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04427">
<div class="article-summary-box-inner">
<span><p>With significant advances in deep learning, many computer vision applications
have reached the inflection point. However, these deep learning models need
large amount of labeled data for model training and optimum parameter
estimation. Limited labeled data for model training results in over-fitting and
impacts their generalization performance. However, the collection and
annotation of large amount of data is a very time consuming and expensive
operation. Further, due to privacy and security concerns, the large amount of
labeled data could not be collected for certain applications such as those
involving medical field. Self-training, Co-training, and Self-ensemble methods
are three types of semi-supervised learning methods that can be used to exploit
unlabeled data. In this paper, we propose self-ensemble based deep learning
model that along with limited labeled data, harness unlabeled data for
improving the generalization performance. We evaluated the proposed
self-ensemble based deep-learning model for soft-biometric gender and age
classification. Experimental evaluation on CelebA and VISOB datasets suggest
gender classification accuracy of 94.46% and 81.00%, respectively, using only
1000 labeled samples and remaining 199k samples as unlabeled samples for CelebA
dataset and similarly,1000 labeled samples with remaining 107k samples as
unlabeled samples for VISOB dataset. Comparative evaluation suggest that there
is $5.74\%$ and $8.47\%$ improvement in the accuracy of the self-ensemble model
when compared with supervised model trained on the entire CelebA and VISOB
dataset, respectively. We also evaluated the proposed learning method for
age-group prediction on Adience dataset and it outperformed the baseline
supervised deep-learning learning model with a better exact accuracy of 55.55
$\pm$ 4.28 which is 3.92% more than the baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RankingMatch: Delving into Semi-Supervised Learning with Consistency Regularization and Ranking Loss. (arXiv:2110.04430v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04430">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning (SSL) has played an important role in leveraging
unlabeled data when labeled data is limited. One of the most successful SSL
approaches is based on consistency regularization, which encourages the model
to produce unchanged with perturbed input. However, there has been less
attention spent on inputs that have the same label. Motivated by the
observation that the inputs having the same label should have the similar model
outputs, we propose a novel method, RankingMatch, that considers not only the
perturbed inputs but also the similarity among the inputs having the same
label. We especially introduce a new objective function, dubbed BatchMean
Triplet loss, which has the advantage of computational efficiency while taking
into account all input samples. Our RankingMatch achieves state-of-the-art
performance across many standard SSL benchmarks with a variety of labeled data
amounts, including 95.13% accuracy on CIFAR-10 with 250 labels, 77.65% accuracy
on CIFAR-100 with 10000 labels, 97.76% accuracy on SVHN with 250 labels, and
97.77% accuracy on SVHN with 1000 labels. We also perform an ablation study to
prove the efficacy of the proposed BatchMean Triplet loss against existing
versions of Triplet loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SOMA: Solving Optical Marker-Based MoCap Automatically. (arXiv:2110.04431v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04431">
<div class="article-summary-box-inner">
<span><p>Marker-based optical motion capture (mocap) is the "gold standard" method for
acquiring accurate 3D human motion in computer vision, medicine, and graphics.
The raw output of these systems are noisy and incomplete 3D points or short
tracklets of points. To be useful, one must associate these points with
corresponding markers on the captured subject; i.e. "labelling". Given these
labels, one can then "solve" for the 3D skeleton or body surface mesh.
Commercial auto-labeling tools require a specific calibration procedure at
capture time, which is not possible for archival data. Here we train a novel
neural network called SOMA, which takes raw mocap point clouds with varying
numbers of points, labels them at scale without any calibration data,
independent of the capture technology, and requiring only minimal human
intervention. Our key insight is that, while labeling point clouds is highly
ambiguous, the 3D body provides strong constraints on the solution that can be
exploited by a learning-based method. To enable learning, we generate massive
training sets of simulated noisy and ground truth mocap markers animated by 3D
bodies from AMASS. SOMA exploits an architecture with stacked self-attention
elements to learn the spatial structure of the 3D body and an optimal transport
layer to constrain the assignment (labeling) problem while rejecting outliers.
We extensively evaluate SOMA both quantitatively and qualitatively. SOMA is
more accurate and robust than existing state of the art research methods and
can be applied where commercial systems cannot. We automatically label over 8
hours of archival mocap data across 4 different datasets captured using various
technologies and output SMPL-X body models. The model and data is released for
research purposes at https://soma.is.tue.mpg.de/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-stage Visual Cues Enhancement Network for Referring Image Segmentation. (arXiv:2110.04435v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04435">
<div class="article-summary-box-inner">
<span><p>Referring Image Segmentation (RIS) aims at segmenting the target object from
an image referred by one given natural language expression. The diverse and
flexible expressions as well as complex visual contents in the images raise the
RIS model with higher demands for investigating fine-grained matching behaviors
between words in expressions and objects presented in images. However, such
matching behaviors are hard to be learned and captured when the visual cues of
referents (i.e. referred objects) are insufficient, as the referents with weak
visual cues tend to be easily confused by cluttered background at boundary or
even overwhelmed by salient objects in the image. And the insufficient visual
cues issue can not be handled by the cross-modal fusion mechanisms as done in
previous work. In this paper, we tackle this problem from a novel perspective
of enhancing the visual information for the referents by devising a Two-stage
Visual cues enhancement Network (TV-Net), where a novel Retrieval and
Enrichment Scheme (RES) and an Adaptive Multi-resolution feature Fusion (AMF)
module are proposed. Through the two-stage enhancement, our proposed TV-Net
enjoys better performances in learning fine-grained matching behaviors between
the natural language expression and image, especially when the visual
information of the referent is inadequate, thus produces better segmentation
results. Extensive experiments are conducted to validate the effectiveness of
the proposed method on the RIS task, with our proposed TV-Net surpassing the
state-of-the-art approaches on four benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientPhys: Enabling Simple, Fast and Accurate Camera-Based Vitals Measurement. (arXiv:2110.04447v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04447">
<div class="article-summary-box-inner">
<span><p>Camera-based physiological measurement is a growing field with neural models
providing state-the-art-performance. Prior research have explored various
``end-to-end'' models; however these methods still require several
preprocessing steps. These additional operations are often non-trivial to
implement making replication and deployment difficult and can even have a
higher computational budget than the ``core'' network itself. In this paper, we
propose two novel and efficient neural models for camera-based physiological
measurement called EfficientPhys that remove the need for face detection,
segmentation, normalization, color space transformation or any other
preprocessing steps. Using an input of raw video frames, our models achieve
state-of-the-art accuracy on three public datasets. We show that this is the
case whether using a transformer or convolutional backbone. We further evaluate
the latency of the proposed networks and show that our most light weight
network also achieves a 33% improvement in efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scene Editing as Teleoperation: A Case Study in 6DoF Kit Assembly. (arXiv:2110.04450v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04450">
<div class="article-summary-box-inner">
<span><p>Studies in robot teleoperation have been centered around action
specifications -- from continuous joint control to discrete end-effector pose
control. However, these robot-centric interfaces often require skilled
operators with extensive robotics expertise. To make teleoperation accessible
to non-expert users, we propose the framework "Scene Editing as Teleoperation"
(SEaT), where the key idea is to transform the traditional "robot-centric"
interface into a "scene-centric" interface -- instead of controlling the robot,
users focus on specifying the task's goal by manipulating digital twins of the
real-world objects. As a result, a user can perform teleoperation without any
expert knowledge of the robot hardware. To achieve this goal, we utilize a
category-agnostic scene-completion algorithm that translates the real-world
workspace (with unknown objects) into a manipulable virtual scene
representation and an action-snapping algorithm that refines the user input
before generating the robot's action plan. To train the algorithms, we
procedurally generated a large-scale, diverse kit-assembly dataset that
contains object-kit pairs that mimic real-world object-kitting tasks. Our
experiments in simulation and on a real-world system demonstrate that our
framework improves both the efficiency and success rate for 6DoF kit-assembly
tasks. A user study demonstrates that SEaT framework participants achieve a
higher task success rate and report a lower subjective workload compared to an
alternative robot-centric interface. Video can be found at
https://www.youtube.com/watch?v=-NdR3mkPbQQ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformer based COVID-19 Detection using Chest X-rays. (arXiv:2110.04458v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04458">
<div class="article-summary-box-inner">
<span><p>COVID-19 is a global pandemic, and detecting them is a momentous task for
medical professionals today due to its rapid mutations. Current methods of
examining chest X-rays and CT scan requires profound knowledge and are time
consuming, which suggests that it shrinks the precious time of medical
practitioners when people's lives are at stake. This study tries to assist this
process by achieving state-of-the-art performance in classifying chest X-rays
by fine-tuning Vision Transformer(ViT). The proposed approach uses pretrained
models, fine-tuned for detecting the presence of COVID-19 disease on chest
X-rays. This approach achieves an accuracy score of 97.61%, precision score of
95.34%, recall score of 93.84% and, f1-score of 94.58%. This result signifies
the performance of transformer-based models on chest X-ray.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Training for Face Recognition Systems using Contrastive Adversarial Learning and Triplet Loss Fine-tuning. (arXiv:2110.04459v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04459">
<div class="article-summary-box-inner">
<span><p>Though much work has been done in the domain of improving the adversarial
robustness of facial recognition systems, a surprisingly small percentage of it
has focused on self-supervised approaches. In this work, we present an approach
that combines Ad-versarial Pre-Training with Triplet Loss
AdversarialFine-Tuning. We compare our methods with the pre-trained ResNet50
model that forms the backbone of FaceNet, finetuned on our CelebA dataset.
Through comparing adversarial robustness achieved without adversarial training,
with triplet loss adversarial training, and our contrastive pre-training
combined with triplet loss adversarial fine-tuning, we find that our method
achieves comparable results with far fewer epochs re-quired during fine-tuning.
This seems promising, increasing the training time for fine-tuning should yield
even better results. In addition to this, a modified semi-supervised experiment
was conducted, which demonstrated the improvement of contrastive adversarial
training with the introduction of small amounts of labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting decision-making in the future: Human versus Machine. (arXiv:2110.04465v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04465">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) have become remarkably successful in data
prediction, and have even been used to predict future actions based on limited
input. This raises the question: do these systems actually "understand" the
event similar to humans? Here, we address this issue using videos taken from an
accident situation in a driving simulation. In this situation, drivers had to
choose between crashing into a suddenly-appeared obstacle or steering their car
off a previously indicated cliff. We compared how well humans and a DNN
predicted this decision as a function of time before the event. The DNN
outperformed humans for early time-points, but had an equal performance for
later time-points. Interestingly, spatio-temporal image manipulations and
Grad-CAM visualizations uncovered some expected behavior, but also highlighted
potential differences in temporal processing for the DNN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label quality in AffectNet: results of crowd-based re-annotation. (arXiv:2110.04476v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04476">
<div class="article-summary-box-inner">
<span><p>AffectNet is one of the most popular resources for facial expression
recognition (FER) on relatively unconstrained in-the-wild images. Given that
images were annotated by only one annotator with limited consistency checks on
the data, however, label quality and consistency may be limited. Here, we take
a similar approach to a study that re-labeled another, smaller dataset
(FER2013) with crowd-based annotations, and report results from a re-labeling
and re-annotation of a subset of difficult AffectNet faces with 13 people on
both expression label, and valence and arousal ratings. Our results show that
human labels overall have medium to good consistency, whereas human ratings
especially for valence are in excellent agreement. Importantly, however,
crowd-based labels are significantly shifting towards neutral and happy
categories and crowd-based affective ratings form a consistent pattern
different from the original ratings. ResNets fully trained on the original
AffectNet dataset do not predict human voting patterns, but when weakly-trained
do so much better, particularly for valence. Our results have important
ramifications for label quality in affective computing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Feature Consistency Driven Attention Erasing Network for Fine-Grained Image Retrieval. (arXiv:2110.04479v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04479">
<div class="article-summary-box-inner">
<span><p>Large-scale fine-grained image retrieval has two main problems. First, low
dimensional feature embedding can fasten the retrieval process but bring
accuracy reduce due to overlooking the feature of significant attention regions
of images in fine-grained datasets. Second, fine-grained images lead to the
same category query hash codes mapping into the different cluster in database
hash latent space. To handle these two issues, we propose a feature consistency
driven attention erasing network (FCAENet) for fine-grained image retrieval.
For the first issue, we propose an adaptive augmentation module in FCAENet,
which is selective region erasing module (SREM). SREM makes the network more
robust on subtle differences of fine-grained task by adaptively covering some
regions of raw images. The feature extractor and hash layer can learn more
representative hash code for fine-grained images by SREM. With regard to the
second issue, we fully exploit the pair-wise similarity information and add the
enhancing space relation loss (ESRL) in FCAENet to make the vulnerable relation
stabler between the query hash code and database hash code. We conduct
extensive experiments on five fine-grained benchmark datasets (CUB2011,
Aircraft, NABirds, VegFru, Food101) for 12bits, 24bits, 32bits, 48bits hash
code. The results show that FCAENet achieves the state-of-the-art (SOTA)
fine-grained retrieval performance compared with other methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Facial Expression Recognition in Humans and Machines: Using CAM, GradCAM, and Extremal Perturbation. (arXiv:2110.04481v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04481">
<div class="article-summary-box-inner">
<span><p>Facial expression recognition (FER) is a topic attracting significant
research in both psychology and machine learning with a wide range of
applications. Despite a wealth of research on human FER and considerable
progress in computational FER made possible by deep neural networks (DNNs),
comparatively less work has been done on comparing the degree to which DNNs may
be comparable to human performance. In this work, we compared the recognition
performance and attention patterns of humans and machines during a
two-alternative forced-choice FER task. Human attention was here gathered
through click data that progressively uncovered a face, whereas model attention
was obtained using three different popular techniques from explainable AI: CAM,
GradCAM and Extremal Perturbation. In both cases, performance was gathered as
percent correct. For this task, we found that humans outperformed machines
quite significantly. In terms of attention patterns, we found that Extremal
Perturbation had the best overall fit with the human attention map during the
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visualizing the embedding space to explain the effect of knowledge distillation. (arXiv:2110.04483v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04483">
<div class="article-summary-box-inner">
<span><p>Recent research has found that knowledge distillation can be effective in
reducing the size of a network and in increasing generalization. A pre-trained,
large teacher network, for example, was shown to be able to bootstrap a student
model that eventually outperforms the teacher in a limited label environment.
Despite these advances, it still is relatively unclear \emph{why} this method
works, that is, what the resulting student model does 'better'. To address this
issue, here, we utilize two non-linear, low-dimensional embedding methods
(t-SNE and IVIS) to visualize representation spaces of different layers in a
network. We perform a set of extensive experiments with different architecture
parameters and distillation methods. The resulting visualizations and metrics
clearly show that distillation guides the network to find a more compact
representation space for higher accuracy already in earlier layers compared to
its non-distilled version.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Colour augmentation for improved semi-supervised semantic segmentation. (arXiv:2110.04487v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04487">
<div class="article-summary-box-inner">
<span><p>Consistency regularization describes a class of approaches that have yielded
state-of-the-art results for semi-supervised classification. While
semi-supervised semantic segmentation proved to be more challenging, a number
of successful approaches have been recently proposed. Recent work explored the
challenges involved in using consistency regularization for segmentation
problems. In their self-supervised work Chen et al. found that colour
augmentation prevents a classification network from using image colour
statistics as a short-cut for self-supervised learning via instance
discrimination. Drawing inspiration from this we find that a similar problem
impedes semi-supervised semantic segmentation and offer colour augmentation as
a solution, improving semi-supervised semantic segmentation performance on
challenging photographic imagery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Demystifying the Transferability of Adversarial Attacks in Computer Networks. (arXiv:2110.04488v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04488">
<div class="article-summary-box-inner">
<span><p>Deep Convolutional Neural Networks (CNN) models are one of the most popular
networks in deep learning. With their large fields of application in different
areas, they are extensively used in both academia and industry. CNN-based
models include several exciting implementations such as early breast cancer
detection or detecting developmental delays in children (e.g., autism, speech
disorders, etc.). However, previous studies demonstrate that these models are
subject to various adversarial attacks. Interestingly, some adversarial
examples could potentially still be effective against different unknown models.
This particular property is known as adversarial transferability, and prior
works slightly analyzed this characteristic in a very limited application
domain. In this paper, we aim to demystify the transferability threats in
computer networks by studying the possibility of transferring adversarial
examples. In particular, we provide the first comprehensive study which
assesses the robustness of CNN-based models for computer networks against
adversarial transferability. In our experiments, we consider five different
attacks: (1) the Iterative Fast Gradient Method (I-FGSM), (2) the
Jacobian-based Saliency Map attack (JSMA), (3) the L-BFGS attack, (4) the
Projected Gradient Descent attack (PGD), and (5) the DeepFool attack. These
attacks are performed against two well-known datasets: the N-BaIoT dataset and
the Domain Generating Algorithms (DGA) dataset. Our results show that the
transferability happens in specific use cases where the adversary can easily
compromise the victim's network with very few knowledge of the targeted model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Invertible Tone Mapping with Selectable Styles. (arXiv:2110.04491v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04491">
<div class="article-summary-box-inner">
<span><p>Although digital cameras can acquire high-dynamic range (HDR) images, the
captured HDR information are mostly quantized to low-dynamic range (LDR) images
for display compatibility and compact storage. In this paper, we propose an
invertible tone mapping method that converts the multi-exposure HDR to a true
LDR (8-bit per color channel) and reserves the capability to accurately restore
the original HDR from this {\em invertible LDR}. Our invertible LDR can mimic
the appearance of a user-selected tone mapping style. It can be shared over any
existing social network platforms that may re-encode or format-convert the
uploaded images, without much hurting the accuracy of the restored HDR
counterpart. To achieve this, we regard the tone mapping and the restoration as
coupled processes, and formulate them as an encoding-and-decoding problem
through convolutional neural networks. Particularly, our model supports
pluggable style modulators, each of which bakes a specific tone mapping style,
and thus favors the application flexibility. Our method is evaluated with a
rich variety of HDR images and multiple tone mapping operators, which shows the
superiority over relevant state-of-the-art methods. Also, we conduct ablation
study to justify our design and discuss the robustness and generality toward
real applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weight Evolution: Improving Deep Neural Networks Training through Evolving Inferior Weight Values. (arXiv:2110.04492v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04492">
<div class="article-summary-box-inner">
<span><p>To obtain good performance, convolutional neural networks are usually
over-parameterized. This phenomenon has stimulated two interesting topics:
pruning the unimportant weights for compression and reactivating the
unimportant weights to make full use of network capability. However, current
weight reactivation methods usually reactivate the entire filters, which may
not be precise enough. Looking back in history, the prosperity of filter
pruning is mainly due to its friendliness to hardware implementation, but
pruning at a finer structure level, i.e., weight elements, usually leads to
better network performance. We study the problem of weight element reactivation
in this paper. Motivated by evolution, we select the unimportant filters and
update their unimportant elements by combining them with the important elements
of important filters, just like gene crossover to produce better offspring, and
the proposed method is called weight evolution (WE). WE is mainly composed of
four strategies. We propose a global selection strategy and a local selection
strategy and combine them to locate the unimportant filters. A forward matching
strategy is proposed to find the matched important filters and a crossover
strategy is proposed to utilize the important elements of the important filters
for updating unimportant filters. WE is plug-in to existing network
architectures. Comprehensive experiments show that WE outperforms the other
reactivation methods and plug-in training methods with typical convolutional
neural networks, especially lightweight networks. Our code is available at
https://github.com/BZQLin/Weight-evolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SGMNet: Scene Graph Matching Network for Few-Shot Remote Sensing Scene Classification. (arXiv:2110.04494v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04494">
<div class="article-summary-box-inner">
<span><p>Few-Shot Remote Sensing Scene Classification (FSRSSC) is an important task,
which aims to recognize novel scene classes with few examples. Recently,
several studies attempt to address the FSRSSC problem by following few-shot
natural image classification methods. These existing methods have made
promising progress and achieved superior performance. However, they all
overlook two unique characteristics of remote sensing images: (i) object
co-occurrence that multiple objects tend to appear together in a scene image
and (ii) object spatial correlation that these co-occurrence objects are
distributed in the scene image following some spatial structure patterns. Such
unique characteristics are very beneficial for FSRSSC, which can effectively
alleviate the scarcity issue of labeled remote sensing images since they can
provide more refined descriptions for each scene class. To fully exploit these
characteristics, we propose a novel scene graph matching-based meta-learning
framework for FSRSSC, called SGMNet. In this framework, a scene graph
construction module is carefully designed to represent each test remote sensing
image or each scene class as a scene graph, where the nodes reflect these
co-occurrence objects meanwhile the edges capture the spatial correlations
between these co-occurrence objects. Then, a scene graph matching module is
further developed to evaluate the similarity score between each test remote
sensing image and each scene class. Finally, based on the similarity scores, we
perform the scene class prediction via a nearest neighbor classifier. We
conduct extensive experiments on UCMerced LandUse, WHU19, AID, and
NWPU-RESISC45 datasets. The experimental results show that our method obtains
superior performance over the previous state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZSpeedL -- Evaluating the Performance of Zero-Shot Learning Methods using Low-Power Devices. (arXiv:2110.04535v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04535">
<div class="article-summary-box-inner">
<span><p>The recognition of unseen objects from a semantic representation or textual
description, usually denoted as zero-shot learning, is more prone to be used in
real-world scenarios when compared to traditional object recognition.
Nevertheless, no work has evaluated the feasibility of deploying zero-shot
learning approaches in these scenarios, particularly when using low-power
devices. In this paper, we provide the first benchmark on the inference time of
zero-shot learning, comprising an evaluation of state-of-the-art approaches
regarding their speed/accuracy trade-off. An analysis to the processing time of
the different phases of the ZSL inference stage reveals that visual feature
extraction is the major bottleneck in this paradigm, but, we show that
lightweight networks can dramatically reduce the overall inference time without
reducing the accuracy obtained by the de facto ResNet101 architecture. Also,
this benchmark evaluates how different ZSL approaches perform in low-power
devices, and how the visual feature extraction phase could be optimized in this
hardware. To foster the research and deployment of ZSL systems capable of
operating in real-world scenarios, we release the evaluation framework used in
this benchmark (https://github.com/CristianoPatricio/zsl-methods).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focus Your Distribution: Coarse-to-Fine Non-Contrastive Learning for Anomaly Detection and Localization. (arXiv:2110.04538v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04538">
<div class="article-summary-box-inner">
<span><p>The essence of unsupervised anomaly detection is to learn the compact
distribution of normal samples and detect outliers as anomalies in testing.
Meanwhile, the anomalies in real-world are usually subtle and fine-grained in a
high-resolution image especially for industrial applications. Towards this end,
we propose a novel framework for unsupervised anomaly detection and
localization. Our method aims at learning dense and compact distribution from
normal images with a coarse-to-fine alignment process. The coarse alignment
stage standardizes the pixel-wise position of objects in both image and feature
levels. The fine alignment stage then densely maximizes the similarity of
features among all corresponding locations in a batch. To facilitate the
learning with only normal images, we propose a new pretext task called
non-contrastive learning for the fine alignment stage. Non-contrastive learning
extracts robust and discriminating normal image representations without making
assumptions on abnormal samples, and it thus empowers our model to generalize
to various anomalous scenarios. Extensive experiments on two typical industrial
datasets of MVTec AD and BenTech AD demonstrate that our framework is effective
in detecting various real-world defects and achieves a new state-of-the-art in
industrial unsupervised anomaly detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class-Balanced Active Learning for Image Classification. (arXiv:2110.04543v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04543">
<div class="article-summary-box-inner">
<span><p>Active learning aims to reduce the labeling effort that is required to train
algorithms by learning an acquisition function selecting the most relevant data
for which a label should be requested from a large unlabeled data pool. Active
learning is generally studied on balanced datasets where an equal amount of
images per class is available. However, real-world datasets suffer from severe
imbalanced classes, the so called long-tail distribution. We argue that this
further complicates the active learning process, since the imbalanced data pool
can result in suboptimal classifiers. To address this problem in the context of
active learning, we proposed a general optimization framework that explicitly
takes class-balancing into account. Results on three datasets showed that the
method is general (it can be combined with most existing active learning
algorithms) and can be effectively applied to boost the performance of both
informative and representative-based active learning methods. In addition, we
showed that also on balanced datasets our method generally results in a
performance gain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP-Adapter: Better Vision-Language Models with Feature Adapters. (arXiv:2110.04544v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04544">
<div class="article-summary-box-inner">
<span><p>Large-scale contrastive vision-language pre-training has shown significant
progress in visual representation learning. Unlike traditional visual systems
trained by a fixed set of discrete labels, a new paradigm was introduced in
\cite{radford2021learning} to directly learn to align images with raw texts in
an open-vocabulary setting. On downstream tasks, a carefully chosen text prompt
is employed to make zero-shot predictions.~To avoid non-trivial prompt
engineering, context optimization \cite{zhou2021coop} has been proposed to
learn continuous vectors as task-specific prompts with few-shot training
examples.~In this paper, we show that there is an alternative path to achieve
better vision-language models other than prompt tuning.~While prompt tuning is
for the textual inputs, we propose CLIP-Adapter to conduct fine-tuning with
feature adapters on either visual or language branch. Specifically,
CLIP-Adapter adopts an additional bottleneck layer to learn new features and
performs residual-style feature blending with the original pre-trained
features.~As a consequence, CLIP-Adapter is able to outperform context
optimization while maintains a simple design. Experiments and extensive
ablation studies on various visual classification tasks demonstrate the
effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Data-Free Domain Generalization. (arXiv:2110.04545v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04545">
<div class="article-summary-box-inner">
<span><p>In this work, we investigate the unexplored intersection of domain
generalization and data-free learning. In particular, we address the question:
How can knowledge contained in models trained on different source data domains
can be merged into a single model that generalizes well to unseen target
domains, in the absence of source and target domain data? Machine learning
models that can cope with domain shift are essential for for real-world
scenarios with often changing data distributions. Prior domain generalization
methods typically rely on using source domain data, making them unsuitable for
private decentralized data. We define the novel problem of Data-Free Domain
Generalization (DFDG), a practical setting where models trained on the source
domains separately are available instead of the original datasets, and
investigate how to effectively solve the domain generalization problem in that
case. We propose DEKAN, an approach that extracts and fuses domain-specific
knowledge from the available teacher models into a student model robust to
domain shift. Our empirical evaluation demonstrates the effectiveness of our
method which achieves first state-of-the-art results in DFDG by significantly
outperforming ensemble and data-free knowledge distillation baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Representation Learning Meets Pseudo-Label Supervised Self-Distillation: A New Approach to Rare Disease Classification. (arXiv:2110.04558v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04558">
<div class="article-summary-box-inner">
<span><p>Rare diseases are characterized by low prevalence and are often chronically
debilitating or life-threatening. Imaging-based classification of rare diseases
is challenging due to the severe shortage in training examples. Few-shot
learning (FSL) methods tackle this challenge by extracting generalizable prior
knowledge from a large base dataset of common diseases and normal controls, and
transferring the knowledge to rare diseases. Yet, most existing methods require
the base dataset to be labeled and do not make full use of the precious
examples of the rare diseases. To this end, we propose in this work a novel
hybrid approach to rare disease classification, featuring two key novelties
targeted at the above drawbacks. First, we adopt the unsupervised
representation learning (URL) based on self-supervising contrastive loss,
whereby to eliminate the overhead in labeling the base dataset. Second, we
integrate the URL with pseudo-label supervised classification for effective
self-distillation of the knowledge about the rare diseases, composing a hybrid
approach taking advantages of both unsupervised and (pseudo-) supervised
learning on the base dataset. Experimental results on classification of rare
skin lesions show that our hybrid approach substantially outperforms existing
FSL methods (including those using fully supervised base dataset) for rare
disease classification via effective integration of the URL and pseudo-label
driven self-distillation, thus establishing a new state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporally Consistent Video Colorization with Deep Feature Propagation and Self-regularization Learning. (arXiv:2110.04562v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04562">
<div class="article-summary-box-inner">
<span><p>Video colorization is a challenging and highly ill-posed problem. Although
recent years have witnessed remarkable progress in single image colorization,
there is relatively less research effort on video colorization and existing
methods always suffer from severe flickering artifacts (temporal inconsistency)
or unsatisfying colorization performance. We address this problem from a new
perspective, by jointly considering colorization and temporal consistency in a
unified framework. Specifically, we propose a novel temporally consistent video
colorization framework (TCVC). TCVC effectively propagates frame-level deep
features in a bidirectional way to enhance the temporal consistency of
colorization. Furthermore, TCVC introduces a self-regularization learning (SRL)
scheme to minimize the prediction difference obtained with different time
steps. SRL does not require any ground-truth color videos for training and can
further improve temporal consistency. Experiments demonstrate that our method
can not only obtain visually pleasing colorized video, but also achieve clearly
better temporal consistency than state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Recognition of Abdominal Organs in Ultrasound Images based on Deep Neural Networks and K-Nearest-Neighbor Classification. (arXiv:2110.04563v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04563">
<div class="article-summary-box-inner">
<span><p>Abdominal ultrasound imaging has been widely used to assist in the diagnosis
and treatment of various abdominal organs. In order to shorten the examination
time and reduce the cognitive burden on the sonographers, we present a
classification method that combines the deep learning techniques and
k-Nearest-Neighbor (k-NN) classification to automatically recognize various
abdominal organs in the ultrasound images in real time. Fine-tuned deep neural
networks are used in combination with PCA dimension reduction to extract
high-level features from raw ultrasound images, and a k-NN classifier is
employed to predict the abdominal organ in the image. We demonstrate the
effectiveness of our method in the task of ultrasound image classification to
automatically recognize six abdominal organs. A comprehensive comparison of
different configurations is conducted to study the influence of different
feature extractors and classifiers on the classification accuracy. Both
quantitative and qualitative results show that with minimal training effort,
our method can "lazily" recognize the abdominal organs in the ultrasound images
in real time with an accuracy of 96.67%. Our implementation code is publicly
available at: https://github.com/LeeKeyu/abdominal_ultrasound_classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Space-Time-Separable Graph Convolutional Network for Pose Forecasting. (arXiv:2110.04573v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04573">
<div class="article-summary-box-inner">
<span><p>Human pose forecasting is a complex structured-data sequence-modelling task,
which has received increasing attention, also due to numerous potential
applications. Research has mainly addressed the temporal dimension as time
series and the interaction of human body joints with a kinematic tree or by a
graph. This has decoupled the two aspects and leveraged progress from the
relevant fields, but it has also limited the understanding of the complex
structural joint spatio-temporal dynamics of the human pose. Here we propose a
novel Space-Time-Separable Graph Convolutional Network (STS-GCN) for pose
forecasting. For the first time, STS-GCN models the human pose dynamics only
with a graph convolutional network (GCN), including the temporal evolution and
the spatial joint interaction within a single-graph framework, which allows the
cross-talk of motion and spatial correlations. Concurrently, STS-GCN is the
first space-time-separable GCN: the space-time graph connectivity is factored
into space and time affinity matrices, which bottlenecks the space-time
cross-talk, while enabling full joint-joint and time-time correlations. Both
affinity matrices are learnt end-to-end, which results in connections
substantially deviating from the standard kinematic tree and the linear-time
time series. In experimental evaluation on three complex, recent and
large-scale benchmarks, Human3.6M [Ionescu et al. TPAMI'14], AMASS [Mahmood et
al. ICCV'19] and 3DPW [Von Marcard et al. ECCV'18], STS-GCN outperforms the
state-of-the-art, surpassing the current best technique [Mao et al. ECCV'20] by
over 32% in average at the most difficult long-term predictions, while only
requiring 1.7% of its parameters. We explain the results qualitatively and
illustrate the graph interactions by the factored joint-joint and time-time
learnt graph connections.
</p>
<p>Our source code is available at: https://github.com/FraLuca/STSGCN
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Long-Tailed Learning: A Survey. (arXiv:2110.04596v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04596">
<div class="article-summary-box-inner">
<span><p>Deep long-tailed learning, one of the most challenging problems in visual
recognition, aims to train well-performing deep models from a large number of
images that follow a long-tailed class distribution. In the last decade, deep
learning has emerged as a powerful recognition model for learning high-quality
image representations and has led to remarkable breakthroughs in generic visual
recognition. However, long-tailed class imbalance, a common problem in
practical visual recognition tasks, often limits the practicality of deep
network based recognition models in real-world applications, since they can be
easily biased towards dominant classes and perform poorly on tail classes. To
address this problem, a large number of studies have been conducted in recent
years, making promising progress in the field of deep long-tailed learning.
Considering the rapid evolution of this field, this paper aims to provide a
comprehensive survey on recent advances in deep long-tailed learning. To be
specific, we group existing deep long-tailed learning studies into three main
categories (i.e., class re-balancing, information augmentation and module
improvement), and review these methods following this taxonomy in detail.
Afterward, we empirically analyze several state-of-the-art methods by
evaluating to what extent they address the issue of class imbalance via a newly
proposed evaluation metric, i.e., relative accuracy. We conclude the survey by
highlighting important applications of deep long-tailed learning and
identifying several promising directions for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Single/Multi-Attribute of Object with Symmetry and Group. (arXiv:2110.04603v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04603">
<div class="article-summary-box-inner">
<span><p>Attributes and objects can compose diverse compositions. To model the
compositional nature of these concepts, it is a good choice to learn them as
transformations, e.g., coupling and decoupling. However, complex
transformations need to satisfy specific principles to guarantee rationality.
Here, we first propose a previously ignored principle of attribute-object
transformation: Symmetry. For example, coupling peeled-apple with attribute
peeled should result in peeled-apple, and decoupling peeled from apple should
still output apple. Incorporating the symmetry, we propose a transformation
framework inspired by group theory, i.e., SymNet. It consists of two modules:
Coupling Network and Decoupling Network. We adopt deep neural networks to
implement SymNet and train it in an end-to-end paradigm with the group axioms
and symmetry as objectives. Then, we propose a Relative Moving Distance (RMD)
based method to utilize the attribute change instead of the attribute pattern
itself to classify attributes. Besides the compositions of single-attribute and
object, our RMD is also suitable for complex compositions of multiple
attributes and objects when incorporating attribute correlations. SymNet can be
utilized for attribute learning, compositional zero-shot learning and
outperforms the state-of-the-art on four widely-used benchmarks. Code is at
https://github.com/DirtyHarryLYL/SymNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning MRI Artifact Removal With Unpaired Data. (arXiv:2110.04604v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04604">
<div class="article-summary-box-inner">
<span><p>Retrospective artifact correction (RAC) improves image quality post
acquisition and enhances image usability. Recent machine learning driven
techniques for RAC are predominantly based on supervised learning and therefore
practical utility can be limited as data with paired artifact-free and
artifact-corrupted images are typically insufficient or even non-existent. Here
we show that unwanted image artifacts can be disentangled and removed from an
image via an RAC neural network learned with unpaired data. This implies that
our method does not require matching artifact-corrupted data to be either
collected via acquisition or generated via simulation. Experimental results
demonstrate that our method is remarkably effective in removing artifacts and
retaining anatomical details in images with different contrasts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Google Landmark Retrieval 2021 Competition Third Place Solution. (arXiv:2110.04619v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04619">
<div class="article-summary-box-inner">
<span><p>We present our solutions to the Google Landmark Challenges 2021, for both the
retrieval and the recognition tracks. Both solutions are ensembles of
transformers and ConvNet models based on Sub-center ArcFace with dynamic
margins. Since the two tracks share the same training data, we used the same
pipeline and training approach, but with different model selections for the
ensemble and different post-processing. The key improvement over last year is
newer state-of-the-art vision architectures, especially transformers which
significantly outperform ConvNets for the retrieval task. We finished third and
fourth places for the retrieval and recognition tracks respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vector-quantized Image Modeling with Improved VQGAN. (arXiv:2110.04627v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04627">
<div class="article-summary-box-inner">
<span><p>Pretraining language models with next-token prediction on massive text
corpora has delivered phenomenal zero-shot, few-shot, transfer learning and
multi-tasking capabilities on both generative and discriminative language
tasks. Motivated by this success, we explore a Vector-quantized Image Modeling
(VIM) approach that involves pretraining a Transformer to predict rasterized
image tokens autoregressively. The discrete image tokens are encoded from a
learned Vision-Transformer-based VQGAN (ViT-VQGAN). We first propose multiple
improvements over vanilla VQGAN from architecture to codebook learning,
yielding better efficiency and reconstruction fidelity. The improved ViT-VQGAN
further improves vector-quantized image modeling tasks, including
unconditional, class-conditioned image generation and unsupervised
representation learning. When trained on ImageNet at 256x256 resolution, we
achieve Inception Score (IS) of 175.1 and Fr'echet Inception Distance (FID) of
4.17, a dramatic improvement over the vanilla VQGAN, which obtains 70.6 and
17.04 for IS and FID, respectively. Based on ViT-VQGAN and unsupervised
pretraining, we further evaluate the pretrained Transformer by averaging
intermediate features, similar to Image GPT (iGPT). This ImageNet-pretrained
VIM-L significantly beats iGPT-L on linear-probe accuracy from 60.3% to 72.2%
for a similar model size. ViM-L also outperforms iGPT-XL which is trained with
extra web image data and larger model size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DenseNet approach to segmentation and classification of dermatoscopic skin lesions images. (arXiv:2110.04632v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04632">
<div class="article-summary-box-inner">
<span><p>At present, cancer is one of the most important health issues in the world.
Because early detection and appropriate treatment in cancer are very effective
in the recovery and survival of patients, image processing as a diagnostic tool
can help doctors to diagnose in the first recognition of cancer. One of the
most important steps in diagnosing a skin lesion is to automatically detect the
border of the skin image because the accuracy of the next steps depends on it.
If these subtleties are identified, they can have a great impact on the
diagnosis of the disease. Therefore, there is a good opportunity to develop
more accurate algorithms to analyze such images. This paper proposes an
improved method for segmentation and classification for skin lesions using two
architectures, the U-Net for image segmentation and the DenseNet121 for image
classification which have excellent accuracy. We tested the segmentation
architecture of our model on the ISIC-2018 dataset and the classification on
the HAM10000 dataset. Our results show that the combination of U-Net and
DenseNet121 architectures provides acceptable results in dermatoscopic image
analysis compared to previous research. Another classification examined in this
study is cancerous and non-cancerous samples. In this classification, cancerous
and non-cancerous samples were detected in DenseNet121 network with 79.49% and
93.11% accuracy respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complex Network-Based Approach for Feature Extraction and Classification of Musical Genres. (arXiv:2110.04654v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04654">
<div class="article-summary-box-inner">
<span><p>Musical genre's classification has been a relevant research topic. The
association between music and genres is fundamental for the media industry,
which manages musical recommendation systems, and for music streaming services,
which may appear classified by genres. In this context, this work presents a
feature extraction method for the automatic classification of musical genres,
based on complex networks and their topological measurements. The proposed
method initially converts the musics into sequences of musical notes and then
maps the sequences as complex networks. Topological measurements are extracted
to characterize the network topology, which composes a feature vector that
applies to the classification of musical genres. The method was evaluated in
the classification of 10 musical genres by adopting the GTZAN dataset and 8
musical genres by adopting the FMA dataset. The results were compared with
methods in the literature. The proposed method outperformed all compared
methods by presenting high accuracy and low standard deviation, showing its
suitability for the musical genre's classification, which contributes to the
media industry in the automatic classification with assertiveness and
robustness. The proposed method is implemented in an open source in the Python
language and freely available at https://github.com/omatheuspimenta/examinner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-appearance-aided Differential Evolution for Motion Transfer. (arXiv:2110.04658v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04658">
<div class="article-summary-box-inner">
<span><p>Image animation transfers the motion of a driving video to a static object in
a source image, while keeping the source identity unchanged. Great progress has
been made in unsupervised motion transfer recently, where no labelled data or
ground truth domain priors are needed. However, current unsupervised approaches
still struggle when there are large motion or viewpoint discrepancies between
the source and driving images. In this paper, we introduce three measures that
we found to be effective for overcoming such large viewpoint changes. Firstly,
to achieve more fine-grained motion deformation fields, we propose to apply
Neural-ODEs for parametrizing the evolution dynamics of the motion transfer
from source to driving. Secondly, to handle occlusions caused by large
viewpoint and motion changes, we take advantage of the appearance flow obtained
from the source image itself ("self-appearance"), which essentially "borrows"
similar structures from other regions of an image to inpaint missing regions.
Finally, our framework is also able to leverage the information from additional
reference views which help to drive the source identity in spite of varying
motion state. Extensive experiments demonstrate that our approach outperforms
the state-of-the-arts by a significant margin (~40%), across six benchmarks
varying from human faces, human bodies to robots and cartoon characters. Model
generality analysis indicates that our approach generalises the best across
different object categories as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">K-Splits: Improved K-Means Clustering Algorithm to Automatically Detect the Number of Clusters. (arXiv:2110.04660v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04660">
<div class="article-summary-box-inner">
<span><p>This paper introduces k-splits, an improved hierarchical algorithm based on
k-means to cluster data without prior knowledge of the number of clusters.
K-splits starts from a small number of clusters and uses the most significant
data distribution axis to split these clusters incrementally into better fits
if needed. Accuracy and speed are two main advantages of the proposed method.
We experiment on six synthetic benchmark datasets plus two real-world datasets
MNIST and Fashion-MNIST, to prove that our algorithm has excellent accuracy in
finding the correct number of clusters under different conditions. We also show
that k-splits is faster than similar methods and can even be faster than the
standard k-means in lower dimensions. Finally, we suggest using k-splits to
uncover the exact position of centroids and then input them as initial points
to the k-means algorithm to fine-tune the results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Road Extraction: A Dataset for Map Update using Aerial Images. (arXiv:2110.04690v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04690">
<div class="article-summary-box-inner">
<span><p>The increasing availability of satellite and aerial imagery has sparked
substantial interest in automatically updating street maps by processing aerial
images. Until now, the community has largely focused on road extraction, where
road networks are inferred from scratch from an aerial image. However, given
that relatively high-quality maps exist in most parts of the world, in
practice, inference approaches must be applied to update existing maps rather
than infer new ones. With recent road extraction methods showing high accuracy,
we argue that it is time to transition to the more practical map update task,
where an existing map is updated by adding, removing, and shifting roads,
without introducing errors in parts of the existing map that remain up-to-date.
In this paper, we develop a new dataset called MUNO21 for the map update task,
and show that it poses several new and interesting research challenges. We
evaluate several state-of-the-art road extraction methods on MUNO21, and find
that substantial further improvements in accuracy will be needed to realize
automatic map update.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unauthorized AI cannot Recognize Me: Reversible Adversarial Example. (arXiv:1811.00189v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1811.00189">
<div class="article-summary-box-inner">
<span><p>In this study, we propose a new methodology to control how user's data is
recognized and used by AI via exploiting the properties of adversarial
examples. For this purpose, we propose reversible adversarial example (RAE), a
new type of adversarial example. A remarkable feature of RAE is that the image
can be correctly recognized and used by the AI model specified by the user
because the authorized AI can recover the original image from the RAE exactly
by eliminating adversarial perturbation. On the other hand, other unauthorized
AI models cannot recognize it correctly because it functions as an adversarial
example. Moreover, RAE can be considered as one type of encryption to computer
vision since reversibility guarantees the decryption. To realize RAE, we
combine three technologies, adversarial example, reversible data hiding for
exact recovery of adversarial perturbation, and encryption for selective
control of AIs who can remove adversarial perturbation. Experimental results
show that the proposed method can achieve comparable attack ability with the
corresponding adversarial attack method and similar visual quality with the
original image, including white-box attacks and black-box attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extreme Low Resolution Activity Recognition with Confident Spatial-Temporal Attention Transfer. (arXiv:1909.03580v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.03580">
<div class="article-summary-box-inner">
<span><p>Activity recognition on extreme low-resolution videos, e.g., a resolution of
12*16 pixels, plays a vital role in far-view surveillance and
privacy-preserving multimedia analysis. Low-resolution videos only contain
limited information. Given the fact that one same activity may be represented
by videos in both high resolution (HR) and extreme low resolution (eLR), it is
worth studying to utilize the relevant HR data to improve the eLR activity
recognition. In this work, we propose a novel Confident Spatial-Temporal
Attention Transfer (CSTAT) for eLR activity recognition. CSTAT can acquire
information from HR data by reducing the attention differences with a
transfer-learning strategy. Besides, the credibility of the supervisory signal
is also taken into consideration for a more confident transferring process.
Experimental results on two well-known datasets, i.e., UCF101 and HMDB51,
demonstrate that, the proposed method can effectively improve the accuracy of
eLR activity recognition and achieve an accuracy of 59.23% on 12*16 videos in
HMDB51, a state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MILA: Multi-Task Learning from Videos via Efficient Inter-Frame Attention. (arXiv:2002.07362v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.07362">
<div class="article-summary-box-inner">
<span><p>Prior work in multi-task learning has mainly focused on predictions on a
single image. In this work, we present a new approach for multi-task learning
from videos via efficient inter-frame local attention (MILA). Our approach
contains a novel inter-frame attention module which allows learning of
task-specific attention across frames. We embed the attention module in a
``slow-fast'' architecture, where the slower network runs on sparsely sampled
keyframes and the light-weight shallow network runs on non-keyframes at a high
frame rate. We also propose an effective adversarial learning strategy to
encourage the slow and fast network to learn similar features. Our approach
ensures low-latency multi-task learning while maintaining high quality
predictions. Experiments show competitive accuracy compared to state-of-the-art
on two multi-task learning benchmarks while reducing the number of floating
point operations (FLOPs) by up to 70\%. In addition, our attention based
feature propagation method (ILA) outperforms prior work in terms of task
accuracy while also reducing up to 90\% of FLOPs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain Structure Preserving Projection for Heterogeneous Domain Adaptation. (arXiv:2004.12427v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.12427">
<div class="article-summary-box-inner">
<span><p>Heterogeneous Domain Adaptation (HDA) addresses the transfer learning
problems where data from the source and target domains are of different
modalities (e.g., texts and images) or feature dimensions (e.g., features
extracted with different methods). It is useful for multi-modal data analysis.
Traditional domain adaptation algorithms assume that the representations of
source and target samples reside in the same feature space, hence are likely to
fail in solving the heterogeneous domain adaptation problem. Contemporary
state-of-the-art HDA approaches are usually composed of complex optimization
objectives for favourable performance and are therefore computationally
expensive and less generalizable. To address these issues, we propose a novel
Cross-Domain Structure Preserving Projection (CDSPP) algorithm for HDA. As an
extension of the classic LPP to heterogeneous domains, CDSPP aims to learn
domain-specific projections to map sample features from source and target
domains into a common subspace such that the class consistency is preserved and
data distributions are sufficiently aligned. CDSPP is simple and has
deterministic solutions by solving a generalized eigenvalue problem. It is
naturally suitable for supervised HDA but has also been extended for
semi-supervised HDA where the unlabelled target domain samples are available.
Extensive experiments have been conducted on commonly used benchmark datasets
(i.e. Office-Caltech, Multilingual Reuters Collection, NUS-WIDE-ImageNet) for
HDA as well as the Office-Home dataset firstly introduced for HDA by ourselves
due to its significantly larger number of classes than the existing ones (65 vs
10, 6 and 8). The experimental results of both supervised and semi-supervised
HDA demonstrate the superior performance of our proposed method against
contemporary state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WaveFuse: A Unified Deep Framework for Image Fusion with Discrete Wavelet Transform. (arXiv:2007.14110v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.14110">
<div class="article-summary-box-inner">
<span><p>We propose an unsupervised image fusion architecture for multiple application
scenarios based on the combination of multi-scale discrete wavelet transform
through regional energy and deep learning. To our best knowledge, this is the
first time the conventional image fusion method has been combined with deep
learning. The useful information of feature maps can be utilized adequately
through multi-scale discrete wavelet transform in our proposed method.Compared
with other state-of-the-art fusion method, the proposed algorithm exhibits
better fusion performance in both subjective and objective evaluation.
Moreover, it's worth mentioning that comparable fusion performance trained in
COCO dataset can be obtained by training with a much smaller dataset with only
hundreds of images chosen randomly from COCO. Hence, the training time is
shortened substantially, leading to the improvement of the model's performance
both in practicality and training efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultAV: Multiplicative Adversarial Videos. (arXiv:2009.08058v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.08058">
<div class="article-summary-box-inner">
<span><p>The majority of adversarial machine learning research focuses on additive
attacks, which add adversarial perturbation to input data. On the other hand,
unlike image recognition problems, only a handful of attack approaches have
been explored in the video domain. In this paper, we propose a novel attack
method against video recognition models, Multiplicative Adversarial Videos
(MultAV), which imposes perturbation on video data by multiplication. MultAV
has different noise distributions to the additive counterparts and thus
challenges the defense methods tailored to resisting additive adversarial
attacks. Moreover, it can be generalized to not only Lp-norm attacks with a new
adversary constraint called ratio bound, but also different types of physically
realizable attacks. Experimental results show that the model adversarially
trained against additive attack is less robust to MultAV.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Relational Image Captioning via Multi-task Triple-Stream Networks. (arXiv:2010.03855v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03855">
<div class="article-summary-box-inner">
<span><p>We introduce dense relational captioning, a novel image captioning task which
aims to generate multiple captions with respect to relational information
between objects in a visual scene. Relational captioning provides explicit
descriptions for each relationship between object combinations. This framework
is advantageous in both diversity and amount of information, leading to a
comprehensive image understanding based on relationships, e.g., relational
proposal generation. For relational understanding between objects, the
part-of-speech (POS; i.e., subject-object-predicate categories) can be a
valuable prior information to guide the causal sequence of words in a caption.
We enforce our framework to learn not only to generate captions but also to
understand the POS of each word. To this end, we propose the multi-task
triple-stream network (MTTSNet) which consists of three recurrent units
responsible for each POS which is trained by jointly predicting the correct
captions and POS for each word. In addition, we found that the performance of
MTTSNet can be improved by modulating the object embeddings with an explicit
relational module. We demonstrate that our proposed model can generate more
diverse and richer captions, via extensive experimental analysis on large scale
datasets and several metrics. Then, we present applications of our framework to
holistic image captioning, scene graph generation, and retrieval tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Domain Adaptation via Clustering Uncertainty-weighted Embeddings. (arXiv:2010.08666v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.08666">
<div class="article-summary-box-inner">
<span><p>Generalizing deep neural networks to new target domains is critical to their
real-world utility. In practice, it may be feasible to get some target data
labeled, but to be cost-effective it is desirable to select a
maximally-informative subset via active learning (AL). We study the problem of
AL under a domain shift, called Active Domain Adaptation (Active DA). We
demonstrate how existing AL approaches based solely on model uncertainty or
diversity sampling are less effective for Active DA. We propose Clustering
Uncertainty-weighted Embeddings (CLUE), a novel label acquisition strategy for
Active DA that performs uncertainty-weighted clustering to identify target
instances for labeling that are both uncertain under the model and diverse in
feature space. CLUE consistently outperforms competing label acquisition
strategies for Active DA and AL across learning settings on 6 diverse domain
shifts for image classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Unsupervised Domain Adaptation for Semantic Segmentation. (arXiv:2010.09236v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09236">
<div class="article-summary-box-inner">
<span><p>Unsupervised Domain Adaptation (UDA) for semantic segmentation has been
favorably applied to real-world scenarios in which pixel-level labels are hard
to be obtained. In most of the existing UDA methods, all target data are
assumed to be introduced simultaneously. Yet, the data are usually presented
sequentially in the real world. Moreover, Continual UDA, which deals with more
practical scenarios with multiple target domains in the continual learning
setting, has not been actively explored. In this light, we propose Continual
UDA for semantic segmentation based on a newly designed Expanding
Target-specific Memory (ETM) framework. Our novel ETM framework contains
Target-specific Memory (TM) for each target domain to alleviate catastrophic
forgetting. Furthermore, a proposed Double Hinge Adversarial (DHA) loss leads
the network to produce better UDA performance overall. Our design of the TM and
training objectives let the semantic segmentation network adapt to the current
target domain while preserving the knowledge learned on previous target
domains. The model with the proposed framework outperforms other
state-of-the-art models in continual learning settings on standard benchmarks
such as GTA5, SYNTHIA, CityScapes, IDD, and Cross-City datasets. The source
code is available at https://github.com/joonh-kim/ETM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Fair Knowledge Transfer for Imbalanced Domain Adaptation. (arXiv:2010.12184v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12184">
<div class="article-summary-box-inner">
<span><p>Domain adaptation (DA) becomes an up-and-coming technique to address the
insufficient or no annotation issue by exploiting external source knowledge.
Existing DA algorithms mainly focus on practical knowledge transfer through
domain alignment. Unfortunately, they ignore the fairness issue when the
auxiliary source is extremely imbalanced across different categories, which
results in severe under-presented knowledge adaptation of minority source set.
To this end, we propose a Towards Fair Knowledge Transfer (TFKT) framework to
handle the fairness challenge in imbalanced cross-domain learning.
Specifically, a novel cross-domain mixup generation is exploited to augment the
minority source set with target information to enhance fairness. Moreover, dual
distinct classifiers and cross-domain prototype alignment are developed to seek
a more robust classifier boundary and mitigate the domain shift. Such three
strategies are formulated into a unified framework to address the fairness
issue and domain shift challenge. Extensive experiments over two popular
benchmarks have verified the effectiveness of our proposed model by comparing
to existing state-of-the-art DA models, and especially our model significantly
improves over 20% on two benchmarks in terms of the overall accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustness May Be at Odds with Fairness: An Empirical Study on Class-wise Accuracy. (arXiv:2010.13365v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.13365">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have made significant advancement,
however, they are widely known to be vulnerable to adversarial attacks.
Adversarial training is the most widely used technique for improving
adversarial robustness to strong white-box attacks. Prior works have been
evaluating and improving the model average robustness without class-wise
evaluation. The average evaluation alone might provide a false sense of
robustness. For example, the attacker can focus on attacking the vulnerable
class, which can be dangerous, especially, when the vulnerable class is a
critical one, such as "human" in autonomous driving. We propose an empirical
study on the class-wise accuracy and robustness of adversarially trained
models. We find that there exists inter-class discrepancy for accuracy and
robustness even when the training dataset has an equal number of samples for
each class. For example, in CIFAR10, "cat" is much more vulnerable than other
classes. Moreover, this inter-class discrepancy also exists for normally
trained models, while adversarial training tends to further increase the
discrepancy. Our work aims to investigate the following questions: (a) is the
phenomenon of inter-class discrepancy universal regardless of datasets, model
architectures and optimization hyper-parameters? (b) If so, what can be
possible explanations for the inter-class discrepancy? (c) Can the techniques
proposed in the long tail classification be readily extended to adversarial
training for addressing the inter-class discrepancy?
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep-Dup: An Adversarial Weight Duplication Attack Framework to Crush Deep Neural Network in Multi-Tenant FPGA. (arXiv:2011.03006v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.03006">
<div class="article-summary-box-inner">
<span><p>The wide deployment of Deep Neural Networks (DNN) in high-performance cloud
computing platforms brought to light multi-tenant cloud field-programmable gate
arrays (FPGA) as a popular choice of accelerator to boost performance due to
its hardware reprogramming flexibility. Such a multi-tenant FPGA setup for DNN
acceleration potentially exposes DNN interference tasks under severe threat
from malicious users. This work, to the best of our knowledge, is the first to
explore DNN model vulnerabilities in multi-tenant FPGAs. We propose a novel
adversarial attack framework: Deep-Dup, in which the adversarial tenant can
inject adversarial faults to the DNN model in the victim tenant of FPGA.
Specifically, she can aggressively overload the shared power distribution
system of FPGA with malicious power-plundering circuits, achieving adversarial
weight duplication (AWD) hardware attack that duplicates certain DNN weight
packages during data transmission between off-chip memory and on-chip buffer,
to hijack the DNN function of the victim tenant. Further, to identify the most
vulnerable DNN weight packages for a given malicious objective, we propose a
generic vulnerable weight package searching algorithm, called Progressive
Differential Evolution Search (P-DES), which is, for the first time, adaptive
to both deep learning white-box and black-box attack models. The proposed
Deep-Dup is experimentally validated in a developed multi-tenant FPGA
prototype, for two popular deep learning applications, i.e., Object Detection
and Image Classification. Successful attacks are demonstrated in six popular
DNN architectures (e.g., YOLOv2, ResNet-50, MobileNet, etc.)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Interpretable Classification and Weakly-Supervised Segmentation of Histology Images via Max-Min Uncertainty. (arXiv:2011.07221v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.07221">
<div class="article-summary-box-inner">
<span><p>Weakly-supervised learning (WSL) has recently triggered substantial interest
as it mitigates the lack of pixel-wise annotations. Given global image labels,
WSL methods yield pixel-level predictions (segmentations), which enable to
interpret class predictions. Despite their recent success, mostly with natural
images, such methods can face important challenges when the foreground and
background regions have similar visual cues, yielding high false-positive rates
in segmentations, as is the case in challenging histology images. WSL training
is commonly driven by standard classification losses, which implicitly maximize
model confidence, and locate the discriminative regions linked to
classification decisions. Therefore, they lack mechanisms for modeling
explicitly non-discriminative regions and reducing false-positive rates. We
propose novel regularization terms, which enable the model to seek both
non-discriminative and discriminative regions, while discouraging unbalanced
segmentations. We introduce high uncertainty as a criterion to localize
non-discriminative regions that do not affect classifier decision, and describe
it with original Kullback-Leibler (KL) divergence losses evaluating the
deviation of posterior predictions from the uniform distribution. Our KL terms
encourage high uncertainty of the model when the latter inputs the latent
non-discriminative regions. Our loss integrates: (i) a cross-entropy seeking a
foreground, where model confidence about class prediction is high; (ii) a KL
regularizer seeking a background, where model uncertainty is high; and (iii)
log-barrier terms discouraging unbalanced segmentations. Comprehensive
experiments and ablation studies over the public GlaS colon cancer data and a
Camelyon16 patch-based benchmark for breast cancer show substantial
improvements over state-of-the-art WSL methods, and confirm the effect of our
new regularizers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Drone LAMS: A Drone-based Face Detection Dataset with Large Angles and Many Scenarios. (arXiv:2011.07689v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.07689">
<div class="article-summary-box-inner">
<span><p>This work presented a new drone-based face detection dataset Drone LAMS in
order to solve issues of low performance of drone-based face detection in
scenarios such as large angles which was a predominant working condition when a
drone flies high. The proposed dataset captured images from 261 videos with
over 43k annotations and 4.0k images with pitch or yaw angle in the range of
-90{\deg} to 90{\deg}. Drone LAMS showed significant improvement over currently
available drone-based face detection datasets in terms of detection
performance, especially with large pitch and yaw angle. Detailed analysis of
how key factors, such as duplication rate, annotation method, etc., impact
dataset performance was also provided to facilitate further usage of a drone on
face detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Saliency-based segmentation of dermoscopic images using color information. (arXiv:2011.13179v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.13179">
<div class="article-summary-box-inner">
<span><p>Skin lesion segmentation is one of the crucial steps for an efficient
non-invasive computer-aided early diagnosis of melanoma. This paper
investigates how color information, besides saliency, can be used to determine
the pigmented lesion region automatically. Unlike most existing segmentation
methods using only the saliency in order to discriminate against the skin
lesion from the surrounding regions, we propose a novel method employing a
binarization process coupled with new perceptual criteria, inspired by the
human visual perception, related to the properties of saliency and color of the
input image data distribution. As a means of refining the accuracy of the
proposed method, the segmentation step is preceded by a pre-processing aimed at
reducing the computation burden, removing artifacts, and improving contrast. We
have assessed the method on two public databases, including 1497 dermoscopic
images. We have also compared its performance with classical and recent
saliency-based methods designed explicitly for dermoscopic images. The
qualitative and quantitative evaluation indicates that the proposed method is
promising since it produces an accurate skin lesion segmentation and performs
satisfactorily compared to other existing saliency-based segmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S2FGAN: Semantically Aware Interactive Sketch-to-Face Translation. (arXiv:2011.14785v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14785">
<div class="article-summary-box-inner">
<span><p>Interactive facial image manipulation attempts to edit single and multiple
face attributes using a photo-realistic face and/or semantic mask as input. In
the absence of the photo-realistic image (only sketch/mask available), previous
methods only retrieve the original face but ignore the potential of aiding
model controllability and diversity in the translation process. This paper
proposes a sketch-to-image generation framework called S2FGAN, aiming to
improve users' ability to interpret and flexibility of face attribute editing
from a simple sketch. The proposed framework modifies the constrained latent
space semantics trained on Generative Adversarial Networks (GANs). We employ
two latent spaces to control the face appearance and adjust the desired
attributes of the generated face. Instead of constraining the translation
process by using a reference image, the users can command the model to retouch
the generated images by involving the semantic information in the generation
process. In this way, our method can manipulate single or multiple face
attributes by only specifying attributes to be changed. Extensive experimental
results on CelebAMask-HQ dataset empirically shows our superior performance and
effectiveness on this task. Our method successfully outperforms
state-of-the-art methods on attribute manipulation by exploiting greater
control of attribute intensity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometric Adversarial Attacks and Defenses on 3D Point Clouds. (arXiv:2012.05657v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05657">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are prone to adversarial examples that maliciously alter
the network's outcome. Due to the increasing popularity of 3D sensors in
safety-critical systems and the vast deployment of deep learning models for 3D
point sets, there is a growing interest in adversarial attacks and defenses for
such models. So far, the research has focused on the semantic level, namely,
deep point cloud classifiers. However, point clouds are also widely used in a
geometric-related form that includes encoding and reconstructing the geometry.
In this work, we are the first to consider the problem of adversarial examples
at a geometric level. In this setting, the question is how to craft a small
change to a clean source point cloud that leads, after passing through an
autoencoder model, to the reconstruction of a different target shape. Our
attack is in sharp contrast to existing semantic attacks on 3D point clouds.
While such works aim to modify the predicted label by a classifier, we alter
the entire reconstructed geometry. Additionally, we demonstrate the robustness
of our attack in the case of defense, where we show that remnant
characteristics of the target shape are still present at the output after
applying the defense to the adversarial input. Our code is publicly available
at https://github.com/itailang/geometric_adv.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EventHands: Real-Time Neural 3D Hand Pose Estimation from an Event Stream. (arXiv:2012.06475v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.06475">
<div class="article-summary-box-inner">
<span><p>3D hand pose estimation from monocular videos is a long-standing and
challenging problem, which is now seeing a strong upturn. In this work, we
address it for the first time using a single event camera, i.e., an
asynchronous vision sensor reacting on brightness changes. Our EventHands
approach has characteristics previously not demonstrated with a single RGB or
depth camera such as high temporal resolution at low data throughputs and
real-time performance at 1000 Hz. Due to the different data modality of event
cameras compared to classical cameras, existing methods cannot be directly
applied to and re-trained for event streams. We thus design a new neural
approach which accepts a new event stream representation suitable for learning,
which is trained on newly-generated synthetic event streams and can generalise
to real data. Experiments show that EventHands outperforms recent monocular
methods using a colour (or depth) camera in terms of accuracy and its ability
to capture hand motions of unprecedented speed. Our method, the event stream
simulator and the dataset are publicly available; see
https://4dqv.mpi-inf.mpg.de/EventHands/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attentional Biased Stochastic Gradient for Imbalanced Classification. (arXiv:2012.06951v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.06951">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a simple yet effective method (ABSGD) for
addressing the data imbalance issue in deep learning. Our method is a simple
modification to momentum SGD where we leverage an attentional mechanism to
assign an individual importance weight to each gradient in the mini-batch.
Unlike many existing heuristic-driven methods for tackling data imbalance, our
method is grounded in {\it theoretically justified distributionally robust
optimization (DRO)}, which is guaranteed to converge to a stationary point of
an information-regularized DRO problem. The individual-level weight of a
sampled data is systematically proportional to the exponential of a scaled loss
value of the data, where the scaling factor is interpreted as the
regularization parameter in the framework of information-regularized DRO.
Compared with existing class-level weighting schemes, our method can capture
the diversity between individual examples within each class. Compared with
existing individual-level weighting methods using meta-learning that require
three backward propagations for computing mini-batch stochastic gradients, our
method is more efficient with only one backward propagation at each iteration
as in standard deep learning methods. To balance between the learning of
feature extraction layers and the learning of the classifier layer, we employ a
two-stage method that uses SGD for pretraining followed by ABSGD for learning a
robust classifier and finetuning lower layers. Our empirical studies on several
benchmark datasets demonstrate the effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Block-based Hybrid Image Compression. (arXiv:2012.09550v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.09550">
<div class="article-summary-box-inner">
<span><p>Recent works on learned image compression perform encoding and decoding
processes in a full-resolution manner, resulting in two problems when deployed
for practical applications. First, parallel acceleration of the autoregressive
entropy model cannot be achieved due to serial decoding. Second,
full-resolution inference often causes the out-of-memory(OOM) problem with
limited GPU resources, especially for high-resolution images. Block partition
is a good design choice to handle the above issues, but it brings about new
challenges in reducing the redundancy between blocks and eliminating block
effects. To tackle the above challenges, this paper provides a learned
block-based hybrid image compression (LBHIC) framework. Specifically, we
introduce explicit intra prediction into a learned image compression framework
to utilize the relation among adjacent blocks. Superior to context modeling by
linear weighting of neighbor pixels in traditional codecs, we propose a
contextual prediction module (CPM) to better capture long-range correlations by
utilizing the strip pooling to extract the most relevant information in
neighboring latent space, thus achieving effective information prediction.
Moreover, to alleviate blocking artifacts, we further propose a boundary-aware
postprocessing module (BPM) with the edge importance taken into account.
Extensive experiments demonstrate that the proposed LBHIC codec outperforms the
VVC, with a bit-rate conservation of 4.1%, and reduces the decoding time by
approximately 86.7% compared with that of state-of-the-art learned image
compression methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SENTRY: Selective Entropy Optimization via Committee Consistency for Unsupervised Domain Adaptation. (arXiv:2012.11460v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.11460">
<div class="article-summary-box-inner">
<span><p>Many existing approaches for unsupervised domain adaptation (UDA) focus on
adapting under only data distribution shift and offer limited success under
additional cross-domain label distribution shift. Recent work based on
self-training using target pseudo-labels has shown promise, but on challenging
shifts pseudo-labels may be highly unreliable, and using them for self-training
may cause error accumulation and domain misalignment. We propose Selective
Entropy Optimization via Committee Consistency (SENTRY), a UDA algorithm that
judges the reliability of a target instance based on its predictive consistency
under a committee of random image transformations. Our algorithm then
selectively minimizes predictive entropy to increase confidence on highly
consistent target instances, while maximizing predictive entropy to reduce
confidence on highly inconsistent ones. In combination with pseudo-label based
approximate target class balancing, our approach leads to significant
improvements over the state-of-the-art on 27/31 domain shifts from standard UDA
benchmarks as well as benchmarks designed to stress-test adaptation under label
distribution shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-Efficient Hierarchical Neural Architecture Search for Image Restoration. (arXiv:2012.13212v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.13212">
<div class="article-summary-box-inner">
<span><p>Recently, much attention has been spent on neural architecture search (NAS),
aiming to outperform those manually-designed neural architectures on high-level
vision recognition tasks. Inspired by the success, here we attempt to leverage
NAS techniques to automatically design efficient network architectures for
low-level image restoration tasks. In particular, we propose a memory-efficient
hierarchical NAS (termed HiNAS) and apply it to two such tasks: image denoising
and image super-resolution. HiNAS adopts gradient based search strategies and
builds a flexible hierarchical search space, including the inner search space
and outer search space. They are in charge of designing cell architectures and
deciding cell widths, respectively. For the inner search space, we propose a
layer-wise architecture sharing strategy (LWAS), resulting in more flexible
architectures and better performance. For the outer search space, we design a
cell-sharing strategy to save memory, and considerably accelerate the search
speed. The proposed HiNAS method is both memory and computation efficient. With
a single GTX1080Ti GPU, it takes only about 1 hour for searching for denoising
network on the BSD-500 dataset and 3.5 hours for searching for the
super-resolution structure on the DIV2K dataset. Experiments show that the
architectures found by HiNAS have fewer parameters and enjoy a faster inference
speed, while achieving highly competitive performance compared with
state-of-the-art methods. Code is available at:
https://github.com/hkzhang91/HiNAS
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistic 3D Multi-Modal, Multi-Object Tracking for Autonomous Driving. (arXiv:2012.13755v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.13755">
<div class="article-summary-box-inner">
<span><p>Multi-object tracking is an important ability for an autonomous vehicle to
safely navigate a traffic scene. Current state-of-the-art follows the
tracking-by-detection paradigm where existing tracks are associated with
detected objects through some distance metric. The key challenges to increase
tracking accuracy lie in data association and track life cycle management. We
propose a probabilistic, multi-modal, multi-object tracking system consisting
of different trainable modules to provide robust and data-driven tracking
results. First, we learn how to fuse features from 2D images and 3D LiDAR point
clouds to capture the appearance and geometric information of an object.
Second, we propose to learn a metric that combines the Mahalanobis and feature
distances when comparing a track and a new detection in data association. And
third, we propose to learn when to initialize a track from an unmatched object
detection. Through extensive quantitative and qualitative results, we show that
when using the same object detectors our method outperforms state-of-the-art
approaches on the NuScenes and KITTI datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spending Your Winning Lottery Better After Drawing It. (arXiv:2101.03255v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.03255">
<div class="article-summary-box-inner">
<span><p>Lottery Ticket Hypothesis (LTH) suggests that a dense neural network contains
a sparse sub-network that can match the performance of the original dense
network when trained in isolation from scratch. Most works retrain the sparse
sub-network with the same training protocols as its dense network, such as
initialization, architecture blocks, and training recipes. However, till now it
is unclear that whether these training protocols are optimal for sparse
networks.
</p>
<p>In this paper, we demonstrate that it is unnecessary for spare retraining to
strictly inherit those properties from the dense network. Instead, by plugging
in purposeful "tweaks" of the sparse subnetwork architecture or its training
recipe, its retraining can be significantly improved than the default,
especially at high sparsity levels. Combining all our proposed "tweaks" can
yield the new state-of-the-art performance of LTH, and these modifications can
be easily adapted to other sparse training algorithms in general. Specifically,
we have achieved a significant and consistent performance gain of1.05% - 4.93%
for ResNet18 on CIFAR-100 over vanilla-LTH. Moreover, our methods are shown to
generalize across datasets (CIFAR10, CIFAR100, TinyImageNet) and architectures
(Vgg16, ResNet-18/ResNet-34, MobileNet). All codes will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Target Detection and Segmentation in Circular-Scan Synthetic-Aperture-Sonar Images using Semi-Supervised Convolutional Encoder-Decoders. (arXiv:2101.03603v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.03603">
<div class="article-summary-box-inner">
<span><p>We propose a framework for saliency-based, multi-target detection and
segmentation of circular-scan, synthetic-aperture-sonar (CSAS) imagery. Our
framework relies on a multi-branch, convolutional encoder-decoder network ({\sc
MB-CEDN}). The encoder portion of the {\sc MB-CEDN} extracts visual contrast
features from CSAS images. These features are fed into dual decoders that
perform pixel-level segmentation to mask targets. Each decoder provides
different perspectives as to what constitutes a salient target. These opinions
are aggregated and cascaded into a deep-parsing network to refine the
segmentation.
</p>
<p>We evaluate our framework using real-world CSAS imagery consisting of five
broad target classes. We compare against existing approaches from the
computer-vision literature. We show that our framework outperforms supervised,
deep-saliency networks designed for natural imagery. It greatly outperforms
unsupervised saliency approaches developed for natural imagery. This
illustrates that natural-image-based models may need to be altered to be
effective for this imaging-sonar modality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classic versus deep learning approaches to address computer vision challenges. (arXiv:2101.09744v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.09744">
<div class="article-summary-box-inner">
<span><p>Computer vision and image processing address many challenging applications.
While the last decade has seen deep neural network architectures
revolutionizing those fields, early methods relied on 'classic', i.e.,
non-learned approaches. In this study, we explore the differences between
classic and deep learning (DL) algorithms to gain new insight regarding which
is more suitable for a given application. The focus is on two challenging
ill-posed problems, namely faint edge detection and multispectral image
registration, studying recent state-of-the-art DL and classic solutions. While
those DL algorithms outperform classic methods in terms of accuracy and
development time, they tend to have higher resource requirements and are unable
to perform outside their training space. Moreover, classic algorithms are more
transparent, which facilitates their adoption for real-life applications. As
both classes of approaches have unique strengths and limitations, the choice of
a solution is clearly application dependent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">K-Hairstyle: A Large-scale Korean Hairstyle Dataset for Virtual Hair Editing and Hairstyle Classification. (arXiv:2102.06288v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06288">
<div class="article-summary-box-inner">
<span><p>The hair and beauty industry is a fast-growing industry. This led to the
development of various applications, such as virtual hair dyeing or hairstyle
transfer, to satisfy the customer's needs. Although several hairstyle datasets
are available for these applications, they often consist of a relatively small
number of images with low resolution, thus limiting their performance on
high-quality hair editing. In response, we introduce a novel large-scale Korean
hairstyle dataset, K-hairstyle, containing 500,000 high-resolution images. In
addition, K-hairstyle includes various hair attributes annotated by Korean
expert hairstylists as well as hair segmentation masks. We validate the
effectiveness of our dataset via several applications, such as hair dyeing,
hairstyle transfer, and hairstyle classification. K-hairstyle is publicly
available at https://psh01087.github.io/K-Hairstyle/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A General Descent Aggregation Framework for Gradient-based Bi-level Optimization. (arXiv:2102.07976v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07976">
<div class="article-summary-box-inner">
<span><p>In recent years, a variety of gradient-based methods have been developed to
solve Bi-Level Optimization (BLO) problems in machine learning and computer
vision areas. However, the theoretical correctness and practical effectiveness
of these existing approaches always rely on some restrictive conditions (e.g.,
Lower-Level Singleton, LLS), which could hardly be satisfied in real-world
applications. Moreover, previous literature only proves theoretical results
based on their specific iteration strategies, thus lack a general recipe to
uniformly analyze the convergence behaviors of different gradient-based BLOs.
In this work, we formulate BLOs from an optimistic bi-level viewpoint and
establish a new gradient-based algorithmic framework, named Bi-level Descent
Aggregation (BDA), to partially address the above issues. Specifically, BDA
provides a modularized structure to hierarchically aggregate both the upper-
and lower-level subproblems to generate our bi-level iterative dynamics.
Theoretically, we establish a general convergence analysis template and derive
a new proof recipe to investigate the essential theoretical properties of
gradient-based BLO methods. Furthermore, this work systematically explores the
convergence behavior of BDA in different optimization scenarios, i.e.,
considering various solution qualities (i.e., global/local/stationary solution)
returned from solving approximation subproblems. Extensive experiments justify
our theoretical results and demonstrate the superiority of the proposed
algorithm for hyper-parameter optimization and meta-learning tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Persistent Homology and Graphs Representation Learning. (arXiv:2102.12926v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12926">
<div class="article-summary-box-inner">
<span><p>This article aims to study the topological invariant properties encoded in
node graph representational embeddings by utilizing tools available in
persistent homology. Specifically, given a node embedding representation
algorithm, we consider the case when these embeddings are real-valued. By
viewing these embeddings as scalar functions on a domain of interest, we can
utilize the tools available in persistent homology to study the topological
information encoded in these representations. Our construction effectively
defines a unique persistence-based graph descriptor, on both the graph and node
levels, for every node representation algorithm. To demonstrate the
effectiveness of the proposed method, we study the topological descriptors
induced by DeepWalk, Node2Vec and Diff2Vec.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Application of Image-to-Image Translation: Chromosome Straightening Framework by Learning from a Single Image. (arXiv:2103.02835v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02835">
<div class="article-summary-box-inner">
<span><p>In medical imaging, chromosome straightening plays a significant role in the
pathological study of chromosomes and in the development of cytogenetic maps.
Whereas different approaches exist for the straightening task, typically
geometric algorithms are used whose outputs are characterized by jagged edges
or fragments with discontinued banding patterns. To address the flaws in the
geometric algorithms, we propose a novel framework based on image-to-image
translation to learn a pertinent mapping dependence for synthesizing
straightened chromosomes with uninterrupted banding patterns and preserved
details. In addition, to avoid the pitfall of deficient input chromosomes, we
construct an augmented dataset using only one single curved chromosome image
for training models. Based on this framework, we apply two popular
image-to-image translation architectures, U-shape networks and conditional
generative adversarial networks, to assess its efficacy. Experiments on a
dataset comprised of 642 real-world chromosomes demonstrate the superiority of
our framework, as compared to the geometric method in straightening
performance, by rendering realistic and continued chromosome details.
Furthermore, our straightened results improve the chromosome classification by
0.98%-1.39% mean accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High Perceptual Quality Image Denoising with a Posterior Sampling CGAN. (arXiv:2103.04192v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04192">
<div class="article-summary-box-inner">
<span><p>The vast work in Deep Learning (DL) has led to a leap in image denoising
research. Most DL solutions for this task have chosen to put their efforts on
the denoiser's architecture while maximizing distortion performance. However,
distortion driven solutions lead to blurry results with sub-optimal perceptual
quality, especially in immoderate noise levels. In this paper we propose a
different perspective, aiming to produce sharp and visually pleasing denoised
images that are still faithful to their clean sources. Formally, our goal is to
achieve high perceptual quality with acceptable distortion. This is attained by
a stochastic denoiser that samples from the posterior distribution, trained as
a generator in the framework of conditional generative adversarial networks
(CGAN). Contrary to distortion-based regularization terms that conflict with
perceptual quality, we introduce to the CGAN objective a theoretically founded
penalty term that does not force a distortion requirement on individual
samples, but rather on their mean. We showcase our proposed method with a novel
denoiser architecture that achieves the reformed denoising goal and produces
vivid and diverse outcomes in immoderate noise levels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calibrated and Partially Calibrated Semi-Generalized Homographies. (arXiv:2103.06535v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06535">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose the first minimal solutions for estimating the
semi-generalized homography given a perspective and a generalized camera. The
proposed solvers use five 2D-2D image point correspondences induced by a scene
plane. One of them assumes the perspective camera to be fully calibrated, while
the other solver estimates the unknown focal length together with the absolute
pose parameters. This setup is particularly important in structure-from-motion
and image-based localization pipelines, where a new camera is localized in each
step with respect to a set of known cameras and 2D-3D correspondences might not
be available. As a consequence of a clever parametrization and the elimination
ideal method, our approach only needs to solve a univariate polynomial of
degree five or three. The proposed solvers are stable and efficient as
demonstrated by a number of synthetic and real-world experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Amend Facial Expression Representation via De-albino and Affinity. (arXiv:2103.10189v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10189">
<div class="article-summary-box-inner">
<span><p>Facial Expression Recognition (FER) is a classification task that points to
face variants. Hence, there are certain affinity features between facial
expressions, receiving little attention in the FER literature. Convolution
padding, despite helping capture the edge information, causes erosion of the
feature map simultaneously. After multi-layer filling convolution, the output
feature map named albino feature definitely weakens the representation of the
expression. To tackle these challenges, we propose a novel architecture named
Amending Representation Module (ARM). ARM is a substitute for the pooling
layer. Theoretically, it can be embedded in the back end of any network to deal
with the Padding Erosion. ARM efficiently enhances facial expression
representation from two different directions: 1) reducing the weight of eroded
features to offset the side effect of padding, and 2) decomposing facial
features to simplify representation learning. Experiments on public benchmarks
prove that our ARM boosts the performance of FER remarkably. The validation
accuracies are respectively 90.42% on RAF-DB, 65.2% on Affect-Net, and 58.71%
on SFEW, exceeding current state-of-the-art methods. Our implementation and
trained models are available at
https://github.com/JiaweiShiCV/Amend-Representation-Module.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNETR: Transformers for 3D Medical Image Segmentation. (arXiv:2103.10504v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10504">
<div class="article-summary-box-inner">
<span><p>Fully Convolutional Neural Networks (FCNNs) with contracting and expanding
paths have shown prominence for the majority of medical image segmentation
applications since the past decade. In FCNNs, the encoder plays an integral
role by learning both global and local features and contextual representations
which can be utilized for semantic output prediction by the decoder. Despite
their success, the locality of convolutional layers in FCNNs, limits the
capability of learning long-range spatial dependencies. Inspired by the recent
success of transformers for Natural Language Processing (NLP) in long-range
sequence learning, we reformulate the task of volumetric (3D) medical image
segmentation as a sequence-to-sequence prediction problem. We introduce a novel
architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer
as the encoder to learn sequence representations of the input volume and
effectively capture the global multi-scale information, while also following
the successful "U-shaped" network design for the encoder and decoder. The
transformer encoder is directly connected to a decoder via skip connections at
different resolutions to compute the final semantic segmentation output. We
have validated the performance of our method on the Multi Atlas Labeling Beyond
The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical
Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation
tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV
leaderboard. Code: https://monai.io/research/unetr
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthesis of Compositional Animations from Textual Descriptions. (arXiv:2103.14675v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14675">
<div class="article-summary-box-inner">
<span><p>"How can we animate 3D-characters from a movie script or move robots by
simply telling them what we would like them to do?" "How unstructured and
complex can we make a sentence and still generate plausible movements from it?"
These are questions that need to be answered in the long-run, as the field is
still in its infancy. Inspired by these problems, we present a new technique
for generating compositional actions, which handles complex input sentences.
Our output is a 3D pose sequence depicting the actions in the input sentence.
We propose a hierarchical two-stream sequential model to explore a finer
joint-level mapping between natural language sentences and 3D pose sequences
corresponding to the given motion. We learn two manifold representations of the
motion -- one each for the upper body and the lower body movements. Our model
can generate plausible pose sequences for short sentences describing single
actions as well as long compositional sentences describing multiple sequential
and superimposed actions. We evaluate our proposed model on the publicly
available KIT Motion-Language Dataset containing 3D pose data with
human-annotated sentences. Experimental results show that our model advances
the state-of-the-art on text-based motion synthesis in objective evaluations by
a margin of 50%. Qualitative evaluations based on a user study indicate that
our synthesized motions are perceived to be the closest to the ground-truth
motion captures for both short and compositional sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards High Fidelity Monocular Face Reconstruction with Rich Reflectance using Self-supervised Learning and Ray Tracing. (arXiv:2103.15432v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15432">
<div class="article-summary-box-inner">
<span><p>Robust face reconstruction from monocular image in general lighting
conditions is challenging. Methods combining deep neural network encoders with
differentiable rendering have opened up the path for very fast monocular
reconstruction of geometry, lighting and reflectance. They can also be trained
in self-supervised manner for increased robustness and better generalization.
However, their differentiable rasterization based image formation models, as
well as underlying scene parameterization, limit them to Lambertian face
reflectance and to poor shape details. More recently, ray tracing was
introduced for monocular face reconstruction within a classic
optimization-based framework and enables state-of-the art results. However
optimization-based approaches are inherently slow and lack robustness. In this
paper, we build our work on the aforementioned approaches and propose a new
method that greatly improves reconstruction quality and robustness in general
scenes. We achieve this by combining a CNN encoder with a differentiable ray
tracer, which enables us to base the reconstruction on much more advanced
personalized diffuse and specular albedos, a more sophisticated illumination
model and a plausible representation of self-shadows. This enables to take a
big leap forward in reconstruction quality of shape, appearance and lighting
even in scenes with difficult illumination. With consistent face attributes
reconstruction, our method leads to practical applications such as relighting
and self-shadows removal. Compared to state-of-the-art methods, our results
show improved accuracy and validity of the approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Procrustean Training for Imbalanced Deep Learning. (arXiv:2104.01769v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01769">
<div class="article-summary-box-inner">
<span><p>Neural networks trained with class-imbalanced data are known to perform
poorly on minor classes of scarce training data. Several recent works attribute
this to over-fitting to minor classes. In this paper, we provide a novel
explanation of this issue. We found that a neural network tends to first
under-fit the minor classes by classifying most of their data into the major
classes in early training epochs. To correct these wrong predictions, the
neural network then must focus on pushing features of minor class data across
the decision boundaries between major and minor classes, leading to much larger
gradients for features of minor classes. We argue that such an under-fitting
phase over-emphasizes the competition between major and minor classes, hinders
the neural network from learning the discriminative knowledge that can be
generalized to test data, and eventually results in over-fitting. To address
this issue, we propose a novel learning strategy to equalize the training
progress across classes. We mix features of the major class data with those of
other data in a mini-batch, intentionally weakening their features to prevent a
neural network from fitting them first. We show that this strategy can largely
balance the training accuracy and feature gradients across classes, effectively
mitigating the under-fitting then over-fitting problem for minor class data. On
several benchmark datasets, our approach achieves the state-of-the-art
accuracy, especially for the challenging step-imbalanced cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Grained Fashion Similarity Prediction by Attribute-Specific Embedding Learning. (arXiv:2104.02429v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02429">
<div class="article-summary-box-inner">
<span><p>This paper strives to predict fine-grained fashion similarity. In this
similarity paradigm, one should pay more attention to the similarity in terms
of a specific design/attribute between fashion items. For example, whether the
collar designs of the two clothes are similar. It has potential value in many
fashion related applications, such as fashion copyright protection. To this
end, we propose an Attribute-Specific Embedding Network (ASEN) to jointly learn
multiple attribute-specific embeddings, thus measure the fine-grained
similarity in the corresponding space. The proposed ASEN is comprised of a
global branch and a local branch. The global branch takes the whole image as
input to extract features from a global perspective, while the local branch
takes as input the zoomed-in region-of-interest (RoI) w.r.t. the specified
attribute thus able to extract more fine-grained features. As the global branch
and the local branch extract the features from different perspectives, they are
complementary to each other. Additionally, in each branch, two attention
modules, i.e., Attribute-aware Spatial Attention and Attribute-aware Channel
Attention, are integrated to make ASEN be able to locate the related regions
and capture the essential patterns under the guidance of the specified
attribute, thus make the learned attribute-specific embeddings better reflect
the fine-grained similarity. Extensive experiments on three fashion-related
datasets, i.e., FashionAI, DARN, and DeepFashion, show the effectiveness of
ASEN for fine-grained fashion similarity prediction and its potential for
fashion reranking. Code and data are available at
https://github.com/maryeon/asenpp .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes. (arXiv:2104.03953v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03953">
<div class="article-summary-box-inner">
<span><p>Neural implicit surface representations have emerged as a promising paradigm
to capture 3D shapes in a continuous and resolution-independent manner.
However, adapting them to articulated shapes is non-trivial. Existing
approaches learn a backward warp field that maps deformed to canonical points.
However, this is problematic since the backward warp field is pose dependent
and thus requires large amounts of data to learn. To address this, we introduce
SNARF, which combines the advantages of linear blend skinning (LBS) for
polygonal meshes with those of neural implicit surfaces by learning a forward
deformation field without direct supervision. This deformation field is defined
in canonical, pose-independent space, allowing for generalization to unseen
poses. Learning the deformation field from posed meshes alone is challenging
since the correspondences of deformed points are defined implicitly and may not
be unique under changes of topology. We propose a forward skinning model that
finds all canonical correspondences of any deformed point using iterative root
finding. We derive analytical gradients via implicit differentiation, enabling
end-to-end training from 3D meshes with bone transformations. Compared to
state-of-the-art neural implicit representations, our approach generalizes
better to unseen poses while preserving accuracy. We demonstrate our method in
challenging scenarios on (clothed) 3D humans in diverse and unseen poses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Reconstruct 3D Non-Cuboid Room Layout from a Single RGB Image. (arXiv:2104.07986v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07986">
<div class="article-summary-box-inner">
<span><p>Single-image room layout reconstruction aims to reconstruct the enclosed 3D
structure of a room from a single image. Most previous work relies on the
cuboid-shape prior. This paper considers a more general indoor assumption,
i.e., the room layout consists of a single ceiling, a single floor, and several
vertical walls. To this end, we first employ Convolutional Neural Networks to
detect planes and vertical lines between adjacent walls. Meanwhile, estimating
the 3D parameters for each plane. Then, a simple yet effective geometric
reasoning method is adopted to achieve room layout reconstruction. Furthermore,
we optimize the 3D plane parameters to reconstruct a geometrically consistent
room layout between planes and lines. The experimental results on public
datasets validate the effectiveness and efficiency of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransVG: End-to-End Visual Grounding with Transformers. (arXiv:2104.08541v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08541">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a neat yet effective transformer-based framework
for visual grounding, namely TransVG, to address the task of grounding a
language query to the corresponding region onto an image. The state-of-the-art
methods, including two-stage or one-stage ones, rely on a complex module with
manually-designed mechanisms to perform the query reasoning and multi-modal
fusion. However, the involvement of certain mechanisms in fusion module design,
such as query decomposition and image scene graph, makes the models easily
overfit to datasets with specific scenarios, and limits the plenitudinous
interaction between the visual-linguistic context. To avoid this caveat, we
propose to establish the multi-modal correspondence by leveraging transformers,
and empirically show that the complex fusion modules (\eg, modular attention
network, dynamic graph, and multi-modal tree) can be replaced by a simple stack
of transformer encoder layers with higher performance. Moreover, we
re-formulate the visual grounding as a direct coordinates regression problem
and avoid making predictions out of a set of candidates (\emph{i.e.}, region
proposals or anchor boxes). Extensive experiments are conducted on five widely
used datasets, and a series of state-of-the-art records are set by our TransVG.
We build the benchmark of transformer-based visual grounding framework and make
the code available at \url{https://github.com/djiajunustc/TransVG}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MemX: An Attention-Aware Smart Eyewear System for Personalized Moment Auto-capture. (arXiv:2105.00916v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00916">
<div class="article-summary-box-inner">
<span><p>This work presents MemX: a biologically-inspired attention-aware eyewear
system developed with the goal of pursuing the long-awaited vision of a
personalized visual Memex. MemX captures human visual attention on the fly,
analyzes the salient visual content, and records moments of personal interest
in the form of compact video snippets. Accurate attentive scene detection and
analysis on resource-constrained platforms is challenging because these tasks
are computation and energy intensive. We propose a new temporal visual
attention network that unifies human visual attention tracking and salient
visual content analysis. Attention tracking focuses computation-intensive video
analysis on salient regions, while video analysis makes human attention
detection and tracking more accurate. Using the YouTube-VIS dataset and 30
participants, we experimentally show that MemX significantly improves the
attention tracking accuracy over the eye-tracking-alone method, while
maintaining high system energy efficiency. We have also conducted 11 in-field
pilot studies across a range of daily usage scenarios, which demonstrate the
feasibility and potential benefits of MemX.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple and Strong Baseline for Universal Targeted Attacks on Siamese Visual Tracking. (arXiv:2105.02480v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.02480">
<div class="article-summary-box-inner">
<span><p>Siamese trackers are shown to be vulnerable to adversarial attacks recently.
However, the existing attack methods craft the perturbations for each video
independently, which comes at a non-negligible computational cost. In this
paper, we show the existence of universal perturbations that can enable the
targeted attack, e.g., forcing a tracker to follow the ground-truth trajectory
with specified offsets, to be video-agnostic and free from inference in a
network. Specifically, we attack a tracker by adding a universal imperceptible
perturbation to the template image and adding a fake target, i.e., a small
universal adversarial patch, into the search images adhering to the predefined
trajectory, so that the tracker outputs the location and size of the fake
target instead of the real target. Our approach allows perturbing a novel video
to come at no additional cost except the mere addition operations -- and not
require gradient optimization or network inference. Experimental results on
several datasets demonstrate that our approach can effectively fool the Siamese
trackers in a targeted attack manner. We show that the proposed perturbations
are not only universal across videos, but also generalize well across different
trackers. Such perturbations are therefore doubly universal, both with respect
to the data and the network architectures. We will make our code publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UVStyle-Net: Unsupervised Few-shot Learning of 3D Style Similarity Measure for B-Reps. (arXiv:2105.02961v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.02961">
<div class="article-summary-box-inner">
<span><p>Boundary Representations (B-Reps) are the industry standard in 3D Computer
Aided Design/Manufacturing (CAD/CAM) and industrial design due to their
fidelity in representing stylistic details. However, they have been ignored in
the 3D style research. Existing 3D style metrics typically operate on meshes or
pointclouds, and fail to account for end-user subjectivity by adopting fixed
definitions of style, either through crowd-sourcing for style labels or
hand-crafted features. We propose UVStyle-Net, a style similarity measure for
B-Reps that leverages the style signals in the second order statistics of the
activations in a pre-trained (unsupervised) 3D encoder, and learns their
relative importance to a subjective end-user through few-shot learning. Our
approach differs from all existing data-driven 3D style methods since it may be
used in completely unsupervised settings, which is desirable given the lack of
publicly available labelled B-Rep datasets. More importantly, the few-shot
learning accounts for the inherent subjectivity associated with style. We show
quantitatively that our proposed method with B-Reps is able to capture stronger
style signals than alternative methods on meshes and pointclouds despite its
significantly greater computational efficiency. We also show it is able to
generate meaningful style gradients with respect to the input shape, and that
few-shot learning with as few as two positive examples selected by an end-user
is sufficient to significantly improve the style measure. Finally, we
demonstrate its efficacy on a large unlabeled public dataset of CAD models.
Source code and data will be released in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised identification of surgical robotic actions from small non homogeneous datasets. (arXiv:2105.08488v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08488">
<div class="article-summary-box-inner">
<span><p>Robot-assisted surgery is an established clinical practice. The automatic
identification of surgical actions is needed for a range of applications,
including performance assessment of trainees and surgical process modeling for
autonomous execution and monitoring. However, supervised action identification
is not feasible, due to the burden of manually annotating recordings of
potentially complex and long surgical executions. Moreover, often few example
executions of a surgical procedure can be recorded. This paper proposes a novel
fast algorithm for unsupervised identification of surgical actions in a
standard surgical training task, the ring transfer, executed with da Vinci
Research Kit. Exploiting kinematic and semantic visual features automatically
extracted from a very limited dataset of executions, we are able to
significantly outperform state-of-the-art results on a dataset of non-expert
executions (58\% vs. 24\% F1-score), and improve performance in the presence of
noise, short actions and non-homogeneous workflows, i.e. non repetitive action
sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Aggressive Adversarial Attacks on 3D Point Cloud. (arXiv:2105.09090v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09090">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are found to be prone to adversarial examples which
could deliberately fool the model to make mistakes. Recently, a few of works
expand this task from 2D image to 3D point cloud by using global point cloud
optimization. However, the perturbations of global point are not effective for
misleading the victim model. First, not all points are important in
optimization toward misleading. Abundant points account considerable distortion
budget but contribute trivially to attack. Second, the multi-label optimization
is suboptimal for adversarial attack, since it consumes extra energy in finding
multi-label victim model collapse and causes instance transformation to be
dissimilar to any particular instance. Third, the independent adversarial and
perceptibility losses, caring misclassification and dissimilarity separately,
treat the updating of each point equally without a focus. Therefore, once
perceptibility loss approaches its budget threshold, all points would be stock
in the surface of hypersphere and attack would be locked in local optimality.
Therefore, we propose a local aggressive adversarial attacks (L3A) to solve
above issues. Technically, we select a bunch of salient points, the high-score
subset of point cloud according to gradient, to perturb. Then a flow of
aggressive optimization strategies are developed to reinforce the unperceptive
generation of adversarial examples toward misleading victim models. Extensive
experiments on PointNet, PointNet++ and DGCNN demonstrate the state-of-the-art
performance of our method against existing adversarial attack methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CFA-Net: Controllable Face Anonymization Network with Identity Representation Manipulation. (arXiv:2105.11137v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11137">
<div class="article-summary-box-inner">
<span><p>De-identification of face data has drawn increasing attention in recent
years. It is important to protect people's identities meanwhile keeping the
utility of the data in many computer vision tasks. We propose a Controllable
Face Anonymization Network (CFA-Net), a novel approach that can anonymize the
identity of given faces in images and videos, based on a generator that can
disentangle face identity from other image contents. We reach the goal of
controllable face anonymization through manipulating identity vectors in the
generator's identity representation space. Various anonymized faces deriving
from an original face can be generated through our method and maintain high
similarity to the original image contents. Quantitative and qualitative results
demonstrate our method's superiority over literature models on visual quality
and anonymization validity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Greedy Bayesian Posterior Approximation with Deep Ensembles. (arXiv:2105.14275v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14275">
<div class="article-summary-box-inner">
<span><p>Ensembles of independently trained neural networks are a state-of-the-art
approach to estimate predictive uncertainty in Deep Learning, and can be
interpreted as an approximation of the posterior distribution via a mixture of
delta functions. The training of ensembles relies on non-convexity of the loss
landscape and random initialization of their individual members, making the
resulting posterior approximation uncontrolled. This paper proposes a novel and
principled method to tackle this limitation, minimizing an $f$-divergence
between the true posterior and a kernel density estimator in a function space.
We analyze this objective from a combinatorial point of view, and show that it
is submodular with respect to mixture components for any $f$. Subsequently, we
consider the problem of ensemble construction, and from the marginal gain of
the total objective, we derive a novel diversity term for training ensembles
greedily. The performance of our approach is demonstrated on computer vision
out-of-distribution detection benchmarks in a range of architectures trained on
multiple datasets. The source code of our method is publicly available at
https://github.com/MIPT-Oulu/greedy_ensembles_training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistent Two-Flow Network for Tele-Registration of Point Clouds. (arXiv:2106.00329v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00329">
<div class="article-summary-box-inner">
<span><p>Rigid registration of partial observations is a fundamental problem in
various applied fields. In computer graphics, special attention has been given
to the registration between two partial point clouds generated by scanning
devices. State-of-the-art registration techniques still struggle when the
overlap region between the two point clouds is small, and completely fail if
there is no overlap between the scan pairs. In this paper, we present a
learning-based technique that alleviates this problem, and allows registration
between point clouds, presented in arbitrary poses, and having little or even
no overlap, a setting that has been referred to as tele-registration. Our
technique is based on a novel neural network design that learns a prior of a
class of shapes and can complete a partial shape. The key idea is combining the
registration and completion tasks in a way that reinforces each other. In
particular, we simultaneously train the registration network and completion
network using two coupled flows, one that register-and-complete, and one that
complete-and-register, and encourage the two flows to produce a consistent
result. We show that, compared with each separate flow, this two-flow training
leads to robust and reliable tele-registration, and hence to a better point
cloud prediction that completes the registered scans. It is also worth
mentioning that each of the components in our neural network outperforms
state-of-the-art methods in both completion and registration. We further
analyze our network with several ablation studies and demonstrate its
performance on a large number of partial point clouds, both synthetic and
real-world, that have only small or no overlap.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning High-Precision Bounding Box for Rotated Object Detection via Kullback-Leibler Divergence. (arXiv:2106.01883v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01883">
<div class="article-summary-box-inner">
<span><p>Existing rotated object detectors are mostly inherited from the horizontal
detection paradigm, as the latter has evolved into a well-developed area.
However, these detectors are difficult to perform prominently in high-precision
detection due to the limitation of current regression loss design, especially
for objects with large aspect ratios. Taking the perspective that horizontal
detection is a special case for rotated object detection, in this paper, we are
motivated to change the design of rotation regression loss from induction
paradigm to deduction methodology, in terms of the relation between rotation
and horizontal detection. We show that one essential challenge is how to
modulate the coupled parameters in the rotation regression loss, as such the
estimated parameters can influence to each other during the dynamic joint
optimization, in an adaptive and synergetic way. Specifically, we first convert
the rotated bounding box into a 2-D Gaussian distribution, and then calculate
the Kullback-Leibler Divergence (KLD) between the Gaussian distributions as the
regression loss. By analyzing the gradient of each parameter, we show that KLD
(and its derivatives) can dynamically adjust the parameter gradients according
to the characteristics of the object. It will adjust the importance (gradient
weight) of the angle parameter according to the aspect ratio. This mechanism
can be vital for high-precision detection as a slight angle error would cause a
serious accuracy drop for large aspect ratios objects. More importantly, we
have proved that KLD is scale invariant. We further show that the KLD loss can
be degenerated into the popular $l_{n}$-norm loss for horizontal detection.
Experimental results on seven datasets using different detectors show its
consistent superiority, and codes are available at
https://github.com/yangxue0827/RotationDetection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combinatorial Optimization for Panoptic Segmentation: A Fully Differentiable Approach. (arXiv:2106.03188v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03188">
<div class="article-summary-box-inner">
<span><p>We propose a fully differentiable architecture for simultaneous semantic and
instance segmentation (a.k.a. panoptic segmentation) consisting of a
convolutional neural network and an asymmetric multiway cut problem solver. The
latter solves a combinatorial optimization problem that elegantly incorporates
semantic and boundary predictions to produce a panoptic labeling. Our
formulation allows to directly maximize a smooth surrogate of the panoptic
quality metric by backpropagating the gradient through the optimization
problem. Experimental evaluation shows improvement by backpropagating through
the optimization problem w.r.t. comparable approaches on Cityscapes and COCO
datasets. Overall, our approach shows the utility of using combinatorial
optimization in tandem with deep learning in a challenging large scale
real-world problem and showcases benefits and insights into training such an
architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shifting Transformation Learning for Out-of-Distribution Detection. (arXiv:2106.03899v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03899">
<div class="article-summary-box-inner">
<span><p>Detecting out-of-distribution (OOD) samples plays a key role in open-world
and safety-critical applications such as autonomous systems and healthcare.
Recently, self-supervised representation learning techniques (via contrastive
learning and pretext learning) have shown effective in improving OOD detection.
However, one major issue with such approaches is the choice of shifting
transformations and pretext tasks which depends on the in-domain distribution.
In this paper, we propose a simple framework that leverages a shifting
transformation learning setting for learning multiple shifted representations
of the training set for improved OOD detection. To address the problem of
selecting optimal shifting transformation and pretext tasks, we propose a
simple mechanism for automatically selecting the transformations and modulating
their effect on representation learning without requiring any OOD training
samples. In extensive experiments, we show that our simple framework
outperforms state-of-the-art OOD detection models on several image datasets. We
also characterize the criteria for a desirable OOD detector for real-world
applications and demonstrate the efficacy of our proposed technique against
state-of-the-art OOD detection techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Affiliate: Mutual Centralized Learning for Few-shot Classification. (arXiv:2106.05517v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05517">
<div class="article-summary-box-inner">
<span><p>Few-shot learning (FSL) aims to learn a classifier that can be easily adapted
to accommodate new tasks not seen during training, given only a few examples.
To handle the limited-data problem in few-shot regimes, recent methods tend to
collectively use a set of local features to densely represent an image instead
of using a mixed global feature. They generally explore a unidirectional
query-to-support paradigm in FSL, e.g., find the nearest/optimal support
feature for each query feature and aggregate these local matches for a joint
classification. In this paper, we propose a new method Mutual Centralized
Learning (MCL) to fully affiliate the two disjoint sets of dense features in a
bidirectional paradigm. We associate each local feature with a particle that
can bidirectionally random walk in a discrete feature space by the
affiliations. To estimate the class probability, we propose the features'
accessibility that measures the expected number of visits to the support
features of that class in a Markov process. We relate our method to learning a
centrality on an affiliation network and demonstrate its capability to be
plugged in existing methods by highlighting centralized local features.
Experiments show that our method achieves the state-of-the-art on both
miniImageNet and tieredImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Network Modeling of Probabilities for Coding the Octree Representation of Point Clouds. (arXiv:2106.06482v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06482">
<div class="article-summary-box-inner">
<span><p>This paper describes a novel lossless point cloud compression algorithm that
uses a neural network for estimating the coding probabilities for the occupancy
status of voxels, depending on wide three dimensional contexts around the voxel
to be encoded. The point cloud is represented as an octree, with each
resolution layer being sequentially encoded and decoded using arithmetic
coding, starting from the lowest resolution, until the final resolution is
reached. The occupancy probability of each voxel of the splitting pattern at
each node of the octree is modeled by a neural network, having at its input the
already encoded occupancy status of several octree nodes (belonging to the past
and current resolutions), corresponding to a 3D context surrounding the node to
be encoded. The algorithm has a fast and a slow version, the fast version
selecting differently several voxels of the context, which allows an increased
parallelization by sending larger batches of templates to be estimated by the
neural network, at both encoder and decoder. The proposed algorithms yield
state-of-the-art results on benchmark datasets. The implementation will be made
available at https://github.com/marmus12/nnctx
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LE-NAS: Learning-based Ensemble with NAS for Dose Prediction. (arXiv:2106.06733v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06733">
<div class="article-summary-box-inner">
<span><p>Radiation therapy treatment planning is a complex process, as the target dose
prescription and normal tissue sparing are conflicting objectives. Automated
and accurate dose prediction for radiation therapy planning is in high demand.
In this study, we propose a novel learning-based ensemble approach, named
LE-NAS, which integrates neural architecture search (NAS) with knowledge
distillation for 3D radiotherapy dose prediction. Specifically, the prediction
network first exhaustively searches each block from enormous architecture
space. Then, multiple architectures are selected with promising performance and
diversity. To reduce the inference time, we adopt the teacher-student paradigm
by treating the combination of diverse outputs from multiple searched networks
as supervisions to guide the student network training. In addition, we apply
adversarial learning to optimize the student network to recover the knowledge
in teacher networks. To the best of our knowledge, we are the first to
investigate the combination of NAS and knowledge distillation. The proposed
method has been evaluated on the public OpenKBP dataset, and experimental
results demonstrate the effectiveness of our method and its superior
performance to the state-of-the-art method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Steerable Partial Differential Operators for Equivariant Neural Networks. (arXiv:2106.10163v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10163">
<div class="article-summary-box-inner">
<span><p>Recent work in equivariant deep learning bears strong similarities to
physics. Fields over a base space are fundamental entities in both subjects, as
are equivariant maps between these fields. In deep learning, however, these
maps are usually defined by convolutions with a kernel, whereas they are
partial differential operators (PDOs) in physics. Developing the theory of
equivariant PDOs in the context of deep learning could bring these subjects
even closer together and lead to a stronger flow of ideas. In this work, we
derive a $G$-steerability constraint that completely characterizes when a PDO
between feature vector fields is equivariant, for arbitrary symmetry groups
$G$. We then fully solve this constraint for several important groups. We use
our solutions as equivariant drop-in replacements for convolutional layers and
benchmark them in that role. Finally, we develop a framework for equivariant
maps based on Schwartz distributions that unifies classical convolutions and
differential operators and gives insight about the relation between the two.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Train Your MAML to Excel in Few-Shot Classification. (arXiv:2106.16245v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.16245">
<div class="article-summary-box-inner">
<span><p>Model-agnostic meta-learning (MAML) is arguably one of the most popular
meta-learning algorithms nowadays. Nevertheless, its performance on few-shot
classification is far behind many recent algorithms dedicated to the problem.
In this paper, we point out several key facets of how to train MAML to excel in
few-shot classification. First, we find that MAML needs a large number of
gradient steps in its inner loop update, which contradicts its common usage in
few-shot classification. Second, we find that MAML is sensitive to the class
label assignments during meta-testing. Concretely, MAML meta-trains the
initialization of an $N$-way classifier. These $N$ ways, during meta-testing,
then have $N!$ different permutations to be paired with a few-shot task of $N$
novel classes. We find that these permutations lead to a huge variance of
accuracy, making MAML unstable in few-shot classification. Third, we
investigate several approaches to make MAML permutation-invariant, among which
meta-training a single vector to initialize all the $N$ weight vectors in the
classification head performs the best. On benchmark datasets like MiniImageNet
and TieredImageNet, our approach, which we name UNICORN-MAML, performs on a par
with or even outperforms state-of-the-art few-shot classification algorithms,
without sacrificing MAML's simplicity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is attention to bounding boxes all you need for pedestrian action prediction?. (arXiv:2107.08031v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08031">
<div class="article-summary-box-inner">
<span><p>The human driver is no longer the only one concerned with the complexity of
the driving scenarios. Autonomous vehicles (AV) are similarly becoming involved
in the process. Nowadays, the development of AV in urban places underpins
essential safety concerns for vulnerable road users (VRUs) such as pedestrians.
Therefore, to make the roads safer, it is critical to classify and predict
their future behavior. In this paper, we present a framework based on multiple
variations of the Transformer models to reason attentively about the dynamic
evolution of the pedestrians' past trajectory and predict its future actions of
crossing or not crossing the street. We proved that using only bounding boxes
as input to our model can outperform the previous state-of-the-art models and
reach a prediction accuracy of 91% and an F1-score of 0.83 on the PIE dataset
up to two seconds ahead in the future. In addition, we introduced a large-size
simulated dataset (CP2A) using CARLA for action prediction. Our model has
similarly reached high accuracy (91%) and F1-score (0.91) on this dataset.
Interestingly, we showed that pre-training our Transformer model on the
simulated dataset and then fine-tuning it on the real dataset can be very
effective for the action prediction task. Finally, we created the "human
attention to bounding boxes" experiment that equally proved the ability of
humans to predict the future sufficiently by only giving attention to the
bounding boxes without the need for environmental context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face.evoLVe: A High-Performance Face Recognition Library. (arXiv:2107.08621v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08621">
<div class="article-summary-box-inner">
<span><p>In this paper, we develop face.evoLVe -- a comprehensive library that
collects and implements a wide range of popular deep learning-based methods for
face recognition. First of all, face.evoLVe is composed of key components that
cover the full process of face analytics, including face alignment, data
processing, various backbones, losses, and alternatives with bags of tricks for
improving performance. Later, face.evoLVe supports multi-GPU training on top of
different deep learning platforms, such as PyTorch and PaddlePaddle, which
facilitates researchers to work on both large-scale datasets with millions of
images and low-shot counterparts with limited well-annotated data. More
importantly, along with face.evoLVe, images before &amp; after alignment in the
common benchmark datasets are released with source codes and trained models
provided. All these efforts lower the technical burdens in reproducing the
existing methods for comparison, while users of our library could focus on
developing advanced approaches more efficiently. Last but not least,
face.evoLVe is well designed and vibrantly evolving, so that new face
recognition approaches can be easily plugged into our framework. Note that we
have used face.evoLVe to participate in a number of face recognition
competitions and secured the first place. The version that supports PyTorch is
publicly available at https://github.com/ZhaoJ9014/face.evoLVe.PyTorch and the
PaddlePaddle version is available at
https://github.com/ZhaoJ9014/face.evoLVe.PyTorch/tree/master/paddle.
Face.evoLVe has been widely used for face analytics, receiving 2.4K stars and
622 forks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SuperCaustics: Real-time, open-source simulation of transparent objects for deep learning applications. (arXiv:2107.11008v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11008">
<div class="article-summary-box-inner">
<span><p>Transparent objects are a very challenging problem in computer vision. They
are hard to segment or classify due to their lack of precise boundaries, and
there is limited data available for training deep neural networks. As such,
current solutions for this problem employ rigid synthetic datasets, which lack
flexibility and lead to severe performance degradation when deployed on
real-world scenarios. In particular, these synthetic datasets omit features
such as refraction, dispersion and caustics due to limitations in the rendering
pipeline. To address this issue, we present SuperCaustics, a real-time,
open-source simulation of transparent objects designed for deep learning
applications. SuperCaustics features extensive modules for stochastic
environment creation; uses hardware ray-tracing to support caustics,
dispersion, and refraction; and enables generating massive datasets with
multi-modal, pixel-perfect ground truth annotations. To validate our proposed
system, we trained a deep neural network from scratch to segment transparent
objects in difficult lighting scenarios. Our neural network achieved
performance comparable to the state-of-the-art on a real-world dataset using
only 10% of the training data and in a fraction of the training time. Further
experiments show that a model trained with SuperCaustics can segment different
types of caustics, even in images with multiple overlapping transparent
objects. To the best of our knowledge, this is the first such result for a
model trained on synthetic data. Both our open-source code and experimental
data are freely available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation. (arXiv:2107.11264v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11264">
<div class="article-summary-box-inner">
<span><p>Identifying unexpected objects on roads in semantic segmentation (e.g.,
identifying dogs on roads) is crucial in safety-critical applications. Existing
approaches use images of unexpected objects from external datasets or require
additional training (e.g., retraining segmentation networks or training an
extra network), which necessitate a non-trivial amount of labor intensity or
lengthy inference time. One possible alternative is to use prediction scores of
a pre-trained network such as the max logits (i.e., maximum values among
classes before the final softmax layer) for detecting such objects. However,
the distribution of max logits of each predicted class is significantly
different from each other, which degrades the performance of identifying
unexpected objects in urban-scene segmentation. To address this issue, we
propose a simple yet effective approach that standardizes the max logits in
order to align the different distributions and reflect the relative meanings of
max logits within each predicted class. Moreover, we consider the local regions
from two different perspectives based on the intuition that neighboring pixels
share similar semantic information. In contrast to previous approaches, our
method does not utilize any external datasets or require additional training,
which makes our method widely applicable to existing pre-trained segmentation
models. Such a straightforward approach achieves a new state-of-the-art
performance on the publicly available Fishyscapes Lost &amp; Found leaderboard with
a large margin. Our code is publicly available at this
$\href{https://github.com/shjung13/Standardized-max-logits}{link}$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Embodied Vision Navigation: A Survey. (arXiv:2108.04097v4 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04097">
<div class="article-summary-box-inner">
<span><p>"Embodied visual navigation" problem requires an agent to navigate in a 3D
environment mainly rely on its first-person observation. This problem has
attracted rising attention in recent years due to its wide application in
autonomous driving, vacuum cleaner, and rescue robot. A navigation agent is
supposed to have various intelligent skills, such as visual perceiving,
mapping, planning, exploring and reasoning, etc. Building such an agent that
observes, thinks, and acts is a key to real intelligence. The remarkable
learning ability of deep learning methods empowered the agents to accomplish
embodied visual navigation tasks. Despite this, embodied visual navigation is
still in its infancy since a lot of advanced skills are required, including
perceiving partially observed visual input, exploring unseen areas, memorizing
and modeling seen scenarios, understanding cross-modal instructions, and
adapting to a new environment, etc. Recently, embodied visual navigation has
attracted rising attention of the community, and numerous works has been
proposed to learn these skills. This paper attempts to establish an outline of
the current works in the field of embodied visual navigation by providing a
comprehensive literature survey. We summarize the benchmarks and metrics,
review different methods, analysis the challenges, and highlight the
state-of-the-art methods. Finally, we discuss unresolved challenges in the
field of embodied visual navigation and give promising directions in pursuing
future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Day-Night Domain Adaptation with a Physics Prior. (arXiv:2108.05137v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05137">
<div class="article-summary-box-inner">
<span><p>We explore the zero-shot setting for day-night domain adaptation. The
traditional domain adaptation setting is to train on one domain and adapt to
the target domain by exploiting unlabeled data samples from the test set. As
gathering relevant test data is expensive and sometimes even impossible, we
remove any reliance on test data imagery and instead exploit a visual inductive
prior derived from physics-based reflection models for domain adaptation. We
cast a number of color invariant edge detectors as trainable layers in a
convolutional neural network and evaluate their robustness to illumination
changes. We show that the color invariant layer reduces the day-night
distribution shift in feature map activations throughout the network. We
demonstrate improved performance for zero-shot day to night domain adaptation
on both synthetic as well as natural datasets in various tasks, including
classification, segmentation and place recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Depth Completion with Calibrated Backprojection Layers. (arXiv:2108.10531v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10531">
<div class="article-summary-box-inner">
<span><p>We propose a deep neural network architecture to infer dense depth from an
image and a sparse point cloud. It is trained using a video stream and
corresponding synchronized sparse point cloud, as obtained from a LIDAR or
other range sensor, along with the intrinsic calibration parameters of the
camera. At inference time, the calibration of the camera, which can be
different than the one used for training, is fed as an input to the network
along with the sparse point cloud and a single image. A Calibrated
Backprojection Layer backprojects each pixel in the image to three-dimensional
space using the calibration matrix and a depth feature descriptor. The
resulting 3D positional encoding is concatenated with the image descriptor and
the previous layer output to yield the input to the next layer of the encoder.
A decoder, exploiting skip-connections, produces a dense depth map. The
resulting Calibrated Backprojection Network, or KBNet, is trained without
supervision by minimizing the photometric reprojection error. KBNet imputes
missing depth value based on the training set, rather than on generic
regularization. We test KBNet on public depth completion benchmarks, where it
outperforms the state of the art by 30.5% indoor and 8.8% outdoor when the same
camera is used for training and testing. When the test camera is different, the
improvement reaches 62%. Code available at:
https://github.com/alexklwong/calibrated-backprojection-network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Scaling Law for Synthetic-to-Real Transfer: How Much Is Your Pre-training Effective?. (arXiv:2108.11018v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11018">
<div class="article-summary-box-inner">
<span><p>Synthetic-to-real transfer learning is a framework in which a synthetically
generated dataset is used to pre-train a model to improve its performance on
real vision tasks. The most significant advantage of using synthetic images is
that the ground-truth labels are automatically available, enabling unlimited
expansion of the data size without human cost. However, synthetic data may have
a huge domain gap, in which case increasing the data size does not improve the
performance. How can we know that? In this study, we derive a simple scaling
law that predicts the performance from the amount of pre-training data. By
estimating the parameters of the law, we can judge whether we should increase
the data or change the setting of image synthesis. Further, we analyze the
theory of transfer learning by considering learning dynamics and confirm that
the derived generalization bound is consistent with our empirical findings. We
empirically validated our scaling law on various experimental settings of
benchmark tasks, model sizes, and complexities of synthetic images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FOVEA: Foveated Image Magnification for Autonomous Navigation. (arXiv:2108.12102v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12102">
<div class="article-summary-box-inner">
<span><p>Efficient processing of high-res video streams is safety-critical for many
robotics applications such as autonomous driving. To maintain real-time
performance, many practical systems downsample the video stream. But this can
hurt downstream tasks such as (small) object detection. Instead, we take
inspiration from biological vision systems that allocate more foveal "pixels"
to salient parts of the scene. We introduce FOVEA, an approach for intelligent
downsampling that ensures salient image regions remain "magnified" in the
downsampled output. Given a high-res image, FOVEA applies a differentiable
resampling layer that outputs a small fixed-size image canvas, which is then
processed with a differentiable vision module (e.g., object detection network),
whose output is then differentiably backward mapped onto the original image
size. The key idea is to resample such that background pixels can make room for
salient pixels of interest. In order to ensure the overall pipeline remains
efficient, FOVEA makes use of cheap and readily available cues for saliency,
including dataset-specific spatial priors or temporal priors computed from
object predictions in the recent past. On the autonomous driving datasets
Argoverse-HD and BDD100K, our proposed method boosts the detection AP over
standard Faster R-CNN, both with and without finetuning. Without any noticeable
increase in compute, we improve accuracy on small objects by over 2x without
degrading performance on large objects. Finally, FOVEA sets a new record for
streaming AP (from 17.8 to 23.0 on a GTX 1080 Ti GPU), a metric designed to
capture both accuracy and latency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OARnet: Automated organs-at-risk delineation in Head and Neck CT images. (arXiv:2108.13987v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13987">
<div class="article-summary-box-inner">
<span><p>A 3D deep learning model (OARnet) is developed and used to delineate 28 H&amp;N
OARs on CT images. OARnet utilizes a densely connected network to detect the
OAR bounding-box, then delineates the OAR within the box. It reuses information
from any layer to subsequent layers and uses skip connections to combine
information from different dense block levels to progressively improve
delineation accuracy. Training uses up to 28 expert manual delineated (MD) OARs
from 165 CTs. Dice similarity coefficient (DSC) and the 95th percentile
Hausdorff distance (HD95) with respect to MD is assessed for 70 other CTs.
Mean, maximum, and root-mean-square dose differences with respect to MD are
assessed for 56 of the 70 CTs. OARnet is compared with UaNet, AnatomyNet, and
Multi-Atlas Segmentation (MAS). Wilcoxon signed-rank tests using 95% confidence
intervals are used to assess significance. Wilcoxon signed ranked tests show
that, compared with UaNet, OARnet improves (p&lt;0.05) the DSC (23/28 OARs) and
HD95 (17/28). OARnet outperforms both AnatomyNet and MAS for DSC (28/28) and
HD95 (27/28). Compared with UaNet, OARnet improves median DSC up to 0.05 and
HD95 up to 1.5mm. Compared with AnatomyNet and MAS, OARnet improves median
(DSC, HD95) by up to (0.08, 2.7mm) and (0.17, 6.3mm). Dosimetrically, OARnet
outperforms UaNet (Dmax 7/28; Dmean 10/28), AnatomyNet (Dmax 21/28; Dmean
24/28), and MAS (Dmax 22/28; Dmean 21/28). The DenseNet architecture is
optimized using a hybrid approach that performs OAR-specific bounding box
detection followed by feature recognition. Compared with other auto-delineation
methods, OARnet is better than or equal to UaNet for all but one geometric
(Temporal Lobe L, HD95) and one dosimetric (Eye L, mean dose) endpoint for the
28 H&amp;N OARs, and is better than or equal to both AnatomyNet and MAS for all
OARs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatiotemporal Inconsistency Learning for DeepFake Video Detection. (arXiv:2109.01860v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01860">
<div class="article-summary-box-inner">
<span><p>The rapid development of facial manipulation techniques has aroused public
concerns in recent years. Following the success of deep learning, existing
methods always formulate DeepFake video detection as a binary classification
problem and develop frame-based and video-based solutions. However, little
attention has been paid to capturing the spatial-temporal inconsistency in
forged videos. To address this issue, we term this task as a Spatial-Temporal
Inconsistency Learning (STIL) process and instantiate it into a novel STIL
block, which consists of a Spatial Inconsistency Module (SIM), a Temporal
Inconsistency Module (TIM), and an Information Supplement Module (ISM).
Specifically, we present a novel temporal modeling paradigm in TIM by
exploiting the temporal difference over adjacent frames along with both
horizontal and vertical directions. And the ISM simultaneously utilizes the
spatial information from SIM and temporal information from TIM to establish a
more comprehensive spatial-temporal representation. Moreover, our STIL block is
flexible and could be plugged into existing 2D CNNs. Extensive experiments and
visualizations are presented to demonstrate the effectiveness of our method
against the state-of-the-art competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Timbre Transfer with Variational Auto Encoding and Cycle-Consistent Adversarial Networks. (arXiv:2109.02096v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02096">
<div class="article-summary-box-inner">
<span><p>This research project investigates the application of deep learning to timbre
transfer, where the timbre of a source audio can be converted to the timbre of
a target audio with minimal loss in quality. The adopted approach combines
Variational Autoencoders with Generative Adversarial Networks to construct
meaningful representations of the source audio and produce realistic
generations of the target audio and is applied to the Flickr 8k Audio dataset
for transferring the vocal timbre between speakers and the URMP dataset for
transferring the musical timbre between instruments. Furthermore, variations of
the adopted approach are trained, and generalised performance is compared using
the metrics SSIM (Structural Similarity Index) and FAD (Frech\'et Audio
Distance). It was found that a many-to-many approach supersedes a one-to-one
approach in terms of reconstructive capabilities, and that the adoption of a
basic over a bottleneck residual block design is more suitable for enriching
content information about a latent space. It was also found that the decision
on whether cyclic loss takes on a variational autoencoder or vanilla
autoencoder approach does not have a significant impact on reconstructive and
adversarial translation aspects of the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TADA: Taxonomy Adaptive Domain Adaptation. (arXiv:2109.04813v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04813">
<div class="article-summary-box-inner">
<span><p>Traditional domain adaptation addresses the task of adapting a model to a
novel target domain under limited or no additional supervision. While tackling
the input domain gap, the standard domain adaptation settings assume no domain
change in the output space. In semantic prediction tasks, different datasets
are often labeled according to different semantic taxonomies. In many
real-world settings, the target domain task requires a different taxonomy than
the one imposed by the source domain. We therefore introduce the more general
taxonomy adaptive domain adaptation (TADA) problem, allowing for inconsistent
taxonomies between the two domains. We further propose an approach that jointly
addresses the image-level and label-level domain adaptation. On the
label-level, we employ a bilateral mixed sampling strategy to augment the
target domain, and a relabelling method to unify and align the label spaces. We
address the image-level domain gap by proposing an uncertainty-rectified
contrastive learning method, leading to more domain-invariant and class
discriminative features. We extensively evaluate the effectiveness of our
framework under different TADA settings: open taxonomy, coarse-to-fine
taxonomy, and partially-overlapping taxonomy. Our framework outperforms
previous state-of-the-art by a large margin, while capable of adapting to
target taxonomies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation. (arXiv:2109.06163v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06163">
<div class="article-summary-box-inner">
<span><p>Scene depth estimation from stereo and monocular imagery is critical for
extracting 3D information for downstream tasks such as scene understanding.
Recently, learning-based methods for depth estimation have received much
attention due to their high performance and flexibility in hardware choice.
However, collecting ground truth data for supervised training of these
algorithms is costly or outright impossible. This circumstance suggests a need
for alternative learning approaches that do not require corresponding depth
measurements. Indeed, self-supervised learning of depth estimation provides an
increasingly popular alternative. It is based on the idea that observed frames
can be synthesized from neighboring frames if accurate depth of the scene is
known - or in this case, estimated. We show empirically that - contrary to
common belief - improvements in image synthesis do not necessitate improvement
in depth estimation. Rather, optimizing for image synthesis can result in
diverging performance with respect to the main prediction objective - depth. We
attribute this diverging phenomenon to aleatoric uncertainties, which originate
from data. Based on our experiments on four datasets (spanning street, indoor,
and medical) and five architectures (monocular and stereo), we conclude that
this diverging phenomenon is independent of the dataset domain and not
mitigated by commonly used regularization techniques. To underscore the
importance of this finding, we include a survey of methods which use image
synthesis, totaling 127 papers over the last six years. This observed
divergence has not been previously reported or studied in depth, suggesting
room for future improvement of self-supervised approaches which might be
impacted the finding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MHFC: Multi-Head Feature Collaboration for Few-Shot Learning. (arXiv:2109.07785v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07785">
<div class="article-summary-box-inner">
<span><p>Few-shot learning (FSL) aims to address the data-scarce problem. A standard
FSL framework is composed of two components: (1) Pre-train. Employ the base
data to generate a CNN-based feature extraction model (FEM). (2) Meta-test.
Apply the trained FEM to acquire the novel data's features and recognize them.
FSL relies heavily on the design of the FEM. However, various FEMs have
distinct emphases. For example, several may focus more attention on the contour
information, whereas others may lay particular emphasis on the texture
information. The single-head feature is only a one-sided representation of the
sample. Besides the negative influence of cross-domain (e.g., the trained FEM
can not adapt to the novel class flawlessly), the distribution of novel data
may have a certain degree of deviation compared with the ground truth
distribution, which is dubbed as distribution-shift-problem (DSP). To address
the DSP, we propose Multi-Head Feature Collaboration (MHFC) algorithm, which
attempts to project the multi-head features (e.g., multiple features extracted
from a variety of FEMs) to a unified space and fuse them to capture more
discriminative information. Typically, first, we introduce a subspace learning
method to transform the multi-head features to aligned low-dimensional
representations. It corrects the DSP via learning the feature with more
powerful discrimination and overcomes the problem of inconsistent measurement
scales from different head features. Then, we design an attention block to
update combination weights for each head feature automatically. It
comprehensively considers the contribution of various perspectives and further
improves the discrimination of features. We evaluate the proposed method on
five benchmark datasets (including cross-domain experiments) and achieve
significant improvements of 2.1%-7.8% compared with state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lifting 2D Object Locations to 3D by Discounting LiDAR Outliers across Objects and Views. (arXiv:2109.07945v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07945">
<div class="article-summary-box-inner">
<span><p>We present a system for automatic converting of 2D mask object predictions
and raw LiDAR point clouds into full 3D bounding boxes of objects. Because the
LiDAR point clouds are partial, directly fitting bounding boxes to the point
clouds is meaningless. Instead, we suggest that obtaining good results requires
sharing information between \emph{all} objects in the dataset jointly, over
multiple frames. We then make three improvements to the baseline. First, we
address ambiguities in predicting the object rotations via direct optimization
in this space while still backpropagating rotation prediction through the
model. Second, we explicitly model outliers and task the network with learning
their typical patterns, thus better discounting them. Third, we enforce
temporal consistency when video data is available. With these contributions,
our method significantly outperforms previous work despite the fact that those
methods use significantly more complex pipelines, 3D models and additional
human-annotated external sources of prior information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring the rogue wave pattern triggered from Gaussian perturbations by deep learning. (arXiv:2109.08909v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08909">
<div class="article-summary-box-inner">
<span><p>Weak Gaussian perturbations on a plane wave background could trigger lots of
rogue waves, due to modulational instability. Numerical simulations showed that
these rogue waves seemed to have similar unit structure. However, to the best
of our knowledge, there is no relative result to prove that these rogue waves
have the similar patterns for different perturbations, partly due to that it is
hard to measure the rogue wave pattern automatically. In this work, we address
these problems from the perspective of computer vision via using deep neural
networks. We propose a Rogue Wave Detection Network (RWD-Net) model to
automatically and accurately detect RWs on the images, which directly indicates
they have the similar computer vision patterns. For this purpose, we herein
meanwhile have designed the related dataset, termed as Rogue Wave Dataset-$10$K
(RWD-$10$K), which has $10,191$ RW images with bounding box annotations for
each RW unit. In our detection experiments, we get $99.29\%$ average precision
on the test splits of the RWD-$10$K dataset. Finally, we derive our novel
metric, the density of RW units (DRW), to characterize the evolution of
Gaussian perturbations and obtain the statistical results on them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iWave3D: End-to-end Brain Image Compression with Trainable 3-D Wavelet Transform. (arXiv:2109.08942v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08942">
<div class="article-summary-box-inner">
<span><p>With the rapid development of whole brain imaging technology, a large number
of brain images have been produced, which puts forward a great demand for
efficient brain image compression methods. At present, the most commonly used
compression methods are all based on 3-D wavelet transform, such as JP3D.
However, traditional 3-D wavelet transforms are designed manually with certain
assumptions on the signal, but brain images are not as ideal as assumed. What's
more, they are not directly optimized for compression task. In order to solve
these problems, we propose a trainable 3-D wavelet transform based on the
lifting scheme, in which the predict and update steps are replaced by 3-D
convolutional neural networks. Then the proposed transform is embedded into an
end-to-end compression scheme called iWave3D, which is trained with a large
amount of brain images to directly minimize the rate-distortion loss.
Experimental results demonstrate that our method outperforms JP3D significantly
by 2.012 dB in terms of average BD-PSNR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Context-Aware Network for Abdominal Multi-organ Segmentation. (arXiv:2109.10601v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10601">
<div class="article-summary-box-inner">
<span><p>The contextual information, presented in abdominal CT scan, is relative
consistent. In order to make full use of the overall 3D context, we develop a
whole-volume-based coarse-to-fine framework for efficient and effective
abdominal multi-organ segmentation. We propose a new efficientSegNet network,
which is composed of encoder, decoder and context block. For the decoder
module, anisotropic convolution with a k*k*1 intra-slice convolution and a
1*1*k inter-slice convolution, is designed to reduce the computation burden.
For the context block, we propose strip pooling module to capture anisotropic
and long-range contextual information, which exists in abdominal scene.
Quantitative evaluation on the FLARE2021 validation cases, this method achieves
the average dice similarity coefficient (DSC) of 0.895 and average normalized
surface distance (NSD) of 0.775. The average running time is 9.8 s per case in
inference phase, and maximum used GPU memory is 1017 MB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two Souls in an Adversarial Image: Towards Universal Adversarial Example Detection using Multi-view Inconsistency. (arXiv:2109.12459v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12459">
<div class="article-summary-box-inner">
<span><p>In the evasion attacks against deep neural networks (DNN), the attacker
generates adversarial instances that are visually indistinguishable from benign
samples and sends them to the target DNN to trigger misclassifications. In this
paper, we propose a novel multi-view adversarial image detector, namely Argos,
based on a novel observation. That is, there exist two "souls" in an
adversarial instance, i.e., the visually unchanged content, which corresponds
to the true label, and the added invisible perturbation, which corresponds to
the misclassified label. Such inconsistencies could be further amplified
through an autoregressive generative approach that generates images with seed
pixels selected from the original image, a selected label, and pixel
distributions learned from the training data. The generated images (i.e., the
"views") will deviate significantly from the original one if the label is
adversarial, demonstrating inconsistencies that Argos expects to detect. To
this end, Argos first amplifies the discrepancies between the visual content of
an image and its misclassified label induced by the attack using a set of
regeneration mechanisms and then identifies an image as adversarial if the
reproduced views deviate to a preset degree. Our experimental results show that
Argos significantly outperforms two representative adversarial detectors in
both detection accuracy and robustness against six well-known adversarial
attacks. Code is available at:
https://github.com/sohaib730/Argos-Adversarial_Detection
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferability Estimation for Semantic Segmentation Task. (arXiv:2109.15242v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15242">
<div class="article-summary-box-inner">
<span><p>Transferability estimation is a fundamental problem in transfer learning to
predict how good the performance is when transferring a source model (or source
task) to a target task. With the guidance of transferability score, we can
efficiently select the highly transferable source models without performing the
real transfer in practice. Recent analytical transferability metrics are mainly
designed for image classification problem, and currently there is no specific
investigation for the transferability estimation of semantic segmentation task,
which is an essential problem in autonomous driving, medical image analysis,
etc. Consequently, we further extend the recent analytical transferability
metric OTCE (Optimal Transport based Conditional Entropy) score to the semantic
segmentation task. The challenge in applying the OTCE score is the high
dimensional segmentation output, which is difficult to find the optimal
coupling between so many pixels under an acceptable computation cost. Thus we
propose to randomly sample N pixels for computing OTCE score and take the
expectation over K repetitions as the final transferability score. Experimental
evaluation on Cityscapes, BDD100K and GTA5 datasets demonstrates that the OTCE
score highly correlates with the transfer performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Few-Shot Action Recognition via Action-Appearance Aligned Meta-Adaptation. (arXiv:2109.15317v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15317">
<div class="article-summary-box-inner">
<span><p>We present MetaUVFS as the first Unsupervised Meta-learning algorithm for
Video Few-Shot action recognition. MetaUVFS leverages over 550K unlabeled
videos to train a two-stream 2D and 3D CNN architecture via contrastive
learning to capture the appearance-specific spatial and action-specific
spatio-temporal video features respectively. MetaUVFS comprises a novel
Action-Appearance Aligned Meta-adaptation (A3M) module that learns to focus on
the action-oriented video features in relation to the appearance features via
explicit few-shot episodic meta-learning over unsupervised hard-mined episodes.
Our action-appearance alignment and explicit few-shot learner conditions the
unsupervised training to mimic the downstream few-shot task, enabling MetaUVFS
to significantly outperform all unsupervised methods on few-shot benchmarks.
Moreover, unlike previous few-shot action recognition methods that are
supervised, MetaUVFS needs neither base-class labels nor a supervised
pretrained backbone. Thus, we need to train MetaUVFS just once to perform
competitively or sometimes even outperform state-of-the-art supervised methods
on popular HMDB51, UCF101, and Kinetics100 few-shot datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable Event Recognition. (arXiv:2110.00755v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00755">
<div class="article-summary-box-inner">
<span><p>The literature shows outstanding capabilities for CNNs in event recognition
in images. However, fewer attempts are made to analyze the potential causes
behind the decisions of the models and exploring whether the predictions are
based on event-salient objects or regions? To explore this important aspect of
event recognition, in this work, we propose an explainable event recognition
framework relying on Grad-CAM and an Xception architecture-based CNN model.
Experiments are conducted on three large-scale datasets covering a diversified
set of natural disasters, social, and sports events. Overall, the model showed
outstanding generalization capabilities obtaining overall F1-scores of 0.91,
0.94, and 0.97 on natural disasters, social, and sports events, respectively.
Moreover, for subjective analysis of activation maps generated through Grad-CAM
for the predicted samples of the model, a crowdsourcing study is conducted to
analyze whether the model's predictions are based on event-related
objects/regions or not? The results of the study indicate that 78%, 84%, and
78% of the model decisions on natural disasters, sports, and social events
datasets, respectively, are based onevent-related objects or regions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does deep learning model calibration improve performance in class-imbalanced medical image classification?. (arXiv:2110.00918v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00918">
<div class="article-summary-box-inner">
<span><p>In medical image classification tasks, it is common to find that the number
of normal samples far exceeds the number of abnormal samples. In such
class-imbalanced situations, reliable training of deep neural networks
continues to be a major challenge. Under these circumstances, the predicted
class probabilities may be biased toward the majority class. Calibration has
been suggested to alleviate some of these effects. However, there is
insufficient analysis explaining when and whether calibrating a model would be
beneficial in improving performance. In this study, we perform a systematic
analysis of the effect of model calibration on its performance on two medical
image modalities, namely, chest X-rays and fundus images, using various deep
learning classifier backbones. For this, we study the following variations: (i)
the degree of imbalances in the dataset used for training; (ii) calibration
methods; and (iii) two classification thresholds, namely, default decision
threshold of 0.5, and optimal threshold from precision-recall curves. Our
results indicate that at the default operating threshold of 0.5, the
performance achieved through calibration is significantly superior (p &lt; 0.05)
to using uncalibrated probabilities. However, at the PR-guided threshold, these
gains are not significantly different (p &gt; 0.05). This finding holds for both
image modalities and at varying degrees of imbalance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Representation Learning for Spatial Image Steganalysis. (arXiv:2110.00957v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00957">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a graph representation learning architecture for
spatial image steganalysis, which is motivated by the assumption that
steganographic modifications unavoidably distort the statistical
characteristics of the hidden graph features derived from cover images. In the
detailed architecture, we translate each image to a graph, where nodes
represent the patches of the image and edges indicate the local associations
between the patches. Each node is associated with a feature vector determined
from the corresponding patch by a shallow convolutional neural network (CNN)
structure. By feeding the graph to an attention network, the discriminative
features can be learned for efficient steganalysis. Experiments indicate that
the reported architecture achieves a competitive performance compared to the
benchmark CNN model, which has shown the potential of graph learning for
steganalysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A free lunch from ViT:Adaptive Attention Multi-scale Fusion Transformer for Fine-grained Visual Recognition. (arXiv:2110.01240v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01240">
<div class="article-summary-box-inner">
<span><p>Learning subtle representation about object parts plays a vital role in
fine-grained visual recognition (FGVR) field. The vision transformer (ViT)
achieves promising results on computer vision due to its attention mechanism.
Nonetheless, with the fixed size of patches in ViT, the class token in deep
layer focuses on the global receptive field and cannot generate
multi-granularity features for FGVR. To capture region attention without box
annotations and compensate for ViT shortcomings in FGVR, we propose a novel
method named Adaptive attention multi-scale Fusion Transformer (AFTrans). The
Selective Attention Collection Module (SACM) in our approach leverages
attention weights in ViT and filters them adaptively to correspond with the
relative importance of input patches. The multiple scales (global and local)
pipeline is supervised by our weights sharing encoder and can be easily trained
end-to-end. Comprehensive experiments demonstrate that AFTrans can achieve SOTA
performance on three published fine-grained benchmarks: CUB-200-2011, Stanford
Dogs and iNat2017.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Procedure Planning in Instructional Videos via Contextual Modeling and Model-based Policy Learning. (arXiv:2110.01770v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01770">
<div class="article-summary-box-inner">
<span><p>Learning new skills by observing humans' behaviors is an essential capability
of AI. In this work, we leverage instructional videos to study humans'
decision-making processes, focusing on learning a model to plan goal-directed
actions in real-life videos. In contrast to conventional action recognition,
goal-directed actions are based on expectations of their outcomes requiring
causal knowledge of potential consequences of actions. Thus, integrating the
environment structure with goals is critical for solving this task. Previous
works learn a single world model will fail to distinguish various tasks,
resulting in an ambiguous latent space; planning through it will gradually
neglect the desired outcomes since the global information of the future goal
degrades quickly as the procedure evolves. We address these limitations with a
new formulation of procedure planning and propose novel algorithms to model
human behaviors through Bayesian Inference and model-based Imitation Learning.
Experiments conducted on real-world instructional videos show that our method
can achieve state-of-the-art performance in reaching the indicated goals.
Furthermore, the learned contextual information presents interesting features
for planning in a latent space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning of Perceptually Optimized Block Motion Estimates for Video Compression. (arXiv:2110.01805v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01805">
<div class="article-summary-box-inner">
<span><p>Block based motion estimation is integral to inter prediction processes
performed in hybrid video codecs. Prevalent block matching based methods that
are used to compute block motion vectors (MVs) rely on computationally
intensive search procedures. They also suffer from the aperture problem, which
can worsen as the block size is reduced. Moreover, the block matching criteria
used in typical codecs do not account for the resulting levels of perceptual
quality of the motion compensated pictures that are created upon decoding.
Towards achieving the elusive goal of perceptually optimized motion estimation,
we propose a search-free block motion estimation framework using a multi-stage
convolutional neural network, which is able to conduct motion estimation on
multiple block sizes simultaneously, using a triplet of frames as input. This
composite block translation network (CBT-Net) is trained in a self-supervised
manner on a large database that we created from publicly available uncompressed
video content. We deploy the multi-scale structural similarity (MS-SSIM) loss
function to optimize the perceptual quality of the motion compensated predicted
frames. Our experimental results highlight the computational efficiency of our
proposed model relative to conventional block matching based motion estimation
algorithms, for comparable prediction errors. Further, when used to perform
inter prediction in AV1, the MV predictions of the perceptually optimized model
result in average Bjontegaard-delta rate (BD-rate) improvements of -1.70% and
-1.52% with respect to the MS-SSIM and Video Multi-Method Assessment Fusion
(VMAF) quality metrics, respectively as compared to the block matching based
motion estimation system employed in the SVT-AV1 encoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review of Computer Vision Technologies for Fish Tracking. (arXiv:2110.02551v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02551">
<div class="article-summary-box-inner">
<span><p>Fish tracking based on computer vision is a complex and challenging task in
fishery production and ecological studies. Most of the applications of fish
tracking use classic filtering algorithms, which lack in accuracy and
efficiency. To solve this issue, deep learning methods utilized deep neural
networks to extract the features, which achieve a good performance in the fish
tracking. Some one-stage detection algorithms have gradually been adopted in
this area for the real-time applications. The transfer learning to fish target
is the current development direction. At present, fish tracking technology is
not enough to cover actual application requirements. According to the
literature data collected by us, there has not been any extensive review about
vision-based fish tracking in the community. In this paper, we introduced the
development and application prospects of fish tracking technology in last ten
years. Firstly, we introduced the open source datasets of fish, and summarized
the preprocessing technologies of underwater images. Secondly, we analyzed the
detection and tracking algorithms for fish, and sorted out some transferable
frontier tracking model. Thirdly, we listed the actual applications, metrics
and bottlenecks of the fish tracking such as occlusion and multi-scale.
Finally, we give the discussion for fish tracking datasets, solutions of the
bottlenecks, and improvements. We expect that our work can help the fish
tracking models to achieve higher accuracy and robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Robustness Comparison of Vision Transformer and MLP-Mixer to CNNs. (arXiv:2110.02797v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02797">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNNs) have become the de facto gold standard
in computer vision applications in the past years. Recently, however, new model
architectures have been proposed challenging the status quo. The Vision
Transformer (ViT) relies solely on attention modules, while the MLP-Mixer
architecture substitutes the self-attention modules with Multi-Layer
Perceptrons (MLPs). Despite their great success, CNNs have been widely known to
be vulnerable to adversarial attacks, causing serious concerns for
security-sensitive applications. Thus, it is critical for the community to know
whether the newly proposed ViT and MLP-Mixer are also vulnerable to adversarial
attacks. To this end, we empirically evaluate their adversarial robustness
under several adversarial attack setups and benchmark them against the widely
used CNNs. Overall, we find that the two architectures, especially ViT, are
more robust than their CNN models. Using a toy example, we also provide
empirical evidence that the lower adversarial robustness of CNNs can be
partially attributed to their shift-invariant property. Our frequency analysis
suggests that the most robust ViT architectures tend to rely more on
low-frequency features compared with CNNs. Additionally, we have an intriguing
finding that MLP-Mixer is extremely vulnerable to universal adversarial
perturbations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TreeGCN-ED: Encoding Point Cloud using a Tree-Structured Graph Network. (arXiv:2110.03170v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03170">
<div class="article-summary-box-inner">
<span><p>Point cloud is an efficient way of representing and storing 3D geometric
data. Deep learning algorithms on point clouds are time and memory efficient.
Several methods such as PointNet and FoldingNet have been proposed for
processing point clouds. This work proposes an autoencoder based framework to
generate robust embeddings for point clouds by utilizing hierarchical
information using graph convolution. We perform multiple experiments to assess
the quality of embeddings generated by the proposed encoder architecture and
visualize the t-SNE map to highlight its ability to distinguish between
different object classes. We further demonstrate the applicability of the
proposed framework in applications like: 3D point cloud completion and Single
image based 3D reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Token Pooling in Vision Transformers. (arXiv:2110.03860v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03860">
<div class="article-summary-box-inner">
<span><p>Despite the recent success in many applications, the high computational
requirements of vision transformers limit their use in resource-constrained
settings. While many existing methods improve the quadratic complexity of
attention, in most vision transformers, self-attention is not the major
computation bottleneck, e.g., more than 80% of the computation is spent on
fully-connected layers. To improve the computational complexity of all layers,
we propose a novel token downsampling method, called Token Pooling, efficiently
exploiting redundancies in the images and intermediate token representations.
We show that, under mild assumptions, softmax-attention acts as a
high-dimensional low-pass (smoothing) filter. Thus, its output contains
redundancy that can be pruned to achieve a better trade-off between the
computational cost and accuracy. Our new technique accurately approximates a
set of tokens by minimizing the reconstruction error caused by downsampling. We
solve this optimization problem via cost-efficient clustering. We rigorously
analyze and compare to prior downsampling methods. Our experiments show that
Token Pooling significantly improves the cost-accuracy trade-off over the
state-of-the-art downsampling. Token Pooling is a simple and effective operator
that can benefit many architectures. Applied to DeiT, it achieves the same
ImageNet top-1 accuracy using 42% fewer computations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi Proxy Anchor Loss and Effectiveness of Deep Metric Learning Performance Metrics. (arXiv:2110.03997v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03997">
<div class="article-summary-box-inner">
<span><p>Deep metric learning (DML) learns the mapping, which maps into embedding
space in which similar data is near and dissimilar data is far. In this paper,
we propose the new proxy-based loss and the new DML performance metric. This
study contributes two following: (1) we propose multi-proxies anchor (MPA)
loss, and we show the effectiveness of the multi-proxies approach on
proxy-based loss. (2) we establish the good stability and flexible normalized
discounted cumulative gain (nDCG@k) metric as the effective DML performance
metric. Finally, we demonstrate MPA loss's effectiveness, and MPA loss achieves
new state-of-the-art performance on two datasets for fine-grained images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Spatial Nonstationarity via Deformable Convolutions for Deep Traffic Flow Prediction. (arXiv:2101.12010v2 [physics.soc-ph] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.12010">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are being increasingly used for short-term traffic flow
prediction, which can be generally categorized as convolutional (CNNs) or graph
neural networks (GNNs). CNNs are preferable for region-wise traffic prediction
by taking advantage of localized spatial correlations, whilst GNNs achieves
better performance for graph-structured traffic data. When applied to
region-wise traffic prediction, CNNs typically partition an underlying
territory into grid-like spatial units, and employ standard convolutions to
learn spatial dependence among the units. However, standard convolutions with
fixed geometric structures cannot fully model the nonstationary characteristics
of local traffic flows. To overcome the deficiency, we introduce deformable
convolution that augments the spatial sampling locations with additional
offsets, to enhance the modeling capability of spatial nonstationarity. On this
basis, we design a deep deformable convolutional residual network, namely
DeFlow-Net, that can effectively model global spatial dependence, local spatial
nonstationarity, and temporal periodicity of traffic flows. Furthermore, to
better fit with convolutions, we suggest to first aggregate traffic flows
according to pre-conceived regions or self-organized regions based on traffic
flows, then dispose to sequentially organized raster images for network input.
Extensive experiments on real-world traffic flows demonstrate that DeFlow-Net
outperforms GNNs and existing CNNs using standard convolutions, and spatial
partition by pre-conceived regions or self-organized regions further enhances
the performance. We also demonstrate the advantage of DeFlow-Net in maintaining
spatial autocorrelation, and reveal the impacts of partition shapes and scales
on deep traffic flow prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image scaling by de la Vall\'ee-Poussin filtered interpolation. (arXiv:2109.13897v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13897">
<div class="article-summary-box-inner">
<span><p>We present a new image scaling method both for downscaling and upscaling,
running with any scale factor or desired size. It is based on the sampling of
an approximating bivariate polynomial, which globally interpolates the data and
is defined by a filter of de la Vall\'ee Poussin type whose action ray is
suitable regulated to improve the approximation. The method has been tested on
a significant number of different image datasets. The results are evaluated in
qualitative and quantitative terms and compared with other available
competitive methods. The perceived quality of the resulting scaled images is
such that important details are preserved, and the appearance of artifacts is
low. Very high-quality measure values in downscaling and the competitive ones
in upscaling evidence the effectiveness of the method. Good visual quality,
limited computational effort, and moderate memory demanding make the method
suitable for real-world applications.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-10-12 23:02:17.523403560 UTC">2021-10-12 23:02:17 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>