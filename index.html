<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-06-16T01:30:00Z">06-16</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A smile is all you need: Predicting limiting activity coefficients from SMILES with natural language processing. (arXiv:2206.07048v1 [physics.chem-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07048">
<div class="article-summary-box-inner">
<span><p>Knowledge of mixtures' phase equilibria is crucial in nature and technical
chemistry. Phase equilibria calculations of mixtures require activity
coefficients. However, experimental data on activity coefficients is often
limited due to high cost of experiments. For an accurate and efficient
prediction of activity coefficients, machine learning approaches have been
recently developed. However, current machine learning approaches still
extrapolate poorly for activity coefficients of unknown molecules. In this
work, we introduce the SMILES-to-Properties-Transformer (SPT), a natural
language processing network to predict binary limiting activity coefficients
from SMILES codes. To overcome the limitations of available experimental data,
we initially train our network on a large dataset of synthetic data sampled
from COSMO-RS (10 Million data points) and then fine-tune the model on
experimental data (20 870 data points). This training strategy enables SPT to
accurately predict limiting activity coefficients even for unknown molecules,
cutting the mean prediction error in half compared to state-of-the-art models
for activity coefficient predictions such as COSMO-RS, UNIFAC, and improving on
recent machine learning approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NewsEdits: A News Article Revision Dataset and a Document-Level Reasoning Challenge. (arXiv:2206.07106v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07106">
<div class="article-summary-box-inner">
<span><p>News article revision histories provide clues to narrative and factual
evolution in news articles. To facilitate analysis of this evolution, we
present the first publicly available dataset of news revision histories,
NewsEdits. Our dataset is large-scale and multilingual; it contains 1.2 million
articles with 4.6 million versions from over 22 English- and French-language
newspaper sources based in three countries, spanning 15 years of coverage
(2006-2021).
</p>
<p>We define article-level edit actions: Addition, Deletion, Edit and Refactor,
and develop a high-accuracy extraction algorithm to identify these actions. To
underscore the factual nature of many edit actions, we conduct analyses showing
that added and deleted sentences are more likely to contain updating events,
main content and quotes than unchanged sentences.
</p>
<p>Finally, to explore whether edit actions are predictable, we introduce three
novel tasks aimed at predicting actions performed during version updates. We
show that these tasks are possible for expert humans but are challenging for
large NLP models. We hope this can spur research in narrative framing and help
provide predictive tools for journalists chasing breaking news.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">If it Bleeds, it Leads: A Computational Approach to Covering Crime in Los Angeles. (arXiv:2206.07115v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07115">
<div class="article-summary-box-inner">
<span><p>Developing and improving computational approaches to covering news can
increase journalistic output and improve the way stories are covered. In this
work we approach the problem of covering crime stories in Los Angeles. We
present a machine-in-the-loop system that covers individual crimes by (1)
learning the prototypical coverage archetypes from classical news articles on
crime to learn their structure and (2) using output from the Los Angeles Police
department to generate "lede paragraphs", first structural unit of
crime-articles. We introduce a probabilistic graphical model for learning
article structure and a rule-based system for generating ledes. We hope our
work can lead to systems that use these components together to form the
skeletons of news articles covering crime.
</p>
<p>This work was done for a class project in Jonathan May's Advanced Natural
Language Processing Course, Fall, 2019.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Clipping: Differentially Private Deep Learning Made Easier and Stronger. (arXiv:2206.07136v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07136">
<div class="article-summary-box-inner">
<span><p>Per-example gradient clipping is a key algorithmic step that enables
practical differential private (DP) training for deep learning models. The
choice of clipping norm $R$, however, is shown to be vital for achieving high
accuracy under DP. We propose an easy-to-use replacement, called AutoClipping,
that eliminates the need to tune $R$ for any DP optimizers, including DP-SGD,
DP-Adam, DP-LAMB and many others. The automatic variants are as private and
computationally efficient as existing DP optimizers, but require no DP-specific
hyperparameters and thus make DP training as amenable as the standard
non-private training. We give a rigorous convergence analysis of automatic
DP-SGD in the non-convex setting, which shows that it enjoys an asymptotic
convergence rate that matches the standard SGD. We also demonstrate on various
language and vision tasks that automatic clipping outperforms or matches the
state-of-the-art, and can be easily employed with minimal changes to existing
codebases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Narratives through Dimensions of Analogy. (arXiv:2206.07167v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07167">
<div class="article-summary-box-inner">
<span><p>Analogical reasoning is a powerful qualitative reasoning tool that enables
humans to connect two situations, and to generalize their knowledge from
familiar to novel situations. Cognitive Science research provides valuable
insights into the richness and complexity of analogical reasoning, together
with implementations of expressive analogical reasoners with limited
scalability. Modern scalable AI techniques with the potential to reason by
analogy have been only applied to the special case of proportional analogy, and
not to understanding higher-order analogies. In this paper, we aim to bridge
the gap by: 1) formalizing six dimensions of analogy based on mature insights
from Cognitive Science research, 2) annotating a corpus of fables with each of
these dimensions, and 3) defining four tasks with increasing complexity that
enable scalable evaluation of AI techniques. Experiments with language models
and neuro-symbolic AI reasoners on these tasks reveal that state-of-the-art
methods can be applied to reason by analogy with a limited success, motivating
the need for further research towards comprehensive and scalable analogical
reasoning by AI. We make all our code and data available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frequency-centroid features for word recognition of non-native English speakers. (arXiv:2206.07176v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07176">
<div class="article-summary-box-inner">
<span><p>The objective of this work is to investigate complementary features which can
aid the quintessential Mel frequency cepstral coefficients (MFCCs) in the task
of closed, limited set word recognition for non-native English speakers of
different mother-tongues. Unlike the MFCCs, which are derived from the spectral
energy of the speech signal, the proposed frequency-centroids (FCs) encapsulate
the spectral centres of the different bands of the speech spectrum, with the
bands defined by the Mel filterbank. These features, in combination with the
MFCCs, are observed to provide relative performance improvement in English word
recognition, particularly under varied noisy conditions. A two-stage
Convolution Neural Network (CNN) is used to model the features of the English
words uttered with Arabic, French and Spanish accents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Codec at SemEval-2022 Task 5: Multi-Modal Multi-Transformer Misogynous Meme Classification Framework. (arXiv:2206.07190v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07190">
<div class="article-summary-box-inner">
<span><p>In this paper we describe our work towards building a generic framework for
both multi-modal embedding and multi-label binary classification tasks, while
participating in task 5 (Multimedia Automatic Misogyny Identification) of
SemEval 2022 competition.
</p>
<p>Since pretraining deep models from scratch is a resource and data hungry
task, our approach is based on three main strategies. We combine different
state-of-the-art architectures to capture a wide spectrum of semantic signals
from the multi-modal input. We employ a multi-task learning scheme to be able
to use multiple datasets from the same knowledge domain to help increase the
model's performance. We also use multiple objectives to regularize and fine
tune different system components.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Event Graphs: Towards Event Centric Understanding of Multimodal World. (arXiv:2206.07207v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07207">
<div class="article-summary-box-inner">
<span><p>Understanding how events described or shown in multimedia content relate to
one another is a critical component to developing robust artificially
intelligent systems which can reason about real-world media. While much
research has been devoted to event understanding in the text, image, and video
domains, none have explored the complex relations that events experience across
domains. For example, a news article may describe a `protest' event while a
video shows an `arrest' event. Recognizing that the visual `arrest' event is a
subevent of the broader `protest' event is a challenging, yet important problem
that prior work has not explored. In this paper, we propose the novel task of
MultiModal Event Event Relations to recognize such cross-modal event relations.
We contribute a large-scale dataset consisting of 100k video-news article
pairs, as well as a benchmark of densely annotated data. We also propose a
weakly supervised multimodal method which integrates commonsense knowledge from
an external knowledge base (KB) to predict rich multimodal event hierarchies.
Experiments show that our model outperforms a number of competitive baselines
on our proposed benchmark. We also perform a detailed analysis of our model's
performance and suggest directions for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Location-based Twitter Filtering for the Creation of Low-Resource Language Datasets in Indonesian Local Languages. (arXiv:2206.07238v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07238">
<div class="article-summary-box-inner">
<span><p>Twitter contains an abundance of linguistic data from the real world. We
examine Twitter for user-generated content in low-resource languages such as
local Indonesian. For NLP to work in Indonesian, it must consider local
dialects, geographic context, and regional culture influence Indonesian
languages. This paper identifies the problems we faced when constructing a
Local Indonesian NLP dataset. Furthermore, we are developing a framework for
creating, collecting, and classifying Local Indonesian datasets for NLP. Using
twitter's geolocation tool for automatic annotating.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TeKo: Text-Rich Graph Neural Networks with External Knowledge. (arXiv:2206.07253v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07253">
<div class="article-summary-box-inner">
<span><p>Graph Neural Networks (GNNs) have gained great popularity in tackling various
analytical tasks on graph-structured data (i.e., networks). Typical GNNs and
their variants follow a message-passing manner that obtains network
representations by the feature propagation process along network topology,
which however ignore the rich textual semantics (e.g., local word-sequence)
that exist in many real-world networks. Existing methods for text-rich networks
integrate textual semantics by mainly utilizing internal information such as
topics or phrases/words, which often suffer from an inability to
comprehensively mine the text semantics, limiting the reciprocal guidance
between network structure and text semantics. To address these problems, we
propose a novel text-rich graph neural network with external knowledge (TeKo),
in order to take full advantage of both structural and textual information
within text-rich networks. Specifically, we first present a flexible
heterogeneous semantic network that incorporates high-quality entities and
interactions among documents and entities. We then introduce two types of
external knowledge, that is, structured triplets and unstructured entity
description, to gain a deeper insight into textual semantics. We further design
a reciprocal convolutional mechanism for the constructed heterogeneous semantic
network, enabling network structure and textual semantics to collaboratively
enhance each other and learn high-level network representations. Extensive
experimental results on four public text-rich networks as well as a large-scale
e-commerce searching dataset illustrate the superior performance of TeKo over
state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Born for Auto-Tagging: Faster and better with new objective functions. (arXiv:2206.07264v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07264">
<div class="article-summary-box-inner">
<span><p>Keyword extraction is a task of text mining. It is applied to increase search
volume in SEO and ads. Implemented in auto-tagging, it makes tagging on a mass
scale of online articles and photos efficiently and accurately. BAT is invented
for auto-tagging which served as awoo's AI marketing platform (AMP). awoo AMP
not only provides service as a customized recommender system but also increases
the converting rate in E-commerce. The strength of BAT converges faster and
better than other SOTA models, as its 4-layer structure achieves the best F
scores at 50 epochs. In other words, it performs better than other models which
require deeper layers at 100 epochs. To generate rich and clean tags, awoo
creates new objective functions to maintain similar ${\rm F_1}$ scores with
cross-entropy while enhancing ${\rm F_2}$ scores simultaneously. To assure the
even better performance of F scores awoo revamps the learning rate strategy
proposed by Transformer \cite{Transformer} to increase ${\rm F_1}$ and ${\rm
F_2}$ scores at the same time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Heuristics for AI-Generated Language Are Flawed. (arXiv:2206.07271v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07271">
<div class="article-summary-box-inner">
<span><p>Human communication is increasingly intermixed with language generated by AI.
Across chat, email, and social media, AI systems produce smart replies,
autocompletes, and translations. AI-generated language is often not identified
as such but poses as human language, raising concerns about novel forms of
deception and manipulation. Here, we study how humans discern whether one of
the most personal and consequential forms of language - a self-presentation -
was generated by AI. Across six experiments, participants (N = 4,650) tried to
identify self-presentations generated by state-of-the-art language models.
Across professional, hospitality, and romantic settings, we find that humans
are unable to identify AI-generated self-presentations. Combining qualitative
analyses with language feature engineering, we find that human judgments of
AI-generated language are handicapped by intuitive but flawed heuristics such
as associating first-person pronouns, authentic words, or family topics with
humanity. We show that these heuristics make human judgment of generated
language predictable and manipulable, allowing AI systems to produce language
perceived as more human than human. We conclude by discussing solutions - such
as AI accents or fair use policies - to reduce the deceptive potential of
generated language, limiting the subversion of human intuition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Knowledge Selection for Grounded Dialogues via Document Semantic Graphs. (arXiv:2206.07296v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07296">
<div class="article-summary-box-inner">
<span><p>Providing conversation models with background knowledge has been shown to
make open-domain dialogues more informative and engaging. Existing models treat
knowledge selection as a sentence ranking or classification problem where each
sentence is handled individually, ignoring the internal semantic connection
among sentences in the background document. In this work, we propose to
automatically convert the background knowledge documents into document semantic
graphs and then perform knowledge selection over such graphs. Our document
semantic graphs preserve sentence-level information through the use of sentence
nodes and provide concept connections between sentences. We jointly apply
multi-task learning for sentence-level and concept-level knowledge selection
and show that it improves sentence-level selection. Our experiments show that
our semantic graph-based knowledge selection improves over sentence selection
baselines for both the knowledge selection task and the end-to-end response
generation task on HollE and improves generalization on unseen topics in WoW.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Management System with NLP-Assisted Annotations: A Brief Survey and Outlook. (arXiv:2206.07304v1 [cs.DB])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07304">
<div class="article-summary-box-inner">
<span><p>Knowledge management systems are in high demand for industrial researchers,
chemical or research enterprises, or evidence-based decision making. However,
existing systems have limitations in categorizing and organizing paper insights
or relationships. Traditional databases are usually disjoint with logging
systems, which limit its utility in generating concise, collated overviews. In
this work, we briefly survey existing approaches of this problem space and
propose a unified framework that utilizes relational databases to log
hierarchical information to facilitate the research and writing process, or
generate useful knowledge from references or insights from connected concepts.
This framework of knowledge management system enables novel functionalities
encompassing improved hierarchical notetaking, AI-assisted brainstorming, and
multi-directional relationships. Potential applications include managing
inventories and changes for manufacture or research enterprises, or generating
analytic reports with evidence-based decision making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CMNEROne at SemEval-2022 Task 11: Code-Mixed Named Entity Recognition by leveraging multilingual data. (arXiv:2206.07318v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07318">
<div class="article-summary-box-inner">
<span><p>Identifying named entities is, in general, a practical and challenging task
in the field of Natural Language Processing. Named Entity Recognition on the
code-mixed text is further challenging due to the linguistic complexity
resulting from the nature of the mixing. This paper addresses the submission of
team CMNEROne to the SEMEVAL 2022 shared task 11 MultiCoNER. The Code-mixed NER
task aimed to identify named entities on the code-mixed dataset. Our work
consists of Named Entity Recognition (NER) on the code-mixed dataset by
leveraging the multilingual data. We achieved a weighted average F1 score of
0.7044, i.e., 6% greater than the baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey : Neural Networks for AMR-to-Text. (arXiv:2206.07328v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07328">
<div class="article-summary-box-inner">
<span><p>AMR-to-text is one of the key techniques in the NLP community that aims at
generating sentences from the Abstract Meaning Representation (AMR) graphs.
Since AMR was proposed in 2013, the study on AMR-to-Text has become
increasingly prevalent as an essential branch of structured data to text
because of the unique advantages of AMR as a high-level semantic description of
natural language. In this paper, we provide a brief survey of AMR-to-Text.
Firstly, we introduce the current scenario of this technique and point out its
difficulties. Secondly, based on the methods used in previous studies, we
roughly divided them into five categories according to their respective
mechanisms, i.e., Rules-based, Seq-to-Seq-based, Graph-to-Seq-based,
Transformer-based, and Pre-trained Language Model (PLM)-based. In particular,
we detail the neural network-based method and present the latest progress of
AMR-to-Text, which refers to AMR reconstruction, Decoder optimization, etc.
Furthermore, we present the benchmarks and evaluation methods of AMR-to-Text.
Eventually, we provide a summary of current techniques and the outlook for
future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Emotion is Not One-hot Encoding: Learning with Grayscale Label for Emotion Recognition in Conversation. (arXiv:2206.07359v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07359">
<div class="article-summary-box-inner">
<span><p>In emotion recognition in conversation (ERC), the emotion of the current
utterance is predicted by considering the previous context, which can be
utilized in many natural language processing tasks. Although multiple emotions
can coexist in a given sentence, most previous approaches take the perspective
of a classification task to predict only a given label. However, it is
expensive and difficult to label the emotion of a sentence with confidence or
multi-label. In this paper, we automatically construct a grayscale label
considering the correlation between emotions and use it for learning. That is,
instead of using a given label as a one-hot encoding, we construct a grayscale
label by measuring scores for different emotions. We introduce several methods
for constructing grayscale labels and confirm that each method improves the
emotion recognition performance. Our method is simple, effective, and
universally applicable to previous systems. The experiments show a significant
improvement in the performance of baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SciTweets -- A Dataset and Annotation Framework for Detecting Scientific Online Discourse. (arXiv:2206.07360v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07360">
<div class="article-summary-box-inner">
<span><p>Scientific topics, claims and resources are increasingly debated as part of
online discourse, where prominent examples include discourse related to
COVID-19 or climate change. This has led to both significant societal impact
and increased interest in scientific online discourse from various disciplines.
For instance, communication studies aim at a deeper understanding of biases,
quality or spreading pattern of scientific information whereas computational
methods have been proposed to extract, classify or verify scientific claims
using NLP and IR techniques. However, research across disciplines currently
suffers from both a lack of robust definitions of the various forms of
science-relatedness as well as appropriate ground truth data for distinguishing
them. In this work, we contribute (a) an annotation framework and corresponding
definitions for different forms of scientific relatedness of online discourse
in Tweets, (b) an expert-annotated dataset of 1261 tweets obtained through our
labeling framework reaching an average Fleiss Kappa $\kappa$ of 0.63, (c) a
multi-label classifier trained on our data able to detect science-relatedness
with 89% F1 and also able to detect distinct forms of scientific knowledge
(claims, references). With this work we aim to lay the foundation for
developing and evaluating robust methods for analysing science as part of
large-scale online discourse.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NatiQ: An End-to-end Text-to-Speech System for Arabic. (arXiv:2206.07373v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07373">
<div class="article-summary-box-inner">
<span><p>NatiQ is end-to-end text-to-speech system for Arabic. Our speech synthesizer
uses an encoder-decoder architecture with attention. We used both
tacotron-based models (tacotron-1 and tacotron-2) and the faster transformer
model for generating mel-spectrograms from characters. We concatenated
Tacotron1 with the WaveRNN vocoder, Tacotron2 with the WaveGlow vocoder and
ESPnet transformer with the parallel wavegan vocoder to synthesize waveforms
from the spectrograms. We used in-house speech data for two voices: 1) neutral
male "Hamza"- narrating general content and news, and 2) expressive female
"Amina"- narrating children story books to train our models. Our best systems
achieve an average Mean Opinion Score (MOS) of 4.21 and 4.40 for Amina and
Hamza respectively. The objective evaluation of the systems using word and
character error rate (WER and CER) as well as the response time measured by
real-time factor favored the end-to-end architecture ESPnet. NatiQ demo is
available on-line at https://tts.qcri.org
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Structure Search for Parameter-Efficient Tuning. (arXiv:2206.07382v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07382">
<div class="article-summary-box-inner">
<span><p>Adapting large pre-trained models (PTMs) through fine-tuning imposes
prohibitive computational and storage burdens. Recent studies of
parameter-efficient tuning (PET) find that only optimizing a small portion of
parameters conditioned on PTMs could yield on-par performance compared to
conventional fine-tuning. Generally, PET methods exquisitely design
parameter-efficient modules (PET modules) which could be applied to arbitrary
fine-grained positions inside PTMs. However, the effectiveness of these
fine-grained positions largely relies on sophisticated manual designation,
thereby usually producing sub-optimal results. In contrast to the manual
designation, we explore constructing PET modules in an automatic manner. We
automatically \textbf{S}earch for the \textbf{S}parse \textbf{S}tructure of
\textbf{P}arameter-\textbf{E}fficient \textbf{T}uning (S$^3$PET). Based on a
unified framework of various PET methods, S$^3$PET conducts the differentiable
PET structure search through bi-level optimization and proposes shifted global
sigmoid method to explicitly control the number of trainable parameters.
Extensive experiments show that S$^3$PET surpasses manual and random structures
with less trainable parameters. The searched structures preserve more than 99\%
fine-tuning performance with 0.01\% trainable parameters. Moreover, the
advantage of S$^3$PET is amplified with extremely low trainable parameters
budgets (0.0009\%$\sim$0.01\%). The searched structures are transferable and
explainable, providing suggestions and guidance for the future design of PET
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimating Confidence of Predictions of Individual Classifiers and Their Ensembles for the Genre Classification Task. (arXiv:2206.07427v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07427">
<div class="article-summary-box-inner">
<span><p>Genre identification is a subclass of non-topical text classification. The
main difference between this task and topical classification is that genres,
unlike topics, usually do not correspond to simple keywords, and thus they need
to be defined in terms of their functions in communication. Neural models based
on pre-trained transformers, such as BERT or XLM-RoBERTa, demonstrate SOTA
results in many NLP tasks, including non-topical classification. However, in
many cases, their downstream application to very large corpora, such as those
extracted from social media, can lead to unreliable results because of dataset
shifts, when some raw texts do not match the profile of the training set. To
mitigate this problem, we experiment with individual models as well as with
their ensembles. To evaluate the robustness of all models we use a prediction
confidence metric, which estimates the reliability of a prediction in the
absence of a gold standard label. We can evaluate robustness via the confidence
gap between the correctly classified texts and the misclassified ones on a
labeled test corpus, higher gaps make it easier to improve our confidence that
our classifier made the right decision. Our results show that for all of the
classifiers tested in this study, there is a confidence gap, but for the
ensembles, the gap is bigger, meaning that ensembles are more robust than their
individual models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BaIT: Barometer for Information Trustworthiness. (arXiv:2206.07535v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07535">
<div class="article-summary-box-inner">
<span><p>This paper presents a new approach to the FNC-1 fake news classification task
which involves employing pre-trained encoder models from similar NLP tasks,
namely sentence similarity and natural language inference, and two neural
network architectures using this approach are proposed. Methods in data
augmentation are explored as a means of tackling class imbalance in the
dataset, employing common pre-existing methods and proposing a method for
sample generation in the under-represented class using a novel sentence
negation algorithm. Comparable overall performance with existing baselines is
achieved, while significantly increasing accuracy on an under-represented but
nonetheless important class for FNC-1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MPI: Evaluating and Inducing Personality in Pre-trained Language Models. (arXiv:2206.07550v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07550">
<div class="article-summary-box-inner">
<span><p>Originated as a philosophical quest, personality discerns how individuals
differ from each other in terms of thinking, feeling, and behaving. Towards
building social machines that work with humans on a daily basis, we are
motivated to ask: (1) Do existing pre-trained language models possess
personality, akin to their human counterpart? If so, (2) how can we evaluate
them? Further, given this evaluation framework, (3) how can we induce a certain
personality in a fully controllable fashion? To tackle these three questions,
we propose the Machine Personality Inventory (MPI) dataset for evaluating the
machine personality; MPI follows standardized personality tests, built upon the
Big Five Personality Factors (Big Five) theory and personality assessment
inventories. By evaluating models with MPI, we provide the first piece of
evidence showing the existence of personality in pre-trained language models.
We further devise a Chain Prompting method to induce the language model with a
specific personality in a controllable manner, capable of producing diversified
behaviors. We hope to shed light on future studies by adopting personality as
the essential psychological guidance for various downstream tasks, building
more human-like and in situ dialogue agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KGEA: A Knowledge Graph Enhanced Article Quality Identification Dataset. (arXiv:2206.07556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07556">
<div class="article-summary-box-inner">
<span><p>With so many articles of varying quality being produced at every moment, it
is a very urgent task to screen this data for quality articles and commit them
out to social media. It is worth noting that high quality articles have many
characteristics, such as relevance, text quality, straightforward, multi-sided,
background, novelty and sentiment. Thus, it would be inadequate to purely use
the content of an article to identify its quality. Therefore, we plan to use
the external knowledge interaction to refine the performance and propose a
knowledge graph enhanced article quality identification dataset (KGEA) based on
Baidu Encyclopedia. We quantified the articles through 7 dimensions and use
co-occurrence of the entities between the articles and the Baidu encyclopedia
to construct the knowledge graph for every article. We also compared some text
classification baselines and found that external knowledge can guide the
articles to a more competitive classification with the graph neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextualization and Generalization in Entity and Relation Extraction. (arXiv:2206.07558v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07558">
<div class="article-summary-box-inner">
<span><p>During the past decade, neural networks have become prominent in Natural
Language Processing (NLP), notably for their capacity to learn relevant word
representations from large unlabeled corpora. These word embeddings can then be
transferred and finetuned for diverse end applications during a supervised
training phase. More recently, in 2018, the transfer of entire pretrained
Language Models and the preservation of their contextualization capacities
enabled to reach unprecedented performance on virtually every NLP benchmark,
sometimes even outperforming human baselines. However, as models reach such
impressive scores, their comprehension abilities still appear as shallow, which
reveal limitations of benchmarks to provide useful insights on their factors of
performance and to accurately measure understanding capabilities.
</p>
<p>In this thesis, we study the behaviour of state-of-the-art models regarding
generalization to facts unseen during training in two important Information
Extraction tasks: Named Entity Recognition (NER) and Relation Extraction (RE).
Indeed, traditional benchmarks present important lexical overlap between
mentions and relations used for training and evaluating models, whereas the
main interest of Information Extraction is to extract previously unknown
information. We propose empirical studies to separate performance based on
mention and relation overlap with the training set and find that pretrained
Language Models are mainly beneficial to detect unseen mentions, in particular
out-of-domain. While this makes them suited for real use cases, there is still
a gap in performance between seen and unseen mentions that hurts generalization
to new facts. In particular, even state-of-the-art ERE models rely on a shallow
retention heuristic, basing their prediction more on arguments surface forms
than context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AMR Alignment: Paying Attention to Cross-Attention. (arXiv:2206.07587v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07587">
<div class="article-summary-box-inner">
<span><p>With the surge of Transformer models, many have investigated how attention
acts on the learned representations. However, attention is still overlooked for
specific tasks, such as Semantic Parsing. A popular approach to the formal
representation of a sentence's meaning is Abstract Meaning Representation
(AMR). Until now, the alignment between a sentence and its AMR representation
has been explored in different ways, such as through rules or via the
Expectation Maximization (EM) algorithm. In this paper, we investigate the
ability of Transformer-based parsing models to yield effective alignments
without ad-hoc strategies. We present the first in-depth exploration of
cross-attention for AMR by proxy of alignment between the sentence spans and
the semantic units in the graph. We show how current Transformer-based parsers
implicitly encode the alignment information in the cross-attention weights and
how to leverage it to extract such alignment. Furthermore, we supervise and
guide cross-attention using alignment, dropping the need for English- and
AMR-specific rules.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HICEM: A High-Coverage Emotion Model for Artificial Emotional Intelligence. (arXiv:2206.07593v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07593">
<div class="article-summary-box-inner">
<span><p>As social robots and other intelligent machines enter the home, artificial
emotional intelligence (AEI) is taking center stage to address users' desire
for deeper, more meaningful human-machine interaction. To accomplish such
efficacious interaction, the next-generation AEI need comprehensive human
emotion models for training. Unlike theory of emotion, which has been the
historical focus in psychology, emotion models are a descriptive tools. In
practice, the strongest models need robust coverage, which means defining the
smallest core set of emotions from which all others can be derived. To achieve
the desired coverage, we turn to word embeddings from natural language
processing. Using unsupervised clustering techniques, our experiments show that
with as few as 15 discrete emotion categories, we can provide maximum coverage
across six major languages--Arabic, Chinese, English, French, Spanish, and
Russian. In support of our findings, we also examine annotations from two
large-scale emotion recognition datasets to assess the validity of existing
emotion models compared to human perception at scale. Because robust,
comprehensive emotion models are foundational for developing real-world
affective computing applications, this work has broad implications in social
robotics, human-machine interaction, mental healthcare, and computational
psychology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The SIGMORPHON 2022 Shared Task on Morpheme Segmentation. (arXiv:2206.07615v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07615">
<div class="article-summary-box-inner">
<span><p>The SIGMORPHON 2022 shared task on morpheme segmentation challenged systems
to decompose a word into a sequence of morphemes and covered most types of
morphology: compounds, derivations, and inflections. Subtask 1, word-level
morpheme segmentation, covered 5 million words in 9 languages (Czech, English,
Spanish, Hungarian, French, Italian, Russian, Latin, Mongolian) and received 13
system submissions from 7 teams and the best system averaged 97.29% F1 score
across all languages, ranging English (93.84%) to Latin (99.38%). Subtask 2,
sentence-level morpheme segmentation, covered 18,735 sentences in 3 languages
(Czech, English, Mongolian), received 10 system submissions from 3 teams, and
the best systems outperformed all three state-of-the-art subword tokenization
methods (BPE, ULM, Morfessor2) by 30.71% absolute. To facilitate error analysis
and support any type of future studies, we released all system predictions, the
evaluation script, and all gold standard datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Capabilities of Monolingual Audio Transformers using Large Datasets in Automatic Speech Recognition of Czech. (arXiv:2206.07627v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07627">
<div class="article-summary-box-inner">
<span><p>In this paper, we present our progress in pretraining Czech monolingual audio
transformers from a large dataset containing more than 80 thousand hours of
unlabeled speech, and subsequently fine-tuning the model on automatic speech
recognition tasks using a combination of in-domain data and almost 6 thousand
hours of out-of-domain transcribed speech. We are presenting a large palette of
experiments with various fine-tuning setups evaluated on two public datasets
(CommonVoice and VoxPopuli) and one extremely challenging dataset from the
MALACH project. Our results show that monolingual Wav2Vec 2.0 models are robust
ASR systems, which can take advantage of large labeled and unlabeled datasets
and successfully compete with state-of-the-art LVCSR systems. Moreover, Wav2Vec
models proved to be good zero-shot learners when no training data are available
for the target ASR task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone. (arXiv:2206.07643v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07643">
<div class="article-summary-box-inner">
<span><p>Vision-language (VL) pre-training has recently received considerable
attention. However, most existing end-to-end pre-training approaches either
only aim to tackle VL tasks such as image-text retrieval, visual question
answering (VQA) and image captioning that test high-level understanding of
images, or only target region-level understanding for tasks such as phrase
grounding and object detection. We present FIBER (Fusion-In-the-Backbone-based
transformER), a new VL model architecture that can seamlessly handle both these
types of tasks. Instead of having dedicated transformer layers for fusion after
the uni-modal backbones, FIBER pushes multimodal fusion deep into the model by
inserting cross-attention into the image and text backbones, bringing gains in
terms of memory and performance. In addition, unlike previous work that is
either only pre-trained on image-text data or on fine-grained data with
box-level annotations, we present a two-stage pre-training strategy that uses
both these kinds of data efficiently: (i) coarse-grained pre-training based on
image-text data; followed by (ii) fine-grained pre-training based on
image-text-box data. We conduct comprehensive experiments on a wide range of VL
tasks, ranging from VQA, image captioning, and retrieval, to phrase grounding,
referring expression comprehension, and object detection. Using deep multimodal
fusion coupled with the two-stage pre-training, FIBER provides consistent
performance improvements over strong baselines across all tasks, often
outperforming methods using magnitudes more data. Code is available at
https://github.com/microsoft/FIBER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-based Automatic Speech Recognition of Formal and Colloquial Czech in MALACH Project. (arXiv:2206.07666v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07666">
<div class="article-summary-box-inner">
<span><p>Czech is a very specific language due to its large differences between the
formal and the colloquial form of speech. While the formal (written) form is
used mainly in official documents, literature, and public speeches, the
colloquial (spoken) form is used widely among people in casual speeches. This
gap introduces serious problems for ASR systems, especially when training or
evaluating ASR models on datasets containing a lot of colloquial speech, such
as the MALACH project. In this paper, we are addressing this problem in the
light of a new paradigm in end-to-end ASR systems -- recently introduced
self-supervised audio Transformers. Specifically, we are investigating the
influence of colloquial speech on the performance of Wav2Vec 2.0 models and
their ability to transcribe colloquial speech directly into formal transcripts.
We are presenting results with both formal and colloquial forms in the training
transcripts, language models, and evaluation transcripts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Sequence Interface for Vision Tasks. (arXiv:2206.07669v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07669">
<div class="article-summary-box-inner">
<span><p>While language tasks are naturally expressed in a single, unified, modeling
framework, i.e., generating sequences of tokens, this has not been the case in
computer vision. As a result, there is a proliferation of distinct
architectures and loss functions for different vision tasks. In this work we
show that a diverse set of "core" computer vision tasks can also be unified if
formulated in terms of a shared pixel-to-sequence interface. We focus on four
tasks, namely, object detection, instance segmentation, keypoint detection, and
image captioning, all with diverse types of outputs, e.g., bounding boxes or
dense masks. Despite that, by formulating the output of each task as a sequence
of discrete tokens with a unified interface, we show that one can train a
neural network with a single model architecture and loss function on all these
tasks, with no task-specific customization. To solve a specific task, we use a
short prompt as task description, and the sequence output adapts to the prompt
so it can produce task-specific output. We show that such a model can achieve
competitive performance compared to well-established task-specific models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent Abilities of Large Language Models. (arXiv:2206.07682v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07682">
<div class="article-summary-box-inner">
<span><p>Scaling up language models has been shown to predictably improve performance
and sample efficiency on a wide range of downstream tasks. This paper instead
discusses an unpredictable phenomenon that we refer to as emergent abilities of
large language models. We consider an ability to be emergent if it is not
present in smaller models but is present in larger models. Thus, emergent
abilities cannot be predicted simply by extrapolating the performance of
smaller models. The existence of such emergence implies that additional scaling
could further expand the range of capabilities of language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DIRECTOR: Generator-Classifiers For Supervised Language Modeling. (arXiv:2206.07694v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07694">
<div class="article-summary-box-inner">
<span><p>Current language models achieve low perplexity but their resulting
generations still suffer from toxic responses, repetitiveness and
contradictions. The standard language modeling setup fails to address these
issues. In this paper, we introduce a new architecture, {\sc Director}, that
consists of a unified generator-classifier with both a language modeling and a
classification head for each output token. Training is conducted jointly using
both standard language modeling data, and data labeled with desirable and
undesirable sequences. Experiments in several settings show that the model has
competitive training and decoding speed compared to standard language models
while yielding superior results, alleviating known issues while maintaining
generation quality. It also outperforms existing model guiding approaches in
terms of both accuracy and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prefix Language Models are Unified Modal Learners. (arXiv:2206.07699v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07699">
<div class="article-summary-box-inner">
<span><p>With the success of vision-language pre-training, we have witnessed the
state-of-the-art has been pushed on multi-modal understanding and generation.
However, the current pre-training paradigm is either incapable of targeting all
modalities at once (e.g., text generation and image generation), or requires
multi-fold well-designed tasks which significantly limits the scalability. We
demonstrate that a unified modal model could be learned with a prefix language
modeling objective upon text and image sequences. Thanks to the simple but
powerful pre-training paradigm, our proposed model, DaVinci, is simple to
train, scalable to huge data, and adaptable to a variety of downstream tasks
across modalities (language / vision / vision+language), types (understanding /
generation) and settings (e.g., zero-shot, fine-tuning, linear evaluation) with
a single unified architecture. DaVinci achieves the competitive performance on
a wide range of 26 understanding / generation tasks, and outperforms previous
unified vision-language models on most tasks, including ImageNet classification
(+1.6%), VQAv2 (+1.4%), COCO caption generation (BLEU@4 +1.1%, CIDEr +1.5%) and
COCO image generation (IS +0.9%, FID -1.0%), at the comparable model and data
scale. Furthermore, we offer a well-defined benchmark for future research by
reporting the performance on different scales of the pre-training dataset on a
heterogeneous and wide distribution coverage. Our results establish new,
stronger baselines for future comparisons at different data scales and shed
light on the difficulties of comparing VLP models more generally.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adposition and Case Supersenses v2.6: Guidelines for English. (arXiv:1704.02134v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1704.02134">
<div class="article-summary-box-inner">
<span><p>This document offers a detailed linguistic description of SNACS (Semantic
Network of Adposition and Case Supersenses; Schneider et al., 2018), an
inventory of 52 semantic labels ("supersenses") that characterize the use of
adpositions and case markers at a somewhat coarse level of granularity, as
demonstrated in the STREUSLE corpus (https://github.com/nert-nlp/streusle/;
version 4.5 tracks guidelines version 2.6). Though the SNACS inventory aspires
to be universal, this document is specific to English; documentation for other
languages will be published separately.
</p>
<p>Version 2 is a revision of the supersense inventory proposed for English by
Schneider et al. (2015, 2016) (henceforth "v1"), which in turn was based on
previous schemes. The present inventory was developed after extensive review of
the v1 corpus annotations for English, plus previously unanalyzed genitive case
possessives (Blodgett and Schneider, 2018), as well as consideration of
adposition and case phenomena in Hebrew, Hindi, Korean, and German. Hwang et
al. (2017) present the theoretical underpinnings of the v2 scheme. Schneider et
al. (2018) summarize the scheme, its application to English corpus data, and an
automatic disambiguation task. Liu et al. (2021) offer an English Lexical
Semantic Recognition tagger that includes SNACS labels in its output.
</p>
<p>This documentation can also be browsed alongside corpus data on the Xposition
website (Gessler et al., 2022): <a href="http://www.xposition.org/">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Re-evaluating Word Mover's Distance. (arXiv:2105.14403v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14403">
<div class="article-summary-box-inner">
<span><p>The word mover's distance (WMD) is a fundamental technique for measuring the
similarity of two documents. As the crux of WMD, it can take advantage of the
underlying geometry of the word space by employing an optimal transport
formulation. The original study on WMD reported that WMD outperforms classical
baselines such as bag-of-words (BOW) and TF-IDF by significant margins in
various datasets. In this paper, we point out that the evaluation in the
original study could be misleading. We re-evaluate the performances of WMD and
the classical baselines and find that the classical baselines are competitive
with WMD if we employ an appropriate preprocessing, i.e., L1 normalization. In
addition, we introduce an analogy between WMD and L1-normalized BOW and find
that not only the performance of WMD but also the distance values resemble
those of BOW in high dimensional spaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Enhancing Multi-filter Sequence-to-Sequence Model. (arXiv:2109.12399v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12399">
<div class="article-summary-box-inner">
<span><p>In sequence-to-sequence tasks, sentences with heterogeneous semantics or
grammatical structures may lead to some difficulties in the model's
convergence. To resolve this problem, we introduce a feature-concentrated model
that focuses on each of the heterogeneous features in the input-output
sequences. Building upon the encoder-decoder architecture, we design a
latent-enhanced multi-filter sequence-to-sequence model (LMS2S) that analyzes
the features preserved by latent space representations and constructs the
outputs accordingly. We divide the latent space into subspaces using a
clustering algorithm and train a set of decoders in which each decoder only
concentrates on the features from its corresponding subspace. We then design a
self-enhancing mechanism that uses reinforcement learning to optimize the
clustering algorithm. We perform two sets of experiments on semantic parsing
and machine translation. We empirically demonstrate the advantages of the
multi-filter architecture and show the performance improvement made by the
self-enhancing mechanism.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Dataset Difficulty with $\mathcal{V}$-Usable Information. (arXiv:2110.08420v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08420">
<div class="article-summary-box-inner">
<span><p>Estimating the difficulty of a dataset typically involves comparing
state-of-the-art models to humans; the bigger the performance gap, the harder
the dataset is said to be. However, this comparison provides little
understanding of how difficult each instance in a given distribution is, or
what attributes make the dataset difficult for a given model. To address these
questions, we frame dataset difficulty -- w.r.t. a model $\mathcal{V}$ -- as
the lack of $\mathcal{V}$-$\textit{usable information}$ (Xu et al., 2019),
where a lower value indicates a more difficult dataset for $\mathcal{V}$. We
further introduce $\textit{pointwise $\mathcal{V}$-information}$ (PVI) for
measuring the difficulty of individual instances w.r.t. a given distribution.
While standard evaluation metrics typically only compare different models for
the same dataset, $\mathcal{V}$-$\textit{usable information}$ and PVI also
permit the converse: for a given model $\mathcal{V}$, we can compare different
datasets, as well as different instances/slices of the same dataset.
Furthermore, our framework allows for the interpretability of different input
attributes via transformations of the input, which we use to discover
annotation artefacts in widely-used NLP benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Private Language Model Adaptation for Speech Recognition. (arXiv:2110.10026v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10026">
<div class="article-summary-box-inner">
<span><p>Speech model adaptation is crucial to handle the discrepancy between
server-side proxy training data and actual data received on local devices of
users. With the use of federated learning (FL), we introduce an efficient
approach on continuously adapting neural network language models (NNLMs) on
private devices with applications on automatic speech recognition (ASR). To
address the potential speech transcription errors in the on-device training
corpus, we perform empirical studies on comparing various strategies of
leveraging token-level confidence scores to improve the NNLM quality in the FL
settings. Experiments show that compared with no model adaptation, the proposed
method achieves relative 2.6% and 10.8% word error rate (WER) reductions on two
speech evaluation datasets, respectively. We also provide analysis in
evaluating privacy guarantees of our presented procedure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models are not Models of Natural Language: they are Corpus Models. (arXiv:2112.07055v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07055">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing (NLP) has become one of the leading application
areas in the current Artificial Intelligence boom. Transfer learning has
enabled large deep learning neural networks trained on the language modeling
task to vastly improve performance in almost all downstream language tasks.
Interestingly, when the language models are trained with data that includes
software code, they demonstrate remarkable abilities in generating functioning
computer code from natural language specifications. We argue that this creates
a conundrum for the claim that eliminative neural models are a radical
restructuring in our understanding of cognition in that they eliminate the need
for symbolic abstractions like generative phrase structure grammars. Because
the syntax of programming languages is by design determined by phrase structure
grammars, neural models that produce syntactic code are apparently
uninformative about the theoretical foundations of programming languages. The
demonstration that neural models perform well on tasks that involve clearly
symbolic systems, proves that they cannot be used as an argument that language
and other cognitive systems are not symbolic. Finally, we argue as a corollary
that the term language model is misleading and propose the adoption of the
working term corpus model instead, which better reflects the genesis and
contents of the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperPrompt: Prompt-based Task-Conditioning of Transformers. (arXiv:2203.00759v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00759">
<div class="article-summary-box-inner">
<span><p>Prompt-Tuning is a new paradigm for finetuning pre-trained language models in
a parameter-efficient way. Here, we explore the use of HyperNetworks to
generate hyper-prompts: we propose HyperPrompt, a novel architecture for
prompt-based task-conditioning of self-attention in Transformers. The
hyper-prompts are end-to-end learnable via generation by a HyperNetwork.
HyperPrompt allows the network to learn task-specific feature maps where the
hyper-prompts serve as task global memories for the queries to attend to, at
the same time enabling flexible information sharing among tasks. We show that
HyperPrompt is competitive against strong multi-task learning baselines with as
few as $0.14\%$ of additional task-conditioning parameters, achieving great
parameter and computational efficiency. Through extensive empirical
experiments, we demonstrate that HyperPrompt can achieve superior performances
over strong T5 multi-task learning baselines and parameter-efficient adapter
variants including Prompt-Tuning and HyperFormer++ on Natural Language
Understanding benchmarks of GLUE and SuperGLUE across many model sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence-Select: Large-Scale Language Model Data Selection for Rare-Word Speech Recognition. (arXiv:2203.05008v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05008">
<div class="article-summary-box-inner">
<span><p>Language model fusion helps smart assistants recognize words which are rare
in acoustic data but abundant in text-only corpora (typed search logs).
However, such corpora have properties that hinder downstream performance,
including being (1) too large, (2) beset with domain-mismatched content, and
(3) heavy-headed rather than heavy-tailed (excessively many duplicate search
queries such as "weather"). We show that three simple strategies for selecting
language modeling data can dramatically improve rare-word recognition without
harming overall performance. First, to address the heavy-headedness, we
downsample the data according to a soft log function, which tunably reduces
high frequency (head) sentences. Second, to encourage rare-word exposure, we
explicitly filter for words rare in the acoustic data. Finally, we tackle
domain-mismatch via perplexity-based contrastive selection, filtering for
examples matched to the target domain. We down-select a large corpus of web
search queries by a factor of 53x and achieve better LM perplexities than
without down-selection. When shallow-fused with a state-of-the-art, production
speech engine, our LM achieves WER reductions of up to 24% relative on
rare-word sentences (without changing overall WER) compared to a baseline LM
trained on the raw corpus. These gains are further validated through favorable
side-by-side evaluations on live voice search traffic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain-based Discriminative Autoencoders for Speech Recognition. (arXiv:2203.13687v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13687">
<div class="article-summary-box-inner">
<span><p>In our previous work, we proposed a discriminative autoencoder (DcAE) for
speech recognition. DcAE combines two training schemes into one. First, since
DcAE aims to learn encoder-decoder mappings, the squared error between the
reconstructed speech and the input speech is minimized. Second, in the code
layer, frame-based phonetic embeddings are obtained by minimizing the
categorical cross-entropy between ground truth labels and predicted
triphone-state scores. DcAE is developed based on the Kaldi toolkit by treating
various TDNN models as encoders. In this paper, we further propose three new
versions of DcAE. First, a new objective function that considers both
categorical cross-entropy and mutual information between ground truth and
predicted triphone-state sequences is used. The resulting DcAE is called a
chain-based DcAE (c-DcAE). For application to robust speech recognition, we
further extend c-DcAE to hierarchical and parallel structures, resulting in
hc-DcAE and pc-DcAE. In these two models, both the error between the
reconstructed noisy speech and the input noisy speech and the error between the
enhanced speech and the reference clean speech are taken into the objective
function. Experimental results on the WSJ and Aurora-4 corpora show that our
DcAE models outperform baseline systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shifted Chunk Encoder for Transformer Based Streaming End-to-End ASR. (arXiv:2203.15206v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15206">
<div class="article-summary-box-inner">
<span><p>Currently, there are mainly three Transformer encoder based streaming End to
End (E2E) Automatic Speech Recognition (ASR) approaches, namely time-restricted
methods, chunk-wise methods, and memory based methods. However, all of them
have some limitations in aspects of global context modeling, linear
computational complexity, and model parallelism. In this work, we aim to build
a single model to achieve the benefits of all the three aspects for streaming
E2E ASR. Particularly, we propose to use a shifted chunk mechanism instead of
the conventional chunk mechanism for streaming Transformer and Conformer. This
shifted chunk mechanism can significantly enhance modeling power through
allowing chunk self-attention to capture global context across local chunks,
while keeping linear computational complexity and parallel trainable. We name
the Shifted Chunk Transformer and Conformer as SChunk-Transofromer and
SChunk-Conformer, respectively. And we verify their performance on the widely
used AISHELL-1 benckmark. Experiments show that the SChunk-Transformer and
SChunk-Conformer achieve CER 6.43% and 5.77%, respectively. That surpasses the
existing chunk-wise and memory based methods by a large margin, and is
competitive even compared with the state-of-the-art time-restricted methods
which have quadratic computational complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vakyansh: ASR Toolkit for Low Resource Indic languages. (arXiv:2203.16512v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16512">
<div class="article-summary-box-inner">
<span><p>We present Vakyansh, an end to end toolkit for Speech Recognition in Indic
languages. India is home to almost 121 languages and around 125 crore speakers.
Yet most of the languages are low resource in terms of data and pretrained
models. Through Vakyansh, we introduce automatic data pipelines for data
creation, model training, model evaluation and deployment. We create 14,000
hours of speech data in 23 Indic languages and train wav2vec 2.0 based
pretrained models. These pretrained models are then finetuned to create state
of the art speech recognition models for 18 Indic languages which are followed
by language models and punctuation restoration models. We open source all these
resources with a mission that this will inspire the speech community to develop
speech first applications using our ASR models in Indic languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Speech Recognition for Indic Languages using Language Model. (arXiv:2203.16595v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16595">
<div class="article-summary-box-inner">
<span><p>We study the effect of applying a language model (LM) on the output of
Automatic Speech Recognition (ASR) systems for Indic languages. We fine-tune
wav2vec $2.0$ models for $18$ Indic languages and adjust the results with
language models trained on text derived from a variety of sources. Our findings
demonstrate that the average Character Error Rate (CER) decreases by over $28$
\% and the average Word Error Rate (WER) decreases by about $36$ \% after
decoding with LM. We show that a large LM may not provide a substantial
improvement as compared to a diverse one. We also demonstrate that high quality
transcriptions can be obtained on domain-specific data without retraining the
ASR model and show results on biomedical domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Word Error Rate a good evaluation metric for Speech Recognition in Indic Languages?. (arXiv:2203.16601v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16601">
<div class="article-summary-box-inner">
<span><p>We propose a new method for the calculation of error rates in Automatic
Speech Recognition (ASR). This new metric is for languages that contain half
characters and where the same character can be written in different forms. We
implement our methodology in Hindi which is one of the main languages from
Indic context and we think this approach is scalable to other similar languages
containing a large character set. We call our metrics Alternate Word Error Rate
(AWER) and Alternate Character Error Rate (ACER).
</p>
<p>We train our ASR models using wav2vec 2.0\cite{baevski2020wav2vec} for Indic
languages. Additionally we use language models to improve our model
performance. Our results show a significant improvement in analyzing the error
rates at word and character level and the interpretability of the ASR system is
improved upto $3$\% in AWER and $7$\% in ACER for Hindi. Our experiments
suggest that in languages which have complex pronunciation, there are multiple
ways of writing words without changing their meaning. In such cases AWER and
ACER will be more useful rather than WER and CER as metrics. Further, we open
source a new benchmarking dataset of 21 hours for Hindi with the new metric
scripts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards End-to-end Unsupervised Speech Recognition. (arXiv:2204.02492v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02492">
<div class="article-summary-box-inner">
<span><p>Unsupervised speech recognition has shown great potential to make Automatic
Speech Recognition (ASR) systems accessible to every language. However,
existing methods still heavily rely on hand-crafted pre-processing. Similar to
the trend of making supervised speech recognition end-to-end, we introduce
wav2vec-U 2.0 which does away with all audio-side pre-processing and improves
accuracy through better architecture. In addition, we introduce an auxiliary
self-supervised objective that ties model predictions back to the input.
Experiments show that wav2vec-U 2.0 improves unsupervised recognition results
across different languages while being conceptually simpler.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linear Complexity Randomized Self-attention Mechanism. (arXiv:2204.04667v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04667">
<div class="article-summary-box-inner">
<span><p>Recently, random feature attentions (RFAs) are proposed to approximate the
softmax attention in linear time and space complexity by linearizing the
exponential kernel. In this paper, we first propose a novel perspective to
understand the bias in such approximation by recasting RFAs as self-normalized
importance samplers. This perspective further sheds light on an \emph{unbiased}
estimator for the whole softmax attention, called randomized attention (RA). RA
constructs positive random features via query-specific distributions and enjoys
greatly improved approximation fidelity, albeit exhibiting quadratic
complexity. By combining the expressiveness in RA and the efficiency in RFA, we
develop a novel linear complexity self-attention mechanism called linear
randomized attention (LARA). Extensive experiments across various domains
demonstrate that RA and LARA significantly improve the performance of RFAs by a
substantial margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Handling sign language transcription system with the computer-friendly numerical multilabels. (arXiv:2204.06924v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06924">
<div class="article-summary-box-inner">
<span><p>This paper presents our recent developments in the field of automatic
processing of sign language corpora using the Hamburg Sign Language Annotation
System (HamNoSys). We designed an automated tool to convert HamNoSys
annotations into numerical labels for defined initial features of body and hand
positions. Our proposed numerical multilabels greatly simplify the structure of
HamNoSys annotation without significant loss of gloss meaning. These numerical
multilabels can potentially be used to feed the machine learning models, which
would accelerate the development of vision-based sign language recognition. In
addition, this tool can assist experts in the annotation process to help
identify semantic errors. The code and sample annotations are publicly
available at https://github.com/hearai/parse-hamnosys.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UTNLP at SemEval-2022 Task 6: A Comparative Analysis of Sarcasm Detection using generative-based and mutation-based data augmentation. (arXiv:2204.08198v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08198">
<div class="article-summary-box-inner">
<span><p>Sarcasm is a term that refers to the use of words to mock, irritate, or amuse
someone. It is commonly used on social media. The metaphorical and creative
nature of sarcasm presents a significant difficulty for sentiment analysis
systems based on affective computing. The methodology and results of our team,
UTNLP, in the SemEval-2022 shared task 6 on sarcasm detection are presented in
this paper. We put different models, and data augmentation approaches to the
test and report on which one works best. The tests begin with traditional
machine learning models and progress to transformer-based and attention-based
models. We employed data augmentation based on data mutation and data
generation. Using RoBERTa and mutation-based data augmentation, our best
approach achieved an F1-sarcastic of 0.38 in the competition's evaluation
phase. After the competition, we fixed our model's flaws and achieved an
F1-sarcastic of 0.414.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E2E Segmenter: Joint Segmenting and Decoding for Long-Form ASR. (arXiv:2204.10749v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10749">
<div class="article-summary-box-inner">
<span><p>Improving the performance of end-to-end ASR models on long utterances ranging
from minutes to hours in length is an ongoing challenge in speech recognition.
A common solution is to segment the audio in advance using a separate voice
activity detector (VAD) that decides segment boundary locations based purely on
acoustic speech/non-speech information. VAD segmenters, however, may be
sub-optimal for real-world speech where, e.g., a complete sentence that should
be taken as a whole may contain hesitations in the middle ("set an alarm for...
5 o'clock").
</p>
<p>We propose to replace the VAD with an end-to-end ASR model capable of
predicting segment boundaries in a streaming fashion, allowing the segmentation
decision to be conditioned not only on better acoustic features but also on
semantic features from the decoded text with negligible extra computation. In
experiments on real world long-form audio (YouTube) with lengths of up to 30
minutes, we demonstrate 8.5% relative WER improvement and 250 ms reduction in
median end-of-segment latency compared to the VAD segmenter baseline on a
state-of-the-art Conformer RNN-T model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Speech Representation Learning: A Review. (arXiv:2205.10643v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10643">
<div class="article-summary-box-inner">
<span><p>Although supervised deep learning has revolutionized speech and audio
processing, it has necessitated the building of specialist models for
individual tasks and application scenarios. It is likewise difficult to apply
this to dialects and languages for which only limited labeled data is
available. Self-supervised representation learning methods promise a single
universal model that would benefit a wide variety of tasks and domains. Such
methods have shown success in natural language processing and computer vision
domains, achieving new levels of performance while reducing the number of
labels required for many downstream scenarios. Speech representation learning
is experiencing similar progress in three main categories: generative,
contrastive, and predictive methods. Other approaches rely on multi-modal data
for pre-training, mixing text or visual data streams with speech. Although
self-supervised speech representation is still a nascent research area, it is
closely related to acoustic word embedding and learning with zero lexical
resources, both of which have seen active research for many years. This review
presents approaches for self-supervised speech representation learning and
their connection to other research areas. Since many current methods focus
solely on automatic speech recognition as a downstream task, we review recent
efforts on benchmarking learned representations to extend the application
beyond speech recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">History Compression via Language Models in Reinforcement Learning. (arXiv:2205.12258v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12258">
<div class="article-summary-box-inner">
<span><p>In a partially observable Markov decision process (POMDP), an agent typically
uses a representation of the past to approximate the underlying MDP. We propose
to utilize a frozen Pretrained Language Transformer (PLT) for history
representation and compression to improve sample efficiency. To avoid training
of the Transformer, we introduce FrozenHopfield, which automatically associates
observations with pretrained token embeddings. To form these associations, a
modern Hopfield network stores these token embeddings, which are retrieved by
queries that are obtained by a random but fixed projection of observations. Our
new method, HELM, enables actor-critic network architectures that contain a
pretrained language Transformer for history representation as a memory module.
Since a representation of the past need not be learned, HELM is much more
sample efficient than competitors. On Minigrid and Procgen environments HELM
achieves new state-of-the-art results. Our code is available at
https://github.com/ml-jku/helm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Dense Graph Do You Need for Self-Attention?. (arXiv:2205.14014v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14014">
<div class="article-summary-box-inner">
<span><p>Transformers have made progress in miscellaneous tasks, but suffer from
quadratic computational and memory complexities. Recent works propose sparse
Transformers with attention on sparse graphs to reduce complexity and remain
strong performance. While effective, the crucial parts of how dense a graph
needs to be to perform well are not fully explored. In this paper, we propose
Normalized Information Payload (NIP), a graph scoring function measuring
information transfer on graph, which provides an analysis tool for trade-offs
between performance and complexity. Guided by this theoretical analysis, we
present Hypercube Transformer, a sparse Transformer that models token
interactions in a hypercube and shows comparable or even better results with
vanilla Transformer while yielding $O(N\log N)$ complexity with sequence length
$N$. Experiments on tasks requiring various sequence lengths lay validation for
our graph function well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval. (arXiv:2206.02873v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02873">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that small distilled language models are strong
competitors to models that are orders of magnitude larger and slower in a wide
range of information retrieval tasks. This has made distilled and dense models,
due to latency constraints, the go-to choice for deployment in real-world
retrieval applications. In this work, we question this practice by showing that
the number of parameters and early query-document interaction play a
significant role in the generalization ability of retrieval models. Our
experiments show that increasing model size results in marginal gains on
in-domain test sets, but much larger gains in new domains never seen during
fine-tuning. Furthermore, we show that rerankers largely outperform dense ones
of similar size in several tasks. Our largest reranker reaches the state of the
art in 12 of the 18 datasets of the Benchmark-IR (BEIR) and surpasses the
previous state of the art by 3 average points. Finally, we confirm that
in-domain effectiveness is not a good indicator of zero-shot effectiveness.
Code is available at
https://github.com/guilhermemr04/scaling-zero-shot-retrieval.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CitySpec: An Intelligent Assistant System for Requirement Specification in Smart Cities. (arXiv:2206.03132v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03132">
<div class="article-summary-box-inner">
<span><p>An increasing number of monitoring systems have been developed in smart
cities to ensure that real-time operations of a city satisfy safety and
performance requirements. However, many existing city requirements are written
in English with missing, inaccurate, or ambiguous information. There is a high
demand for assisting city policy makers in converting human-specified
requirements to machine-understandable formal specifications for monitoring
systems. To tackle this limitation, we build CitySpec, the first intelligent
assistant system for requirement specification in smart cities. To create
CitySpec, we first collect over 1,500 real-world city requirements across
different domains from over 100 cities and extract city-specific knowledge to
generate a dataset of city vocabulary with 3,061 words. We also build a
translation model and enhance it through requirement synthesis and develop a
novel online learning framework with validation under uncertainty. The
evaluation results on real-world city requirements show that CitySpec increases
the sentence-level accuracy of requirement specification from 59.02% to 86.64%,
and has strong adaptability to a new city and a new domain (e.g., F1 score for
requirements in Seattle increases from 77.6% to 93.75% with online learning).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Words are all you need? Capturing human sensory similarity with textual descriptors. (arXiv:2206.04105v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04105">
<div class="article-summary-box-inner">
<span><p>Recent advances in multimodal training use textual descriptions to
significantly enhance machine understanding of images and videos. Yet, it
remains unclear to what extent language can fully capture sensory experiences
across different modalities. A well-established approach for characterizing
sensory experiences relies on similarity judgments, namely, the degree to which
people perceive two distinct stimuli as similar. We explore the relation
between human similarity judgments and language in a series of large-scale
behavioral studies ($N=1,823$ participants) across three modalities (images,
audio, and video) and two types of text descriptors: simple word tags and
free-text captions. In doing so, we introduce a novel adaptive pipeline for tag
mining that is both efficient and domain-general. We show that our prediction
pipeline based on text descriptors exhibits excellent performance, and we
compare it against a comprehensive array of 611 baseline models based on
vision-, audio-, and video-processing architectures. We further show that the
degree to which textual descriptors and models predict human similarity varies
across and within modalities. Taken together, these studies illustrate the
value of integrating machine learning and cognitive science approaches to
better understand the similarities and differences between human and machine
representations. We present an interactive visualization at
https://words-are-all-you-need.s3.amazonaws.com/index.html for exploring the
similarity between stimuli as experienced by humans and different methods
reported in the paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Chinese Dialect TTS Frontend with Non-Autoregressive Neural Machine Translation. (arXiv:2206.04922v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04922">
<div class="article-summary-box-inner">
<span><p>Chinese dialect text-to-speech(TTS) system usually can only be utilized by
native linguists, because the written form of Chinese dialects has different
characters, idioms, grammar and usage from Mandarin, and even the local speaker
cannot input a correct sentence. For Mandarin text inputs, Chinese dialect TTS
can only generate partly-meaningful speech with relatively poor prosody and
naturalness. To lower the bar of use and make it more practical in commercial,
we propose a novel Chinese dialect TTS frontend with a translation module. It
helps to convert Mandarin text into idiomatic expressions with correct
orthography and grammar, so that the intelligibility and naturalness of the
synthesized speech can be improved. A non-autoregressive neural machine
translation model with a glancing sampling strategy is proposed for the
translation task. It is the first known work to incorporate translation with
TTS frontend. Our experiments on Cantonese approve that the proposed frontend
can help Cantonese TTS system achieve a 0.27 improvement in MOS with Mandarin
inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks. (arXiv:2206.06565v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06565">
<div class="article-summary-box-inner">
<span><p>Fine-tuning pretrained language models (LMs) without making any architectural
changes has become a norm for learning various language downstream tasks.
However, for non-language downstream tasks, a common practice is to employ
task-specific designs for input, output layers, and loss functions. For
instance, it is possible to fine-tune an LM into an MNIST classifier by
replacing the word embedding layer with an image patch embedding layer, the
word token output layer with a 10-way output layer, and the word prediction
loss with a 10-way classification loss, respectively. A natural question
arises: can LM fine-tuning solve non-language downstream tasks without changing
the model architecture or loss function? To answer this, we propose
Language-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations
by conducting an extensive empirical study on a suite of non-language
classification and regression tasks. LIFT does not make any changes to the
model architecture or loss function, and it solely relies on the natural
language interface, enabling "no-code machine learning with LMs." We find that
LIFT performs relatively well across a wide range of low-dimensional
classification and regression tasks, matching the performances of the best
baselines in many cases, especially for the classification tasks. We report the
experimental results on the fundamental properties of LIFT, including its
inductive bias, sample efficiency, ability to extrapolate, robustness to
outliers and label noise, and generalization. We also analyze a few
properties/techniques specific to LIFT, e.g., context-aware learning via
appropriate prompting, quantification of predictive uncertainty, and two-stage
fine-tuning. Our code is available at
https://github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CHQ-Summ: A Dataset for Consumer Healthcare Question Summarization. (arXiv:2206.06581v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06581">
<div class="article-summary-box-inner">
<span><p>The quest for seeking health information has swamped the web with consumers'
health-related questions. Generally, consumers use overly descriptive and
peripheral information to express their medical condition or other healthcare
needs, contributing to the challenges of natural language understanding. One
way to address this challenge is to summarize the questions and distill the key
information of the original question. To address this issue, we introduce a new
dataset, CHQ-Summ that contains 1507 domain-expert annotated consumer health
questions and corresponding summaries. The dataset is derived from the
community question-answering forum and therefore provides a valuable resource
for understanding consumer health-related posts on social media. We benchmark
the dataset on multiple state-of-the-art summarization models to show the
effectiveness of the dataset.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Applications of Generative Adversarial Networks in Neuroimaging and Clinical Neuroscience. (arXiv:2206.07081v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07081">
<div class="article-summary-box-inner">
<span><p>Generative adversarial networks (GANs) are one powerful type of deep learning
models that have been successfully utilized in numerous fields. They belong to
a broader family called generative methods, which generate new data with a
probabilistic model by learning sample distribution from real examples. In the
clinical context, GANs have shown enhanced capabilities in capturing spatially
complex, nonlinear, and potentially subtle disease effects compared to
traditional generative methods. This review appraises the existing literature
on the applications of GANs in imaging studies of various neurological
conditions, including Alzheimer's disease, brain tumors, brain aging, and
multiple sclerosis. We provide an intuitive explanation of various GAN methods
for each application and further discuss the main challenges, open questions,
and promising future directions of leveraging GANs in neuroimaging. We aim to
bridge the gap between advanced deep learning methods and neurology research by
highlighting how GANs can be leveraged to support clinical decision making and
contribute to a better understanding of the structural and functional patterns
of brain diseases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TriHorn-Net: A Model for Accurate Depth-Based 3D Hand Pose Estimation. (arXiv:2206.07117v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07117">
<div class="article-summary-box-inner">
<span><p>3D hand pose estimation methods have made significant progress recently.
However, estimation accuracy is often far from sufficient for specific
real-world applications, and thus there is significant room for improvement.
This paper proposes TriHorn-Net, a novel model that uses specific innovations
to improve hand pose estimation accuracy on depth images. The first innovation
is the decomposition of the 3D hand pose estimation into the estimation of 2D
joint locations in the depth image space (UV), and the estimation of their
corresponding depths aided by two complementary attention maps. This
decomposition prevents depth estimation, which is a more difficult task, from
interfering with the UV estimations at both the prediction and feature levels.
The second innovation is PixDropout, which is, to the best of our knowledge,
the first appearance-based data augmentation method for hand depth images.
Experimental results demonstrate that the proposed model outperforms the
state-of-the-art methods on three public benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Loss Functions for Classification using Structured Entropy. (arXiv:2206.07122v1 [stat.ML])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07122">
<div class="article-summary-box-inner">
<span><p>Cross-entropy loss is the standard metric used to train classification models
in deep learning and gradient boosting. It is well-known that this loss
function fails to account for similarities between the different values of the
target. We propose a generalization of entropy called {\em structured entropy}
which uses a random partition to incorporate the structure of the target
variable in a manner which retains many theoretical properties of standard
entropy. We show that a structured cross-entropy loss yields better results on
several classification problems where the target variable has an a priori known
structure. The approach is simple, flexible, easily computable, and does not
rely on a hierarchically defined notion of structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Pretraining for Differentially Private Learning. (arXiv:2206.07125v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07125">
<div class="article-summary-box-inner">
<span><p>We demonstrate self-supervised pretraining (SSP) is a scalable solution to
deep learning with differential privacy (DP) regardless of the size of
available public datasets in image classification. When facing the lack of
public datasets, we show the features generated by SSP on only one single image
enable a private classifier to obtain much better utility than the non-learned
handcrafted features under the same privacy budget. When a moderate or large
size public dataset is available, the features produced by SSP greatly
outperform the features trained with labels on various complex private datasets
under the same private budget. We also compared multiple DP-enabled training
frameworks to train a private classifier on the features generated by SSP.
Finally, we report a non-trivial utility 25.3\% of a private ImageNet-1K
dataset when $\epsilon=3$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Clipping: Differentially Private Deep Learning Made Easier and Stronger. (arXiv:2206.07136v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07136">
<div class="article-summary-box-inner">
<span><p>Per-example gradient clipping is a key algorithmic step that enables
practical differential private (DP) training for deep learning models. The
choice of clipping norm $R$, however, is shown to be vital for achieving high
accuracy under DP. We propose an easy-to-use replacement, called AutoClipping,
that eliminates the need to tune $R$ for any DP optimizers, including DP-SGD,
DP-Adam, DP-LAMB and many others. The automatic variants are as private and
computationally efficient as existing DP optimizers, but require no DP-specific
hyperparameters and thus make DP training as amenable as the standard
non-private training. We give a rigorous convergence analysis of automatic
DP-SGD in the non-convex setting, which shows that it enjoys an asymptotic
convergence rate that matches the standard SGD. We also demonstrate on various
language and vision tasks that automatic clipping outperforms or matches the
state-of-the-art, and can be easily employed with minimal changes to existing
codebases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It's Time for Artistic Correspondence in Music and Video. (arXiv:2206.07148v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07148">
<div class="article-summary-box-inner">
<span><p>We present an approach for recommending a music track for a given video, and
vice versa, based on both their temporal alignment and their correspondence at
an artistic level. We propose a self-supervised approach that learns this
correspondence directly from data, without any need of human annotations. In
order to capture the high-level concepts that are required to solve the task,
we propose modeling the long-term temporal context of both the video and the
music signals, using Transformer networks for each modality. Experiments show
that this approach strongly outperforms alternatives that do not exploit the
temporal context. The combination of our contributions improve retrieval
accuracy up to 10x over prior state of the art. This strong improvement allows
us to introduce a wide range of analyses and applications. For instance, we can
condition music retrieval based on visually defined attributes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervision on Images and Text Reduces Reliance on Visual Shortcut Features. (arXiv:2206.07155v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07155">
<div class="article-summary-box-inner">
<span><p>Deep learning models trained in a fully supervised manner have been shown to
rely on so-called "shortcut" features. Shortcut features are inputs that are
associated with the outcome of interest in the training data, but are either no
longer associated or not present in testing or deployment settings. Here we
provide experiments that show recent self-supervised models trained on images
and text provide more robust image representations and reduce the model's
reliance on visual shortcut features on a realistic medical imaging example.
Additionally, we find that these self-supervised models "forget" shortcut
features more quickly than fully supervised ones when fine-tuned on labeled
data. Though not a complete solution, our experiments provide compelling
evidence that self-supervised models trained on images and text provide some
resilience to visual shortcut features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Multi-organ Segmentation with Partially Labeled Data. (arXiv:2206.07156v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07156">
<div class="article-summary-box-inner">
<span><p>Federated learning is an emerging paradigm allowing large-scale decentralized
learning without sharing data across different data owners, which helps address
the concern of data privacy in medical image analysis. However, the requirement
for label consistency across clients by the existing methods largely narrows
its application scope. In practice, each clinical site may only annotate
certain organs of interest with partial or no overlap with other sites.
Incorporating such partially labeled data into a unified federation is an
unexplored problem with clinical significance and urgency. This work tackles
the challenge by using a novel federated multi-encoding U-Net (Fed-MENU) method
for multi-organ segmentation. In our method, a multi-encoding U-Net (MENU-Net)
is proposed to extract organ-specific features through different encoding
sub-networks. Each sub-network can be seen as an expert of a specific organ and
trained for that client. Moreover, to encourage the organ-specific features
extracted by different sub-networks to be informative and distinctive, we
regularize the training of the MENU-Net by designing an auxiliary generic
decoder (AGD). Extensive experiments on four public datasets show that our
Fed-MENU method can effectively obtain a federated learning model using the
partially labeled datasets with superior performance to other models trained by
either localized or centralized learning methods. Source code will be made
publicly available at the time of paper publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAVENDER: Unifying Video-Language Understanding as Masked Language Modeling. (arXiv:2206.07160v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07160">
<div class="article-summary-box-inner">
<span><p>Unified vision-language frameworks have greatly advanced in recent years,
most of which adopt an encoder-decoder architecture to unify image-text tasks
as sequence-to-sequence generation. However, existing video-language (VidL)
models still require task-specific designs in model architecture and training
objectives for each task. In this work, we explore a unified VidL framework
LAVENDER, where Masked Language Modeling (MLM) is used as the common interface
for all pre-training and downstream tasks. Such unification leads to a
simplified model architecture, where only a lightweight MLM head, instead of a
decoder with much more parameters, is needed on top of the multimodal encoder.
Surprisingly, experimental results show that this unified framework achieves
competitive performance on 14 VidL benchmarks, covering video question
answering, text-to-video retrieval and video captioning. Extensive analyses
further demonstrate the advantage of LAVENDER over existing VidL methods in:
(i) supporting all downstream tasks with just a single set of parameter values
when multi-task finetuned; (ii) few-shot generalization on various downstream
tasks; and (iii) enabling zero-shot evaluation on video question answering
tasks. Code is available at https://github.com/microsoft/LAVENDER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Category-Agnostic 6D Pose Estimation with Conditional Neural Processes. (arXiv:2206.07162v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07162">
<div class="article-summary-box-inner">
<span><p>We present a novel meta-learning approach for 6D pose estimation on unknown
objects. In contrast to "instance-level" pose estimation methods, our algorithm
learns object representation in a category-agnostic way, which endows it with
strong generalization capabilities within and across object categories.
Specifically, we employ a conditional neural process-based meta-learning
approach to train an encoder to capture texture and geometry of an object in a
latent representation, based on very few RGB-D images and ground-truth
keypoints. The latent representation is then used by a simultaneously
meta-trained decoder to predict the 6D pose of the object in new images. To
evaluate our algorithm, experiments are conducted on our new fully-annotated
synthetic datasets generated from Multiple Categories in Multiple Scenes
(MCMS). Experimental results demonstrate that our model performs well on unseen
objects with various shapes and appearances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepRecon: Joint 2D Cardiac Segmentation and 3D Volume Reconstruction via A Structure-Specific Generative Method. (arXiv:2206.07163v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07163">
<div class="article-summary-box-inner">
<span><p>Joint 2D cardiac segmentation and 3D volume reconstruction are fundamental to
building statistical cardiac anatomy models and understanding functional
mechanisms from motion patterns. However, due to the low through-plane
resolution of cine MR and high inter-subject variance, accurately segmenting
cardiac images and reconstructing the 3D volume are challenging. In this study,
we propose an end-to-end latent-space-based framework, DeepRecon, that
generates multiple clinically essential outcomes, including accurate image
segmentation, synthetic high-resolution 3D image, and 3D reconstructed volume.
Our method identifies the optimal latent representation of the cine image that
contains accurate semantic information for cardiac structures. In particular,
our model jointly generates synthetic images with accurate semantic information
and segmentation of the cardiac structures using the optimal latent
representation. We further explore downstream applications of 3D shape
reconstruction and 4D motion pattern adaptation by the different latent-space
manipulation strategies.The simultaneously generated high-resolution images
present a high interpretable value to assess the cardiac shape and
motion.Experimental results demonstrate the effectiveness of our approach on
multiple fronts including 2D segmentation, 3D reconstruction, downstream 4D
motion pattern adaption performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated image analysis in large-scale cellular electron microscopy: A literature survey. (arXiv:2206.07171v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07171">
<div class="article-summary-box-inner">
<span><p>Large-scale electron microscopy (EM) datasets generated using (semi-)
automated microscopes are becoming the standard in EM. Given the vast amounts
of data, manual analysis of all data is not feasible, thus automated analysis
is crucial. The main challenges in automated analysis include the annotation
that is needed to analyse and interpret biomedical images, coupled with
achieving high-throughput. Here, we review the current state-of-the-art of
automated computer techniques and major challenges for the analysis of
structures in cellular EM. The advanced computer vision, deep learning and
software tools that have been developed in the last five years for automatic
biomedical image analysis are discussed with respect to annotation,
segmentation and scalability for EM data. Integration of automatic image
acquisition and analysis will allow for high-throughput analysis of
millimeter-range datasets with nanometer resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Representational Harms in Image Captioning. (arXiv:2206.07173v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07173">
<div class="article-summary-box-inner">
<span><p>Previous work has largely considered the fairness of image captioning systems
through the underspecified lens of "bias." In contrast, we present a set of
techniques for measuring five types of representational harms, as well as the
resulting measurements obtained for two of the most popular image captioning
datasets using a state-of-the-art image captioning system. Our goal was not to
audit this image captioning system, but rather to develop normatively grounded
measurement techniques, in turn providing an opportunity to reflect on the many
challenges involved. We propose multiple measurement techniques for each type
of harm. We argue that by doing so, we are better able to capture the
multi-faceted nature of each type of harm, in turn improving the (collective)
validity of the resulting measurements. Throughout, we discuss the assumptions
underlying our measurement approach and point out when they do not hold.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proximal Splitting Adversarial Attacks for Semantic Segmentation. (arXiv:2206.07179v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07179">
<div class="article-summary-box-inner">
<span><p>Classification has been the focal point of research on adversarial attacks,
but only a few works investigate methods suited to denser prediction tasks,
such as semantic segmentation. The methods proposed in these works do not
accurately solve the adversarial segmentation problem and, therefore, are
overoptimistic in terms of size of the perturbations required to fool models.
Here, we propose a white-box attack for these models based on a proximal
splitting to produce adversarial perturbations with much smaller $\ell_1$,
$\ell_2$, or $\ell_\infty$ norms. Our attack can handle large numbers of
constraints within a nonconvex minimization framework via an Augmented
Lagrangian approach, coupled with adaptive constraint scaling and masking
strategies. We demonstrate that our attack significantly outperforms previously
proposed ones, as well as classification attacks that we adapted for
segmentation, providing a first comprehensive benchmark for this dense task.
Our results push current limits concerning robustness evaluations in
segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surgical Phase Recognition in Laparoscopic Cholecystectomy. (arXiv:2206.07198v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07198">
<div class="article-summary-box-inner">
<span><p>Automatic recognition of surgical phases in surgical videos is a fundamental
task in surgical workflow analysis. In this report, we propose a
Transformer-based method that utilizes calibrated confidence scores for a
2-stage inference pipeline, which dynamically switches between a baseline model
and a separately trained transition model depending on the calibrated
confidence level. Our method outperforms the baseline model on the Cholec80
dataset, and can be applied to a variety of action segmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Event Graphs: Towards Event Centric Understanding of Multimodal World. (arXiv:2206.07207v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07207">
<div class="article-summary-box-inner">
<span><p>Understanding how events described or shown in multimedia content relate to
one another is a critical component to developing robust artificially
intelligent systems which can reason about real-world media. While much
research has been devoted to event understanding in the text, image, and video
domains, none have explored the complex relations that events experience across
domains. For example, a news article may describe a `protest' event while a
video shows an `arrest' event. Recognizing that the visual `arrest' event is a
subevent of the broader `protest' event is a challenging, yet important problem
that prior work has not explored. In this paper, we propose the novel task of
MultiModal Event Event Relations to recognize such cross-modal event relations.
We contribute a large-scale dataset consisting of 100k video-news article
pairs, as well as a benchmark of densely annotated data. We also propose a
weakly supervised multimodal method which integrates commonsense knowledge from
an external knowledge base (KB) to predict rich multimodal event hierarchies.
Experiments show that our model outperforms a number of competitive baselines
on our proposed benchmark. We also perform a detailed analysis of our model's
performance and suggest directions for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Projection-Based K-space Transformer Network for Undersampled Radial MRI Reconstruction with Limited Training Subjects. (arXiv:2206.07219v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07219">
<div class="article-summary-box-inner">
<span><p>The recent development of deep learning combined with compressed sensing
enables fast reconstruction of undersampled MR images and has achieved
state-of-the-art performance for Cartesian k-space trajectories. However,
non-Cartesian trajectories such as the radial trajectory need to be transformed
onto a Cartesian grid in each iteration of the network training, slowing down
the training process and posing inconvenience and delay during training.
Multiple iterations of nonuniform Fourier transform in the networks offset the
deep learning advantage of fast inference. Current approaches typically either
work on image-to-image networks or grid the non-Cartesian trajectories before
the network training to avoid the repeated gridding process. However, the
image-to-image networks cannot ensure the k-space data consistency in the
reconstructed images and the pre-processing of non-Cartesian k-space leads to
gridding errors which cannot be compensated by the network training. Inspired
by the Transformer network to handle long-range dependencies in sequence
transduction tasks, we propose to rearrange the radial spokes to sequential
data based on the chronological order of acquisition and use the Transformer to
predict unacquired radial spokes from acquired ones. We propose novel data
augmentation methods to generate a large amount of training data from a limited
number of subjects. The network can be generated to different anatomical
structures. Experimental results show superior performance of the proposed
framework compared to state-of-the-art deep neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Test-Time Adaptation for Visual Document Understanding. (arXiv:2206.07240v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07240">
<div class="article-summary-box-inner">
<span><p>Self-supervised pretraining has been able to produce transferable
representations for various visual document understanding (VDU) tasks. However,
the ability of such representations to adapt to new distribution shifts at
test-time has not been studied yet. We propose DocTTA, a novel test-time
adaptation approach for documents that leverages cross-modality self-supervised
learning via masked visual language modeling as well as pseudo labeling to
adapt models learned on a \textit{source} domain to an unlabeled
\textit{target} domain at test time. We also introduce new benchmarks using
existing public datasets for various VDU tasks including entity recognition,
key-value extraction, and document visual question answering tasks where DocTTA
improves the source model performance up to 1.79\% in (F1 score), 3.43\% (F1
score), and 17.68\% (ANLS score), respectively while drastically reducing
calibration error on target data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRAM-HD: 3D-Consistent Image Generation at High Resolution with Generative Radiance Manifolds. (arXiv:2206.07255v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07255">
<div class="article-summary-box-inner">
<span><p>Recent works have shown that 3D-aware GANs trained on unstructured single
image collections can generate multiview images of novel instances. The key
underpinnings to achieve this are a 3D radiance field generator and a volume
rendering process. However, existing methods either cannot generate
high-resolution images (e.g., up to 256X256) due to the high computation cost
of neural volume rendering, or rely on 2D CNNs for image-space upsampling which
jeopardizes the 3D consistency across different views. This paper proposes a
novel 3D-aware GAN that can generate high resolution images (up to 1024X1024)
while keeping strict 3D consistency as in volume rendering. Our motivation is
to achieve super-resolution directly in the 3D space to preserve 3D
consistency. We avoid the otherwise prohibitively-expensive computation cost by
applying 2D convolutions on a set of 2D radiance manifolds defined in the
recent generative radiance manifold (GRAM) approach, and apply dedicated loss
functions for effective GAN training at high resolution. Experiments on FFHQ
and AFHQv2 datasets show that our method can produce high-quality 3D-consistent
results that significantly outperform existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning of Image Scale and Orientation. (arXiv:2206.07259v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07259">
<div class="article-summary-box-inner">
<span><p>We study the problem of learning to assign a characteristic pose, i.e., scale
and orientation, for an image region of interest. Despite its apparent
simplicity, the problem is non-trivial; it is hard to obtain a large-scale set
of image regions with explicit pose annotations that a model directly learns
from. To tackle the issue, we propose a self-supervised learning framework with
a histogram alignment technique. It generates pairs of image patches by random
rescaling/rotating and then train an estimator to predict their
scale/orientation values so that their relative difference is consistent with
the rescaling/rotating used. The estimator learns to predict a non-parametric
histogram distribution of scale/orientation without any supervision.
Experiments show that it significantly outperforms previous methods in
scale/orientation estimation and also improves image matching and 6 DoF camera
pose estimation by incorporating our patch poses into a matching process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Enforcing Better Conditioned Meta-Learning for Rapid Few-Shot Adaptation. (arXiv:2206.07260v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07260">
<div class="article-summary-box-inner">
<span><p>Inspired by the concept of preconditioning, we propose a novel method to
increase adaptation speed for gradient-based meta-learning methods without
incurring extra parameters. We demonstrate that recasting the optimization
problem to a non-linear least-squares formulation provides a principled way to
actively enforce a $\textit{well-conditioned}$ parameter space for
meta-learning models based on the concepts of the condition number and local
curvature. Our comprehensive evaluations show that the proposed method
significantly outperforms its unconstrained counterpart especially during
initial adaptation steps, while achieving comparable or better overall results
on several few-shot classification tasks -- creating the possibility of
dynamically choosing the number of adaptation steps at inference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Generalization in Few-Shot Classification. (arXiv:2206.07267v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07267">
<div class="article-summary-box-inner">
<span><p>Single image-level annotations only correctly describe an often small subset
of an image's content, particularly when complex real-world scenes are
depicted. While this might be acceptable in many classification scenarios, it
poses a significant challenge for applications where the set of classes differs
significantly between training and test time. In this paper, we take a closer
look at the implications in the context of $\textit{few-shot learning}$.
Splitting the input samples into patches and encoding these via the help of
Vision Transformers allows us to establish semantic correspondences between
local regions across images and independent of their respective class. The most
informative patch embeddings for the task at hand are then determined as a
function of the support set via online optimization at inference time,
additionally providing visual interpretability of `$\textit{what matters
most}$' in the image. We build on recent advances in unsupervised training of
networks via masked image modelling to overcome the lack of fine-grained labels
and learn the more general statistical structure of the data while avoiding
negative image-level annotation influence, $\textit{aka}$ supervision collapse.
Experimental results show the competitiveness of our approach, achieving new
state-of-the-art results on four popular few-shot classification benchmarks for
$5$-shot and $1$-shot scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine vision for vial positioning detection toward the safe automation of material synthesis. (arXiv:2206.07272v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07272">
<div class="article-summary-box-inner">
<span><p>Although robot-based automation in chemistry laboratories can accelerate the
material development process, surveillance-free environments may lead to
dangerous accidents primarily due to machine control errors. Object detection
techniques can play vital roles in addressing these safety issues; however,
state-of-the-art detectors, including single-shot detector (SSD) models, suffer
from insufficient accuracy in environments involving complex and noisy scenes.
With the aim of improving safety in a surveillance-free laboratory, we report a
novel deep learning (DL)-based object detector, namely, DenseSSD. For the
foremost and frequent problem of detecting vial positions, DenseSSD achieved a
mean average precision (mAP) over 95% based on a complex dataset involving both
empty and solution-filled vials, greatly exceeding those of conventional
detectors; such high precision is critical to minimizing failure-induced
accidents. Additionally, DenseSSD was observed to be highly insensitive to the
environmental changes, maintaining its high precision under the variations of
solution colors or testing view angles. The robustness of DenseSSD would allow
the utilized equipment settings to be more flexible. This work demonstrates
that DenseSSD is useful for enhancing safety in an automated material synthesis
environment, and it can be extended to various applications where high
detection accuracy and speed are both needed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ERNAS: An Evolutionary Neural Architecture Search for Magnetic Resonance Image Reconstructions. (arXiv:2206.07280v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07280">
<div class="article-summary-box-inner">
<span><p>Magnetic resonance imaging (MRI) is one of the noninvasive imaging modalities
that can produce high-quality images. However, the scan procedure is relatively
slow, which causes patient discomfort and motion artifacts in images.
Accelerating MRI hardware is constrained by physical and physiological
limitations. A popular alternative approach to accelerated MRI is to
undersample the k-space data. While undersampling speeds up the scan procedure,
it generates artifacts in the images, and advanced reconstruction algorithms
are needed to produce artifact-free images. Recently deep learning has emerged
as a promising MRI reconstruction method to address this problem. However,
straightforward adoption of the existing deep learning neural network
architectures in MRI reconstructions is not usually optimal in terms of
efficiency and reconstruction quality. In this work, MRI reconstruction from
undersampled data was carried out using an optimized neural network using a
novel evolutionary neural architecture search algorithm. Brain and knee MRI
datasets show that the proposed algorithm outperforms manually designed neural
network-based MR reconstruction models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Super-resolution image display using diffractive decoders. (arXiv:2206.07281v1 [physics.optics])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07281">
<div class="article-summary-box-inner">
<span><p>High-resolution synthesis/projection of images over a large field-of-view
(FOV) is hindered by the restricted space-bandwidth-product (SBP) of wavefront
modulators. We report a deep learning-enabled diffractive display design that
is based on a jointly-trained pair of an electronic encoder and a diffractive
optical decoder to synthesize/project super-resolved images using
low-resolution wavefront modulators. The digital encoder, composed of a trained
convolutional neural network (CNN), rapidly pre-processes the high-resolution
images of interest so that their spatial information is encoded into
low-resolution (LR) modulation patterns, projected via a low SBP wavefront
modulator. The diffractive decoder processes this LR encoded information using
thin transmissive layers that are structured using deep learning to
all-optically synthesize and project super-resolved images at its output FOV.
Our results indicate that this diffractive image display can achieve a
super-resolution factor of ~4, demonstrating a ~16-fold increase in SBP. We
also experimentally validate the success of this diffractive super-resolution
display using 3D-printed diffractive decoders that operate at the THz spectrum.
This diffractive image decoder can be scaled to operate at visible wavelengths
and inspire the design of large FOV and high-resolution displays that are
compact, low-power, and computationally efficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Eyes Inspired Recurrent Neural Networks are More Robust Against Adversarial Noises. (arXiv:2206.07282v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07282">
<div class="article-summary-box-inner">
<span><p>Compared to human vision, computer vision based on convolutional neural
networks (CNN) are more vulnerable to adversarial noises. This difference is
likely attributable to how the eyes sample visual input and how the brain
processes retinal samples through its dorsal and ventral visual pathways, which
are under-explored for computer vision. Inspired by the brain, we design
recurrent neural networks, including an input sampler that mimics the human
retina, a dorsal network that guides where to look next, and a ventral network
that represents the retinal samples. Taking these modules together, the models
learn to take multiple glances at an image, attend to a salient part at each
glance, and accumulate the representation over time to recognize the image. We
test such models for their robustness against a varying level of adversarial
noises with a special focus on the effect of different input sampling
strategies. Our findings suggest that retinal foveation and sampling renders a
model more robust against adversarial noises, and the model may correct itself
from an attack when it is given a longer time to take more glances at an image.
In conclusion, robust visual recognition can benefit from the combined use of
three brain-inspired mechanisms: retinal transformation, attention guided eye
movement, and recurrent processing, as opposed to feedforward-only CNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Top-k Classification Learning. (arXiv:2206.07290v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07290">
<div class="article-summary-box-inner">
<span><p>The top-k classification accuracy is one of the core metrics in machine
learning. Here, k is conventionally a positive integer, such as 1 or 5, leading
to top-1 or top-5 training objectives. In this work, we relax this assumption
and optimize the model for multiple k simultaneously instead of using a single
k. Leveraging recent advances in differentiable sorting and ranking, we propose
a differentiable top-k cross-entropy classification loss. This allows training
the network while not only considering the top-1 prediction, but also, e.g.,
the top-2 and top-5 predictions. We evaluate the proposed loss function for
fine-tuning on state-of-the-art architectures, as well as for training from
scratch. We find that relaxing k does not only produce better top-5 accuracies,
but also leads to top-1 accuracy improvements. When fine-tuning publicly
available ImageNet models, we achieve a new state-of-the-art for these models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S\textsuperscript{2}-FPN: Scale-ware Strip Attention Guided Feature Pyramid Network for Real-time Semantic Segmentation. (arXiv:2206.07298v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07298">
<div class="article-summary-box-inner">
<span><p>Modern high-performance semantic segmentation methods employ a heavy backbone
and dilated convolution to extract the relevant feature. Although extracting
features with both contextual and semantic information is critical for the
segmentation tasks, it brings a memory footprint and high computation cost for
real-time applications. This paper presents a new model to achieve a trade-off
between accuracy/speed for real-time road scene semantic segmentation.
Specifically, we proposed a lightweight model named Scale-aware Strip Attention
Guided Feature Pyramid Network (S\textsuperscript{2}-FPN). Our network consists
of three main modules: Attention Pyramid Fusion (APF) module, Scale-aware Strip
Attention Module (SSAM), and Global Feature Upsample (GFU) module. APF adopts
an attention mechanisms to learn discriminative multi-scale features and help
close the semantic gap between different levels. APF uses the scale-aware
attention to encode global context with vertical stripping operation and models
the long-range dependencies, which helps relate pixels with similar semantic
label. In addition, APF employs channel-wise reweighting block (CRB) to
emphasize the channel features. Finally, the decoder of
S\textsuperscript{2}-FPN then adopts GFU, which is used to fuse features from
APF and the encoder. Extensive experiments have been conducted on two
challenging semantic segmentation benchmarks, which demonstrate that our
approach achieves better accuracy/speed trade-off with different model
settings. The proposed models have achieved a results of 76.2\%mIoU/87.3FPS,
77.4\%mIoU/67FPS, and 77.8\%mIoU/30.5FPS on Cityscapes dataset, and
69.6\%mIoU,71.0\% mIoU, and 74.2\% mIoU on Camvid dataset. The code for this
work will be made available at \url{https://github.com/mohamedac29/S2-FPN
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VCT: A Video Compression Transformer. (arXiv:2206.07307v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07307">
<div class="article-summary-box-inner">
<span><p>We show how transformers can be used to vastly simplify neural video
compression. Previous methods have been relying on an increasing number of
architectural biases and priors, including motion prediction and warping
operations, resulting in complex models. Instead, we independently map input
frames to representations and use a transformer to model their dependencies,
letting it predict the distribution of future representations given the past.
The resulting video compression transformer outperforms previous methods on
standard video compression data sets. Experiments on synthetic data show that
our model learns to handle complex motion patterns such as panning, blurring
and fading purely from data. Our approach is easy to implement, and we release
code to facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent Advances in Scene Image Representation and Classification. (arXiv:2206.07326v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07326">
<div class="article-summary-box-inner">
<span><p>With the rise of deep learning algorithms nowadays, scene image
representation methods on big data (e.g., SUN-397) have achieved a significant
performance boost in classification. However, the performance is still limited
because the scene images are mostly complex in nature having higher intra-class
dissimilarity and inter-class similarity problems. To deal with such problems,
there are several methods proposed in the literature with their own advantages
and limitations. A detailed study of previous works is necessary to understand
their pros and cons in image representation and classification. In this paper,
we review the existing scene image representation methods that are being used
widely for image classification. For this, we, first, devise the taxonomy using
the seminal existing methods proposed in the literature to this date. Next, we
compare their performance both qualitatively (e.g., quality of outputs,
pros/cons, etc.) and quantitatively (e.g., accuracy). Last, we speculate the
prominent research directions in scene image representation tasks. Overall,
this survey provides in-depth insights and applications of recent scene image
representation methods for traditional Computer Vision (CV)-based methods, Deep
Learning (DL)-based methods, and Search Engine (SE)-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Detection of Rice Disease in Images of Various Leaf Sizes. (arXiv:2206.07344v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07344">
<div class="article-summary-box-inner">
<span><p>Fast, accurate and affordable rice disease detection method is required to
assist rice farmers tackling equipment and expertise shortages problems. In
this paper, we focused on the solution using computer vision technique to
detect rice diseases from rice field photograph images. Dealing with images
took in real-usage situation by general farmers is quite challenging due to
various environmental factors, and rice leaf object size variation is one major
factor caused performance gradation. To solve this problem, we presented a
technique combining a CNN object detection with image tiling technique, based
on automatically estimated width size of rice leaves in the images as a size
reference for dividing the original input image. A model to estimate leaf width
was created by small size CNN such as 18 layer ResNet architecture model. A new
divided tiled sub-image set with uniformly sized object was generated and used
as input for training a rice disease prediction model. Our technique was
evaluated on 4,960 images of eight different types of rice leaf diseases,
including blast, blight, brown spot, narrow brown spot, orange, red stripe,
rice grassy stunt virus, and streak disease. The mean absolute percentage error
(MAPE) for leaf width prediction task evaluated on all eight classes was 11.18%
in the experiment, indicating that the leaf width prediction model performed
well. The mean average precision (mAP) of the prediction performance on YOLOv4
architecture was enhanced from 87.56% to 91.14% when trained and tested with
the tiled dataset. According to our study, the proposed image tiling technique
improved rice disease detection efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Capsule Networks of High-Dimension Point Clouds classification. (arXiv:2206.07348v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07348">
<div class="article-summary-box-inner">
<span><p>Three-dimensional point clouds learning is widely applied, but the point
clouds are still unable to deal with classification and recognition tasks
satisfactorily in the cases of irregular geometric structures and
high-dimensional space. In 3D space, point clouds tend to have regular
Euclidean structure because of their density. On the contrary, due to the high
dimensionality, the spatial structure of high-dimensional space is more
complex, and point clouds are mostly presented in non-European structure.
Furthermore, among current 3D point clouds classification algorithms, Canonical
Capsules algorithm based on Euclidean distance is difficult to decompose and
identify non-Euclidean structures effectively. Thus, aiming at the point clouds
classification task of non-Euclidean structure in 3D and high-dimensional
space, this paper refers to the LLE algorithm based on geodesic distance for
optimizing and proposes the unsupervised algorithm of high-dimensional point
clouds capsule. In this paper, the geometric features of point clouds are
considered in the extraction process, so as to transform the high-dimensional
non-Euclidean structure into a lower-dimensional Euclidean structure with
retaining spatial geometric features. To verify the feasibility of the
unsupervised algorithm of high-dimensional point clouds capsule, experiments
are conducted in Swiss Roll dataset, point clouds MNIST dataset and point
clouds LFW dataset. The results show that (1) non-Euclidean structures can be
can effectively identified by this model in Swiss Roll dataset; (2) a
significant unsupervised learning effect is realized in point clouds MNIST
dataset. In conclusion, the high-dimensional point clouds capsule unsupervised
algorithm proposed in this paper is conducive to expand the application
scenarios of current point clouds classification and recognition tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XMorpher: Full Transformer for Deformable Medical Image Registration via Cross Attention. (arXiv:2206.07349v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07349">
<div class="article-summary-box-inner">
<span><p>An effective backbone network is important to deep learning-based Deformable
Medical Image Registration (DMIR), because it extracts and matches the features
between two images to discover the mutual correspondence for fine registration.
However, the existing deep networks focus on single image situation and are
limited in registration task which is performed on paired images. Therefore, we
advance a novel backbone network, XMorpher, for the effective corresponding
feature representation in DMIR. 1) It proposes a novel full transformer
architecture including dual parallel feature extraction networks which exchange
information through cross attention, thus discovering multi-level semantic
correspondence while extracting respective features gradually for final
effective registration. 2) It advances the Cross Attention Transformer (CAT)
blocks to establish the attention mechanism between images which is able to
find the correspondence automatically and prompts the features to fuse
efficiently in the network. 3) It constrains the attention computation between
base windows and searching windows with different sizes, and thus focuses on
the local transformation of deformable registration and enhances the computing
efficiency at the same time. Without any bells and whistles, our XMorpher gives
Voxelmorph 2.8% improvement on DSC , demonstrating its effective representation
of the features from the paired images in DMIR. We believe that our XMorpher
has great application potential in more paired medical images. Our XMorpher is
open on https://github.com/Solemoon/XMorpher
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust SAR ATR on MSTAR with Deep Learning Models trained on Full Synthetic MOCEM data. (arXiv:2206.07352v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07352">
<div class="article-summary-box-inner">
<span><p>The promising potential of Deep Learning for Automatic Target Recognition
(ATR) on Synthetic Aperture Radar (SAR) images vanishes when considering the
complexity of collecting training datasets measurements. Simulation can
overcome this issue by producing synthetic training datasets. However, because
of the limited representativeness of simulation, models trained in a classical
way with synthetic images have limited generalization abilities when dealing
with real measurement at test time. Previous works identified a set of equally
promising deep-learning algorithms to tackle this issue. However, these
approaches have been evaluated in a very favorable scenario with a synthetic
training dataset that overfits the ground truth of the measured test data. In
this work, we study the ATR problem outside of this ideal condition, which is
unlikely to occur in real operational contexts. Our contribution is threefold.
(1) Using the MOCEM simulator (developed by SCALIAN DS for the French MoD/DGA),
we produce a synthetic MSTAR training dataset that differs significantly from
the real measurements. (2) We experimentally demonstrate the limits of the
state-of-the-art. (3) We show that domain randomization techniques and
adversarial training can be combined to overcome this issue. We demonstrate
that this approach is more robust than the state-of-the-art, with an accuracy
of 75 %, while having a limited impact on computing performance during
training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seeking Common Ground While Reserving Differences: Multiple Anatomy Collaborative Framework for Undersampled MRI Reconstruction. (arXiv:2206.07364v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07364">
<div class="article-summary-box-inner">
<span><p>Recently, deep neural networks have greatly advanced undersampled Magnetic
Resonance Image (MRI) reconstruction, wherein most studies follow the
one-anatomy-one-network fashion, i.e., each expert network is trained and
evaluated for a specific anatomy. Apart from inefficiency in training multiple
independent models, such convention ignores the shared de-aliasing knowledge
across various anatomies which can benefit each other. To explore the shared
knowledge, one naive way is to combine all the data from various anatomies to
train an all-round network. Unfortunately, despite the existence of the shared
de-aliasing knowledge, we reveal that the exclusive knowledge across different
anatomies can deteriorate specific reconstruction targets, yielding overall
performance degradation. Observing this, in this study, we present a novel deep
MRI reconstruction framework with both anatomy-shared and anatomy-specific
parameterized learners, aiming to "seek common ground while reserving
differences" across different anatomies.Particularly, the primary
anatomy-shared learners are exposed to different anatomies to model flourishing
shared knowledge, while the efficient anatomy-specific learners are trained
with their target anatomy for exclusive knowledge. Four different
implementations of anatomy-specific learners are presented and explored on the
top of our framework in two MRI reconstruction networks. Comprehensive
experiments on brain, knee and cardiac MRI datasets demonstrate that three of
these learners are able to enhance reconstruction performance via multiple
anatomy collaborative learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MonoGround: Detecting Monocular 3D Objects from the Ground. (arXiv:2206.07372v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07372">
<div class="article-summary-box-inner">
<span><p>Monocular 3D object detection has attracted great attention for its
advantages in simplicity and cost. Due to the ill-posed 2D to 3D mapping
essence from the monocular imaging process, monocular 3D object detection
suffers from inaccurate depth estimation and thus has poor 3D detection
results. To alleviate this problem, we propose to introduce the ground plane as
a prior in the monocular 3d object detection. The ground plane prior serves as
an additional geometric condition to the ill-posed mapping and an extra source
in depth estimation. In this way, we can get a more accurate depth estimation
from the ground. Meanwhile, to take full advantage of the ground plane prior,
we propose a depth-align training strategy and a precise two-stage depth
inference method tailored for the ground plane prior. It is worth noting that
the introduced ground plane prior requires no extra data sources like LiDAR,
stereo images, and depth information. Extensive experiments on the KITTI
benchmark show that our method could achieve state-of-the-art results compared
with other methods while maintaining a very fast speed. Our code and models are
available at https://github.com/cfzd/MonoGround.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Manifold Hypothesis for Gradient-Based Explanations. (arXiv:2206.07387v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07387">
<div class="article-summary-box-inner">
<span><p>When do gradient-based explanation algorithms provide meaningful
explanations? We propose a necessary criterion: their feature attributions need
to be aligned with the tangent space of the data manifold. To provide evidence
for this hypothesis, we introduce a framework based on variational autoencoders
that allows to estimate and generate image manifolds. Through experiments
across a range of different datasets -- MNIST, EMNIST, CIFAR10, X-ray pneumonia
and Diabetic Retinopathy detection -- we demonstrate that the more a feature
attribution is aligned with the tangent space of the data, the more structured
and explanatory it tends to be. In particular, the attributions provided by
popular post-hoc methods such as Integrated Gradients, SmoothGrad and Input
$\times$ Gradient tend to be more strongly aligned with the data manifold than
the raw gradient. As a consequence, we suggest that explanation algorithms
should actively strive to align their explanations with the data manifold. In
part, this can be achieved by adversarial training, which leads to better
alignment across all datasets. Some form of adjustment to the model
architecture or training algorithm is necessary, since we show that
generalization of neural networks alone does not imply the alignment of model
gradients with the data manifold.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subsurface Depths Structure Maps Reconstruction with Generative Adversarial Networks. (arXiv:2206.07388v1 [physics.geo-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07388">
<div class="article-summary-box-inner">
<span><p>This paper described a method for reconstruction of detailed-resolution depth
structure maps, usually obtained after the 3D seismic surveys, using the data
from 2D seismic depth maps. The method uses two algorithms based on the
generative-adversarial neural network architecture. The first algorithm
StyleGAN2-ADA accumulates in the hidden space of the neural network the
semantic images of mountainous terrain forms first, and then with help of
transfer learning, in the ideal case - the structure geometry of stratigraphic
horizons. The second algorithm, the Pixel2Style2Pixel encoder, using the
semantic level of generalization of the first algorithm, learns to reconstruct
the original high-resolution images from their degraded copies
(super-resolution technology). There was demonstrated a methodological approach
to transferring knowledge on the structural forms of stratigraphic horizon
boundaries from the well-studied areas to the underexplored ones. Using the
multimodal synthesis of Pixel2Style2Pixel encoder, it is proposed to create a
probabilistic depth space, where each point of the project area is represented
by the density of probabilistic depth distribution of equally probable
reconstructed geological forms of structural images. Assessment of the
reconstruction quality was carried out for two blocks. Using this method,
credible detailed depth reconstructions comparable with the quality of 3D
seismic maps have been obtained from 2D seismic maps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ultra Fast Deep Lane Detection with Hybrid Anchor Driven Ordinal Classification. (arXiv:2206.07389v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07389">
<div class="article-summary-box-inner">
<span><p>Modern methods mainly regard lane detection as a problem of pixel-wise
segmentation, which is struggling to address the problems of efficiency and
challenging scenarios like severe occlusions and extreme lighting conditions.
Inspired by human perception, the recognition of lanes under severe occlusions
and extreme lighting conditions is mainly based on contextual and global
information. Motivated by this observation, we propose a novel, simple, yet
effective formulation aiming at ultra fast speed and the problem of challenging
scenarios. Specifically, we treat the process of lane detection as an
anchor-driven ordinal classification problem using global features. First, we
represent lanes with sparse coordinates on a series of hybrid (row and column)
anchors. With the help of the anchor-driven representation, we then reformulate
the lane detection task as an ordinal classification problem to get the
coordinates of lanes. Our method could significantly reduce the computational
cost with the anchor-driven representation. Using the large receptive field
property of the ordinal classification formulation, we could also handle
challenging scenarios. Extensive experiments on four lane detection datasets
show that our method could achieve state-of-the-art performance in terms of
both speed and accuracy. A lightweight version could even achieve 300+ frames
per second(FPS). Our code is at
https://github.com/cfzd/Ultra-Fast-Lane-Detection-v2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Adaptive Ensembling for Image Classification. (arXiv:2206.07394v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07394">
<div class="article-summary-box-inner">
<span><p>In recent times, except for sporadic cases, the trend in Computer Vision is
to achieve minor improvements over considerable increases in complexity.
</p>
<p>To reverse this tendency, we propose a novel method to boost image
classification performances without an increase in complexity.
</p>
<p>To this end, we revisited ensembling, a powerful approach, not often
adequately used due to its nature of increased complexity and training time,
making it viable by specific design choices. First, we trained end-to-end two
EfficientNet-b0 models (known to be the architecture with the best overall
accuracy/complexity trade-off in image classification) on disjoint subsets of
data (i.e. bagging). Then, we made an efficient adaptive ensemble by performing
fine-tuning of a trainable combination layer. In this way, we were able to
outperform the state-of-the-art by an average of 0.5\% on the accuracy with
restrained complexity both in terms of number of parameters (by 5-60 times),
and FLoating point Operations Per Second (by 10-100 times) on several major
benchmark datasets, fully embracing the green AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable differential diagnosis for Alzheimer's disease and Frontotemporal dementia. (arXiv:2206.07417v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07417">
<div class="article-summary-box-inner">
<span><p>Alzheimer's disease and Frontotemporal dementia are two major types of
dementia. Their accurate diagnosis and differentiation is crucial for
determining specific intervention and treatment. However, differential
diagnosis of these two types of dementia remains difficult at the early stage
of disease due to similar patterns of clinical symptoms. Therefore, the
automatic classification of multiple types of dementia has an important
clinical value. So far, this challenge has not been actively explored. Recent
development of deep learning in the field of medical image has demonstrated
high performance for various classification tasks. In this paper, we propose to
take advantage of two types of biomarkers: structure grading and structure
atrophy. To this end, we propose first to train a large ensemble of 3D U-Nets
to locally discriminate healthy versus dementia anatomical patterns. The result
of these models is an interpretable 3D grading map capable of indicating
abnormal brain regions. This map can also be exploited in various
classification tasks using graph convolutional neural network. Finally, we
propose to combine deep grading and atrophy-based classifications to improve
dementia type discrimination. The proposed framework showed competitive
performance compared to state-of-the-art methods for different tasks of disease
detection and differential diagnosis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Neural Network Pruning for Nuclei Instance Segmentation in Hematoxylin & Eosin-Stained Histological Images. (arXiv:2206.07422v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07422">
<div class="article-summary-box-inner">
<span><p>Recently, pruning deep neural networks (DNNs) has received a lot of attention
for improving accuracy and generalization power, reducing network size, and
increasing inference speed on specialized hardwares. Although pruning was
mainly tested on computer vision tasks, its application in the context of
medical image analysis has hardly been explored. This work investigates the
impact of well-known pruning techniques, namely layer-wise and network-wide
magnitude pruning, on the nuclei instance segmentation performance in
histological images. Our utilized instance segmentation model consists of two
main branches: (1) a semantic segmentation branch, and (2) a deep regression
branch. We investigate the impact of weight pruning on the performance of both
branches separately and on the final nuclei instance segmentation result.
Evaluated on two publicly available datasets, our results show that layer-wise
pruning delivers slightly better performance than networkwide pruning for small
compression ratios (CRs) while for large CRs, network-wide pruning yields
superior performance. For semantic segmentation, deep regression and final
instance segmentation, 93.75 %, 95 %, and 80 % of the model weights can be
pruned by layer-wise pruning with less than 2 % reduction in the performance of
respective models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot object goal visual navigation. (arXiv:2206.07423v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07423">
<div class="article-summary-box-inner">
<span><p>Object goal visual navigation is a challenging task that aims to guide a
robot to find the target object only based on its visual observation, and the
target is limited to the classes specified in the training stage. However, in
real households, there may exist numerous object classes that the robot needs
to deal with, and it is hard for all of these classes to be contained in the
training stage. To address this challenge, we propose a zero-shot object
navigation task by combining zero-shot learning with object goal visual
navigation, which aims at guiding robots to find objects belonging to novel
classes without any training samples. This task gives rise to the need to
generalize the learned policy to novel classes, which is a less addressed issue
of object navigation using deep reinforcement learning. To address this issue,
we utilize "class-unrelated" data as input to alleviate the overfitting of the
classes specified in the training stage. The class-unrelated input consists of
detection results and cosine similarity of word embeddings, and does not
contain any class-related visual features or knowledge graphs. Extensive
experiments on the AI2-THOR platform show that our model outperforms the
baseline models in both seen and unseen classes, which proves that our model is
less class-sensitive and generalizes better. Our code is available at
https://github.com/pioneer-innovation/Zero-Shot-Object-Navigation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physically-admissible polarimetric data augmentation for road-scene analysis. (arXiv:2206.07431v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07431">
<div class="article-summary-box-inner">
<span><p>Polarimetric imaging, along with deep learning, has shown improved
performances on different tasks including scene analysis. However, its
robustness may be questioned because of the small size of the training
datasets. Though the issue could be solved by data augmentation, polarization
modalities are subject to physical feasibility constraints unaddressed by
classical data augmentation techniques. To address this issue, we propose to
use CycleGAN, an image translation technique based on deep generative models
that solely relies on unpaired data, to transfer large labeled road scene
datasets to the polarimetric domain. We design several auxiliary loss terms
that, alongside the CycleGAN losses, deal with the physical constraints of
polarimetric images. The efficiency of this solution is demonstrated on road
scene object detection tasks where generated realistic polarimetric images
allow to improve performances on cars and pedestrian detection up to 9%. The
resulting constrained CycleGAN is publicly released, allowing anyone to
generate their own polarimetric images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Implicit Attention: Guided Attention by The Model Itself. (arXiv:2206.07434v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07434">
<div class="article-summary-box-inner">
<span><p>We propose Self-Supervised Implicit Attention (SSIA), a new approach that
adaptively guides deep neural network models to gain attention by exploiting
the properties of the models themselves. SSIA is a novel attention mechanism
that does not require any extra parameters, computation, or memory access costs
during inference, which is in contrast to existing attention mechanism. In
short, by considering attention weights as higher-level semantic information,
we reconsidered the implementation of existing attention mechanisms and further
propose generating supervisory signals from higher network layers to guide
lower network layers for parameter updates. We achieved this by building a
self-supervised learning task using the hierarchical features of the network
itself, which only works at the training stage. To verify the effectiveness of
SSIA, we performed a particular implementation (called an SSIA block) in
convolutional neural network models and validated it on several image
classification datasets. The experimental results show that an SSIA block can
significantly improve the model performance, even outperforms many popular
attention methods that require additional parameters and computation costs,
such as Squeeze-and-Excitation and Convolutional Block Attention Module. Our
implementation will be available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forecasting of depth and ego-motion with transformers and self-supervision. (arXiv:2206.07435v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07435">
<div class="article-summary-box-inner">
<span><p>This paper addresses the problem of end-to-end self-supervised forecasting of
depth and ego motion. Given a sequence of raw images, the aim is to forecast
both the geometry and ego-motion using a self supervised photometric loss. The
architecture is designed using both convolution and transformer modules. This
leverages the benefits of both modules: Inductive bias of CNN, and the
multi-head attention of transformers, thus enabling a rich spatio-temporal
representation that enables accurate depth forecasting. Prior work attempts to
solve this problem using multi-modal input/output with supervised ground-truth
data which is not practical since a large annotated dataset is required.
Alternatively to prior methods, this paper forecasts depth and ego motion using
only self-supervised raw images as input. The approach performs significantly
well on the KITTI dataset benchmark with several performance criteria being
even comparable to prior non-forecasting self-supervised monocular depth
inference methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisageSynTalk: Unseen Speaker Video-to-Speech Synthesis via Speech-Visage Feature Selection. (arXiv:2206.07458v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07458">
<div class="article-summary-box-inner">
<span><p>The goal of this work is to reconstruct speech from a silent talking face
video. Recent studies have shown impressive performance on synthesizing speech
from silent talking face videos. However, they have not explicitly considered
on varying identity characteristics of different speakers, which place a
challenge in the video-to-speech synthesis, and this becomes more critical in
unseen-speaker settings. Distinct from the previous methods, our approach is to
separate the speech content and the visage-style from a given silent talking
face video. By guiding the model to independently focus on modeling the two
representations, we can obtain the speech of high intelligibility from the
model even when the input video of an unseen subject is given. To this end, we
introduce speech-visage selection module that separates the speech content and
the speaker identity from the visual features of the input video. The
disentangled representations are jointly incorporated to synthesize speech
through visage-style based synthesizer which generates speech by coating the
visage-styles while maintaining the speech content. Thus, the proposed
framework brings the advantage of synthesizing the speech containing the right
content even when the silent talking face video of an unseen subject is given.
We validate the effectiveness of the proposed framework on the GRID, TCD-TIMIT
volunteer, and LRW datasets. The synthesized speech can be heard in
supplementary materials.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">READ: Aggregating Reconstruction Error into Out-of-distribution Detection. (arXiv:2206.07459v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07459">
<div class="article-summary-box-inner">
<span><p>Detecting out-of-distribution (OOD) samples is crucial to the safe deployment
of a classifier in the real world. However, deep neural networks are known to
be overconfident for abnormal data. Existing works directly design score
function by mining the inconsistency from classifier for in-distribution (ID)
and OOD. In this paper, we further complement this inconsistency with
reconstruction error, based on the assumption that an autoencoder trained on ID
data can not reconstruct OOD as well as ID. We propose a novel method, READ
(Reconstruction Error Aggregated Detector), to unify inconsistencies from
classifier and autoencoder. Specifically, the reconstruction error of raw
pixels is transformed to latent space of classifier. We show that the
transformed reconstruction error bridges the semantic gap and inherits
detection performance from the original. Moreover, we propose an adjustment
strategy to alleviate the overconfidence problem of autoencoder according to a
fine-grained characterization of OOD data. Under two scenarios of pre-training
and retraining, we respectively present two variants of our method, namely
READ-MD (Mahalanobis Distance) only based on pre-trained classifier and READ-ED
(Euclidean Distance) which retrains the classifier. Our methods do not require
access to test time OOD data for fine-tuning hyperparameters. Finally, we
demonstrate the effectiveness of the proposed methods through extensive
comparisons with state-of-the-art OOD detection algorithms. On a CIFAR-10
pre-trained WideResNet, our method reduces the average FPR@95TPR by up to 9.8%
compared with previous state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-fine Deep Video Coding with Hyperprior-guided Mode Prediction. (arXiv:2206.07460v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07460">
<div class="article-summary-box-inner">
<span><p>The previous deep video compression approaches only use the single scale
motion compensation strategy and rarely adopt the mode prediction technique
from the traditional standards like H.264/H.265 for both motion and residual
compression. In this work, we first propose a coarse-to-fine (C2F) deep video
compression framework for better motion compensation, in which we perform
motion estimation, compression and compensation twice in a coarse to fine
manner. Our C2F framework can achieve better motion compensation results
without significantly increasing bit costs. Observing hyperprior information
(i.e., the mean and variance values) from the hyperprior networks contains
discriminant statistical information of different patches, we also propose two
efficient hyperprior-guided mode prediction methods. Specifically, using
hyperprior information as the input, we propose two mode prediction networks to
respectively predict the optimal block resolutions for better motion coding and
decide whether to skip residual information from each block for better residual
coding without introducing additional bit cost while bringing negligible extra
computation cost. Comprehensive experimental results demonstrate our proposed
C2F video compression framework equipped with the new hyperprior-guided mode
prediction methods achieves the state-of-the-art performance on HEVC, UVG and
MCL-JCV datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PolyU-BPCoMa: A Dataset and Benchmark Towards Mobile Colorized Mapping Using a Backpack Multisensorial System. (arXiv:2206.07468v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07468">
<div class="article-summary-box-inner">
<span><p>Constructing colorized point clouds from mobile laser scanning and images is
a fundamental work in surveying and mapping. It is also an essential
prerequisite for building digital twins for smart cities. However, existing
public datasets are either in relatively small scales or lack accurate
geometrical and color ground truth. This paper documents a multisensorial
dataset named PolyU-BPCoMA which is distinctively positioned towards mobile
colorized mapping. The dataset incorporates resources of 3D LiDAR, spherical
imaging, GNSS and IMU on a backpack platform. Color checker boards are pasted
in each surveyed area as targets and ground truth data are collected by an
advanced terrestrial laser scanner (TLS). 3D geometrical and color information
can be recovered in the colorized point clouds produced by the backpack system
and the TLS, respectively. Accordingly, we provide an opportunity to benchmark
the mapping and colorization accuracy simultaneously for a mobile
multisensorial system. The dataset is approximately 800 GB in size covering
both indoor and outdoor environments. The dataset and development kits are
available at https://github.com/chenpengxin/PolyU-BPCoMa.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Detection Methods for Die Attachment and Wire Bonding Defects in Integrated Circuit Manufacturing. (arXiv:2206.07481v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07481">
<div class="article-summary-box-inner">
<span><p>Defect detection plays a vital role in the manufacturing process of
integrated circuits (ICs). Die attachment and wire bonding are two steps of the
manufacturing process that determine the quality and reliability of the power
and signal transmission in an IC. This paper presents a survey or literature
review of the methods used for detecting these defects based on different
sensing modalities used including optical, radiological, acoustical, and
infrared thermography. A discussion of the detection methods used is provided
in this survey. Both conventional and deep learning approaches for detecting
die attachment and wire bonding defects are considered along with challenges
and future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Multi-Task Networks For Occluded Pedestrian Pose Estimation. (arXiv:2206.07510v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07510">
<div class="article-summary-box-inner">
<span><p>Most of the existing works on pedestrian pose estimation do not consider
estimating the pose of an occluded pedestrians, as the annotations of the
occluded parts are not available in relevant automotive datasets. For example,
CityPersons, a well-known dataset for pedestrian detection in automotive scenes
does not provide pose annotations, whereas MS-COCO, a non-automotive dataset,
contains human pose estimation. In this work, we propose a multi-task framework
to extract pedestrian features through detection and instance segmentation
tasks performed separately on these two distributions. Thereafter, an encoder
learns pose specific features using an unsupervised instance-level domain
adaptation method for the pedestrian instances from both distributions. The
proposed framework has improved state-of-the-art performances of pose
estimation, pedestrian detection, and instance segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Body Gesture Recognition to Control a Social Robot. (arXiv:2206.07538v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07538">
<div class="article-summary-box-inner">
<span><p>In this work, we propose a gesture based language to allow humans to interact
with robots using their body in a natural way. We have created a new gesture
detection model using neural networks and a custom dataset of humans performing
a set of body gestures to train our network. Furthermore, we compare body
gesture communication with other communication channels to acknowledge the
importance of adding this knowledge to robots. The presented approach is
extensively validated in diverse simulations and real-life experiments with
non-trained volunteers. This attains remarkable results and shows that it is a
valuable framework for social robotics applications, such as human robot
collaboration or human-robot interaction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Generative Model of Neonatal Cortical Surface Development. (arXiv:2206.07542v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07542">
<div class="article-summary-box-inner">
<span><p>The neonatal cortical surface is known to be affected by preterm birth, and
the subsequent changes to cortical organisation have been associated with
poorer neurodevelopmental outcomes. Deep Generative models have the potential
to lead to clinically interpretable models of disease, but developing these on
the cortical surface is challenging since established techniques for learning
convolutional filters are inappropriate on non-flat topologies. To close this
gap, we implement a surface-based CycleGAN using mixture model CNNs (MoNet) to
translate sphericalised neonatal cortical surface features (curvature and
T1w/T2w cortical myelin) between different stages of cortical maturity. Results
show our method is able to reliably predict changes in individual patterns of
cortical organisation at later stages of gestation, validated by comparison to
longitudinal data; and translate appearance between preterm and term gestation
(&gt; 37 weeks gestation), validated through comparison with a trained
term/preterm classifier. Simulated differences in cortical maturation are
consistent with observations in the literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Reduce Change Detection to Semantic Segmentation. (arXiv:2206.07557v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07557">
<div class="article-summary-box-inner">
<span><p>Change detection (CD) aims to identify changes that occur in an image pair
taken different times. Prior methods devise specific networks from scratch to
predict change masks in pixel-level, and struggle with general segmentation
problems. In this paper, we propose a new paradigm that reduces CD to semantic
segmentation which means tailoring an existing and powerful semantic
segmentation network to solve CD. This new paradigm conveniently enjoys the
mainstream semantic segmentation techniques to deal with general segmentation
problems in CD. Hence we can concentrate on studying how to detect changes. We
propose a novel and importance insight that different change types exist in CD
and they should be learned separately. Based on it, we devise a module named
MTF to extract the change information and fuse temporal features. MTF enjoys
high interpretability and reveals the essential characteristic of CD. And most
segmentation networks can be adapted to solve the CD problems with our MTF
module. Finally, we propose C-3PO, a network to detect changes at pixel-level.
C-3PO achieves state-of-the-art performance without bells and whistles. It is
simple but effective and can be considered as a new baseline in this field. Our
code will be available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Meta-Analysis of Distributionally-Robust Models. (arXiv:2206.07565v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07565">
<div class="article-summary-box-inner">
<span><p>State-of-the-art image classifiers trained on massive datasets (such as
ImageNet) have been shown to be vulnerable to a range of both intentional and
incidental distribution shifts. On the other hand, several recent classifiers
with favorable out-of-distribution (OOD) robustness properties have emerged,
achieving high accuracy on their target tasks while maintaining their
in-distribution accuracy on challenging benchmarks. We present a meta-analysis
on a wide range of publicly released models, most of which have been published
over the last twelve months. Through this meta-analysis, we empirically
identify four main commonalities for all the best-performing OOD-robust models,
all of which illuminate the considerable promise of vision-language
pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E2V-SDE: From Asynchronous Events to Fast and Continuous Video Reconstruction via Neural Stochastic Differential Equations. (arXiv:2206.07578v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07578">
<div class="article-summary-box-inner">
<span><p>Event cameras respond to brightness changes in the scene asynchronously and
independently for every pixel. Due to the properties, these cameras have
distinct features: high dynamic range (HDR), high temporal resolution, and low
power consumption. However, the results of event cameras should be processed
into an alternative representation for computer vision tasks. Also, they are
usually noisy and cause poor performance in areas with few events. In recent
years, numerous researchers have attempted to reconstruct videos from events.
However, they do not provide good quality videos due to a lack of temporal
information from irregular and discontinuous data. To overcome these
difficulties, we introduce an E2V-SDE whose dynamics are governed in a latent
space by Stochastic differential equations (SDE). Therefore, E2V-SDE can
rapidly reconstruct images at arbitrary time steps and make realistic
predictions on unseen data. In addition, we successfully adopted a variety of
image composition techniques for improving image clarity and temporal
consistency. By conducting extensive experiments on simulated and real-scene
datasets, we verify that our model outperforms state-of-the-art approaches
under various video reconstruction settings. In terms of image quality, the
LPIPS score improves by up to 12% and the reconstruction speed is 87% higher
than that of ET-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating object detector ensembles for improving the robustness of artifact detection in endoscopic video streams. (arXiv:2206.07580v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07580">
<div class="article-summary-box-inner">
<span><p>In this contribution we use an ensemble deep-learning method for combining
the prediction of two individual one-stage detectors (i.e., YOLOv4 and Yolact)
with the aim to detect artefacts in endoscopic images. This ensemble strategy
enabled us to improve the robustness of the individual models without harming
their real-time computation capabilities. We demonstrated the effectiveness of
our approach by training and testing the two individual models and various
ensemble configurations on the "Endoscopic Artifact Detection Challenge"
dataset. Extensive experiments show the superiority, in terms of mean average
precision, of the ensemble approach over the individual models and previous
works in the state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BIO-CXRNET: A Robust Multimodal Stacking Machine Learning Technique for Mortality Risk Prediction of COVID-19 Patients using Chest X-Ray Images and Clinical Data. (arXiv:2206.07595v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07595">
<div class="article-summary-box-inner">
<span><p>Fast and accurate detection of the disease can significantly help in reducing
the strain on the healthcare facility of any country to reduce the mortality
during any pandemic. The goal of this work is to create a multimodal system
using a novel machine learning framework that uses both Chest X-ray (CXR)
images and clinical data to predict severity in COVID-19 patients. In addition,
the study presents a nomogram-based scoring technique for predicting the
likelihood of death in high-risk patients. This study uses 25 biomarkers and
CXR images in predicting the risk in 930 COVID-19 patients admitted during the
first wave of COVID-19 (March-June 2020) in Italy. The proposed multimodal
stacking technique produced the precision, sensitivity, and F1-score, of
89.03%, 90.44%, and 89.03%, respectively to identify low or high-risk patients.
This multimodal approach improved the accuracy by 6% in comparison to the CXR
image or clinical data alone. Finally, nomogram scoring system using
multivariate logistic regression -- was used to stratify the mortality risk
among the high-risk patients identified in the first stage. Lactate
Dehydrogenase (LDH), O2 percentage, White Blood Cells (WBC) Count, Age, and
C-reactive protein (CRP) were identified as useful predictor using random
forest feature selection model. Five predictors parameters and a CXR image
based nomogram score was developed for quantifying the probability of death and
categorizing them into two risk groups: survived (&lt;50%), and death (&gt;=50%),
respectively. The multi-modal technique was able to predict the death
probability of high-risk patients with an F1 score of 92.88 %. The area under
the curves for the development and validation cohorts are 0.981 and 0.939,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How GNNs Facilitate CNNs in Mining Geometric Information from Large-Scale Medical Images. (arXiv:2206.07599v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07599">
<div class="article-summary-box-inner">
<span><p>Gigapixel medical images provide massive data, both morphological textures
and spatial information, to be mined. Due to the large data scale in histology,
deep learning methods play an increasingly significant role as feature
extractors. Existing solutions heavily rely on convolutional neural networks
(CNNs) for global pixel-level analysis, leaving the underlying local geometric
structure such as the interaction between cells in the tumor microenvironment
unexplored. The topological structure in medical images, as proven to be
closely related to tumor evolution, can be well characterized by graphs. To
obtain a more comprehensive representation for downstream oncology tasks, we
propose a fusion framework for enhancing the global image-level representation
captured by CNNs with the geometry of cell-level spatial information learned by
graph neural networks (GNN). The fusion layer optimizes an integration between
collaborative features of global images and cell graphs. Two fusion strategies
have been developed: one with MLP which is simple but turns out efficient
through fine-tuning, and the other with Transformer gains a champion in fusing
multiple networks. We evaluate our fusion strategies on histology datasets
curated from large patient cohorts of colorectal and gastric cancers for three
biomarker prediction tasks. Both two models outperform plain CNNs or GNNs,
reaching a consistent AUC improvement of more than 5% on various network
backbones. The experimental results yield the necessity for combining
image-level morphological features with cell spatial relations in medical image
analysis. Codes are available at https://github.com/yiqings/HEGnnEnhanceCnn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real3D-Aug: Point Cloud Augmentation by Placing Real Objects with Occlusion Handling for 3D Detection and Segmentation. (arXiv:2206.07634v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07634">
<div class="article-summary-box-inner">
<span><p>Object detection and semantic segmentation with the 3D lidar point cloud data
require expensive annotation. We propose a data augmentation method that takes
advantage of already annotated data multiple times. We propose an augmentation
framework that reuses real data, automatically finds suitable placements in the
scene to be augmented, and handles occlusions explicitly. Due to the usage of
the real data, the scan points of newly inserted objects in augmentation
sustain the physical characteristics of the lidar, such as intensity and
raydrop. The pipeline proves competitive in training top-performing models for
3D object detection and semantic segmentation. The new augmentation provides a
significant performance gain in rare and essential classes, notably 6.65%
average precision gain for "Hard" pedestrian class in KITTI object detection or
2.14 mean IoU gain in the SemanticKITTI segmentation challenge over the state
of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone. (arXiv:2206.07643v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07643">
<div class="article-summary-box-inner">
<span><p>Vision-language (VL) pre-training has recently received considerable
attention. However, most existing end-to-end pre-training approaches either
only aim to tackle VL tasks such as image-text retrieval, visual question
answering (VQA) and image captioning that test high-level understanding of
images, or only target region-level understanding for tasks such as phrase
grounding and object detection. We present FIBER (Fusion-In-the-Backbone-based
transformER), a new VL model architecture that can seamlessly handle both these
types of tasks. Instead of having dedicated transformer layers for fusion after
the uni-modal backbones, FIBER pushes multimodal fusion deep into the model by
inserting cross-attention into the image and text backbones, bringing gains in
terms of memory and performance. In addition, unlike previous work that is
either only pre-trained on image-text data or on fine-grained data with
box-level annotations, we present a two-stage pre-training strategy that uses
both these kinds of data efficiently: (i) coarse-grained pre-training based on
image-text data; followed by (ii) fine-grained pre-training based on
image-text-box data. We conduct comprehensive experiments on a wide range of VL
tasks, ranging from VQA, image captioning, and retrieval, to phrase grounding,
referring expression comprehension, and object detection. Using deep multimodal
fusion coupled with the two-stage pre-training, FIBER provides consistent
performance improvements over strong baselines across all tasks, often
outperforming methods using magnitudes more data. Code is available at
https://github.com/microsoft/FIBER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SP-ViT: Learning 2D Spatial Priors for Vision Transformers. (arXiv:2206.07662v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07662">
<div class="article-summary-box-inner">
<span><p>Recently, transformers have shown great potential in image classification and
established state-of-the-art results on the ImageNet benchmark. However,
compared to CNNs, transformers converge slowly and are prone to overfitting in
low-data regimes due to the lack of spatial inductive biases. Such spatial
inductive biases can be especially beneficial since the 2D structure of an
input image is not well preserved in transformers. In this work, we present
Spatial Prior-enhanced Self-Attention (SP-SA), a novel variant of vanilla
Self-Attention (SA) tailored for vision transformers. Spatial Priors (SPs) are
our proposed family of inductive biases that highlight certain groups of
spatial relations. Unlike convolutional inductive biases, which are forced to
focus exclusively on hard-coded local regions, our proposed SPs are learned by
the model itself and take a variety of spatial relations into account.
Specifically, the attention score is calculated with emphasis on certain kinds
of spatial relations at each head, and such learned spatial foci can be
complementary to each other. Based on SP-SA we propose the SP-ViT family, which
consistently outperforms other ViT models with similar GFlops or parameters.
Our largest model SP-ViT-L achieves a record-breaking 86.3% Top-1 accuracy with
a reduction in the number of parameters by almost 50% compared to previous
state-of-the-art model (150M for SP-ViT-L vs 271M for CaiT-M-36) among all
ImageNet-1K models trained on 224x224 and fine-tuned on 384x384 resolution w/o
extra data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CRISP - Reliable Uncertainty Estimation for Medical Image Segmentation. (arXiv:2206.07664v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07664">
<div class="article-summary-box-inner">
<span><p>Accurate uncertainty estimation is a critical need for the medical imaging
community. A variety of methods have been proposed, all direct extensions of
classification uncertainty estimations techniques. The independent pixel-wise
uncertainty estimates, often based on the probabilistic interpretation of
neural networks, do not take into account anatomical prior knowledge and
consequently provide sub-optimal results to many segmentation tasks. For this
reason, we propose CRISP a ContRastive Image Segmentation for uncertainty
Prediction method. At its core, CRISP implements a contrastive method to learn
a joint latent space which encodes a distribution of valid segmentations and
their corresponding images. We use this joint latent space to compare
predictions to thousands of latent vectors and provide anatomically consistent
uncertainty maps. Comprehensive studies performed on four medical image
databases involving different modalities and organs underlines the superiority
of our method compared to state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Sequence Interface for Vision Tasks. (arXiv:2206.07669v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07669">
<div class="article-summary-box-inner">
<span><p>While language tasks are naturally expressed in a single, unified, modeling
framework, i.e., generating sequences of tokens, this has not been the case in
computer vision. As a result, there is a proliferation of distinct
architectures and loss functions for different vision tasks. In this work we
show that a diverse set of "core" computer vision tasks can also be unified if
formulated in terms of a shared pixel-to-sequence interface. We focus on four
tasks, namely, object detection, instance segmentation, keypoint detection, and
image captioning, all with diverse types of outputs, e.g., bounding boxes or
dense masks. Despite that, by formulating the output of each task as a sequence
of discrete tokens with a unified interface, we show that one can train a
neural network with a single model architecture and loss function on all these
tasks, with no task-specific customization. To solve a specific task, we use a
short prompt as task description, and the sequence output adapts to the prompt
so it can produce task-specific output. We show that such a model can achieve
competitive performance compared to well-established task-specific models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AVATAR: Unconstrained Audiovisual Speech Recognition. (arXiv:2206.07684v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07684">
<div class="article-summary-box-inner">
<span><p>Audio-visual automatic speech recognition (AV-ASR) is an extension of ASR
that incorporates visual cues, often from the movements of a speaker's mouth.
Unlike works that simply focus on the lip motion, we investigate the
contribution of entire visual frames (visual actions, objects, background
etc.). This is particularly useful for unconstrained videos, where the speaker
is not necessarily visible. To solve this task, we propose a new
sequence-to-sequence AudioVisual ASR TrAnsformeR (AVATAR) which is trained
end-to-end from spectrograms and full-frame RGB. To prevent the audio stream
from dominating training, we propose different word-masking strategies, thereby
encouraging our model to pay attention to the visual stream. We demonstrate the
contribution of the visual modality on the How2 AV-ASR benchmark, especially in
the presence of simulated noise, and show that our model outperforms all other
prior work by a large margin. Finally, we also create a new, real-world test
bed for AV-ASR called VisSpeech, which demonstrates the contribution of the
visual modality under challenging audio conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Residual Sparsity Connection Learning for Efficient Video Super-Resolution. (arXiv:2206.07687v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07687">
<div class="article-summary-box-inner">
<span><p>Lighter and faster models are crucial for the deployment of video
super-resolution (VSR) on resource-limited devices, e.g., smartphones and
wearable devices. In this paper, we develop Residual Sparsity Connection
Learning (RSCL), a structured pruning scheme, to reduce the redundancy of
convolution kernels and obtain a compact VSR network with a minor performance
drop. However, residual blocks require the pruned filter indices of skip and
residual connections to be the same, which is tricky for pruning. Thus, to
mitigate the pruning restrictions of residual blocks, we design a Residual
Sparsity Connection (RSC) scheme by preserving the feature channels and only
operating on the important channels. Moreover, for the pixel-shuffle operation,
we design a special pruning scheme by grouping several filters as pruning units
to guarantee the accuracy of feature channel-space conversion after pruning. In
addition, we introduce Temporal Finetuning (TF) to reduce the pruning error
amplification of hidden states with temporal propagation. Extensive experiments
show that the proposed RSCL significantly outperforms recent methods
quantitatively and qualitatively. Codes and models will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Video Tokens @ Ego4D PNR Temporal Localization Challenge 2022. (arXiv:2206.07689v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07689">
<div class="article-summary-box-inner">
<span><p>This technical report describes the SViT approach for the Ego4D Point of No
Return (PNR) Temporal Localization Challenge. We propose a learning framework
StructureViT (SViT for short), which demonstrates how utilizing the structure
of a small number of images only available during training can improve a video
model. SViT relies on two key insights. First, as both images and videos
contain structured information, we enrich a transformer model with a set of
\emph{object tokens} that can be used across images and videos. Second, the
scene representations of individual frames in video should "align" with those
of still images. This is achieved via a "Frame-Clip Consistency" loss, which
ensures the flow of structured information between images and videos. SViT
obtains strong performance on the challenge test set with 0.656 absolute
temporal localization error.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELUDE: Generating interpretable explanations via a decomposition into labelled and unlabelled features. (arXiv:2206.07690v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07690">
<div class="article-summary-box-inner">
<span><p>Deep learning models have achieved remarkable success in different areas of
machine learning over the past decade; however, the size and complexity of
these models make them difficult to understand. In an effort to make them more
interpretable, several recent works focus on explaining parts of a deep neural
network through human-interpretable, semantic attributes. However, it may be
impossible to completely explain complex models using only semantic attributes.
In this work, we propose to augment these attributes with a small set of
uninterpretable features. Specifically, we develop a novel explanation
framework ELUDE (Explanation via Labelled and Unlabelled DEcomposition) that
decomposes a model's prediction into two parts: one that is explainable through
a linear combination of the semantic attributes, and another that is dependent
on the set of uninterpretable features. By identifying the latter, we are able
to analyze the "unexplained" portion of the model, obtaining insights into the
information used by the model. We show that the set of unlabelled features can
generalize to multiple models trained with the same feature space and compare
our work to two popular attribute-oriented methods, Interpretable Basis
Decomposition and Concept Bottleneck, and discuss the additional insights ELUDE
provides.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Data Mixing Prior for Improving Self-Supervised Learning. (arXiv:2206.07692v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07692">
<div class="article-summary-box-inner">
<span><p>Data mixing (e.g., Mixup, Cutmix, ResizeMix) is an essential component for
advancing recognition models. In this paper, we focus on studying its
effectiveness in the self-supervised setting. By noticing the mixed images that
share the same source images are intrinsically related to each other, we hereby
propose SDMP, short for $\textbf{S}$imple $\textbf{D}$ata $\textbf{M}$ixing
$\textbf{P}$rior, to capture this straightforward yet essential prior, and
position such mixed images as additional $\textbf{positive pairs}$ to
facilitate self-supervised representation learning. Our experiments verify that
the proposed SDMP enables data mixing to help a set of self-supervised learning
frameworks (e.g., MoCo) achieve better accuracy and out-of-distribution
robustness. More notably, our SDMP is the first method that successfully
leverages data mixing to improve (rather than hurt) the performance of Vision
Transformers in the self-supervised setting. Code is publicly available at
https://github.com/OliverRensu/SDMP
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids. (arXiv:2206.07695v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07695">
<div class="article-summary-box-inner">
<span><p>State-of-the-art 3D-aware generative models rely on coordinate-based MLPs to
parameterize 3D radiance fields. While demonstrating impressive results,
querying an MLP for every sample along each ray leads to slow rendering.
Therefore, existing approaches often render low-resolution feature maps and
process them with an upsampling network to obtain the final image. Albeit
efficient, neural rendering often entangles viewpoint and content such that
changing the camera pose results in unwanted changes of geometry or appearance.
Motivated by recent results in voxel-based novel view synthesis, we investigate
the utility of sparse voxel grid representations for fast and 3D-consistent
generative modeling in this paper. Our results demonstrate that monolithic MLPs
can indeed be replaced by 3D convolutions when combining sparse voxel grids
with progressive growing, free space pruning and appropriate regularization. To
obtain a compact representation of the scene and allow for scaling to higher
voxel resolutions, our model disentangles the foreground object (modeled in 3D)
from the background (modeled in 2D). In contrast to existing approaches, our
method requires only a single forward pass to generate a full 3D scene. It
hence allows for efficient rendering from arbitrary viewpoints while yielding
3D consistent results with high visual fidelity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffusion Models for Video Prediction and Infilling. (arXiv:2206.07696v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07696">
<div class="article-summary-box-inner">
<span><p>To predict and anticipate future outcomes or reason about missing information
in a sequence is a key ability for agents to be able to make intelligent
decisions. This requires strong temporally coherent generative capabilities.
Diffusion models have shown huge success in several generative tasks lately,
but have not been extensively explored in the video domain. We present
Random-Mask Video Diffusion (RaMViD), which extends image diffusion models to
videos using 3D convolutions, and introduces a new conditioning technique
during training. By varying the mask we condition on, the model is able to
perform video prediction, infilling and upsampling. Since we do not use
concatenation to condition on a mask, as done in most conditionally trained
diffusion models, we are able to decrease the memory footprint. We evaluated
the model on two benchmark datasets for video prediction and one for video
generation on which we achieved competitive results. On Kinetics-600 we
achieved state-of-the-art for video prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Deformable Voxel Grid for Fast Optimization of Dynamic View Synthesis. (arXiv:2206.07698v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07698">
<div class="article-summary-box-inner">
<span><p>Recently, Neural Radiance Fields (NeRF) is revolutionizing the task of novel
view synthesis (NVS) for its superior performance. However, NeRF and its
variants generally require a lengthy per-scene training procedure, where a
multi-layer perceptron (MLP) is fitted to the captured images. To remedy the
challenge, the voxel-grid representation has been proposed to significantly
speed up the training. However, these existing methods can only deal with
static scenes. How to develop an efficient and accurate dynamic view synthesis
method remains an open problem. Extending the methods for static scenes to
dynamic scenes is not straightforward as both the scene geometry and appearance
change over time. In this paper, built on top of the recent advances in
voxel-grid optimization, we propose a fast deformable radiance field method to
handle dynamic scenes. Our method consists of two modules. The first module
adopts a deformation grid to store 3D dynamic features, and a light-weight MLP
for decoding the deformation that maps a 3D point in observation space to the
canonical space using the interpolated features. The second module contains a
density and a color grid to model the geometry and density of the scene. The
occlusion is explicitly modeled to further improve the rendering quality.
Experimental results show that our method achieves comparable performance to
D-NeRF using only 20 minutes for training, which is more than 70x faster than
D-NeRF, clearly demonstrating the efficiency of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prefix Language Models are Unified Modal Learners. (arXiv:2206.07699v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07699">
<div class="article-summary-box-inner">
<span><p>With the success of vision-language pre-training, we have witnessed the
state-of-the-art has been pushed on multi-modal understanding and generation.
However, the current pre-training paradigm is either incapable of targeting all
modalities at once (e.g., text generation and image generation), or requires
multi-fold well-designed tasks which significantly limits the scalability. We
demonstrate that a unified modal model could be learned with a prefix language
modeling objective upon text and image sequences. Thanks to the simple but
powerful pre-training paradigm, our proposed model, DaVinci, is simple to
train, scalable to huge data, and adaptable to a variety of downstream tasks
across modalities (language / vision / vision+language), types (understanding /
generation) and settings (e.g., zero-shot, fine-tuning, linear evaluation) with
a single unified architecture. DaVinci achieves the competitive performance on
a wide range of 26 understanding / generation tasks, and outperforms previous
unified vision-language models on most tasks, including ImageNet classification
(+1.6%), VQAv2 (+1.4%), COCO caption generation (BLEU@4 +1.1%, CIDEr +1.5%) and
COCO image generation (IS +0.9%, FID -1.0%), at the comparable model and data
scale. Furthermore, we offer a well-defined benchmark for future research by
reporting the performance on different scales of the pre-training dataset on a
heterogeneous and wide distribution coverage. Our results establish new,
stronger baselines for future comparisons at different data scales and shed
light on the difficulties of comparing VLP models more generally.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Siamese ConvNets. (arXiv:2206.07700v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07700">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning has shown superior performances over supervised
methods on various vision benchmarks. The siamese network, which encourages
embeddings to be invariant to distortions, is one of the most successful
self-supervised visual representation learning approaches. Among all the
augmentation methods, masking is the most general and straightforward method
that has the potential to be applied to all kinds of input and requires the
least amount of domain knowledge. However, masked siamese networks require
particular inductive bias and practically only work well with Vision
Transformers. This work empirically studies the problems behind masked siamese
networks with ConvNets. We propose several empirical designs to overcome these
problems gradually. Our method performs competitively on low-shot image
classification and outperforms previous methods on object detection benchmarks.
We discuss several remaining issues and hope this work can provide useful data
points for future general-purpose self-supervised learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Waymo Open Dataset: Panoramic Video Panoptic Segmentation. (arXiv:2206.07704v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07704">
<div class="article-summary-box-inner">
<span><p>Panoptic image segmentation is the computer vision task of finding groups of
pixels in an image and assigning semantic classes and object instance
identifiers to them. Research in image segmentation has become increasingly
popular due to its critical applications in robotics and autonomous driving.
The research community thereby relies on publicly available benchmark dataset
to advance the state-of-the-art in computer vision. Due to the high costs of
densely labeling the images, however, there is a shortage of publicly available
ground truth labels that are suitable for panoptic segmentation. The high
labeling costs also make it challenging to extend existing datasets to the
video domain and to multi-camera setups. We therefore present the Waymo Open
Dataset: Panoramic Video Panoptic Segmentation Dataset, a large-scale dataset
that offers high-quality panoptic segmentation labels for autonomous driving.
We generate our dataset using the publicly available Waymo Open Dataset,
leveraging the diverse set of camera images. Our labels are consistent over
time for video processing and consistent across multiple cameras mounted on the
vehicles for full panoramic scene understanding. Specifically, we offer labels
for 28 semantic categories and 2,860 temporal sequences that were captured by
five cameras mounted on autonomous vehicles driving in three different
geographical locations, leading to a total of 100k labeled camera images. To
the best of our knowledge, this makes our dataset an order of magnitude larger
than existing datasets that offer video panoptic segmentation labels. We
further propose a new benchmark for Panoramic Video Panoptic Segmentation and
establish a number of strong baselines based on the DeepLab family of models.
We will make the benchmark and the code publicly available. Find the dataset at
https://waymo.com/open.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LET-3D-AP: Longitudinal Error Tolerant 3D Average Precision for Camera-Only 3D Detection. (arXiv:2206.07705v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07705">
<div class="article-summary-box-inner">
<span><p>The popular object detection metric 3D Average Precision (3D AP) relies on
the intersection over union between predicted bounding boxes and ground truth
bounding boxes. However, depth estimation based on cameras has limited
accuracy, which may cause otherwise reasonable predictions that suffer from
such longitudinal localization errors to be treated as false positives and
false negatives. We therefore propose variants of the popular 3D AP metric that
are designed to be more permissive with respect to depth estimation errors.
Specifically, our novel longitudinal error tolerant metrics, LET-3D-AP and
LET-3D-APL, allow longitudinal localization errors of the predicted bounding
boxes up to a given tolerance. The proposed metrics have been used in the Waymo
Open Dataset 3D Camera-Only Detection Challenge. We believe that they will
facilitate advances in the field of camera-only 3D detection by providing more
informative performance signals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Frequency Modeling for Self-Supervised Visual Pre-Training. (arXiv:2206.07706v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07706">
<div class="article-summary-box-inner">
<span><p>We present Masked Frequency Modeling (MFM), a unified frequency-domain-based
approach for self-supervised pre-training of visual models. Instead of randomly
inserting mask tokens to the input embeddings in the spatial domain, in this
paper, we shift the perspective to the frequency domain. Specifically, MFM
first masks out a portion of frequency components of the input image and then
predicts the missing frequencies on the frequency spectrum. Our key insight is
that predicting masked components in the frequency domain is more ideal to
reveal underlying image patterns rather than predicting masked patches in the
spatial domain, due to the heavy spatial redundancy. Our findings suggest that
with the right configuration of mask-and-predict strategy, both the structural
information within high-frequency components and the low-level statistics among
low-frequency counterparts are useful in learning good representations. For the
first time, MFM demonstrates that, for both ViT and CNN, a simple non-Siamese
framework can learn meaningful representations even using none of the
following: (i) extra data, (ii) extra model, (iii) mask token. Experimental
results on ImageNet and several robustness benchmarks show the competitive
performance and advanced robustness of MFM compared with recent masked image
modeling approaches. Furthermore, we also comprehensively investigate the
effectiveness of classical image restoration tasks for representation learning
from a unified frequency perspective and reveal their intriguing relations with
our MFM approach. Project page:
https://www.mmlab-ntu.com/project/mfm/index.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variable Bitrate Neural Fields. (arXiv:2206.07707v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07707">
<div class="article-summary-box-inner">
<span><p>Neural approximations of scalar and vector fields, such as signed distance
functions and radiance fields, have emerged as accurate, high-quality
representations. State-of-the-art results are obtained by conditioning a neural
approximation with a lookup from trainable feature grids that take on part of
the learning task and allow for smaller, more efficient neural networks.
Unfortunately, these feature grids usually come at the cost of significantly
increased memory consumption compared to stand-alone neural network models. We
present a dictionary method for compressing such feature grids, reducing their
memory consumption by up to 100x and permitting a multiresolution
representation which can be useful for out-of-core streaming. We formulate the
dictionary optimization as a vector-quantized auto-decoder problem which lets
us learn end-to-end discrete neural representations in a space where no direct
supervision is available and with dynamic topology and structure. Our source
code will be available at https://github.com/nv-tlabs/vqad.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PlanarRecon: Real-time 3D Plane Detection and Reconstruction from Posed Monocular Videos. (arXiv:2206.07710v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07710">
<div class="article-summary-box-inner">
<span><p>We present PlanarRecon -- a novel framework for globally coherent detection
and reconstruction of 3D planes from a posed monocular video. Unlike previous
works that detect planes in 2D from a single image, PlanarRecon incrementally
detects planes in 3D for each video fragment, which consists of a set of key
frames, from a volumetric representation of the scene using neural networks. A
learning-based tracking and fusion module is designed to merge planes from
previous fragments to form a coherent global plane reconstruction. Such design
allows PlanarRecon to integrate observations from multiple views within each
fragment and temporal information across different ones, resulting in an
accurate and coherent reconstruction of the scene abstraction with
low-polygonal geometry. Experiments show that the proposed approach achieves
state-of-the-art performances on the ScanNet dataset while being real-time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Are How You Walk: Uncooperative MoCap Gait Identification for Video Surveillance with Incomplete and Noisy Data. (arXiv:1706.09443v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1706.09443">
<div class="article-summary-box-inner">
<span><p>This work offers a design of a video surveillance system based on a soft
biometric -- gait identification from MoCap data. The main focus is on two
substantial issues of the video surveillance scenario: (1) the walkers do not
cooperate in providing learning data to establish their identities and (2) the
data are often noisy or incomplete. We show that only a few examples of human
gait cycles are required to learn a projection of raw MoCap data onto a
low-dimensional sub-space where the identities are well separable. Latent
features learned by Maximum Margin Criterion (MMC) method discriminate better
than any collection of geometric features. The MMC method is also highly robust
to noisy data and works properly even with only a fraction of joints tracked.
The overall workflow of the design is directly applicable for a day-to-day
operation based on the available MoCap technology and algorithms for gait
analysis. In the concept we introduce, a walker's identity is represented by a
cluster of gait data collected at their incidents within the surveillance
system: They are how they walk.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Understanding Adversarial Robustness of Optical Flow Networks. (arXiv:2103.16255v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16255">
<div class="article-summary-box-inner">
<span><p>Recent work demonstrated the lack of robustness of optical flow networks to
physical patch-based adversarial attacks. The possibility to physically attack
a basic component of automotive systems is a reason for serious concerns. In
this paper, we analyze the cause of the problem and show that the lack of
robustness is rooted in the classical aperture problem of optical flow
estimation in combination with bad choices in the details of the network
architecture. We show how these mistakes can be rectified in order to make
optical flow networks robust to physical patch-based attacks. Additionally, we
take a look at global white-box attacks in the scope of optical flow. We find
that targeted white-box attacks can be crafted to bias flow estimation models
towards any desired output, but this requires access to the input images and
model weights. However, in the case of universal attacks, we find that optical
flow networks are robust. Code is available at
https://github.com/lmb-freiburg/understanding_flow_robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graphical Modeling for Multi-Source Domain Adaptation. (arXiv:2104.13057v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13057">
<div class="article-summary-box-inner">
<span><p>Multi-Source Domain Adaptation (MSDA) focuses on transferring the knowledge
from multiple source domains to the target domain, which is a more practical
and challenging problem compared to the conventional single-source domain
adaptation. In this problem, it is essential to model multiple source domains
and target domain jointly, and an effective domain combination scheme is also
highly required. The graphical structure among different domains is useful to
tackle these challenges, in which the interdependency among various
instances/categories can be effectively modeled. In this work, we propose two
types of graphical models, i.e. Conditional Random Field for MSDA (CRF-MSDA)
and Markov Random Field for MSDA (MRF-MSDA), for cross-domain joint modeling
and learnable domain combination. In a nutshell, given an observation set
composed of a query sample and the semantic prototypes (i.e. representative
category embeddings) on various domains, the CRF-MSDA model seeks to learn the
joint distribution of labels conditioned on the observations. We attain this
goal by constructing a relational graph over all observations and conducting
local message passing on it. By comparison, MRF-MSDA aims to model the joint
distribution of observations over different Markov networks via an energy-based
formulation, and it can naturally perform label prediction by summing the joint
likelihoods over several specific networks. Compared to the CRF-MSDA
counterpart, the MRF-MSDA model is more expressive and possesses lower
computational cost. We evaluate these two models on four standard benchmark
data sets of MSDA with distinct domain shift and data complexity, and both
models achieve superior performance over existing methods on all benchmarks. In
addition, the analytical studies illustrate the effect of different model
components and provide insights about how the cross-domain joint modeling
performs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformers with Hierarchical Attention. (arXiv:2106.03180v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03180">
<div class="article-summary-box-inner">
<span><p>This paper tackles the low-efficiency flaw of the vision transformer caused
by the high computational/space complexity in Multi-Head Self-Attention (MHSA).
To this end, we propose the Hierarchical MHSA (H-MHSA), whose representation is
computed in a hierarchical manner. Specifically, we first divide the input
image into patches as commonly done, and each patch is viewed as a token. Then,
the proposed H-MHSA learns token relationships within local patches, serving as
local relationship modeling. Then, the small patches are merged into larger
ones, and H-MHSA models the global dependencies for the small number of the
merged tokens. At last, the local and global attentive features are aggregated
to obtain features with powerful representation capacity. Since we only
calculate attention for a limited number of tokens at each step, the
computational load is reduced dramatically. Hence, H-MHSA can efficiently model
global relationships among tokens without sacrificing fine-grained information.
With the H-MHSA module incorporated, we build a family of
Hierarchical-Attention-based Transformer Networks, namely HAT-Net. To
demonstrate the superiority of HAT-Net in scene understanding, we conduct
extensive experiments on fundamental vision tasks, including image
classification, semantic segmentation, object detection, and instance
segmentation. Therefore, HAT-Net provides a new perspective for the vision
transformer. Code and pretrained models are available at
https://github.com/yun-liu/HAT-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuralMVS: Bridging Multi-View Stereo and Novel View Synthesis. (arXiv:2108.03880v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03880">
<div class="article-summary-box-inner">
<span><p>Multi-View Stereo (MVS) is a core task in 3D computer vision. With the surge
of novel deep learning methods, learned MVS has surpassed the accuracy of
classical approaches, but still relies on building a memory intensive dense
cost volume. Novel View Synthesis (NVS) is a parallel line of research and has
recently seen an increase in popularity with Neural Radiance Field (NeRF)
models, which optimize a per scene radiance field. However, NeRF methods do not
generalize to novel scenes and are slow to train and test. We propose to bridge
the gap between these two methodologies with a novel network that can recover
3D scene geometry as a distance function, together with high-resolution color
images. Our method uses only a sparse set of images as input and can generalize
well to novel scenes. Additionally, we propose a coarse-to-fine sphere tracing
approach in order to significantly increase speed. We show on various datasets
that our method reaches comparable accuracy to per-scene optimized methods
while being able to generalize and running significantly faster. We provide the
source code at https://github.com/AIS-Bonn/neural_mvs
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PhysGNN: A Physics-Driven Graph Neural Network Based Model for Predicting Soft Tissue Deformation in Image-Guided Neurosurgery. (arXiv:2109.04352v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04352">
<div class="article-summary-box-inner">
<span><p>Correctly capturing intraoperative brain shift in image-guided neurosurgical
procedures is a critical task for aligning preoperative data with
intraoperative geometry for ensuring accurate surgical navigation. While the
finite element method (FEM) is a proven technique to effectively approximate
soft tissue deformation through biomechanical formulations, their degree of
success boils down to a trade-off between accuracy and speed. To circumvent
this problem, the most recent works in this domain have proposed leveraging
data-driven models obtained by training various machine learning algorithms,
e.g. random forests, artificial neural networks (ANNs), with the results of
finite element analysis (FEA) to speed up tissue deformation approximations by
prediction. These methods, however, do not account for the structure of the
finite element (FE) mesh during training that provides information on node
connectivities as well as the distance between them, which can aid with
approximating tissue deformation based on the proximity of force load points
with the rest of the mesh nodes. Therefore, this work proposes a novel
framework, PhysGNN, a data-driven model that approximates the solution of FEM
by leveraging graph neural networks (GNNs), which are capable of accounting for
the mesh structural information and inductive learning over unstructured grids
and complex topological structures. Empirically, we demonstrate that the
proposed architecture, PhysGNN, promises accurate and fast soft tissue
deformation approximations and is competitive with the state-of-the-art (SOTA)
algorithms while promising enhanced computational feasibility, therefore
suitable for neurosurgical settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ripple Attention for Visual Perception with Sub-quadratic Complexity. (arXiv:2110.02453v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02453">
<div class="article-summary-box-inner">
<span><p>Transformer architectures are now central to sequence modeling tasks. At its
heart is the attention mechanism, which enables effective modeling of long-term
dependencies in a sequence. Recently, transformers have been successfully
applied in the computer vision domain, where 2D images are first segmented into
patches and then treated as 1D sequences. Such linearization, however, impairs
the notion of spatial locality in images, which bears important visual clues.
To bridge the gap, we propose ripple attention, a sub-quadratic attention
mechanism for vision transformers. Built upon the recent kernel-based efficient
attention mechanisms, we design a novel dynamic programming algorithm that
weights contributions of different tokens to a query with respect to their
relative spatial distances in the 2D space in linear observed time. Extensive
experiments and analyses demonstrate the effectiveness of ripple attention on
various visual tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shifting Capsule Networks from the Cloud to the Deep Edge. (arXiv:2110.02911v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02911">
<div class="article-summary-box-inner">
<span><p>Capsule networks (CapsNets) are an emerging trend in image processing. In
contrast to a convolutional neural network, CapsNets are not vulnerable to
object deformation, as the relative spatial information of the objects is
preserved across the network. However, their complexity is mainly related to
the capsule structure and the dynamic routing mechanism, which makes it almost
unreasonable to deploy a CapsNet, in its original form, in a
resource-constrained device powered by a small microcontroller (MCU). In an era
where intelligence is rapidly shifting from the cloud to the edge, this high
complexity imposes serious challenges to the adoption of CapsNets at the very
edge. To tackle this issue, we present an API for the execution of quantized
CapsNets in Arm Cortex-M and RISC-V MCUs. Our software kernels extend the Arm
CMSIS-NN and RISC-V PULP-NN to support capsule operations with 8-bit integers
as operands. Along with it, we propose a framework to perform post-training
quantization of a CapsNet. Results show a reduction in memory footprint of
almost 75%, with accuracy loss ranging from 0.07% to 0.18%. In terms of
throughput, our Arm Cortex-M API enables the execution of primary capsule and
capsule layers with medium-sized kernels in just 119.94 and 90.60 milliseconds
(ms), respectively (STM32H755ZIT6U, Cortex-M7 @ 480 MHz). For the GAP-8 SoC
(RISC-V RV32IMCXpulp @ 170 MHz), the latency drops to 7.02 and 38.03 ms,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TLDR: Twin Learning for Dimensionality Reduction. (arXiv:2110.09455v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09455">
<div class="article-summary-box-inner">
<span><p>Dimensionality reduction methods are unsupervised approaches which learn
low-dimensional spaces where some properties of the initial space, typically
the notion of "neighborhood", are preserved. Such methods usually require
propagation on large k-NN graphs or complicated optimization solvers. On the
other hand, self-supervised learning approaches, typically used to learn
representations from scratch, rely on simple and more scalable frameworks for
learning. In this paper, we propose TLDR, a dimensionality reduction method for
generic input spaces that is porting the recent self-supervised learning
framework of Zbontar et al. (2021) to the specific task of dimensionality
reduction, over arbitrary representations. We propose to use nearest neighbors
to build pairs from a training set and a redundancy reduction loss to learn an
encoder that produces representations invariant across such pairs. TLDR is a
method that is simple, easy to train, and of broad applicability; it consists
of an offline nearest neighbor computation step that can be highly
approximated, and a straightforward learning process. Aiming for scalability,
we focus on improving linear dimensionality reduction, and show consistent
gains on image and document retrieval tasks, e.g. gaining +4% mAP over PCA on
ROxford for GeM- AP, improving the performance of DINO on ImageNet or retaining
it with a 10x compression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Network Kalman filtering for 3D object tracking from linear array ultrasound data. (arXiv:2111.09631v3 [stat.AP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09631">
<div class="article-summary-box-inner">
<span><p>Many interventional surgical procedures rely on medical imaging to visualise
and track instruments. Such imaging methods not only need to be real-time
capable, but also provide accurate and robust positional information. In
ultrasound applications, typically only two-dimensional data from a linear
array are available, and as such obtaining accurate positional estimation in
three dimensions is non-trivial. In this work, we first train a neural network,
using realistic synthetic training data, to estimate the out-of-plane offset of
an object with the associated axial aberration in the reconstructed ultrasound
image. The obtained estimate is then combined with a Kalman filtering approach
that utilises positioning estimates obtained in previous time-frames to improve
localisation robustness and reduce the impact of measurement noise. The
accuracy of the proposed method is evaluated using simulations, and its
practical applicability is demonstrated on experimental data obtained using a
novel optical ultrasound imaging setup. Accurate and robust positional
information is provided in real-time. Axial and lateral coordinates for
out-of-plane objects are estimated with a mean error of 0.1mm for simulated
data and a mean error of 0.2mm for experimental data. Three-dimensional
localisation is most accurate for elevational distances larger than 1mm, with a
maximum distance of 6mm considered for a 25mm aperture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAE-DET: Revisiting Maximum Entropy Principle in Zero-Shot NAS for Efficient Object Detection. (arXiv:2111.13336v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13336">
<div class="article-summary-box-inner">
<span><p>In object detection, the detection backbone consumes more than half of the
overall inference cost. Recent researches attempt to reduce this cost by
optimizing the backbone architecture with the help of Neural Architecture
Search (NAS). However, existing NAS methods for object detection require
hundreds to thousands of GPU hours of searching, making them impractical in
fast-paced research and development. In this work, we propose a novel zero-shot
NAS method to address this issue. The proposed method, named MAE-DET,
automatically designs efficient detection backbones via the Maximum Entropy
Principle without training network parameters, reducing the architecture design
cost to nearly zero yet delivering the state-of-the-art (SOTA) performance.
Under the hood, MAE-DET maximizes the differential entropy of detection
backbones, leading to a better feature extractor for object detection under the
same computational budgets. After merely one GPU day of fully automatic design,
MAE-DET innovates SOTA detection backbones on multiple detection benchmark
datasets with little human intervention. Comparing to ResNet-50 backbone,
MAE-DET is $+2.0\%$ better in mAP when using the same amount of
FLOPs/parameters, and is $1.54$ times faster on NVIDIA V100 at the same mAP.
Code and pre-trained models are available at
https://github.com/alibaba/lightweight-neuralarchitecture-search.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning a model of shape selectivity in V4 cells reveals shape encoding mechanisms in the brain. (arXiv:2111.14250v3 [q-bio.NC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14250">
<div class="article-summary-box-inner">
<span><p>The mechanisms involved in transforming early visual signals to curvature
representations in V4 are unknown. We propose a hierarchical model that reveals
V1/V2 encodings that are essential components for this transformation to the
reported curvature representations in V4. Then, by relaxing the often-imposed
prior of a single Gaussian, V4 shape selectivity is learned in the last layer
of the hierarchy from Macaque V4 responses. We found that V4 cells integrate
multiple shape parts from the full spatial extent of their receptive fields
with similar excitatory and inhibitory contributions. Our results uncover new
details in existing data about shape selectivity in V4 neurons that with
further experiments can enhance our understanding of processing in this area.
Accordingly, we propose designs for a stimulus set that allow removing shape
parts without disturbing the curvature signal to isolate part contributions to
V4 responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Facial-Sketch Synthesis: A New Challenge. (arXiv:2112.15439v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15439">
<div class="article-summary-box-inner">
<span><p>This paper aims to conduct a comprehensive study on facial-sketch synthesis
(FSS). However, due to the high costs of obtaining hand-drawn sketch datasets,
there lacks a complete benchmark for assessing the development of FSS
algorithms over the last decade. We first introduce a high-quality dataset for
FSS, named FS2K, which consists of 2,104 image-sketch pairs spanning three
types of sketch styles, image backgrounds, lighting conditions, skin colors,
and facial attributes. FS2K differs from previous FSS datasets in difficulty,
diversity, and scalability and should thus facilitate the progress of FSS
research. Second, we present the largest-scale FSS investigation by reviewing
89 classical methods, including 25 handcrafted feature-based facial-sketch
synthesis approaches, 29 general translation methods, and 35 image-to-sketch
approaches. Besides, we elaborate comprehensive experiments on the existing 19
cutting-edge models. Third, we present a simple baseline for FSS, named FSGAN.
With only two straightforward components, i.e., facial-aware masking and
style-vector expansion, FSGAN surpasses the performance of all previous
state-of-the-art models on the proposed FS2K dataset by a large margin.
Finally, we conclude with lessons learned over the past years and point out
several unsolved challenges. Our code is available at
https://github.com/DengPingFan/FSGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-supervised continual learning for class-incremental segmentation. (arXiv:2201.01029v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01029">
<div class="article-summary-box-inner">
<span><p>Transfer learning is a powerful way to adapt existing deep learning models to
new emerging use-cases in remote sensing. Starting from a neural network
already trained for semantic segmentation, we propose to modify its label space
to swiftly adapt it to new classes under weak supervision. To alleviate the
background shift and the catastrophic forgetting problems inherent to this form
of continual learning, we compare different regularization terms and leverage a
pseudo-label strategy. We experimentally show the relevance of our approach on
three public remote sensing datasets. Code is open-source and released in this
repository:
https://github.com/alteia-ai/ICSS}{https://github.com/alteia-ai/ICSS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Egocentric 3D Pose Estimation with Third Person Views. (arXiv:2201.02017v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02017">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel approach to enhance the 3D body pose
estimation of a person computed from videos captured from a single wearable
camera. The key idea is to leverage high-level features linking first- and
third-views in a joint embedding space. To learn such embedding space we
introduce First2Third-Pose, a new paired synchronized dataset of nearly 2,000
videos depicting human activities captured from both first- and third-view
perspectives. We explicitly consider spatial- and motion-domain features,
combined using a semi-Siamese architecture trained in a self-supervised
fashion. Experimental results demonstrate that the joint multi-view embedded
space learned with our dataset is useful to extract discriminatory features
from arbitrary single-view egocentric videos, without needing domain adaptation
nor knowledge of camera parameters. We achieve significant improvement of
egocentric 3D body pose estimation performance on two unconstrained datasets,
over three supervised state-of-the-art approaches. Our dataset and code will be
available for research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video. (arXiv:2201.04127v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04127">
<div class="article-summary-box-inner">
<span><p>We introduce a free-viewpoint rendering method -- HumanNeRF -- that works on
a given monocular video of a human performing complex body motions, e.g. a
video from YouTube. Our method enables pausing the video at any frame and
rendering the subject from arbitrary new camera viewpoints or even a full
360-degree camera path for that particular frame and body pose. This task is
particularly challenging, as it requires synthesizing photorealistic details of
the body, as seen from various camera angles that may not exist in the input
video, as well as synthesizing fine details such as cloth folds and facial
appearance. Our method optimizes for a volumetric representation of the person
in a canonical T-pose, in concert with a motion field that maps the estimated
canonical representation to every frame of the video via backward warps. The
motion field is decomposed into skeletal rigid and non-rigid motions, produced
by deep networks. We show significant performance improvements over prior work,
and compelling examples of free-viewpoint renderings from monocular video of
moving humans in challenging uncontrolled capture scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Maximizing Self-supervision from Thermal Image for Effective Self-supervised Learning of Depth and Ego-motion. (arXiv:2201.04387v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04387">
<div class="article-summary-box-inner">
<span><p>Recently, self-supervised learning of depth and ego-motion from thermal
images shows strong robustness and reliability under challenging scenarios.
However, the inherent thermal image properties such as weak contrast, blurry
edges, and noise hinder to generate effective self-supervision from thermal
images. Therefore, most research relies on additional self-supervision sources
such as well-lit RGB images, generative models, and Lidar information. In this
paper, we conduct an in-depth analysis of thermal image characteristics that
degenerates self-supervision from thermal images. Based on the analysis, we
propose an effective thermal image mapping method that significantly increases
image information, such as overall structure, contrast, and details, while
preserving temporal consistency. The proposed method shows outperformed depth
and pose results than previous state-of-the-art networks without leveraging
additional RGB guidance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Only Cut Once: Boosting Data Augmentation with a Single Cut. (arXiv:2201.12078v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12078">
<div class="article-summary-box-inner">
<span><p>We present You Only Cut Once (YOCO) for performing data augmentations. YOCO
cuts one image into two pieces and performs data augmentations individually
within each piece. Applying YOCO improves the diversity of the augmentation per
sample and encourages neural networks to recognize objects from partial
information. YOCO enjoys the properties of parameter-free, easy usage, and
boosting almost all augmentations for free. Thorough experiments are conducted
to evaluate its effectiveness. We first demonstrate that YOCO can be seamlessly
applied to varying data augmentations, neural network architectures, and brings
performance gains on CIFAR and ImageNet classification tasks, sometimes
surpassing conventional image-level augmentation by large margins. Moreover, we
show YOCO benefits contrastive pre-training toward a more powerful
representation that can be better transferred to multiple downstream tasks.
Finally, we study a number of variants of YOCO and empirically analyze the
performance for respective settings. Code is available at GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VRT: A Video Restoration Transformer. (arXiv:2201.12288v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12288">
<div class="article-summary-box-inner">
<span><p>Video restoration (e.g., video super-resolution) aims to restore high-quality
frames from low-quality frames. Different from single image restoration, video
restoration generally requires to utilize temporal information from multiple
adjacent but usually misaligned video frames. Existing deep methods generally
tackle with this by exploiting a sliding window strategy or a recurrent
architecture, which either is restricted by frame-by-frame restoration or lacks
long-range modelling ability. In this paper, we propose a Video Restoration
Transformer (VRT) with parallel frame prediction and long-range temporal
dependency modelling abilities. More specifically, VRT is composed of multiple
scales, each of which consists of two kinds of modules: temporal mutual self
attention (TMSA) and parallel warping. TMSA divides the video into small clips,
on which mutual attention is applied for joint motion estimation, feature
alignment and feature fusion, while self attention is used for feature
extraction. To enable cross-clip interactions, the video sequence is shifted
for every other layer. Besides, parallel warping is used to further fuse
information from neighboring frames by parallel feature warping. Experimental
results on five tasks, including video super-resolution, video deblurring,
video denoising, video frame interpolation and space-time video
super-resolution, demonstrate that VRT outperforms the state-of-the-art methods
by large margins ($\textbf{up to 2.16dB}$) on fourteen benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TPC: Transformation-Specific Smoothing for Point Cloud Models. (arXiv:2201.12733v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12733">
<div class="article-summary-box-inner">
<span><p>Point cloud models with neural network architectures have achieved great
success and have been widely used in safety-critical applications, such as
Lidar-based recognition systems in autonomous vehicles. However, such models
are shown vulnerable to adversarial attacks which aim to apply stealthy
semantic transformations such as rotation and tapering to mislead model
predictions. In this paper, we propose a transformation-specific smoothing
framework TPC, which provides tight and scalable robustness guarantees for
point cloud models against semantic transformation attacks. We first categorize
common 3D transformations into three categories: additive (e.g., shearing),
composable (e.g., rotation), and indirectly composable (e.g., tapering), and we
present generic robustness certification strategies for all categories
respectively. We then specify unique certification protocols for a range of
specific semantic transformations and their compositions. Extensive experiments
on several common 3D transformations show that TPC significantly outperforms
the state of the art. For example, our framework boosts the certified accuracy
against twisting transformation along z-axis (within 20$^\circ$) from 20.3$\%$
to 83.8$\%$. Codes and models are available at
https://github.com/Qianhewu/Point-Cloud-Smoothing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eikonal Fields for Refractive Novel-View Synthesis. (arXiv:2202.00948v3 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00948">
<div class="article-summary-box-inner">
<span><p>We tackle the problem of generating novel-view images from collections of 2D
images showing refractive and reflective objects. Current solutions assume
opaque or transparent light transport along straight paths following the
emission-absorption model. Instead, we optimize for a field of 3D-varying Index
of Refraction (IoR) and trace light through it that bends toward the spatial
gradients of said IoR according to the laws of eikonal light transport.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task Specific Attention is one more thing you need for object detection. (arXiv:2202.09048v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09048">
<div class="article-summary-box-inner">
<span><p>Various models have been proposed to perform object detection. However, most
require many handdesigned components such as anchors and
non-maximum-suppression(NMS) to demonstrate good performance. To mitigate these
issues, Transformer-based DETR and its variant, Deformable DETR, were
suggested. These have solved much of the complex issue in designing a head for
object detection models; however, doubts about performance still exist when
considering Transformer-based models as state-of-the-art methods in object
detection for other models depending on anchors and NMS revealed better
results. Furthermore, it has been unclear whether it would be possible to build
an end-to-end pipeline in combination only with attention modules, because the
DETR-adapted Transformer method used a convolutional neural network (CNN) for
the backbone body. In this study, we propose that combining several attention
modules with our new Task Specific Split Transformer (TSST) is a powerful
method to produce the state-of-the art performance on COCO results without
traditionally hand-designed components. By splitting the general-purpose
attention module into two separated goal-specific attention modules, the
proposed method allows for the design of simpler object detection models.
Extensive experiments on the COCO benchmark demonstrate the effectiveness of
our approach. Code is available at https://github.com/navervision/tsst
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatio-Temporal Gating-Adjacency GCN for Human Motion Prediction. (arXiv:2203.01474v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01474">
<div class="article-summary-box-inner">
<span><p>Predicting future motion based on historical motion sequence is a fundamental
problem in computer vision, and it has wide applications in autonomous driving
and robotics. Some recent works have shown that Graph Convolutional
Networks(GCN) are instrumental in modeling the relationship between different
joints. However, considering the variants and diverse action types in human
motion data, the cross-dependency of the spatio-temporal relationships will be
difficult to depict due to the decoupled modeling strategy, which may also
exacerbate the problem of insufficient generalization. Therefore, we propose
the Spatio-Temporal Gating-Adjacency GCN(GAGCN) to learn the complex
spatio-temporal dependencies over diverse action types. Specifically, we adopt
gating networks to enhance the generalization of GCN via the trainable adaptive
adjacency matrix obtained by blending the candidate spatio-temporal adjacency
matrices. Moreover, GAGCN addresses the cross-dependency of space and time by
balancing the weights of spatio-temporal modeling and fusing the decoupled
spatio-temporal features. Extensive experiments on Human 3.6M, AMASS, and 3DPW
demonstrate that GAGCN achieves state-of-the-art performance in both short-term
and long-term predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PETR: Position Embedding Transformation for Multi-View 3D Object Detection. (arXiv:2203.05625v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05625">
<div class="article-summary-box-inner">
<span><p>In this paper, we develop position embedding transformation (PETR) for
multi-view 3D object detection. PETR encodes the position information of 3D
coordinates into image features, producing the 3D position-aware features.
Object query can perceive the 3D position-aware features and perform end-to-end
object detection. PETR achieves state-of-the-art performance (50.4% NDS and
44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark.
It can serve as a simple yet strong baseline for future research. Code is
available at \url{https://github.com/megvii-research/PETR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transform your Smartphone into a DSLR Camera: Learning the ISP in the Wild. (arXiv:2203.10636v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10636">
<div class="article-summary-box-inner">
<span><p>We propose a trainable Image Signal Processing (ISP) framework that produces
DSLR quality images given RAW images captured by a smartphone. To address the
color misalignments between training image pairs, we employ a color-conditional
ISP network and optimize a novel parametric color mapping between each input
RAW and reference DSLR image. During inference, we predict the target color
image by designing a color prediction network with efficient Global Context
Transformer modules. The latter effectively leverage global information to
learn consistent color and tone mappings. We further propose a robust masked
aligned loss to identify and discard regions with inaccurate motion estimation
during training. Lastly, we introduce the ISP in the Wild (ISPW) dataset,
consisting of weakly paired phone RAW and DSLR sRGB images. We extensively
evaluate our method, setting a new state-of-the-art on two datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Associating Objects with Scalable Transformers for Video Object Segmentation. (arXiv:2203.11442v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11442">
<div class="article-summary-box-inner">
<span><p>This paper investigates how to realize better and more efficient embedding
learning to tackle the semi-supervised video object segmentation under
challenging multi-object scenarios. The state-of-the-art methods learn to
decode features with a single positive object and thus have to match and
segment each target separately under multi-object scenarios, consuming multiple
times computation resources. To solve the problem, we propose an Associating
Objects with Transformers (AOT) approach to match and decode multiple objects
jointly and collaboratively. In detail, AOT employs an identification mechanism
to associate multiple targets into the same high-dimensional embedding space.
Thus, we can simultaneously process multiple objects' matching and segmentation
decoding as efficiently as processing a single object. To sufficiently model
multi-object association, a Long Short-Term Transformer (LSTT) is devised to
construct hierarchical matching and propagation. Based on AOT, we further
propose a more flexible and robust framework, Associating Objects with Scalable
Transformers (AOST), in which a scalable version of LSTT is designed to enable
run-time adaptation of accuracy-efficiency trade-offs. Besides, AOST introduces
a better layer-wise manner to couple identification and vision embeddings. We
conduct extensive experiments on multi-object and single-object benchmarks to
examine AOT series frameworks. Compared to the state-of-the-art competitors,
our methods can maintain times of run-time efficiency with superior
performance. Notably, we achieve new state-of-the-art performance on three
popular benchmarks, i.e., YouTube-VOS (86.5%), DAVIS 2017 Val/Test
(87.0%/84.7%), and DAVIS 2016 (93.0%). Project page:
https://github.com/z-x-yang/AOT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Multi-Scale Feature Fusion for Semantic Segmentation. (arXiv:2203.12683v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12683">
<div class="article-summary-box-inner">
<span><p>It is commonly believed that high internal resolution combined with expensive
operations (e.g. atrous convolutions) are necessary for accurate semantic
segmentation, resulting in slow speed and large memory usage. In this paper, we
question this belief and demonstrate that neither high internal resolution nor
atrous convolutions are necessary. Our intuition is that although segmentation
is a dense per-pixel prediction task, the semantics of each pixel often depend
on both nearby neighbors and far-away context; therefore, a more powerful
multi-scale feature fusion network plays a critical role. Following this
intuition, we revisit the conventional multi-scale feature space (typically
capped at P5) and extend it to a much richer space, up to P9, where the
smallest features are only 1/512 of the input size and thus have very large
receptive fields. To process such a rich feature space, we leverage the recent
BiFPN to fuse the multi-scale features. Based on these insights, we develop a
simplified segmentation model, named ESeg, which has neither high internal
resolution nor expensive atrous convolutions. Perhaps surprisingly, our simple
method can achieve better accuracy with faster speed than prior art across
multiple datasets. In real-time settings, ESeg-Lite-S achieves 76.0% mIoU on
CityScapes [12] at 189 FPS, outperforming FasterSeg [9] (73.1% mIoU at 170
FPS). Our ESeg-Lite-L runs at 79 FPS and achieves 80.1% mIoU, largely closing
the gap between real-time and high-performance segmentation models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Reduce Information Bottleneck for Object Detection in Aerial Images. (arXiv:2204.02033v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02033">
<div class="article-summary-box-inner">
<span><p>Object detection in aerial images is a fundamental research task in the
domain of geoscience and remote sensing. However, the advanced progress on this
topic mainly focuses on designing progressive backbone architectures or head
networks but ignores the neck network. In this letter, we first analyze the
importance of the neck network in object detection from the perspective of
information bottleneck. Then, to alleviate the information deficiency problem
in the current neck networks, we propose a Global Semantic Network (GSNet),
which acts as a bridge from the backbone to the head network in a bidirectional
global pattern. Compared to the existing neck networks, our model can capture
rich and detailed image features with less computational costs. Besides, we
further propose a feature Fusion Refinement Module (FRM) for different levels
of feature maps, which are suffering from a big information gap. To demonstrate
the effectiveness and efficiency of our approach, experiments are carried out
on two challenging datasets (i.e., DOTA and HRSC2016). Experimental results in
terms of recognition accuracy and computational complexity validate the
superiority of our method. The code has been open-sourced at GSNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linear Complexity Randomized Self-attention Mechanism. (arXiv:2204.04667v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04667">
<div class="article-summary-box-inner">
<span><p>Recently, random feature attentions (RFAs) are proposed to approximate the
softmax attention in linear time and space complexity by linearizing the
exponential kernel. In this paper, we first propose a novel perspective to
understand the bias in such approximation by recasting RFAs as self-normalized
importance samplers. This perspective further sheds light on an \emph{unbiased}
estimator for the whole softmax attention, called randomized attention (RA). RA
constructs positive random features via query-specific distributions and enjoys
greatly improved approximation fidelity, albeit exhibiting quadratic
complexity. By combining the expressiveness in RA and the efficiency in RFA, we
develop a novel linear complexity self-attention mechanism called linear
randomized attention (LARA). Extensive experiments across various domains
demonstrate that RA and LARA significantly improve the performance of RFAs by a
substantial margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastMapSVM: Classifying Complex Objects Using the FastMap Algorithm and Support-Vector Machines. (arXiv:2204.05112v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05112">
<div class="article-summary-box-inner">
<span><p>Neural Networks and related Deep Learning methods are currently at the
leading edge of technologies used for classifying objects. However, they
generally demand large amounts of time and data for model training; and their
learned models can sometimes be difficult to interpret. In this paper, we
advance FastMapSVM -- an interpretable Machine Learning framework for
classifying complex objects -- as an advantageous alternative to Neural
Networks for general classification tasks. FastMapSVM extends the applicability
of Support-Vector Machines (SVMs) to domains with complex objects by combining
the complementary strengths of FastMap and SVMs. FastMap is an efficient
linear-time algorithm that maps complex objects to points in a Euclidean space
while preserving pairwise domain-specific distances between them. We
demonstrate the efficiency and effectiveness of FastMapSVM in the context of
classifying seismograms. We show that its performance, in terms of precision,
recall, and accuracy, is comparable to that of other state-of-the-art methods.
However, compared to other methods, FastMapSVM uses significantly smaller
amounts of time and data for model training. It also provides a perspicuous
visualization of the objects and the classification boundaries between them. We
expect FastMapSVM to be viable for classification tasks in many other
real-world domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Dual Emotion with Fusion of Visual Sentiment for Rumor Detection. (arXiv:2204.11515v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11515">
<div class="article-summary-box-inner">
<span><p>In recent years, rumors have had a devastating impact on society, making
rumor detection a significant challenge. However, the studies on rumor
detection ignore the intense emotions of images in the rumor content. This
paper verifies that the image emotion improves the rumor detection efficiency.
A Multimodal Dual Emotion feature in rumor detection, which consists of visual
and textual emotions, is proposed. To the best of our knowledge, this is the
first study which uses visual emotion in rumor detection. The experiments on
real datasets verify that the proposed features outperform the state-of-the-art
sentiment features, and can be extended in rumor detectors while improving
their performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Symbolic Expression Transformer: A Computer Vision Approach for Symbolic Regression. (arXiv:2205.11798v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11798">
<div class="article-summary-box-inner">
<span><p>Symbolic Regression (SR) is a type of regression analysis to automatically
find the mathematical expression that best fits the data. Currently, SR still
basically relies on various searching strategies so that a sample-specific
model is required to be optimized for every expression, which significantly
limits the model's generalization and efficiency. Inspired by the fact that
human beings can infer a mathematical expression based on the curve of it, we
propose Symbolic Expression Transformer (SET), a sample-agnostic model from the
perspective of computer vision for SR. Specifically, the collected data is
represented as images and an image caption model is employed for translating
images to symbolic expressions. A large-scale dataset without overlap between
training and testing sets in the image domain is released. Our results
demonstrate the effectiveness of SET and suggest the promising direction of
image-based model for solving the challenging SR problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MUG: Multi-human Graph Network for 3D Mesh Reconstruction from 2D Pose. (arXiv:2205.12583v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12583">
<div class="article-summary-box-inner">
<span><p>Reconstructing multi-human body mesh from a single monocular image is an
important but challenging computer vision problem. In addition to the
individual body mesh models, we need to estimate relative 3D positions among
subjects to generate a coherent representation. In this work, through a single
graph neural network, named MUG (Multi-hUman Graph network), we construct
coherent multi-human meshes using only multi-human 2D pose as input. Compared
with existing methods, which adopt a detection-style pipeline (i.e., extracting
image features and then locating human instances and recovering body meshes
from that) and suffer from the significant domain gap between lab-collected
training datasets and in-the-wild testing datasets, our method benefits from
the 2D pose which has a relatively consistent geometric property across
datasets. Our method works like the following: First, to model the multi-human
environment, it processes multi-human 2D poses and builds a novel heterogeneous
graph, where nodes from different people and within one person are connected to
capture inter-human interactions and draw the body geometry (i.e., skeleton and
mesh structure). Second, it employs a dual-branch graph neural network
structure -- one for predicting inter-human depth relation and the other one
for predicting root-joint-relative mesh coordinates. Finally, the entire
multi-human 3D meshes are constructed by combining the output from both
branches. Extensive experiments demonstrate that MUG outperforms previous
multi-human mesh estimation methods on standard 3D human benchmarks --
Panoptic, MuPoTS-3D and 3DPW.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAINNFlow: Convolutional block Attention modules and Invertible Neural Networks Flow for anomaly detection and localization tasks. (arXiv:2206.01992v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01992">
<div class="article-summary-box-inner">
<span><p>Detection of object anomalies is crucial in industrial processes, but
unsupervised anomaly detection and localization is particularly important due
to the difficulty of obtaining a large number of defective samples and the
unpredictable types of anomalies in real life. Among the existing unsupervised
anomaly detection and localization methods, the NF-based scheme has achieved
better results. However, the two subnets (complex functions) $s_{i}(u_{i})$ and
$t_{i}(u_{i})$ in NF are usually multilayer perceptrons, which need to squeeze
the input visual features from 2D flattening to 1D, destroying the spatial
location relationship in the feature map and losing the spatial structure
information. In order to retain and effectively extract spatial structure
information, we design in this study a complex function model with alternating
CBAM embedded in a stacked $3\times3$ full convolution, which is able to retain
and effectively extract spatial structure information in the normalized flow
model. Extensive experimental results on the MVTec AD dataset show that
CAINNFlow achieves advanced levels of accuracy and inference efficiency based
on CNN and Transformer backbone networks as feature extractors, and CAINNFlow
achieves a pixel-level AUC of $98.64\%$ for anomaly detection in MVTec AD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Utility of Equivariant Message Passing in Cortical Mesh Segmentation. (arXiv:2206.03164v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03164">
<div class="article-summary-box-inner">
<span><p>The automated segmentation of cortical areas has been a long-standing
challenge in medical image analysis. The complex geometry of the cortex is
commonly represented as a polygon mesh, whose segmentation can be addressed by
graph-based learning methods. When cortical meshes are misaligned across
subjects, current methods produce significantly worse segmentation results,
limiting their ability to handle multi-domain data. In this paper, we
investigate the utility of E(n)-equivariant graph neural networks (EGNNs),
comparing their performance against plain graph neural networks (GNNs). Our
evaluation shows that GNNs outperform EGNNs on aligned meshes, due to their
ability to leverage the presence of a global coordinate system. On misaligned
meshes, the performance of plain GNNs drop considerably, while E(n)-equivariant
message passing maintains the same segmentation results. The best results can
also be obtained by using plain GNNs on realigned data (co-registered meshes in
a global coordinate system).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeMF: Neural Motion Fields for Kinematic Animation. (arXiv:2206.03287v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03287">
<div class="article-summary-box-inner">
<span><p>We present an implicit neural representation to learn the spatio-temporal
space of kinematic motions. Unlike previous work that represents motion as
discrete sequential samples, we propose to express the vast motion space as a
continuous function over time, hence the name Neural Motion Fields (NeMF).
Specifically, we use a neural network to learn this function for miscellaneous
sets of motions, which is designed to be a generative model conditioned on a
temporal coordinate $t$ and a random vector $z$ for controlling the style. The
model is then trained as a Variational Autoencoder (VAE) with motion encoders
to sample the latent space. We train our model with diverse human motion
dataset and quadruped dataset to prove its versatility, and finally deploy it
as a generic motion prior to solve task-agnostic problems and show its
superiority in different motion generation and editing applications, such as
motion interpolation, in-betweening, and re-navigating. More details can be
found on our project page: https://cs.yale.edu/homes/che/projects/nemf/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delving into the Pre-training Paradigm of Monocular 3D Object Detection. (arXiv:2206.03657v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03657">
<div class="article-summary-box-inner">
<span><p>The labels of monocular 3D object detection (M3OD) are expensive to obtain.
Meanwhile, there usually exists numerous unlabeled data in practical
applications, and pre-training is an efficient way of exploiting the knowledge
in unlabeled data. However, the pre-training paradigm for M3OD is hardly
studied. We aim to bridge this gap in this work. To this end, we first draw two
observations: (1) The guideline of devising pre-training tasks is imitating the
representation of the target task. (2) Combining depth estimation and 2D object
detection is a promising M3OD pre-training baseline. Afterwards, following the
guideline, we propose several strategies to further improve this baseline,
which mainly include target guided semi-dense depth estimation, keypoint-aware
2D object detection, and class-level loss adjustment. Combining all the
developed techniques, the obtained pre-training framework produces pre-trained
backbones that improve M3OD performance significantly on both the KITTI-3D and
nuScenes benchmarks. For example, by applying a DLA34 backbone to a naive
center-based M3OD detector, the moderate ${\rm AP}_{3D}70$ score of Car on the
KITTI-3D testing set is boosted by 18.71\% and the NDS score on the nuScenes
validation set is improved by 40.41\% relatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bringing Image Scene Structure to Video via Frame-Clip Consistency of Object Tokens. (arXiv:2206.06346v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06346">
<div class="article-summary-box-inner">
<span><p>Recent action recognition models have achieved impressive results by
integrating objects, their locations and interactions. However, obtaining dense
structured annotations for each frame is tedious and time-consuming, making
these methods expensive to train and less scalable. At the same time, if a
small set of annotated images is available, either within or outside the domain
of interest, how could we leverage these for a video downstream task? We
propose a learning framework StructureViT (SViT for short), which demonstrates
how utilizing the structure of a small number of images only available during
training can improve a video model. SViT relies on two key insights. First, as
both images and videos contain structured information, we enrich a transformer
model with a set of \emph{object tokens} that can be used across images and
videos. Second, the scene representations of individual frames in video should
"align" with those of still images. This is achieved via a \emph{Frame-Clip
Consistency} loss, which ensures the flow of structured information between
images and videos. We explore a particular instantiation of scene structure,
namely a \emph{Hand-Object Graph}, consisting of hands and objects with their
locations as nodes, and physical relations of contact/no-contact as edges. SViT
shows strong performance improvements on multiple video understanding tasks and
datasets. Furthermore, it won in the Ego4D CVPR'22 Object State Localization
challenge. For code and pretrained models, visit the project page at
\url{https://eladb3.github.io/SViT/}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fitting Segmentation Networks on Varying Image Resolutions using Splatting. (arXiv:2206.06445v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06445">
<div class="article-summary-box-inner">
<span><p>Data used in image segmentation are not always defined on the same grid. This
is particularly true for medical images, where the resolution, field-of-view
and orientation can differ across channels and subjects. Images and labels are
therefore commonly resampled onto the same grid, as a pre-processing step.
However, the resampling operation introduces partial volume effects and
blurring, thereby changing the effective resolution and reducing the contrast
between structures. In this paper we propose a splat layer, which automatically
handles resolution mismatches in the input data. This layer pushes each image
onto a mean space where the forward pass is performed. As the splat operator is
the adjoint to the resampling operator, the mean-space prediction can be pulled
back to the native label space, where the loss function is computed. Thus, the
need for explicit resolution adjustment using interpolation is removed. We show
on two publicly available datasets, with simulated and real multi-modal
magnetic resonance images, that this model improves segmentation results
compared to resampling as a pre-processing step.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RF-Next: Efficient Receptive Field Search for Convolutional Neural Networks. (arXiv:2206.06637v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06637">
<div class="article-summary-box-inner">
<span><p>Temporal/spatial receptive fields of models play an important role in
sequential/spatial tasks. Large receptive fields facilitate long-term
relations, while small receptive fields help to capture the local details.
Existing methods construct models with hand-designed receptive fields in
layers. Can we effectively search for receptive field combinations to replace
hand-designed patterns? To answer this question, we propose to find better
receptive field combinations through a global-to-local search scheme. Our
search scheme exploits both global search to find the coarse combinations and
local search to get the refined receptive field combinations further. The
global search finds possible coarse combinations other than human-designed
patterns. On top of the global search, we propose an expectation-guided
iterative local search scheme to refine combinations effectively. Our RF-Next
models, plugging receptive field search to various models, boost the
performance on many tasks, e.g., temporal action segmentation, object
detection, instance segmentation, and speech synthesis. The source code is
publicly available on <a href="http://mmcheng.net/rfnext.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Gait Recognition by Granger Causality. (arXiv:2206.06714v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06714">
<div class="article-summary-box-inner">
<span><p>Which joint interactions in the human gait cycle can be used as biometric
characteristics? Most current methods on gait recognition suffer from the lack
of interpretability. We propose an interpretable feature representation of gait
sequences by the graphical Granger causal inference. Gait sequence of a person
in the standardized motion capture format, constituting a set of 3D joint
spatial trajectories, is envisaged as a causal system of joints interacting in
time. We apply the graphical Granger model (GGM) to obtain the so-called
Granger causal graph among joints as a discriminative and visually
interpretable representation of a person's gait. We evaluate eleven distance
functions in the GGM feature space by established classification and
class-separability evaluation metrics. Our experiments indicate that, depending
on the metric, the most appropriate distance functions for the GGM are the
total norm distance and the Ky-Fan 1-norm distance. Experiments also show that
the GGM is able to detect the most discriminative joint interactions and that
it outperforms five related interpretable models in correct classification rate
and in Davies-Bouldin index. The proposed GGM model can serve as a
complementary tool for gait analysis in kinesiology or for gait recognition in
video surveillance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Decoder-free Object Detection with Transformers. (arXiv:2206.06829v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06829">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) are changing the landscape of object detection
approaches. A natural usage of ViTs in detection is to replace the CNN-based
backbone with a transformer-based backbone, which is straightforward and
effective, with the price of bringing considerable computation burden for
inference. More subtle usage is the DETR family, which eliminates the need for
many hand-designed components in object detection but introduces a decoder
demanding an extra-long time to converge. As a result, transformer-based object
detection can not prevail in large-scale applications. To overcome these
issues, we propose a novel decoder-free fully transformer-based (DFFT) object
detector, achieving high efficiency in both training and inference stages, for
the first time. We simplify objection detection into an encoder-only
single-level anchor-based dense prediction problem by centering around two
entry points: 1) Eliminate the training-inefficient decoder and leverage two
strong encoders to preserve the accuracy of single-level feature map
prediction; 2) Explore low-level semantic features for the detection task with
limited computational resources. In particular, we design a novel lightweight
detection-oriented transformer backbone that efficiently captures low-level
features with rich semantics based on a well-conceived ablation study.
Extensive experiments on the MS COCO benchmark demonstrate that DFFT_SMALL
outperforms DETR by 2.5% AP with 28% computation cost reduction and more than
$10\times$ fewer training epochs. Compared with the cutting-edge anchor-based
detector RetinaNet, DFFT_SMALL obtains over 5.5% AP gain while cutting down 70%
computation cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Segmentation of Mitochondria from Electron Microscopy Images Using Spatial Continuity. (arXiv:2206.02392v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02392">
<div class="article-summary-box-inner">
<span><p>Morphology of mitochondria plays critical roles in mediating their
physiological functions. Accurate segmentation of mitochondria from 3D electron
microscopy (EM) images is essential to quantitative characterization of their
morphology at the nanometer scale. Fully supervised deep learning models
developed for this task achieve excellent performance but require substantial
amounts of annotated data for training. However, manual annotation of EM images
is laborious and time-consuming because of their large volumes, limited
contrast, and low signal-to-noise ratios (SNRs). To overcome this challenge, we
propose a semi-supervised deep learning model that segments mitochondria by
leveraging the spatial continuity of their structural, morphological, and
contextual information in both labeled and unlabeled images. We use random
piecewise affine transformation to synthesize comprehensive and realistic
mitochondrial morphology for augmentation of training data. Experiments on the
EPFL dataset show that our model achieves performance similar as that of
state-of-the-art fully supervised models but requires only ~20% of their
annotated training data. Our semi-supervised model is versatile and can also
accurately segment other spatially continuous structures from EM images. Data
and code of this study are openly accessible at
https://github.com/cbmi-group/MPP.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-06-16 23:08:29.322317228 UTC">2022-06-16 23:08:29 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>