<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-02T01:30:00Z">09-02</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.AI updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Working Memory Connections for LSTM. (arXiv:2109.00020v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00020">
<div class="article-summary-box-inner">
<span><p>Recurrent Neural Networks with Long Short-Term Memory (LSTM) make use of
gating mechanisms to mitigate exploding and vanishing gradients when learning
long-term dependencies. For this reason, LSTMs and other gated RNNs are widely
adopted, being the standard de facto for many sequence modeling tasks. Although
the memory cell inside the LSTM contains essential information, it is not
allowed to influence the gating mechanism directly. In this work, we improve
the gate potential by including information coming from the internal cell
state. The proposed modification, named Working Memory Connection, consists in
adding a learnable nonlinear projection of the cell content into the network
gates. This modification can fit into the classical LSTM gates without any
assumption on the underlying task, being particularly effective when dealing
with longer sequences. Previous research effort in this direction, which goes
back to the early 2000s, could not bring a consistent improvement over vanilla
LSTM. As part of this paper, we identify a key issue tied to previous
connections that heavily limits their effectiveness, hence preventing a
successful integration of the knowledge coming from the internal cell state. We
show through extensive experimental evaluation that Working Memory Connections
constantly improve the performance of LSTMs on a variety of tasks. Numerical
results suggest that the cell state contains useful information that is worth
including in the gate structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep DNA Storage: Scalable and Robust DNA Storage via Coding Theory and Deep Learning. (arXiv:2109.00031v1 [cs.IT])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00031">
<div class="article-summary-box-inner">
<span><p>The concept of DNA storage was first suggested in 1959 by Richard Feynman who
shared his vision regarding nanotechnology in the talk "There is plenty of room
at the bottom". Later, towards the end of the 20-th century, the interest in
storage solutions based on DNA molecules was increased as a result of the human
genome project which in turn led to a significant progress in sequencing and
assembly methods. DNA storage enjoys major advantages over the well-established
magnetic and optical storage solutions. As opposed to magnetic solutions, DNA
storage does not require electrical supply to maintain data integrity and is
superior to other storage solutions in both density and durability. Given the
trends in cost decreases of DNA synthesis and sequencing, it is now
acknowledged that within the next 10-15 years DNA storage may become a highly
competitive archiving technology and probably later the main such technology.
With that said, the current implementations of DNA based storage systems are
very limited and are not fully optimized to address the unique pattern of
errors which characterize the synthesis and sequencing processes. In this work,
we propose a robust, efficient and scalable solution to implement DNA-based
storage systems. Our method deploys Deep Neural Networks (DNN) which
reconstruct a sequence of letters based on imperfect cluster of copies
generated by the synthesis and sequencing processes. A tailor-made
Error-Correcting Code (ECC) is utilized to combat patterns of errors which
occur during this process. Since our reconstruction method is adapted to
imperfect clusters, our method overcomes the time bottleneck of the noisy DNA
copies clustering process by allowing the use of a rapid and scalable
pseudo-clustering instead. Our architecture combines between convolutions and
transformers blocks and is trained using synthetic data modelled after real
data statistics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Informing Autonomous Deception Systems with Cyber Expert Performance Data. (arXiv:2109.00066v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00066">
<div class="article-summary-box-inner">
<span><p>The performance of artificial intelligence (AI) algorithms in practice
depends on the realism and correctness of the data, models, and feedback
(labels or rewards) provided to the algorithm. This paper discusses methods for
improving the realism and ecological validity of AI used for autonomous cyber
defense by exploring the potential to use Inverse Reinforcement Learning (IRL)
to gain insight into attacker actions, utilities of those actions, and
ultimately decision points which cyber deception could thwart. The Tularosa
study, as one example, provides experimental data of real-world techniques and
tools commonly used by attackers, from which core data vectors can be leveraged
to inform an autonomous cyber defense system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proceedings of KDD 2021 Workshop on Data-driven Humanitarian Mapping: Harnessing Human-Machine Intelligence for High-Stake Public Policy and Resilience Planning. (arXiv:2109.00100v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00100">
<div class="article-summary-box-inner">
<span><p>Humanitarian challenges, including natural disasters, food insecurity,
climate change, racial and gender violence, environmental crises, the COVID-19
coronavirus pandemic, human rights violations, and forced displacements,
disproportionately impact vulnerable communities worldwide. According to UN
OCHA, 235 million people will require humanitarian assistance in 20211 .
Despite these growing perils, there remains a notable paucity of data science
research to scientifically inform equitable public policy decisions for
improving the livelihood of at-risk populations. Scattered data science efforts
exist to address these challenges, but they remain isolated from practice and
prone to algorithmic harms concerning lack of privacy, fairness,
interpretability, accountability, transparency, and ethics. Biases in
data-driven methods carry the risk of amplifying inequalities in high-stakes
policy decisions that impact the livelihood of millions of people.
Consequently, proclaimed benefits of data-driven innovations remain
inaccessible to policymakers, practitioners, and marginalized communities at
the core of humanitarian actions and global development. To help fill this gap,
we propose the Data-driven Humanitarian Mapping Research Program, which focuses
on developing novel data science methodologies that harness human-machine
intelligence for high-stakes public policy and resilience planning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic non-invasive Cough Detection based on Accelerometer and Audio Signals. (arXiv:2109.00103v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00103">
<div class="article-summary-box-inner">
<span><p>We present an automatic non-invasive way of detecting cough events based on
both accelerometer and audio signals.
</p>
<p>The acceleration signals are captured by a smartphone firmly attached to the
patient's bed, using its integrated accelerometer.
</p>
<p>The audio signals are captured simultaneously by the same smartphone using an
external microphone.
</p>
<p>We have compiled a manually-annotated dataset containing such
simultaneously-captured acceleration and audio signals for approximately 6000
cough and 68000 non-cough events from 14 adult male patients in a tuberculosis
clinic.
</p>
<p>LR, SVM and MLP are evaluated as baseline classifiers and compared with deep
architectures such as CNN, LSTM, and Resnet50 using a leave-one-out
cross-validation scheme.
</p>
<p>We find that the studied classifiers can use either acceleration or audio
signals to distinguish between coughing and other activities including
sneezing, throat-clearing, and movement on the bed with high accuracy.
</p>
<p>However, in all cases, the deep neural networks outperform the shallow
classifiers by a clear margin and the Resnet50 offers the best performance by
achieving an AUC exceeding 0.98 and 0.99 for acceleration and audio signals
respectively.
</p>
<p>While audio-based classification consistently offers a better performance
than acceleration-based classification, we observe that the difference is very
small for the best systems.
</p>
<p>Since the acceleration signal requires less processing power, and since the
need to record audio is sidestepped and thus privacy is inherently secured, and
since the recording device is attached to the bed and not worn, an
accelerometer-based highly accurate non-invasive cough detector may represent a
more convenient and readily accepted method in long-term cough monitoring.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics. (arXiv:2109.00110v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00110">
<div class="article-summary-box-inner">
<span><p>We present miniF2F, a dataset of formal Olympiad-level mathematics problems
statements intended to provide a unified cross-system benchmark for neural
theorem proving. The miniF2F benchmark currently targets Metamath, Lean, and
Isabelle and consists of 488 problem statements drawn from the AIME, AMC, and
the International Mathematical Olympiad (IMO), as well as material from
high-school and undergraduate mathematics courses. We report baseline results
using GPT-f, a neural theorem prover based on GPT-3 and provide an analysis of
its performance. We intend for miniF2F to be a community-driven effort and hope
that our benchmark will help spur advances in neural theorem proving.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cognitive science as a source of forward and inverse models of human decisions for robotics and control. (arXiv:2109.00127v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00127">
<div class="article-summary-box-inner">
<span><p>Those designing autonomous systems that interact with humans will invariably
face questions about how humans think and make decisions. Fortunately,
computational cognitive science offers insight into human decision-making using
tools that will be familiar to those with backgrounds in optimization and
control (e.g., probability theory, statistical machine learning, and
reinforcement learning). Here, we review some of this work, focusing on how
cognitive science can provide forward models of human decision-making and
inverse models of how humans think about others' decision-making. We highlight
relevant recent developments, including approaches that synthesize blackbox and
theory-driven modeling, accounts that recast heuristics and biases as forms of
bounded optimality, and models that characterize human theory of mind and
communication in decision-theoretic terms. In doing so, we aim to provide
readers with a glimpse of the range of frameworks, methodologies, and
actionable insights that lie at the intersection of cognitive science and
control research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Exploration Methods in Reinforcement Learning. (arXiv:2109.00157v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00157">
<div class="article-summary-box-inner">
<span><p>Exploration is an essential component of reinforcement learning algorithms,
where agents need to learn how to predict and control unknown and often
stochastic environments. Reinforcement learning agents depend crucially on
exploration to obtain informative data for the learning process as the lack of
enough information could hinder effective learning. In this article, we provide
a survey of modern exploration methods in (Sequential) reinforcement learning,
as well as a taxonomy of exploration methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Problem Learning: Towards the Free Will of Machines. (arXiv:2109.00177v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00177">
<div class="article-summary-box-inner">
<span><p>A machine intelligence pipeline usually consists of six components: problem,
representation, model, loss, optimizer and metric. Researchers have worked hard
trying to automate many components of the pipeline. However, one key component
of the pipeline--problem definition--is still left mostly unexplored in terms
of automation. Usually, it requires extensive efforts from domain experts to
identify, define and formulate important problems in an area. However,
automatically discovering research or application problems for an area is
beneficial since it helps to identify valid and potentially important problems
hidden in data that are unknown to domain experts, expand the scope of tasks
that we can do in an area, and even inspire completely new findings.
</p>
<p>This paper describes Problem Learning, which aims at learning to discover and
define valid and ethical problems from data or from the machine's interaction
with the environment. We formalize problem learning as the identification of
valid and ethical problems in a problem space and introduce several possible
approaches to problem learning. In a broader sense, problem learning is an
approach towards the free will of intelligent machines. Currently, machines are
still limited to solving the problems defined by humans, without the ability or
flexibility to freely explore various possible problems that are even unknown
to humans. Though many machine learning techniques have been developed and
integrated into intelligent systems, they still focus on the means rather than
the purpose in that machines are still solving human defined problems. However,
proposing good problems is sometimes even more important than solving problems,
because a good problem can help to inspire new ideas and gain deeper
understandings. The paper also discusses the ethical implications of problem
learning under the background of Responsible AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CTAL: Pre-training Cross-modal Transformer for Audio-and-Language Representations. (arXiv:2109.00181v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00181">
<div class="article-summary-box-inner">
<span><p>Existing audio-language task-specific predictive approaches focus on building
complicated late-fusion mechanisms. However, these models are facing challenges
of overfitting with limited labels and low model generalization abilities. In
this paper, we present a Cross-modal Transformer for Audio-and-Language, i.e.,
CTAL, which aims to learn the intra-modality and inter-modality connections
between audio and language through two proxy tasks on a large amount of
audio-and-language pairs: masked language modeling and masked cross-modal
acoustic modeling. After fine-tuning our pre-trained model on multiple
downstream audio-and-language tasks, we observe significant improvements across
various tasks, such as, emotion classification, sentiment analysis, and speaker
verification. On this basis, we further propose a specially-designed fusion
mechanism that can be used in fine-tuning phase, which allows our pre-trained
model to achieve better performance. Lastly, we demonstrate detailed ablation
studies to prove that both our novel cross-modality fusion component and
audio-language pre-training methods significantly contribute to the promising
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep $\mathcal{L}^1$ Stochastic Optimal Control Policies for Planetary Soft-landing. (arXiv:2109.00183v1 [eess.SY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00183">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a novel deep learning based solution to the
Powered-Descent Guidance (PDG) problem, grounded in principles of nonlinear
Stochastic Optimal Control (SOC) and Feynman-Kac theory. Our algorithm solves
the PDG problem by framing it as an $\mathcal{L}^1$ SOC problem for minimum
fuel consumption. Additionally, it can handle practically useful control
constraints, nonlinear dynamics and enforces state constraints as
soft-constraints. This is achieved by building off of recent work on deep
Forward-Backward Stochastic Differential Equations (FBSDEs) and differentiable
non-convex optimization neural-network layers based on stochastic search. In
contrast to previous approaches, our algorithm does not require convexification
of the constraints or linearization of the dynamics and is empirically shown to
be robust to stochastic disturbances and the initial position of the
spacecraft. After training offline, our controller can be activated once the
spacecraft is within a pre-specified radius of the landing zone and at a
pre-specified altitude i.e., the base of an inverted cone with the tip at the
landing zone. We demonstrate empirically that our controller can successfully
and safely land all trajectories initialized at the base of this cone while
minimizing fuel consumption.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study on the Joint Impact of Feature Selection and Data Resampling on Imbalance Classification. (arXiv:2109.00201v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00201">
<div class="article-summary-box-inner">
<span><p>Real-world datasets often present different degrees of imbalanced (i.e.,
long-tailed or skewed) distributions. While the majority (a.k.a., head or
frequent) classes have sufficient samples, the minority (a.k.a., tail or rare)
classes can be under-represented by a rather limited number of samples. On one
hand, data resampling is a common approach to tackling class imbalance. On the
other hand, dimension reduction, which reduces the feature space, is a
conventional machine learning technique for building stronger classification
models on a dataset. However, the possible synergy between feature selection
and data resampling for high-performance imbalance classification has rarely
been investigated before. To address this issue, this paper carries out a
comprehensive empirical study on the joint influence of feature selection and
resampling on two-class imbalance classification. Specifically, we study the
performance of two opposite pipelines for imbalance classification, i.e.,
applying feature selection before or after data resampling. We conduct a large
amount of experiments (a total of 9225 experiments) on 52 publicly available
datasets, using 9 feature selection methods, 6 resampling approaches for class
imbalance learning, and 3 well-known classification algorithms. Experimental
results show that there is no constant winner between the two pipelines, thus
both of them should be considered to derive the best performing model for
imbalance classification. We also find that the performance of an imbalance
classification model depends on the classifier adopted, the ratio between the
number of majority and minority samples (IR), as well as on the ratio between
the number of samples and features (SFR). Overall, this study should provide
new reference value for researchers and practitioners in imbalance learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Learning: Issues in Medical Application. (arXiv:2109.00202v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00202">
<div class="article-summary-box-inner">
<span><p>Since the federated learning, which makes AI learning possible without moving
local data around, was introduced by google in 2017 it has been actively
studied particularly in the field of medicine. In fact, the idea of machine
learning in AI without collecting data from local clients is very attractive
because data remain in local sites. However, federated learning techniques
still have various open issues due to its own characteristics such as non
identical distribution, client participation management, and vulnerable
environments. In this presentation, the current issues to make federated
learning flawlessly useful in the real world will be briefly overviewed. They
are related to data/system heterogeneity, client management, traceability, and
security. Also, we introduce the modularized federated learning framework, we
currently develop, to experiment various techniques and protocols to find
solutions for aforementioned issues. The framework will be open to public after
development completes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Sample based Contrastive Loss for Top-k Recommendation. (arXiv:2109.00217v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00217">
<div class="article-summary-box-inner">
<span><p>The top-k recommendation is a fundamental task in recommendation systems
which is generally learned by comparing positive and negative pairs. The
Contrastive Loss (CL) is the key in contrastive learning that has received more
attention recently and we find it is well suited for top-k recommendations.
However, it is a problem that CL treats the importance of the positive and
negative samples as the same. On the one hand, CL faces the imbalance problem
of one positive sample and many negative samples. On the other hand, positive
items are so few in sparser datasets that their importance should be
emphasized. Moreover, the other important issue is that the sparse positive
items are still not sufficiently utilized in recommendations. So we propose a
new data augmentation method by using multiple positive items (or samples)
simultaneously with the CL loss function. Therefore, we propose a Multi-Sample
based Contrastive Loss (MSCL) function which solves the two problems by
balancing the importance of positive and negative samples and data
augmentation. And based on the graph convolution network (GCN) method,
experimental results demonstrate the state-of-the-art performance of MSCL. The
proposed MSCL is simple and can be applied in many methods. We will release our
code on GitHub upon the acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting all Aspect-polarity Pairs Jointly in a Text with Relation Extraction Approach. (arXiv:2109.00256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00256">
<div class="article-summary-box-inner">
<span><p>Extracting aspect-polarity pairs from texts is an important task of
fine-grained sentiment analysis. While the existing approaches to this task
have gained many progresses, they are limited at capturing relationships among
aspect-polarity pairs in a text, thus degrading the extraction performance.
Moreover, the existing state-of-the-art approaches, namely token-based
se-quence tagging and span-based classification, have their own defects such as
polarity inconsistency resulted from separately tagging tokens in the former
and the heterogeneous categorization in the latter where aspect-related and
polarity-related labels are mixed. In order to remedy the above defects,
in-spiring from the recent advancements in relation extraction, we propose to
generate aspect-polarity pairs directly from a text with relation extraction
technology, regarding aspect-pairs as unary relations where aspects are
enti-ties and the corresponding polarities are relations. Based on the
perspective, we present a position- and aspect-aware sequence2sequence model
for joint extraction of aspect-polarity pairs. The model is characterized with
its ability to capture not only relationships among aspect-polarity pairs in a
text through the sequence decoding, but also correlations between an aspect and
its polarity through the position- and aspect-aware attentions. The
experi-ments performed on three benchmark datasets demonstrate that our model
outperforms the existing state-of-the-art approaches, making significant
im-provement over them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complex Event Forecasting with Prediction Suffix Trees: Extended Technical Report. (arXiv:2109.00287v1 [cs.DB])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00287">
<div class="article-summary-box-inner">
<span><p>Complex Event Recognition (CER) systems have become popular in the past two
decades due to their ability to "instantly" detect patterns on real-time
streams of events. However, there is a lack of methods for forecasting when a
pattern might occur before such an occurrence is actually detected by a CER
engine. We present a formal framework that attempts to address the issue of
Complex Event Forecasting (CEF). Our framework combines two formalisms: a)
symbolic automata which are used to encode complex event patterns; and b)
prediction suffix trees which can provide a succinct probabilistic description
of an automaton's behavior. We compare our proposed approach against
state-of-the-art methods and show its advantage in terms of accuracy and
efficiency. In particular, prediction suffix trees, being variable-order Markov
models, have the ability to capture long-term dependencies in a stream by
remembering only those past sequences that are informative enough. Our
experimental results demonstrate the benefits, in terms of accuracy, of being
able to capture such long-term dependencies. This is achieved by increasing the
order of our model beyond what is possible with full-order Markov models that
need to perform an exhaustive enumeration of all possible past sequences of a
given order. We also discuss extensively how CEF solutions should be best
evaluated on the quality of their forecasts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intrinsic Argument Strength in Structured Argumentation: a Principled Approach. (arXiv:2109.00318v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00318">
<div class="article-summary-box-inner">
<span><p>Abstract argumentation provides us with methods such as gradual and Dung
semantics with which to evaluate arguments after potential attacks by other
arguments. Some of these methods can take intrinsic strengths of arguments as
input, with which to modulate the effects of attacks between arguments. Coming
from abstract argumentation, these methods look only at the relations between
arguments and not at the structure of the arguments themselves. In structured
argumentation the way an argument is constructed, by chaining inference rules
starting from premises, is taken into consideration. In this paper we study
methods for assigning an argument its intrinsic strength, based on the
strengths of the premises and inference rules used to form said argument. We
first define a set of principles, which are properties that strength assigning
methods might satisfy. We then propose two such methods and analyse which
principles they satisfy. Finally, we present a generalised system for creating
novel strength assigning methods and speak to the properties of this system
regarding the proposed principles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building a Legal Dialogue System: Development Process, Challenges and Opportunities. (arXiv:2109.00381v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00381">
<div class="article-summary-box-inner">
<span><p>This paper presents key principles and solutions to the challenges faced in
designing a domain-specific conversational agent for the legal domain. It
includes issues of scope, platform, architecture and preparation of input data.
It provides functionality in answering user queries and recording user
information including contact details and case-related information. It utilises
deep learning technology built upon Amazon Web Services (AWS) LEX in
combination with AWS Lambda. Due to lack of publicly available data, we
identified two methods including crowdsourcing experiments and archived
enquiries to develop a number of linguistic resources. This includes a training
dataset, set of predetermined responses for the conversational agent, a set of
regression test cases and a further conversation test set. We propose a
hierarchical bot structure that facilitates multi-level delegation and report
model accuracy on the regression test set. Additionally, we highlight features
that are added to the bot to improve the conversation flow and overall user
experience.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boolean proportions. (arXiv:2109.00388v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00388">
<div class="article-summary-box-inner">
<span><p>Analogy-making is at the core of human intelligence and creativity with
applications to such diverse tasks as commonsense reasoning, learning, language
acquisition, and story telling. This paper studies analogical proportions
between booleans of the form `$a$ is to $b$ what $c$ is to $d$' called boolean
proportions. Technically, we instantiate an abstract algebraic framework of
analogical proportions -- recently introduced by the author -- in the boolean
domain consisting of the truth values true and false together with boolean
functions. It turns out that our notion of boolean proportions has appealing
mathematical properties and that it coincides with a prominent model of boolean
proportions in the general case. In a broader sense, this paper is a further
step towards a theory of analogical reasoning and learning systems with
potential applications to fundamental AI-problems like commonsense reasoning
and computational learning and creativity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis. (arXiv:2109.00412v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00412">
<div class="article-summary-box-inner">
<span><p>In multimodal sentiment analysis (MSA), the performance of a model highly
depends on the quality of synthesized embeddings. These embeddings are
generated from the upstream process called multimodal fusion, which aims to
extract and combine the input unimodal raw data to produce a richer multimodal
representation. Previous work either back-propagates the task loss or
manipulates the geometric property of feature spaces to produce favorable
fusion results, which neglects the preservation of critical task-related
information that flows from input to the fusion results. In this work, we
propose a framework named MultiModal InfoMax (MMIM), which hierarchically
maximizes the Mutual Information (MI) in unimodal input pairs (inter-modality)
and between multimodal fusion result and unimodal input in order to maintain
task-related information through multimodal fusion. The framework is jointly
trained with the main task (MSA) to improve the performance of the downstream
MSA task. To address the intractable issue of MI bounds, we further formulate a
set of computationally simple parametric and non-parametric methods to
approximate their truth value. Experimental results on the two widely used
datasets demonstrate the efficacy of our approach. The implementation of this
work is publicly available at
https://github.com/declare-lab/Multimodal-Infomax.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balancing Performance and Human Autonomy with Implicit Guidance Agent. (arXiv:2109.00414v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00414">
<div class="article-summary-box-inner">
<span><p>The human-agent team, which is a problem in which humans and autonomous
agents collaborate to achieve one task, is typical in human-AI collaboration.
For effective collaboration, humans want to have an effective plan, but in
realistic situations, they might have difficulty calculating the best plan due
to cognitive limitations. In this case, guidance from an agent that has many
computational resources may be useful. However, if an agent guides the human
behavior explicitly, the human may feel that they have lost autonomy and are
being controlled by the agent. We therefore investigated implicit guidance
offered by means of an agent's behavior. With this type of guidance, the agent
acts in a way that makes it easy for the human to find an effective plan for a
collaborative task, and the human can then improve the plan. Since the human
improves their plan voluntarily, he or she maintains autonomy. We modeled a
collaborative agent with implicit guidance by integrating the Bayesian Theory
of Mind into existing collaborative-planning algorithms and demonstrated
through a behavioral experiment that implicit guidance is effective for
enabling humans to maintain a balance between improving their plans and
retaining autonomy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Adversarial Generation for Neural Machine Translation. (arXiv:2109.00417v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00417">
<div class="article-summary-box-inner">
<span><p>Attacking Neural Machine Translation models is an inherently combinatorial
task on discrete sequences, solved with approximate heuristics. Most methods
use the gradient to attack the model on each sample independently. Instead of
mechanically applying the gradient, could we learn to produce meaningful
adversarial attacks ? In contrast to existing approaches, we learn to attack a
model by training an adversarial generator based on a language model. We
propose the Masked Adversarial Generation (MAG) model, that learns to perturb
the translation model throughout the training process. The experiments show
that it improves the robustness of machine translation models, while being
faster than competing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M^2-MedDialog: A Dataset and Benchmarks for Multi-domain Multi-service Medical Dialogues. (arXiv:2109.00430v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00430">
<div class="article-summary-box-inner">
<span><p>Medical dialogue systems (MDSs) aim to assist doctors and patients with a
range of professional medical services, i.e., diagnosis, consultation, and
treatment. However, one-stop MDS is still unexplored because: (1) no dataset
has so large-scale dialogues contains both multiple medical services and
fine-grained medical labels (i.e., intents, slots, values); (2) no model has
addressed a MDS based on multiple-service conversations in a unified framework.
In this work, we first build a Multiple-domain Multiple-service medical
dialogue (M^2-MedDialog)dataset, which contains 1,557 conversations between
doctors and patients, covering 276 types of diseases, 2,468 medical entities,
and 3 specialties of medical services. To the best of our knowledge, it is the
only medical dialogue dataset that includes both multiple medical services and
fine-grained medical labels. Then, we formulate a one-stop MDS as a
sequence-to-sequence generation problem. We unify a MDS with causal language
modeling and conditional causal language modeling, respectively. Specifically,
we employ several pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5)
and their variants to get benchmarks on M^2-MedDialog dataset. We also propose
pseudo labeling and natural perturbation methods to expand M2-MedDialog dataset
and enhance the state-of-the-art pretrained models. We demonstrate the results
achieved by the benchmarks so far through extensive experiments on
M2-MedDialog. We release the dataset, the code, as well as the evaluation
scripts to facilitate future research in this important research direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proceedings of KDD 2020 Workshop on Data-driven Humanitarian Mapping: Harnessing Human-Machine Intelligence for High-Stake Public Policy and Resilience Planning. (arXiv:2109.00435v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00435">
<div class="article-summary-box-inner">
<span><p>Humanitarian challenges, including natural disasters, food insecurity,
climate change, racial and gender violence, environmental crises, the COVID-19
coronavirus pandemic, human rights violations, and forced displacements,
disproportionately impact vulnerable communities worldwide. According to UN
OCHA, 235 million people will require humanitarian assistance in 20211 .
Despite these growing perils, there remains a notable paucity of data science
research to scientifically inform equitable public policy decisions for
improving the livelihood of at-risk populations. Scattered data science efforts
exist to address these challenges, but they remain isolated from practice and
prone to algorithmic harms concerning lack of privacy, fairness,
interpretability, accountability, transparency, and ethics. Biases in
data-driven methods carry the risk of amplifying inequalities in high-stakes
policy decisions that impact the livelihood of millions of people.
Consequently, proclaimed benefits of data-driven innovations remain
inaccessible to policymakers, practitioners, and marginalized communities at
the core of humanitarian actions and global development. To help fill this gap,
we propose the Data-driven Humanitarian Mapping Research Program, which focuses
on developing novel data science methodologies that harness human-machine
intelligence for high-stakes public policy and resilience planning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Planning from video game descriptions. (arXiv:2109.00449v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00449">
<div class="article-summary-box-inner">
<span><p>This project proposes a methodology for the automatic generation of action
models from video game dynamics descriptions, as well as its integration with a
planning agent for the execution and monitoring of the plans. Planners use
these action models to get the deliberative behaviour for an agent in many
different video games and, combined with a reactive module, solve deterministic
and no-deterministic levels. Experimental results validate the methodology and
prove that the effort put by a knowledge engineer can be greatly reduced in the
definition of such complex domains. Furthermore, benchmarks of the domains has
been produced that can be of interest to the international planning community
to evaluate planners in international planning competitions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Movement Kinematics to Object Properties: Online Recognition of Human Carefulness. (arXiv:2109.00460v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00460">
<div class="article-summary-box-inner">
<span><p>When manipulating objects, humans finely adapt their motions to the
characteristics of what they are handling. Thus, an attentive observer can
foresee hidden properties of the manipulated object, such as its weight,
temperature, and even whether it requires special care in manipulation. This
study is a step towards endowing a humanoid robot with this last capability.
Specifically, we study how a robot can infer online, from vision alone, whether
or not the human partner is careful when moving an object. We demonstrated that
a humanoid robot could perform this inference with high accuracy (up to 81.3%)
even with a low-resolution camera. Only for short movements without obstacles,
carefulness recognition was insufficient. The prompt recognition of movement
carefulness from observing the partner's action will allow robots to adapt
their actions on the object to show the same degree of care as their human
partners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impossibility Results in AI: A Survey. (arXiv:2109.00484v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00484">
<div class="article-summary-box-inner">
<span><p>An impossibility theorem demonstrates that a particular problem or set of
problems cannot be solved as described in the claim. Such theorems put limits
on what is possible to do concerning artificial intelligence, especially the
super-intelligent one. As such, these results serve as guidelines, reminders,
and warnings to AI safety, AI policy, and governance researchers. These might
enable solutions to some long-standing questions in the form of formalizing
theories in the framework of constraint satisfaction without committing to one
option. In this paper, we have categorized impossibility theorems applicable to
the domain of AI into five categories: deduction, indistinguishability,
induction, tradeoffs, and intractability. We found that certain theorems are
too specific or have implicit assumptions that limit application. Also, we
added a new result (theorem) about the unfairness of explainability, the first
explainability-related result in the induction category. We concluded that
deductive impossibilities deny 100%-guarantees for security. In the end, we
give some ideas that hold potential in explainability, controllability, value
alignment, ethics, and group decision-making. They can be deepened by further
investigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Hierarchically Structured Concepts. (arXiv:1909.04559v5 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.04559">
<div class="article-summary-box-inner">
<span><p>We study the question of how concepts that have structure get represented in
the brain. Specifically, we introduce a model for hierarchically structured
concepts and we show how a biologically plausible neural network can recognize
these concepts, and how it can learn them in the first place. Our main goal is
to introduce a general framework for these tasks and prove formally how both
(recognition and learning) can be achieved.
</p>
<p>We show that both tasks can be accomplished even in presence of noise. For
learning, we analyze Oja's rule formally, a well-known biologically-plausible
rule for adjusting the weights of synapses. We complement the learning results
with lower bounds asserting that, in order to recognize concepts of a certain
hierarchical depth, neural networks must have a corresponding number of layers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Momentum-based Gradient Methods in Multi-Objective Recommendation. (arXiv:2009.04695v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.04695">
<div class="article-summary-box-inner">
<span><p>Multi-objective gradient methods are becoming the standard for solving
multi-objective problems. Among others, they show promising results in
developing multi-objective recommender systems with both correlated and
conflicting objectives. Classic multi-gradient~descent usually relies on the
combination of the gradients, not including the computation of first and second
moments of the gradients. This leads to a brittle behavior and misses important
areas in the solution space. In this work, we create a multi-objective
model-agnostic Adamize method that leverages the benefits of the Adam optimizer
in single-objective problems. This corrects and stabilizes~the~gradients of
every objective before calculating a common gradient descent vector that
optimizes all the objectives simultaneously. We evaluate the benefits of
Multi-objective Adamize on two multi-objective recommender systems and for
three different objective combinations, both correlated or conflicting. We
report significant improvements, measured with three different Pareto front
metrics: hypervolume, coverage, and spacing. Finally, we show that the
\textit{Adamized} Pareto front strictly dominates the previous one on multiple
objective pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta Automatic Curriculum Learning. (arXiv:2011.08463v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.08463">
<div class="article-summary-box-inner">
<span><p>A major challenge in the Deep RL (DRL) community is to train agents able to
generalize their control policy over situations never seen in training.
Training on diverse tasks has been identified as a key ingredient for good
generalization, which pushed researchers towards using rich procedural task
generation systems controlled through complex continuous parameter spaces. In
such complex task spaces, it is essential to rely on some form of Automatic
Curriculum Learning (ACL) to adapt the task sampling distribution to a given
learning agent, instead of randomly sampling tasks, as many could end up being
either trivial or unfeasible. Since it is hard to get prior knowledge on such
task spaces, many ACL algorithms explore the task space to detect progress
niches over time, a costly tabula-rasa process that needs to be performed for
each new learning agents, although they might have similarities in their
capabilities profiles. To address this limitation, we introduce the concept of
Meta-ACL, and formalize it in the context of black-box RL learners, i.e.
algorithms seeking to generalize curriculum generation to an (unknown)
distribution of learners. In this work, we present AGAIN, a first instantiation
of Meta-ACL, and showcase its benefits for curriculum generation over classical
ACL in multiple simulated environments including procedurally generated parkour
environments with learners of varying morphologies. Videos and code are
available at https://sites.google.com/view/meta-acl .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchically Decoupled Spatial-Temporal Contrast for Self-supervised Video Representation Learning. (arXiv:2011.11261v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11261">
<div class="article-summary-box-inner">
<span><p>We present a novel technique for self-supervised video representation
learning by: (a) decoupling the learning objective into two contrastive
subtasks respectively emphasizing spatial and temporal features, and (b)
performing it hierarchically to encourage multi-scale understanding. Motivated
by their effectiveness in supervised learning, we first introduce
spatial-temporal feature learning decoupling and hierarchical learning to the
context of unsupervised video learning. We show by experiments that
augmentations can be manipulated as regularization to guide the network to
learn desired semantics in contrastive learning, and we propose a way for the
model to separately capture spatial and temporal features at multiple scales.
We also introduce an approach to overcome the problem of divergent levels of
instance invariance at different hierarchies by modeling the invariance as loss
weights for objective re-weighting. Experiments on downstream action
recognition benchmarks on UCF101 and HMDB51 show that our proposed
Hierarchically Decoupled Spatial-Temporal Contrast (HDC) makes substantial
improvements over directly learning spatial-temporal features as a whole and
achieves competitive performance when compared with other state-of-the-art
unsupervised methods. Code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Width-Based Planning and Learning. (arXiv:2101.06177v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06177">
<div class="article-summary-box-inner">
<span><p>Width-based search methods have demonstrated state-of-the-art performance in
a wide range of testbeds, from classical planning problems to image-based
simulators such as Atari games. These methods scale independently of the size
of the state-space, but exponentially in the problem width. In practice,
running the algorithm with a width larger than 1 is computationally
intractable, prohibiting IW from solving higher width problems. In this paper,
we present a hierarchical algorithm that plans at two levels of abstraction. A
high-level planner uses abstract features that are incrementally discovered
from low-level pruning decisions. We illustrate this algorithm in classical
planning PDDL domains as well as in pixel-based simulator domains. In classical
planning, we show how IW(1) at two levels of abstraction can solve problems of
width 2. For pixel-based domains, we show how in combination with a learned
policy and a learned value function, the proposed hierarchical IW can
outperform current flat IW-based planners in Atari games with sparse rewards.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting the Monotonicity Constraint in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2102.03479v14 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03479">
<div class="article-summary-box-inner">
<span><p>Many complex multi-agent systems such as robot swarms control and autonomous
vehicle coordination can be modeled as Multi-Agent Reinforcement Learning
(MARL) tasks. QMIX, a popular MARL algorithm base on the monotonicity
constraint, has been used as a baseline for the benchmark environments, e.g.,
Starcraft Multi-Agent Challenge (SMAC), Predator-Prey (PP). Recent variants of
QMIX target relaxing the monotonicity constraint of QMIX to improve the
expressive power of QMIX, allowing for performance improvement in SMAC.
However, we find that such performance improvements of the variants are
significantly affected by various implementation tricks. In this paper, we
revisit the monotonicity constraint of QMIX, (1) we design a novel model RMC to
further investigate the monotonicity constraint; the results show that
monotonicity constraint can improve sample efficiency in some purely
cooperative tasks. (2) we then re-evaluate the performance of QMIX and these
variants by a grid hyperparameter search for the tricks; the results show QMIX
achieves the best performance among them; (3) we analyze the monotonic mixing
network from a theoretical perspective and show that it can represent any tasks
which can be interpreted as purely cooperative. These analyses demonstrate that
relaxing the monotonicity constraint of the mixing network will not always
improve the performance of QMIX, which breaks our previous impressions of the
monotonicity constraints. We open-source the code at
\url{https://github.com/hijkzzz/pymarl2}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Data-Centric Framework for Composable NLP Workflows. (arXiv:2103.01834v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01834">
<div class="article-summary-box-inner">
<span><p>Empirical natural language processing (NLP) systems in application domains
(e.g., healthcare, finance, education) involve interoperation among multiple
components, ranging from data ingestion, human annotation, to text retrieval,
analysis, generation, and visualization. We establish a unified open-source
framework to support fast development of such sophisticated NLP workflows in a
composable manner. The framework introduces a uniform data representation to
encode heterogeneous results by a wide range of NLP tasks. It offers a large
repository of processors for NLP tasks, visualization, and annotation, which
can be easily assembled with full interoperability under the unified
representation. The highly extensible framework allows plugging in custom
processors from external off-the-shelf NLP and deep learning libraries. The
whole framework is delivered through two modularized yet integratable
open-source projects, namely Forte (for workflow infrastructure and NLP
function processors) and Stave (for user interaction, visualization, and
annotation).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering. (arXiv:2104.03149v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03149">
<div class="article-summary-box-inner">
<span><p>We introduce an evaluation methodology for visual question answering (VQA) to
better diagnose cases of shortcut learning. These cases happen when a model
exploits spurious statistical regularities to produce correct answers but does
not actually deploy the desired behavior. There is a need to identify possible
shortcuts in a dataset and assess their use before deploying a model in the
real world. The research community in VQA has focused exclusively on
question-based shortcuts, where a model might, for example, answer "What is the
color of the sky" with "blue" by relying mostly on the question-conditional
training prior and give little weight to visual evidence. We go a step further
and consider multimodal shortcuts that involve both questions and images. We
first identify potential shortcuts in the popular VQA v2 training set by mining
trivial predictive rules such as co-occurrences of words and visual elements.
We then introduce VQA-CounterExamples (VQA-CE), an evaluation protocol based on
our subset of CounterExamples i.e. image-question-answer triplets where our
rules lead to incorrect answers. We use this new evaluation in a large-scale
study of existing approaches for VQA. We demonstrate that even state-of-the-art
models perform poorly and that existing techniques to reduce biases are largely
ineffective in this context. Our findings suggest that past work on
question-based biases in VQA has only addressed one facet of a complex issue.
The code for our method is available at
https://github.com/cdancette/detect-shortcuts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-NERD: A Few-Shot Named Entity Recognition Dataset. (arXiv:2105.07464v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07464">
<div class="article-summary-box-inner">
<span><p>Recently, considerable literature has grown up around the theme of few-shot
named entity recognition (NER), but little published benchmark data
specifically focused on the practical and challenging task. Current approaches
collect existing supervised NER datasets and re-organize them to the few-shot
setting for empirical study. These strategies conventionally aim to recognize
coarse-grained entity types with few examples, while in practice, most unseen
entity types are fine-grained. In this paper, we present Few-NERD, a
large-scale human-annotated few-shot NER dataset with a hierarchy of 8
coarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238
sentences from Wikipedia, 4,601,160 words are included and each is annotated as
context or a part of a two-level entity type. To the best of our knowledge,
this is the first few-shot NER dataset and the largest human-crafted NER
dataset. We construct benchmark tasks with different emphases to
comprehensively assess the generalization capability of models. Extensive
empirical results and analysis show that Few-NERD is challenging and the
problem requires further research. We make Few-NERD public at
https://ningding97.github.io/fewnerd/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Domain Active Learning: A Comparative Study. (arXiv:2106.13516v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13516">
<div class="article-summary-box-inner">
<span><p>Building classifiers on multiple domains is a practical problem in the real
life. Instead of building classifiers one by one, multi-domain learning (MDL)
simultaneously builds classifiers on all the domains. MDL utilizes the
information shared among the domains to improve the performance. As a
supervised learning problem, the labeling effort is still high in MDL problems.
Usually, this high labeling cost issue could be relieved by using active
learning. Thus, it is natural to utilize active learning to reduce the labeling
effort in MDL, and we refer this setting as multi-domain active learning
(MDAL). However, there are only few works which are built on this setting. And
when the researchers have to face this problem, there is no off-the-shelf
solution. Under this circumstance, combining the current multi-domain learning
models and single-domain active learning strategies might be a preliminary
solution for MDAL problem. To find out the potential of this preliminary
solution, a comparative study over 5 models and 4 active learning strategies is
made in this paper. To the best of our knowledge, this is the first work
provides the formal definition of MDAL. Besides, this is the first comparative
work for MDAL problem. From the results, the Multinomial Adversarial Networks
(MAN) model with a simple best vs second best (BvSB) uncertainty strategy shows
its superiority in most cases. We take this combination as our off-the-shelf
recommendation for the MDAL problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SocialAI: Benchmarking Socio-Cognitive Abilities in Deep Reinforcement Learning Agents. (arXiv:2107.00956v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00956">
<div class="article-summary-box-inner">
<span><p>Building embodied autonomous agents capable of participating in social
interactions with humans is one of the main challenges in AI. Within the Deep
Reinforcement Learning (DRL) field, this objective motivated multiple works on
embodied language use. However, current approaches focus on language as a
communication tool in very simplified and non-diverse social situations: the
"naturalness" of language is reduced to the concept of high vocabulary size and
variability. In this paper, we argue that aiming towards human-level AI
requires a broader set of key social skills: 1) language use in complex and
variable social contexts; 2) beyond language, complex embodied communication in
multimodal settings within constantly evolving social worlds. We explain how
concepts from cognitive sciences could help AI to draw a roadmap towards
human-like intelligence, with a focus on its social dimensions. As a first
step, we propose to expand current research to a broader set of core social
skills. To do this, we present SocialAI, a benchmark to assess the acquisition
of social skills of DRL agents using multiple grid-world environments featuring
other (scripted) social agents. We then study the limits of a recent SOTA DRL
approach when tested on SocialAI and discuss important next steps towards
proficient social agents. Videos and code are available at
https://sites.google.com/view/socialai.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Data-Driven Biophysical Computational Model of Parkinson's Disease based on Marmoset Monkeys. (arXiv:2107.12536v2 [q-bio.NC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12536">
<div class="article-summary-box-inner">
<span><p>In this work we propose a new biophysical computational model of brain
regions relevant to Parkinson's Disease based on local field potential data
collected from the brain of marmoset monkeys. Parkinson's disease is a
neurodegenerative disorder, linked to the death of dopaminergic neurons at the
substantia nigra pars compacta, which affects the normal dynamics of the basal
ganglia-thalamus-cortex neuronal circuit of the brain. Although there are
multiple mechanisms underlying the disease, a complete description of those
mechanisms and molecular pathogenesis are still missing, and there is still no
cure. To address this gap, computational models that resemble neurobiological
aspects found in animal models have been proposed. In our model, we performed a
data-driven approach in which a set of biologically constrained parameters is
optimised using differential evolution. Evolved models successfully resembled
single-neuron mean firing rates and spectral signatures of local field
potentials from healthy and parkinsonian marmoset brain data. As far as we are
concerned, this is the first computational model of Parkinson's Disease based
on simultaneous electrophysiological recordings from seven brain regions of
Marmoset monkeys. Results show that the proposed model could facilitate the
investigation of the mechanisms of PD and support the development of techniques
that can indicate new therapies. It could also be applied to other
computational neuroscience problems in which biological data could be used to
fit multi-scale models of brain circuits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Creating Powerful and Interpretable Models with Regression Networks. (arXiv:2107.14417v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14417">
<div class="article-summary-box-inner">
<span><p>As the discipline has evolved, research in machine learning has been focused
more and more on creating more powerful neural networks, without regard for the
interpretability of these networks. Such "black-box models" yield
state-of-the-art results, but we cannot understand why they make a particular
decision or prediction. Sometimes this is acceptable, but often it is not. We
propose a novel architecture, Regression Networks, which combines the power of
neural networks with the understandability of regression analysis. While some
methods for combining these exist in the literature, our architecture
generalizes these approaches by taking interactions into account, offering the
power of a dense neural network without forsaking interpretability. We
demonstrate that the models exceed the state-of-the-art performance of
interpretable models on several benchmark datasets, matching the power of a
dense neural network. Finally, we discuss how these techniques can be
generalized to other neural architectures, such as convolutional and recurrent
neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Adversarial Examples" for Proof-of-Learning. (arXiv:2108.09454v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09454">
<div class="article-summary-box-inner">
<span><p>In S&amp;P '21, Jia et al. proposed a new concept/mechanism named
proof-of-learning (PoL), which allows a prover to demonstrate ownership of a
machine learning model by proving integrity of the training procedure. It
guarantees that an adversary cannot construct a valid proof with less cost (in
both computation and storage) than that made by the prover in generating the
proof. A PoL proof includes a set of intermediate models recorded during
training, together with the corresponding data points used to obtain each
recorded model. Jia et al. claimed that an adversary merely knowing the final
model and training dataset cannot efficiently find a set of intermediate models
with correct data points. In this paper, however, we show that PoL is
vulnerable to "adversarial examples"! Specifically, in a similar way as
optimizing an adversarial example, we could make an arbitrarily-chosen data
point "generate" a given model, hence efficiently generating intermediate
models with correct data points. We demonstrate, both theoretically and
empirically, that we are able to generate a valid proof with significantly less
cost than generating a proof by the prover, thereby we successfully break PoL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MS-DARTS: Mean-Shift Based Differentiable Architecture Search. (arXiv:2108.09996v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09996">
<div class="article-summary-box-inner">
<span><p>Differentiable Architecture Search (DARTS) is an effective continuous
relaxation-based network architecture search (NAS) method with low search cost.
It has attracted significant attentions in Auto-ML research and becomes one of
the most useful paradigms in NAS. Although DARTS can produce superior
efficiency over traditional NAS approaches with better control of complex
parameters, oftentimes it suffers from stabilization issues in producing
deteriorating architectures when discretizing the continuous architecture. We
observed considerable loss of validity causing dramatic decline in performance
at this final discretization step of DARTS. To address this issue, we propose a
Mean-Shift based DARTS (MS-DARTS) to improve stability based on sampling and
perturbation. Our approach can improve bot the stability and accuracy of DARTS,
by smoothing the loss landscape and sampling architecture parameters within a
suitable bandwidth. We investigate the convergence of our mean-shift approach,
together with the effects of bandwidth selection that affects stability and
accuracy. Evaluations performed on CIFAR-10, CIFAR-100, and ImageNet show that
MS-DARTS archives higher performance over other state-of-the-art NAS methods
with reduced search cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E-Commerce Promotions Personalization via Online Multiple-Choice Knapsack with Uplift Modeling. (arXiv:2108.13298v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13298">
<div class="article-summary-box-inner">
<span><p>Promotions and discounts are essential components of modern e-commerce
platforms, where they are often used to incentivize customers towards purchase
completion. Promotions also affect revenue and may incur a monetary loss that
is often limited by a dedicated promotional budget. We study the Online
Constrained Multiple-Choice Promotions Personalization Problem, where the
optimization goal is to select for each customer which promotion to present in
order to maximize purchase completions, while also complying with global budget
limitations. Our work formalizes the problem as an Online Multiple Choice
Knapsack Problem and extends the existent literature by addressing cases with
negative weights and values. We provide a real-time adaptive method that
guarantees budget constraints compliance and achieves above 99.7% of the
optimal promotional impact on various datasets. Our method is evaluated on a
large-scale experimental study at one of the leading online travel platforms in
the world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Oriented Dialogue System as Natural Language Generation. (arXiv:2108.13679v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13679">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose to formulate the task-oriented dialogue system as
the purely natural language generation task, so as to fully leverage the
large-scale pre-trained models like GPT-2 and simplify complicated
delexicalization prepossessing. However, directly applying this method heavily
suffers from the dialogue entity inconsistency caused by the removal of
delexicalized tokens, as well as the catastrophic forgetting problem of the
pre-trained model during fine-tuning, leading to unsatisfactory performance. To
alleviate these problems, we design a novel GPT-Adapter-CopyNet network, which
incorporates the lightweight adapter and CopyNet modules into GPT-2 to achieve
better performance on transfer learning and dialogue entity generation.
Experimental results conducted on the DSTC8 Track 1 benchmark and MultiWOZ
dataset demonstrate that our proposed approach significantly outperforms
baseline models with a remarkable performance on automatic and human
evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fiducial marker recovery and detection from severely truncated data in navigation assisted spine surgery. (arXiv:2108.13844v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13844">
<div class="article-summary-box-inner">
<span><p>Fiducial markers are commonly used in navigation assisted minimally invasive
spine surgery (MISS) and they help transfer image coordinates into real world
coordinates. In practice, these markers might be located outside the
field-of-view (FOV), due to the limited detector sizes of C-arm cone-beam
computed tomography (CBCT) systems used in intraoperative surgeries. As a
consequence, reconstructed markers in CBCT volumes suffer from artifacts and
have distorted shapes, which sets an obstacle for navigation. In this work, we
propose two fiducial marker detection methods: direct detection from distorted
markers (direct method) and detection after marker recovery (recovery method).
For direct detection from distorted markers in reconstructed volumes, an
efficient automatic marker detection method using two neural networks and a
conventional circle detection algorithm is proposed. For marker recovery, a
task-specific learning strategy is proposed to recover markers from severely
truncated data. Afterwards, a conventional marker detection algorithm is
applied for position detection. The two methods are evaluated on simulated data
and real data, both achieving a marker registration error smaller than 0.2 mm.
Our experiments demonstrate that the direct method is capable of detecting
distorted markers accurately and the recovery method with task-specific
learning has high robustness and generalizability on various data sets. In
addition, the task-specific learning is able to reconstruct other structures of
interest accurately, e.g. ribs for image-guided needle biopsy, from severely
truncated data, which empowers CBCT systems with new potential applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Subsampling-Based Method for Causal Discovery on Discrete Data. (arXiv:2108.13984v2 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13984">
<div class="article-summary-box-inner">
<span><p>Inferring causal directions on discrete and categorical data is an important
yet challenging problem. Even though the additive noise models (ANMs) approach
can be adapted to the discrete data, the functional structure assumptions make
it not applicable on categorical data. Inspired by the principle that the cause
and mechanism are independent, various methods have been developed, leveraging
independence tests such as the distance correlation measure. In this work, we
take an alternative perspective and propose a subsampling-based method to test
the independence between the generating schemes of the cause and that of the
mechanism. Our methodology works for both discrete and categorical data and
does not imply any functional model on the data, making it a more flexible
approach. To demonstrate the efficacy of our methodology, we compare it with
existing baselines over various synthetic data and real data experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DASH: Modularized Human Manipulation Simulation with Vision and Language for Embodied AI. (arXiv:2108.12536v1 [cs.GR] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12536">
<div class="article-summary-box-inner">
<span><p>Creating virtual humans with embodied, human-like perceptual and actuation
constraints has the promise to provide an integrated simulation platform for
many scientific and engineering applications. We present Dynamic and Autonomous
Simulated Human (DASH), an embodied virtual human that, given natural language
commands, performs grasp-and-stack tasks in a physically-simulated cluttered
environment solely using its own visual perception, proprioception, and touch,
without requiring human motion data. By factoring the DASH system into a vision
module, a language module, and manipulation modules of two skill categories, we
can mix and match analytical and machine learning techniques for different
modules so that DASH is able to not only perform randomly arranged tasks with a
high success rate, but also do so under anthropomorphic constraints and with
fluid and diverse motions. The modular design also favors analysis and
extensibility to more complex manipulation skills.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Working Memory Connections for LSTM. (arXiv:2109.00020v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00020">
<div class="article-summary-box-inner">
<span><p>Recurrent Neural Networks with Long Short-Term Memory (LSTM) make use of
gating mechanisms to mitigate exploding and vanishing gradients when learning
long-term dependencies. For this reason, LSTMs and other gated RNNs are widely
adopted, being the standard de facto for many sequence modeling tasks. Although
the memory cell inside the LSTM contains essential information, it is not
allowed to influence the gating mechanism directly. In this work, we improve
the gate potential by including information coming from the internal cell
state. The proposed modification, named Working Memory Connection, consists in
adding a learnable nonlinear projection of the cell content into the network
gates. This modification can fit into the classical LSTM gates without any
assumption on the underlying task, being particularly effective when dealing
with longer sequences. Previous research effort in this direction, which goes
back to the early 2000s, could not bring a consistent improvement over vanilla
LSTM. As part of this paper, we identify a key issue tied to previous
connections that heavily limits their effectiveness, hence preventing a
successful integration of the knowledge coming from the internal cell state. We
show through extensive experimental evaluation that Working Memory Connections
constantly improve the performance of LSTMs on a variety of tasks. Numerical
results suggest that the cell state contains useful information that is worth
including in the gate structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine-Learning media bias. (arXiv:2109.00024v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00024">
<div class="article-summary-box-inner">
<span><p>We present an automated method for measuring media bias. Inferring which
newspaper published a given article, based only on the frequencies with which
it uses different phrases, leads to a conditional probability distribution
whose analysis lets us automatically map newspapers and phrases into a bias
space. By analyzing roughly a million articles from roughly a hundred
newspapers for bias in dozens of news topics, our method maps newspapers into a
two-dimensional bias landscape that agrees well with previous bias
classifications based on human judgement. One dimension can be interpreted as
traditional left-right bias, the other as establishment bias. This means that
although news bias is inherently political, its measurement need not be.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sense representations for Portuguese: experiments with sense embeddings and deep neural language models. (arXiv:2109.00025v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00025">
<div class="article-summary-box-inner">
<span><p>Sense representations have gone beyond word representations like Word2Vec,
GloVe and FastText and achieved innovative performance on a wide range of
natural language processing tasks. Although very useful in many applications,
the traditional approaches for generating word embeddings have a strict
drawback: they produce a single vector representation for a given word ignoring
the fact that ambiguous words can assume different meanings. In this paper, we
explore unsupervised sense representations which, different from traditional
word embeddings, are able to induce different senses of a word by analyzing its
contextual semantics in a text. The unsupervised sense representations
investigated in this paper are: sense embeddings and deep neural language
models. We present the first experiments carried out for generating sense
embeddings for Portuguese. Our experiments show that the sense embedding model
(Sense2vec) outperformed traditional word embeddings in syntactic and semantic
analogies task, proving that the language resource generated here can improve
the performance of NLP tasks in Portuguese. We also evaluated the performance
of pre-trained deep neural language models (ELMo and BERT) in two transfer
learning approaches: feature based and fine-tuning, in the semantic textual
similarity task. Our experiments indicate that the fine tuned Multilingual and
Portuguese BERT language models were able to achieve better accuracy than the
ELMo model and baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence Bottleneck Autoencoders from Transformer Language Models. (arXiv:2109.00055v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00055">
<div class="article-summary-box-inner">
<span><p>Representation learning for text via pretraining a language model on a large
corpus has become a standard starting point for building NLP systems. This
approach stands in contrast to autoencoders, also trained on raw text, but with
the objective of learning to encode each input as a vector that allows full
reconstruction. Autoencoders are attractive because of their latent space
structure and generative properties. We therefore explore the construction of a
sentence-level autoencoder from a pretrained, frozen transformer language
model. We adapt the masked language modeling objective as a generative,
denoising one, while only training a sentence bottleneck and a single-layer
modified transformer decoder. We demonstrate that the sentence representations
discovered by our model achieve better quality than previous methods that
extract representations from pretrained transformers on text similarity tasks,
style transfer (an example of controlled generation), and single-sentence
classification tasks in the GLUE benchmark, while using fewer parameters than
large pretrained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effectiveness of Deep Networks in NLP using BiDAF as an example architecture. (arXiv:2109.00074v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00074">
<div class="article-summary-box-inner">
<span><p>Question Answering with NLP has progressed through the evolution of advanced
model architectures like BERT and BiDAF and earlier word, character, and
context-based embeddings. As BERT has leapfrogged the accuracy of models, an
element of the next frontier can be the introduction of deep networks and an
effective way to train them. In this context, I explored the effectiveness of
deep networks focussing on the model encoder layer of BiDAF. BiDAF with its
heterogeneous layers provides the opportunity not only to explore the
effectiveness of deep networks but also to evaluate whether the refinements
made in lower layers are additive to the refinements made in the upper layers
of the model architecture. I believe the next greatest model in NLP will in
fact fold in a solid language modeling like BERT with a composite architecture
which will bring in refinements in addition to generic language modeling and
will have a more extensive layered architecture. I experimented with the Bypass
network, Residual Highway network, and DenseNet architectures. In addition, I
evaluated the effectiveness of ensembling the last few layers of the network. I
also studied the difference character embeddings make in adding them to the
word embeddings, and whether the effects are additive with deep networks. My
studies indicate that deep networks are in fact effective in giving a boost.
Also, the refinements in the lower layers like embeddings are passed on
additively to the gains made through deep networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Machine Comprehension with Dynamic Knowledge Graphs. (arXiv:2109.00077v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00077">
<div class="article-summary-box-inner">
<span><p>Interactive machine reading comprehension (iMRC) is machine comprehension
tasks where knowledge sources are partially observable. An agent must interact
with an environment sequentially to gather necessary knowledge in order to
answer a question. We hypothesize that graph representations are good inductive
biases, which can serve as an agent's memory mechanism in iMRC tasks. We
explore four different categories of graphs that can capture text information
at various levels. We describe methods that dynamically build and update these
graphs during information gathering, as well as neural models to encode graph
representations in RL agents. Extensive experiments on iSQuAD suggest that
graph representations can result in significant performance improvements for RL
agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MergeBERT: Program Merge Conflict Resolution via Neural Transformers. (arXiv:2109.00084v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00084">
<div class="article-summary-box-inner">
<span><p>Collaborative software development is an integral part of the modern software
development life cycle, essential to the success of large-scale software
projects. When multiple developers make concurrent changes around the same
lines of code, a merge conflict may occur. Such conflicts stall pull requests
and continuous integration pipelines for hours to several days, seriously
hurting developer productivity.
</p>
<p>In this paper, we introduce MergeBERT, a novel neural program merge framework
based on the token-level three-way differencing and a transformer encoder
model. Exploiting restricted nature of merge conflict resolutions, we
reformulate the task of generating the resolution sequence as a classification
task over a set of primitive merge patterns extracted from real-world merge
commit data.
</p>
<p>Our model achieves 64--69% precision of merge resolution synthesis, yielding
nearly a 2x performance improvement over existing structured and neural program
merge tools. Finally, we demonstrate versatility of our model, which is able to
perform program merge in a multilingual setting with Java, JavaScript,
TypeScript, and C# programming languages, generalizing zero-shot to unseen
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It's not Rocket Science : Interpreting Figurative Language in Narratives. (arXiv:2109.00087v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00087">
<div class="article-summary-box-inner">
<span><p>Figurative language is ubiquitous in English. Yet, the vast majority of NLP
research focuses on literal language. Existing text representations by design
rely on compositionality, while figurative language is often non-compositional.
In this paper, we study the interpretation of two non-compositional figurative
languages (idioms and similes). We collected datasets of fictional narratives
containing a figurative expression along with crowd-sourced plausible and
implausible continuations relying on the correct interpretation of the
expression. We then trained models to choose or generate the plausible
continuation. Our experiments show that models based solely on pre-trained
language models perform substantially worse than humans on these tasks. We
additionally propose knowledge-enhanced models, adopting human strategies for
interpreting figurative language: inferring meaning from the context and
relying on the constituent word's literal meanings. The knowledge-enhanced
models improve the performance on both the discriminative and generative tasks,
further bridging the gap from human performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FinQA: A Dataset of Numerical Reasoning over Financial Data. (arXiv:2109.00122v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00122">
<div class="article-summary-box-inner">
<span><p>The sheer volume of financial statements makes it difficult for humans to
access and analyze a business's financials. Robust numerical reasoning likewise
faces unique challenges in this domain. In this work, we focus on answering
deep questions over financial data, aiming to automate the analysis of a large
corpus of financial documents. In contrast to existing tasks on general domain,
the finance domain includes complex numerical reasoning and understanding of
heterogeneous representations. To facilitate analytical progress, we propose a
new large-scale dataset, FinQA, with Question-Answering pairs over Financial
reports, written by financial experts. We also annotate the gold reasoning
programs to ensure full explainability. We further introduce baselines and
conduct comprehensive experiments in our dataset. The results demonstrate that
popular, large, pre-trained models fall far short of expert humans in acquiring
finance knowledge and in complex multi-step numerical reasoning on that
knowledge. Our dataset -- the first of its kind -- should therefore enable
significant, new community research into complex application domains. The
dataset and code are publicly available\url{https://github.com/czyssrs/FinQA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Unsupervised Method for Building Sentence Simplification Corpora in Multiple Languages. (arXiv:2109.00165v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00165">
<div class="article-summary-box-inner">
<span><p>The availability of parallel sentence simplification (SS) is scarce for
neural SS modelings. We propose an unsupervised method to build SS corpora from
large-scale bilingual translation corpora, alleviating the need for SS
supervised corpora. Our method is motivated by the following two findings:
neural machine translation model usually tends to generate more high-frequency
tokens and the difference of text complexity levels exists between the source
and target language of a translation corpus. By taking the pair of the source
sentences of translation corpus and the translations of their references in a
bridge language, we can construct large-scale pseudo parallel SS data. Then, we
keep these sentence pairs with a higher complexity difference as SS sentence
pairs. The building SS corpora with an unsupervised approach can satisfy the
expectations that the aligned sentences preserve the same meanings and have
difference in text complexity levels. Experimental results show that SS methods
trained by our corpora achieve the state-of-the-art results and significantly
outperform the results on English benchmark WikiLarge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Have Been Learned & What Should Be Learned? An Empirical Study of How to Selectively Augment Text for Classification. (arXiv:2109.00175v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00175">
<div class="article-summary-box-inner">
<span><p>Text augmentation techniques are widely used in text classification problems
to improve the performance of classifiers, especially in low-resource
scenarios. Whilst lots of creative text augmentation methods have been
designed, they augment the text in a non-selective manner, which means the less
important or noisy words have the same chances to be augmented as the
informative words, and thereby limits the performance of augmentation. In this
work, we systematically summarize three kinds of role keywords, which have
different functions for text classification, and design effective methods to
extract them from the text. Based on these extracted role keywords, we propose
STA (Selective Text Augmentation) to selectively augment the text, where the
informative, class-indicating words are emphasized but the irrelevant or noisy
words are diminished. Extensive experiments on four English and Chinese text
classification benchmark datasets demonstrate that STA can substantially
outperform the non-selective text augmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Problem Learning: Towards the Free Will of Machines. (arXiv:2109.00177v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00177">
<div class="article-summary-box-inner">
<span><p>A machine intelligence pipeline usually consists of six components: problem,
representation, model, loss, optimizer and metric. Researchers have worked hard
trying to automate many components of the pipeline. However, one key component
of the pipeline--problem definition--is still left mostly unexplored in terms
of automation. Usually, it requires extensive efforts from domain experts to
identify, define and formulate important problems in an area. However,
automatically discovering research or application problems for an area is
beneficial since it helps to identify valid and potentially important problems
hidden in data that are unknown to domain experts, expand the scope of tasks
that we can do in an area, and even inspire completely new findings.
</p>
<p>This paper describes Problem Learning, which aims at learning to discover and
define valid and ethical problems from data or from the machine's interaction
with the environment. We formalize problem learning as the identification of
valid and ethical problems in a problem space and introduce several possible
approaches to problem learning. In a broader sense, problem learning is an
approach towards the free will of intelligent machines. Currently, machines are
still limited to solving the problems defined by humans, without the ability or
flexibility to freely explore various possible problems that are even unknown
to humans. Though many machine learning techniques have been developed and
integrated into intelligent systems, they still focus on the means rather than
the purpose in that machines are still solving human defined problems. However,
proposing good problems is sometimes even more important than solving problems,
because a good problem can help to inspire new ideas and gain deeper
understandings. The paper also discusses the ethical implications of problem
learning under the background of Responsible AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapted End-to-End Coreference Resolution System for Anaphoric Identities in Dialogues. (arXiv:2109.00185v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00185">
<div class="article-summary-box-inner">
<span><p>We present an effective system adapted from the end-to-end neural coreference
resolution model, targeting on the task of anaphora resolution in dialogues.
Three aspects are specifically addressed in our approach, including the support
of singletons, encoding speakers and turns throughout dialogue interactions,
and knowledge transfer utilizing existing resources. Despite the simplicity of
our adaptation strategies, they are shown to bring significant impact to the
final performance, with up to 27 F1 improvement over the baseline. Our final
system ranks the 1st place on the leaderboard of the anaphora resolution track
in the CRAC 2021 shared task, and achieves the best evaluation results on all
four datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Predictive Uncertainty under Distributional Shift on Dialogue Dataset. (arXiv:2109.00186v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00186">
<div class="article-summary-box-inner">
<span><p>In open-domain dialogues, predictive uncertainties are mainly evaluated in a
domain shift setting to cope with out-of-distribution inputs. However, in
real-world conversations, there could be more extensive distributional shifted
inputs than the out-of-distribution. To evaluate this, we first propose two
methods, Unknown Word (UW) and Insufficient Context (IC), enabling gradual
distributional shifts by corruption on the dialogue dataset. We then
investigate the effect of distributional shifts on accuracy and calibration.
Our experiments show that the performance of existing uncertainty estimation
methods consistently degrades with intensifying the shift. The results suggest
that the proposed methods could be useful for evaluating the calibration of
dialogue systems under distributional shifts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Cross-Lingual Transfer via Self-Learning with Uncertainty Estimation. (arXiv:2109.00194v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00194">
<div class="article-summary-box-inner">
<span><p>Recent multilingual pre-trained language models have achieved remarkable
zero-shot performance, where the model is only finetuned on one source language
and directly evaluated on target languages. In this work, we propose a
self-learning framework that further utilizes unlabeled data of target
languages, combined with uncertainty estimation in the process to select
high-quality silver labels. Three different uncertainties are adapted and
analyzed specifically for the cross lingual transfer: Language
Heteroscedastic/Homoscedastic Uncertainty (LEU/LOU), Evidential Uncertainty
(EVI). We evaluate our framework with uncertainties on two cross-lingual tasks
including Named Entity Recognition (NER) and Natural Language Inference (NLI)
covering 40 languages in total, which outperforms the baselines significantly
by 10 F1 on average for NER and 2.5 accuracy score for NLI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pattern-based Acquisition of Scientific Entities from Scholarly Article Titles. (arXiv:2109.00199v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00199">
<div class="article-summary-box-inner">
<span><p>We describe a rule-based approach for the automatic acquisition of scientific
entities from scholarly article titles. Two observations motivated the
approach: (i) noting the concentration of an article's contribution information
in its title; and (ii) capturing information pattern regularities via a system
of rules that alleviate the human annotation task in creating gold standards
that annotate single instances at a time. We identify a set of lexico-syntactic
patterns that are easily recognizable, that occur frequently, and that
generally indicates the scientific entity type of interest about the scholarly
contribution.
</p>
<p>A subset of the acquisition algorithm is implemented for article titles in
the Computational Linguistics (CL) scholarly domain. The tool called
ORKG-Title-Parser, in its first release, identifies the following six concept
types of scientific terminology from the CL paper titles, viz. research
problem, solution, resource, language, tool, and method. It has been
empirically evaluated on a collection of 50,237 titles that cover nearly all
articles in the ACL Anthology. It has extracted 19,799 research problems;
18,111 solutions; 20,033 resources; 1,059 languages; 6,878 tools; and 21,687
methods at an average extraction precision of 75%. The code and related data
resources are publicly available at
https://gitlab.com/TIBHannover/orkg/orkg-title-parser.
</p>
<p>Finally, in the article, we discuss extensions and applications to areas such
as scholarly knowledge graph (SKG) creation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dataset for Identification of Homophobia and Transophobia in Multilingual YouTube Comments. (arXiv:2109.00227v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00227">
<div class="article-summary-box-inner">
<span><p>The increased proliferation of abusive content on social media platforms has
a negative impact on online users. The dread, dislike, discomfort, or mistrust
of lesbian, gay, transgender or bisexual persons is defined as
homophobia/transphobia. Homophobic/transphobic speech is a type of offensive
language that may be summarized as hate speech directed toward LGBT+ people,
and it has been a growing concern in recent years. Online
homophobia/transphobia is a severe societal problem that can make online
platforms poisonous and unwelcome to LGBT+ people while also attempting to
eliminate equality, diversity, and inclusion. We provide a new hierarchical
taxonomy for online homophobia and transphobia, as well as an expert-labelled
dataset that will allow homophobic/transphobic content to be automatically
identified. We educated annotators and supplied them with comprehensive
annotation rules because this is a sensitive issue, and we previously
discovered that untrained crowdsourcing annotators struggle with diagnosing
homophobia due to cultural and other prejudices. The dataset comprises 15,141
annotated multilingual comments. This paper describes the process of building
the dataset, qualitative analysis of data, and inter-annotator agreement. In
addition, we create baseline models for the dataset. To the best of our
knowledge, our dataset is the first such dataset created. Warning: This paper
contains explicit statements of homophobia, transphobia, stereotypes which may
be distressing to some readers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OptAGAN: Entropy-based finetuning on text VAE-GAN. (arXiv:2109.00239v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00239">
<div class="article-summary-box-inner">
<span><p>Transfer learning through large pre-trained models has changed the landscape
of current applications in natural language processing (NLP). Recently Optimus,
a variational autoencoder (VAE) which combines two pre-trained models, BERT and
GPT-2, has been released, and its combination with generative adversial
networks (GANs) has been shown to produce novel, yet very human-looking text.
The Optimus and GANs combination avoids the troublesome application of GANs to
the discrete domain of text, and prevents the exposure bias of standard maximum
likelihood methods. We combine the training of GANs in the latent space, with
the finetuning of the decoder of Optimus for single word generation. This
approach lets us model both the high-level features of the sentences, and the
low-level word-by-word generation. We finetune using reinforcement learning
(RL) by exploiting the structure of GPT-2 and by adding entropy-based
intrinsically motivated rewards to balance between quality and diversity. We
benchmark the results of the VAE-GAN model, and show the improvements brought
by our RL finetuning on three widely used datasets for text generation, with
results that greatly surpass the current state-of-the-art for the quality of
the generated texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aligning Cross-lingual Sentence Representations with Dual Momentum Contrast. (arXiv:2109.00253v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00253">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose to align sentence representations from different
languages into a unified embedding space, where semantic similarities (both
cross-lingual and monolingual) can be computed with a simple dot product.
Pre-trained language models are fine-tuned with the translation ranking task.
Existing work (Feng et al., 2020) uses sentences within the same batch as
negatives, which can suffer from the issue of easy negatives. We adapt MoCo (He
et al., 2020) to further improve the quality of alignment. As the experimental
results show, the sentence representations produced by our model achieve the
new state-of-the-art on several tasks, including Tatoeba en-zh similarity
search (Artetxe and Schwenk, 2019b), BUCC en-zh bitext mining, and semantic
textual similarity on 7 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting all Aspect-polarity Pairs Jointly in a Text with Relation Extraction Approach. (arXiv:2109.00256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00256">
<div class="article-summary-box-inner">
<span><p>Extracting aspect-polarity pairs from texts is an important task of
fine-grained sentiment analysis. While the existing approaches to this task
have gained many progresses, they are limited at capturing relationships among
aspect-polarity pairs in a text, thus degrading the extraction performance.
Moreover, the existing state-of-the-art approaches, namely token-based
se-quence tagging and span-based classification, have their own defects such as
polarity inconsistency resulted from separately tagging tokens in the former
and the heterogeneous categorization in the latter where aspect-related and
polarity-related labels are mixed. In order to remedy the above defects,
in-spiring from the recent advancements in relation extraction, we propose to
generate aspect-polarity pairs directly from a text with relation extraction
technology, regarding aspect-pairs as unary relations where aspects are
enti-ties and the corresponding polarities are relations. Based on the
perspective, we present a position- and aspect-aware sequence2sequence model
for joint extraction of aspect-polarity pairs. The model is characterized with
its ability to capture not only relationships among aspect-polarity pairs in a
text through the sequence decoding, but also correlations between an aspect and
its polarity through the position- and aspect-aware attentions. The
experi-ments performed on three benchmark datasets demonstrate that our model
outperforms the existing state-of-the-art approaches, making significant
im-provement over them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Context and High-Coverage Grammar for Conversational Question Answering over Knowledge Graphs. (arXiv:2109.00269v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00269">
<div class="article-summary-box-inner">
<span><p>We tackle the problem of weakly-supervised conversational Question Answering
over large Knowledge Graphs using a neural semantic parsing approach. We
introduce a new Logical Form (LF) grammar that can model a wide range of
queries on the graph while remaining sufficiently simple to generate
supervision data efficiently. Our Transformer-based model takes a JSON-like
structure as input, allowing us to easily incorporate both Knowledge Graph and
conversational contexts. This structured input is transformed to lists of
embeddings and then fed to standard attention layers. We validate our approach,
both in terms of grammar coverage and LF execution accuracy, on two publicly
available datasets, CSQA and ConvQuestions, both grounded in Wikidata. On CSQA,
our approach increases the coverage from $80\%$ to $96.2\%$, and the LF
execution accuracy from $70.6\%$ to $75.6\%$, with respect to previous
state-of-the-art results. On ConvQuestions, we achieve competitive results with
respect to the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering Representation Sprachbund For Multilingual Pre-Training. (arXiv:2109.00271v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00271">
<div class="article-summary-box-inner">
<span><p>Multilingual pre-trained models have demonstrated their effectiveness in many
multilingual NLP tasks and enabled zero-shot or few-shot transfer from
high-resource languages to low resource ones. However, due to significant
typological differences and contradictions between some languages, such models
usually perform poorly on many languages and cross-lingual settings, which
shows the difficulty of learning a single model to handle massive diverse
languages well at the same time. To alleviate this issue, we present a new
multilingual pre-training pipeline. We propose to generate language
representation from multilingual pre-trained models and conduct linguistic
analysis to show that language representation similarity reflect linguistic
similarity from multiple perspectives, including language family, geographical
sprachbund, lexicostatistics and syntax. Then we cluster all the target
languages into multiple groups and name each group as a representation
sprachbund. Thus, languages in the same representation sprachbund are supposed
to boost each other in both pre-training and fine-tuning as they share rich
linguistic similarity. We pre-train one multilingual model for each
representation sprachbund. Experiments are conducted on cross-lingual
benchmarks and significant improvements are achieved compared to strong
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discourse Analysis of Covid-19 in Persian Twitter Social Networks Using Graph Mining and Natural Language Processing. (arXiv:2109.00298v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00298">
<div class="article-summary-box-inner">
<span><p>One of the new scientific ways of understanding discourse dynamics is
analyzing the public data of social networks. This research's aim is
Post-structuralist Discourse Analysis (PDA) of Covid-19 phenomenon (inspired by
Laclau and Mouffe's Discourse Theory) by using Intelligent Data Mining for
Persian Society. The examined big data is five million tweets from 160,000
users of the Persian Twitter network to compare two discourses. Besides
analyzing the tweet texts individually, a social network graph database has
been created based on retweets relationships. We use the VoteRank algorithm to
introduce and rank people whose posts become word of mouth, provided that the
total information spreading scope is maximized over the network. These users
are also clustered according to their word usage pattern (the Gaussian Mixture
Model is used). The constructed discourse of influential spreaders is compared
to the most active users. This analysis is done based on Covid-related posts
over eight episodes. Also, by relying on the statistical content analysis and
polarity of tweet words, discourse analysis is done for the whole mentioned
subpopulations, especially for the top individuals. The most important result
of this research is that the Twitter subjects' discourse construction is
government-based rather than community-based. The analyzed Iranian society does
not consider itself responsible for the Covid-19 wicked problem, does not
believe in participation, and expects the government to solve all problems. The
most active and most influential users' similarity is that political, national,
and critical discourse construction is the predominant one. In addition to the
advantages of its research methodology, it is necessary to pay attention to the
study's limitations. Suggestion for future encounters of Iranian society with
similar crises is given.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$\infty$-former: Infinite Memory Transformer. (arXiv:2109.00301v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00301">
<div class="article-summary-box-inner">
<span><p>Transformers struggle when attending to long contexts, since the amount of
computation grows with the context length, and therefore they cannot model
long-term memories effectively. Several variations have been proposed to
alleviate this problem, but they all have a finite memory capacity, being
forced to drop old information. In this paper, we propose the $\infty$-former,
which extends the vanilla transformer with an unbounded long-term memory. By
making use of a continuous-space attention mechanism to attend over the
long-term memory, the $\infty$-former's attention complexity becomes
independent of the context length. Thus, it is able to model arbitrarily long
contexts and maintain "sticky memories" while keeping a fixed computation
budget. Experiments on a synthetic sorting task demonstrate the ability of the
$\infty$-former to retain information from long sequences. We also perform
experiments on language modeling, by training a model from scratch and by
fine-tuning a pre-trained language model, which show benefits of unbounded
long-term memories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring deep learning methods for recognizing rare diseases and their clinical manifestations from texts. (arXiv:2109.00343v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00343">
<div class="article-summary-box-inner">
<span><p>Although rare diseases are characterized by low prevalence, approximately 300
million people are affected by a rare disease. The early and accurate diagnosis
of these conditions is a major challenge for general practitioners, who do not
have enough knowledge to identify them. In addition to this, rare diseases
usually show a wide variety of manifestations, which might make the diagnosis
even more difficult. A delayed diagnosis can negatively affect the patient's
life. Therefore, there is an urgent need to increase the scientific and medical
knowledge about rare diseases. Natural Language Processing (NLP) and Deep
Learning can help to extract relevant information about rare diseases to
facilitate their diagnosis and treatments. The paper explores the use of
several deep learning techniques such as Bidirectional Long Short Term Memory
(BiLSTM) networks or deep contextualized word representations based on
Bidirectional Encoder Representations from Transformers (BERT) to recognize
rare diseases and their clinical manifestations (signs and symptoms) in the
RareDis corpus. This corpus contains more than 5,000 rare diseases and almost
6,000 clinical manifestations. BioBERT, a domain-specific language
representation based on BERT and trained on biomedical corpora, obtains the
best results. In particular, this model obtains an F1-score of 85.2% for rare
diseases, outperforming all the other models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConRPG: Paraphrase Generation using Contexts as Regularizer. (arXiv:2109.00363v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00363">
<div class="article-summary-box-inner">
<span><p>A long-standing issue with paraphrase generation is how to obtain reliable
supervision signals. In this paper, we propose an unsupervised paradigm for
paraphrase generation based on the assumption that the probabilities of
generating two sentences with the same meaning given the same context should be
the same. Inspired by this fundamental idea, we propose a pipelined system
which consists of paraphrase candidate generation based on contextual language
models, candidate filtering using scoring functions, and paraphrase model
training based on the selected candidates. The proposed paradigm offers merits
over existing paraphrase generation methods: (1) using the context regularizer
on meanings, the model is able to generate massive amounts of high-quality
paraphrase pairs; and (2) using human-interpretable scoring functions to select
paraphrase pairs from candidates, the proposed framework provides a channel for
developers to intervene with the data generation process, leading to a more
controllable model. Experimental results across different tasks and datasets
demonstrate that the effectiveness of the proposed model in both supervised and
unsupervised setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chronic Pain and Language: A Topic Modelling Approach to Personal Pain Descriptions. (arXiv:2109.00402v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00402">
<div class="article-summary-box-inner">
<span><p>Chronic pain is recognized as a major health problem, with impacts not only
at the economic, but also at the social, and individual levels. Being a private
and subjective experience, it is impossible to externally and impartially
experience, describe, and interpret chronic pain as a purely noxious stimulus
that would directly point to a causal agent and facilitate its mitigation,
contrary to acute pain, the assessment of which is usually straightforward.
Verbal communication is, thus, key to convey relevant information to health
professionals that would otherwise not be accessible to external entities,
namely, intrinsic qualities about the painful experience and the patient. We
propose and discuss a topic modelling approach to recognize patterns in verbal
descriptions of chronic pain, and use these patterns to quantify and qualify
experiences of pain. Our approaches allow for the extraction of novel insights
on chronic pain experiences from the obtained topic models and latent spaces.
We argue that our results are clinically relevant for the assessment and
management of chronic pain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis. (arXiv:2109.00412v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00412">
<div class="article-summary-box-inner">
<span><p>In multimodal sentiment analysis (MSA), the performance of a model highly
depends on the quality of synthesized embeddings. These embeddings are
generated from the upstream process called multimodal fusion, which aims to
extract and combine the input unimodal raw data to produce a richer multimodal
representation. Previous work either back-propagates the task loss or
manipulates the geometric property of feature spaces to produce favorable
fusion results, which neglects the preservation of critical task-related
information that flows from input to the fusion results. In this work, we
propose a framework named MultiModal InfoMax (MMIM), which hierarchically
maximizes the Mutual Information (MI) in unimodal input pairs (inter-modality)
and between multimodal fusion result and unimodal input in order to maintain
task-related information through multimodal fusion. The framework is jointly
trained with the main task (MSA) to improve the performance of the downstream
MSA task. To address the intractable issue of MI bounds, we further formulate a
set of computationally simple parametric and non-parametric methods to
approximate their truth value. Experimental results on the two widely used
datasets demonstrate the efficacy of our approach. The implementation of this
work is publicly available at
https://github.com/declare-lab/Multimodal-Infomax.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Adversarial Generation for Neural Machine Translation. (arXiv:2109.00417v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00417">
<div class="article-summary-box-inner">
<span><p>Attacking Neural Machine Translation models is an inherently combinatorial
task on discrete sequences, solved with approximate heuristics. Most methods
use the gradient to attack the model on each sample independently. Instead of
mechanically applying the gradient, could we learn to produce meaningful
adversarial attacks ? In contrast to existing approaches, we learn to attack a
model by training an adversarial generator based on a language model. We
propose the Masked Adversarial Generation (MAG) model, that learns to perturb
the translation model throughout the training process. The experiments show
that it improves the robustness of machine translation models, while being
faster than competing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M^2-MedDialog: A Dataset and Benchmarks for Multi-domain Multi-service Medical Dialogues. (arXiv:2109.00430v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00430">
<div class="article-summary-box-inner">
<span><p>Medical dialogue systems (MDSs) aim to assist doctors and patients with a
range of professional medical services, i.e., diagnosis, consultation, and
treatment. However, one-stop MDS is still unexplored because: (1) no dataset
has so large-scale dialogues contains both multiple medical services and
fine-grained medical labels (i.e., intents, slots, values); (2) no model has
addressed a MDS based on multiple-service conversations in a unified framework.
In this work, we first build a Multiple-domain Multiple-service medical
dialogue (M^2-MedDialog)dataset, which contains 1,557 conversations between
doctors and patients, covering 276 types of diseases, 2,468 medical entities,
and 3 specialties of medical services. To the best of our knowledge, it is the
only medical dialogue dataset that includes both multiple medical services and
fine-grained medical labels. Then, we formulate a one-stop MDS as a
sequence-to-sequence generation problem. We unify a MDS with causal language
modeling and conditional causal language modeling, respectively. Specifically,
we employ several pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5)
and their variants to get benchmarks on M^2-MedDialog dataset. We also propose
pseudo labeling and natural perturbation methods to expand M2-MedDialog dataset
and enhance the state-of-the-art pretrained models. We demonstrate the results
achieved by the benchmarks so far through extensive experiments on
M2-MedDialog. We release the dataset, the code, as well as the evaluation
scripts to facilitate future research in this important research direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Position Masking for Improved Layout-Aware Document Understanding. (arXiv:2109.00442v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00442">
<div class="article-summary-box-inner">
<span><p>Natural language processing for document scans and PDFs has the potential to
enormously improve the efficiency of business processes. Layout-aware word
embeddings such as LayoutLM have shown promise for classification of and
information extraction from such documents. This paper proposes a new
pre-training task called that can improve performance of layout-aware word
embeddings that incorporate 2-D position embeddings. We compare models
pre-trained with only language masking against models pre-trained with both
language masking and position masking, and we find that position masking
improves performance by over 5% on a form understanding task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Capturing Stance Dynamics in Social Media: Open Challenges and Research Directions. (arXiv:2109.00475v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00475">
<div class="article-summary-box-inner">
<span><p>Social media platforms provide a goldmine for mining public opinion on issues
of wide societal interest. Opinion mining is a problem that can be
operationalised by capturing and aggregating the stance of individual social
media posts as supporting, opposing or being neutral towards the issue at hand.
While most prior work in stance detection has investigated datasets with
limited time coverage, interest in investigating longitudinal datasets has
recently increased. Evolving dynamics in linguistic and behavioural patterns
observed in new data require in turn adapting stance detection systems to deal
with the changes. In this survey paper, we investigate the intersection between
computational linguistics and the temporal evolution of human communication in
digital media. We perform a critical review in emerging research considering
dynamics, exploring different semantic and pragmatic factors that impact
linguistic data in general, and stance particularly. We further discuss current
directions in capturing stance dynamics in social media. We organise the
challenges of dealing with stance dynamics, identify open challenges and
discuss future directions in three key dimensions: utterance, context and
influence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey of Low-Resource Machine Translation. (arXiv:2109.00486v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00486">
<div class="article-summary-box-inner">
<span><p>We present a survey covering the state of the art in low-resource machine
translation. There are currently around 7000 languages spoken in the world and
almost all language pairs lack significant resources for training machine
translation models. There has been increasing interest in research addressing
the challenge of producing useful translation models when very little
translated training data is available. We present a high level summary of this
topical field and provide an overview of best practices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Iterative Knowledge Transfer Network with Routing for Aspect-based Sentiment Analysis. (arXiv:2004.01935v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.01935">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) mainly involves three subtasks: aspect
term extraction, opinion term extraction, and aspect-level sentiment
classification, which are typically handled in a separate or joint manner.
However, previous approaches do not well exploit the interactive relations
among three subtasks and do not pertinently leverage the easily available
document-level labeled domain/sentiment knowledge, which restricts their
performances. To address these issues, we propose a novel Iterative
Multi-Knowledge Transfer Network (IMKTN) for end-to-end ABSA. For one thing,
through the interactive correlations between the ABSA subtasks, our IMKTN
transfers the task-specific knowledge from any two of the three subtasks to
another one at the token level by utilizing a well-designed routing algorithm,
that is, any two of the three subtasks will help the third one. For another,
our IMKTN pertinently transfers the document-level knowledge, i.e.,
domain-specific and sentiment-related knowledge, to the aspect-level subtasks
to further enhance the corresponding performance. Experimental results on three
benchmark datasets demonstrate the effectiveness and superiority of our
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speaker anonymisation using the McAdams coefficient. (arXiv:2011.01130v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.01130">
<div class="article-summary-box-inner">
<span><p>Anonymisation has the goal of manipulating speech signals in order to degrade
the reliability of automatic approaches to speaker recognition, while
preserving other aspects of speech, such as those relating to intelligibility
and naturalness. This paper reports an approach to anonymisation that, unlike
other current approaches, requires no training data, is based upon well-known
signal processing techniques and is both efficient and effective. The proposed
solution uses the McAdams coefficient to transform the spectral envelope of
speech signals. Results derived using common VoicePrivacy 2020 databases and
protocols show that random, optimised transformations can outperform competing
solutions in terms of anonymisation while causing only modest, additional
degradations to intelligibility, even in the case of a semi-informed privacy
adversary.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Evolution of Word Order. (arXiv:2101.09579v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.09579">
<div class="article-summary-box-inner">
<span><p>Most natural languages have a predominant or fixed word order. For example in
English the word order is usually Subject-Verb-Object. This work attempts to
explain this phenomenon as well as other typological findings regarding word
order from a functional perspective. In particular, we examine whether fixed
word order provides a functional advantage, explaining why these languages are
prevalent. To this end, we consider an evolutionary model of language and
demonstrate, both theoretically and using genetic algorithms, that a language
with a fixed word order is optimal. We also show that adding information to the
sentence, such as case markers and noun-verb distinction, reduces the need for
fixed word order, in accordance with the typological findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion Dynamics in Movie Dialogues. (arXiv:2103.01345v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01345">
<div class="article-summary-box-inner">
<span><p>Emotion dynamics is a framework for measuring how an individual's emotions
change over time. It is a powerful tool for understanding how we behave and
interact with the world. In this paper, we introduce a framework to track
emotion dynamics through one's utterances. Specifically we introduce a number
of utterance emotion dynamics (UED) metrics inspired by work in Psychology. We
use this approach to trace emotional arcs of movie characters. We analyze
thousands of such character arcs to test hypotheses that inform our broader
understanding of stories. Notably, we show that there is a tendency for
characters to use increasingly more negative words and become increasingly
emotionally discordant with each other until about 90 percent of the narrative
length. UED also has applications in behavior studies, social sciences, and
public health.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Data-Centric Framework for Composable NLP Workflows. (arXiv:2103.01834v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01834">
<div class="article-summary-box-inner">
<span><p>Empirical natural language processing (NLP) systems in application domains
(e.g., healthcare, finance, education) involve interoperation among multiple
components, ranging from data ingestion, human annotation, to text retrieval,
analysis, generation, and visualization. We establish a unified open-source
framework to support fast development of such sophisticated NLP workflows in a
composable manner. The framework introduces a uniform data representation to
encode heterogeneous results by a wide range of NLP tasks. It offers a large
repository of processors for NLP tasks, visualization, and annotation, which
can be easily assembled with full interoperability under the unified
representation. The highly extensible framework allows plugging in custom
processors from external off-the-shelf NLP and deep learning libraries. The
whole framework is delivered through two modularized yet integratable
open-source projects, namely Forte (for workflow infrastructure and NLP
function processors) and Stave (for user interaction, visualization, and
annotation).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SciCo: Hierarchical Cross-Document Coreference for Scientific Concepts. (arXiv:2104.08809v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08809">
<div class="article-summary-box-inner">
<span><p>Determining coreference of concept mentions across multiple documents is a
fundamental task in natural language understanding. Previous work on
cross-document coreference resolution (CDCR) typically considers mentions of
events in the news, which seldom involve abstract technical concepts that are
prevalent in science and technology. These complex concepts take diverse or
ambiguous forms and have many hierarchical levels of granularity (e.g., tasks
and subtasks), posing challenges for CDCR. We present a new task of
Hierarchical CDCR (H-CDCR) with the goal of jointly inferring coreference
clusters and hierarchy between them. We create SciCo, an expert-annotated
dataset for H-CDCR in scientific papers, 3X larger than the prominent ECB+
resource. We study strong baseline models that we customize for H-CDCR, and
highlight challenges for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-training for Spoken Language Understanding with Joint Textual and Phonetic Representation Learning. (arXiv:2104.10357v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10357">
<div class="article-summary-box-inner">
<span><p>In the traditional cascading architecture for spoken language understanding
(SLU), it has been observed that automatic speech recognition errors could be
detrimental to the performance of natural language understanding. End-to-end
(E2E) SLU models have been proposed to directly map speech input to desired
semantic frame with a single model, hence mitigating ASR error propagation.
Recently, pre-training technologies have been explored for these E2E models. In
this paper, we propose a novel joint textual-phonetic pre-training approach for
learning spoken language representations, aiming at exploring the full
potentials of phonetic information to improve SLU robustness to ASR errors. We
explore phoneme labels as high-level speech features, and design and compare
pre-training tasks based on conditional masked language model objectives and
inter-sentence relation objectives. We also investigate the efficacy of
combining textual and phonetic information during fine-tuning. Experimental
results on spoken language understanding benchmarks, Fluent Speech Commands and
SNIPS, show that the proposed approach significantly outperforms strong
baseline models and improves robustness of spoken language understanding to ASR
errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-NERD: A Few-Shot Named Entity Recognition Dataset. (arXiv:2105.07464v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07464">
<div class="article-summary-box-inner">
<span><p>Recently, considerable literature has grown up around the theme of few-shot
named entity recognition (NER), but little published benchmark data
specifically focused on the practical and challenging task. Current approaches
collect existing supervised NER datasets and re-organize them to the few-shot
setting for empirical study. These strategies conventionally aim to recognize
coarse-grained entity types with few examples, while in practice, most unseen
entity types are fine-grained. In this paper, we present Few-NERD, a
large-scale human-annotated few-shot NER dataset with a hierarchy of 8
coarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238
sentences from Wikipedia, 4,601,160 words are included and each is annotated as
context or a part of a two-level entity type. To the best of our knowledge,
this is the first few-shot NER dataset and the largest human-crafted NER
dataset. We construct benchmark tasks with different emphases to
comprehensively assess the generalization capability of models. Extensive
empirical results and analysis show that Few-NERD is challenging and the
problem requires further research. We make Few-NERD public at
https://ningding97.github.io/fewnerd/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence-Level Training for Non-Autoregressive Neural Machine Translation. (arXiv:2106.08122v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08122">
<div class="article-summary-box-inner">
<span><p>In recent years, Neural Machine Translation (NMT) has achieved notable
results in various translation tasks. However, the word-by-word generation
manner determined by the autoregressive mechanism leads to high translation
latency of the NMT and restricts its low-latency applications.
Non-Autoregressive Neural Machine Translation (NAT) removes the autoregressive
mechanism and achieves significant decoding speedup through generating target
words independently and simultaneously. Nevertheless, NAT still takes the
word-level cross-entropy loss as the training objective, which is not optimal
because the output of NAT cannot be properly evaluated due to the multimodality
problem. In this article, we propose using sequence-level training objectives
to train NAT models, which evaluate the NAT outputs as a whole and correlates
well with the real translation quality. Firstly, we propose training NAT models
to optimize sequence-level evaluation metrics (e.g., BLEU) based on several
novel reinforcement algorithms customized for NAT, which outperforms the
conventional method by reducing the variance of gradient estimation. Secondly,
we introduce a novel training objective for NAT models, which aims to minimize
the Bag-of-Ngrams (BoN) difference between the model output and the reference
sentence. The BoN training objective is differentiable and can be calculated
efficiently without doing any approximations. Finally, we apply a three-stage
training strategy to combine these two methods to train the NAT model. We
validate our approach on four translation tasks (WMT14 En$\leftrightarrow$De,
WMT16 En$\leftrightarrow$Ro), which shows that our approach largely outperforms
NAT baselines and achieves remarkable performance on all translation tasks. The
source code is available at https://github.com/ictnlp/Seq-NAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SocialAI: Benchmarking Socio-Cognitive Abilities in Deep Reinforcement Learning Agents. (arXiv:2107.00956v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00956">
<div class="article-summary-box-inner">
<span><p>Building embodied autonomous agents capable of participating in social
interactions with humans is one of the main challenges in AI. Within the Deep
Reinforcement Learning (DRL) field, this objective motivated multiple works on
embodied language use. However, current approaches focus on language as a
communication tool in very simplified and non-diverse social situations: the
"naturalness" of language is reduced to the concept of high vocabulary size and
variability. In this paper, we argue that aiming towards human-level AI
requires a broader set of key social skills: 1) language use in complex and
variable social contexts; 2) beyond language, complex embodied communication in
multimodal settings within constantly evolving social worlds. We explain how
concepts from cognitive sciences could help AI to draw a roadmap towards
human-like intelligence, with a focus on its social dimensions. As a first
step, we propose to expand current research to a broader set of core social
skills. To do this, we present SocialAI, a benchmark to assess the acquisition
of social skills of DRL agents using multiple grid-world environments featuring
other (scripted) social agents. We then study the limits of a recent SOTA DRL
approach when tested on SocialAI and discuss important next steps towards
proficient social agents. Videos and code are available at
https://sites.google.com/view/socialai.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Higher-Order Concurrency for Microcontrollers. (arXiv:2108.07805v2 [cs.PL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07805">
<div class="article-summary-box-inner">
<span><p>Programming microcontrollers involves low-level interfacing with hardware and
peripherals that are concurrent and reactive. Such programs are typically
written in a mixture of C and assembly using concurrent language extensions
(like $\texttt{FreeRTOS tasks}$ and $\texttt{semaphores}$), resulting in
unsafe, callback-driven, error-prone and difficult-to-maintain code.
</p>
<p>We address this challenge by introducing $\texttt{SenseVM}$ - a
bytecode-interpreted virtual machine that provides a message-passing based
$\textit{higher-order concurrency}$ model, originally introduced by Reppy, for
microcontroller programming. This model treats synchronous operations as
first-class values (called $\texttt{Events}$) akin to the treatment of
first-class functions in functional languages. This primarily allows the
programmer to compose and tailor their own concurrency abstractions and,
additionally, abstracts away unsafe memory operations, common in shared-memory
concurrency models, thereby making microcontroller programs safer, composable
and easier-to-maintain.
</p>
<p>Our VM is made portable via a low-level $\textit{bridge}$ interface, built
atop the embedded OS - Zephyr. The bridge is implemented by all drivers and
designed such that programming in response to a software message or a hardware
interrupt remains uniform and indistinguishable. In this paper we demonstrate
the features of our VM through an example, written in a Caml-like functional
language, running on the $\texttt{nRF52840}$ and $\texttt{STM32F4}$
microcontrollers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attentive fine-tuning of Transformers for Translation of low-resourced languages @LoResMT 2021. (arXiv:2108.08556v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08556">
<div class="article-summary-box-inner">
<span><p>This paper reports the Machine Translation (MT) systems submitted by the
IIITT team for the English-&gt;Marathi and English-&gt;Irish language pairs LoResMT
2021 shared task. The task focuses on getting exceptional translations for
rather low-resourced languages like Irish and Marathi. We fine-tune IndicTrans,
a pretrained multilingual NMT model for English-&gt;Marathi, using external
parallel corpus as input for additional training. We have used a pretrained
Helsinki-NLP Opus MT English-&gt;Irish model for the latter language pair. Our
approaches yield relatively promising results on the BLEU metrics. Under the
team name IIITT, our systems ranked 1, 1, and 2 in English-&gt;Marathi,
Irish-&gt;English, and English-&gt;Irish, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fastformer: Additive Attention Can Be All You Need. (arXiv:2108.09084v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09084">
<div class="article-summary-box-inner">
<span><p>Transformer is a powerful model for text understanding. However, it is
inefficient due to its quadratic complexity to input sequence length. Although
there are many methods on Transformer acceleration, they are still either
inefficient on long sequences or not effective enough. In this paper, we
propose Fastformer, which is an efficient Transformer model based on additive
attention. In Fastformer, instead of modeling the pair-wise interactions
between tokens, we first use additive attention mechanism to model global
contexts, and then further transform each token representation based on its
interaction with global context representations. In this way, Fastformer can
achieve effective context modeling with linear complexity. Extensive
experiments on five datasets show that Fastformer is much more efficient than
many existing Transformer models and can meanwhile achieve comparable or even
better long text modeling performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Wikipedia Article Quality in One Dimension by Extending ORES with Ordinal Regression. (arXiv:2108.10684v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10684">
<div class="article-summary-box-inner">
<span><p>Organizing complex peer production projects and advancing scientific
knowledge of open collaboration each depend on the ability to measure quality.
Article quality ratings on English language Wikipedia have been widely used by
both Wikipedia community members and academic researchers for purposes like
tracking knowledge gaps and studying how political polarization shapes
collaboration. Even so, measuring quality presents many methodological
challenges. The most widely used systems use labels on discrete ordinal scales
when assessing quality, but such labels can be inconvenient for statistics and
machine learning. Prior work handles this by assuming that different levels of
quality are "evenly spaced" from one another. This assumption runs counter to
intuitions about the relative degrees of effort needed to raise Wikipedia
encyclopedia articles to different quality levels. Furthermore, models from
prior work are fit to datasets that oversample high-quality articles. This
limits their accuracy for representative samples of articles or revisions. I
describe a technique extending the Wikimedia Foundations' ORES article quality
model to address these limitations. My method uses weighted ordinal regression
models to construct one-dimensional continuous measures of quality. While
scores from my technique and from prior approaches are correlated, my approach
improves accuracy for research datasets and provides evidence that the "evenly
spaced" assumption is unfounded in practice on English Wikipedia. I conclude
with recommendations for using quality scores in future research and include
the full code, data, and models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Why Intermediate-Task Fine-Tuning Works. (arXiv:2108.11696v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11696">
<div class="article-summary-box-inner">
<span><p>Supplementary Training on Intermediate Labeled-data Tasks (STILTs) is a
widely applied technique, which first fine-tunes the pretrained language models
on an intermediate task before on the target task of interest. While STILTs is
able to further improve the performance of pretrained language models, it is
still unclear why and when it works. Previous research shows that those
intermediate tasks involving complex inference, such as commonsense reasoning,
work especially well for RoBERTa. In this paper, we discover that the
improvement from an intermediate task could be orthogonal to it containing
reasoning or other complex skills -- a simple real-fake discrimination task
synthesized by GPT2 can benefit diverse target tasks. We conduct extensive
experiments to study the impact of different factors on STILTs. These findings
suggest rethinking the role of intermediate fine-tuning in the STILTs pipeline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Position-Invariant Truecasing with a Word-and-Character Hierarchical Recurrent Neural Network. (arXiv:2108.11943v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11943">
<div class="article-summary-box-inner">
<span><p>Truecasing is the task of restoring the correct case (uppercase or lowercase)
of noisy text generated either by an automatic system for speech recognition or
machine translation or by humans. It improves the performance of downstream NLP
tasks such as named entity recognition and language modeling. We propose a
fast, accurate and compact two-level hierarchical word-and-character-based
recurrent neural network model, the first of its kind for this problem. Using
sequence distillation, we also address the problem of truecasing while ignoring
token positions in the sentence, i.e. in a position-invariant manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Oriented Dialogue System as Natural Language Generation. (arXiv:2108.13679v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13679">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose to formulate the task-oriented dialogue system as
the purely natural language generation task, so as to fully leverage the
large-scale pre-trained models like GPT-2 and simplify complicated
delexicalization prepossessing. However, directly applying this method heavily
suffers from the dialogue entity inconsistency caused by the removal of
delexicalized tokens, as well as the catastrophic forgetting problem of the
pre-trained model during fine-tuning, leading to unsatisfactory performance. To
alleviate these problems, we design a novel GPT-Adapter-CopyNet network, which
incorporates the lightweight adapter and CopyNet modules into GPT-2 to achieve
better performance on transfer learning and dialogue entity generation.
Experimental results conducted on the DSTC8 Track 1 benchmark and MultiWOZ
dataset demonstrate that our proposed approach significantly outperforms
baseline models with a remarkable performance on automatic and human
evaluations.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Working Memory Connections for LSTM. (arXiv:2109.00020v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00020">
<div class="article-summary-box-inner">
<span><p>Recurrent Neural Networks with Long Short-Term Memory (LSTM) make use of
gating mechanisms to mitigate exploding and vanishing gradients when learning
long-term dependencies. For this reason, LSTMs and other gated RNNs are widely
adopted, being the standard de facto for many sequence modeling tasks. Although
the memory cell inside the LSTM contains essential information, it is not
allowed to influence the gating mechanism directly. In this work, we improve
the gate potential by including information coming from the internal cell
state. The proposed modification, named Working Memory Connection, consists in
adding a learnable nonlinear projection of the cell content into the network
gates. This modification can fit into the classical LSTM gates without any
assumption on the underlying task, being particularly effective when dealing
with longer sequences. Previous research effort in this direction, which goes
back to the early 2000s, could not bring a consistent improvement over vanilla
LSTM. As part of this paper, we identify a key issue tied to previous
connections that heavily limits their effectiveness, hence preventing a
successful integration of the knowledge coming from the internal cell state. We
show through extensive experimental evaluation that Working Memory Connections
constantly improve the performance of LSTMs on a variety of tasks. Numerical
results suggest that the cell state contains useful information that is worth
including in the gate structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DensePose 3D: Lifting Canonical Surface Maps of Articulated Objects to the Third Dimension. (arXiv:2109.00033v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00033">
<div class="article-summary-box-inner">
<span><p>We tackle the problem of monocular 3D reconstruction of articulated objects
like humans and animals. We contribute DensePose 3D, a method that can learn
such reconstructions in a weakly supervised fashion from 2D image annotations
only. This is in stark contrast with previous deformable reconstruction methods
that use parametric models such as SMPL pre-trained on a large dataset of 3D
object scans. Because it does not require 3D scans, DensePose 3D can be used
for learning a wide range of articulated categories such as different animal
species. The method learns, in an end-to-end fashion, a soft partition of a
given category-specific 3D template mesh into rigid parts together with a
monocular reconstruction network that predicts the part motions such that they
reproject correctly onto 2D DensePose-like surface annotations of the object.
The decomposition of the object into parts is regularized by expressing part
assignments as a combination of the smooth eigenfunctions of the
Laplace-Beltrami operator. We show significant improvements compared to
state-of-the-art non-rigid structure-from-motion baselines on both synthetic
and real data on categories of humans and animals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bio-inspired robot perception coupled with robot-modeled human perception. (arXiv:2109.00097v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00097">
<div class="article-summary-box-inner">
<span><p>My overarching research goal is to provide robots with perceptional abilities
that allow interactions with humans in a human-like manner. To develop these
perceptional abilities, I believe that it is useful to study the principles of
the human visual system. I use these principles to develop new computer vision
algorithms and validate their effectiveness in intelligent robotic systems. I
am enthusiastic about this approach as it offers the dual benefit of uncovering
principles inherent in the human visual system, as well as applying these
principles to its artificial counterpart. Fig. 1 contains a depiction of my
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-step Domain Adaptation for Mitosis Cell Detection in Histopathology Images. (arXiv:2109.00109v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00109">
<div class="article-summary-box-inner">
<span><p>We propose a two-step domain shift-invariant mitosis cell detection method
based on Faster RCNN and a convolutional neural network (CNN). We generate
various domain-shifted versions of existing histopathology images using a stain
augmentation technique, enabling our method to effectively learn various stain
domains and achieve better generalization. The performance of our method is
evaluated on the preliminary test data set of the MIDOG-2021 challenge. The
experimental results demonstrate that the proposed mitosis detection method can
achieve promising performance for domain-shifted histopathology images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPFN: Cascaded Primitive Fitting Networks for High-Resolution Point Clouds. (arXiv:2109.00113v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00113">
<div class="article-summary-box-inner">
<span><p>Representing human-made objects as a collection of base primitives has a long
history in computer vision and reverse engineering. In the case of
high-resolution point cloud scans, the challenge is to be able to detect both
large primitives as well as those explaining the detailed parts. While the
classical RANSAC approach requires case-specific parameter tuning,
state-of-the-art networks are limited by memory consumption of their backbone
modules such as PointNet++, and hence fail to detect the fine-scale primitives.
We present Cascaded Primitive Fitting Networks (CPFN) that relies on an
adaptive patch sampling network to assemble detection results of global and
local primitive detection networks. As a key enabler, we present a merging
formulation that dynamically aggregates the primitives across global and local
scales. Our evaluation demonstrates that CPFN improves the state-of-the-art
SPFN performance by 13-14% on high-resolution point cloud datasets and
specifically improves the detection of fine-scale primitives by 20-22%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty Quantified Deep Learning for Predicting Dice Coefficient of Digital Histopathology Image Segmentation. (arXiv:2109.00115v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00115">
<div class="article-summary-box-inner">
<span><p>Deep learning models (DLMs) can achieve state of the art performance in
medical image segmentation and classification tasks. However, DLMs that do not
provide feedback for their predictions such as Dice coefficients (Dice) have
limited deployment potential in real world clinical settings. Uncertainty
estimates can increase the trust of these automated systems by identifying
predictions that need further review but remain computationally prohibitive to
deploy. In this study, we use a DLM with randomly initialized weights and Monte
Carlo dropout (MCD) to segment tumors from microscopic Hematoxylin and Eosin
(H&amp;E) dye stained prostate core biopsy RGB images. We devise a novel approach
that uses multiple clinical region based uncertainties from a single image
(instead of the entire image) to predict Dice of the DLM model output by linear
models. Image level uncertainty maps were generated and showed correspondence
between imperfect model segmentation and high levels of uncertainty associated
with specific prostate tissue regions with or without tumors. Results from this
study suggest that linear models can learn coefficients of uncertainty
quantified deep learning and correlations ((Spearman's correlation (p&lt;0.05)) to
predict Dice scores of specific regions of medical images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Multiview Coding with Electro-optics for SAR Semantic Segmentation. (arXiv:2109.00120v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00120">
<div class="article-summary-box-inner">
<span><p>In the training of deep learning models, how the model parameters are
initialized greatly affects the model performance, sample efficiency, and
convergence speed. Representation learning for model initialization has
recently been actively studied in the remote sensing field. In particular, the
appearance characteristics of the imagery obtained using the a synthetic
aperture radar (SAR) sensor are quite different from those of general
electro-optical (EO) images, and thus representation learning is even more
important in remote sensing domain. Motivated from contrastive multiview
coding, we propose multi-modal representation learning for SAR semantic
segmentation. Unlike previous studies, our method jointly uses EO imagery, SAR
imagery, and a label mask. Several experiments show that our approach is
superior to the existing methods in model performance, sample efficiency, and
convergence speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DPA: Learning Robust Physical Adversarial Camouflages for Object Detectors. (arXiv:2109.00124v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00124">
<div class="article-summary-box-inner">
<span><p>Adversarial attacks are feasible in the real world for object detection.
However, most of the previous works have tried to learn "patches" applied to an
object to fool detectors, which become less effective or even ineffective in
squint view angles. To address this issue, we propose the Dense Proposals
Attack (DPA) to learn robust, physical and targeted adversarial camouflages for
detectors. The camouflages are robust because they remain adversarial when
filmed under arbitrary viewpoint and different illumination conditions,
physical because they function well both in the 3D virtual scene and the real
world, and targeted because they can cause detectors to misidentify an object
as a specific target class. In order to make the generated camouflages robust
in the physical world, we introduce a combination of viewpoint shifts, lighting
and other natural transformations to model the physical phenomena. In addition,
to improve the attacks, DPA substantially attacks all the classifications in
the fixed region proposals. Moreover, we build a virtual 3D scene using the
Unity simulation engine to fairly and reproducibly evaluate different physical
attacks. Extensive experiments demonstrate that DPA outperforms the
state-of-the-art methods significantly, and generalizes well to the real world,
posing a potential threat to the security-critical computer vision systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Behavioral Cloning. (arXiv:2109.00137v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00137">
<div class="article-summary-box-inner">
<span><p>We find that across a wide range of robot policy learning scenarios, treating
supervised policy learning with an implicit model generally performs better, on
average, than commonly used explicit models. We present extensive experiments
on this finding, and we provide both intuitive insight and theoretical
arguments distinguishing the properties of implicit models compared to their
explicit counterparts, particularly with respect to approximating complex,
potentially discontinuous and multi-valued (set-valued) functions. On robotic
policy learning tasks we show that implicit behavioral cloning policies with
energy-based models (EBM) often outperform common explicit (Mean Square Error,
or Mixture Density) behavioral cloning policies, including on tasks with
high-dimensional action spaces and visual image inputs. We find these policies
provide competitive results or outperform state-of-the-art offline
reinforcement learning methods on the challenging human-expert tasks from the
D4RL benchmark suite, despite using no reward information. In the real world,
robots with implicit policies can learn complex and remarkably subtle behaviors
on contact-rich tasks from human demonstrations, including tasks with high
combinatorial complexity and tasks requiring 1mm precision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eyes Tell All: Irregular Pupil Shapes Reveal GAN-generated Faces. (arXiv:2109.00162v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00162">
<div class="article-summary-box-inner">
<span><p>Generative adversary network (GAN) generated high-realistic human faces have
been used as profile images for fake social media accounts and are visually
challenging to discern from real ones. In this work, we show that GAN-generated
faces can be exposed via irregular pupil shapes. This phenomenon is caused by
the lack of physiological constraints in the GAN models. We demonstrate that
such artifacts exist widely in high-quality GAN-generated faces and further
describe an automatic method to extract the pupils from two eyes and analysis
their shapes for exposing the GAN-generated faces. Qualitative and quantitative
evaluations of our method suggest its simplicity and effectiveness in
distinguishing GAN-generated faces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Architecture Aware Latency Constrained Sparse Neural Networks. (arXiv:2109.00170v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00170">
<div class="article-summary-box-inner">
<span><p>Acceleration of deep neural networks to meet a specific latency constraint is
essential for their deployment on mobile devices. In this paper, we design an
architecture aware latency constrained sparse (ALCS) framework to prune and
accelerate CNN models. Taking modern mobile computation architectures into
consideration, we propose Single Instruction Multiple Data (SIMD)-structured
pruning, along with a novel sparse convolution algorithm for efficient
computation. Besides, we propose to estimate the run time of sparse models with
piece-wise linear interpolation. The whole latency constrained pruning task is
formulated as a constrained optimization problem that can be efficiently solved
with Alternating Direction Method of Multipliers (ADMM). Extensive experiments
show that our system-algorithm co-design framework can achieve much better
Pareto frontier among network accuracy and latency on resource-constrained
mobile devices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Problem Learning: Towards the Free Will of Machines. (arXiv:2109.00177v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00177">
<div class="article-summary-box-inner">
<span><p>A machine intelligence pipeline usually consists of six components: problem,
representation, model, loss, optimizer and metric. Researchers have worked hard
trying to automate many components of the pipeline. However, one key component
of the pipeline--problem definition--is still left mostly unexplored in terms
of automation. Usually, it requires extensive efforts from domain experts to
identify, define and formulate important problems in an area. However,
automatically discovering research or application problems for an area is
beneficial since it helps to identify valid and potentially important problems
hidden in data that are unknown to domain experts, expand the scope of tasks
that we can do in an area, and even inspire completely new findings.
</p>
<p>This paper describes Problem Learning, which aims at learning to discover and
define valid and ethical problems from data or from the machine's interaction
with the environment. We formalize problem learning as the identification of
valid and ethical problems in a problem space and introduce several possible
approaches to problem learning. In a broader sense, problem learning is an
approach towards the free will of intelligent machines. Currently, machines are
still limited to solving the problems defined by humans, without the ability or
flexibility to freely explore various possible problems that are even unknown
to humans. Though many machine learning techniques have been developed and
integrated into intelligent systems, they still focus on the means rather than
the purpose in that machines are still solving human defined problems. However,
proposing good problems is sometimes even more important than solving problems,
because a good problem can help to inspire new ideas and gain deeper
understandings. The paper also discusses the ethical implications of problem
learning under the background of Responsible AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatio-temporal Self-Supervised Representation Learning for 3D Point Clouds. (arXiv:2109.00179v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00179">
<div class="article-summary-box-inner">
<span><p>To date, various 3D scene understanding tasks still lack practical and
generalizable pre-trained models, primarily due to the intricate nature of 3D
scene understanding tasks and their immense variations introduced by camera
views, lighting, occlusions, etc. In this paper, we tackle this challenge by
introducing a spatio-temporal representation learning (STRL) framework, capable
of learning from unlabeled 3D point clouds in a self-supervised fashion.
Inspired by how infants learn from visual data in the wild, we explore the rich
spatio-temporal cues derived from the 3D data. Specifically, STRL takes two
temporally-correlated frames from a 3D point cloud sequence as the input,
transforms it with the spatial data augmentation, and learns the invariant
representation self-supervisedly. To corroborate the efficacy of STRL, we
conduct extensive experiments on three types (synthetic, indoor, and outdoor)
of datasets. Experimental results demonstrate that, compared with supervised
learning methods, the learned self-supervised representation facilitates
various models to attain comparable or even better performances while capable
of generalizing pre-trained models to downstream tasks, including 3D shape
classification, 3D object detection, and 3D semantic segmentation. Moreover,
the spatio-temporal contextual cues embedded in 3D point clouds significantly
improve the learned representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptually Optimized Deep High-Dynamic-Range Image Tone Mapping. (arXiv:2109.00180v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00180">
<div class="article-summary-box-inner">
<span><p>We describe a deep high-dynamic-range (HDR) image tone mapping operator that
is computationally efficient and perceptually optimized. We first decompose an
HDR image into a normalized {Laplacian} pyramid, and use two deep neural
networks (DNNs) to estimate the {Laplacian} pyramid of the desired tone-mapped
image from the normalized representation. We then end-to-end optimize the
entire method over a database of HDR images by minimizing the normalized
{Laplacian} pyramid distance (NLPD), a recently proposed perceptual metric.
Qualitative and quantitative experiments demonstrate that our method produces
images with better visual quality, and runs the fastest among existing local
tone mapping algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Only Hypothesize Once: Point Cloud Registration with Rotation-equivariant Descriptors. (arXiv:2109.00182v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00182">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel local descriptor-based framework, called
You Only Hypothesize Once (YOHO), for the registration of two unaligned point
clouds. In contrast to most existing local descriptors which rely on a fragile
local reference frame to gain rotation invariance, the proposed descriptor
achieves the rotation invariance by recent technologies of group equivariant
feature learning, which brings more robustness to point density and noise.
Meanwhile, the descriptor in YOHO also has a rotation equivariant part, which
enables us to estimate the registration from just one correspondence
hypothesis. Such property reduces the searching space for feasible
transformations, thus greatly improves both the accuracy and the efficiency of
YOHO. Extensive experiments show that YOHO achieves superior performances with
much fewer needed RANSAC iterations on four widely-used datasets, the
3DMatch/3DLoMatch datasets, the ETH dataset and the WHU-TLS dataset. More
details are shown in our project page: https://hpwang-whu.github.io/YOHO/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study on the Joint Impact of Feature Selection and Data Resampling on Imbalance Classification. (arXiv:2109.00201v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00201">
<div class="article-summary-box-inner">
<span><p>Real-world datasets often present different degrees of imbalanced (i.e.,
long-tailed or skewed) distributions. While the majority (a.k.a., head or
frequent) classes have sufficient samples, the minority (a.k.a., tail or rare)
classes can be under-represented by a rather limited number of samples. On one
hand, data resampling is a common approach to tackling class imbalance. On the
other hand, dimension reduction, which reduces the feature space, is a
conventional machine learning technique for building stronger classification
models on a dataset. However, the possible synergy between feature selection
and data resampling for high-performance imbalance classification has rarely
been investigated before. To address this issue, this paper carries out a
comprehensive empirical study on the joint influence of feature selection and
resampling on two-class imbalance classification. Specifically, we study the
performance of two opposite pipelines for imbalance classification, i.e.,
applying feature selection before or after data resampling. We conduct a large
amount of experiments (a total of 9225 experiments) on 52 publicly available
datasets, using 9 feature selection methods, 6 resampling approaches for class
imbalance learning, and 3 well-known classification algorithms. Experimental
results show that there is no constant winner between the two pipelines, thus
both of them should be considered to derive the best performing model for
imbalance classification. We also find that the performance of an imbalance
classification model depends on the classifier adopted, the ratio between the
number of majority and minority samples (IR), as well as on the ratio between
the number of samples and features (SFR). Overall, this study should provide
new reference value for researchers and practitioners in imbalance learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EventPoint: Self-Supervised Local Descriptor Learning for Event Cameras. (arXiv:2109.00210v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00210">
<div class="article-summary-box-inner">
<span><p>We proposes a method of extracting intrest points and descriptors using
self-supervised learning method on frame-based event data, which is called
EventPoint. Different from other feature extraction methods on event data, we
train our model on real event-form driving dataset--DSEC with the
self-supervised learning method we proposed, the training progress fully
consider the characteristics of event data.To verify the effectiveness of our
work,we conducted several complete evaluations: we emulated DART and carried
out feature matching experiments on N-caltech101 dataset, the results shows
that the effect of EventPoint is better than DART; We use Vid2e tool provided
by UZH to convert Oxford robotcar data into event-based format, and combined
with INS information provided to carry out the global pose estimation
experiment which is important in SLAM. As far as we know, this is the first
work to carry out this challenging task.Sufficient experimental data show that
EventPoint can get better results while achieve real time on CPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Person Search: An Anchor-Free Approach. (arXiv:2109.00211v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00211">
<div class="article-summary-box-inner">
<span><p>Person search aims to simultaneously localize and identify a query person
from realistic, uncropped images. To achieve this goal, state-of-the-art models
typically add a re-id branch upon two-stage detectors like Faster R-CNN. Owing
to the ROI-Align operation, this pipeline yields promising accuracy as re-id
features are explicitly aligned with the corresponding object regions, but in
the meantime, it introduces high computational overhead due to dense object
anchors. In this work, we present an anchor-free approach to efficiently
tackling this challenging task, by introducing the following dedicated designs.
First, we select an anchor-free detector (i.e., FCOS) as the prototype of our
framework. Due to the lack of dense object anchors, it exhibits significantly
higher efficiency compared with existing person search models. Second, when
directly accommodating this anchor-free detector for person search, there exist
several major challenges in learning robust re-id features, which we summarize
as the misalignment issues in different levels (i.e., scale, region, and task).
To address these issues, we propose an aligned feature aggregation module to
generate more discriminative and robust feature embeddings. Accordingly, we
name our model as Feature-Aligned Person Search Network (AlignPS). Third, by
investigating the advantages of both anchor-based and anchor-free models, we
further augment AlignPS with an ROI-Align head, which significantly improves
the robustness of re-id features while still keeping our model highly
efficient. Extensive experiments conducted on two challenging benchmarks (i.e.,
CUHK-SYSU and PRW) demonstrate that our framework achieves state-of-the-art or
competitive performance, while displaying higher efficiency. All the source
codes, data, and trained models are available at:
https://github.com/daodaofr/alignps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse Sample Generation: Pushing the Limit of Data-free Quantization. (arXiv:2109.00212v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00212">
<div class="article-summary-box-inner">
<span><p>Recently, generative data-free quantization emerges as a practical approach
that compresses the neural network to low bit-width without access to real
data. It generates data to quantize the network by utilizing the batch
normalization (BN) statistics of its full-precision counterpart. However, our
study shows that in practice, the synthetic data completely constrained by BN
statistics suffers severe homogenization at distribution and sample level,
which causes serious accuracy degradation of the quantized network. This paper
presents a generic Diverse Sample Generation (DSG) scheme for the generative
data-free post-training quantization and quantization-aware training, to
mitigate the detrimental homogenization. In our DSG, we first slack the
statistics alignment for features in the BN layer to relax the distribution
constraint. Then we strengthen the loss impact of the specific BN layer for
different samples and inhibit the correlation among samples in the generation
process, to diversify samples from the statistical and spatial perspective,
respectively. Extensive experiments show that for large-scale image
classification tasks, our DSG can consistently outperform existing data-free
quantization methods on various neural architectures, especially under
ultra-low bit-width (e.g., 22% gain under W4A4 setting). Moreover, data
diversifying caused by our DSG brings a general gain in various quantization
methods, demonstrating diversity is an important property of high-quality
synthetic data for data-free quantization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatio-Temporal Perturbations for Video Attribution. (arXiv:2109.00222v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00222">
<div class="article-summary-box-inner">
<span><p>The attribution method provides a direction for interpreting opaque neural
networks in a visual way by identifying and visualizing the input
regions/pixels that dominate the output of a network. Regarding the attribution
method for visually explaining video understanding networks, it is challenging
because of the unique spatiotemporal dependencies existing in video inputs and
the special 3D convolutional or recurrent structures of video understanding
networks. However, most existing attribution methods focus on explaining
networks taking a single image as input and a few works specifically devised
for video attribution come short of dealing with diversified structures of
video understanding networks. In this paper, we investigate a generic
perturbation-based attribution method that is compatible with diversified video
understanding networks. Besides, we propose a novel regularization term to
enhance the method by constraining the smoothness of its attribution results in
both spatial and temporal dimensions. In order to assess the effectiveness of
different video attribution methods without relying on manual judgement, we
introduce reliable objective metrics which are checked by a newly proposed
reliability measurement. We verified the effectiveness of our method by both
subjective and objective evaluation and comparison with multiple significant
attribution methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Protection Method of Trained CNN Model Using Feature Maps Transformed With Secret Key From Unauthorized Access. (arXiv:2109.00224v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00224">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a model protection method for convolutional neural
networks (CNNs) with a secret key so that authorized users get a high
classification accuracy, and unauthorized users get a low classification
accuracy. The proposed method applies a block-wise transformation with a secret
key to feature maps in the network. Conventional key-based model protection
methods cannot maintain a high accuracy when a large key space is selected. In
contrast, the proposed method not only maintains almost the same accuracy as
non-protected accuracy, but also has a larger key space. Experiments were
carried out on the CIFAR-10 dataset, and results show that the proposed model
protection method outperformed the previous key-based model protection methods
in terms of classification accuracy, key space, and robustness against key
estimation attacks and fine-tuning attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Graph Learning and Matching for Semantic Feature Correspondence. (arXiv:2109.00240v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00240">
<div class="article-summary-box-inner">
<span><p>In recent years, powered by the learned discriminative representation via
graph neural network (GNN) models, deep graph matching methods have made great
progresses in the task of matching semantic features. However, these methods
usually rely on heuristically generated graph patterns, which may introduce
unreliable relationships to hurt the matching performance. In this paper, we
propose a joint \emph{graph learning and matching} network, named GLAM, to
explore reliable graph structures for boosting graph matching. GLAM adopts a
pure attention-based framework for both graph learning and graph matching.
Specifically, it employs two types of attention mechanisms, self-attention and
cross-attention for the task. The self-attention discovers the relationships
between features and to further update feature representations over the learnt
structures; and the cross-attention computes cross-graph correlations between
the two feature sets to be matched for feature reconstruction. Moreover, the
final matching solution is directly derived from the output of the
cross-attention layer, without employing a specific matching decision module.
The proposed method is evaluated on three popular visual matching benchmarks
(Pascal VOC, Willow Object and SPair-71k), and it outperforms previous
state-of-the-art graph matching methods by significant margins on all
benchmarks. Furthermore, the graph patterns learnt by our model are validated
to be able to remarkably enhance previous deep graph matching methods by
replacing their handcrafted graph structures with the learnt ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seeing Implicit Neural Representations as Fourier Series. (arXiv:2109.00249v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00249">
<div class="article-summary-box-inner">
<span><p>Implicit Neural Representations (INR) use multilayer perceptrons to represent
high-frequency functions in low-dimensional problem domains. Recently these
representations achieved state-of-the-art results on tasks related to complex
3D objects and scenes. A core problem is the representation of highly detailed
signals, which is tackled using networks with periodic activation functions
(SIRENs) or applying Fourier mappings to the input. This work analyzes the
connection between the two methods and shows that a Fourier mapped perceptron
is structurally like one hidden layer SIREN. Furthermore, we identify the
relationship between the previously proposed Fourier mapping and the general
d-dimensional Fourier series, leading to an integer lattice mapping. Moreover,
we modify a progressive training strategy to work on arbitrary Fourier mappings
and show that it improves the generalization of the interpolation task. Lastly,
we compare the different mappings on the image regression and novel view
synthesis tasks. We confirm the previous finding that the main contributor to
the mapping performance is the size of the embedding and standard deviation of
its elements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BVMatch: Lidar-based Place Recognition Using Bird's-eye View Images. (arXiv:2109.00317v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00317">
<div class="article-summary-box-inner">
<span><p>Recognizing places using Lidar in large-scale environments is challenging due
to the sparse nature of point cloud data. In this paper we present BVMatch, a
Lidar-based frame-to-frame place recognition framework, that is capable of
estimating 2D relative poses. Based on the assumption that the ground area can
be approximated as a plane, we uniformly discretize the ground area into grids
and project 3D Lidar scans to bird's-eye view (BV) images. We further use a
bank of Log-Gabor filters to build a maximum index map (MIM) that encodes the
orientation information of the structures in the images. We analyze the
orientation characteristics of MIM theoretically and introduce a novel
descriptor called bird's-eye view feature transform (BVFT). The proposed BVFT
is insensitive to rotation and intensity variations of BV images. Leveraging
the BVFT descriptors, we unify the Lidar place recognition and pose estimation
tasks into the BVMatch framework. The experiments conducted on three
large-scale datasets show that BVMatch outperforms the state-of-the-art methods
in terms of both recall rate of place recognition and pose estimation accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Category-Level Metric Scale Object Shape and Pose Estimation. (arXiv:2109.00326v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00326">
<div class="article-summary-box-inner">
<span><p>Advances in deep learning recognition have led to accurate object detection
with 2D images. However, these 2D perception methods are insufficient for
complete 3D world information. Concurrently, advanced 3D shape estimation
approaches focus on the shape itself, without considering metric scale. These
methods cannot determine the accurate location and orientation of objects. To
tackle this problem, we propose a framework that jointly estimates a metric
scale shape and pose from a single RGB image. Our framework has two branches:
the Metric Scale Object Shape branch (MSOS) and the Normalized Object
Coordinate Space branch (NOCS). The MSOS branch estimates the metric scale
shape observed in the camera coordinates. The NOCS branch predicts the
normalized object coordinate space (NOCS) map and performs similarity
transformation with the rendered depth map from a predicted metric scale mesh
to obtain 6d pose and size. Additionally, we introduce the Normalized Object
Center Estimation (NOCE) to estimate the geometrically aligned distance from
the camera to the object center. We validated our method on both synthetic and
real-world datasets to evaluate category-level object pose and shape.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-Free Generative Replay For Class-Incremental Learning. (arXiv:2109.00328v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00328">
<div class="article-summary-box-inner">
<span><p>Regularization-based methods are beneficial to alleviate the catastrophic
forgetting problem in class-incremental learning. With the absence of old task
images, they often assume that old knowledge is well preserved if the
classifier produces similar output on new images. In this paper, we find that
their effectiveness largely depends on the nature of old classes: they work
well on classes that are easily distinguishable between each other but may fail
on more fine-grained ones, e.g., boy and girl. In spirit, such methods project
new data onto the feature space spanned by the weight vectors in the fully
connected layer, corresponding to old classes. The resulting projections would
be similar on fine-grained old classes, and as a consequence the new classifier
will gradually lose the discriminative ability on these classes. To address
this issue, we propose a memory-free generative replay strategy to preserve the
fine-grained old classes characteristics by generating representative old
images directly from the old classifier and combined with new data for new
classifier training. To solve the homogenization problem of the generated
samples, we also propose a diversity loss that maximizes Kullback Leibler (KL)
divergence between generated samples. Our method is best complemented by prior
regularization-based methods proved to be effective for easily distinguishable
old classes. We validate the above design and insights on CUB-200-2011,
Caltech-101, CIFAR-100 and Tiny ImageNet and show that our strategy outperforms
existing memory-free methods with a clear margin. Code is available at
https://github.com/xmengxin/MFGR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A survey on IQA. (arXiv:2109.00347v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00347">
<div class="article-summary-box-inner">
<span><p>Image quality assessment(IQA) is of increasing importance for image-based
applications. Its purpose is to establish a model that can replace humans for
accurately evaluating image quality. According to whether the reference image
is complete and available, image quality evaluation can be divided into three
categories: full-reference(FR), reduced-reference(RR), and non-reference(NR)
image quality assessment. Due to the vigorous development of deep learning and
the widespread attention of researchers, several non-reference image quality
assessment methods based on deep learning have been proposed in recent years,
and some have exceeded the performance of reduced -reference or even
full-reference image quality assessment models. This article will review the
concepts and metrics of image quality assessment and also video quality
assessment, briefly introduce some methods of full-reference and semi-reference
image quality assessment, and focus on the non-reference image quality
assessment methods based on deep learning. Then introduce the commonly used
synthetic database and real-world database. Finally, summarize and present
challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The University of California San Francisco Preoperative Diffuse Glioma (UCSF-PDGM) MRI Dataset. (arXiv:2109.00356v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00356">
<div class="article-summary-box-inner">
<span><p>Here we present the University of California San Francisco Preoperative
Diffuse Glioma MRI (UCSF-PDGM) dataset. The UCSF-PDGM dataset includes 500
subjects with histopathologically-proven diffuse gliomas who were imaged with a
standardized 3 Tesla preoperative brain tumor MRI protocol featuring
predominantly 3D imaging, as well as advanced diffusion and perfusion imaging
techniques. The dataset also includes isocitrate dehydrogenase (IDH) mutation
status for all cases and O6-methylguanine-DNA methyltransferase (MGMT) promotor
methylation status for World Health Organization (WHO) grade III and IV
gliomas. The UCSF-PDGM has been made publicly available in the hopes that
researchers around the world will use these data to continue to push the
boundaries of AI applications for diffuse gliomas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory Based Video Scene Parsing. (arXiv:2109.00373v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00373">
<div class="article-summary-box-inner">
<span><p>Video scene parsing is a long-standing challenging task in computer vision,
aiming to assign pre-defined semantic labels to pixels of all frames in a given
video. Compared with image semantic segmentation, this task pays more attention
on studying how to adopt the temporal information to obtain higher predictive
accuracy. In this report, we introduce our solution for the 1st Video Scene
Parsing in the Wild Challenge, which achieves a mIoU of 57.44 and obtained the
2nd place (our team name is CharlesBLWX).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ImageTBAD: A 3D Computed Tomography Angiography Image Dataset for Automatic Segmentation of Type-B Aortic Dissection. (arXiv:2109.00374v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00374">
<div class="article-summary-box-inner">
<span><p>Type-B Aortic Dissection (TBAD) is one of the most serious cardiovascular
events characterized by a growing yearly incidence,and the severity of disease
prognosis. Currently, computed tomography angiography (CTA) has been widely
adopted for the diagnosis and prognosis of TBAD. Accurate segmentation of true
lumen (TL), false lumen (FL), and false lumen thrombus (FLT) in CTA are crucial
for the precise quantification of anatomical features. However, existing works
only focus on only TL and FL without considering FLT. In this paper, we propose
ImageTBAD, the first 3D computed tomography angiography (CTA) image dataset of
TBAD with annotation of TL, FL, and FLT. The proposed dataset contains 100 TBAD
CTA images, which is of decent size compared with existing medical imaging
datasets. As FLT can appear almost anywhere along the aorta with irregular
shapes, segmentation of FLT presents a wide class of segmentation problems
where targets exist in a variety of positions with irregular shapes. We further
propose a baseline method for automatic segmentation of TBAD. Results show that
the baseline method can achieve comparable results with existing works on aorta
and TL segmentation. However, the segmentation accuracy of FLT is only 52%,
which leaves large room for improvement and also shows the challenge of our
dataset. To facilitate further research on this challenging problem, our
dataset and codes are released to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Integrated Framework for the Heterogeneous Spatio-Spectral-Temporal Fusion of Remote Sensing Images. (arXiv:2109.00400v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00400">
<div class="article-summary-box-inner">
<span><p>Image fusion technology is widely used to fuse the complementary information
between multi-source remote sensing images. Inspired by the frontier of deep
learning, this paper first proposes a heterogeneous-integrated framework based
on a novel deep residual cycle GAN. The proposed network consists of a forward
fusion part and a backward degeneration feedback part. The forward part
generates the desired fusion result from the various observations; the backward
degeneration feedback part considers the imaging degradation process and
regenerates the observations inversely from the fusion result. The proposed
network can effectively fuse not only the homogeneous but also the
heterogeneous information. In addition, for the first time, a
heterogeneous-integrated fusion framework is proposed to simultaneously merge
the complementary heterogeneous spatial, spectral and temporal information of
multi-source heterogeneous observations. The proposed heterogeneous-integrated
framework also provides a uniform mode that can complete various fusion tasks,
including heterogeneous spatio-spectral fusion, spatio-temporal fusion, and
heterogeneous spatio-spectral-temporal fusion. Experiments are conducted for
two challenging scenarios of land cover changes and thick cloud coverage.
Images from many remote sensing satellites, including MODIS, Landsat-8,
Sentinel-1, and Sentinel-2, are utilized in the experiments. Both qualitative
and quantitative evaluations confirm the effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EVReflex: Dense Time-to-Impact Prediction for Event-based Obstacle Avoidance. (arXiv:2109.00405v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00405">
<div class="article-summary-box-inner">
<span><p>The broad scope of obstacle avoidance has led to many kinds of computer
vision-based approaches. Despite its popularity, it is not a solved problem.
Traditional computer vision techniques using cameras and depth sensors often
focus on static scenes, or rely on priors about the obstacles. Recent
developments in bio-inspired sensors present event cameras as a compelling
choice for dynamic scenes. Although these sensors have many advantages over
their frame-based counterparts, such as high dynamic range and temporal
resolution, event-based perception has largely remained in 2D. This often leads
to solutions reliant on heuristics and specific to a particular task. We show
that the fusion of events and depth overcomes the failure cases of each
individual modality when performing obstacle avoidance. Our proposed approach
unifies event camera and lidar streams to estimate metric time-to-impact
without prior knowledge of the scene geometry or obstacles. In addition, we
release an extensive event-based dataset with six visual streams spanning over
700 scanned scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point Cloud Pre-training by Mixing and Disentangling. (arXiv:2109.00452v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00452">
<div class="article-summary-box-inner">
<span><p>The annotation for large-scale point clouds is still time-consuming and
unavailable for many real-world tasks. Point cloud pre-training is one
potential solution for obtaining a scalable model for fast adaptation.
Therefore, in this paper, we investigate a new self-supervised learning
approach, called Mixing and Disentangling (MD), for point cloud pre-training.
As the name implies, we explore how to separate the original point cloud from
the mixed point cloud, and leverage this challenging task as a pretext
optimization objective for model training. Considering the limited training
data in the original dataset, which is much less than prevailing ImageNet, the
mixing process can efficiently generate more high-quality samples. We build one
baseline network to verify our intuition, which simply contains two modules,
encoder and decoder. Given a mixed point cloud, the encoder is first
pre-trained to extract the semantic embedding. Then an instance-adaptive
decoder is harnessed to disentangle the point clouds according to the
embedding. Albeit simple, the encoder is inherently able to capture the point
cloud keypoints after training and can be fast adapted to downstream tasks
including classification and segmentation by the pre-training and fine-tuning
paradigm. Extensive experiments on two datasets show that the encoder + ours
(MD) significantly surpasses that of the encoder trained from scratch and
converges quickly. In ablation studies, we further study the effect of each
component and discuss the advantages of the proposed self-supervised learning
strategy. We hope this self-supervised learning attempt on point clouds can
pave the way for reducing the deeply-learned model dependence on large-scale
labeled data and saving a lot of annotation costs in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Weakly-Supervised Surface Crack Segmentation Method using Localisation with a Classifier and Thresholding. (arXiv:2109.00456v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00456">
<div class="article-summary-box-inner">
<span><p>Surface cracks are a common sight on public infrastructure nowadays. Recent
work has been addressing this problem by supporting structural maintenance
measures using machine learning methods which segment surface cracks from their
background so that they are easy to localize. However, a common issue with
those methods is that to create a well functioning algorithm, the training data
needs to have detailed annotations of pixels that belong to cracks. Our work
proposes a weakly supervised approach which leverages a CNN classifier to
create surface crack segmentation maps. We use this classifier to create a
rough crack localisation map by using its class activation maps and a patch
based classification approach and fuse this with a thresholding based approach
to segment the mostly darker crack pixels. The classifier assists in
suppressing noise from the background regions, which commonly are incorrectly
highlighted as cracks by standard thresholding methods. We focus on the ease of
implementation of our method and it is shown to perform well on several surface
crack datasets, segmenting cracks efficiently even though the only data that
was used for training were simple classification labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse to Dense Motion Transfer for Face Image Animation. (arXiv:2109.00471v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00471">
<div class="article-summary-box-inner">
<span><p>Face image animation from a single image has achieved remarkable progress.
However, it remains challenging when only sparse landmarks are available as the
driving signal. Given a source face image and a sequence of sparse face
landmarks, our goal is to generate a video of the face imitating the motion of
landmarks. We develop an efficient and effective method for motion transfer
from sparse landmarks to the face image. We then combine global and local
motion estimation in a unified model to faithfully transfer the motion. The
model can learn to segment the moving foreground from the background and
generate not only global motion, such as rotation and translation of the face,
but also subtle local motion such as the gaze change. We further improve face
landmark detection on videos. With temporally better aligned landmark sequences
for training, our method can generate temporally coherent videos with higher
visual quality. Experiments suggest we achieve results comparable to the
state-of-the-art image driven method on the same identity testing and better
results on cross identity testing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Learning a Vocabulary of Visual Concepts and Operators using Deep Neural Networks. (arXiv:2109.00479v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00479">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have become the default choice for many applications
like image and video recognition, segmentation and other image and video
related tasks.However, a critical challenge with these models is the lack of
explainability.This requirement of generating explainable predictions has
motivated the research community to perform various analysis on trained
models.In this study, we analyze the learned feature maps of trained models
using MNIST images for achieving more explainable predictions.Our study is
focused on deriving a set of primitive elements, here called visual concepts,
that can be used to generate any arbitrary sample from the data generating
distribution.We derive the primitive elements from the feature maps learned by
the model.We illustrate the idea by generating visual concepts from a
Variational Autoencoder trained using MNIST images.We augment the training data
of MNIST dataset by adding about 60,000 new images generated with visual
concepts chosen at random.With this we were able to reduce the reconstruction
loss (mean square error) from an initial value of 120 without augmentation to
60 with augmentation.Our approach is a first step towards the final goal of
achieving trained deep neural network models whose predictions, features in
hidden layers and the learned filters can be well explained.Such a model when
deployed in production can easily be modified to adapt to new data, whereas
existing deep learning models need a re training or fine tuning. This process
again needs a huge number of data samples that are not easy to generate unless
the model has good explainability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Looking at the whole picture: constrained unsupervised anomaly segmentation. (arXiv:2109.00482v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00482">
<div class="article-summary-box-inner">
<span><p>Current unsupervised anomaly localization approaches rely on generative
models to learn the distribution of normal images, which is later used to
identify potential anomalous regions derived from errors on the reconstructed
images. However, a main limitation of nearly all prior literature is the need
of employing anomalous images to set a class-specific threshold to locate the
anomalies. This limits their usability in realistic scenarios, where only
normal data is typically accessible. Despite this major drawback, only a
handful of works have addressed this limitation, by integrating supervision on
attention maps during training. In this work, we propose a novel formulation
that does not require accessing images with abnormalities to define the
threshold. Furthermore, and in contrast to very recent work, the proposed
constraint is formulated in a more principled manner, leveraging well-known
knowledge in constrained optimization. In particular, the equality constraint
on the attention maps in prior work is replaced by an inequality constraint,
which allows more flexibility. In addition, to address the limitations of
penalty-based functions we employ an extension of the popular log-barrier
methods to handle the constraint. Comprehensive experiments on the popular
BRATS'19 dataset demonstrate that the proposed approach substantially
outperforms relevant literature, establishing new state-of-the-art results for
unsupervised lesion segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction. (arXiv:2109.00512v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00512">
<div class="article-summary-box-inner">
<span><p>Traditional approaches for learning 3D object categories have been
predominantly trained and evaluated on synthetic datasets due to the
unavailability of real 3D-annotated category-centric data. Our main goal is to
facilitate advances in this field by collecting real-world data in a magnitude
similar to the existing synthetic counterparts. The principal contribution of
this work is thus a large-scale dataset, called Common Objects in 3D, with real
multi-view images of object categories annotated with camera poses and ground
truth 3D point clouds. The dataset contains a total of 1.5 million frames from
nearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such,
it is significantly larger than alternatives both in terms of the number of
categories and objects. We exploit this new dataset to conduct one of the first
large-scale "in-the-wild" evaluations of several new-view-synthesis and
category-centric 3D reconstruction methods. Finally, we contribute NerFormer -
a novel neural rendering method that leverages the powerful Transformer to
reconstruct an object given a small number of its views. The CO3D dataset is
available at https://github.com/facebookresearch/co3d .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RP2K: A Large-Scale Retail Product Dataset for Fine-Grained Image Classification. (arXiv:2006.12634v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.12634">
<div class="article-summary-box-inner">
<span><p>We introduce RP2K, a new large-scale retail product dataset for fine-grained
image classification. Unlike previous datasets focusing on relatively few
products, we collect more than 500,000 images of retail products on shelves
belonging to 2000 different products. Our dataset aims to advance the research
in retail object recognition, which has massive applications such as automatic
shelf auditing and image-based product information retrieval. Our dataset
enjoys following properties: (1) It is by far the largest scale dataset in
terms of product categories. (2) All images are captured manually in physical
retail stores with natural lightings, matching the scenario of real
applications. (3) We provide rich annotations to each object, including the
sizes, shapes and flavors/scents. We believe our dataset could benefit both
computer vision research and retail industry. Our dataset is publicly available
at https://www.pinlandata.com/rp2k_dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autonomous Removal of Perspective Distortion of Elevator Button Images based on Corner Detection. (arXiv:2007.11806v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.11806">
<div class="article-summary-box-inner">
<span><p>Elevator button recognition is a critical function to realize the autonomous
operation of elevators. However, challenging image conditions and various image
distortions make it difficult to recognize buttons accurately. To fill this
gap, we propose a novel deep learning-based approach, which aims to
autonomously correct perspective distortions of elevator button images based on
button corner detection results. First, we leverage a novel image segmentation
model and the Hough Transform method to obtain button segmentation and button
corner detection results. Then, pixel coordinates of standard button corners
are used as reference features to estimate camera motions for correcting
perspective distortions. Fifteen elevator button images are captured from
different angles of view as the dataset. The experimental results demonstrate
that our proposed approach is capable of estimating camera motions and removing
perspective distortions of elevator button images with high accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Elements of End-to-end Deep Face Recognition: A Survey of Recent Advances. (arXiv:2009.13290v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.13290">
<div class="article-summary-box-inner">
<span><p>Face recognition is one of the most popular and long-standing topics in
computer vision. With the recent development of deep learning techniques and
large-scale datasets, deep face recognition has made remarkable progress and
been widely used in many real-world applications. Given a natural image or
video frame as input, an end-to-end deep face recognition system outputs the
face feature for recognition. To achieve this, a typical end-to-end system is
generally built with three key elements: face detection, face alignment, and
face representation. The face detection locates faces in the image or frame.
Then, the face alignment is proceeded to calibrate the faces to a canonical
view and crop them to a normalized pixel size. Finally, in the stage of face
representation, the discriminative features are extracted from the aligned face
for recognition. Nowadays, all of the three elements are fulfilled by the
technique of deep convolutional neural network.In this survey article, we
present a comprehensive review about the recent advance of each element of the
end-to-end deep face recognition, since the thriving deep learning techniques
have greatly improved the capability of them. To start with, we present an
overview of the end-to-end deep face recognition. Then, we review the advance
of each element, respectively, covering many aspects such as the to-date
algorithm designs, evaluation metrics, datasets, performance comparison,
existing challenges, and promising directions for future research. Through this
survey, we wish to bring contributions in two aspects: first, readers can
conveniently identify the methods which are quite strong-baseline style in the
subcategory for further exploration; second, one can also employ suitable
methods for establishing a state-of-the-art end-to-end face recognition system
from scratch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring DeshuffleGANs in Self-Supervised Generative Adversarial Networks. (arXiv:2011.01730v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.01730">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) have become the most used networks
towards solving the problem of image generation. Self-supervised GANs are later
proposed to avoid the catastrophic forgetting of the discriminator and to
improve the image generation quality without needing the class labels. However,
the generalizability of the self-supervision tasks on different GAN
architectures is not studied before. To that end, we extensively analyze the
contribution of a previously proposed self-supervision task, deshuffling of the
DeshuffleGANs in the generalizability context. We assign the deshuffling task
to two different GAN discriminators and study the effects of the task on both
architectures. We extend the evaluations compared to the previously proposed
DeshuffleGANs on various datasets. We show that the DeshuffleGAN obtains the
best FID results for several datasets compared to the other self-supervised
GANs. Furthermore, we compare the deshuffling with the rotation prediction that
is firstly deployed to the GAN training and demonstrate that its contribution
exceeds the rotation prediction. We design the conditional DeshuffleGAN called
cDeshuffleGAN to evaluate the quality of the learnt representations. Lastly, we
show the contribution of the self-supervision tasks to the GAN training on the
loss landscape and present that the effects of these tasks may not be
cooperative to the adversarial training in some settings. Our code can be found
at https://github.com/gulcinbaykal/DeshuffleGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchically Decoupled Spatial-Temporal Contrast for Self-supervised Video Representation Learning. (arXiv:2011.11261v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11261">
<div class="article-summary-box-inner">
<span><p>We present a novel technique for self-supervised video representation
learning by: (a) decoupling the learning objective into two contrastive
subtasks respectively emphasizing spatial and temporal features, and (b)
performing it hierarchically to encourage multi-scale understanding. Motivated
by their effectiveness in supervised learning, we first introduce
spatial-temporal feature learning decoupling and hierarchical learning to the
context of unsupervised video learning. We show by experiments that
augmentations can be manipulated as regularization to guide the network to
learn desired semantics in contrastive learning, and we propose a way for the
model to separately capture spatial and temporal features at multiple scales.
We also introduce an approach to overcome the problem of divergent levels of
instance invariance at different hierarchies by modeling the invariance as loss
weights for objective re-weighting. Experiments on downstream action
recognition benchmarks on UCF101 and HMDB51 show that our proposed
Hierarchically Decoupled Spatial-Temporal Contrast (HDC) makes substantial
improvements over directly learning spatial-temporal features as a whole and
achieves competitive performance when compared with other state-of-the-art
unsupervised methods. Code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransPose: Keypoint Localization via Transformer. (arXiv:2012.14214v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14214">
<div class="article-summary-box-inner">
<span><p>While CNN-based models have made remarkable progress on human pose
estimation, what spatial dependencies they capture to localize keypoints
remains unclear. In this work, we propose a model called \textbf{TransPose},
which introduces Transformer for human pose estimation. The attention layers
built in Transformer enable our model to capture long-range relationships
efficiently and also can reveal what dependencies the predicted keypoints rely
on. To predict keypoint heatmaps, the last attention layer acts as an
aggregator, which collects contributions from image clues and forms maximum
positions of keypoints. Such a heatmap-based localization approach via
Transformer conforms to the principle of Activation
Maximization~\cite{erhan2009visualizing}. And the revealed dependencies are
image-specific and fine-grained, which also can provide evidence of how the
model handles special cases, e.g., occlusion. The experiments show that
TransPose achieves 75.8 AP and 75.0 AP on COCO validation and test-dev sets,
while being more lightweight and faster than mainstream CNN architectures. The
TransPose model also transfers very well on MPII benchmark, achieving superior
performance on the test set when fine-tuned with small training costs. Code and
pre-trained models are publicly
available\footnote{\url{https://github.com/yangsenius/TransPose}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning-based Face Super-Resolution: A Survey. (arXiv:2101.03749v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.03749">
<div class="article-summary-box-inner">
<span><p>Face super-resolution (FSR), also known as face hallucination, which is aimed
at enhancing the resolution of low-resolution (LR) face images to generate
high-resolution (HR) face images, is a domain-specific image super-resolution
problem. Recently, FSR has received considerable attention and witnessed
dazzling advances with the development of deep learning techniques. To date,
few summaries of the studies on the deep learning-based FSR are available. In
this survey, we present a comprehensive review of deep learning-based FSR
methods in a systematic manner. First, we summarize the problem formulation of
FSR and introduce popular assessment metrics and loss functions. Second, we
elaborate on the facial characteristics and popular datasets used in FSR.
Third, we roughly categorize existing methods according to the utilization of
facial characteristics. In each category, we start with a general description
of design principles, then present an overview of representative approaches,
and then discuss the pros and cons among them. Fourth, we evaluate the
performance of some state-of-the-art methods. Fifth, joint FSR and other tasks,
and FSR-related applications are roughly introduced. Finally, we envision the
prospects of further technological advancement in this field. A curated list of
papers and resources to face super-resolution are available at
\url{https://github.com/junjun-jiang/Face-Hallucination-Benchmark}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes. (arXiv:2101.06085v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06085">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation is a key technology for autonomous vehicles to
understand the surrounding scenes. The appealing performances of contemporary
models usually come at the expense of heavy computations and lengthy inference
time, which is intolerable for self-driving. Using light-weight architectures
(encoder-decoder or two-pathway) or reasoning on low-resolution images, recent
methods realize very fast scene parsing, even running at more than 100 FPS on a
single 1080Ti GPU. However, there is still a significant gap in performance
between these real-time methods and the models based on dilation backbones. To
tackle this problem, we proposed a family of efficient backbones specially
designed for real-time semantic segmentation. The proposed deep dual-resolution
networks (DDRNets) are composed of two deep branches between which multiple
bilateral fusions are performed. Additionally, we design a new contextual
information extractor named Deep Aggregation Pyramid Pooling Module (DAPPM) to
enlarge effective receptive fields and fuse multi-scale context based on
low-resolution feature maps. Our method achieves a new state-of-the-art
trade-off between accuracy and speed on both Cityscapes and CamVid dataset. In
particular, on a single 2080Ti GPU, DDRNet-23-slim yields 77.4% mIoU at 102 FPS
on Cityscapes test set and 74.7% mIoU at 230 FPS on CamVid test set. With
widely used test augmentation, our method is superior to most state-of-the-art
models and requires much less computation. Codes and trained models are
available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FlowReg: Fast Deformable Unsupervised Medical Image Registration using Optical Flow. (arXiv:2101.09639v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.09639">
<div class="article-summary-box-inner">
<span><p>We propose FlowReg, a deep learning-based framework for unsupervised image
registration for neuroimaging applications. The system is composed of two
architectures that are trained sequentially: FlowReg-A which affinely corrects
for gross differences between moving and fixed volumes in 3D followed by
FlowReg-O which performs pixel-wise deformations on a slice-by-slice basis for
fine tuning in 2D. The affine network regresses the 3D affine matrix based on a
correlation loss function that enforces global similarity. The deformable
network operates on 2D image slices based on the optical flow network
FlowNet-Simple but with three loss components. The photometric loss minimizes
pixel intensity differences differences, the smoothness loss encourages similar
magnitudes between neighbouring vectors, and a correlation loss that is used to
maintain the intensity similarity between fixed and moving image slices. The
proposed method is compared to four open source registration techniques ANTs,
Demons, SE, and Voxelmorph. In total, 4643 FLAIR MR imaging volumes are used
from dementia and vascular disease cohorts, acquired from over 60 international
centres with varying acquisition parameters. A battery of quantitative novel
registration validation metrics are proposed that focus on the structural
integrity of tissues, spatial alignment, and intensity similarity. Experimental
results show FlowReg (FlowReg-A+O) performs better than iterative-based
registration algorithms for intensity and spatial alignment metrics with a
Pixelwise Agreement of 0.65, correlation coefficient of 0.80, and Mutual
Information of 0.29. Among the deep learning frameworks, FlowReg-A or
FlowReg-A+O provided the highest performance over all but one of the metrics.
Results show that FlowReg is able to obtain high intensity and spatial
similarity while maintaining the shape and structure of anatomy and pathology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instance and Panoptic Segmentation Using Conditional Convolutions. (arXiv:2102.03026v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03026">
<div class="article-summary-box-inner">
<span><p>We propose a simple yet effective framework for instance and panoptic
segmentation, termed CondInst (conditional convolutions for instance and
panoptic segmentation). In the literature, top-performing instance segmentation
methods typically follow the paradigm of Mask R-CNN and rely on ROI operations
(typically ROIAlign) to attend to each instance. In contrast, we propose to
attend to the instances with dynamic conditional convolutions. Instead of using
instance-wise ROIs as inputs to the instance mask head of fixed weights, we
design dynamic instance-aware mask heads, conditioned on the instances to be
predicted. CondInst enjoys three advantages: 1.) Instance and panoptic
segmentation are unified into a fully convolutional network, eliminating the
need for ROI cropping and feature alignment. 2.) The elimination of the ROI
cropping also significantly improves the output instance mask resolution. 3.)
Due to the much improved capacity of dynamically-generated conditional
convolutions, the mask head can be very compact (e.g., 3 conv. layers, each
having only 8 channels), leading to significantly faster inference time per
instance and making the overall inference time almost constant, irrelevant to
the number of instances. We demonstrate a simpler method that can achieve
improved accuracy and inference speed on both instance and panoptic
segmentation tasks. On the COCO dataset, we outperform a few state-of-the-art
methods. We hope that CondInst can be a strong baseline for instance and
panoptic segmentation. Code is available at: https://git.io/AdelaiDet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AttributeNet: Attribute Enhanced Vehicle Re-Identification. (arXiv:2102.03898v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03898">
<div class="article-summary-box-inner">
<span><p>Vehicle Re-Identification (V-ReID) is a critical task that associates the
same vehicle across images from different camera viewpoints. Many works explore
attribute clues to enhance V-ReID; however, there is usually a lack of
effective interaction between the attribute-related modules and final V-ReID
objective. In this work, we propose a new method to efficiently explore
discriminative information from vehicle attributes (for instance, color and
type). We introduce AttributeNet (ANet) that jointly extracts identity-relevant
features and attribute features. We enable the interaction by distilling the
ReID-helpful attribute feature and adding it into the general ReID feature to
increase the discrimination power. Moreover, we propose a constraint, named
Amelioration Constraint (AC), which encourages the feature after adding
attribute features onto the general ReID feature to be more discriminative than
the original general ReID feature. We validate the effectiveness of our
framework on three challenging datasets. Experimental results show that our
method achieves the state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning for Unconstrained Space-Time Video Super-Resolution. (arXiv:2102.13011v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.13011">
<div class="article-summary-box-inner">
<span><p>Recent years have seen considerable research activities devoted to video
enhancement that simultaneously increases temporal frame rate and spatial
resolution. However, the existing methods either fail to explore the intrinsic
relationship between temporal and spatial information or lack flexibility in
the choice of final temporal/spatial resolution. In this work, we propose an
unconstrained space-time video super-resolution network, which can effectively
exploit space-time correlation to boost performance. Moreover, it has complete
freedom in adjusting the temporal frame rate and spatial resolution through the
use of the optical flow technique and a generalized pixelshuffle operation. Our
extensive experiments demonstrate that the proposed method not only outperforms
the state-of-the-art, but also requires far fewer parameters and less running
time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Trust Region for Weakly Supervised Segmentation. (arXiv:2104.01948v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01948">
<div class="article-summary-box-inner">
<span><p>Acquisition of training data for the standard semantic segmentation is
expensive if requiring that each pixel is labeled. Yet, current methods
significantly deteriorate in weakly supervised settings, e.g. where a fraction
of pixels is labeled or when only image-level tags are available. It has been
shown that regularized losses - originally developed for unsupervised low-level
segmentation and representing geometric priors on pixel labels - can
considerably improve the quality of weakly supervised training. However, many
common priors require optimization stronger than gradient descent. Thus, such
regularizers have limited applicability in deep learning. We propose a new
robust trust region approach for regularized losses improving the
state-of-the-art results. Our approach can be seen as a higher-order
generalization of the classic chain rule. It allows neural network optimization
to use strong low-level solvers for the corresponding regularizers, including
discrete ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering. (arXiv:2104.03149v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03149">
<div class="article-summary-box-inner">
<span><p>We introduce an evaluation methodology for visual question answering (VQA) to
better diagnose cases of shortcut learning. These cases happen when a model
exploits spurious statistical regularities to produce correct answers but does
not actually deploy the desired behavior. There is a need to identify possible
shortcuts in a dataset and assess their use before deploying a model in the
real world. The research community in VQA has focused exclusively on
question-based shortcuts, where a model might, for example, answer "What is the
color of the sky" with "blue" by relying mostly on the question-conditional
training prior and give little weight to visual evidence. We go a step further
and consider multimodal shortcuts that involve both questions and images. We
first identify potential shortcuts in the popular VQA v2 training set by mining
trivial predictive rules such as co-occurrences of words and visual elements.
We then introduce VQA-CounterExamples (VQA-CE), an evaluation protocol based on
our subset of CounterExamples i.e. image-question-answer triplets where our
rules lead to incorrect answers. We use this new evaluation in a large-scale
study of existing approaches for VQA. We demonstrate that even state-of-the-art
models perform poorly and that existing techniques to reduce biases are largely
ineffective in this context. Our findings suggest that past work on
question-based biases in VQA has only addressed one facet of a complex issue.
The code for our method is available at
https://github.com/cdancette/detect-shortcuts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STRUDEL: Self-Training with Uncertainty Dependent Label Refinement across Domains. (arXiv:2104.11596v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11596">
<div class="article-summary-box-inner">
<span><p>We propose an unsupervised domain adaptation (UDA) approach for white matter
hyperintensity (WMH) segmentation, which uses Self-Training with Uncertainty
DEpendent Label refinement (STRUDEL). Self-training has recently been
introduced as a highly effective method for UDA, which is based on
self-generated pseudo labels. However, pseudo labels can be very noisy and
therefore deteriorate model performance. We propose to predict the uncertainty
of pseudo labels and integrate it in the training process with an
uncertainty-guided loss function to highlight labels with high certainty.
STRUDEL is further improved by incorporating the segmentation output of an
existing method in the pseudo label generation that showed high robustness for
WMH segmentation. In our experiments, we evaluate STRUDEL with a standard U-Net
and a modified network with a higher receptive field. Our results on WMH
segmentation across datasets demonstrate the significant improvement of STRUDEL
with respect to standard self-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visformer: The Vision-friendly Transformer. (arXiv:2104.12533v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12533">
<div class="article-summary-box-inner">
<span><p>The past year has witnessed the rapid development of applying the Transformer
module to vision problems. While some researchers have demonstrated that
Transformer-based models enjoy a favorable ability of fitting data, there are
still growing number of evidences showing that these models suffer over-fitting
especially when the training data is limited. This paper offers an empirical
study by performing step-by-step operations to gradually transit a
Transformer-based model to a convolution-based model. The results we obtain
during the transition process deliver useful messages for improving visual
recognition. Based on these observations, we propose a new architecture named
Visformer, which is abbreviated from the `Vision-friendly Transformer'. With
the same computational complexity, Visformer outperforms both the
Transformer-based and convolution-based models in terms of ImageNet
classification accuracy, and the advantage becomes more significant when the
model complexity is lower or the training set is smaller. The code is available
at https://github.com/danczs/Visformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining in Style: Training a GAN to explain a classifier in StyleSpace. (arXiv:2104.13369v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13369">
<div class="article-summary-box-inner">
<span><p>Image classification models can depend on multiple different semantic
attributes of the image. An explanation of the decision of the classifier needs
to both discover and visualize these properties. Here we present StylEx, a
method for doing this, by training a generative model to specifically explain
multiple attributes that underlie classifier decisions. A natural source for
such attributes is the StyleSpace of StyleGAN, which is known to generate
semantically meaningful dimensions in the image. However, because standard GAN
training is not dependent on the classifier, it may not represent these
attributes which are important for the classifier decision, and the dimensions
of StyleSpace may represent irrelevant attributes. To overcome this, we propose
a training procedure for a StyleGAN, which incorporates the classifier model,
in order to learn a classifier-specific StyleSpace. Explanatory attributes are
then selected from this space. These can be used to visualize the effect of
changing multiple attributes per image, thus providing image-specific
explanations. We apply StylEx to multiple domains, including animals, leaves,
faces and retinal images. For these, we show how an image can be modified in
different ways to change its classifier output. Our results show that the
method finds attributes that align well with semantic ones, generate meaningful
image-specific explanations, and are human-interpretable as measured in
user-studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACDC: The Adverse Conditions Dataset with Correspondences for Semantic Driving Scene Understanding. (arXiv:2104.13395v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13395">
<div class="article-summary-box-inner">
<span><p>Level 5 autonomy for self-driving cars requires a robust visual perception
system that can parse input images under any visual condition. However,
existing semantic segmentation datasets are either dominated by images captured
under normal conditions or are small in scale. To address this, we introduce
ACDC, the Adverse Conditions Dataset with Correspondences for training and
testing semantic segmentation methods on adverse visual conditions. ACDC
consists of a large set of 4006 images which are equally distributed between
four common adverse conditions: fog, nighttime, rain, and snow. Each
adverse-condition image comes with a high-quality fine pixel-level semantic
annotation, a corresponding image of the same scene taken under normal
conditions, and a binary mask that distinguishes between intra-image regions of
clear and uncertain semantic content. Thus, ACDC supports both standard
semantic segmentation and the newly introduced uncertainty-aware semantic
segmentation. A detailed empirical study demonstrates the challenges that the
adverse domains of ACDC pose to state-of-the-art supervised and unsupervised
approaches and indicates the value of our dataset in steering future progress
in the field. Our dataset and benchmark are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous-discrete multiple target tracking with out-of-sequence measurements. (arXiv:2106.04898v2 [eess.SY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04898">
<div class="article-summary-box-inner">
<span><p>This paper derives the optimal Bayesian processing of an out-of-sequence
(OOS) set of measurements in continuous-time for multiple target tracking. We
consider a multi-target system modelled in continuous time that is discretised
at the time steps when we receive the measurements, which are distributed
according to the standard point target model. All information about this system
at the sampled time steps is provided by the posterior density on the set of
all trajectories. This density can be computed via the continuous-discrete
trajectory Poisson multi-Bernoulli mixture (TPMBM) filter. When we receive an
OOS measurement, the optimal Bayesian processing performs a retrodiction step
that adds trajectory information at the OOS measurement time stamp followed by
an update step. After the OOS measurement update, the posterior remains in
TPMBM form. We also provide a computationally lighter alternative based on a
trajectory Poisson multi-Bernoulli filter. The effectiveness of the two
approaches to handle OOS measurements is evaluated via simulations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Video Classification Meets Incremental Classes. (arXiv:2106.15827v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15827">
<div class="article-summary-box-inner">
<span><p>With the rapid development of social media, tremendous videos with new
classes are generated daily, which raise an urgent demand for video
classification methods that can continuously update new classes while
maintaining the knowledge of old videos with limited storage and computing
resources. In this paper, we summarize this task as Class-Incremental Video
Classification (CIVC) and propose a novel framework to address it. As a subarea
of incremental learning tasks, the challenge of catastrophic forgetting is
unavoidable in CIVC. To better alleviate it, we utilize some characteristics of
videos. First, we decompose the spatio-temporal knowledge before distillation
rather than treating it as a whole in the knowledge transfer process;
trajectory is also used to refine the decomposition. Second, we propose a dual
granularity exemplar selection method to select and store representative video
instances of old classes and key-frames inside videos under a tight storage
budget. We benchmark our method and previous SOTA class-incremental learning
methods on Something-Something V2 and Kinetics datasets, and our method
outperforms previous methods significantly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CI-Net: Contextual Information for Joint Semantic Segmentation and Depth Estimation. (arXiv:2107.13800v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13800">
<div class="article-summary-box-inner">
<span><p>Monocular depth estimation and semantic segmentation are two fundamental
goals of scene understanding. Due to the advantages of task interaction, many
works study the joint task learning algorithm. However, most existing methods
fail to fully leverage the semantic labels, ignoring the provided context
structures and only using them to supervise the prediction of segmentation
split, which limit the performance of both tasks. In this paper, we propose a
network injected with contextual information (CI-Net) to solve the problem.
Specifically, we introduce self-attention block in the encoder to generate
attention map. With supervision from the ideal attention map created by
semantic label, the network is embedded with contextual information so that it
could understand scene better and utilize correlated features to make accurate
prediction. Besides, a feature sharing module is constructed to make the
task-specific features deeply fused and a consistency loss is devised to make
the features mutually guided. We evaluate the proposed CI-Net on the
NYU-Depth-v2 and SUN-RGBD datasets. The experimental results validate that our
proposed CI-Net could effectively improve the accuracy of semantic segmentation
and depth estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep graph matching meets mixed-integer linear programming: Relax at your own risk ?. (arXiv:2108.00394v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00394">
<div class="article-summary-box-inner">
<span><p>Graph matching is an important problem that has received widespread
attention, especially in the field of computer vision. Recently,
state-of-the-art methods seek to incorporate graph matching with deep learning.
However, there is no research to explain what role the graph matching algorithm
plays in the model. Therefore, we propose an approach integrating a MILP
formulation of the graph matching problem. This formulation is solved to
optimal and it provides inherent baseline. Meanwhile, similar approaches are
derived by releasing the optimal guarantee of the graph matching solver and by
introducing a quality level. This quality level controls the quality of the
solutions provided by the graph matching solver. In addition, several
relaxations of the graph matching problem are put to the test. Our experimental
evaluation gives several theoretical insights and guides the direction of deep
graph matching methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Relighting against Face Recognition. (arXiv:2108.07920v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07920">
<div class="article-summary-box-inner">
<span><p>Deep face recognition (FR) has achieved significantly high accuracy on
several challenging datasets and fosters successful real-world applications,
even showing high robustness to the illumination variation that is usually
regarded as a main threat to the FR system. However, in the real world,
illumination variation caused by diverse lighting conditions cannot be fully
covered by the limited face dataset. In this paper, we study the threat of
lighting against FR from a new angle, i.e., adversarial attack, and identify a
new task, i.e., adversarial relighting. Given a face image, adversarial
relighting aims to produce a naturally relighted counterpart while fooling the
state-of-the-art deep FR methods. To this end, we first propose the physical
model-based adversarial relighting attack (ARA) denoted as
albedo-quotient-based adversarial relighting attack (AQ-ARA). It generates
natural adversarial light under the physical lighting model and guidance of FR
systems and synthesizes adversarially relighted face images. Moreover, we
propose the auto-predictive adversarial relighting attack (AP-ARA) by training
an adversarial relighting network (ARNet) to automatically predict the
adversarial light in a one-step manner according to different input faces,
allowing efficiency-sensitive applications. More importantly, we propose to
transfer the above digital attacks to physical ARA (Phy-ARA) through a precise
relighting device, making the estimated adversarial lighting condition
reproducible in the real world. We validate our methods on three
state-of-the-art deep FR methods, i.e., FaceNet, ArcFace, and CosFace, on two
public datasets. The extensive and insightful results demonstrate our work can
generate realistic adversarial relighted face images fooling FR easily,
revealing the threat of specific light directions and strengths.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual-and-Language Navigation: A Survey and Taxonomy. (arXiv:2108.11544v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11544">
<div class="article-summary-box-inner">
<span><p>An agent that can understand natural-language instruction and carry out
corresponding actions in the visual world is one of the long-term challenges of
Artificial Intelligent (AI). Due to multifarious instructions from humans, it
requires the agent can link natural language to vision and action in
unstructured, previously unseen environments. If the instruction given by human
is a navigation task, this challenge is called Visual-and-Language Navigation
(VLN). It is a booming multi-disciplinary field of increasing importance and
with extraordinary practicality. Instead of focusing on the details of specific
methods, this paper provides a comprehensive survey on VLN tasks and makes a
classification carefully according the different characteristics of language
instructions in these tasks. According to when the instructions are given, the
tasks can be divided into single-turn and multi-turn. For single-turn tasks, we
further divided them into goal-orientation and route-orientation based on
whether the instructions contain a route. For multi-turn tasks, we divided them
into imperative task and interactive task based on whether the agent responses
to the instructions. This taxonomy enable researchers to better grasp the key
point of a specific task and identify directions for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ThresholdNet: Pruning Tool for Densely Connected Convolutional Networks. (arXiv:2108.12604v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12604">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have made significant progress in the field of computer
vision. Recent studies have shown that depth, width and shortcut connections of
neural network architectures play a crucial role in their performance. One of
the most advanced neural network architectures, DenseNet, has achieved
excellent convergence rates through dense connections. However, it still has
obvious shortcomings in the usage of amount of memory. In this paper, we
introduce a new type of pruning tool, threshold, which refers to the principle
of the threshold voltage in MOSFET. This work employs this method to connect
blocks of different depths in different ways to reduce the usage of memory. It
is denoted as ThresholdNet. We evaluate ThresholdNet and other different
networks on datasets of CIFAR10. Experiments show that HarDNet is twice as fast
as DenseNet, and on this basis, ThresholdNet is 10% faster and 10% lower error
rate than HarDNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scene Synthesis via Uncertainty-Driven Attribute Synchronization. (arXiv:2108.13499v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13499">
<div class="article-summary-box-inner">
<span><p>Developing deep neural networks to generate 3D scenes is a fundamental
problem in neural synthesis with immediate applications in architectural CAD,
computer graphics, as well as in generating virtual robot training
environments. This task is challenging because 3D scenes exhibit diverse
patterns, ranging from continuous ones, such as object sizes and the relative
poses between pairs of shapes, to discrete patterns, such as occurrence and
co-occurrence of objects with symmetrical relationships. This paper introduces
a novel neural scene synthesis approach that can capture diverse feature
patterns of 3D scenes. Our method combines the strength of both neural
network-based and conventional scene synthesis approaches. We use the
parametric prior distributions learned from training data, which provide
uncertainties of object attributes and relative attributes, to regularize the
outputs of feed-forward neural models. Moreover, instead of merely predicting a
scene layout, our approach predicts an over-complete set of attributes. This
methodology allows us to utilize the underlying consistency constraints among
the predicted attributes to prune infeasible predictions. Experimental results
show that our approach outperforms existing methods considerably. The generated
3D scenes interpolate the training data faithfully while preserving both
continuous and discrete feature patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fiducial marker recovery and detection from severely truncated data in navigation assisted spine surgery. (arXiv:2108.13844v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13844">
<div class="article-summary-box-inner">
<span><p>Fiducial markers are commonly used in navigation assisted minimally invasive
spine surgery (MISS) and they help transfer image coordinates into real world
coordinates. In practice, these markers might be located outside the
field-of-view (FOV), due to the limited detector sizes of C-arm cone-beam
computed tomography (CBCT) systems used in intraoperative surgeries. As a
consequence, reconstructed markers in CBCT volumes suffer from artifacts and
have distorted shapes, which sets an obstacle for navigation. In this work, we
propose two fiducial marker detection methods: direct detection from distorted
markers (direct method) and detection after marker recovery (recovery method).
For direct detection from distorted markers in reconstructed volumes, an
efficient automatic marker detection method using two neural networks and a
conventional circle detection algorithm is proposed. For marker recovery, a
task-specific learning strategy is proposed to recover markers from severely
truncated data. Afterwards, a conventional marker detection algorithm is
applied for position detection. The two methods are evaluated on simulated data
and real data, both achieving a marker registration error smaller than 0.2 mm.
Our experiments demonstrate that the direct method is capable of detecting
distorted markers accurately and the recovery method with task-specific
learning has high robustness and generalizability on various data sets. In
addition, the task-specific learning is able to reconstruct other structures of
interest accurately, e.g. ribs for image-guided needle biopsy, from severely
truncated data, which empowers CBCT systems with new potential applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simultaneous Nuclear Instance and Layer Segmentation in Oral Epithelial Dysplasia. (arXiv:2108.13904v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13904">
<div class="article-summary-box-inner">
<span><p>Oral epithelial dysplasia (OED) is a pre-malignant histopathological
diagnosis given to lesions of the oral cavity. Predicting OED grade or whether
a case will transition to malignancy is critical for early detection and
appropriate treatment. OED typically begins in the lower third of the
epithelium before progressing upwards with grade severity, thus we have
suggested that segmenting intra-epithelial layers, in addition to individual
nuclei, may enable researchers to evaluate important layer-specific
morphological features for grade/malignancy prediction. We present HoVer-Net+,
a deep learning framework to simultaneously segment (and classify) nuclei and
(intra-)epithelial layers in H&amp;E stained slides from OED cases. The proposed
architecture consists of an encoder branch and four decoder branches for
simultaneous instance segmentation of nuclei and semantic segmentation of the
epithelial layers. We show that the proposed model achieves the
state-of-the-art (SOTA) performance in both tasks, with no additional costs
when compared to previous SOTA methods for each task. To the best of our
knowledge, ours is the first method for simultaneous nuclear instance
segmentation and semantic tissue segmentation, with potential for use in
computational pathology for other similar simultaneous tasks and for future
studies into malignancy prediction.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-02 23:02:24.704083977 UTC">2021-09-02 23:02:24 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.1</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>