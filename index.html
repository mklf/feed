<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-02-01T01:30:00Z">02-01</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Schema-Free Dependency Parsing via Sequence Generation. (arXiv:2201.12407v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12407">
<div class="article-summary-box-inner">
<span><p>Dependency parsing aims to extract syntactic dependency structure or semantic
dependency structure for sentences. Existing methods suffer the drawbacks of
lacking universality or highly relying on the auxiliary decoder. To remedy
these drawbacks, we propose to achieve universal and schema-free Dependency
Parsing (DP) via Sequence Generation (SG) DPSG by utilizing only the
pre-trained language model (PLM) without any auxiliary structures or parsing
algorithms. We first explore different serialization designing strategies for
converting parsing structures into sequences. Then we design dependency units
and concatenate these units into the sequence for DPSG. Thanks to the high
flexibility of the sequence generation, our DPSG can achieve both syntactic DP
and semantic DP using a single model. By concatenating the prefix to indicate
the specific schema with the sequence, our DPSG can even accomplish
multi-schemata parsing. The effectiveness of our DPSG is demonstrated by the
experiments on widely used DP benchmarks, i.e., PTB, CODT, SDP15, and
SemEval16. DPSG achieves comparable results with the first-tier methods on all
the benchmarks and even the state-of-the-art (SOTA) performance in CODT and
SemEval16. This paper demonstrates our DPSG has the potential to be a new
parsing paradigm. We will release our codes upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Approach to Entity-Centric Context Tracking in Social Conversations. (arXiv:2201.12409v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12409">
<div class="article-summary-box-inner">
<span><p>In human-human conversations, Context Tracking deals with identifying
important entities and keeping track of their properties and relationships.
This is a challenging problem that encompasses several subtasks such as slot
tagging, coreference resolution, resolving plural mentions and entity linking.
We approach this problem as an end-to-end modeling task where the
conversational context is represented by an entity repository containing the
entity references mentioned so far, their properties and the relationships
between them. The repository is updated turn-by-turn, thus making training and
inference computationally efficient even for long conversations. This paper
lays the groundwork for an investigation of this framework in two ways. First,
we release Contrack, a large scale human-human conversation corpus for context
tracking with people and location annotations. It contains over 7000
conversations with an average of 11.8 turns, 5.8 entities and 15.2 references
per conversation. Second, we open-source a neural network architecture for
context tracking. Finally we compare this network to state-of-the-art
approaches for the subtasks it subsumes and report results on the involved
tradeoffs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval. (arXiv:2201.12431v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12431">
<div class="article-summary-box-inner">
<span><p>Retrieval-based language models (R-LM) model the probability of natural
language text by combining a standard language model (LM) with examples
retrieved from an external datastore at test time. While effective, a major
bottleneck of using these models in practice is the computationally costly
datastore search, which can be performed as frequently as every time step. In
this paper, we present RetoMaton -- retrieval automaton -- which approximates
the datastore search, based on (1) clustering of entries into "states", and (2)
state transitions from previous entries. This effectively results in a weighted
finite automaton built on top of the datastore, instead of representing the
datastore as a flat list. The creation of the automaton is unsupervised, and a
RetoMaton can be constructed from any text collection: either the original
training corpus or from another domain. Traversing this automaton at inference
time, in parallel to the LM inference, reduces its perplexity, or alternatively
saves up to 83% of the nearest neighbor searches over kNN-LM (Khandelwal et
al., 2020), without hurting perplexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey. (arXiv:2201.12438v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12438">
<div class="article-summary-box-inner">
<span><p>While commonsense knowledge acquisition and reasoning has traditionally been
a core research topic in the knowledge representation and reasoning community,
recent years have seen a surge of interest in the natural language processing
community in developing pre-trained models and testing their ability to address
a variety of newly designed commonsense knowledge reasoning and generation
tasks. This paper presents a survey of these tasks, discusses the strengths and
weaknesses of state-of-the-art pre-trained models for commonsense reasoning and
generation as revealed by these tasks, and reflects on future research
directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ScaLA: Accelerating Adaptation of Pre-Trained Transformer-Based Language Models via Efficient Large-Batch Adversarial Noise. (arXiv:2201.12469v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12469">
<div class="article-summary-box-inner">
<span><p>In recent years, large pre-trained Transformer-based language models have led
to dramatic improvements in many natural language understanding tasks. To train
these models with increasing sizes, many neural network practitioners attempt
to increase the batch sizes in order to leverage multiple GPUs to improve
training speed. However, increasing the batch size often makes the optimization
more difficult, leading to slow convergence or poor generalization that can
require orders of magnitude more training time to achieve the same model
quality. In this paper, we explore the steepness of the loss landscape of
large-batch optimization for adapting pre-trained Transformer-based language
models to domain-specific tasks and find that it tends to be highly complex and
irregular, posing challenges to generalization on downstream tasks.
</p>
<p>To tackle this challenge, we propose ScaLA, a novel and efficient method to
accelerate the adaptation speed of pre-trained transformer networks. Different
from prior methods, we take a sequential game-theoretic approach by adding
lightweight adversarial noise into large-batch optimization, which
significantly improves adaptation speed while preserving model generalization.
Experiment results show that ScaLA attains 2.7--9.8$\times$ adaptation speedups
over the baseline for GLUE on BERT-base and RoBERTa-large, while achieving
comparable and sometimes higher accuracy than the state-of-the-art large-batch
optimization methods. Finally, we also address the theoretical aspect of
large-batch optimization with adversarial noise and provide a theoretical
convergence rate analysis for ScaLA using techniques for analyzing non-convex
saddle-point problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Transliteration Help Multilingual Language Modeling?. (arXiv:2201.12501v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12501">
<div class="article-summary-box-inner">
<span><p>As there is a scarcity of large representative corpora for most languages, it
is important for Multilingual Language Models (MLLM) to extract the most out of
existing corpora. In this regard, script diversity presents a challenge to
MLLMs by reducing lexical overlap among closely related languages. Therefore,
transliterating closely related languages that use different writing scripts to
a common script may improve the downstream task performance of MLLMs. In this
paper, we pretrain two ALBERT models to empirically measure the effect of
transliteration on MLLMs. We specifically focus on the Indo-Aryan language
family, which has the highest script diversity in the world. Afterward, we
evaluate our models on the IndicGLUE benchmark. We perform Mann-Whitney U test
to rigorously verify whether the effect of transliteration is significant or
not. We find that transliteration benefits the low-resource languages without
negatively affecting the comparatively high-resource languages. We also measure
the cross-lingual representation similarity (CLRS) of the models using centered
kernel alignment (CKA) on parallel sentences of eight languages from the
FLORES-101 dataset. We find that the hidden representations of the
transliteration-based model have higher and more stable CLRS scores. Our code
is available at Github (github.com/ibraheem-moosa/XLM-Indic) and Hugging Face
Hub (huggingface.co/ibraheemmoosa/xlmindic-base-multiscript and
huggingface.co/ibraheemmoosa/xlmindic-base-uniscript).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Summarization with Customized Granularities. (arXiv:2201.12502v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12502">
<div class="article-summary-box-inner">
<span><p>Text summarization is a personalized and customized task, i.e., for one
document, users often have different preferences for the summary. As a key
aspect of customization in summarization, granularity is used to measure the
semantic coverage between summary and source document. Coarse-grained summaries
can only contain the most central event in the original text, while
fine-grained summaries cover more sub-events and corresponding details.
However, previous studies mostly develop systems in the single-granularity
scenario. And models that can generate summaries with customizable semantic
coverage still remain an under-explored topic. In this paper, we propose the
first unsupervised multi-granularity summarization framework, GranuSum. We take
events as the basic semantic units of the source documents and propose to rank
these events by their salience. We also develop a model to summarize input
documents with given events as anchors and hints. By inputting different
numbers of events, GranuSum is capable of producing multi-granular summaries in
an unsupervised manner. Meanwhile, to evaluate multi-granularity summarization
models, we annotate a new benchmark GranuDUC, in which we write multiple
summaries of different granularities for each document cluster. Experimental
results confirm the substantial superiority of GranuSum on multi-granularity
summarization over several baseline systems. Furthermore, by experimenting on
conventional unsupervised abstractive summarization tasks, we find that
GranuSum, by exploiting the event information, can also achieve new
state-of-the-art results under this scenario, outperforming strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoDistil: Few-shot Task-agnostic Neural Architecture Search for Distilling Large Language Models. (arXiv:2201.12507v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12507">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) methods compress large models into smaller
students with manually-designed student architectures given pre-specified
computational cost. This requires several trials to find a viable student, and
further repeating the process for each student or computational budget change.
We use Neural Architecture Search (NAS) to automatically distill several
compressed students with variable cost from a large model. Current works train
a single SuperLM consisting of millions of subnetworks with weight-sharing,
resulting in interference between subnetworks of different sizes. Our framework
AutoDistil addresses above challenges with the following steps: (a)
Incorporates inductive bias and heuristics to partition Transformer search
space into K compact sub-spaces (K=3 for typical student sizes of base, small
and tiny); (b) Trains one SuperLM for each sub-space using task-agnostic
objective (e.g., self-attention distillation) with weight-sharing of students;
(c) Lightweight search for the optimal student without re-training. Fully
task-agnostic training and search allow students to be reused for fine-tuning
on any downstream task. Experiments on GLUE benchmark against state-of-the-art
KD and NAS methods demonstrate AutoDistil to outperform leading compression
techniques with upto 2.7x reduction in computational cost and negligible loss
in task performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Commonsense Knowledge into Story Ending Generation via Heterogeneous Graph Networks. (arXiv:2201.12538v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12538">
<div class="article-summary-box-inner">
<span><p>Story ending generation is an interesting and challenging task, which aims to
generate a coherent and reasonable ending given a story context. The key
challenges of the task lie in how to comprehend the story context sufficiently
and handle the implicit knowledge behind story clues effectively, which are
still under-explored by previous work. In this paper, we propose a Story
Heterogeneous Graph Network (SHGN) to explicitly model both the information of
story context at different granularity levels and the multi-grained interactive
relations among them. In detail, we consider commonsense knowledge, words and
sentences as three types of nodes. To aggregate non-local information, a global
node is also introduced. Given this heterogeneous graph network, the node
representations are updated through graph propagation, which adequately
utilizes commonsense knowledge to facilitate story comprehension. Moreover, we
design two auxiliary tasks to implicitly capture the sentiment trend and key
events lie in the context. The auxiliary tasks are jointly optimized with the
primary story ending generation task in a multi-task learning strategy.
Extensive experiments on the ROCStories Corpus show that the developed model
achieves new state-of-the-art performances. Human study further demonstrates
that our model generates more reasonable story endings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Continual Learning for Spoken Keyword Spotting. (arXiv:2201.12546v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12546">
<div class="article-summary-box-inner">
<span><p>Catastrophic forgetting is a thorny challenge when updating keyword spotting
(KWS) models after deployment. To tackle such challenges, we propose a
progressive continual learning strategy for small-footprint spoken keyword
spotting (PCL-KWS). Specifically, the proposed PCL-KWS framework introduces a
network instantiator to generate the task-specific sub-networks for remembering
previously learned keywords. As a result, the PCL-KWS approach incrementally
learns new keywords without forgetting prior knowledge. Besides, the
keyword-aware network scaling mechanism of PCL-KWS constrains the growth of
model parameters while achieving high performance. Experimental results show
that after learning five new tasks sequentially, our proposed PCL-KWS approach
archives the new state-of-the-art performance of 92.8% average accuracy for all
the tasks on Google Speech Command dataset compared with other baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Information-Based Approach to Unsupervised Domain-Adaptive Aspect-Based Sentiment Analysis. (arXiv:2201.12549v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12549">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task which aims to extract the aspects from sentences and identify their
corresponding sentiments. Aspect term extraction (ATE) is the crucial step for
ABSA. Due to the expensive annotation for aspect terms, we often lack labeled
target domain data for fine-tuning. To address this problem, many approaches
have been proposed recently to transfer common knowledge in an unsupervised
way, but such methods have too many modules and require expensive multi-stage
preprocessing. In this paper, we propose a simple but effective technique based
on mutual information maximization, which can serve as an additional component
to enhance any kind of model for cross-domain ABSA and ATE. Furthermore, we
provide some analysis of this approach. Experiment results show that our
proposed method outperforms the state-of-the-art methods for cross-domain ABSA
by 4.32% Micro-F1 on average over 10 different domain pairs. Apart from that,
our method can be extended to other sequence labeling tasks, such as named
entity recognition (NER).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Le Processus Powered Dirichlet-Hawkes comme A Priori Flexible pour Clustering Temporel de Textes. (arXiv:2201.12568v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12568">
<div class="article-summary-box-inner">
<span><p>The textual content of a document and its publication date are intertwined.
For example, the publication of a news article on a topic is influenced by
previous publications on similar issues, according to underlying temporal
dynamics. However, it can be challenging to retrieve meaningful information
when textual information conveys little. Furthermore, the textual content of a
document is not always correlated to its temporal dynamics. We develop a method
to create clusters of textual documents according to both their content and
publication time, the Powered Dirichlet-Hawkes process (PDHP). PDHP yields
significantly better results than state-of-the-art models when temporal
information or textual content is weakly informative. PDHP also alleviates the
hypothesis that textual content and temporal dynamics are perfectly correlated.
We demonstrate that PDHP generalizes previous work --such as DHP and UP.
Finally, we illustrate a possible application using a real-world dataset from
Reddit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hand Gesture Recognition of Dumb Person Using one Against All Neural Network. (arXiv:2201.12622v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12622">
<div class="article-summary-box-inner">
<span><p>We propose a new technique for recognition of dumb person hand gesture in
real world environment. In this technique, the hand image containing the
gesture is preprocessed and then hand region is segmented by convergent the RGB
color image to L.a.b color space. Only few statistical features are used to
classify the segmented image to different classes. Artificial Neural Network is
trained in sequential manner using one against all. When the system gets
trained, it becomes capable of recognition of each class in parallel manner.
The result of proposed technique is much better than existing techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep CNN Architecture with Novel Pooling Layer Applied to Two Sudanese Arabic Sentiment Datasets. (arXiv:2201.12664v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12664">
<div class="article-summary-box-inner">
<span><p>Arabic sentiment analysis has become an important research field in recent
years. Initially, work focused on Modern Standard Arabic (MSA), which is the
most widely-used form. Since then, work has been carried out on several
different dialects, including Egyptian, Levantine and Moroccan. Moreover, a
number of datasets have been created to support such work. However, up until
now, less work has been carried out on Sudanese Arabic, a dialect which has 32
million speakers. In this paper, two new publicly available datasets are
introduced, the 2-Class Sudanese Sentiment Dataset (SudSenti2) and the 3-Class
Sudanese Sentiment Dataset (SudSenti3). Furthermore, a CNN architecture, SCM,
is proposed, comprising five CNN layers together with a novel pooling layer,
MMA, to extract the best features. This SCM+MMA model is applied to SudSenti2
and SudSenti3 with accuracies of 92.75% and 84.39%. Next, the model is compared
to other deep learning classifiers and shown to be superior on these new
datasets. Finally, the proposed model is applied to the existing Saudi
Sentiment Dataset and to the MSA Hotel Arabic Review Dataset with accuracies
85.55% and 90.01%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models. (arXiv:2201.12675v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12675">
<div class="article-summary-box-inner">
<span><p>A central tenet of Federated learning (FL), which trains models without
centralizing user data, is privacy. However, previous work has shown that the
gradient updates used in FL can leak user information. While the most
industrial uses of FL are for text applications (e.g. keystroke prediction),
nearly all attacks on FL privacy have focused on simple image classifiers. We
propose a novel attack that reveals private user text by deploying malicious
parameter vectors, and which succeeds even with mini-batches, multiple users,
and long sequences. Unlike previous attacks on FL, the attack exploits
characteristics of both the Transformer architecture and the token embedding,
separately extracting tokens and positional embeddings to retrieve
high-fidelity text. This work suggests that FL on text, which has historically
been resistant to privacy attacks, is far more vulnerable than previously
thought.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VC-GPT: Visual Conditioned GPT for End-to-End Generative Vision-and-Language Pre-training. (arXiv:2201.12723v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12723">
<div class="article-summary-box-inner">
<span><p>Vision-and-language pre-training models (VLMs) have achieved tremendous
success in the cross-modal area, but most of them require millions of parallel
image-caption data for pre-training. Collating such data is expensive and
labor-intensive. In this work, we focus on reducing such need for generative
vision-and-language pre-training (G-VLP) by taking advantage of the visual
pre-trained model (CLIP-ViT) as encoder and language pre-trained model (GPT2)
as decoder. Unfortunately, GPT2 lacks a necessary cross-attention module, which
hinders the direct connection of CLIP-ViT and GPT2. To remedy such defects, we
conduct extensive experiments to empirically investigate how to design and
pre-train our model. Based on our experimental results, we propose a novel
G-VLP framework, Visual Conditioned GPT (VC-GPT), and pre-train it with a
small-scale parallel image-caption corpus (Visual Genome, only 110k distinct
images). Evaluating on the image captioning downstream tasks (MSCOCO and
Flickr30k Captioning), VC-GPT achieves either the best or the second-best
performance across all evaluation metrics over the previous works which consume
around 30 times more parallel data during pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Part of Speech Tagging (POST) of a Low-resource Language using another Language (Developing a POS-Tagged Lexicon for Kurdish (Sorani) using a Tagged Persian (Farsi) Corpus). (arXiv:2201.12793v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12793">
<div class="article-summary-box-inner">
<span><p>Tagged corpora play a crucial role in a wide range of Natural Language
Processing. The Part of Speech Tagging (POST) is essential in developing tagged
corpora. It is time-and-effort-consuming and costly, and therefore, it could be
more affordable if it is automated. The Kurdish language currently lacks
publicly available tagged corpora of proper sizes. Tagging the publicly
available Kurdish corpora can leverage the capability of those resources to a
higher level than what raw or segmented corpora can provide. Developing
POS-tagged lexicons can assist the mentioned task. We use a tagged corpus
(Bijankhan corpus) in Persian (Farsi) as a close language to Kurdish to develop
a POS-tagged lexicon. This paper presents the approach of leveraging the
resource of a close language to Kurdish to enrich its resources. A partial
dataset of the results is publicly available for non-commercial use under CC
BY-NC-SA 4.0 license at https://kurdishblark.github.io/. We plan to make the
whole tagged corpus available after further investigation on the outcome. The
dataset can help in developing POS-tagged lexicons for other Kurdish dialects
and automated Kurdish corpora tagging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Co-Regularized Adversarial Learning for Multi-Domain Text Classification. (arXiv:2201.12796v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12796">
<div class="article-summary-box-inner">
<span><p>Multi-domain text classification (MDTC) aims to leverage all available
resources from multiple domains to learn a predictive model that can generalize
well on these domains. Recently, many MDTC methods adopt adversarial learning,
shared-private paradigm, and entropy minimization to yield state-of-the-art
results. However, these approaches face three issues: (1) Minimizing domain
divergence can not fully guarantee the success of domain alignment; (2)
Aligning marginal feature distributions can not fully guarantee the
discriminability of the learned features; (3) Standard entropy minimization may
make the predictions on unlabeled data over-confident, deteriorating the
discriminability of the learned features. In order to address the above issues,
we propose a co-regularized adversarial learning (CRAL) mechanism for MDTC.
This approach constructs two diverse shared latent spaces, performs domain
alignment in each of them, and punishes the disagreements of these two
alignments with respect to the predictions on unlabeled data. Moreover, virtual
adversarial training (VAT) with entropy minimization is incorporated to impose
consistency regularization to the CRAL method. Experiments show that our model
outperforms state-of-the-art methods on two MDTC benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recognition of Implicit Geographic Movement in Text. (arXiv:2201.12799v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12799">
<div class="article-summary-box-inner">
<span><p>Analyzing the geographic movement of humans, animals, and other phenomena is
a growing field of research. This research has benefited urban planning,
logistics, animal migration understanding, and much more. Typically, the
movement is captured as precise geographic coordinates and time stamps with
Global Positioning Systems (GPS). Although some research uses computational
techniques to take advantage of implicit movement in descriptions of route
directions, hiking paths, and historical exploration routes, innovation would
accelerate with a large and diverse corpus. We created a corpus of sentences
labeled as describing geographic movement or not and including the type of
entity moving. Creating this corpus proved difficult without any comparable
corpora to start with, high human labeling costs, and since movement can at
times be interpreted differently. To overcome these challenges, we developed an
iterative process employing hand labeling, crowd voting for confirmation, and
machine learning to predict more labels. By merging advances in word embeddings
with traditional machine learning models and model ensembling, prediction
accuracy is at an acceptable level to produce a large silver-standard corpus
despite the small gold-standard corpus training set. Our corpus will likely
benefit computational processing of geography in text and spatial cognition, in
addition to detection of movement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving End-to-End Contextual Speech Recognition with Fine-grained Contextual Knowledge Selection. (arXiv:2201.12806v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12806">
<div class="article-summary-box-inner">
<span><p>Nowadays, most methods in end-to-end contextual speech recognition bias the
recognition process towards contextual knowledge. Since all-neural contextual
biasing methods rely on phrase-level contextual modeling and attention-based
relevance modeling, they may encounter confusion between similar
context-specific phrases, which hurts predictions at the token level. In this
work, we focus on mitigating confusion problems with fine-grained contextual
knowledge selection (FineCoS). In FineCoS, we introduce fine-grained knowledge
to reduce the uncertainty of token predictions. Specifically, we first apply
phrase selection to narrow the range of phrase candidates, and then conduct
token attention on the tokens in the selected phrase candidates. Moreover, we
re-normalize the attention weights of most relevant phrases in inference to
obtain more focused phrase-level contextual representations, and inject
position information to better discriminate phrases or tokens. On LibriSpeech
and an in-house 160,000-hour dataset, we explore the proposed methods based on
a controllable all-neural biasing method, collaborative decoding (ColDec). The
proposed methods provide at most 6.1% relative word error rate reduction on
LibriSpeech and 16.4% relative character error rate reduction on the in-house
dataset over ColDec.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TTS-Portuguese Corpus: a corpus for speech synthesis in Brazilian Portuguese. (arXiv:2005.05144v4 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.05144">
<div class="article-summary-box-inner">
<span><p>Speech provides a natural way for human-computer interaction. In particular,
speech synthesis systems are popular in different applications, such as
personal assistants, GPS applications, screen readers and accessibility tools.
However, not all languages are on the same level when in terms of resources and
systems for speech synthesis. This work consists of creating publicly available
resources for Brazilian Portuguese in the form of a novel dataset along with
deep learning models for end-to-end speech synthesis. Such dataset has 10.5
hours from a single speaker, from which a Tacotron 2 model with the RTISI-LA
vocoder presented the best performance, achieving a 4.03 MOS value. The
obtained results are comparable to related works covering English language and
the state-of-the-art in Portuguese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Boundary Regression Model for Nested Named Entity Recognition. (arXiv:2011.14330v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14330">
<div class="article-summary-box-inner">
<span><p>Recognizing named entities (NEs) is commonly conducted as a classification
problem that predicts a class tag for a word or a NE candidate in a sentence.
In shallow structures, categorized features are weighted to support the
prediction. Recent developments in neural networks have adopted deep structures
that map categorized features into continuous representations. This approach
unfolds a dense space saturated with high-order abstract semantic information,
where the prediction is based on distributed feature representations. In this
paper, positions of NEs in a sentence are represented as continuous values.
Then, a regression operation is introduced to regress boundaries of NEs in a
sentence. Based on boundary regression, we design a boundary regression model
to support nested NE recognition. It is a multiobjective learning framework,
which simultaneously predicts the classification score of a NE candidate and
refine its spatial location in a sentence. It has the advantage to resolve
nested NEs and support boundary regression for locating NEs in a sntence. By
sharing parameters for predicting and locating, this model enables more potent
nonlinear function approximators to enhance model discriminability. Experiments
demonstrate state-of-the-art performance for nested NE recognition\footnote{Our
codes to implement the BR model are available at:
\url{https://github.com/wuyuefei3/BR}.}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integer-only Zero-shot Quantization for Efficient Speech Recognition. (arXiv:2103.16827v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16827">
<div class="article-summary-box-inner">
<span><p>End-to-end neural network models achieve improved performance on various
automatic speech recognition (ASR) tasks. However, these models perform poorly
on edge hardware due to large memory and computation requirements. While
quantizing model weights and/or activations to low-precision can be a promising
solution, previous research on quantizing ASR models is limited. In particular,
the previous approaches use floating-point arithmetic during inference and thus
they cannot fully exploit efficient integer processing units. Moreover, they
require training and/or validation data during quantization, which may not be
available due to security or privacy concerns. To address these limitations, we
propose an integer-only, zero-shot quantization scheme for ASR models. In
particular, we generate synthetic data whose runtime statistics resemble the
real data, and we use it to calibrate models during quantization. We apply our
method to quantize QuartzNet, Jasper, and Conformer and show negligible WER
degradation as compared to the full-precision baseline models, even without
using any data. Moreover, we achieve up to 2.35x speedup on a T4 GPU and 4x
compression rate, with a modest WER degradation of &lt;1% with INT8 quantization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Pre-trained Convolutions Better than Pre-trained Transformers?. (arXiv:2105.03322v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03322">
<div class="article-summary-box-inner">
<span><p>In the era of pre-trained language models, Transformers are the de facto
choice of model architectures. While recent research has shown promise in
entirely convolutional, or CNN, architectures, they have not been explored
using the pre-train-fine-tune paradigm. In the context of language models, are
convolutional models competitive to Transformers when pre-trained? This paper
investigates this research question and presents several interesting findings.
Across an extensive set of experiments on 8 datasets/tasks, we find that
CNN-based pre-trained models are competitive and outperform their Transformer
counterpart in certain scenarios, albeit with caveats. Overall, the findings
outlined in this paper suggest that conflating pre-training and architectural
advances is misguided and that both advances should be considered
independently. We believe our research paves the way for a healthy amount of
optimism in alternative architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence Similarity Based on Contexts. (arXiv:2105.07623v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07623">
<div class="article-summary-box-inner">
<span><p>Existing methods to measure sentence similarity are faced with two
challenges: (1) labeled datasets are usually limited in size, making them
insufficient to train supervised neural models; (2) there is a training-test
gap for unsupervised language modeling (LM) based models to compute semantic
scores between sentences, since sentence-level semantics are not explicitly
modeled at training. This results in inferior performances in this task. In
this work, we propose a new framework to address these two issues. The proposed
framework is based on the core idea that the meaning of a sentence should be
defined by its contexts, and that sentence similarity can be measured by
comparing the probabilities of generating two sentences given the same context.
The proposed framework is able to generate high-quality, large-scale dataset
with semantic similarity scores between two sentences in an unsupervised
manner, with which the train-test gap can be largely bridged. Extensive
experiments show that the proposed framework achieves significant performance
boosts over existing baselines under both the supervised and unsupervised
settings across different datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World. (arXiv:2106.00188v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00188">
<div class="article-summary-box-inner">
<span><p>We propose PIGLeT: a model that learns physical commonsense knowledge through
interaction, and then uses this knowledge to ground language. We factorize
PIGLeT into a physical dynamics model, and a separate language model. Our
dynamics model learns not just what objects are but also what they do: glass
cups break when thrown, plastic ones don't. We then use it as the interface to
our language model, giving us a unified model of linguistic form and grounded
meaning. PIGLeT can read a sentence, simulate neurally what might happen next,
and then communicate that result through a literal symbolic representation, or
natural language.
</p>
<p>Experimental results show that our model effectively learns world dynamics,
along with how to communicate them. It is able to correctly forecast "what
happens next" given an English sentence over 80% of the time, outperforming a
100x larger, text-to-text approach by over 10%. Likewise, its natural language
summaries of physical interactions are also judged by humans as more accurate
than LM alternatives. We present comprehensive analysis showing room for future
work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discovery. (arXiv:2108.05669v3 [cs.DL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05669">
<div class="article-summary-box-inner">
<span><p>Isolated silos of scientific research and the growing challenge of
information overload limit awareness across the literature and hinder
innovation. Algorithmic curation and recommendation, which often prioritize
relevance, can further reinforce these informational "filter bubbles." In
response, we describe Bridger, a system for facilitating discovery of scholars
and their work. We construct a faceted representation of authors with
information gleaned from their papers and inferred author personas, and use it
to develop an approach that locates commonalities and contrasts between
scientists to balance relevance and novelty. In studies with computer science
researchers, this approach helps users discover authors considered useful for
generating novel research directions. We also demonstrate an approach for
displaying information about authors, boosting the ability to understand the
work of new, unfamiliar scholars. Our analysis reveals that Bridger connects
authors who have different citation profiles and publish in different venues,
raising the prospect of bridging diverse scientific communities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pairwise Supervised Contrastive Learning of Sentence Representations. (arXiv:2109.05424v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05424">
<div class="article-summary-box-inner">
<span><p>Many recent successes in sentence representation learning have been achieved
by simply fine-tuning on the Natural Language Inference (NLI) datasets with
triplet loss or siamese loss. Nevertheless, they share a common weakness:
sentences in a contradiction pair are not necessarily from different semantic
categories. Therefore, optimizing the semantic entailment and contradiction
reasoning objective alone is inadequate to capture the high-level semantic
structure. The drawback is compounded by the fact that the vanilla siamese or
triplet losses only learn from individual sentence pairs or triplets, which
often suffer from bad local optima. In this paper, we propose PairSupCon, an
instance discrimination based approach aiming to bridge semantic entailment and
contradiction understanding with high-level categorical concept encoding. We
evaluate PairSupCon on various downstream tasks that involve understanding
sentence semantics at different granularities. We outperform the previous
state-of-the-art method with $10\%$--$13\%$ averaged improvement on eight
clustering tasks, and $5\%$--$6\%$ averaged improvement on seven semantic
textual similarity (STS) tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers. (arXiv:2109.10686v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10686">
<div class="article-summary-box-inner">
<span><p>There remain many open questions pertaining to the scaling behaviour of
Transformer architectures. These scaling decisions and findings can be
critical, as training runs often come with an associated computational cost
which have both financial and/or environmental impact. The goal of this paper
is to present scaling insights from pretraining and finetuning Transformers.
While Kaplan et al. presents a comprehensive study of the scaling behaviour of
Transformer language models, the scope is only on the upstream (pretraining)
loss. Therefore, it is still unclear if these set of findings transfer to
downstream task within the context of the pretrain-finetune paradigm. The key
findings of this paper are as follows: (1) we show that aside from only the
model size, model shape matters for downstream fine-tuning, (2) scaling
protocols operate differently at different compute regions, (3) widely adopted
T5-base and T5-large sizes are Pareto-inefficient. To this end, we present
improved scaling protocols whereby our redesigned models achieve similar
downstream fine-tuning quality while having 50\% fewer parameters and training
40\% faster compared to the widely adopted T5-base model. We publicly release
over 100 pretrained checkpoints of different T5 configurations to facilitate
future research and analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breaking BERT: Understanding its Vulnerabilities for Named Entity Recognition through Adversarial Attack. (arXiv:2109.11308v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11308">
<div class="article-summary-box-inner">
<span><p>Both generic and domain-specific BERT models are widely used for natural
language processing (NLP) tasks. In this paper we investigate the vulnerability
of BERT models to variation in input data for Named Entity Recognition (NER)
through adversarial attack. Experimental results show that BERT models are
vulnerable to variation in the entity context with 20.2 to 45.0% of entities
predicted completely wrong and another 29.3 to 53.3% of entities predicted
wrong partially. BERT models seem most vulnerable to changes in the local
context of entities and often a single change is sufficient to fool the model.
The domain-specific BERT model trained from scratch (SciBERT) is more
vulnerable than the original BERT model or the domain-specific model that
retains the BERT vocabulary (BioBERT). We also find that BERT models are
particularly vulnerable to emergent entities. Our results chart the
vulnerabilities of BERT models for NER and emphasize the importance of further
research into uncovering and reducing these weaknesses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FacTeR-Check: Semi-automated fact-checking through Semantic Similarity and Natural Language Inference. (arXiv:2110.14532v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14532">
<div class="article-summary-box-inner">
<span><p>Our society produces and shares overwhelming amounts of information through
Online Social Networks (OSNs). Within this environment, misinformation and
disinformation have proliferated, becoming a public safety concern in most
countries. Allowing the public and professionals to efficiently find reliable
evidences about the factual veracity of a claim is a crucial step to mitigate
this harmful spread. To this end, we propose FacTeR-Check, a multilingual
architecture for semi-automated fact-checking that can be used for either
applications designed for the general public and by fact-checking
organisations. FacTeR-Check enables retrieving fact-checked information,
unchecked claims verification and tracking dangerous information over social
media. This architectures involves several modules developed to evaluate
semantic similarity, to calculate natural language inference and to retrieve
information from Online Social Networks. The union of all these components
builds a semi-automated fact-checking tool able of verifying new claims, to
extract related evidence, and to track the evolution of a hoax on a OSN. While
individual modules are validated on related benchmarks (mainly MSTS and SICK),
the complete architecture is validated using a new dataset called NLI19-SP that
is publicly released with COVID-19 related hoaxes and tweets from Spanish
social media. Our results show state-of-the-art performance on the individual
benchmarks, as well as producing a useful analysis of the evolution over time
of 61 different hoaxes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning. (arXiv:2111.10952v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10952">
<div class="article-summary-box-inner">
<span><p>Despite the recent success of multi-task learning and transfer learning for
natural language processing (NLP), few works have systematically studied the
effect of scaling up the number of tasks during pre-training. Towards this
goal, this paper introduces ExMix (Extreme Mixture): a massive collection of
107 supervised NLP tasks across diverse domains and task-families. Using ExMix,
we study the effect of multi-task pre-training at the largest scale to date,
and analyze co-training transfer amongst common families of tasks. Through this
analysis, we show that manually curating an ideal set of tasks for multi-task
pre-training is not straightforward, and that multi-task scaling can vastly
improve models on its own. Finally, we propose ExT5: a model pre-trained using
a multi-task objective of self-supervised span denoising and supervised ExMix.
Via extensive experiments, we show that ExT5 outperforms strong T5 baselines on
SuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of
ExMix. ExT5 also significantly improves sample efficiency while pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object-aware Video-language Pre-training for Retrieval. (arXiv:2112.00656v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00656">
<div class="article-summary-box-inner">
<span><p>Recently, by introducing large-scale dataset and strong transformer network,
video-language pre-training has shown great success especially for retrieval.
Yet, existing video-language transformer models do not explicitly fine-grained
semantic align. In this work, we present Object-aware Transformers, an
object-centric approach that extends video-language transformer to incorporate
object representations. The key idea is to leverage the bounding boxes and
object tags to guide the training process. We evaluate our model on three
standard sub-tasks of video-text matching on four widely used benchmarks. We
also provide deep analysis and detailed ablation about the proposed method. We
show clear improvement in performance across all tasks and datasets considered,
demonstrating the value of a model that incorporates object representations
into a video-language architecture. The code will be released at
\url{https://github.com/FingerRec/OA-Transformer}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone. (arXiv:2112.02418v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02418">
<div class="article-summary-box-inner">
<span><p>YourTTS brings the power of a multilingual approach to the task of zero-shot
multi-speaker TTS. Our method builds upon the VITS model and adds several novel
modifications for zero-shot multi-speaker and multilingual training. We
achieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and
results comparable to SOTA in zero-shot voice conversion on the VCTK dataset.
Additionally, our approach achieves promising results in a target language with
a single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS
and zero-shot voice conversion systems in low-resource languages. Finally, it
is possible to fine-tune the YourTTS model with less than 1 minute of speech
and achieve state-of-the-art results in voice similarity and with reasonable
quality. This is important to allow synthesis for speakers with a very
different voice or recording characteristics from those seen during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training. (arXiv:2201.10207v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10207">
<div class="article-summary-box-inner">
<span><p>We introduce a new approach for speech pre-training named SPIRAL which works
by learning denoising representation of perturbed data in a teacher-student
framework. Specifically, given a speech utterance, we first feed the utterance
to a teacher network to obtain corresponding representation. Then the same
utterance is perturbed and fed to a student network. The student network is
trained to output representation resembling that of the teacher. At the same
time, the teacher network is updated as moving average of student's weights
over training steps. In order to prevent representation collapse, we apply an
in-utterance contrastive loss as pre-training objective and impose position
randomization on the input to the teacher. SPIRAL achieves competitive or
better results compared to state-of-the-art speech pre-training method wav2vec
2.0, with significant reduction of training cost (80% for Base model, 65% for
Large model). Furthermore, we address the problem of noise-robustness that is
critical to real-world speech applications. We propose multi-condition
pre-training by perturbing the student's input with various types of additive
noise. We demonstrate that multi-condition pre-trained SPIRAL models are more
robust to noisy speech (9.0% - 13.3% relative word error rate reduction on real
noisy test data), compared to applying multi-condition training solely in the
fine-tuning stage. The code will be released after publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural-FST Class Language Model for End-to-End Speech Recognition. (arXiv:2201.11867v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11867">
<div class="article-summary-box-inner">
<span><p>We propose Neural-FST Class Language Model (NFCLM) for end-to-end speech
recognition, a novel method that combines neural network language models
(NNLMs) and finite state transducers (FSTs) in a mathematically consistent
framework. Our method utilizes a background NNLM which models generic
background text together with a collection of domain-specific entities modeled
as individual FSTs. Each output token is generated by a mixture of these
components; the mixture weights are estimated with a separately trained neural
decider. We show that NFCLM significantly outperforms NNLM by 15.8% relative in
terms of Word Error Rate. NFCLM achieves similar performance as traditional
NNLM and FST shallow fusion while being less prone to overbiasing and 12 times
more compact, making it more suitable for on-device usage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model. (arXiv:2201.11990v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11990">
<div class="article-summary-box-inner">
<span><p>Pretrained general-purpose language models can achieve state-of-the-art
accuracies in various natural language processing domains by adapting to
downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of
their success, the size of these models has increased rapidly, requiring
high-performance hardware, software, and algorithmic techniques to enable
training such large models. As the result of a joint effort between Microsoft
and NVIDIA, we present details on the training of the largest monolithic
transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530
billion parameters. In this paper, we first focus on the infrastructure as well
as the 3D parallelism methodology used to train this model using DeepSpeed and
Megatron. Next, we detail the training process, the design of our training
corpus, and our data curation techniques, which we believe is a key ingredient
to the success of the model. Finally, we discuss various evaluation results, as
well as other interesting observations and new properties exhibited by MT-NLG.
We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning
accuracies on several NLP benchmarks and establishes new state-of-the-art
results. We believe that our contributions will help further the development of
large-scale training infrastructures, large-scale language models, and natural
language generations.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">DiriNet: A network to estimate the spatial and spectral degradation functions. (arXiv:2201.12346v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12346">
<div class="article-summary-box-inner">
<span><p>The spatial and spectral degradation functions are critical to hyper- and
multi-spectral image fusion. However, few work has been payed on the estimation
of the degradation functions. To learn the spatial response function and the
point spread function from the image pairs to be fused, we propose a Dirichlet
network, where both functions are properly constrained. Specifically, the
spatial response function is constrained with positivity, while the Dirichlet
distribution along with a total variation is imposed on the point spread
function. To the best of our knowledge, the neural netwrok and the Dirichlet
regularization are exclusively investigated, for the first time, to estimate
the degradation functions. Both image degradation and fusion experiments
demonstrate the effectiveness and superiority of the proposed Dirichlet
network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-rank features based double transformation matrices learning for image classification. (arXiv:2201.12351v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12351">
<div class="article-summary-box-inner">
<span><p>Linear regression is a supervised method that has been widely used in
classification tasks. In order to apply linear regression to classification
tasks, a technique for relaxing regression targets was proposed. However,
methods based on this technique ignore the pressure on a single transformation
matrix due to the complex information contained in the data. A single
transformation matrix in this case is too strict to provide a flexible
projection, thus it is necessary to adopt relaxation on transformation matrix.
This paper proposes a double transformation matrices learning method based on
latent low-rank feature extraction. The core idea is to use double
transformation matrices for relaxation, and jointly projecting the learned
principal and salient features from two directions into the label space, which
can share the pressure of a single transformation matrix. Firstly, the low-rank
features are learned by the latent low rank representation (LatLRR) method
which processes the original data from two directions. In this process, sparse
noise is also separated, which alleviates its interference on projection
learning to some extent. Then, two transformation matrices are introduced to
process the two features separately, and the information useful for the
classification is extracted. Finally, the two transformation matrices can be
easily obtained by alternate optimization methods. Through such processing,
even when a large amount of redundant information is contained in samples, our
method can also obtain projection results that are easy to classify.
Experiments on multiple data sets demonstrate the effectiveness of our approach
for classification, especially for complex scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven's Progressive Matrices. (arXiv:2201.12382v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12382">
<div class="article-summary-box-inner">
<span><p>Abstract visual reasoning (AVR) domain encompasses problems solving which
requires the ability to reason about relations among entities present in a
given scene. While humans, generally, solve AVR tasks in a ``natural'' way,
even without prior experience, this type of problems has proven difficult for
current machine learning systems. The paper summarises recent progress in
applying deep learning methods to solving AVR problems, as a proxy for studying
machine intelligence. We focus on the most common type of AVR tasks -- the
Raven's Progressive Matrices (RPMs) -- and provide a comprehensive review of
the learning methods and deep neural models applied to solve RPMs, as well as,
the RPM benchmark sets. Performance analysis of the state-of-the-art approaches
to solving RPMs leads to formulation of certain insights and remarks on the
current and future trends in this area. We conclude the paper by demonstrating
how real-world problems can benefit from the discoveries of RPM studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Developing a Machine-Learning Algorithm to Diagnose Age-Related Macular Degeneration. (arXiv:2201.12384v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12384">
<div class="article-summary-box-inner">
<span><p>Today, more than 12 million people over the age of 40 suffer from ocular
diseases. Most commonly, older patients are susceptible to age related macular
degeneration, an eye disease that causes blurring of the central vision due to
the deterioration of the retina. The former can only be detected through
complex and expensive imaging software, markedly a visual field test; this
leaves a significant population with untreated eye disease and holds them at
risk for complete vision loss. The use of machine learning algorithms has been
proposed for treating eye disease. However, the development of these models is
limited by a lack of understanding regarding appropriate model and training
parameters to maximize model performance. In our study, we address these points
by generating 6 models, each with a learning rate of 1 * 10^n where n is 0, -1,
-2, ... -6, and calculated a f1 score for each of the models. Our analysis
shows that sample imbalance is a key challenge in training of machine learning
models and can result in deceptive improvements in training cost which does not
translate to true improvements in model predictive performance. Considering the
wide ranging impact of the disease and its adverse effects, we developed a
machine learning algorithm to treat the same. We trained our model on varying
eye disease datasets consisting of over 5000 patients, and the pictures of
their infected eyes. In the future, we hope this model is used extensively,
especially in areas that are under-resourced, to better diagnose eye disease
and improve well being for humanity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A deep Q-learning method for optimizing visual search strategies in backgrounds of dynamic noise. (arXiv:2201.12385v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12385">
<div class="article-summary-box-inner">
<span><p>Humans process visual information with varying resolution (foveated visual
system) and explore images by orienting through eye movements the
high-resolution fovea to points of interest. The Bayesian ideal searcher (IS)
that employs complete knowledge of task-relevant information optimizes eye
movement strategy and achieves the optimal search performance. The IS can be
employed as an important tool to evaluate the optimality of human eye
movements, and potentially provide guidance to improve human observer visual
search strategies. Najemnik and Geisler (2005) derived an IS for backgrounds of
spatial 1/f noise. The corresponding template responses follow Gaussian
distributions and the optimal search strategy can be analytically determined.
However, the computation of the IS can be intractable when considering more
realistic and complex backgrounds such as medical images. Modern reinforcement
learning methods, successfully applied to obtain optimal policy for a variety
of tasks, do not require complete knowledge of the background generating
functions and can be potentially applied to anatomical backgrounds. An
important first step is to validate the optimality of the reinforcement
learning method. In this study, we investigate the ability of a reinforcement
learning method that employs Q-network to approximate the IS. We demonstrate
that the search strategy corresponding to the Q-network is consistent with the
IS search strategy. The findings show the potential of the reinforcement
learning with Q-network approach to estimate optimal eye movement planning with
real anatomical backgrounds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Unsupervised Domain Adaptation for Multi-modal Cardiac Image Segmentation. (arXiv:2201.12386v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12386">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) methods intend to reduce the gap between
source and target domains by using unlabeled target domain and labeled source
domain data, however, in the medical domain, target domain data may not always
be easily available, and acquiring new samples is generally time-consuming.
This restricts the development of UDA methods for new domains. In this paper,
we explore the potential of UDA in a more challenging while realistic scenario
where only one unlabeled target patient sample is available. We call it
Few-shot Unsupervised Domain adaptation (FUDA). We first generate target-style
images from source images and explore diverse target styles from a single
target patient with Random Adaptive Instance Normalization (RAIN). Then, a
segmentation network is trained in a supervised manner with the generated
target images. Our experiments demonstrate that FUDA improves the segmentation
performance by 0.33 of Dice score on the target domain compared with the
baseline, and it also gives 0.28 of Dice score improvement in a more rigorous
one-shot setting. Our code is available at
\url{https://github.com/MingxuanGu/Few-shot-UDA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DoubleU-Net++: Architecture with Exploit Multiscale Features for Vertebrae Segmentation. (arXiv:2201.12389v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12389">
<div class="article-summary-box-inner">
<span><p>Accurate segmentation of the vertebra is an important prerequisite in various
medical applications (E.g. tele surgery) to assist surgeons. Following the
successful development of deep neural networks, recent studies have focused on
the essential rule of vertebral segmentation. Prior works contain a large
number of parameters, and their segmentation is restricted to only one view.
Inspired by DoubleU-Net, we propose a novel model named DoubleU-Net++ in which
DensNet as feature extractor, special attention module from Convolutional Block
Attention on Module (CBAM) and, Pyramid Squeeze Attention (PSA) module are
employed to improve extracted features. We evaluate our proposed model on three
different views (sagittal, coronal, and axial) of VerSe2020 and xVertSeg
datasets. Compared with state-of-the-art studies, our architecture is trained
faster and achieves higher precision, recall, and F1-score as evaluation
(imporoved by 4-6%) and the result of above 94% for sagittal view and above 94%
for both coronal view and above 93% axial view were gained for VerSe2020
dataset, respectively. Also, for xVertSeg dataset, we achieved precision,
recall,and F1-score of above 97% for sagittal view, above 93% for coronal view
,and above 96% for axial view.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Syfer: Neural Obfuscation for Private Data Release. (arXiv:2201.12406v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12406">
<div class="article-summary-box-inner">
<span><p>Balancing privacy and predictive utility remains a central challenge for
machine learning in healthcare. In this paper, we develop Syfer, a neural
obfuscation method to protect against re-identification attacks. Syfer composes
trained layers with random neural networks to encode the original data (e.g.
X-rays) while maintaining the ability to predict diagnoses from the encoded
data. The randomness in the encoder acts as the private key for the data owner.
We quantify privacy as the number of attacker guesses required to re-identify a
single image (guesswork). We propose a contrastive learning algorithm to
estimate guesswork. We show empirically that differentially private methods,
such as DP-Image, obtain privacy at a significant loss of utility. In contrast,
Syfer achieves strong privacy while preserving utility. For example, X-ray
classifiers built with DP-image, Syfer, and original data achieve average AUCs
of 0.53, 0.78, and 0.86, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoordX: Accelerating Implicit Neural Representation with a Split MLP Architecture. (arXiv:2201.12425v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12425">
<div class="article-summary-box-inner">
<span><p>Implicit neural representations with multi-layer perceptrons (MLPs) have
recently gained prominence for a wide variety of tasks such as novel view
synthesis and 3D object representation and rendering. However, a significant
challenge with these representations is that both training and inference with
an MLP over a large number of input coordinates to learn and represent an
image, video, or 3D object, require large amounts of computation and incur long
processing times. In this work, we aim to accelerate inference and training of
coordinate-based MLPs for implicit neural representations by proposing a new
split MLP architecture, CoordX. With CoordX, the initial layers are split to
learn each dimension of the input coordinates separately. The intermediate
features are then fused by the last layers to generate the learned signal at
the corresponding coordinate point. This significantly reduces the amount of
computation required and leads to large speedups in training and inference,
while achieving similar accuracy as the baseline MLP. This approach thus aims
at first learning functions that are a decomposition of the original signal and
then fusing them to generate the learned signal. Our proposed architecture can
be generally used for many implicit neural representation tasks with no
additional memory overheads. We demonstrate a speedup of up to 2.92x compared
to the baseline model for image, video, and 3D shape representation and
rendering tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Focused Few-Shot Object Detection for Robot Manipulation. (arXiv:2201.12437v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12437">
<div class="article-summary-box-inner">
<span><p>This paper addresses the problem of mobile robot manipulation of novel
objects via detection. Our approach uses vision and control as complementary
functions that learn from real-world tasks. We develop a manipulation method
based solely on detection then introduce task-focused few-shot object detection
to learn new objects and settings. The current paradigm for few-shot object
detection uses existing annotated examples. In contrast, we extend this
paradigm by using active data collection and annotation selection that improves
performance for specific downstream tasks (e.g., depth estimation and
grasping). In experiments for our interactive approach to few-shot learning, we
train a robot to manipulate objects directly from detection (ClickBot).
ClickBot learns visual servo control from a single click of annotation, grasps
novel objects in clutter and other settings, and achieves state-of-the-art
results on an existing visual servo control and depth estimation benchmark.
Finally, we establish a task-focused few-shot object detection benchmark to
support future research: https://github.com/griffbr/TFOD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters. (arXiv:2201.12467v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12467">
<div class="article-summary-box-inner">
<span><p>The growing public concerns on data privacy in face recognition can be
greatly addressed by the federated learning (FL) paradigm. However,
conventional FL methods perform poorly due to the uniqueness of the task:
broadcasting class centers among clients is crucial for recognition
performances but leads to privacy leakage. To resolve the privacy-utility
paradox, this work proposes PrivacyFace, a framework largely improves the
federated learning face recognition via communicating auxiliary and
privacy-agnostic information among clients. PrivacyFace mainly consists of two
components: First, a practical Differentially Private Local Clustering (DPLC)
mechanism is proposed to distill sanitized clusters from local class centers.
Second, a consensus-aware recognition loss subsequently encourages global
consensuses among clients, which ergo results in more discriminative features.
The proposed framework is mathematically proved to be differentially private,
introducing a lightweight overhead as well as yielding prominent performance
boosts (\textit{e.g.}, +9.63\% and +10.26\% for TAR@FAR=1e-4 on IJB-B and IJB-C
respectively). Extensive experiments and ablation studies on a large-scale
dataset have demonstrated the efficacy and practicability of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reconstruction of Power Lines from Point Clouds. (arXiv:2201.12499v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12499">
<div class="article-summary-box-inner">
<span><p>This paper proposes a novel solution for constructing line features modeling
each catenary curve present within a series of points representing multiple
catenary curves. The solution can be applied to extract power lines from lidar
point clouds, which can then be used in downstream applications like creating
digital twin geospatial models and evaluating the encroachment of vegetation.
This paper offers an example of how the results obtained by the proposed
solution could be used to assess vegetation growth near transmission power
lines based on freely available lidar data for the City of Utrecht, Netherlands
[1].
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">2D+3D facial expression recognition via embedded tensor manifold regularization. (arXiv:2201.12506v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12506">
<div class="article-summary-box-inner">
<span><p>In this paper, a novel approach via embedded tensor manifold regularization
for 2D+3D facial expression recognition (FERETMR) is proposed. Firstly, 3D
tensors are constructed from 2D face images and 3D face shape models to keep
the structural information and correlations. To maintain the local structure
(geometric information) of 3D tensor samples in the low-dimensional tensors
space during the dimensionality reduction, the $\ell_0$-norm of the core
tensors and a tensor manifold regularization scheme embedded on core tensors
are adopted via a low-rank truncated Tucker decomposition on the generated
tensors. As a result, the obtained factor matrices will be used for facial
expression classification prediction. To make the resulting tensor optimization
more tractable, $\ell_1$-norm surrogate is employed to relax $\ell_0$-norm and
hence the resulting tensor optimization problem has a nonsmooth objective
function due to the $\ell_1$-norm and orthogonal constraints from the
orthogonal Tucker decomposition. To efficiently tackle this tensor optimization
problem, we establish the first-order optimality condition in terms of
stationary points, and then design a block coordinate descent (BCD) algorithm
with convergence analysis and the computational complexity. Numerical results
on BU-3DFE database and Bosphorus databases demonstrate the effectiveness of
our proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spherical Convolution empowered FoV Prediction in 360-degree Video Multicast with Limited FoV Feedback. (arXiv:2201.12525v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12525">
<div class="article-summary-box-inner">
<span><p>Field of view (FoV) prediction is critical in 360-degree video multicast,
which is a key component of the emerging Virtual Reality (VR) and Augmented
Reality (AR) applications. Most of the current prediction methods combining
saliency detection and FoV information neither take into account that the
distortion of projected 360-degree videos can invalidate the weight sharing of
traditional convolutional networks, nor do they adequately consider the
difficulty of obtaining complete multi-user FoV information, which degrades the
prediction performance. This paper proposes a spherical convolution-empowered
FoV prediction method, which is a multi-source prediction framework combining
salient features extracted from 360-degree video with limited FoV feedback
information. A spherical convolution neural network (CNN) is used instead of a
traditional two-dimensional CNN to eliminate the problem of weight sharing
failure caused by video projection distortion. Specifically, salient
spatial-temporal features are extracted through a spherical convolution-based
saliency detection model, after which the limited feedback FoV information is
represented as a time-series model based on a spherical convolution-empowered
gated recurrent unit network. Finally, the extracted salient video features are
combined to predict future user FoVs. The experimental results show that the
performance of the proposed method is better than other prediction methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scale-Invariant Adversarial Attack for Evaluating and Enhancing Adversarial Defenses. (arXiv:2201.12527v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12527">
<div class="article-summary-box-inner">
<span><p>Efficient and effective attacks are crucial for reliable evaluation of
defenses, and also for developing robust models. Projected Gradient Descent
(PGD) attack has been demonstrated to be one of the most successful adversarial
attacks. However, the effect of the standard PGD attack can be easily weakened
by rescaling the logits, while the original decision of every input will not be
changed. To mitigate this issue, in this paper, we propose Scale-Invariant
Adversarial Attack (SI-PGD), which utilizes the angle between the features in
the penultimate layer and the weights in the softmax layer to guide the
generation of adversaries. The cosine angle matrix is used to learn angularly
discriminative representation and will not be changed with the rescaling of
logits, thus making SI-PGD attack to be stable and effective. We evaluate our
attack against multiple defenses and show improved performance when compared
with existing attacks. Further, we propose Scale-Invariant (SI) adversarial
defense mechanism based on the cosine angle matrix, which can be embedded into
the popular adversarial defenses. The experimental results show the defense
method with our SI mechanism achieves state-of-the-art performance among
multi-step and single-step defenses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SupWMA: Consistent and Efficient Tractography Parcellation of Superficial White Matter with Deep Learning. (arXiv:2201.12528v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12528">
<div class="article-summary-box-inner">
<span><p>White matter parcellation classifies tractography streamlines into clusters
or anatomically meaningful tracts to enable quantification and visualization.
Most parcellation methods focus on the deep white matter (DWM), while fewer
methods address the superficial white matter (SWM) due to its complexity. We
propose a deep-learning-based framework, Superficial White Matter Analysis
(SupWMA), that performs an efficient and consistent parcellation of 198 SWM
clusters from whole-brain tractography. A point-cloud-based network is modified
for our SWM parcellation task, and supervised contrastive learning enables more
discriminative representations between plausible streamlines and outliers. We
perform evaluation on a large tractography dataset with ground truth labels and
on three independently acquired testing datasets from individuals across ages
and health conditions. Compared to several state-of-the-art methods, SupWMA
obtains a highly consistent and accurate SWM parcellation result. In addition,
the computational speed of SupWMA is much faster than other methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Light field Rectification based on relative pose estimation. (arXiv:2201.12533v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12533">
<div class="article-summary-box-inner">
<span><p>Hand-held light field (LF) cameras have unique advantages in computer vision
such as 3D scene reconstruction and depth estimation. However, the related
applications are limited by the ultra-small baseline, e.g., leading to the
extremely low depth resolution in reconstruction. To solve this problem, we
propose to rectify LF to obtain a large baseline. Specifically, the proposed
method aligns two LFs captured by two hand-held LF cameras with a random
relative pose, and extracts the corresponding row-aligned sub-aperture images
(SAIs) to obtain an LF with a large baseline. For an accurate rectification, a
method for pose estimation is also proposed, where the relative rotation and
translation between the two LF cameras are estimated. The proposed pose
estimation minimizes the degree of freedom (DoF) in the LF-point-LF-point
correspondence model and explicitly solves this model in a linear way. The
proposed pose estimation outperforms the state-of-the-art algorithms by
providing more accurate results to support rectification. The significantly
improved depth resolution in 3D reconstruction demonstrates the effectiveness
of the proposed LF rectification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Differentiable Matrix Square Root and Inverse Square Root. (arXiv:2201.12543v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12543">
<div class="article-summary-box-inner">
<span><p>Computing the matrix square root and its inverse in a differentiable manner
is important in a variety of computer vision tasks. Previous methods either
adopt the Singular Value Decomposition (SVD) to explicitly factorize the matrix
or use the Newton-Schulz iteration (NS iteration) to derive the approximate
solution. However, both methods are not computationally efficient enough in
either the forward pass or the backward pass. In this paper, we propose two
more efficient variants to compute the differentiable matrix square root and
the inverse square root. For the forward propagation, one method is to use
Matrix Taylor Polynomial (MTP), and the other method is to use Matrix Pad\'e
Approximants (MPA). The backward gradient is computed by iteratively solving
the continuous-time Lyapunov equation using the matrix sign function. A series
of numerical tests show that both methods yield considerable speed-up compared
with the SVD or the NS iteration. Moreover, we validate the effectiveness of
our methods in several real-world applications, including de-correlated batch
normalization, second-order vision transformer, global covariance pooling for
large-scale and fine-grained recognition, attentive covariance pooling for
video recognition, and neural style transfer. The experimental results
demonstrate that our methods can also achieve competitive and even slightly
better performances. The Pytorch implementation is available at
\href{https://github.com/KingJamesSong/FastDifferentiableMatSqrt}{https://github.com/KingJamesSong/FastDifferentiableMatSqrt}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The KFIoU Loss for Rotated Object Detection. (arXiv:2201.12558v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12558">
<div class="article-summary-box-inner">
<span><p>Differing from the well-developed horizontal object detection area whereby
the computing-friendly IoU based loss is readily adopted and well fits with the
detection metrics. In contrast, rotation detectors often involve a more
complicated loss based on SkewIoU which is unfriendly to gradient-based
training. In this paper, we argue that one effective alternative is to devise
an approximate loss who can achieve trend-level alignment with SkewIoU loss
instead of the strict value-level identity. Specifically, we model the objects
as Gaussian distribution and adopt Kalman filter to inherently mimic the
mechanism of SkewIoU by its definition, and show its alignment with the SkewIoU
at trend-level. This is in contrast to recent Gaussian modeling based rotation
detectors e.g. GWD, KLD that involves a human-specified distribution distance
metric which requires additional hyperparameter tuning. The resulting new loss
called KFIoU is easier to implement and works better compared with exact
SkewIoU, thanks to its full differentiability and ability to handle the
non-overlapping cases. We further extend our technique to the 3-D case which
also suffers from the same issues as 2-D detection. Extensive results on
various public datasets (2-D/3-D, aerial/text/face images) with different base
detectors show the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Balanced Batch Normalization for Exemplar-based Class-Incremental Learning. (arXiv:2201.12559v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12559">
<div class="article-summary-box-inner">
<span><p>Batch Normalization (BN) is an essential layer for training neural network
models in various computer vision tasks. It has been widely used in continual
learning scenarios with little discussion, but we find that BN should be
carefully applied, particularly for the exemplar memory based class incremental
learning (CIL). We first analyze that the empirical mean and variance obtained
for normalization in a BN layer become highly biased toward the current task.
To tackle its significant problems in training and test phases, we propose
Task-Balanced Batch Normalization (TBBN). Given each mini-batch imbalanced
between the current and previous tasks, TBBN first reshapes and repeats the
batch, calculating near task-balanced mean and variance. Second, we show that
when the affine transformation parameters of BN are learned from a reshaped
feature map, they become less-biased toward the current task. Based on our
extensive CIL experiments with CIFAR-100 and ImageNet-100 datasets, we
demonstrate that our TBBN is easily applicable to most of existing
exemplar-based CIL algorithms, improving their performance by decreasing the
forgetting on the previous tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scale-arbitrary Invertible Image Downscaling. (arXiv:2201.12576v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12576">
<div class="article-summary-box-inner">
<span><p>Downscaling is indispensable when distributing high-resolution (HR) images
over the Internet to fit the displays of various resolutions, while upscaling
is also necessary when users want to see details of the distributed images.
Recent invertible image downscaling methods jointly model these two problems
and achieve significant improvements. However, they only consider fixed integer
scale factors that cannot meet the requirement of conveniently fitting the
displays of various resolutions in real-world applications. In this paper, we
propose a scale-Arbitrary Invertible image Downscaling Network (AIDN), to
natively downscale HR images with arbitrary scale factors for fitting various
target resolutions. Meanwhile, the HR information is embedded in the downscaled
low-resolution (LR) counterparts in a nearly imperceptible form such that our
AIDN can also restore the original HR images solely from the LR images. The key
to supporting arbitrary scale factors is our proposed Conditional Resampling
Module (CRM) that conditions the downscaling/upscaling kernels and sampling
locations on both scale factors and image content. Extensive experimental
results demonstrate that our AIDN achieves top performance for invertible
downscaling with both arbitrary integer and non-integer scale factors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Matrix-Encoding Method for Privacy-Preserving Neural Networks (Inference). (arXiv:2201.12577v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12577">
<div class="article-summary-box-inner">
<span><p>In this work, we present $\texttt{Volley Revolver}$, a novel matrix-encoding
method that is particularly convenient for privacy-preserving neural networks
to make predictions, and use it to implement a CNN for handwritten image
classification. Based on this encoding method, we develop several additional
operations for putting into practice the secure matrix multiplication over
encrypted data matrices. For two matrices $A$ and $B$ to perform multiplication
$A \times B$, the main idea is, in a simple version, to encrypt matrix $A$ and
the transposition of the matrix $B$ into two ciphertexts respectively. Along
with the additional operations, the homomorphic matrix multiplication $A \times
B$ can be calculated over encrypted data matrices efficiently. For the
convolution operation in CNN, on the basis of the $\texttt{Volley Revolver}$
encoding method, we develop a feasible and efficient evaluation strategy for
performing the convolution operation. We in advance span each convolution
kernel of CNN to a matrix space of the same size as the input image so as to
generate several ciphertexts, each of which is later used together with the
input image for calculating some part of the final convolution result. We
accumulate all these part results of convolution operation and thus obtain the
final convolution result.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedMed-ATL: Misaligned Unpaired Brain Image Synthesis via Affine Transform Loss. (arXiv:2201.12589v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12589">
<div class="article-summary-box-inner">
<span><p>The existence of completely aligned and paired multi-modal neuroimaging data
has proved its effectiveness in the diagnosis of brain diseases. However,
collecting the full set of well-aligned and paired data is impractical or even
luxurious, since the practical difficulties may include high cost, long time
acquisition, image corruption, and privacy issues. Previously, the misaligned
unpaired neuroimaging data (termed as MUD) are generally treated as noisy
label. However, such a noisy label-based method could not work very well when
misaligned data occurs distortions severely, for example, different angles of
rotation. In this paper, we propose a novel federated self-supervised learning
(FedMed) for brain image synthesis. An affine transform loss (ATL) was
formulated to make use of severely distorted images without violating privacy
legislation for the hospital. We then introduce a new data augmentation
procedure for self-supervised training and fed it into three auxiliary heads,
namely auxiliary rotation, auxiliary translation, and auxiliary scaling heads.
The proposed method demonstrates advanced performance in both the quality of
synthesized results under a severely misaligned and unpaired data setting, and
better stability than other GAN-based algorithms. The proposed method also
reduces the demand for deformable registration while encouraging to realize the
usage of those misaligned and unpaired data. Experimental results verify the
outstanding ability of our learning paradigm compared to other state-of-the-art
approaches. Our code is available on the website:
https://github.com/FedMed-Meta/FedMed-ATL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exact Decomposition of Joint Low Rankness and Local Smoothness Plus Sparse Matrices. (arXiv:2201.12592v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12592">
<div class="article-summary-box-inner">
<span><p>It is known that the decomposition in low-rank and sparse matrices
(\textbf{L+S} for short) can be achieved by several Robust PCA techniques.
Besides the low rankness, the local smoothness (\textbf{LSS}) is a vitally
essential prior for many real-world matrix data such as hyperspectral images
and surveillance videos, which makes such matrices have low-rankness and local
smoothness properties at the same time. This poses an interesting question: Can
we make a matrix decomposition in terms of \textbf{L\&amp;LSS +S } form exactly? To
address this issue, we propose in this paper a new RPCA model based on
three-dimensional correlated total variation regularization (3DCTV-RPCA for
short) by fully exploiting and encoding the prior expression underlying such
joint low-rank and local smoothness matrices. Specifically, using a
modification of Golfing scheme, we prove that under some mild assumptions, the
proposed 3DCTV-RPCA model can decompose both components exactly, which should
be the first theoretical guarantee among all such related methods combining low
rankness and local smoothness. In addition, by utilizing Fast Fourier Transform
(FFT), we propose an efficient ADMM algorithm with a solid convergence
guarantee for solving the resulting optimization problem. Finally, a series of
experiments on both simulations and real applications are carried out to
demonstrate the general validity of the proposed 3DCTV-RPCA model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MVP: Multi-Stage Vision-Language Pre-Training via Multi-Level Semantic Alignment. (arXiv:2201.12596v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12596">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a Multi-stage Vision-language Pre-training (MVP)
framework to learn cross-modality representation via multi-level semantic
alignment. We introduce concepts in both modalities to construct two-level
semantic representations for language and vision. Based on the multi-level
input, we train the cross-modality model in two stages, namely, uni-modal
learning and cross-modal learning. The former stage enforces within-modality
interactions to learn multi-level semantics for each single modality. The
latter stage enforces interactions across modalities via both coarse-grain and
fine-grain semantic alignment tasks. Image-text matching and masked language
modeling are then used to further optimize the pre-training model. Our model
generates the-state-of-the-art results on several vision and language tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-assisted image compression. (arXiv:2201.12599v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12599">
<div class="article-summary-box-inner">
<span><p>Conventional image compression methods typically aim at pixel-level
consistency while ignoring the performance of downstream AI tasks.To solve this
problem, this paper proposes a Semantic-Assisted Image Compression method
(SAIC), which can maintain semantic-level consistency to enable high
performance of downstream AI tasks.To this end, we train the compression
network using semantic-level loss function. In particular, semantic-level loss
is measured using gradient-based semantic weights mechanism (GSW). GSW directly
consider downstream AI tasks' perceptual results. Then, this paper proposes a
semantic-level distortion evaluation metric to quantify the amount of semantic
information retained during the compression process. Experimental results show
that the proposed SAIC method can retain more semantic-level information and
achieve better performance of downstream AI tasks compared to the traditional
deep learning-based method and the advanced perceptual method at the same
compression ratio.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System. (arXiv:2201.12604v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12604">
<div class="article-summary-box-inner">
<span><p>Humans excel at continually learning from an ever-changing environment
whereas it remains a challenge for deep neural networks which exhibit
catastrophic forgetting. The complementary learning system (CLS) theory
suggests that the interplay between rapid instance-based learning and slow
structured learning in the brain is crucial for accumulating and retaining
knowledge. Here, we propose CLS-ER, a novel dual memory experience replay (ER)
method which maintains short-term and long-term semantic memories that interact
with the episodic memory. Our method employs an effective replay mechanism
whereby new knowledge is acquired while aligning the decision boundaries with
the semantic memories. CLS-ER does not utilize the task boundaries or make any
assumption about the distribution of the data which makes it versatile and
suited for "general continual learning". Our approach achieves state-of-the-art
performance on standard benchmarks as well as more realistic general continual
learning settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hand Gesture Recognition of Dumb Person Using one Against All Neural Network. (arXiv:2201.12622v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12622">
<div class="article-summary-box-inner">
<span><p>We propose a new technique for recognition of dumb person hand gesture in
real world environment. In this technique, the hand image containing the
gesture is preprocessed and then hand region is segmented by convergent the RGB
color image to L.a.b color space. Only few statistical features are used to
classify the segmented image to different classes. Artificial Neural Network is
trained in sequential manner using one against all. When the system gets
trained, it becomes capable of recognition of each class in parallel manner.
The result of proposed technique is much better than existing techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADC-Net: An Open-Source Deep Learning Network for Automated Dispersion Compensation in Optical Coherence Tomography. (arXiv:2201.12625v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12625">
<div class="article-summary-box-inner">
<span><p>Chromatic dispersion is a common problem to degrade the system resolution in
optical coherence tomography (OCT). This study is to develop a deep learning
network for automated dispersion compensation (ADC-Net) in OCT. The ADC-Net is
based on a redesigned UNet architecture which employs an encoder-decoder
pipeline. The input section encompasses partially compensated OCT B-scans with
individual retinal layers optimized. Corresponding output is a fully
compensated OCT B-scans with all retinal layers optimized. Two numeric
parameters, i.e., peak signal to noise ratio (PSNR) and structural similarity
index metric computed at multiple scales (MS-SSIM), were used for objective
assessment of the ADC-Net performance. Comparative analysis of training models,
including single, three, five, seven and nine input channels were implemented.
The five-input channels implementation was observed as the optimal mode for
ADC-Net training to achieve robust dispersion compensation in OCT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing Cross-dataset Generalization of Pedestrian Crossing Predictors. (arXiv:2201.12626v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12626">
<div class="article-summary-box-inner">
<span><p>Pedestrian crossing prediction has been a topic of active research, resulting
in many new algorithmic solutions. While measuring the overall progress of
those solutions over time tends to be more and more established due to the new
publicly available benchmark and standardized evaluation procedures, knowing
how well existing predictors react to unseen data remains an unanswered
question. This evaluation is imperative as serviceable crossing behavior
predictors should be set to work in various scenarii without compromising
pedestrian safety due to misprediction. To this end, we conduct a study based
on direct cross-dataset evaluation. Our experiments show that current
state-of-the-art pedestrian behavior predictors generalize poorly in
cross-dataset evaluation scenarii, regardless of their robustness during a
direct training-test set evaluation setting. In the light of what we observe,
we argue that the future of pedestrian crossing prediction, e.g. reliable and
generalizable implementations, should not be about tailoring models, trained
with very little available data, and tested in a classical train-test scenario
with the will to infer anything about their behavior in real life. It should be
about evaluating models in a cross-dataset setting while considering their
uncertainty estimates under domain shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Classification using Graph Neural Network and Multiscale Wavelet Superpixels. (arXiv:2201.12633v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12633">
<div class="article-summary-box-inner">
<span><p>Prior studies using graph neural networks (GNNs) for image classification
have focused on graphs generated from a regular grid of pixels or similar-sized
superpixels. In the latter, a single target number of superpixels is defined
for an entire dataset irrespective of differences across images and their
intrinsic multiscale structure. On the contrary, this study investigates image
classification using graphs generated from an image-specific number of
multiscale superpixels. We propose WaveMesh, a new wavelet-based superpixeling
algorithm, where the number and sizes of superpixels in an image are
systematically computed based on its content. WaveMesh superpixel graphs are
structurally different from similar-sized superpixel graphs. We use SplineCNN,
a state-of-the-art network for image graph classification, to compare WaveMesh
and similar-sized superpixels. Using SplineCNN, we perform extensive
experiments on three benchmark datasets under three local-pooling settings: 1)
no pooling, 2) GraclusPool, and 3) WavePool, a novel spatially heterogeneous
pooling scheme tailored to WaveMesh superpixels. Our experiments demonstrate
that SplineCNN learns from multiscale WaveMesh superpixels on-par with
similar-sized superpixels. In all WaveMesh experiments, GraclusPool performs
poorer than no pooling / WavePool, indicating that poor choice of pooling can
result in inferior performance while learning from multiscale superpixels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self Semi Supervised Neural Architecture Search for Semantic Segmentation. (arXiv:2201.12646v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12646">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a Neural Architecture Search strategy based on self
supervision and semi-supervised learning for the task of semantic segmentation.
Our approach builds an optimized neural network (NN) model for this task by
jointly solving a jigsaw pretext task discovered with self-supervised learning
over unlabeled training data, and, exploiting the structure of the unlabeled
data with semi-supervised learning. The search of the architecture of the NN
model is performed by dynamic routing using a gradient descent algorithm.
Experiments on the Cityscapes and PASCAL VOC 2012 datasets demonstrate that the
discovered neural network is more efficient than a state-of-the-art
hand-crafted NN model with four times less floating operations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer Learning for Estimation of Pendubot Angular Position Using Deep Neural Networks. (arXiv:2201.12649v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12649">
<div class="article-summary-box-inner">
<span><p>In this paper, a machine learning based approach is introduced to estimate
Pendubot angular position from its captured images. Initially, a baseline
algorithm is introduced to estimate the angle using conventional image
processing technique. The baseline algorithm performs well for the cases that
the Pendubot is not moving fast. However, when moving quickly due to a free
fall, the Pendubot appears as a blurred object in the captured image in a way
that the baseline algorithm fails to estimate the angle. Consequently, a Deep
Neural Network (DNN) based algorithm is introduced to cope with this challenge.
The approach relies on the concept of transfer learning to allow the training
of the DNN on a very small fine-tuning dataset. The base algorithm is used to
create the ground truth labels of the fine-tuning dataset. Experimental results
on the held-out evaluation set show that the proposed approach achieves a
median absolute error of 0.02 and 0.06 degrees for the sharp and blurry images
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Stochastic Bundle Method for Interpolating Networks. (arXiv:2201.12678v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12678">
<div class="article-summary-box-inner">
<span><p>We propose a novel method for training deep neural networks that are capable
of interpolation, that is, driving the empirical loss to zero. At each
iteration, our method constructs a stochastic approximation of the learning
objective. The approximation, known as a bundle, is a pointwise maximum of
linear functions. Our bundle contains a constant function that lower bounds the
empirical loss. This enables us to compute an automatic adaptive learning rate,
thereby providing an accurate solution. In addition, our bundle includes linear
approximations computed at the current iterate and other linear estimates of
the DNN parameters. The use of these additional approximations makes our method
significantly more robust to its hyperparameters. Based on its desirable
empirical properties, we term our method Bundle Optimisation for Robust and
Accurate Training (BORAT). In order to operationalise BORAT, we design a novel
algorithm for optimising the bundle approximation efficiently at each
iteration. We establish the theoretical convergence of BORAT in both convex and
non-convex settings. Using standard publicly available data sets, we provide a
thorough comparison of BORAT to other single hyperparameter optimisation
algorithms. Our experiments demonstrate BORAT matches the state-of-the-art
generalisation performance for these methods and is the most robust.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Contrastive Learning is Provably (almost) Principal Component Analysis. (arXiv:2201.12680v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12680">
<div class="article-summary-box-inner">
<span><p>We show that Contrastive Learning (CL) under a family of loss functions
(including InfoNCE) has a game-theoretical formulation, where the \emph{max
player} finds representation to maximize contrastiveness, and the \emph{min
player} puts weights on pairs of samples with similar representation. We show
that the max player who does \emph{representation learning} reduces to
Principal Component Analysis for deep linear network, and almost all local
minima are global, recovering optimal PCA solutions. Experiments show that the
formulation yields comparable (or better) performance on CIFAR10 and STL-10
when extending beyond InfoNCE, yielding novel contrastive losses. Furthermore,
we extend our theoretical analysis to 2-layer ReLU networks, showing its
difference from linear ones, and proving that feature composition is preferred
over picking single dominant feature under strong augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Built Environment Features for Planning Research with Computer Vision: A Review and Discussion of State-of-the-Art Approaches. (arXiv:2201.12693v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12693">
<div class="article-summary-box-inner">
<span><p>This is an extended abstract for a presentation at The 17th International
Conference on CUPUM - Computational Urban Planning and Urban Management in June
2021. This study presents an interdisciplinary synthesis of the
state-of-the-art approaches in computer vision technologies to extract built
environment features that could improve the robustness of empirical research in
planning. We discussed the findings from the review of studies in both planning
and computer science.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Robust Framework for Deep Learning Approaches to Facial Emotion Recognition and Evaluation. (arXiv:2201.12705v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12705">
<div class="article-summary-box-inner">
<span><p>Facial emotion recognition is a vast and complex problem space within the
domain of computer vision and thus requires a universally accepted baseline
method with which to evaluate proposed models. While test datasets have served
this purpose in the academic sphere real world application and testing of such
models lacks any real comparison. Therefore we propose a framework in which
models developed for FER can be compared and contrasted against one another in
a constant standardized fashion. A lightweight convolutional neural network is
trained on the AffectNet dataset a large variable dataset for facial emotion
recognition and a web application is developed and deployed with our proposed
framework as a proof of concept. The CNN is embedded into our application and
is capable of instant real time facial emotion recognition. When tested on the
AffectNet test set this model achieves high accuracy for emotion classification
of eight different emotions. Using our framework the validity of this model and
others can be properly tested by evaluating a model efficacy not only based on
its accuracy on a sample test dataset, but also on in the wild experiments.
Additionally, our application is built with the ability to save and store any
image captured or uploaded to it for emotion recognition, allowing for the
curation of more quality and diverse facial emotion recognition datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tensor Recovery Based on Tensor Equivalent Minimax-Concave Penalty. (arXiv:2201.12709v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12709">
<div class="article-summary-box-inner">
<span><p>Tensor recovery is an important problem in computer vision and machine
learning. It usually uses the convex relaxation of tensor rank and $l_{0}$
norm, i.e., the nuclear norm and $l_{1}$ norm respectively, to solve the
problem. It is well known that convex approximations produce biased estimators.
In order to overcome this problem, a corresponding non-convex regularizer has
been proposed to solve it. Inspired by matrix equivalent Minimax-Concave
Penalty (EMCP), we propose and prove theorems of tensor equivalent
Minimax-Concave Penalty (TEMCP). The tensor equivalent MCP (TEMCP) as a
non-convex regularizer and the equivalent weighted tensor $\gamma$ norm (EWTGN)
which can represent the low-rank part are obtained. Both of them can realize
weight adaptive. At the same time, we propose two corresponding adaptive models
for two classical tensor recovery problems, low-rank tensor completion (LRTC)
and tensor robust principal component analysis (TRPCA), and the optimization
algorithm is based on alternating direction multiplier (ADMM). This novel
iterative adaptive algorithm can produce more accurate tensor recovery effect.
For the tensor completion model, multispectral image (MSI), magnetic resonance
imaging (MRI) and color video (CV) data sets are considered, while for the
tensor robust principal component analysis model, hyperspectral image (HSI)
denoising under gaussian noise plus salt and pepper noise is considered. The
proposed algorithm is superior to the state-of-arts method, and the algorithm
is guaranteed to meet the reduction and convergence through experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Win the Lottery Ticket via Fourier Analysis: Frequencies Guided Network Pruning. (arXiv:2201.12712v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12712">
<div class="article-summary-box-inner">
<span><p>With the remarkable success of deep learning recently, efficient network
compression algorithms are urgently demanded for releasing the potential
computational power of edge devices, such as smartphones or tablets. However,
optimal network pruning is a non-trivial task which mathematically is an
NP-hard problem. Previous researchers explain training a pruned network as
buying a lottery ticket. In this paper, we investigate the Magnitude-Based
Pruning (MBP) scheme and analyze it from a novel perspective through Fourier
analysis on the deep learning model to guide model designation. Besides
explaining the generalization ability of MBP using Fourier transform, we also
propose a novel two-stage pruning approach, where one stage is to obtain the
topological structure of the pruned network and the other stage is to retrain
the pruned network to recover the capacity using knowledge distillation from
lower to higher on the frequency domain. Extensive experiments on CIFAR-10 and
CIFAR-100 demonstrate the superiority of our novel Fourier analysis based MBP
compared to other traditional MBP algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Only Demonstrate Once: Category-Level Manipulation from Single Visual Demonstration. (arXiv:2201.12716v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12716">
<div class="article-summary-box-inner">
<span><p>Promising results have been achieved recently in category-level manipulation
that generalizes across object instances. Nevertheless, it often requires
expensive real-world data collection and manual specification of semantic
keypoints for each object category and task. Additionally, coarse keypoint
predictions and ignoring intermediate action sequences hinder adoption in
complex manipulation tasks beyond pick-and-place. This work proposes a novel,
category-level manipulation framework that leverages an object-centric,
category-level representation and model-free 6 DoF motion tracking. The
canonical object representation is learned solely in simulation and then used
to parse a category-level, task trajectory from a single demonstration video.
The demonstration is reprojected to a target trajectory tailored to a novel
object via the canonical representation. During execution, the manipulation
horizon is decomposed into long-range, collision-free motion and last-inch
manipulation. For the latter part, a category-level behavior cloning (CatBC)
method leverages motion tracking to perform closed-loop control. CatBC follows
the target trajectory, projected from the demonstration and anchored to a
dynamically selected category-level coordinate frame. The frame is
automatically selected along the manipulation horizon by a local attention
mechanism. This framework allows to teach different manipulation strategies by
solely providing a single demonstration, without complicated manual
programming. Extensive experiments demonstrate its efficacy in a range of
challenging industrial tasks in high-precision assembly, which involve learning
complex, long-horizon policies. The process exhibits robustness against
uncertainty due to dynamics as well as generalization across object instances
and scene configurations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VC-GPT: Visual Conditioned GPT for End-to-End Generative Vision-and-Language Pre-training. (arXiv:2201.12723v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12723">
<div class="article-summary-box-inner">
<span><p>Vision-and-language pre-training models (VLMs) have achieved tremendous
success in the cross-modal area, but most of them require millions of parallel
image-caption data for pre-training. Collating such data is expensive and
labor-intensive. In this work, we focus on reducing such need for generative
vision-and-language pre-training (G-VLP) by taking advantage of the visual
pre-trained model (CLIP-ViT) as encoder and language pre-trained model (GPT2)
as decoder. Unfortunately, GPT2 lacks a necessary cross-attention module, which
hinders the direct connection of CLIP-ViT and GPT2. To remedy such defects, we
conduct extensive experiments to empirically investigate how to design and
pre-train our model. Based on our experimental results, we propose a novel
G-VLP framework, Visual Conditioned GPT (VC-GPT), and pre-train it with a
small-scale parallel image-caption corpus (Visual Genome, only 110k distinct
images). Evaluating on the image captioning downstream tasks (MSCOCO and
Flickr30k Captioning), VC-GPT achieves either the best or the second-best
performance across all evaluation metrics over the previous works which consume
around 30 times more parallel data during pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Architecture Ranker. (arXiv:2201.12725v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12725">
<div class="article-summary-box-inner">
<span><p>Architecture ranking has recently been advocated to design an efficient and
effective performance predictor for Neural Architecture Search (NAS). The
previous contrastive method solves the ranking problem by comparing pairs of
architectures and predicting their relative performance, which may suffer
generalization issues due to local pair-wise comparison. Inspired by the
quality stratification phenomenon in the search space, we propose a predictor,
namely Neural Architecture Ranker (NAR), from a new and global perspective by
exploiting the quality distribution of the whole search space. The NAR learns
the similar characteristics of the same quality tier (i.e., level) and
distinguishes among different individuals by first matching architectures with
the representation of tiers, and then classifying and scoring them. It can
capture the features of different quality tiers and thus generalize its ranking
ability to the entire search space. Besides, distributions of different quality
tiers are also beneficial to guide the sampling procedure, which is free of
training a search algorithm and thus simplifies the NAS pipeline. The proposed
NAR achieves better performance than the state-of-the-art methods on two widely
accepted datasets. On NAS-Bench-101, it finds the architectures with top
0.01$\unicode{x2030}$ performance among the search space and stably focuses on
the top architectures. On NAS-Bench-201, it identifies the optimal
architectures on CIFAR-10, CIFAR-100 and, ImageNet-16-120. We expand and
release these two datasets covering detailed cell computational information to
boost the study of NAS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video-based Facial Micro-Expression Analysis: A Survey of Datasets, Features and Algorithms. (arXiv:2201.12728v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12728">
<div class="article-summary-box-inner">
<span><p>Unlike the conventional facial expressions, micro-expressions are involuntary
and transient facial expressions capable of revealing the genuine emotions that
people attempt to hide. Therefore, they can provide important information in a
broad range of applications such as lie detection, criminal detection, etc.
Since micro-expressions are transient and of low intensity, however, their
detection and recognition is difficult and relies heavily on expert
experiences. Due to its intrinsic particularity and complexity, video-based
micro-expression analysis is attractive but challenging, and has recently
become an active area of research. Although there have been numerous
developments in this area, thus far there has been no comprehensive survey that
provides researchers with a systematic overview of these developments with a
unified evaluation. Accordingly, in this survey paper, we first highlight the
key differences between macro- and micro-expressions, then use these
differences to guide our research survey of video-based micro-expression
analysis in a cascaded structure, encompassing the neuropsychological basis,
datasets, features, spotting algorithms, recognition algorithms, applications
and evaluation of state-of-the-art approaches. For each aspect, the basic
techniques, advanced developments and major challenges are addressed and
discussed. Furthermore, after considering the limitations of existing
micro-expression datasets, we present and release a new dataset - called
micro-and-macro expression warehouse (MMEW) - containing more video samples and
more labeled emotion types. We then perform a unified comparison of
representative methods on CAS(ME)2 for spotting, and on MMEW and SAMM for
recognition, respectively. Finally, some potential future research directions
are explored and outlined.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TPC: Transformation-Specific Smoothing for Point Cloud Models. (arXiv:2201.12733v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12733">
<div class="article-summary-box-inner">
<span><p>Point cloud models with neural network architectures have achieved great
success and have been widely used in safety-critical applications, such as
Lidar-based recognition systems in autonomous vehicles. However, such models
are shown vulnerable against adversarial attacks which aim to apply stealthy
semantic transformations such as rotation and tapering to mislead model
predictions. In this paper, we propose a transformation-specific smoothing
framework TPC, which provides tight and scalable robustness guarantees for
point cloud models against semantic transformation attacks. We first categorize
common 3D transformations into three categories: additive (e.g., shearing),
composable (e.g., rotation), and indirectly composable (e.g., tapering), and we
present generic robustness certification strategies for all categories
respectively. We then specify unique certification protocols for a range of
specific semantic transformations and their compositions. Extensive experiments
on several common 3D transformations show that TPC significantly outperforms
the state of the art. For example, our framework boosts the certified accuracy
against twisting transformation along z-axis (within 20$^\circ$) from 20.3$\%$
to 83.8$\%$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RIM-Net: Recursive Implicit Fields for Unsupervised Learning of Hierarchical Shape Structures. (arXiv:2201.12763v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12763">
<div class="article-summary-box-inner">
<span><p>We introduce RIM-Net, a neural network which learns recursive implicit fields
for unsupervised inference of hierarchical shape structures. Our network
recursively decomposes an input 3D shape into two parts, resulting in a binary
tree hierarchy. Each level of the tree corresponds to an assembly of shape
parts, represented as implicit functions, to reconstruct the input shape. At
each node of the tree, simultaneous feature decoding and shape decomposition
are carried out by their respective feature and part decoders, with weight
sharing across the same hierarchy level. As an implicit field decoder, the part
decoder is designed to decompose a sub-shape, via a two-way branched
reconstruction, where each branch predicts a set of parameters defining a
Gaussian to serve as a local point distribution for shape reconstruction. With
reconstruction losses accounted for at each hierarchy level and a decomposition
loss at each node, our network training does not require any ground-truth
segmentations, let alone hierarchies. Through extensive experiments and
comparisons to state-of-the-art alternatives, we demonstrate the quality,
consistency, and interpretability of hierarchical structural inference by
RIM-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Corruption and Adversarial Robustness by Enhancing Weak Subnets. (arXiv:2201.12765v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12765">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have achieved great success in many computer vision
tasks. However, deep networks have been shown to be very susceptible to
corrupted or adversarial images, which often result in significant performance
drops. In this paper, we observe that weak subnetwork (subnet) performance is
correlated with a lack of robustness against corruptions and adversarial
attacks. Based on that observation, we propose a novel robust training method
which explicitly identifies and enhances weak subnets (EWS) during training to
improve robustness. Specifically, we develop a search algorithm to find
particularly weak subnets and propose to explicitly strengthen them via
knowledge distillation from the full network. We show that our EWS greatly
improves the robustness against corrupted images as well as the accuracy on
clean data. Being complementary to many state-of-the-art data augmentation
approaches, EWS consistently improves corruption robustness on top of many of
these approaches. Moreover, EWS is also able to boost the adversarial
robustness when combined with popular adversarial training methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MVP-Net: Multiple View Pointwise Semantic Segmentation of Large-Scale Point Clouds. (arXiv:2201.12769v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12769">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation of 3D point cloud is an essential task for autonomous
driving environment perception. The pipeline of most pointwise point cloud
semantic segmentation methods includes points sampling, neighbor searching,
feature aggregation, and classification. Neighbor searching method like
K-nearest neighbors algorithm, KNN, has been widely applied. However, the
complexity of KNN is always a bottleneck of efficiency. In this paper, we
propose an end-to-end neural architecture, Multiple View Pointwise Net,
MVP-Net, to efficiently and directly infer large-scale outdoor point cloud
without KNN or any complex pre/postprocessing. Instead, assumption-based
sorting and multi-rotation of point cloud methods are introduced to point
feature aggregation and receptive field expanding. Numerical experiments show
that the proposed MVP-Net is 11 times faster than the most efficient pointwise
semantic segmentation method RandLA-Net and achieves the same accuracy on the
large-scale benchmark SemanticKITTI dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Moving Vehicle Detection from Audio-Visual Cues. (arXiv:2201.12771v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12771">
<div class="article-summary-box-inner">
<span><p>Robust detection of moving vehicles is a critical task for any autonomously
operating outdoor robot or self-driving vehicle. Most modern approaches for
solving this task rely on training image-based detectors using large-scale
vehicle detection datasets such as nuScenes or the Waymo Open Dataset.
Providing manual annotations is an expensive and laborious exercise that does
not scale well in practice. To tackle this problem, we propose a
self-supervised approach that leverages audio-visual cues to detect moving
vehicles in videos. Our approach employs contrastive learning for localizing
vehicles in images from corresponding pairs of images and recorded audio. In
extensive experiments carried out with a real-world dataset, we demonstrate
that our approach provides accurate detections of moving vehicles and does not
require manual annotations. We furthermore show that our model can be used as a
teacher to supervise an audio-only detection model. This student model is
invariant to illumination changes and thus effectively bridges the domain gap
inherent to models leveraging exclusively vision as the predominant modality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Practical Noise Simulation for RGB Images. (arXiv:2201.12773v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12773">
<div class="article-summary-box-inner">
<span><p>This document describes a noise generator that simulates realistic noise
found in smartphone cameras. The generator simulates Poissonian-Gaussian noise
whose parameters have been estimated on the Smartphone Image Denoising Dataset
(SIDD). The generator is available online, and is currently being used in
compressed-domain denoising exploration experiments in JPEG AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransBTSV2: Wider Instead of Deeper Transformer for Medical Image Segmentation. (arXiv:2201.12785v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12785">
<div class="article-summary-box-inner">
<span><p>Transformer, benefiting from global (long-range) information modeling using
self-attention mechanism, has been successful in natural language processing
and computer vision recently. Convolutional Neural Networks, capable of
capturing local features, are unable to model explicit long-distance
dependencies from global feature space. However, both local and global features
are crucial for dense prediction tasks, especially for 3D medical image
segmentation. In this paper, we exploit Transformer in 3D CNN for 3D medical
image volumetric segmentation and propose a novel network named TransBTSV2
based on the encoder-decoder structure. Different from our original TransBTS,
the proposed TransBTSV2 is not limited to brain tumor segmentation (BTS) but
focuses on general medical image segmentation, providing a strong and efficient
3D baseline for volumetric segmentation of medical images. As a hybrid
CNN-Transformer architecture, TransBTSV2 can achieve accurate segmentation of
medical images without any pre-training. With the proposed insight to redesign
the internal structure of Transformer and the introduced Deformable Bottleneck
Module, a highly efficient architecture is achieved with superior performance.
Extensive experimental results on four medical image datasets (BraTS 2019,
BraTS 2020, LiTS 2017 and KiTS 2019) demonstrate that TransBTSV2 achieves
comparable or better results as compared to the state-of-the-art methods for
the segmentation of brain tumor, liver tumor as well as kidney tumor. Code is
available at https://github.com/Wenxuan-1119/TransBTS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SelfRecon: Self Reconstruction Your Digital Avatar from Monocular Video. (arXiv:2201.12792v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12792">
<div class="article-summary-box-inner">
<span><p>We propose SelfRecon, a clothed human body reconstruction method that
combines implicit and explicit representations to recover space-time coherent
geometries from a monocular self-rotating human video. Explicit methods require
a predefined template mesh for a given sequence, while the template is hard to
acquire for a specific subject. Meanwhile, the fixed topology limits the
reconstruction accuracy and clothing types. Implicit methods support arbitrary
topology and have high quality due to continuous geometric representation.
However, it is difficult to integrate multi-frame information to produce a
consistent registration sequence for downstream applications. We propose to
combine the advantages of both representations. We utilize differential mask
loss of the explicit mesh to obtain the coherent overall shape, while the
details on the implicit surface are refined with the differentiable neural
rendering. Meanwhile, the explicit mesh is updated periodically to adjust its
topology changes, and a consistency loss is designed to match both
representations closely. Compared with existing methods, SelfRecon can produce
high-fidelity surfaces for arbitrary clothed humans with self-supervised
optimization. Extensive experimental results demonstrate its effectiveness on
real captured monocular videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Similarity and Generalization: From Noise to Corruption. (arXiv:2201.12803v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12803">
<div class="article-summary-box-inner">
<span><p>Contrastive learning aims to extract distinctive features from data by
finding an embedding representation where similar samples are close to each
other, and different ones are far apart. We study generalization in contrastive
learning, focusing on its simplest representative: Siamese Neural Networks
(SNNs). We show that Double Descent also appears in SNNs and is exacerbated by
noise. We point out that SNNs can be affected by two distinct sources of noise:
Pair Label Noise (PLN) and Single Label Noise (SLN). The effect of SLN is
asymmetric, but it preserves similarity relations, while PLN is symmetric but
breaks transitivity. We show that the dataset topology crucially affects
generalization. While sparse datasets show the same performances under SLN and
PLN for an equal amount of noise, SLN outperforms PLN in the overparametrized
region in dense datasets. Indeed, in this regime, PLN similarity violation
becomes macroscopical, corrupting the dataset to the point where complete
overfitting cannot be achieved. We call this phenomenon Density-Induced Break
of Similarity (DIBS). We also probe the equivalence between online optimization
and offline generalization for similarity tasks. We observe that an
online/offline correspondence in similarity learning can be affected by both
the network architecture and label noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Segmentation of Left Ventricle in Cardiac Magnetic Resonance Images. (arXiv:2201.12805v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12805">
<div class="article-summary-box-inner">
<span><p>Segmentation of the left ventricle in cardiac magnetic resonance imaging MRI
scans enables cardiologists to calculate the volume of the left ventricle and
subsequently its ejection fraction. The ejection fraction is a measurement that
expresses the percentage of blood leaving the heart with each contraction.
Cardiologists often use ejection fraction to determine one's cardiac function.
We propose multiscale template matching technique for detection and an
elliptical active disc for automated segmentation of the left ventricle in MR
images. The elliptical active disc optimizes the local energy function with
respect to its five free parameters which define the disc. Gradient descent is
used to minimize the energy function along with Green's theorem to optimize the
computation expenses. We report validations on 320 scans containing 5,273
annotated slices which are publicly available through the Multi-Centre,
Multi-Vendor, and Multi-Disease Cardiac Segmentation (M&amp;Ms) Challenge. We
achieved successful localization of the left ventricle in 89.63% of the cases
and a Dice coefficient of 0.873 on diastole slices and 0.770 on systole slices.
The proposed technique is based on traditional image processing techniques with
a performance on par with the deep learning techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning from Demonstrations. (arXiv:2201.12813v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12813">
<div class="article-summary-box-inner">
<span><p>This paper presents a framework for learning visual representations from
unlabeled video demonstrations captured from multiple viewpoints. We show that
these representations are applicable for imitating several robotic tasks,
including pick and place. We optimize a recently proposed self-supervised
learning algorithm by applying contrastive learning to enhance task-relevant
information while suppressing irrelevant information in the feature embeddings.
We validate the proposed method on the publicly available Multi-View Pouring
and a custom Pick and Place data sets and compare it with the TCN triplet
baseline. We evaluate the learned representations using three metrics:
viewpoint alignment, stage classification and reinforcement learning, and in
all cases the results improve when compared to state-of-the-art approaches,
with the added benefit of reduced number of training iterations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimizing Gradient-driven Criteria in Network Sparsity: Gradient is All You Need. (arXiv:2201.12826v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12826">
<div class="article-summary-box-inner">
<span><p>Network sparsity receives popularity mostly due to its capability to reduce
the network complexity. Extensive studies excavate gradient-driven sparsity.
Typically, these methods are constructed upon premise of weight independence,
which however, is contrary to the fact that weights are mutually influenced.
Thus, their performance remains to be improved. In this paper, we propose to
further optimize gradient-driven sparsity (OptG) by solving this independence
paradox. Our motive comes from the recent advances on supermask training which
shows that sparse subnetworks can be located in a randomly initialized network
by simply updating mask values without modifying any weight. We prove that
supermask training is to accumulate the weight gradients and can partly solve
the independence paradox. Consequently, OptG integrates supermask training into
gradient-driven sparsity, and a specialized mask optimizer is designed to solve
the independence paradox. Experiments show that OptG can well surpass many
existing state-of-the-art competitors. Our code is available at
\url{https://github.com/zyxxmu/OptG}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comprehensive Saliency Fusion for Object Co-segmentation. (arXiv:2201.12828v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12828">
<div class="article-summary-box-inner">
<span><p>Object co-segmentation has drawn significant attention in recent years,
thanks to its clarity on the expected foreground, the shared object in a group
of images. Saliency fusion has been one of the promising ways to carry it out.
However, prior works either fuse saliency maps of the same image or saliency
maps of different images to extract the expected foregrounds. Also, they rely
on hand-crafted saliency extraction and correspondence processes in most cases.
This paper revisits the problem and proposes fusing saliency maps of both the
same image and different images. It also leverages advances in deep learning
for the saliency extraction and correspondence processes. Hence, we call it
comprehensive saliency fusion. Our experiments reveal that our approach
achieves much-improved object co-segmentation results compared to prior works
on important benchmark datasets such as iCoseg, MSRC, and Internet Images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Infrared and visible image fusion using Latent Low-Rank Representation. (arXiv:1804.08992v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1804.08992">
<div class="article-summary-box-inner">
<span><p>Infrared and visible image fusion is an important problem in the field of
image fusion which has been applied widely in many fields. To better preserve
the useful information from source images, in this paper, we propose a novel
image fusion method based on latent low-rank representation(LatLRR) which is
simple and effective. Firstly, the source images are decomposed into low-rank
parts(global structure) and salient parts(local structure) by LatLRR. Then, the
low-rank parts are fused by weighted-average strategy to preserve more contour
information. Then, the salient parts are simply fused by sum strategy which is
a efficient operation in this fusion framework. Finally, the fused image is
obtained by combining the fused low-rank part and the fused salient part.
Compared with other fusion methods experimentally, the proposed method has
better fusion performance than state-of-the-art fusion methods in both
subjective and objective evaluation. The Code of our fusion method is available
at https://github.com/hli1221/imagefusion\_Infrared\_visible\_latlrr
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-focus Noisy Image Fusion using Low-Rank Representation. (arXiv:1804.09325v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1804.09325">
<div class="article-summary-box-inner">
<span><p>Multi-focus noisy image fusion represents an important task in the field of
image fusion which generates a single, clear and focused image from all source
images. In this paper, we propose a novel multi-focus noisy image fusion method
based on low-rank representation (LRR) which is a powerful tool in
representation learning. A multi-scale transform framework is adopted in which
source images are decomposed into low frequency and high frequency
coefficients, respectively. For low frequency coefficients, the fused low
frequency coefficients are determined by a spatial frequency strategy, while
the high frequency coefficients are fused by the LRR-based fusion strategy.
Finally, the fused image is reconstructed by inverse multi-scale transforms
with fused coefficients. Experimental results demonstrate that the proposed
algorithm offers state-of-the-art performance even when the source images
contain noise. The Code of our fusion method is available at
https://github.com/hli1221/imagefusion_noisy_lrr
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optical Flow Techniques for Facial Expression Analysis -- a Practical Evaluation Study. (arXiv:1904.11592v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.11592">
<div class="article-summary-box-inner">
<span><p>Optical flow techniques are becoming increasingly performant and robust when
estimating motion in a scene, but their performance has yet to be proven in the
area of facial expression recognition. In this work, a variety of optical flow
approaches are evaluated across multiple facial expression datasets, so as to
provide a consistent performance evaluation. The aim of this work is not to
propose a new expression recognition technique, but to understand better the
adequacy of existing state-of-the art optical flow for encoding facial motion
in the context of facial expression recognition. Our evaluations highlight the
fact that motion approximation methods used to overcome motion discontinuities
have a significant impact when optical flows are used to characterize facial
expressions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-focus Image Fusion Based on Similarity Characteristics. (arXiv:1912.07959v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.07959">
<div class="article-summary-box-inner">
<span><p>A novel multi-focus image fusion algorithm performed in spatial domain based
on similarity characteristics is proposed incorporating with region
segmentation. In this paper, a new similarity measure is developed based on the
structural similarity (SSIM) index, which is more suitable for multi-focus
image segmentation. Firstly, the SSNSIM map is calculated between two input
images. Then we segment the SSNSIM map using watershed method, and merge the
small homogeneous regions with fuzzy c-means clustering algorithm (FCM). For
three source images, a joint region segmentation method based on segmentation
of two images is used to obtain the final segmentation result. Finally, the
corresponding segmented regions of the source images are fused according to
their average gradient. The performance of the image fusion method is evaluated
by several criteria including spatial frequency, average gradient, entropy,
edge retention etc. The evaluation results indicate that the proposed method is
effective and has good visual perception.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved dual channel pulse coupled neural network and its application to multi-focus image fusion. (arXiv:2002.01102v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.01102">
<div class="article-summary-box-inner">
<span><p>This paper presents an improved dual channel pulse coupled neural network
(IDC-PCNN) model for image fusion. The model can overcome some defects of
standard PCNN model. In this fusion scheme, the multiplication rule is replaced
by addition rule in the information fusion pool of dual channel PCNN (DC-PCNN)
model. Meanwhile the sum of modified Laplacian (SML) measure is adopted, which
is better than other focus measures. This method not only inherits the good
characteristics of the standard PCNN model but also enhances the computing
efficiency and fusion quality. The performance of the proposed method is
evaluated by using four criteria including average cross entropy, root mean
square error, peak value signal to noise ratio and structure similarity index.
Comparative studies show that the proposed fusion algorithm outperforms the
standard PCNN method and the DC-PCNN method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepEMD: Differentiable Earth Mover's Distance for Few-Shot Learning. (arXiv:2003.06777v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.06777">
<div class="article-summary-box-inner">
<span><p>In this work, we develop methods for few-shot image classification from a new
perspective of optimal matching between image regions. We employ the Earth
Mover's Distance (EMD) as a metric to compute a structural distance between
dense image representations to determine image relevance. The EMD generates the
optimal matching flows between structural elements that have the minimum
matching cost, which is used to calculate the image distance for
classification. To generate the important weights of elements in the EMD
formulation, we design a cross-reference mechanism, which can effectively
alleviate the adverse impact caused by the cluttered background and large
intra-class appearance variations. To implement k-shot classification, we
propose to learn a structured fully connected layer that can directly classify
dense image representations with the EMD. Based on the implicit function
theorem, the EMD can be inserted as a layer into the network for end-to-end
training. Our extensive experiments validate the effectiveness of our algorithm
which outperforms state-of-the-art methods by a significant margin on five
widely used few-shot classification benchmarks, namely, miniImageNet,
tieredImageNet, Fewshot-CIFAR100 (FC100), Caltech-UCSD Birds-200-2011 (CUB),
and CIFAR-FewShot (CIFAR-FS). We also demonstrate the effectiveness of our
method on the image retrieval task in our experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Increasing-Margin Adversarial (IMA) Training to Improve Adversarial Robustness of Neural Networks. (arXiv:2005.09147v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.09147">
<div class="article-summary-box-inner">
<span><p>Convolutional neural network (CNN) has surpassed traditional methods for
medical image classification. However, CNN is vulnerable to adversarial attacks
which may lead to disastrous consequences in medical applications. Although
adversarial noises are usually generated by attack algorithms,
white-noise-induced adversarial samples can exist, and therefore the threats
are real. In this study, we propose a novel training method, named IMA, to
improve the robust-ness of CNN against adversarial noises. During training, the
IMA method increases the margins of training samples in the input space, i.e.,
moving CNN decision boundaries far away from the training samples to improve
robustness. The IMA method is evaluated on publicly available datasets under
strong 100-PGD white-box adversarial attacks, and the results show that the
proposed method significantly improved CNN classification and segmentation
accuracy on noisy data while keeping a high accuracy on clean data. We hope our
approach may facilitate the development of robust applications in medical
field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can we Generalize and Distribute Private Representation Learning?. (arXiv:2010.01792v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.01792">
<div class="article-summary-box-inner">
<span><p>We study the problem of learning representations that are private yet
informative, i.e., provide information about intended "ally" targets while
hiding sensitive "adversary" attributes. We propose Exclusion-Inclusion
Generative Adversarial Network (EIGAN), a generalized private representation
learning (PRL) architecture that accounts for multiple ally and adversary
attributes unlike existing PRL solutions. While centrally-aggregated dataset is
a prerequisite for most PRL techniques, data in real-world is often siloed
across multiple distributed nodes unwilling to share the raw data because of
privacy concerns. We address this practical constraint by developing D-EIGAN,
the first distributed PRL method that learns representations at each node
without transmitting the source data. We theoretically analyze the behavior of
adversaries under the optimal EIGAN and D-EIGAN encoders and the impact of
dependencies among ally and adversary tasks on the optimization objective. Our
experiments on various datasets demonstrate the advantages of EIGAN in terms of
performance, robustness, and scalability. In particular, EIGAN outperforms the
previous state-of-the-art by a significant accuracy margin (47% improvement),
and D-EIGAN's performance is consistently on par with EIGAN under different
network settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FSOCO: The Formula Student Objects in Context Dataset. (arXiv:2012.07139v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.07139">
<div class="article-summary-box-inner">
<span><p>This paper presents the FSOCO dataset, a collaborative dataset for
vision-based cone detection systems in Formula Student Driverless competitions.
It contains human annotated ground truth labels for both bounding boxes and
instance-wise segmentation masks. The data buy-in philosophy of FSOCO asks
student teams to contribute to the database first before being granted access
ensuring continuous growth. By providing clear labeling guidelines and tools
for a sophisticated raw image selection, new annotations are guaranteed to meet
the desired quality. The effectiveness of the approach is shown by comparing
prediction results of a network trained on FSOCO and its unregulated
predecessor. The FSOCO dataset can be found at fsoco-dataset.com.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Disentanglement of Structured Representations. (arXiv:2101.04041v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.04041">
<div class="article-summary-box-inner">
<span><p>We introduce the first metric for evaluating disentanglement at individual
hierarchy levels of a structured latent representation. Applied to
object-centric generative models, this offers a systematic, unified approach to
evaluating (i) object separation between latent slots (ii) disentanglement of
object properties inside individual slots (iii) disentanglement of intrinsic
and extrinsic object properties. We theoretically show that for structured
representations, our framework gives stronger guarantees of selecting a good
model than previous disentanglement metrics. Experimentally, we demonstrate
that viewing object compositionality as a disentanglement problem addresses
several issues with prior visual metrics of object separation. As a core
technical component, we present the first representation probing algorithm
handling slot permutation invariance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Automated Machine Learning from Transfer Learning. (arXiv:2103.00241v6 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00241">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a neural architecture search framework based on a
similarity measure between some baseline tasks and a target task. We first
define the notion of the task similarity based on the log-determinant of the
Fisher Information matrix. Next, we compute the task similarity from each of
the baseline tasks to the target task. By utilizing the relation between a
target and a set of learned baseline tasks, the search space of architectures
for the target task can be significantly reduced, making the discovery of the
best candidates in the set of possible architectures tractable and efficient,
in terms of GPU days. This method eliminates the requirement for training the
networks from scratch for a given target task as well as introducing the bias
in the initialization of the search space from the human domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simplicial Complex Representation Learning. (arXiv:2103.04046v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04046">
<div class="article-summary-box-inner">
<span><p>Simplicial complexes form an important class of topological spaces that are
frequently used in many application areas such as computer-aided design,
computer graphics, and simulation. Representation learning on graphs, which are
just 1-d simplicial complexes, has witnessed a great attention in recent years.
However, there has not been enough effort to extend representation learning to
higher dimensional simplicial objects due to the additional complexity these
objects hold, especially when it comes to entire-simplicial complex
representation learning. In this work, we propose a method for simplicial
complex-level representation learning that embeds a simplicial complex to a
universal embedding space in a way that complex-to-complex proximity is
preserved. Our method uses our novel geometric message passing schemes to learn
an entire simplicial complex representation in an end-to-end fashion. We
demonstrate the proposed model on publicly available mesh dataset. To the best
of our knowledge, this work presents the first method for learning simplicial
complex-level representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation. (arXiv:2103.17235v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.17235">
<div class="article-summary-box-inner">
<span><p>The increase of available large clinical and experimental datasets has
contributed to a substantial amount of important contributions in the area of
biomedical image analysis. Image segmentation, which is crucial for any
quantitative analysis, has especially attracted attention. Recent hardware
advancement has led to the success of deep learning approaches. However,
although deep learning models are being trained on large datasets, existing
methods do not use the information from different learning epochs effectively.
In this work, we leverage the information of each training epoch to prune the
prediction maps of the subsequent epochs. We propose a novel architecture
called feedback attention network (FANet) that unifies the previous epoch mask
with the feature map of the current training epoch. The previous epoch mask is
then used to provide hard attention to the learned feature maps at different
convolutional layers. The network also allows to rectify the predictions in an
iterative fashion during the test time. We show that our proposed feedback
attention model provides a substantial improvement on most segmentation metrics
tested on seven publicly available biomedical imaging datasets demonstrating
the effectiveness of FANet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bootstrapping Semantic Segmentation with Regional Contrast. (arXiv:2104.04465v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04465">
<div class="article-summary-box-inner">
<span><p>We present ReCo, a contrastive learning framework designed at a regional
level to assist learning in semantic segmentation. ReCo performs
semi-supervised or supervised pixel-level contrastive learning on a sparse set
of hard negative pixels, with minimal additional memory footprint. ReCo is easy
to implement, being built on top of off-the-shelf segmentation networks, and
consistently improves performance in both semi-supervised and supervised
semantic segmentation methods, achieving smoother segmentation boundaries and
faster convergence. The strongest effect is in semi-supervised learning with
very few labels. With ReCo, we achieve high-quality semantic segmentation
models, requiring only 5 examples of each semantic class. Code is available at
https://github.com/lorenmt/reco.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MSRF-Net: A Multi-Scale Residual Fusion Network for Biomedical Image Segmentation. (arXiv:2105.07451v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07451">
<div class="article-summary-box-inner">
<span><p>Methods based on convolutional neural networks have improved the performance
of biomedical image segmentation. However, most of these methods cannot
efficiently segment objects of variable sizes and train on small and biased
datasets, which are common for biomedical use cases. While methods exist that
incorporate multi-scale fusion approaches to address the challenges arising
with variable sizes, they usually use complex models that are more suitable for
general semantic segmentation problems. In this paper, we propose a novel
architecture called Multi-Scale Residual Fusion Network (MSRF-Net), which is
specially designed for medical image segmentation. The proposed MSRF-Net is
able to exchange multi-scale features of varying receptive fields using a
Dual-Scale Dense Fusion (DSDF) block. Our DSDF block can exchange information
rigorously across two different resolution scales, and our MSRF sub-network
uses multiple DSDF blocks in sequence to perform multi-scale fusion. This
allows the preservation of resolution, improved information flow and
propagation of both high- and low-level features to obtain accurate
segmentation maps. The proposed MSRF-Net allows to capture object variabilities
and provides improved results on different biomedical datasets. Extensive
experiments on MSRF-Net demonstrate that the proposed method outperforms the
cutting-edge medical image segmentation methods on four publicly available
datasets. We achieve the dice coefficient of 0.9217, 0.9420, and 0.9224, 0.8824
on Kvasir-SEG, CVC-ClinicDB, 2018 Data Science Bowl dataset, and ISIC-2018 skin
lesion segmentation challenge dataset respectively. We further conducted
generalizability tests and achieved a dice coefficient of 0.7921 and 0.7575 on
CVC-ClinicDB and Kvasir-SEG, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UncertaintyFuseNet: Robust Uncertainty-aware Hierarchical Feature Fusion Model with Ensemble Monte Carlo Dropout for COVID-19 Detection. (arXiv:2105.08590v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08590">
<div class="article-summary-box-inner">
<span><p>The COVID-19 (Coronavirus disease 2019) pandemic has become a major global
threat to human health and well-being. Thus, the development of computer-aided
detection (CAD) systems that are capable to accurately distinguish COVID-19
from other diseases using chest computed tomography (CT) and X-ray data is of
immediate priority. Such automatic systems are usually based on traditional
machine learning or deep learning methods. Differently from most of existing
studies, which used either CT scan or X-ray images in COVID-19-case
classification, we present a simple but efficient deep learning feature fusion
model, called UncertaintyFuseNet, which is able to classify accurately large
datasets of both of these types of images. We argue that the uncertainty of the
model's predictions should be taken into account in the learning process, even
though most of existing studies have overlooked it. We quantify the prediction
uncertainty in our feature fusion model using effective Ensemble MC Dropout
(EMCD) technique. A comprehensive simulation study has been conducted to
compare the results of our new model to the existing approaches, evaluating the
performance of competing models in terms of Precision, Recall, F-Measure,
Accuracy and ROC curves. The obtained results prove the efficiency of our model
which provided the prediction accuracy of 99.08\% and 96.35\% for the
considered CT scan and X-ray datasets, respectively. Moreover, our
UncertaintyFuseNet model was generally robust to noise and performed well with
previously unseen data. The source code of our implementation is freely
available at:
https://github.com/moloud1987/UncertaintyFuseNet-for-COVID-19-Classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProtoRes: Proto-Residual Network for Pose Authoring via Learned Inverse Kinematics. (arXiv:2106.01981v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01981">
<div class="article-summary-box-inner">
<span><p>Our work focuses on the development of a learnable neural representation of
human pose for advanced AI assisted animation tooling. Specifically, we tackle
the problem of constructing a full static human pose based on sparse and
variable user inputs (e.g. locations and/or orientations of a subset of body
joints). To solve this problem, we propose a novel neural architecture that
combines residual connections with prototype encoding of a partially specified
pose to create a new complete pose from the learned latent space. We show that
our architecture outperforms a baseline based on Transformer, both in terms of
accuracy and computational efficiency. Additionally, we develop a user
interface to integrate our neural model in Unity, a real-time 3D development
platform. Furthermore, we introduce two new datasets representing the static
human pose modeling problem, based on high-quality human motion capture data,
which will be released publicly along with model code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BR-NPA: A Non-Parametric High-Resolution Attention Model to improve the Interpretability of Attention. (arXiv:2106.02566v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02566">
<div class="article-summary-box-inner">
<span><p>The prevalence of employing attention mechanisms has brought along concerns
on the interpretability of attention distributions. Although it provides
insights about how a model is operating, utilizing attention as the explanation
of model predictions is still highly dubious. The community is still seeking
more interpretable strategies for better identifying local active regions that
contribute the most to the final decision. To improve the interpretability of
existing attention models, we propose a novel Bilinear Representative
Non-Parametric Attention (BR-NPA) strategy that captures the task-relevant
human-interpretable information. The target model is first distilled to have
higher-resolution intermediate feature maps. From which, representative
features are then grouped based on local pairwise feature similarity, to
produce finer-grained, more precise attention maps highlighting task-relevant
parts of the input. The obtained attention maps are ranked according to the
activity level of the compound feature, which provides information regarding
the important level of the highlighted regions. The proposed model can be
easily adapted in a wide variety of modern deep models, where classification is
involved. Extensive quantitative and qualitative experiments showcase more
comprehensive and accurate visual explanations compared to state-of-the-art
attention models and visualizations methods across multiple tasks including
fine-grained image classification, few-shot classification, and person
re-identification, without compromising the classification accuracy. The
proposed visualization model sheds imperative light on how neural networks `pay
their attention' differently in different tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Implicit 3D Shapes from Single Images with Spatial Patterns. (arXiv:2106.03087v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03087">
<div class="article-summary-box-inner">
<span><p>Neural implicit functions have achieved impressive results for reconstructing
3D shapes from single images. However, the image features for describing 3D
point samplings of implicit functions are less effective when significant
variations of occlusions, views, and appearances exist from the image. To
better encode image features, we study a geometry-aware convolutional kernel to
leverage geometric relationships of point samplings by the proposed
\emph{spatial pattern}, i.e., a structured point set. Specifically, the kernel
operates at 2D projections of 3D points from the spatial pattern. Supported by
the spatial pattern, the 2D kernel encodes geometric information that is
crucial for 3D reconstruction tasks, while traditional ones mainly consider
appearance information. Furthermore, to enable the network to discover more
adaptive spatial patterns for further capturing non-local contextual
information, the kernel is devised to be deformable manipulated by a spatial
pattern generator. Experimental results on both synthetic and real datasets
demonstrate the superiority of the proposed method. Pre-trained models, codes,
and data are available at https://github.com/yixin26/SVR-SP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-scale Unsupervised Semantic Segmentation. (arXiv:2106.03149v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03149">
<div class="article-summary-box-inner">
<span><p>Powered by the ImageNet dataset, unsupervised learning on large-scale data
has made significant advances for classification tasks. There are two major
challenges to allowing such an attractive learning modality for segmentation
tasks: i) a large-scale benchmark for assessing algorithms is missing; ii)
unsupervised category/shape representation learning is difficult. We propose a
new problem of large-scale unsupervised semantic segmentation (LUSS) with a
newly created benchmark dataset to track the research progress. Based on the
ImageNet dataset, we propose the ImageNet-S dataset with 1.2 million training
images and 50k high-quality semantic segmentation annotations for evaluation.
Our benchmark has a high data diversity and a clear task objective. We also
present a simple yet effective method that works surprisingly well for LUSS. In
addition, we benchmark related un/weakly/fully supervised methods accordingly,
identifying the challenges and possible directions of LUSS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-learning with implicit gradients in a few-shot setting for medical image segmentation. (arXiv:2106.03223v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03223">
<div class="article-summary-box-inner">
<span><p>Widely used traditional supervised deep learning methods require a large
number of training samples but often fail to generalize on unseen datasets.
Therefore, a more general application of any trained model is quite limited for
medical imaging for clinical practice. Using separately trained models for each
unique lesion category or a unique patient population will require sufficiently
large curated datasets, which is not practical to use in a real-world clinical
set-up. Few-shot learning approaches can not only minimize the need for an
enormous number of reliable ground truth labels that are labour-intensive and
expensive but can also be used to model on a dataset coming from a new
population. To this end, we propose to exploit an optimization-based implicit
model agnostic meta-learning (iMAML) algorithm under few-shot settings for
medical image segmentation. Our approach can leverage the learned weights from
diverse but small training samples to perform analysis on unseen datasets with
high accuracy. We show that, unlike classical few-shot learning approaches, our
method improves generalization capability. To our knowledge, this is the first
work that exploits iMAML for medical image segmentation and explores the
strength of the model on scenarios such as meta-training on unique and mixed
instances of lesion datasets. Our quantitative results on publicly available
skin and polyp datasets show that the proposed method outperforms the naive
supervised baseline model and two recent few-shot segmentation approaches by
large margins. In addition, our iMAML approach shows an improvement of 2%-4% in
dice score compared to its counterpart MAML for most experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entropy-based Logic Explanations of Neural Networks. (arXiv:2106.06804v4 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06804">
<div class="article-summary-box-inner">
<span><p>Explainable artificial intelligence has rapidly emerged since lawmakers have
started requiring interpretable models for safety-critical domains.
Concept-based neural networks have arisen as explainable-by-design methods as
they leverage human-understandable symbols (i.e. concepts) to predict class
memberships. However, most of these approaches focus on the identification of
the most relevant concepts but do not provide concise, formal explanations of
how such concepts are leveraged by the classifier to make predictions. In this
paper, we propose a novel end-to-end differentiable approach enabling the
extraction of logic explanations from neural networks using the formalism of
First-Order Logic. The method relies on an entropy-based criterion which
automatically identifies the most relevant concepts. We consider four different
case studies to demonstrate that: (i) this entropy-based criterion enables the
distillation of concise logic explanations in safety-critical domains from
clinical data to computer vision; (ii) the proposed approach outperforms
state-of-the-art white-box models in terms of classification accuracy and
matches black box performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The 2021 Image Similarity Dataset and Challenge. (arXiv:2106.09672v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09672">
<div class="article-summary-box-inner">
<span><p>This paper introduces a new benchmark for large-scale image similarity
detection. This benchmark is used for the Image Similarity Challenge at
NeurIPS'21 (ISC2021). The goal is to determine whether a query image is a
modified copy of any image in a reference corpus of size 1~million. The
benchmark features a variety of image transformations such as automated
transformations, hand-crafted image edits and machine-learning based
manipulations. This mimics real-life cases appearing in social media, for
example for integrity-related problems dealing with misinformation and
objectionable content. The strength of the image manipulations, and therefore
the difficulty of the benchmark, is calibrated according to the performance of
a set of baseline approaches. Both the query and reference set contain a
majority of "distractor" images that do not match, which corresponds to a
real-life needle-in-haystack setting, and the evaluation metric reflects that.
We expect the DISC21 benchmark to promote image copy detection as an important
and challenging computer vision task and refresh the state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global and Local Contrastive Self-Supervised Learning for Semantic Segmentation of HR Remote Sensing Images. (arXiv:2106.10605v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10605">
<div class="article-summary-box-inner">
<span><p>Supervised learning for semantic segmentation requires a large number of
labeled samples, which is difficult to obtain in the field of remote sensing.
Self-supervised learning (SSL), can be used to solve such problems by
pre-training a general model with a large number of unlabeled images and then
fine-tuning it on a downstream task with very few labeled samples. Contrastive
learning is a typical method of SSL that can learn general invariant features.
However, most existing contrastive learning methods are designed for
classification tasks to obtain an image-level representation, which may be
suboptimal for semantic segmentation tasks requiring pixel-level
discrimination. Therefore, we propose a global style and local matching
contrastive learning network (GLCNet) for remote sensing image semantic
segmentation. Specifically, 1) the global style contrastive learning module is
used to better learn an image-level representation, as we consider that style
features can better represent the overall image features. 2) The local features
matching contrastive learning module is designed to learn representations of
local regions, which is beneficial for semantic segmentation. The experimental
results show that our method mostly outperforms SOTA self-supervised methods
and the ImageNet pre-training method. Specifically, with 1\% annotation from
the original dataset, our approach improves Kappa by 6\% on the ISPRS Potsdam
dataset relative to the existing baseline. Moreover, our method outperforms
supervised learning methods when there are some differences between the
datasets of upstream tasks and downstream tasks. Since SSL could directly learn
the essential characteristics of data from unlabeled data, which is easy to
obtain in the remote sensing field, this may be of great significance for tasks
such as global mapping. The source code is available at
https://github.com/GeoX-Lab/G-RSIM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Steerable 3D Spherical Neurons. (arXiv:2106.13863v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13863">
<div class="article-summary-box-inner">
<span><p>Emerging from low-level vision theory, steerable filters found their
counterpart in prior work on steerable convolutional neural networks
equivariant to rigid transformations. In our work, we propose a steerable
feed-forward learning-based approach that consists of neurons with spherical
decision surfaces and operates on point clouds. Such spherical neurons are
obtained by conformal embedding of Euclidean space and have recently been
revisited in the context of learning representations of point sets. Focusing on
3D geometry, we exploit the isometry property of spherical neurons and derive a
3D steerability constraint. After training spherical neurons to classify point
clouds in a canonical orientation, we use a tetrahedron basis to quadruplicate
the neurons and construct rotation-equivariant spherical filter banks. We then
apply the derived constraint to interpolate the filter bank outputs and, thus,
obtain a rotation-invariant network. Finally, we use a synthetic point set and
real-world 3D skeleton data to verify our theoretical findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Split, embed and merge: An accurate table structure recognizer. (arXiv:2107.05214v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05214">
<div class="article-summary-box-inner">
<span><p>Table structure recognition is an essential part for making machines
understand tables. Its main task is to recognize the internal structure of a
table. However, due to the complexity and diversity in their structure and
style, it is very difficult to parse the tabular data into the structured
format which machines can understand easily, especially for complex tables. In
this paper, we introduce Split, Embed and Merge (SEM), an accurate table
structure recognizer. Our model takes table images as input and can correctly
recognize the structure of tables, whether they are simple or a complex tables.
SEM is mainly composed of three parts, splitter, embedder and merger. In the
first stage, we apply the splitter to predict the potential regions of the
table row (column) separators, and obtain the fine grid structure of the table.
In the second stage, by taking a full consideration of the textual information
in the table, we fuse the output features for each table grid from both vision
and language modalities. Moreover, we achieve a higher precision in our
experiments through adding additional semantic features. Finally, we process
the merging of these basic table grids in a self-regression manner. The
correspondent merging results is learned through the attention mechanism. In
our experiments, SEM achieves an average F1-Measure of 97.11% on the SciTSR
dataset which outperforms other methods by a large margin. We also won the
first place in the complex table and third place in all tables in ICDAR 2021
Competition on Scientific Literature Parsing, Task-B. Extensive experiments on
other publicly available datasets demonstrate that our model achieves
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LASOR: Learning Accurate 3D Human Pose and Shape Via Synthetic Occlusion-Aware Data and Neural Mesh Rendering. (arXiv:2108.00351v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00351">
<div class="article-summary-box-inner">
<span><p>A key challenge in the task of human pose and shape estimation is occlusion,
including self-occlusions, object-human occlusions, and inter-person
occlusions. The lack of diverse and accurate pose and shape training data
becomes a major bottleneck, especially for scenes with occlusions in the wild.
In this paper, we focus on the estimation of human pose and shape in the case
of inter-person occlusions, while also handling object-human occlusions and
self-occlusion. We propose a novel framework that synthesizes occlusion-aware
silhouette and 2D keypoints data and directly regress to the SMPL pose and
shape parameters. A neural 3D mesh renderer is exploited to enable silhouette
supervision on the fly, which contributes to great improvements in shape
estimation. In addition, keypoints-and-silhouette-driven training data in
panoramic viewpoints are synthesized to compensate for the lack of viewpoint
diversity in any existing dataset. Experimental results show that we are among
the state-of-the-art on the 3DPW and 3DPW-Crowd datasets in terms of pose
estimation accuracy. The proposed method evidently outperforms Mesh
Transformer, 3DCrowdNet and ROMP in terms of shape estimation. Top performance
is also achieved on SSP-3D in terms of shape prediction accuracy. Demo and code
will be available at https://igame-lab.github.io/LASOR/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pro-UIGAN: Progressive Face Hallucination from Occluded Thumbnails. (arXiv:2108.00602v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00602">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the task of hallucinating an authentic
high-resolution (HR) face from an occluded thumbnail. We propose a multi-stage
Progressive Upsampling and Inpainting Generative Adversarial Network, dubbed
Pro-UIGAN, which exploits facial geometry priors to replenish and upsample (8*)
the occluded and tiny faces (16*16 pixels). Pro-UIGAN iteratively (1) estimates
facial geometry priors for low-resolution (LR) faces and (2) acquires
non-occluded HR face images under the guidance of the estimated priors. Our
multi-stage hallucination network super-resolves and inpaints occluded LR faces
in a coarse-to-fine manner, thus reducing unwanted blurriness and artifacts
significantly. Specifically, we design a novel cross-modal transformer module
for facial priors estimation, in which an input face and its landmark features
are formulated as queries and keys, respectively. Such a design encourages
joint feature learning across the input facial and landmark features, and deep
feature correspondences will be discovered by attention. Thus, facial
appearance features and facial geometry priors are learned in a mutual
promotion manner. Extensive experiments demonstrate that our Pro-UIGAN achieves
visually pleasing HR faces, reaching superior performance in downstream tasks,
i.e., face alignment, face parsing, face recognition and expression
classification, compared with other state-of-the-art (SotA) methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BIGRoC: Boosting Image Generation via a Robust Classifier. (arXiv:2108.03702v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03702">
<div class="article-summary-box-inner">
<span><p>The interest of the machine learning community in image synthesis has grown
significantly in recent years, with the introduction of a wide range of deep
generative models and means for training them. In this work, we propose a
general model-agnostic technique for improving the image quality and the
distribution fidelity of generated images, obtained by any generative model.
Our method, termed BIGRoC (Boosting Image Generation via a Robust Classifier),
is based on a post-processing procedure via the guidance of a given robust
classifier and without a need for additional training of the generative model.
Given a synthesized image, we propose to update it through projected gradient
steps over the robust classifier, in an attempt to refine its recognition. We
demonstrate this post-processing algorithm on various image synthesis methods
and show a significant improvement of the generated images, both quantitatively
and qualitatively, on CIFAR-10 and ImageNet. Specifically, BIGRoC improves the
image synthesis state of the art on ImageNet 128x128 by 14.81%, attaining an
FID score of 2.53 and on 256x256 by 13.29%, achieving an FID of 3.98.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FT-TDR: Frequency-guided Transformer and Top-Down Refinement Network for Blind Face Inpainting. (arXiv:2108.04424v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04424">
<div class="article-summary-box-inner">
<span><p>Blind face inpainting refers to the task of reconstructing visual contents
without explicitly indicating the corrupted regions in a face image.
Inherently, this task faces two challenges: (1) how to detect various mask
patterns of different shapes and contents; (2) how to restore visually
plausible and pleasing contents in the masked regions. In this paper, we
propose a novel two-stage blind face inpainting method named Frequency-guided
Transformer and Top-Down Refinement Network (FT-TDR) to tackle these
challenges. Specifically, we first use a transformer-based network to detect
the corrupted regions to be inpainted as masks by modeling the relation among
different patches. We also exploit the frequency modality as complementary
information for improved detection results and capture the local contextual
incoherence to enhance boundary consistency. Then a top-down refinement network
is proposed to hierarchically restore features at different levels and generate
contents that are semantically consistent with the unmasked face regions.
Extensive experiments demonstrate that our method outperforms current
state-of-the-art blind and non-blind face inpainting methods qualitatively and
quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistent Relative Confidence and Label-Free Model Selection for Convolutional Neural Networks. (arXiv:2108.11845v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11845">
<div class="article-summary-box-inner">
<span><p>This letter is concerned with image classification with deep convolutional
neural networks (CNNs). The focus is on the following question: given a set of
candidate CNN models, how to select the right one with the best generalization
property for the current task? Present model selection methods require access
to a batch of labeled data for computing a pre-specified performance metric,
such as the cross-entropy loss, the classification error rate, the negative
log-likelihood. In many practical cases, labels are not available in time as
labeling itself is a time-consuming and expensive task. To this end, this
letter presents an approach to CNN model selection using only unlabeled data.
This method is developed based on a principle termed consistent relative
confidence. The effectiveness and efficiency of the proposed method are
demonstrated by experiments using benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers. (arXiv:2109.10686v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10686">
<div class="article-summary-box-inner">
<span><p>There remain many open questions pertaining to the scaling behaviour of
Transformer architectures. These scaling decisions and findings can be
critical, as training runs often come with an associated computational cost
which have both financial and/or environmental impact. The goal of this paper
is to present scaling insights from pretraining and finetuning Transformers.
While Kaplan et al. presents a comprehensive study of the scaling behaviour of
Transformer language models, the scope is only on the upstream (pretraining)
loss. Therefore, it is still unclear if these set of findings transfer to
downstream task within the context of the pretrain-finetune paradigm. The key
findings of this paper are as follows: (1) we show that aside from only the
model size, model shape matters for downstream fine-tuning, (2) scaling
protocols operate differently at different compute regions, (3) widely adopted
T5-base and T5-large sizes are Pareto-inefficient. To this end, we present
improved scaling protocols whereby our redesigned models achieve similar
downstream fine-tuning quality while having 50\% fewer parameters and training
40\% faster compared to the widely adopted T5-base model. We publicly release
over 100 pretrained checkpoints of different T5 configurations to facilitate
future research and analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frequency Disentangled Residual Network. (arXiv:2109.12556v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12556">
<div class="article-summary-box-inner">
<span><p>Residual networks (ResNets) have been utilized for various computer vision
and image processing applications. The residual connection improves the
training of the network with better gradient flow. A residual block consists of
few convolutional layers having trainable parameters, which leads to
overfitting. Moreover, the present residual networks are not able to utilize
the high and low frequency information suitably, which also challenges the
generalization capability of the network. In this paper, a frequency
disentangled residual network (FDResNet) is proposed to tackle these issues.
Specifically, FDResNet includes separate connections in the residual block for
low and high frequency components, respectively. Basically, the proposed model
disentangles the low and high frequency components to increase the
generalization ability. Moreover, the computation of low and high frequency
components using fixed filters further avoids the overfitting. The proposed
model is tested on benchmark CIFAR10/100, Caltech and TinyImageNet datasets for
image classification. The performance of the proposed model is also tested in
image retrieval framework. It is noticed that the proposed model outperforms
its counterpart residual model. The effect of kernel size and standard
deviation is also evaluated. The impact of the frequency disentangling is also
analyzed using saliency map.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spike-inspired Rank Coding for Fast and Accurate Recurrent Neural Networks. (arXiv:2110.02865v2 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02865">
<div class="article-summary-box-inner">
<span><p>Biological spiking neural networks (SNNs) can temporally encode information
in their outputs, e.g. in the rank order in which neurons fire, whereas
artificial neural networks (ANNs) conventionally do not. As a result, models of
SNNs for neuromorphic computing are regarded as potentially more rapid and
efficient than ANNs when dealing with temporal input. On the other hand, ANNs
are simpler to train, and usually achieve superior performance. Here we show
that temporal coding such as rank coding (RC) inspired by SNNs can also be
applied to conventional ANNs such as LSTMs, and leads to computational savings
and speedups. In our RC for ANNs, we apply backpropagation through time using
the standard real-valued activations, but only from a strategically early time
step of each sequential input example, decided by a threshold-crossing event.
Learning then incorporates naturally also _when_ to produce an output, without
other changes to the model or the algorithm. Both the forward and the backward
training pass can be significantly shortened by skipping the remaining input
sequence after that first event. RC-training also significantly reduces
time-to-insight during inference, with a minimal decrease in accuracy. The
desired speed-accuracy trade-off is tunable by varying the threshold or a
regularization parameter that rewards output entropy. We demonstrate these in
two toy problems of sequence classification, and in a temporally-encoded MNIST
dataset where our RC model achieves 99.19% accuracy after the first input
time-step, outperforming the state of the art in temporal coding with SNNs, as
well as in spoken-word classification of Google Speech Commands, outperforming
non-RC-trained early inference with LSTMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A transformer-based deep learning approach for classifying brain metastases into primary organ sites using clinical whole brain MRI. (arXiv:2110.03588v5 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03588">
<div class="article-summary-box-inner">
<span><p>Treatment decisions for brain metastatic disease rely on knowledge of the
primary organ site, and currently made with biopsy and histology. Here we
develop a novel deep learning approach for accurate non-invasive digital
histology with whole-brain MRI data. Our IRB-approved single-site retrospective
study was comprised of patients (n=1,399) referred for MRI treatment-planning
and gamma knife radiosurgery over 21 years. Contrast-enhanced T1-weighted and
T2-weighted Fluid-Attenuated Inversion Recovery brain MRI exams (n=1,582) were
preprocessed and input to the proposed deep learning workflow for tumor
segmentation, modality transfer, and primary site classification into one of
five classes (lung, breast, melanoma, renal, and others). Ten-fold
cross-validation generated overall AUC of 0.947 (95%CI:0.938,0.955), lung class
AUC of 0.899 (95%CI:0.884,0.915), breast class AUC of 0.990
(95%CI:0.983,0.997), melanoma class AUC of 0.882 (95%CI:0.858,0.906), renal
class AUC of 0.870 (95%CI:0.823,0.918), and other class AUC of 0.885
(95%CI:0.843,0.949). These data establish that whole-brain imaging features are
discriminative to allow accurate diagnosis of the primary organ site of
malignancy. Our end-to-end deep radiomic approach has great potential for
classifying metastatic tumor types from whole-brain MRI images. Further
refinement may offer an invaluable clinical tool to expedite primary cancer
site identification for precision treatment and improved outcomes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZARTS: On Zero-order Optimization for Neural Architecture Search. (arXiv:2110.04743v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04743">
<div class="article-summary-box-inner">
<span><p>Differentiable architecture search (DARTS) has been a popular one-shot
paradigm for NAS due to its high efficiency. It introduces trainable
architecture parameters to represent the importance of candidate operations and
proposes first/second-order approximation to estimate their gradients, making
it possible to solve NAS by gradient descent algorithm. However, our in-depth
empirical results show that the approximation will often distort the loss
landscape, leading to the biased objective to optimize and in turn inaccurate
gradient estimation for architecture parameters. This work turns to zero-order
optimization and proposes a novel NAS scheme, called ZARTS, to search without
enforcing the above approximation. Specifically, three representative
zero-order optimization methods are introduced: RS, MGS, and GLD, among which
MGS performs best by balancing the accuracy and speed. Moreover, we explore the
connections between RS/MGS and gradient descent algorithm and show that our
ZARTS can be seen as a robust gradient-free counterpart to DARTS. Extensive
experiments on multiple datasets and search spaces show the remarkable
performance of our method. In particular, results on 12 benchmarks verify the
outstanding robustness of ZARTS, where the performance of DARTS collapses due
to its known instability issue. Also, we search on the search space of DARTS to
compare with peer methods, and our discovered architecture achieves 97.54%
accuracy on CIFAR-10 and 75.7% top-1 accuracy on ImageNet, which are
state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Is Graph: Structured Graph Module for Video Action Recognition. (arXiv:2110.05904v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05904">
<div class="article-summary-box-inner">
<span><p>In the field of action recognition, video clips are always treated as ordered
frames for subsequent processing. To achieve spatio-temporal perception,
existing approaches propose to embed adjacent temporal interaction in the
convolutional layer. The global semantic information can therefore be obtained
by stacking multiple local layers hierarchically. However, such global temporal
accumulation can only reflect the high-level semantics in deep layers,
neglecting the potential low-level holistic clues in shallow layers. In this
paper, we first propose to transform a video sequence into a graph to obtain
direct long-term dependencies among temporal frames. To preserve sequential
information during transformation, we devise a structured graph module (SGM),
achieving fine-grained temporal interactions throughout the entire network. In
particular, SGM divides the neighbors of each node into several temporal
regions so as to extract global structural information with diverse sequential
flows. Extensive experiments are performed on standard benchmark datasets,
i.e., Something-Something V1 &amp; V2, Diving48, Kinetics-400, UCF101, and HMDB51.
The reported performance and analysis demonstrate that SGM can achieve
outstanding precision with less computational complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling. (arXiv:2110.08263v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08263">
<div class="article-summary-box-inner">
<span><p>The recently proposed FixMatch achieved state-of-the-art results on most
semi-supervised learning (SSL) benchmarks. However, like other modern SSL
algorithms, FixMatch uses a pre-defined constant threshold for all classes to
select unlabeled data that contribute to the training, thus failing to consider
different learning status and learning difficulties of different classes. To
address this issue, we propose Curriculum Pseudo Labeling (CPL), a curriculum
learning approach to leverage unlabeled data according to the model's learning
status. The core of CPL is to flexibly adjust thresholds for different classes
at each time step to let pass informative unlabeled data and their pseudo
labels. CPL does not introduce additional parameters or computations (forward
or backward propagation). We apply CPL to FixMatch and call our improved
algorithm FlexMatch. FlexMatch achieves state-of-the-art performance on a
variety of SSL benchmarks, with especially strong performances when the labeled
data are extremely limited or when the task is challenging. For example,
FlexMatch achieves 13.96% and 18.96% error rate reduction over FixMatch on
CIFAR-100 and STL-10 datasets respectively, when there are only 4 labels per
class. CPL also significantly boosts the convergence speed, e.g., FlexMatch can
use only 1/5 training time of FixMatch to achieve even better performance.
Furthermore, we show that CPL can be easily adapted to other SSL algorithms and
remarkably improve their performances. We open-source our code at
https://github.com/TorchSSL/TorchSSL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PlaneRecNet: Multi-Task Learning with Cross-Task Consistency for Piece-Wise Plane Detection and Reconstruction from a Single RGB Image. (arXiv:2110.11219v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11219">
<div class="article-summary-box-inner">
<span><p>Piece-wise 3D planar reconstruction provides holistic scene understanding of
man-made environments, especially for indoor scenarios. Most recent approaches
focused on improving the segmentation and reconstruction results by introducing
advanced network architectures but overlooked the dual characteristics of
piece-wise planes as objects and geometric models. Different from other
existing approaches, we start from enforcing cross-task consistency for our
multi-task convolutional neural network, PlaneRecNet, which integrates a
single-stage instance segmentation network for piece-wise planar segmentation
and a depth decoder to reconstruct the scene from a single RGB image. To
achieve this, we introduce several novel loss functions (geometric constraint)
that jointly improve the accuracy of piece-wise planar segmentation and depth
estimation. Meanwhile, a novel Plane Prior Attention module is used to guide
depth estimation with the awareness of plane instances. Exhaustive experiments
are conducted in this work to validate the effectiveness and efficiency of our
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Unlearning via Class-Discriminative Pruning. (arXiv:2110.11794v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11794">
<div class="article-summary-box-inner">
<span><p>We explore the problem of selectively forgetting categories from trained CNN
classification models in the federated learning (FL). Given that the data used
for training cannot be accessed globally in FL, our insights probe deep into
the internal influence of each channel. Through the visualization of feature
maps activated by different channels, we observe that different channels have a
varying contribution to different categories in image classification. Inspired
by this, we propose a method for scrubbing the model clean of information about
particular categories. The method does not require retraining from scratch, nor
global access to the data used for training. Instead, we introduce the concept
of Term Frequency Inverse Document Frequency (TF-IDF) to quantize the class
discrimination of channels. Channels with high TF-IDF scores have more
discrimination on the target categories and thus need to be pruned to unlearn.
The channel pruning is followed by a fine-tuning process to recover the
performance of the pruned model. Evaluated on CIFAR10 dataset, our method
accelerates the speed of unlearning by 8.9x for the ResNet model, and 7.9x for
the VGG model under no degradation in accuracy, compared to retraining from
scratch. For CIFAR100 dataset, the speedups are 9.9x and 8.4x, respectively. We
envision this work as a complementary block for FL towards compliance with
legal and ethical criteria.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards the Generalization of Contrastive Self-Supervised Learning. (arXiv:2111.00743v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00743">
<div class="article-summary-box-inner">
<span><p>Recently, self-supervised learning has attracted great attention, since it
only requires unlabeled data for training. Contrastive learning is a popular
approach for self-supervised learning and achieves promising empirical
performance. However, the theoretical understanding of its generalization
ability is still limited. To this end, we define a kind of
$(\sigma,\delta)$-measure to mathematically quantify the data augmentation, and
then provide an upper bound of the downstream classification error based on the
measure. We show that the generalization ability of contrastive self-supervised
learning depends on three key factors: alignment of positive samples,
divergence of class centers, and concentration of augmented data. The first two
factors can be optimized by contrastive algorithms, while the third one is
priorly determined by pre-defined data augmentation. With the above theoretical
findings, we further study two canonical contrastive losses, InfoNCE and
cross-correlation loss, and prove that both of them are able to obtain the
embedding space satisfying the aforementioned factors. Finally, we conduct
various experiments on the real-world dataset, and show that our theoretical
inferences on the relationship between the data augmentation and the
generalization of contrastive self-supervised learning agree with the empirical
observations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering and Explaining the Representation Bottleneck of DNNs. (arXiv:2111.06236v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06236">
<div class="article-summary-box-inner">
<span><p>This paper explores the bottleneck of feature representations of deep neural
networks (DNNs), from the perspective of the complexity of interactions between
input variables encoded in DNNs. To this end, we focus on the multi-order
interaction between input variables, where the order represents the complexity
of interactions. We discover that a DNN is more likely to encode both too
simple interactions and too complex interactions, but usually fails to learn
interactions of intermediate complexity. Such a phenomenon is widely shared by
different DNNs for different tasks. This phenomenon indicates a cognition gap
between DNNs and human beings, and we call it a representation bottleneck. We
theoretically prove the underlying reason for the representation bottleneck.
Furthermore, we propose a loss to encourage/penalize the learning of
interactions of specific complexities, and analyze the representation
capacities of interactions of different complexities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable multiple abnormality classification of chest CT volumes with deep learning. (arXiv:2111.12215v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12215">
<div class="article-summary-box-inner">
<span><p>Understanding model predictions is critical in healthcare, to facilitate
rapid verification of model correctness and to guard against use of models that
exploit confounding variables. We introduce the challenging new task of
explainable multiple abnormality classification in volumetric medical images,
in which a model must indicate the regions used to predict each abnormality. To
solve this task, we propose a multiple instance learning convolutional neural
network, AxialNet, that allows identification of top slices for each
abnormality. Next we incorporate HiResCAM, an attention mechanism, to identify
sub-slice regions. We prove that for AxialNet, HiResCAM explanations are
guaranteed to reflect the locations the model used, unlike Grad-CAM which
sometimes highlights irrelevant locations. Armed with a model that produces
faithful explanations, we then aim to improve the model's learning through a
novel mask loss that leverages HiResCAM and 3D allowed regions to encourage the
model to predict abnormalities based only on the organs in which those
abnormalities appear. The 3D allowed regions are obtained automatically through
a new approach, PARTITION, that combines location information extracted from
radiology reports with organ segmentation maps obtained through morphological
image processing. Overall, we propose the first model for explainable
multi-abnormality prediction in volumetric medical images, and then use the
mask loss to achieve a 33% improvement in organ localization of multiple
abnormalities in the RAD-ChestCT data set of 36,316 scans, representing the
state of the art. This work advances the clinical applicability of multiple
abnormality modeling in chest CT volumes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object-aware Video-language Pre-training for Retrieval. (arXiv:2112.00656v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00656">
<div class="article-summary-box-inner">
<span><p>Recently, by introducing large-scale dataset and strong transformer network,
video-language pre-training has shown great success especially for retrieval.
Yet, existing video-language transformer models do not explicitly fine-grained
semantic align. In this work, we present Object-aware Transformers, an
object-centric approach that extends video-language transformer to incorporate
object representations. The key idea is to leverage the bounding boxes and
object tags to guide the training process. We evaluate our model on three
standard sub-tasks of video-text matching on four widely used benchmarks. We
also provide deep analysis and detailed ablation about the proposed method. We
show clear improvement in performance across all tasks and datasets considered,
demonstrating the value of a model that incorporates object representations
into a video-language architecture. The code will be released at
\url{https://github.com/FingerRec/OA-Transformer}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sample Prior Guided Robust Model Learning to Suppress Noisy Labels. (arXiv:2112.01197v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01197">
<div class="article-summary-box-inner">
<span><p>Imperfect labels are ubiquitous in real-world datasets and seriously harm the
model performance. Several recent effective methods for handling noisy labels
have two key steps: 1) dividing samples into cleanly labeled and wrongly
labeled sets by training loss, 2) using semi-supervised methods to generate
pseudo-labels for samples in the wrongly labeled set. However, current methods
always hurt the informative hard samples due to the similar loss distribution
between the hard samples and the noisy ones. In this paper, we proposed PGDF
(Prior Guided Denoising Framework), a novel framework to learn a deep model to
suppress noise by generating the samples' prior knowledge, which is integrated
into both dividing samples step and semi-supervised step. Our framework can
save more informative hard clean samples into the cleanly labeled set. Besides,
our framework also promotes the quality of pseudo-labels during the
semi-supervised step by suppressing the noise in the current pseudo-labels
generating scheme. To further enhance the hard samples, we reweight the samples
in the cleanly labeled set during training. We evaluated our method using
synthetic datasets based on CIFAR-10 and CIFAR-100, as well as on the
real-world datasets WebVision and Clothing1M. The results demonstrate
substantial improvements over state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Contrastive Learning through the Lens of Neighborhood Component Analysis: an Integrated Framework. (arXiv:2112.04468v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04468">
<div class="article-summary-box-inner">
<span><p>As a seminal tool in self-supervised representation learning, contrastive
learning has gained unprecedented attention in recent years. In essence,
contrastive learning aims to leverage pairs of positive and negative samples
for representation learning, which relates to exploiting neighborhood
information in a feature space. By investigating the connection between
contrastive learning and neighborhood component analysis (NCA), we provide a
novel stochastic nearest neighbor viewpoint of contrastive learning and
subsequently propose a series of contrastive losses that outperform the
existing ones. Under our proposed framework, we show a new methodology to
design integrated contrastive losses that could simultaneously achieve good
accuracy and robustness on downstream tasks. With the integrated framework, we
achieve up to 6\% improvement on the standard accuracy and 17\% improvement on
the robust accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Curvature-guided dynamic scale networks for Multi-view Stereo. (arXiv:2112.05999v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05999">
<div class="article-summary-box-inner">
<span><p>Multi-view stereo (MVS) is a crucial task for precise 3D reconstruction. Most
recent studies tried to improve the performance of matching cost volume in MVS
by designing aggregated 3D cost volumes and their regularization. This paper
focuses on learning a robust feature extraction network to enhance the
performance of matching costs without heavy computation in the other steps. In
particular, we present a dynamic scale feature extraction network, namely,
CDSFNet. It is composed of multiple novel convolution layers, each of which can
select a proper patch scale for each pixel guided by the normal curvature of
the image surface. As a result, CDFSNet can estimate the optimal patch scales
to learn discriminative features for accurate matching computation between
reference and source images. By combining the robust extracted features with an
appropriate cost formulation strategy, our resulting MVS architecture can
estimate depth maps more precisely. Extensive experiments showed that the
proposed method outperforms other state-of-the-art methods on complex outdoor
scenes. It significantly improves the completeness of reconstructed models. As
a result, the method can process higher resolution inputs within faster
run-time and lower memory than other MVS methods. Our source code is available
at url{https://github.com/TruongKhang/cds-mvsnet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">360{\deg} Optical Flow using Tangent Images. (arXiv:2112.14331v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14331">
<div class="article-summary-box-inner">
<span><p>Omnidirectional 360{\deg} images have found many promising and exciting
applications in computer vision, robotics and other fields, thanks to their
increasing affordability, portability and their 360{\deg} field of view. The
most common format for storing, processing and visualising 360{\deg} images is
equirectangular projection (ERP). However, the distortion introduced by the
nonlinear mapping from 360{\deg} image to ERP image is still a barrier that
holds back ERP images from being used as easily as conventional perspective
images. This is especially relevant when estimating 360{\deg} optical flow, as
the distortions need to be mitigated appropriately. In this paper, we propose a
360{\deg} optical flow method based on tangent images. Our method leverages
gnomonic projection to locally convert ERP images to perspective images, and
uniformly samples the ERP image by projection to a cubemap and regular
icosahedron vertices, to incrementally refine the estimated 360{\deg} flow
fields even in the presence of large rotations. Our experiments demonstrate the
benefits of our proposed method both quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Res2NetFuse: A Fusion Method for Infrared and Visible Images. (arXiv:2112.14540v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14540">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel Res2Net-based fusion framework for infrared and
visible images. The proposed fusion model has three parts: an encoder, a fusion
layer and a decoder, respectively. The Res2Net-based encoder is used to extract
multi-scale features of source images, the paper introducing a new training
strategy for training a Res2Net-based encoder that uses only a single image.
Then, a new fusion strategy is developed based on the attention model. Finally,
the fused image is reconstructed by the decoder. The proposed approach is also
analyzed in detail. Experiments show that our method achieves state-of-the-art
fusion performance in objective and subjective assessment by comparing with the
existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When less is more: Simplifying inputs aids neural network understanding. (arXiv:2201.05610v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05610">
<div class="article-summary-box-inner">
<span><p>How do neural network image classifiers respond to simpler and simpler
inputs? And what do such responses reveal about the learning process? To answer
these questions, we need a clear measure of input simplicity (or inversely,
complexity), an optimization objective that correlates with simplification, and
a framework to incorporate such objective into training and inference. Lastly
we need a variety of testbeds to experiment and evaluate the impact of such
simplification on learning. In this work, we measure simplicity with the
encoding bit size given by a pretrained generative model, and minimize the bit
size to simplify inputs in training and inference. We investigate the effect of
such simplification in several scenarios: conventional training, dataset
condensation and post-hoc explanations. In all settings, inputs are simplified
along with the original classification task, and we investigate the trade-off
between input simplicity and task performance. For images with injected
distractors, such simplification naturally removes superfluous information. For
dataset condensation, we find that inputs can be simplified with almost no
accuracy degradation. When used in post-hoc explanation, our learning-based
simplification approach offers a valuable new tool to explore the basis of
network decisions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Specificity in Mammography Using Cross-correlation between Wavelet and Fourier Transform. (arXiv:2201.08385v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08385">
<div class="article-summary-box-inner">
<span><p>Breast cancer is in the most common malignant tumor in women. It accounted
for 30% of new malignant tumor cases. Although the incidence of breast cancer
remains high around the world, the mortality rate has been continuously
reduced. This is mainly due to recent developments in molecular biology
technology and improved level of comprehensive diagnosis and standard
treatment. Early detection by mammography is an integral part of that. The most
common breast abnormalities that may indicate breast cancer are masses and
calcifications. Previous detection approaches usually obtain relatively high
sensitivity but unsatisfactory specificity. We will investigate an approach
that applies the discrete wavelet transform and Fourier transform to parse the
images and extracts statistical features that characterize an image's content,
such as the mean intensity and the skewness of the intensity. A naive Bayesian
classifier uses these features to classify the images. We expect to achieve an
optimal high specificity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Aggregation for Adaptive RGBT Tracking. (arXiv:2201.08949v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08949">
<div class="article-summary-box-inner">
<span><p>Visual object tracking with RGB and thermal infrared (TIR) spectra available,
shorted in RGBT tracking, is a novel and challenging research topic which draws
increasing attention nowadays. In this paper, we propose an RGBT tracker which
takes spatio-temporal clues into account for robust appearance model learning,
and simultaneously, constructs an adaptive fusion sub-network for cross-modal
interactions. Unlike most existing RGBT trackers that implement object tracking
tasks with only spatial information included, temporal information is further
considered in this method. Specifically, different from traditional Siamese
trackers, which only obtain one search image during the process of picking up
template-search image pairs, an extra search sample adjacent to the original
one is selected to predict the temporal transformation, resulting in improved
robustness of tracking performance.As for multi-modal tracking, constrained to
the limited RGBT datasets, the adaptive fusion sub-network is appended to our
method at the decision level to reflect the complementary characteristics
contained in two modalities. To design a thermal infrared assisted RGB tracker,
the outputs of the classification head from the TIR modality are taken into
consideration before the residual connection from the RGB modality. Extensive
experimental results on three challenging datasets, i.e. VOT-RGBT2019, GTOT and
RGBT210, verify the effectiveness of our method. Code will be shared at
\textcolor{blue}{\emph{https://github.com/Zhangyong-Tang/TAAT}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Representation Learning with Self-Supervised Attention for Low-Label High-data Regime. (arXiv:2201.08951v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08951">
<div class="article-summary-box-inner">
<span><p>Self-supervision has shown outstanding results for natural language
processing, and more recently, for image recognition. Simultaneously, vision
transformers and its variants have emerged as a promising and scalable
alternative to convolutions on various computer vision tasks. In this paper, we
are the first to question if self-supervised vision transformers (SSL-ViTs) can
be adapted to two important computer vision tasks in the low-label, high-data
regime: few-shot image classification and zero-shot image retrieval. The
motivation is to reduce the number of manual annotations required to train a
visual embedder, and to produce generalizable and semantically meaningful
embeddings. For few-shot image classification we train SSL-ViTs without any
supervision, on external data, and use this trained embedder to adapt quickly
to novel classes with limited number of labels. For zero-shot image retrieval,
we use SSL-ViTs pre-trained on a large dataset without any labels and fine-tune
them with several metric learning objectives. Our self-supervised attention
representations outperforms the state-of-the-art on several public benchmarks
for both tasks, namely miniImageNet and CUB200 for few-shot image
classification by up-to 6%-10%, and Stanford Online Products, Cars196 and
CUB200 for zero-shot image retrieval by up-to 4%-11%. Code is available at
\url{https://github.com/AutoVision-cloud/SSL-ViT-lowlabel-highdata}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Object Tracking on Multi-modal RGB-D Videos: A Review. (arXiv:2201.09207v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09207">
<div class="article-summary-box-inner">
<span><p>The development of visual object tracking has continued for decades. Recent
years, as the wide accessibility of the low-cost RGBD sensors, the task of
visual object tracking on RGB-D videos has drawn much attention. Compared to
conventional RGB-only tracking, the RGB-D videos can provide more information
that facilitates objecting tracking in some complicated scenarios. The goal of
this review is to summarize the relative knowledge of the research filed of
RGB-D tracking. To be specific, we will generalize the related RGB-D tracking
benchmarking datasets as well as the corresponding performance measurements.
Besides, the existing RGB-D tracking methods are summarized in the paper.
Moreover, we discuss the possible future direction in the field of RGB-D
tracking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face recognition via compact second order image gradient orientations. (arXiv:2201.09246v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09246">
<div class="article-summary-box-inner">
<span><p>Conventional subspace learning approaches based on image gradient
orientations only employ the first-order gradient information. However, recent
researches on human vision system (HVS) uncover that the neural image is a
landscape or a surface whose geometric properties can be captured through the
second order gradient information. The second order image gradient orientations
(SOIGO) can mitigate the adverse effect of noises in face images. To reduce the
redundancy of SOIGO, we propose compact SOIGO (CSOIGO) by applying linear
complex principal component analysis (PCA) in SOIGO. Combined with
collaborative representation based classification (CRC) algorithm, the
classification performance of CSOIGO is further enhanced. CSOIGO is evaluated
under real-world disguise, synthesized occlusion and mixed variations.
Experimental results indicate that the proposed method is superior to its
competing approaches with few training samples, and even outperforms some
prevailing deep neural network based approaches. The source code of CSOIGO is
available at https://github.com/yinhefeng/SOIGO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey for Deep RGBT Tracking. (arXiv:2201.09296v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09296">
<div class="article-summary-box-inner">
<span><p>Visual object tracking with the visible (RGB) and thermal infrared (TIR)
electromagnetic waves, shorted in RGBT tracking, recently draws increasing
attention in the tracking community. Considering the rapid development of deep
learning, a survey for the recent deep neural network based RGBT trackers is
presented in this paper. Firstly, we give brief introduction for the RGBT
trackers concluded into this category. Then, a comparison among the existing
RGBT trackers on several challenging benchmarks is given statistically.
Specifically, MDNet and Siamese architectures are the two mainstream frameworks
in the RGBT community, especially the former. Trackers based on MDNet achieve
higher performance while Siamese-based trackers satisfy the real-time
requirement. In summary, since the large-scale dataset LasHeR is published, the
integration of end-to-end framework, e.g., Siamese and Transformer, should be
further considered to fulfil the real-time as well as more robust performance.
Furthermore, the mathematical meaning should be more considered during
designing the network. This survey can be treated as a look-up-table for
researchers who are concerned about RGBT tracking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hybrid Quantum-Classical Algorithm for Robust Fitting. (arXiv:2201.10110v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10110">
<div class="article-summary-box-inner">
<span><p>Fitting geometric models onto outlier contaminated data is provably
intractable. Many computer vision systems rely on random sampling heuristics to
solve robust fitting, which do not provide optimality guarantees and error
bounds. It is therefore critical to develop novel approaches that can bridge
the gap between exact solutions that are costly, and fast heuristics that offer
no quality assurances. In this paper, we propose a hybrid quantum-classical
algorithm for robust fitting. Our core contribution is a novel robust fitting
formulation that solves a sequence of integer programs and terminates with a
global solution or an error bound. The combinatorial subproblems are amenable
to a quantum annealer, which helps to tighten the bound efficiently. While our
usage of quantum computing does not surmount the fundamental intractability of
robust fitting, by providing error bounds our algorithm is a practical
improvement over randomised heuristics. Moreover, our work represents a
concrete application of quantum computing in computer vision. We present
results obtained using an actual quantum computer (D-Wave Advantage) and via
simulation. Source code: https://github.com/dadung/HQC-robust-fitting
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MSNet: A Deep Multi-scale Submanifold Network for Visual Classification. (arXiv:2201.10145v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10145">
<div class="article-summary-box-inner">
<span><p>The Symmetric Positive Definite (SPD) matrix has received wide attention as a
tool for visual data representation in computer vision. Although there are many
different attempts to develop effective deep architectures for data processing
on the Riemannian manifold of SPD matrices, a very few solutions explicitly
mine the local geometrical information in deep SPD feature representations.
While CNNs have demonstrated the potential of hierarchical local pattern
extraction even for SPD represented data, we argue that it is of utmost
importance to ensure the preservation of local geometric information in the SPD
networks. Accordingly, in this work we propose an SPD network designed with
this objective in mind. In particular, we propose an architecture, referred to
as MSNet, which fuses geometrical multi-scale information. We first analyse the
convolution operator commonly used for mapping the local information in
Euclidean deep networks from the perspective of a higher level of abstraction
afforded by the Category Theory. Based on this analysis, we postulate a
submanifold selection principle to guide the design of our MSNet. In
particular, we use it to design a submanifold fusion block to take advantage of
the rich local geometry encoded in the network layers. The experiments
involving multiple visual tasks show that our algorithm outperforms most
Riemannian SOTA competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Image Fusion Method based on Feature Mutual Mapping. (arXiv:2201.10152v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10152">
<div class="article-summary-box-inner">
<span><p>Deep learning-based image fusion approaches have obtained wide attention in
recent years, achieving promising performance in terms of visual perception.
However, the fusion module in the current deep learning-based methods suffers
from two limitations, \textit{i.e.}, manually designed fusion function, and
input-independent network learning. In this paper, we propose an unsupervised
adaptive image fusion method to address the above issues. We propose a feature
mutual mapping fusion module and dual-branch multi-scale autoencoder. More
specifically, we construct a global map to measure the connections of pixels
between the input source images. % The found mapping relationship guides the
image fusion. Besides, we design a dual-branch multi-scale network through
sampling transformation to extract discriminative image features. We further
enrich feature representations of different scales through feature aggregation
in the decoding process. Finally, we propose a modified loss function to train
the network with efficient convergence property. Through sufficient training on
infrared and visible image data sets, our method also shows excellent
generalized performance in multi-focus and medical image fusion. Our method
achieves superior performance in both visual perception and objective
evaluation. Experiments prove that the performance of our proposed method on a
variety of image fusion tasks surpasses other state-of-the-art methods, proving
the effectiveness and versatility of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains. (arXiv:2201.11528v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11528">
<div class="article-summary-box-inner">
<span><p>Adversarial examples have posed a severe threat to deep neural networks due
to their transferable nature. Currently, various works have paid great efforts
to enhance the cross-model transferability, which mostly assume the substitute
model is trained in the same domain as the target model. However, in reality,
the relevant information of the deployed model is unlikely to leak. Hence, it
is vital to build a more practical black-box threat model to overcome this
limitation and evaluate the vulnerability of deployed models. In this paper,
with only the knowledge of the ImageNet domain, we propose a Beyond ImageNet
Attack (BIA) to investigate the transferability towards black-box domains
(unknown classification tasks). Specifically, we leverage a generative model to
learn the adversarial function for disrupting low-level features of input
images. Based on this framework, we further propose two variants to narrow the
gap between the source and target domains from the data and model perspectives,
respectively. Extensive experiments on coarse-grained and fine-grained domains
demonstrate the effectiveness of our proposed methods. Notably, our methods
outperform state-of-the-art approaches by up to 7.71\% (towards coarse-grained
domains) and 25.91\% (towards fine-grained domains) on average. Our code is
available at \url{https://github.com/qilong-zhang/Beyond-ImageNet-Attack}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Checklist: Towards Testable Error Analysis of Image Models to Help System Designers Interrogate Model Capabilities. (arXiv:2201.11674v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11674">
<div class="article-summary-box-inner">
<span><p>Using large pre-trained models for image recognition tasks is becoming
increasingly common owing to the well acknowledged success of recent models
like vision transformers and other CNN-based models like VGG and Resnet. The
high accuracy of these models on benchmark tasks has translated into their
practical use across many domains including safety-critical applications like
autonomous driving and medical diagnostics. Despite their widespread use, image
models have been shown to be fragile to changes in the operating environment,
bringing their robustness into question. There is an urgent need for methods
that systematically characterise and quantify the capabilities of these models
to help designers understand and provide guarantees about their safety and
robustness. In this paper, we propose Vision Checklist, a framework aimed at
interrogating the capabilities of a model in order to produce a report that can
be used by a system designer for robustness evaluations. This framework
proposes a set of perturbation operations that can be applied on the underlying
data to generate test samples of different types. The perturbations reflect
potential changes in operating environments, and interrogate various properties
ranging from the strictly quantitative to more qualitative. Our framework is
evaluated on multiple datasets like Tinyimagenet, CIFAR10, CIFAR100 and
Camelyon17 and for models like ViT and Resnet. Our Vision Checklist proposes a
specific set of evaluations that can be integrated into the previously proposed
concept of a model card. Robustness evaluations like our checklist will be
crucial in future safety evaluations of visual perception modules, and be
useful for a wide range of stakeholders including designers, deployers, and
regulators involved in the certification of these systems. Source code of
Vision Checklist would be open for public use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural JPEG: End-to-End Image Compression Leveraging a Standard JPEG Encoder-Decoder. (arXiv:2201.11795v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11795">
<div class="article-summary-box-inner">
<span><p>Recent advances in deep learning have led to superhuman performance across a
variety of applications. Recently, these methods have been successfully
employed to improve the rate-distortion performance in the task of image
compression. However, current methods either use additional post-processing
blocks on the decoder end to improve compression or propose an end-to-end
compression scheme based on heuristics. For the majority of these, the trained
deep neural networks (DNNs) are not compatible with standard encoders and would
be difficult to deply on personal computers and cellphones. In light of this,
we propose a system that learns to improve the encoding performance by
enhancing its internal neural representations on both the encoder and decoder
ends, an approach we call Neural JPEG. We propose frequency domain pre-editing
and post-editing methods to optimize the distribution of the DCT coefficients
at both encoder and decoder ends in order to improve the standard compression
(JPEG) method. Moreover, we design and integrate a scheme for jointly learning
quantization tables within this hybrid neural compression framework.Experiments
demonstrate that our approach successfully improves the rate-distortion
performance over JPEG across various quality metrics, such as PSNR and MS-SSIM,
and generates visually appealing images with better color retention quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computer-aided Recognition and Assessment of a Porous Bioelastomer on Ultrasound Images for Regenerative Medicine Applications. (arXiv:2201.11987v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11987">
<div class="article-summary-box-inner">
<span><p>Biodegradable elastic scaffolds have attracted more and more attention in the
field of soft tissue repair and tissue engineering. These scaffolds made of
porous bioelastomers support tissue ingrowth along with their own degradation.
It is necessary to develop a computer-aided analyzing method based on
ultrasound images to identify the degradation performance of the scaffold, not
only to obviate the need to do destructive testing, but also to monitor the
scaffold's degradation and tissue ingrowth over time. It is difficult using a
single traditional image processing algorithm to extract continuous and
accurate contour of a porous bioelastomer. This paper proposes a joint
algorithm for the bioelastomer's contour detection and a texture feature
extraction method for monitoring the degradation behavior of the bioelastomer.
Mean-shift clustering method is used to obtain the bioelastomer's and native
tissue's clustering feature information. Then the OTSU image binarization
method automatically selects the optimal threshold value to convert the
grayscale ultrasound image into a binary image. The Canny edge detector is used
to extract the complete bioelastomer's contour. The first-order and
second-order statistical features of texture are extracted. The proposed joint
algorithm not only achieves the ideal extraction of the bioelastomer's contours
in ultrasound images, but also gives valuable feedback of the degradation
behavior of the bioelastomer at the implant site based on the changes of
texture characteristics and contour area. The preliminary results of this study
suggest that the proposed computer-aided image processing techniques have
values and potentials in the non-invasive analysis of tissue scaffolds in vivo
based on ultrasound images and may help tissue engineers evaluate the tissue
scaffold's degradation and cellular ingrowth progress and improve the scaffold
designs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Single-shot Depth Estimation using Perceptual Reconstruction. (arXiv:2201.12170v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12170">
<div class="article-summary-box-inner">
<span><p>Real-time estimation of actual object depth is a module that is essential to
performing various autonomous system tasks such as 3D reconstruction, scene
understanding and condition assessment of machinery parts. During the last
decade of machine learning, extensive deployment of deep learning methods to
computer vision tasks has yielded approaches that succeed in achieving
realistic depth synthesis out of a simple RGB modality. While most of these
models are based on paired depth data or availability of video sequences and
stereo images, methods for single-view depth synthesis in a fully unsupervised
setting have hardly been explored. This study presents the most recent advances
in the field of generative neural networks, leveraging them to perform fully
unsupervised single-shot depth synthesis. Two generators for RGB-to-depth and
depth-to-RGB transfer are implemented and simultaneously optimized using the
Wasserstein-1 distance and a novel perceptual reconstruction term. To ensure
that the proposed method is plausible, we comprehensively evaluate the models
using industrial surface depth data as well as the Texas 3D Face Recognition
Database and the SURREAL dataset that records body depth. The success observed
in this study suggests the great potential for unsupervised single-shot depth
estimation in real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Learning Music Composition and Dance Choreography. (arXiv:2201.11999v1 [cs.SD] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11999">
<div class="article-summary-box-inner">
<span><p>Music and dance have always co-existed as pillars of human activities,
contributing immensely to the cultural, social, and entertainment functions in
virtually all societies. Notwithstanding the gradual systematization of music
and dance into two independent disciplines, their intimate connection is
undeniable and one art-form often appears incomplete without the other. Recent
research works have studied generative models for dance sequences conditioned
on music. The dual task of composing music for given dances, however, has been
largely overlooked. In this paper, we propose a novel extension, where we
jointly model both tasks in a dual learning approach. To leverage the duality
of the two modalities, we introduce an optimal transport objective to align
feature embeddings, as well as a cycle consistency loss to foster overall
consistency. Experimental results demonstrate that our dual learning framework
improves individual task performance, delivering generated music compositions
and dance choreographs that are realistic and faithful to the conditioned
inputs.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-02-01 23:06:42.665401470 UTC">2022-02-01 23:06:42 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>