<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-06-06T01:30:00Z">06-06</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">MMTM: Multi-Tasking Multi-Decoder Transformer for Math Word Problems. (arXiv:2206.01268v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01268">
<div class="article-summary-box-inner">
<span><p>Recently, quite a few novel neural architectures were derived to solve math
word problems by predicting expression trees. These architectures varied from
seq2seq models, including encoders leveraging graph relationships combined with
tree decoders. These models achieve good performance on various MWPs datasets
but perform poorly when applied to an adversarial challenge dataset, SVAMP. We
present a novel model MMTM that leverages multi-tasking and multi-decoder
during pre-training. It creates variant tasks by deriving labels using
pre-order, in-order and post-order traversal of expression trees, and uses
task-specific decoders in a multi-tasking framework. We leverage transformer
architectures with lower dimensionality and initialize weights from RoBERTa
model. MMTM model achieves better mathematical reasoning ability and
generalisability, which we demonstrate by outperforming the best state of the
art baseline models from Seq2Seq, GTS, and Graph2Tree with a relative
improvement of 19.4% on an adversarial challenge dataset SVAMP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multiset Version of Even-Odd Permutations Identity. (arXiv:2206.01291v1 [math.CO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01291">
<div class="article-summary-box-inner">
<span><p>In this paper, we give a new bijective proof of a multiset analogue of
even-odd permutations identity. This multiset version is equivalent to the
original coin arrangements lemma which is a key combinatorial lemma in the
Sherman's Proof of a conjecture of Feynman about an identity on paths in planar
graphs related to combinatorial solution of two dimensional Ising model in
statistical physics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmenting Scientific Creativity with Retrieval across Knowledge Domains. (arXiv:2206.01328v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01328">
<div class="article-summary-box-inner">
<span><p>Exposure to ideas in domains outside a scientist's own may benefit her in
reformulating existing research problems in novel ways and discovering new
application domains for existing solution ideas. While improved performance in
scholarly search engines can help scientists efficiently identify relevant
advances in domains they may already be familiar with, it may fall short of
helping them explore diverse ideas \textit{outside} such domains. In this paper
we explore the design of systems aimed at augmenting the end-user ability in
cross-domain exploration with flexible query specification. To this end, we
develop an exploratory search system in which end-users can select a portion of
text core to their interest from a paper abstract and retrieve papers that have
a high similarity to the user-selected core aspect but differ in terms of
domains. Furthermore, end-users can `zoom in' to specific domain clusters to
retrieve more papers from them and understand nuanced differences within the
clusters. Our case studies with scientists uncover opportunities and design
implications for systems aimed at facilitating cross-domain exploration and
inspiration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plumber: A Modular Framework to Create Information Extraction Pipelines. (arXiv:2206.01442v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01442">
<div class="article-summary-box-inner">
<span><p>Information Extraction (IE) tasks are commonly studied topics in various
domains of research. Hence, the community continuously produces multiple
techniques, solutions, and tools to perform such tasks. However, running those
tools and integrating them within existing infrastructure requires time,
expertise, and resources. One pertinent task here is triples extraction and
linking, where structured triples are extracted from a text and aligned to an
existing Knowledge Graph (KG). In this paper, we present PLUMBER, the first
framework that allows users to manually and automatically create suitable IE
pipelines from a community-created pool of tools to perform triple extraction
and alignment on unstructured text. Our approach provides an interactive medium
to alter the pipelines and perform IE tasks. A short video to show the working
of the framework for different use-cases is available online under:
https://www.youtube.com/watch?v=XC9rJNIUv8g
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Adaptive Pre-Training for Boosting Learning With Noisy Labels: A Study on Text Classification for African Languages. (arXiv:2206.01476v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01476">
<div class="article-summary-box-inner">
<span><p>For high-resource languages like English, text classification is a
well-studied task. The performance of modern NLP models easily achieves an
accuracy of more than 90% in many standard datasets for text classification in
English (Xie et al., 2019; Yang et al., 2019; Zaheer et al., 2020). However,
text classification in low-resource languages is still challenging due to the
lack of annotated data. Although methods like weak supervision and
crowdsourcing can help ease the annotation bottleneck, the annotations obtained
by these methods contain label noise. Models trained with label noise may not
generalize well. To this end, a variety of noise-handling techniques have been
proposed to alleviate the negative impact caused by the errors in the
annotations (for extensive surveys see (Hedderich et al., 2021; Algan &amp; Ulusoy,
2021)). In this work, we experiment with a group of standard noisy-handling
methods on text classification tasks with noisy labels. We study both simulated
noise and realistic noise induced by weak supervision. Moreover, we find
task-adaptive pre-training techniques (Gururangan et al., 2020) are beneficial
for learning with noisy labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Topology Induction for Understanding Contextualized Representations. (arXiv:2206.01512v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01512">
<div class="article-summary-box-inner">
<span><p>In this work, we study the representation space of contextualized embeddings
and gain insight into the hidden topology of large language models. We show
there exists a network of latent states that summarize linguistic properties of
contextualized representations. Instead of seeking alignments to existing
well-defined annotations, we infer this latent network in a fully unsupervised
way using a structured variational autoencoder. The induced states not only
serve as anchors that mark the topology (neighbors and connectivity) of the
representation manifold but also reveal the internal mechanism of encoding
sentences. With the induced network, we: (1). decompose the representation
space into a spectrum of latent states which encode fine-grained word meanings
with lexical, morphological, syntactic and semantic information; (2). show
state-state transitions encode rich phrase constructions and serve as the
backbones of the latent space. Putting the two together, we show that sentences
are represented as a traversal over the latent network where state-state
transition chains encode syntactic templates and state-word emissions fill in
the content. We demonstrate these insights with extensive experiments and
visualizations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Acquiring and Modelling Abstract Commonsense Knowledge via Conceptualization. (arXiv:2206.01532v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01532">
<div class="article-summary-box-inner">
<span><p>Conceptualization, or viewing entities and situations as instances of
abstract concepts in mind and making inferences based on that, is a vital
component in human intelligence for commonsense reasoning. Although recent
artificial intelligence has made progress in acquiring and modelling
commonsense, attributed to large neural language models and commonsense
knowledge graphs (CKGs), conceptualization is yet to thoroughly be introduced,
making current approaches ineffective to cover knowledge about countless
diverse entities and situations in the real world. To address the problem, we
thoroughly study the possible role of conceptualization in commonsense
reasoning, and formulate a framework to replicate human conceptual induction
from acquiring abstract knowledge about abstract concepts. Aided by the
taxonomy Probase, we develop tools for contextualized conceptualization on
ATOMIC, a large-scale human annotated CKG. We annotate a dataset for the
validity of conceptualizations for ATOMIC on both event and triple level,
develop a series of heuristic rules based on linguistic features, and train a
set of neural models, so as to generate and verify abstract knowledge. Based on
these components, a pipeline to acquire abstract knowledge is built. A large
abstract CKG upon ATOMIC is then induced, ready to be instantiated to infer
about unseen entities or situations. Furthermore, experiments find directly
augmenting data with abstract triples to be helpful in commonsense modelling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Opinion Mining: Summarizing Opinions of Customer Reviews. (arXiv:2206.01543v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01543">
<div class="article-summary-box-inner">
<span><p>Customer reviews are vital for making purchasing decisions in the Information
Age. Such reviews can be automatically summarized to provide the user with an
overview of opinions. In this tutorial, we present various aspects of opinion
summarization that are useful for researchers and practitioners. First, we will
introduce the task and major challenges. Then, we will present existing opinion
summarization solutions, both pre-neural and neural. We will discuss how
summarizers can be trained in the unsupervised, few-shot, and supervised
regimes. Each regime has roots in different machine learning methods, such as
auto-encoding, controllable text generation, and variational inference.
Finally, we will discuss resources and evaluation methods and conclude with the
future directions. This three-hour tutorial will provide a comprehensive
overview over major advances in opinion summarization. The listeners will be
well-equipped with the knowledge that is both useful for research and practical
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TCE at Qur'an QA 2022: Arabic Language Question Answering Over Holy Qur'an Using a Post-Processed Ensemble of BERT-based Models. (arXiv:2206.01550v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01550">
<div class="article-summary-box-inner">
<span><p>In recent years, we witnessed great progress in different tasks of natural
language understanding using machine learning. Question answering is one of
these tasks which is used by search engines and social media platforms for
improved user experience. Arabic is the language of the Holy Qur'an; the sacred
text for 1.8 billion people across the world. Arabic is a challenging language
for Natural Language Processing (NLP) due to its complex structures. In this
article, we describe our attempts at OSACT5 Qur'an QA 2022 Shared Task, which
is a question answering challenge on the Holy Qur'an in Arabic. We propose an
ensemble learning model based on Arabic variants of BERT models. In addition,
we perform post-processing to enhance the model predictions. Our system
achieves a Partial Reciprocal Rank (pRR) score of 56.6% on the official test
set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Findings of the The RuATD Shared Task 2022 on Artificial Text Detection in Russian. (arXiv:2206.01583v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01583">
<div class="article-summary-box-inner">
<span><p>We present the shared task on artificial text detection in Russian, which is
organized as a part of the Dialogue Evaluation initiative, held in 2022. The
shared task dataset includes texts from 14 text generators, i.e., one human
writer and 13 text generative models fine-tuned for one or more of the
following generation tasks: machine translation, paraphrase generation, text
summarization, text simplification. We also consider back-translation and
zero-shot generation approaches. The human-written texts are collected from
publicly available resources across multiple domains. The shared task consists
of two sub-tasks: (i) to determine if a given text is automatically generated
or written by a human; (ii) to identify the author of a given text. The first
task is framed as a binary classification problem. The second task is a
multi-class classification problem. We provide count-based and BERT-based
baselines, along with the human evaluation on the first sub-task. A total of 30
and 8 systems have been submitted to the binary and multi-class sub-tasks,
correspondingly. Most teams outperform the baselines by a wide margin. We
publicly release our codebase, human evaluation results, and other materials in
our GitHub repository (https://github.com/dialogue-evaluation/RuATD).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Similar Questions From Naturally-occurring Business Conversations. (arXiv:2206.01585v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01585">
<div class="article-summary-box-inner">
<span><p>Pre-trained contextualized embedding models such as BERT are a standard
building block in many natural language processing systems. We demonstrate that
the sentence-level representations produced by some off-the-shelf
contextualized embedding models have a narrow distribution in the embedding
space, and thus perform poorly for the task of identifying semantically similar
questions in real-world English business conversations. We describe a method
that uses appropriately tuned representations and a small set of exemplars to
group questions of interest to business users in a visualization that can be
used for data exploration or employee coaching.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ArgRewrite V.2: an Annotated Argumentative Revisions Corpus. (arXiv:2206.01677v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01677">
<div class="article-summary-box-inner">
<span><p>Analyzing how humans revise their writings is an interesting research
question, not only from an educational perspective but also in terms of
artificial intelligence. Better understanding of this process could facilitate
many NLP applications, from intelligent tutoring systems to supportive and
collaborative writing environments. Developing these applications, however,
requires revision corpora, which are not widely available. In this work, we
present ArgRewrite V.2, a corpus of annotated argumentative revisions,
collected from two cycles of revisions to argumentative essays about
self-driving cars. Annotations are provided at different levels of purpose
granularity (coarse and fine) and scope (sentential and subsentential). In
addition, the corpus includes the revision goal given to each writer, essay
scores, annotation verification, pre- and post-study surveys collected from
participants as meta-data. The variety of revision unit scope and purpose
granularity levels in ArgRewrite, along with the inclusion of new types of
meta-data, can make it a useful resource for research and applications that
involve revision analysis. We demonstrate some potential applications of
ArgRewrite V.2 in the development of automatic revision purpose predictors, as
a training source and benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward a realistic model of speech processing in the brain with self-supervised learning. (arXiv:2206.01685v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01685">
<div class="article-summary-box-inner">
<span><p>Several deep neural networks have recently been shown to generate activations
similar to those of the brain in response to the same input. These algorithms,
however, remain largely implausible: they require (1) extraordinarily large
amounts of data, (2) unobtainable supervised labels, (3) textual rather than
raw sensory input, and / or (4) implausibly large memory (e.g. thousands of
contextual words). These elements highlight the need to identify algorithms
that, under these limitations, would suffice to account for both behavioral and
brain responses. Focusing on the issue of speech processing, we here
hypothesize that self-supervised algorithms trained on the raw waveform
constitute a promising candidate. Specifically, we compare a recent
self-supervised architecture, Wav2Vec 2.0, to the brain activity of 412
English, French, and Mandarin individuals recorded with functional Magnetic
Resonance Imaging (fMRI), while they listened to ~1h of audio books. Our
results are four-fold. First, we show that this algorithm learns brain-like
representations with as little as 600 hours of unlabelled speech -- a quantity
comparable to what infants can be exposed to during language acquisition.
Second, its functional hierarchy aligns with the cortical hierarchy of speech
processing. Third, different training regimes reveal a functional
specialization akin to the cortex: Wav2Vec 2.0 learns sound-generic,
speech-specific and language-specific representations similar to those of the
prefrontal and temporal cortices. Fourth, we confirm the similarity of this
specialization with the behavior of 386 additional participants. These
elements, resulting from the largest neuroimaging benchmark to date, show how
self-supervised learning can account for a rich organization of speech
processing in the brain, and thus delineate a path to identify the laws of
language acquisition which shape the human brain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Gender Bias in Word Embeddings of Gendered Languages Requires Disentangling Grammatical Gender Signals. (arXiv:2206.01691v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01691">
<div class="article-summary-box-inner">
<span><p>Does the grammatical gender of a language interfere when measuring the
semantic gender information captured by its word embeddings? A number of
anomalous gender bias measurements in the embeddings of gendered languages
suggest this possibility. We demonstrate that word embeddings learn the
association between a noun and its grammatical gender in grammatically gendered
languages, which can skew social gender bias measurements. Consequently, word
embedding post-processing methods are introduced to quantify, disentangle, and
evaluate grammatical gender signals. The evaluation is performed on five
gendered languages from the Germanic, Romance, and Slavic branches of the
Indo-European language family. Our method reduces the strength of grammatical
gender signals, which is measured in terms of effect size (Cohen's d), by a
significant average of d = 1.3 for French, German, and Italian, and d = 0.56
for Polish and Spanish. Once grammatical gender is disentangled, the
association between over 90% of 10,000 inanimate nouns and their assigned
grammatical gender weakens, and cross-lingual bias results from the Word
Embedding Association Test (WEAT) become more congruent with country-level
implicit bias measurements. The results further suggest that disentangling
grammatical gender signals from word embeddings may lead to improvement in
semantic machine learning tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge. (arXiv:2206.01718v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01718">
<div class="article-summary-box-inner">
<span><p>The Visual Question Answering (VQA) task aspires to provide a meaningful
testbed for the development of AI models that can jointly reason over visual
and natural language inputs. Despite a proliferation of VQA datasets, this goal
is hindered by a set of common limitations. These include a reliance on
relatively simplistic questions that are repetitive in both concepts and
linguistic structure, little world knowledge needed outside of the paired
image, and limited reasoning required to arrive at the correct answer. We
introduce A-OKVQA, a crowdsourced dataset composed of a diverse set of about
25K questions requiring a broad base of commonsense and world knowledge to
answer. In contrast to the existing knowledge-based VQA datasets, the questions
generally cannot be answered by simply querying a knowledge base, and instead
require some form of commonsense reasoning about the scene depicted in the
image. We demonstrate the potential of this new dataset through a detailed
analysis of its contents and baseline performance measurements over a variety
of state-of-the-art vision-language models. Project page:
<a href="http://a-okvqa.allenai.org/">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting the "Video" in Video-Language Understanding. (arXiv:2206.01720v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01720">
<div class="article-summary-box-inner">
<span><p>What makes a video task uniquely suited for videos, beyond what can be
understood from a single image? Building on recent progress in self-supervised
image-language models, we revisit this question in the context of video and
language tasks. We propose the atemporal probe (ATP), a new model for
video-language analysis which provides a stronger bound on the baseline
accuracy of multimodal models constrained by image-level understanding. By
applying this model to standard discriminative video and language tasks, such
as video question answering and text-to-video retrieval, we characterize the
limitations and potential of current video-language benchmarks. We find that
understanding of event temporality is often not necessary to achieve strong or
state-of-the-art performance, even compared with recent large-scale
video-language models and in contexts intended to benchmark deeper video-level
understanding. We also demonstrate how ATP can improve both video-language
dataset and model design. We describe a technique for leveraging ATP to better
disentangle dataset subsets with a higher concentration of temporally
challenging data, improving benchmarking efficacy for causal and temporal
understanding. Further, we show that effectively integrating ATP into full
video-level temporal models can improve efficiency and state-of-the-art
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The geometry of integration in text classification RNNs. (arXiv:2010.15114v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.15114">
<div class="article-summary-box-inner">
<span><p>Despite the widespread application of recurrent neural networks (RNNs) across
a variety of tasks, a unified understanding of how RNNs solve these tasks
remains elusive. In particular, it is unclear what dynamical patterns arise in
trained RNNs, and how those patterns depend on the training dataset or task.
This work addresses these questions in the context of a specific natural
language processing task: text classification. Using tools from dynamical
systems analysis, we study recurrent networks trained on a battery of both
natural and synthetic text classification tasks. We find the dynamics of these
trained RNNs to be both interpretable and low-dimensional. Specifically, across
architectures and datasets, RNNs accumulate evidence for each class as they
process the text, using a low-dimensional attractor manifold as the underlying
mechanism. Moreover, the dimensionality and geometry of the attractor manifold
are determined by the structure of the training dataset; in particular, we
describe how simple word-count statistics computed on the training dataset can
be used to predict these properties. Our observations span multiple
architectures and datasets, reflecting a common mechanism RNNs employ to
perform text classification. To the degree that integration of evidence towards
a decision is a common computational primitive, this work lays the foundation
for using dynamical systems techniques to study the inner workings of RNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BlonDe: An Automatic Evaluation Metric for Document-level Machine Translation. (arXiv:2103.11878v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11878">
<div class="article-summary-box-inner">
<span><p>Standard automatic metrics, e.g. BLEU, are not reliable for document-level MT
evaluation. They can neither distinguish document-level improvements in
translation quality from sentence-level ones, nor identify the discourse
phenomena that cause context-agnostic translations. This paper introduces a
novel automatic metric BlonDe to widen the scope of automatic MT evaluation
from sentence to document level. BlonDe takes discourse coherence into
consideration by categorizing discourse-related spans and calculating the
similarity-based F1 measure of categorized spans. We conduct extensive
comparisons on a newly constructed dataset BWB. The experimental results show
that BlonDe possesses better selectivity and interpretability at the
document-level, and is more sensitive to document-level nuances. In a
large-scale human study, BlonDe also achieves significantly higher Pearson's r
correlation with human judgments compared to previous metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Token Pruning for Transformers. (arXiv:2107.00910v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00910">
<div class="article-summary-box-inner">
<span><p>Deploying transformer models in practice is challenging due to their
inference cost, which scales quadratically with input sequence length. To
address this, we present a novel Learned Token Pruning (LTP) method which
adaptively removes unimportant tokens as an input sequence passes through
transformer layers. In particular, LTP prunes tokens with an attention score
below a threshold value which is learned for each layer during training. Our
threshold-based method allows the length of the pruned sequence to vary
adaptively based on the input sequence, and avoids algorithmically expensive
operations such as top-k token selection. We extensively test the performance
of LTP on GLUE tasks and show that our method outperforms the prior
state-of-the-art token pruning methods by up to ~2.5% higher accuracy with the
same amount of FLOPs. In particular, LTP achieves up to 2.1x FLOPs reduction
with less than 1% accuracy drop, which results in up to 1.9x and 2.0x
throughput improvement on Intel Haswell CPUs and NVIDIA V100 GPUs,
respectively. Furthermore, we demonstrate that LTP is more robust than prior
methods to variations on input sentence lengths. Our code has been developed in
PyTorch and has been open-sourced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing. (arXiv:2110.07572v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07572">
<div class="article-summary-box-inner">
<span><p>Semantic parsing is the task of producing a structured meaning representation
for natural language utterances or questions. Recent research has pointed out
that the commonly-used sequence-to-sequence (seq2seq) semantic parsers struggle
to generalize systematically, i.e. to handle examples that require recombining
known knowledge in novel settings. In this work, we show that better systematic
generalization can be achieved by producing the meaning representation (MR)
directly as a graph and not as a sequence. To this end we propose LAGr, the
Labeling Aligned Graphs algorithm that produces semantic parses by predicting
node and edge labels for a complete multi-layer input-aligned graph. The
strongly-supervised LAGr algorithm requires aligned graphs as inputs, whereas
weakly-supervised LAGr infers alignments for originally unaligned target graphs
using an approximate MAP inference procedure. On the COGS and CFQ compositional
generalization benchmarks the strongly- and weakly- supervised LAGr algorithms
achieve significant improvements upon the baseline seq2seq parsers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OntoProtein: Protein Pretraining With Gene Ontology Embedding. (arXiv:2201.11147v6 [q-bio.BM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11147">
<div class="article-summary-box-inner">
<span><p>Self-supervised protein language models have proved their effectiveness in
learning the proteins representations. With the increasing computational power,
current protein language models pre-trained with millions of diverse sequences
can advance the parameter scale from million-level to billion-level and achieve
remarkable improvement. However, those prevailing approaches rarely consider
incorporating knowledge graphs (KGs), which can provide rich structured
knowledge facts for better protein representations. We argue that informative
biology knowledge in KGs can enhance protein representation with external
knowledge. In this work, we propose OntoProtein, the first general framework
that makes use of structure in GO (Gene Ontology) into protein pre-training
models. We construct a novel large-scale knowledge graph that consists of GO
and its related proteins, and gene annotation texts or protein sequences
describe all nodes in the graph. We propose novel contrastive learning with
knowledge-aware negative sampling to jointly optimize the knowledge graph and
protein embedding during pre-training. Experimental results show that
OntoProtein can surpass state-of-the-art methods with pre-trained protein
language models in TAPE benchmark and yield better performance compared with
baselines in protein-protein interaction and protein function prediction. Code
and datasets are available in https://github.com/zjunlp/OntoProtein.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JARVix at SemEval-2022 Task 2: It Takes One to Know One? Idiomaticity Detection using Zero and One Shot Learning. (arXiv:2202.02394v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02394">
<div class="article-summary-box-inner">
<span><p>Large Language Models have been successful in a wide variety of Natural
Language Processing tasks by capturing the compositionality of the text
representations. In spite of their great success, these vector representations
fail to capture meaning of idiomatic multi-word expressions (MWEs). In this
paper, we focus on the detection of idiomatic expressions by using binary
classification. We use a dataset consisting of the literal and idiomatic usage
of MWEs in English and Portuguese. Thereafter, we perform the classification in
two different settings: zero shot and one shot, to determine if a given
sentence contains an idiom or not. N shot classification for this task is
defined by N number of common idioms between the training and testing sets. In
this paper, we train multiple Large Language Models in both the settings and
achieve an F1 score (macro) of 0.73 for the zero shot setting and an F1 score
(macro) of 0.85 for the one shot setting. An implementation of our work can be
found at
https://github.com/ashwinpathak20/Idiomaticity_Detection_Using_Few_Shot_Learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep neural networks for fine-grained surveillance of overdose mortality. (arXiv:2202.12448v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12448">
<div class="article-summary-box-inner">
<span><p>Surveillance of drug overdose deaths relies on death certificates for
identification of the substances that caused death. Drugs and drug classes can
be identified through the International Classification of Diseases, 10th
Revision (ICD-10) codes present on death certificates. However, ICD-10 codes do
not always provide high levels of specificity in drug identification. To
achieve more fine-grained identification of substances on a death certificate,
the free-text cause of death section, completed by the medical certifier, must
be analyzed. Current methods for analyzing free-text death certificates rely
solely on look-up tables for identifying specific substances, which must be
frequently updated and maintained. To improve identification of drugs on death
certificates, a deep learning named-entity recognition model was developed,
which achieved an F1-score of 99.13%. This model can identify new drug
misspellings and novel substances that are not present on current surveillance
look-up tables, enhancing the surveillance of drug overdose deaths.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioADAPT-MRC: Adversarial Learning-based Domain Adaptation Improves Biomedical Machine Reading Comprehension Task. (arXiv:2202.13174v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13174">
<div class="article-summary-box-inner">
<span><p>Biomedical machine reading comprehension (biomedical-MRC) aims to comprehend
complex biomedical narratives and assist healthcare professionals in retrieving
information from them. The high performance of modern neural network-based MRC
systems depends on high-quality, large-scale, human-annotated training
datasets. In the biomedical domain, a crucial challenge in creating such
datasets is the requirement for domain knowledge, inducing the scarcity of
labeled data and the need for transfer learning from the labeled
general-purpose (source) domain to the biomedical (target) domain. However,
there is a discrepancy in marginal distributions between the general-purpose
and biomedical domains due to the variances in topics. Therefore,
direct-transferring of learned representations from a model trained on a
general-purpose domain to the biomedical domain can hurt the model's
performance. We present an adversarial learning-based domain adaptation
framework for the biomedical machine reading comprehension task (BioADAPT-MRC),
a neural network-based method to address the discrepancies in the marginal
distributions between the general and biomedical domain datasets. BioADAPT-MRC
relaxes the need for generating pseudo labels for training a well-performing
biomedical-MRC model. We extensively evaluate the performance of BioADAPT-MRC
by comparing it with the best existing methods on three widely used benchmark
biomedical-MRC datasets -- BioASQ-7b, BioASQ-8b, and BioASQ-9b. Our results
suggest that without using any synthetic or human-annotated data from the
biomedical domain, BioADAPT-MRC can achieve state-of-the-art performance on
these datasets. Availability: BioADAPT-MRC is freely available as an
open-source project at \url{https://github.com/mmahbub/BioADAPT-MRC}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RigoBERTa: A State-of-the-Art Language Model For Spanish. (arXiv:2205.10233v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10233">
<div class="article-summary-box-inner">
<span><p>This paper presents RigoBERTa, a State-of-the-Art Language Model for Spanish.
RigoBERTa is trained over a well-curated corpus formed up from different
subcorpora with key features. It follows the DeBERTa architecture, which has
several advantages over other architectures of similar size as BERT or RoBERTa.
RigoBERTa performance is assessed over 13 NLU tasks in comparison with other
available Spanish language models, namely, MarIA, BERTIN and BETO. RigoBERTa
outperformed the three models in 10 out of the 13 tasks, achieving new
"State-of-the-Art" results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linear Connectivity Reveals Generalization Strategies. (arXiv:2205.12411v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12411">
<div class="article-summary-box-inner">
<span><p>It is widely accepted in the mode connectivity literature that when two
neural networks are trained similarly on the same data, they are connected by a
path through parameter space over which test set accuracy is maintained. Under
some circumstances, including transfer learning from pretrained models, these
paths are presumed to be linear. In contrast to existing results, we find that
among text classifiers (trained on MNLI, QQP, and CoLA), some pairs of
finetuned models have large barriers of increasing loss on the linear paths
between them. On each task, we find distinct clusters of models which are
linearly connected on the test loss surface, but are disconnected from models
outside the cluster -- models that occupy separate basins on the surface. By
measuring performance on specially-crafted diagnostic datasets, we find that
these clusters correspond to different generalization strategies: one cluster
behaves like a bag of words model under domain shift, while another cluster
uses syntactic heuristics. Our work demonstrates how the geometry of the loss
surface can guide models towards different heuristic functions.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">What Are Expected Queries in End-to-End Object Detection?. (arXiv:2206.01232v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01232">
<div class="article-summary-box-inner">
<span><p>End-to-end object detection is rapidly progressed after the emergence of
DETR. DETRs use a set of sparse queries that replace the dense candidate boxes
in most traditional detectors. In comparison, the sparse queries cannot
guarantee a high recall as dense priors. However, making queries dense is not
trivial in current frameworks. It not only suffers from heavy computational
cost but also difficult optimization. As both sparse and dense queries are
imperfect, then \emph{what are expected queries in end-to-end object
detection}? This paper shows that the expected queries should be Dense Distinct
Queries (DDQ). Concretely, we introduce dense priors back to the framework to
generate dense queries. A duplicate query removal pre-process is applied to
these queries so that they are distinguishable from each other. The dense
distinct queries are then iteratively processed to obtain final sparse outputs.
We show that DDQ is stronger, more robust, and converges faster. It obtains
44.5 AP on the MS COCO detection dataset with only 12 epochs. DDQ is also
robust as it outperforms previous methods on both object detection and instance
segmentation tasks on various datasets. DDQ blends advantages from traditional
dense priors and recent end-to-end detectors. We hope it can serve as a new
baseline and inspires researchers to revisit the complementarity between
traditional methods and end-to-end detectors. The source code is publicly
available at \url{https://github.com/jshilong/DDQ}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Portrait Stylization on the Edge. (arXiv:2206.01244v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01244">
<div class="article-summary-box-inner">
<span><p>In this work we demonstrate real-time portrait stylization, specifically,
translating self-portrait into cartoon or anime style on mobile devices. We
propose a latency-driven differentiable architecture search method, maintaining
realistic generative quality. With our framework, we obtain $10\times$
computation reduction on the generative model and achieve real-time video
stylization on off-the-shelf smartphone using mobile GPUs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expressiveness and Learnability: A Unifying View for Evaluating Self-Supervised Learning. (arXiv:2206.01251v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01251">
<div class="article-summary-box-inner">
<span><p>We propose a unifying view to analyze the representation quality of
self-supervised learning (SSL) models without access to supervised labels,
while being agnostic to the architecture, learning algorithm or data
manipulation used during training. We argue that representations can be
evaluated through the lens of expressiveness and learnability. We propose to
use the Intrinsic Dimension (ID) to assess expressiveness and introduce Cluster
Learnability (CL) to assess learnability. CL is measured as the learning speed
of a KNN classifier trained to predict labels obtained by clustering the
representations with K-means. We thus combine CL and ID into a single
predictor: CLID. Through a large-scale empirical study with a diverse family of
SSL algorithms, we find that CLID better correlates with in-distribution model
performance than other competing recent evaluation schemes. We also benchmark
CLID on out-of-domain generalization, where CLID serves as a predictor of the
transfer performance of SSL models on several classification tasks, yielding
improvements with respect to the competing baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images. (arXiv:2206.01256v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01256">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose PETRv2, a unified framework for 3D perception from
multi-view images. Based on PETR, PETRv2 explores the effectiveness of temporal
modeling, which utilizes the temporal information of previous frames to boost
3D object detection. More specifically, we extend the 3D position embedding (3D
PE) in PETR for temporal modeling. The 3D PE achieves the temporal alignment on
object position of different frames. A feature-guided position encoder is
further introduced to improve the data adaptability of 3D PE. To support for
high-quality BEV segmentation, PETRv2 provides a simply yet effective solution
by adding a set of segmentation queries. Each segmentation query is responsible
for segmenting one specific patch of BEV map. PETRv2 achieves state-of-the-art
performance on 3D object detection and BEV segmentation. Detailed robustness
analysis is also conducted on PETR framework. We hope PETRv2 can serve as a
unified framework for 3D perception.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Points2NeRF: Generating Neural Radiance Fields from 3D point cloud. (arXiv:2206.01290v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01290">
<div class="article-summary-box-inner">
<span><p>Contemporary registration devices for 3D visual information, such as LIDARs
and various depth cameras, capture data as 3D point clouds. In turn, such
clouds are challenging to be processed due to their size and complexity.
Existing methods address this problem by fitting a mesh to the point cloud and
rendering it instead. This approach, however, leads to the reduced fidelity of
the resulting visualization and misses color information of the objects crucial
in computer graphics applications. In this work, we propose to mitigate this
challenge by representing 3D objects as Neural Radiance Fields (NeRFs). We
leverage a hypernetwork paradigm and train the model to take a 3D point cloud
with the associated color values and return a NeRF network's weights that
reconstruct 3D objects from input 2D images. Our method provides efficient 3D
object representation and offers several advantages over the existing
approaches, including the ability to condition NeRFs and improved
generalization beyond objects seen in training. The latter we also confirmed in
the results of our empirical evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lossless Compression of Point Cloud Sequences Using Sequence Optimized CNN Models. (arXiv:2206.01297v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01297">
<div class="article-summary-box-inner">
<span><p>We propose a new paradigm for encoding the geometry of point cloud sequences,
where the convolutional neural network (CNN) which estimates the encoding
distributions is optimized on several frames of the sequence to be compressed.
We adopt lightweight CNN structures, we perform training as part of the
encoding process, and the CNN parameters are transmitted as part of the
bitstream. The newly proposed encoding scheme operates on the octree
representation for each point cloud, encoding consecutively each octree
resolution layer. At every octree resolution layer, the voxel grid is traversed
section-by-section (each section being perpendicular to a selected coordinate
axis) and in each section the occupancies of groups of two-by-two voxels are
encoded at once, in a single arithmetic coding operation. A context for the
conditional encoding distribution is defined for each two-by-two group of
voxels, based on the information available about the occupancy of neighbor
voxels in the current and lower resolution layers of the octree. The CNN
estimates the probability distributions of occupancy patterns of all voxel
groups from one section in four phases. In each new phase the contexts are
updated with the occupancies encoded in the previous phase, and each phase
estimates the probabilities in parallel, providing a reasonable trade-off
between the parallelism of processing and the informativeness of the contexts.
The CNN training time is comparable to the time spent in the remaining encoding
steps, leading to competitive overall encoding times. Bitrates and
encoding-decoding times compare favorably with those of recently published
compression schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">H-EMD: A Hierarchical Earth Mover's Distance Method for Instance Segmentation. (arXiv:2206.01309v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01309">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) based semantic segmentation methods have achieved
excellent performance in biomedical image segmentation, producing high quality
probability maps to allow extraction of rich instance information to facilitate
good instance segmentation. While numerous efforts were put into developing new
DL semantic segmentation models, less attention was paid to a key issue of how
to effectively explore their probability maps to attain the best possible
instance segmentation. We observe that probability maps by DL semantic
segmentation models can be used to generate many possible instance candidates,
and accurate instance segmentation can be achieved by selecting from them a set
of "optimized" candidates as output instances. Further, the generated instance
candidates form a well-behaved hierarchical structure (a forest), which allows
selecting instances in an optimized manner. Hence, we propose a novel
framework, called hierarchical earth mover's distance (H-EMD), for instance
segmentation in biomedical 2D+time videos and 3D images, which judiciously
incorporates consistent instance selection with semantic-segmentation-generated
probability maps. H-EMD contains two main stages. (1) Instance candidate
generation: capturing instance-structured information in probability maps by
generating many instance candidates in a forest structure. (2) Instance
candidate selection: selecting instances from the candidate set for final
instance segmentation. We formulate a key instance selection problem on the
instance candidate forest as an optimization problem based on the earth mover's
distance (EMD), and solve it by integer linear programming. Extensive
experiments on eight biomedical video or 3D datasets demonstrate that H-EMD
consistently boosts DL semantic segmentation models and is highly competitive
with state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Unbiased Transferability for Domain Adaptation by Uncertainty Modeling. (arXiv:2206.01319v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01319">
<div class="article-summary-box-inner">
<span><p>Domain adaptation (DA) aims to transfer knowledge learned from a labeled
source domain to an unlabeled or a less labeled but related target domain.
Ideally, the source and target distributions should be aligned to each other
equally to achieve unbiased knowledge transfer. However, due to the significant
imbalance between the amount of annotated data in the source and target
domains, usually only the target distribution is aligned to the source domain,
leading to adapting unnecessary source specific knowledge to the target domain,
i.e., biased domain adaptation. To resolve this problem, in this work, we delve
into the transferability estimation problem in domain adaptation and propose a
non-intrusive Unbiased Transferability Estimation Plug-in (UTEP) by modeling
the uncertainty of a discriminator in adversarial-based DA methods to optimize
unbiased transfer. We theoretically analyze the effectiveness of the proposed
approach to unbiased transferability learning in DA. Furthermore, to alleviate
the impact of imbalanced annotated data, we utilize the estimated uncertainty
for pseudo label selection of unlabeled samples in the target domain, which
helps achieve better marginal and conditional distribution alignments between
domains. Extensive experimental results on a high variety of DA benchmark
datasets show that the proposed approach can be readily incorporated into
various adversarial-based DA methods, achieving state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Fairness in Large-Scale Object Recognition by CrowdSourced Demographic Information. (arXiv:2206.01326v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01326">
<div class="article-summary-box-inner">
<span><p>There has been increasing awareness of ethical issues in machine learning,
and fairness has become an important research topic. Most fairness efforts in
computer vision have been focused on human sensing applications and preventing
discrimination by people's physical attributes such as race, skin color or age
by increasing visual representation for particular demographic groups. We argue
that ML fairness efforts should extend to object recognition as well.
Buildings, artwork, food and clothing are examples of the objects that define
human culture. Representing these objects fairly in machine learning datasets
will lead to models that are less biased towards a particular culture and more
inclusive of different traditions and values. There exist many research
datasets for object recognition, but they have not carefully considered which
classes should be included, or how much training data should be collected per
class. To address this, we propose a simple and general approach, based on
crowdsourcing the demographic composition of the contributors: we define fair
relevance scores, estimate them, and assign them to each class. We showcase its
application to the landmark recognition domain, presenting a detailed analysis
and the final fairer landmark rankings. We present analysis which leads to a
much fairer coverage of the world compared to existing datasets. The evaluation
dataset was used for the 2021 Google Landmark Challenges, which was the first
of a kind with an emphasis on fairness in generic object recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RELAY: Robotic EyeLink AnalYsis of the EyeLink 1000 using an Artificial Eye. (arXiv:2206.01327v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01327">
<div class="article-summary-box-inner">
<span><p>There is a widespread assumption that the peak velocities of visually guided
saccades in the dark are up to 10~\% slower than those made in the light.
Studies that questioned the impact of the surrounding brightness conditions,
come to differing conclusions, whether they have an influence or not and if so,
in which manner. The problem is of a complex nature as the illumination
condition itself may not contribute to different measured peak velocities
solely but in combination with the estimation of the pupil size due to its
deformation during saccades or different gaze positions. Even the measurement
technique of video-based eye tracking itself could play a significant role. To
investigate this issue, we constructed a stepper motor driven artificial eye
with fixed pupil size to mimic human saccades with predetermined peak velocity
\&amp; amplitudes under three different brightness conditions with the EyeLink
1000, one of the most common used eye trackers. The aim was to control the
pupil and brightness. With our device, an overall good accuracy and precision
of the EyeLink 1000 could be confirmed. Furthermore, we could find that there
is no artifact for pupil based eye tracking in relation to changing brightness
conditions, neither for the pupil size nor for the peak velocities. What we
found, was a systematic, small, yet significant change of the measured pupil
sizes as a function of different gaze directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long Scale Error Control in Low Light Image and Video Enhancement Using Equivariance. (arXiv:2206.01334v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01334">
<div class="article-summary-box-inner">
<span><p>Image frames obtained in darkness are special. Just multiplying by a constant
doesn't restore the image. Shot noise, quantization effects and camera
non-linearities mean that colors and relative light levels are estimated
poorly. Current methods learn a mapping using real dark-bright image pairs.
These are very hard to capture. A recent paper has shown that simulated data
pairs produce real improvements in restoration, likely because huge volumes of
simulated data are easy to obtain. In this paper, we show that respecting
equivariance -- the color of a restored pixel should be the same, however the
image is cropped -- produces real improvements over the state of the art for
restoration. We show that a scale selection mechanism can be used to improve
reconstructions. Finally, we show that our approach produces improvements on
video restoration as well. Our methods are evaluated both quantitatively and
qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Pulmonary Embolism from Computed Tomography Using Convolutional Neural Network. (arXiv:2206.01344v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01344">
<div class="article-summary-box-inner">
<span><p>The clinical symptoms of pulmonary embolism (PE) are very diverse and
non-specific, which makes it difficult to diagnose. In addition, pulmonary
embolism has multiple triggers and is one of the major causes of vascular
death. Therefore, if it can be detected and treated quickly, it can
significantly reduce the risk of death in hospitalized patients. In the
detection process, the cost of computed tomography pulmonary angiography (CTPA)
is high, and angiography requires the injection of contrast agents, which
increase the risk of damage to the patient. Therefore, this study will use a
deep learning approach to detect pulmonary embolism in all patients who take a
CT image of the chest using a convolutional neural network. With the proposed
pulmonary embolism detection system, we can detect the possibility of pulmonary
embolism at the same time as the patient's first CT image, and schedule the
CTPA test immediately, saving more than a week of CT image screening time and
providing timely diagnosis and treatment to the patient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Attacks on Human Vision. (arXiv:2206.01365v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01365">
<div class="article-summary-box-inner">
<span><p>This article presents an introduction to visual attention retargeting, its
connection to visual saliency, the challenges associated with it, and ideas for
how it can be approached. The difficulty of attention retargeting as a saliency
inversion problem lies in the lack of one-to-one mapping between saliency and
the image domain, in addition to the possible negative impact of saliency
alterations on image aesthetics. A few approaches from recent literature to
solve this challenging problem are reviewed, and several suggestions for future
development are presented.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supernet Training for Federated Image Classification under System Heterogeneity. (arXiv:2206.01366v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01366">
<div class="article-summary-box-inner">
<span><p>Efficient deployment of deep neural networks across many devices and resource
constraints, especially on edge devices, is one of the most challenging
problems in the presence of data-privacy preservation issues. Conventional
approaches have evolved to either improve a single global model while keeping
each local training data decentralized (i.e., data-heterogeneity) or to train a
once-for-all network that supports diverse architectural settings to address
heterogeneous systems equipped with different computational capabilities (i.e.,
model-heterogeneity). However, little research has considered both directions
simultaneously. In this work, we propose a novel framework to consider both
scenarios, namely Federation of Supernet Training (FedSup), where clients send
and receive a supernet whereby it contains all possible architectures sampled
from itself. It is inspired by how averaging parameters in the model
aggregation stage of Federated Learning (FL) is similar to weight-sharing in
supernet training. Specifically, in the FedSup framework, a weight-sharing
approach widely used in the training single shot model is combined with the
averaging of Federated Learning (FedAvg). Under our framework, we present an
efficient algorithm (E-FedSup) by sending the sub-model to clients in the
broadcast stage for reducing communication costs and training overhead. We
demonstrate several strategies to enhance supernet training in the FL
environment and conduct extensive empirical evaluations. The resulting
framework is shown to pave the way for the robustness of both data- and
model-heterogeneity on several standard benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Learning Meets Transfer Learning: Application to Multi-site Prostate MRI Segmentation. (arXiv:2206.01369v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01369">
<div class="article-summary-box-inner">
<span><p>Many medical datasets have recently been created for medical image
segmentation tasks, and it is natural to question whether we can use them to
sequentially train a single model that (1) performs better on all these
datasets, and (2) generalizes well and transfers better to the unknown target
site domain. Prior works have achieved this goal by jointly training one model
on multi-site datasets, which achieve competitive performance on average but
such methods rely on the assumption about the availability of all training
data, thus limiting its effectiveness in practical deployment. In this paper,
we propose a novel multi-site segmentation framework called
incremental-transfer learning (ITL), which learns a model from multi-site
datasets in an end-to-end sequential fashion. Specifically, "incremental"
refers to training sequentially constructed datasets, and "transfer" is
achieved by leveraging useful information from the linear combination of
embedding features on each dataset. In addition, we introduce our ITL
framework, where we train the network including a site-agnostic encoder with
pre-trained weights and at most two segmentation decoder heads. We also design
a novel site-level incremental loss in order to generalize well on the target
domain. Second, we show for the first time that leveraging our ITL training
scheme is able to alleviate challenging catastrophic forgetting problems in
incremental learning. We conduct experiments using five challenging benchmark
datasets to validate the effectiveness of our incremental-transfer learning
approach. Our approach makes minimal assumptions on computation resources and
domain-specific expertise, and hence constitutes a strong starting point in
multi-site medical image segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Slot Order Matters for Compositional Scene Understanding. (arXiv:2206.01370v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01370">
<div class="article-summary-box-inner">
<span><p>Empowering agents with a compositional understanding of their environment is
a promising next step toward solving long-horizon planning problems. On the one
hand, we have seen encouraging progress on variational inference algorithms for
obtaining sets of object-centric latent representations ("slots") from
unstructured scene observations. On the other hand, generating scenes from
slots has received less attention, in part because it is complicated by the
lack of a canonical object order. A canonical object order is useful for
learning the object correlations necessary to generate physically plausible
scenes similar to how raster scan order facilitates learning pixel correlations
for pixel-level autoregressive image generation. In this work, we address this
lack by learning a fixed object order for a hierarchical variational
autoencoder with a single level of autoregressive slots and a global scene
prior. We cast autoregressive slot inference as a set-to-sequence modeling
problem. We introduce an auxiliary loss to train the slot prior to generate
objects in a fixed order. During inference, we align a set of inferred slots to
the object order obtained from a slot prior rollout. To ensure the rolled out
objects are meaningful for the given scene, we condition the prior on an
inferred global summary of the input. Experiments on compositional environments
and ablations demonstrate that our model with global prior, inference with
aligned slot order, and auxiliary loss achieves state-of-the-art sample
quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CF-YOLO: Cross Fusion YOLO for Object Detection in Adverse Weather with a High-quality Real Snow Dataset. (arXiv:2206.01381v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01381">
<div class="article-summary-box-inner">
<span><p>Snow is one of the toughest adverse weather conditions for object detection
(OD). Currently, not only there is a lack of snowy OD datasets to train
cutting-edge detectors, but also these detectors have difficulties learning
latent information beneficial for detection in snow. To alleviate the two above
problems, we first establish a real-world snowy OD dataset, named RSOD.
Besides, we develop an unsupervised training strategy with a distinctive
activation function, called $Peak \ Act$, to quantitatively evaluate the effect
of snow on each object. Peak Act helps grading the images in RSOD into
four-difficulty levels. To our knowledge, RSOD is the first quantitatively
evaluated and graded snowy OD dataset. Then, we propose a novel Cross Fusion
(CF) block to construct a lightweight OD network based on YOLOv5s (call
CF-YOLO). CF is a plug-and-play feature aggregation module, which integrates
the advantages of Feature Pyramid Network and Path Aggregation Network in a
simpler yet more flexible form. Both RSOD and CF lead our CF-YOLO to possess an
optimization ability for OD in real-world snow. That is, CF-YOLO can handle
unfavorable detection problems of vagueness, distortion and covering of snow.
Experiments show that our CF-YOLO achieves better detection results on RSOD,
compared to SOTAs. The code and dataset are available at
https://github.com/qqding77/CF-YOLO-and-RSOD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Falconn++: A Locality-sensitive Filtering Approach for Approximate Nearest Neighbor Search. (arXiv:2206.01382v1 [cs.DS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01382">
<div class="article-summary-box-inner">
<span><p>We present Falconn++, a novel locality-sensitive filtering (LSF) approach for
approximate nearest neighbor search on angular distance. Falconn++ can filter
out potential far away points in any hash bucket before querying, which results
in higher quality candidates compared to other hashing-based solutions.
Theoretically, Falconn++ asymptotically achieves lower query time complexity
than Falconn, an optimal locality-sensitive hashing scheme on angular distance.
Empirically, Falconn++ achieves a higher recall-speed tradeoff than Falconn on
many real-world data sets. Falconn++ is also competitive against HNSW, an
efficient representative of graph-based solutions on high search recall
regimes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End 3D Hand Pose Estimation from Stereo Cameras. (arXiv:2206.01384v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01384">
<div class="article-summary-box-inner">
<span><p>This work proposes an end-to-end approach to estimate full 3D hand pose from
stereo cameras. Most existing methods of estimating hand pose from stereo
cameras apply stereo matching to obtain depth map and use depth-based solution
to estimate hand pose. In contrast, we propose to bypass the stereo matching
and directly estimate the 3D hand pose from the stereo image pairs. The
proposed neural network architecture extends from any keypoint predictor to
estimate the sparse disparity of the hand joints. In order to effectively train
the model, we propose a large scale synthetic dataset that is composed of
stereo image pairs and ground truth 3D hand pose annotations. Experiments show
that the proposed approach outperforms the existing methods based on the stereo
depth.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Structured Illumination Microscopy with a Neural Space-time Model. (arXiv:2206.01397v1 [physics.optics])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01397">
<div class="article-summary-box-inner">
<span><p>Structured illumination microscopy (SIM) reconstructs a super-resolved image
from multiple raw images; hence, acquisition speed is limited, making it
unsuitable for dynamic scenes. We propose a new method, Speckle Flow SIM, that
models sample motion during the data capture in order to reconstruct dynamic
scenes with super-resolution. Speckle Flow SIM uses fixed speckle illumination
and relies on sample motion to capture a sequence of raw images. Then, the
spatio-temporal relationship of the dynamic scene is modeled using a neural
space-time model with coordinate-based multi-layer perceptrons (MLPs), and the
motion dynamics and the super-resolved scene are jointly recovered. We
validated Speckle Flow SIM in simulation and built a simple, inexpensive
experimental setup with off-the-shelf components. We demonstrated that Speckle
Flow SIM can reconstruct a dynamic scene with deformable motion and 1.88x the
diffraction-limited resolution in experiment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaLR: Layer-wise Learning Rate based on Meta-Learning for Adaptively Fine-tuning Medical Pre-trained Models. (arXiv:2206.01408v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01408">
<div class="article-summary-box-inner">
<span><p>When applying transfer learning for medical image analysis, downstream tasks
often have significant gaps with the pre-training tasks. Previous methods
mainly focus on improving the transferabilities of the pre-trained models to
bridge the gaps. In fact, model fine-tuning can also play a very important role
in tackling this problem. A conventional fine-tuning method is updating all
deep neural networks (DNNs) layers by a single learning rate (LR), which
ignores the unique transferabilities of different layers. In this work, we
explore the behaviors of different layers in the fine-tuning stage. More
precisely, we first hypothesize that lower-level layers are more
domain-specific while higher-level layers are more task-specific, which is
verified by a simple bi-directional fine-tuning scheme. It is harder for the
pre-trained specific layers to transfer to new tasks than general layers. On
this basis, to make different layers better co-adapt to the downstream tasks
according to their transferabilities, a meta-learning-based LR learner, namely
MetaLR, is proposed to assign LRs for each layer automatically. Extensive
experiments on various medical applications (i.e., POCUS, BUSI, Chest X-ray,
and LiTS) well confirm our hypothesis and show the superior performance of the
proposed methods to previous state-of-the-art fine-tuning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning an Adaptation Function to Assess Image Visual Similarities. (arXiv:2206.01417v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01417">
<div class="article-summary-box-inner">
<span><p>Human perception is routinely assessing the similarity between images, both
for decision making and creative thinking. But the underlying cognitive process
is not really well understood yet, hence difficult to be mimicked by computer
vision systems. State-of-the-art approaches using deep architectures are often
based on the comparison of images described as feature vectors learned for
image categorization task. As a consequence, such features are powerful to
compare semantically related images but not really efficient to compare images
visually similar but semantically unrelated. Inspired by previous works on
neural features adaptation to psycho-cognitive representations, we focus here
on the specific task of learning visual image similarities when analogy
matters. We propose to compare different supervised, semi-supervised and
self-supervised networks, pre-trained on distinct scales and contents datasets
(such as ImageNet-21k, ImageNet-1K or VGGFace2) to conclude which model may be
the best to approximate the visual cortex and learn only an adaptation function
corresponding to the approximation of the the primate IT cortex through the
metric learning framework. Our experiments conducted on the Totally Looks Like
image dataset highlight the interest of our method, by increasing the retrieval
scores of the best model @1 by 2.25x. This research work was recently accepted
for publication at the ICIP 2021 international conference [1]. In this new
article, we expand on this previous work by using and comparing new pre-trained
feature extractors on other datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning rich optical embeddings for privacy-preserving lensless image classification. (arXiv:2206.01429v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01429">
<div class="article-summary-box-inner">
<span><p>By replacing the lens with a thin optical element, lensless imaging enables
new applications and solutions beyond those supported by traditional camera
design and post-processing, e.g. compact and lightweight form factors and
visual privacy. The latter arises from the highly multiplexed measurements of
lensless cameras, which require knowledge of the imaging system to recover a
recognizable image. In this work, we exploit this unique multiplexing property:
casting the optics as an encoder that produces learned embeddings directly at
the camera sensor. We do so in the context of image classification, where we
jointly optimize the encoder's parameters and those of an image classifier in
an end-to-end fashion. Our experiments show that jointly learning the lensless
optical encoder and the digital processing allows for lower resolution
embeddings at the sensor, and hence better privacy as it is much harder to
recover meaningful images from these measurements. Additional experiments show
that such an optimization allows for lensless measurements that are more robust
to typical real-world image transformations. While this work focuses on
classification, the proposed programmable lensless camera and end-to-end
optimization can be applied to other computational imaging tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LenslessPiCam: A Hardware and Software Platform for Lensless Computational Imaging with a Raspberry Pi. (arXiv:2206.01430v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01430">
<div class="article-summary-box-inner">
<span><p>Lensless imaging seeks to replace/remove the lens in a conventional imaging
system. The earliest cameras were in fact lensless, relying on long exposure
times to form images on the other end of a small aperture in a darkened
room/container (camera obscura). The introduction of a lens allowed for more
light throughput and therefore shorter exposure times, while retaining sharp
focus. The incorporation of digital sensors readily enabled the use of
computational imaging techniques to post-process and enhance raw images (e.g.
via deblurring, inpainting, denoising, sharpening). Recently, imaging
scientists have started leveraging computational imaging as an integral part of
lensless imaging systems, allowing them to form viewable images from the highly
multiplexed raw measurements of lensless cameras (see [5] and references
therein for a comprehensive treatment of lensless imaging). This represents a
real paradigm shift in camera system design as there is more flexibility to
cater the hardware to the application at hand (e.g. lightweight or flat
designs). This increased flexibility comes however at the price of a more
demanding post-processing of the raw digital recordings and a tighter
integration of sensing and computation, often difficult to achieve in practice
due to inefficient interactions between the various communities of scientists
involved. With LenslessPiCam, we provide an easily accessible hardware and
software framework to enable researchers, hobbyists, and students to implement
and explore practical and computational aspects of lensless imaging. We also
provide detailed guides and exercises so that LenslessPiCam can be used as an
educational resource, and point to results from our graduate-level signal
processing course.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Transformers for Behavioural Biometrics: A Case Study in Gait Recognition. (arXiv:2206.01441v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01441">
<div class="article-summary-box-inner">
<span><p>Biometrics on mobile devices has attracted a lot of attention in recent years
as it is considered a user-friendly authentication method. This interest has
also been motivated by the success of Deep Learning (DL). Architectures based
on Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs)
have been established to be convenient for the task, improving the performance
and robustness in comparison to traditional machine learning techniques.
However, some aspects must still be revisited and improved. To the best of our
knowledge, this is the first article that intends to explore and propose novel
gait biometric recognition systems based on Transformers, which currently
obtain state-of-the-art performance in many applications. Several
state-of-the-art architectures (Vanilla, Informer, Autoformer, Block-Recurrent
Transformer, and THAT) are considered in the experimental framework. In
addition, new configurations of the Transformers are proposed to further
increase the performance. Experiments are carried out using the two popular
public databases whuGAIT and OU-ISIR. The results achieved prove the high
ability of the proposed Transformer, outperforming state-of-the-art CNN and RNN
architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Bird Species Recognition by Learning from Field Guides. (arXiv:2206.01466v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01466">
<div class="article-summary-box-inner">
<span><p>We exploit field guides to learn bird species recognition, in particular
zero-shot recognition of unseen species. The illustrations contained in field
guides deliberately focus on discriminative properties of a species, and can
serve as side information to transfer knowledge from seen to unseen classes. We
study two approaches: (1) a contrastive encoding of illustrations that can be
fed into zero-shot learning schemes; and (2) a novel method that leverages the
fact that illustrations are also images and as such structurally more similar
to photographs than other kinds of side information. Our results show that
illustrations from field guides, which are readily available for a wide range
of species, are indeed a competitive source of side information. On the
iNaturalist2021 subset, we obtain a harmonic mean from 749 seen and 739 unseen
classes greater than $45\%$ (@top-10) and $15\%$ (@top-1). Which shows that
field guides are a valuable option for challenging real-world scenarios with
many species.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Transfer-based Targeted Adversarial Perturbations against Real-World Computer Vision Systems based on Human Judgments. (arXiv:2206.01467v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01467">
<div class="article-summary-box-inner">
<span><p>Computer vision systems are remarkably vulnerable to adversarial
perturbations. Transfer-based adversarial images are generated on one (source)
system and used to attack another (target) system. In this paper, we take the
first step to investigate transfer-based targeted adversarial images in a
realistic scenario where the target system is trained on some private data with
its inventory of semantic labels not publicly available. Our main contributions
include an extensive human-judgment-based evaluation of attack success on the
Google Cloud Vision API and additional analysis of the different behaviors of
Google Cloud Vision in face of original images vs. adversarial images.
Resources are publicly available at
\url{https://github.com/ZhengyuZhao/Targeted-Tansfer/blob/main/google_results.zip}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distributional loss for convolutional neural network regression and application to GNSS multi-path estimation. (arXiv:2206.01473v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01473">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Network (CNN) have been widely used in image
classification. Over the years, they have also benefited from various
enhancements and they are now considered as state of the art techniques for
image like data. However, when they are used for regression to estimate some
function value from images, fewer recommendations are available. In this study,
a novel CNN regression model is proposed. It combines convolutional neural
layers to extract high level features representations from images with a soft
labelling technique. More specifically, as the deep regression task is
challenging, the idea is to account for some uncertainty in the targets that
are seen as distributions around their mean. The estimations are carried out by
the model in the form of distributions. Building from earlier work, a specific
histogram loss function based on the Kullback-Leibler (KL) divergence is
applied during training. The model takes advantage of the CNN feature
representation and is able to carry out estimation from multi-channel input
images. To assess and illustrate the technique, the model is applied to Global
Navigation Satellite System (GNSS) multi-path estimation where multi-path
signal parameters have to be estimated from correlator output images from the I
and Q channels. The multi-path signal delay, magnitude, Doppler shift frequency
and phase parameters are estimated from synthetically generated datasets of
satellite signals. Experiments are conducted under various receiving conditions
and various input images resolutions to test the estimation performances
quality and robustness. The results show that the proposed soft labelling CNN
technique using distributional loss outperforms classical CNN regression under
all conditions. Furthermore, the extra learning performance achieved by the
model allows the reduction of input image resolution from 80x80 down to 40x40
or sometimes 20x20.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YOLOv5s-GTB: light-weighted and improved YOLOv5s for bridge crack detection. (arXiv:2206.01498v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01498">
<div class="article-summary-box-inner">
<span><p>In response to the situation that the conventional bridge crack manual
detection method has a large amount of human and material resources wasted,
this study is aimed to propose a light-weighted, high-precision, deep
learning-based bridge apparent crack recognition model that can be deployed in
mobile devices' scenarios. In order to enhance the performance of YOLOv5,
firstly, the data augmentation methods are supplemented, and then the YOLOv5
series algorithm is trained to select a suitable basic framework. The YOLOv5s
is identified as the basic framework for the light-weighted crack detection
model through experiments for comparison and validation.By replacing the
traditional DarkNet backbone network of YOLOv5s with GhostNet backbone network,
introducing Transformer multi-headed self-attention mechanism and
bi-directional feature pyramid network (BiFPN) to replace the commonly used
feature pyramid network, the improved model not only has 42% fewer parameters
and faster inference response, but also significantly outperforms the original
model in terms of accuracy and mAP (8.5% and 1.1% improvement, respectively).
Luckily each improved part has a positive impact on the result. This paper
provides a feasible idea to establish a digital operation management system in
the field of highway and bridge in the future and to implement the whole life
cycle structure health monitoring of civil infrastructure in China.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anomaly detection in surveillance videos using transformer based attention model. (arXiv:2206.01524v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01524">
<div class="article-summary-box-inner">
<span><p>Surveillance footage can catch a wide range of realistic anomalies. This
research suggests using a weakly supervised strategy to avoid annotating
anomalous segments in training videos, which is time consuming. In this
approach only video level labels are used to obtain frame level anomaly scores.
Weakly supervised video anomaly detection (WSVAD) suffers from the wrong
identification of abnormal and normal instances during the training process.
Therefore it is important to extract better quality features from the available
videos. WIth this motivation, the present paper uses better quality
transformer-based features named Videoswin Features followed by the attention
layer based on dilated convolution and self attention to capture long and short
range dependencies in temporal domain. This gives us a better understanding of
available videos. The proposed framework is validated on real-world dataset
i.e. ShanghaiTech Campus dataset which results in competitive performance than
current state-of-the-art methods. The model and the code are available at
https://github.com/kapildeshpande/Anomaly-Detection-in-Surveillance-Videos
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OmniXAI: A Library for Explainable AI. (arXiv:2206.01612v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01612">
<div class="article-summary-box-inner">
<span><p>We introduce OmniXAI, an open-source Python library of eXplainable AI (XAI),
which offers omni-way explainable AI capabilities and various interpretable
machine learning techniques to address the pain points of understanding and
interpreting the decisions made by machine learning (ML) in practice. OmniXAI
aims to be a one-stop comprehensive library that makes explainable AI easy for
data scientists, ML researchers and practitioners who need explanation for
various types of data, models and explanation methods at different stages of ML
process (data exploration, feature engineering, model development, evaluation,
and decision-making, etc). In particular, our library includes a rich family of
explanation methods integrated in a unified interface, which supports multiple
data types (tabular data, images, texts, time-series), multiple types of ML
models (traditional ML in Scikit-learn and deep learning models in
PyTorch/TensorFlow), and a range of diverse explanation methods including
"model-specific" and "model-agnostic" ones (such as feature-attribution
explanation, counterfactual explanation, gradient-based explanation, etc). For
practitioners, the library provides an easy-to-use unified interface to
generate the explanations for their applications by only writing a few lines of
codes, and also a GUI dashboard for visualization of different explanations for
more insights about decisions. In this technical report, we present OmniXAI's
design principles, system architectures, and major functionalities, and also
demonstrate several example use cases across different types of data, tasks,
and models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pruning for Interpretable, Feature-Preserving Circuits in CNNs. (arXiv:2206.01627v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01627">
<div class="article-summary-box-inner">
<span><p>Deep convolutional neural networks are a powerful model class for a range of
computer vision problems, but it is difficult to interpret the image filtering
process they implement, given their sheer size. In this work, we introduce a
method for extracting 'feature-preserving circuits' from deep CNNs, leveraging
methods from saliency-based neural network pruning. These circuits are modular
sub-functions, embedded within the network, containing only a subset of
convolutional kernels relevant to a target feature. We compare the efficacy of
3 saliency-criteria for extracting these sparse circuits. Further, we show how
'sub-feature' circuits can be extracted, that preserve a feature's responses to
particular images, dividing the feature into even sparser filtering processes.
We also develop a tool for visualizing 'circuit diagrams', which render the
entire image filtering process implemented by circuits in a parsable format.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforcement Learning with Neural Radiance Fields. (arXiv:2206.01634v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01634">
<div class="article-summary-box-inner">
<span><p>It is a long-standing problem to find effective representations for training
reinforcement learning (RL) agents. This paper demonstrates that learning state
representations with supervision from Neural Radiance Fields (NeRFs) can
improve the performance of RL compared to other learned representations or even
low-dimensional, hand-engineered state information. Specifically, we propose to
train an encoder that maps multiple image observations to a latent space
describing the objects in the scene. The decoder built from a
latent-conditioned NeRF serves as the supervision signal to learn the latent
space. An RL algorithm then operates on the learned latent space as its state
representation. We call this NeRF-RL. Our experiments indicate that NeRF as
supervision leads to a latent space better suited for the downstream RL tasks
involving robotic object manipulations like hanging mugs on hooks, pushing
objects, or opening doors. Video: https://dannydriess.github.io/nerf-rl
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mirror modular cloning and fast quantum associative retrieval. (arXiv:2206.01644v1 [quant-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01644">
<div class="article-summary-box-inner">
<span><p>We show that a quantum state can be perfectly cloned up to global mirroring
with a unitary transformation that depends on one single parameter. We then
show that this is equivalent to "perfect" cloning for quantum associative
memories which, as a consequence efficiently hold exponentially more
information than their classical counterparts. Finally, we present a quantum
associative retrieval algorithm which can correct corrupted inputs and is
exponentially faster than the Grover algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Positive Sampling for Contrastive Learning with Kernel. (arXiv:2206.01646v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01646">
<div class="article-summary-box-inner">
<span><p>Data augmentation is a crucial component in unsupervised contrastive learning
(CL). It determines how positive samples are defined and, ultimately, the
quality of the representation. While efficient augmentations have been found
for standard vision datasets, such as ImageNet, it is still an open problem in
other applications, such as medical imaging, or in datasets with easy-to-learn
but irrelevant imaging features. In this work, we propose a new way to define
positive samples using kernel theory along with a novel loss called decoupled
uniformity. We propose to integrate prior information, learnt from generative
models or given as auxiliary attributes, into contrastive learning, to make it
less dependent on data augmentation. We draw a connection between contrastive
learning and the conditional mean embedding theory to derive tight bounds on
the downstream classification loss. In an unsupervised setting, we empirically
demonstrate that CL benefits from generative models, such as VAE and GAN, to
less rely on data augmentations. We validate our framework on vision datasets
including CIFAR10, CIFAR100, STL10 and ImageNet100 and a brain MRI dataset. In
the weakly supervised setting, we demonstrate that our formulation provides
state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D'ARTAGNAN: Counterfactual Video Generation. (arXiv:2206.01651v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01651">
<div class="article-summary-box-inner">
<span><p>Causally-enabled machine learning frameworks could help clinicians to
identify the best course of treatments by answering counterfactual questions.
We explore this path for the case of echocardiograms by looking into the
variation of the Left Ventricle Ejection Fraction, the most essential clinical
metric gained from these examinations. We combine deep neural networks, twin
causal networks and generative adversarial methods for the first time to build
D'ARTAGNAN (Deep ARtificial Twin-Architecture GeNerAtive Networks), a novel
causal generative model. We demonstrate the soundness of our approach on a
synthetic dataset before applying it to cardiac ultrasound videos by answering
the question: "What would this echocardiogram look like if the patient had a
different ejection fraction?". To do so, we generate new ultrasound videos,
retaining the video style and anatomy of the original patient, with variations
of the Ejection Fraction conditioned on a given input. We achieve an SSIM score
of 0.79 and an R2 score of 0.51 on the counterfactual videos. Code and models
are available at https://github.com/HReynaud/dartagnan.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metrics reloaded: Pitfalls and recommendations for image analysis validation. (arXiv:2206.01653v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01653">
<div class="article-summary-box-inner">
<span><p>The field of automatic biomedical image analysis crucially depends on robust
and meaningful performance metrics for algorithm validation. Current metric
usage, however, is often ill-informed and does not reflect the underlying
domain interest. Here, we present a comprehensive framework that guides
researchers towards choosing performance metrics in a problem-aware manner.
Specifically, we focus on biomedical image analysis problems that can be
interpreted as a classification task at image, object or pixel level. The
framework first compiles domain interest-, target structure-, data set- and
algorithm output-related properties of a given problem into a problem
fingerprint, while also mapping it to the appropriate problem category, namely
image-level classification, semantic segmentation, instance segmentation, or
object detection. It then guides users through the process of selecting and
applying a set of appropriate validation metrics while making them aware of
potential pitfalls related to individual choices. In this paper, we describe
the current status of the Metrics Reloaded recommendation framework, with the
goal of obtaining constructive feedback from the image analysis community. The
current version has been developed within an international consortium of more
than 60 image analysis experts and will be made openly available as a
user-friendly toolkit after community-driven optimization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identification via Retinal Vessels Combining LBP and HOG. (arXiv:2206.01658v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01658">
<div class="article-summary-box-inner">
<span><p>With development of information technology and necessity for high security,
using different identification methods has become very important. Each
biometric feature has its own advantages and disadvantages and choosing each of
them depends on our usage. Retinal scanning is a bio scale method for
identification. The retina is composed of vessels and optical disk. The vessels
distribution pattern is one the remarkable retinal identification methods. In
this paper, a new approach is presented for identification via retinal images
using LBP and hog methods. In the proposed method, it will be tried to separate
the retinal vessels accurately via machine vision techniques which will have
good sustainability in rotation and size change. HOG-based or LBP-based methods
or their combination can be used for separation and also HSV color space can be
used too. Having extracted the features, the similarity criteria can be used
for identification. The implementation of proposed method and its comparison
with one of the newly-presented methods in this area shows better performance
of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Style-Content Disentanglement in Language-Image Pretraining Representations for Zero-Shot Sketch-to-Image Synthesis. (arXiv:2206.01661v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01661">
<div class="article-summary-box-inner">
<span><p>In this work, we propose and validate a framework to leverage language-image
pretraining representations for training-free zero-shot sketch-to-image
synthesis. We show that disentangled content and style representations can be
utilized to guide image generators to employ them as sketch-to-image generators
without (re-)training any parameters. Our approach for disentangling style and
content entails a simple method consisting of elementary arithmetic assuming
compositionality of information in representations of input sketches. Our
results demonstrate that this approach is competitive with state-of-the-art
instance-level open-domain sketch-to-image models, while only depending on
pretrained off-the-shelf models and a fraction of the data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Egocentric Video-Language Pretraining. (arXiv:2206.01670v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01670">
<div class="article-summary-box-inner">
<span><p>Video-Language Pretraining (VLP), aiming to learn transferable representation
to advance a wide range of video-text downstream tasks, has recently received
increasing attention. Dominant works that achieve strong performance rely on
large-scale, 3rd-person video-text datasets, such as HowTo100M. In this work,
we exploit the recently released Ego4D dataset to pioneer Egocentric VLP along
three directions. (i) We create EgoClip, a 1st-person video-text pretraining
dataset comprising 3.8M clip-text pairs well-chosen from Ego4D, covering a
large variety of human daily activities. (ii) We propose a novel pretraining
objective, dubbed as EgoNCE, which adapts video-text contrastive learning to
egocentric domain by mining egocentric-aware positive and negative samples.
(iii) We introduce EgoMCQ, a development benchmark that is close to EgoClip and
hence can support effective validation and fast exploration of our design
decisions regarding EgoClip and EgoNCE. Furthermore, we demonstrate strong
performance on five egocentric downstream tasks across three datasets:
video-text retrieval on EPIC-KITCHENS-100; action recognition on Charades-Ego;
and natural language query, moment query, and object state change
classification on Ego4D challenge benchmarks. The dataset and code will be
available at https://github.com/showlab/EgoVLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Kernel Selection for Improved Generalization and Memory Efficiency in Meta-learning. (arXiv:2206.01690v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01690">
<div class="article-summary-box-inner">
<span><p>Gradient based meta-learning methods are prone to overfit on the
meta-training set, and this behaviour is more prominent with large and complex
networks. Moreover, large networks restrict the application of meta-learning
models on low-power edge devices. While choosing smaller networks avoid these
issues to a certain extent, it affects the overall generalization leading to
reduced performance. Clearly, there is an approximately optimal choice of
network architecture that is best suited for every meta-learning problem,
however, identifying it beforehand is not straightforward. In this paper, we
present MetaDOCK, a task-specific dynamic kernel selection strategy for
designing compressed CNN models that generalize well on unseen tasks in
meta-learning. Our method is based on the hypothesis that for a given set of
similar tasks, not all kernels of the network are needed by each individual
task. Rather, each task uses only a fraction of the kernels, and the selection
of the kernels per task can be learnt dynamically as a part of the inner update
steps. MetaDOCK compresses the meta-model as well as the task-specific inner
models, thus providing significant reduction in model size for each task, and
through constraining the number of active kernels for every task, it implicitly
mitigates the issue of meta-overfitting. We show that for the same inference
budget, pruned versions of large CNN models obtained using our approach
consistently outperform the conventional choices of CNN models. MetaDOCK
couples well with popular meta-learning approaches such as iMAML. The efficacy
of our method is validated on CIFAR-fs and mini-ImageNet datasets, and we have
observed that our approach can provide improvements in model accuracy of up to
2% on standard meta-learning benchmark, while reducing the model size by more
than 75%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gradient Obfuscation Checklist Test Gives a False Sense of Security. (arXiv:2206.01705v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01705">
<div class="article-summary-box-inner">
<span><p>One popular group of defense techniques against adversarial attacks is based
on injecting stochastic noise into the network. The main source of robustness
of such stochastic defenses however is often due to the obfuscation of the
gradients, offering a false sense of security. Since most of the popular
adversarial attacks are optimization-based, obfuscated gradients reduce their
attacking ability, while the model is still susceptible to stronger or
specifically tailored adversarial attacks. Recently, five characteristics have
been identified, which are commonly observed when the improvement in robustness
is mainly caused by gradient obfuscation. It has since become a trend to use
these five characteristics as a sufficient test, to determine whether or not
gradient obfuscation is the main source of robustness. However, these
characteristics do not perfectly characterize all existing cases of gradient
obfuscation, and therefore can not serve as a basis for a conclusive test. In
this work, we present a counterexample, showing this test is not sufficient for
concluding that gradient obfuscation is not the main cause of improvements in
robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional Visual Generation with Composable Diffusion Models. (arXiv:2206.01714v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01714">
<div class="article-summary-box-inner">
<span><p>Large text-guided diffusion models, such as DALLE-2, are able to generate
stunning photorealistic images given natural language descriptions. While such
models are highly flexible, they struggle to understand the composition of
certain concepts, such as confusing the attributes of different objects or
relations between objects. In this paper, we propose an alternative structured
approach for compositional generation using diffusion models. An image is
generated by composing a set of diffusion models, with each of them modeling a
certain component of the image. To do this, we interpret diffusion models as
energy-based models in which the data distributions defined by the energy
functions may be explicitly combined. The proposed method can generate scenes
at test time that are substantially more complex than those seen in training,
composing sentence descriptions, object relations, human facial attributes, and
even generalizing to new combinations that are rarely seen in the real world.
We further illustrate how our approach may be used to compose pre-trained
text-guided diffusion models and generate photorealistic images containing all
the details described in the input descriptions, including the binding of
certain object attributes that have been shown difficult for DALLE-2. These
results point to the effectiveness of the proposed method in promoting
structured generalization for visual generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge. (arXiv:2206.01718v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01718">
<div class="article-summary-box-inner">
<span><p>The Visual Question Answering (VQA) task aspires to provide a meaningful
testbed for the development of AI models that can jointly reason over visual
and natural language inputs. Despite a proliferation of VQA datasets, this goal
is hindered by a set of common limitations. These include a reliance on
relatively simplistic questions that are repetitive in both concepts and
linguistic structure, little world knowledge needed outside of the paired
image, and limited reasoning required to arrive at the correct answer. We
introduce A-OKVQA, a crowdsourced dataset composed of a diverse set of about
25K questions requiring a broad base of commonsense and world knowledge to
answer. In contrast to the existing knowledge-based VQA datasets, the questions
generally cannot be answered by simply querying a knowledge base, and instead
require some form of commonsense reasoning about the scene depicted in the
image. We demonstrate the potential of this new dataset through a detailed
analysis of its contents and baseline performance measurements over a variety
of state-of-the-art vision-language models. Project page:
<a href="http://a-okvqa.allenai.org/">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting the "Video" in Video-Language Understanding. (arXiv:2206.01720v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01720">
<div class="article-summary-box-inner">
<span><p>What makes a video task uniquely suited for videos, beyond what can be
understood from a single image? Building on recent progress in self-supervised
image-language models, we revisit this question in the context of video and
language tasks. We propose the atemporal probe (ATP), a new model for
video-language analysis which provides a stronger bound on the baseline
accuracy of multimodal models constrained by image-level understanding. By
applying this model to standard discriminative video and language tasks, such
as video question answering and text-to-video retrieval, we characterize the
limitations and potential of current video-language benchmarks. We find that
understanding of event temporality is often not necessary to achieve strong or
state-of-the-art performance, even compared with recent large-scale
video-language models and in contexts intended to benchmark deeper video-level
understanding. We also demonstrate how ATP can improve both video-language
dataset and model design. We describe a technique for leveraging ATP to better
disentangle dataset subsets with a higher concentration of temporally
challenging data, improving benchmarking efficacy for causal and temporal
understanding. Further, we show that effectively integrating ATP into full
video-level temporal models can improve efficiency and state-of-the-art
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SNAKE: Shape-aware Neural 3D Keypoint Field. (arXiv:2206.01724v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01724">
<div class="article-summary-box-inner">
<span><p>Detecting 3D keypoints from point clouds is important for shape
reconstruction, while this work investigates the dual question: can shape
reconstruction benefit 3D keypoint detection? Existing methods either seek
salient features according to statistics of different orders or learn to
predict keypoints that are invariant to transformation. Nevertheless, the idea
of incorporating shape reconstruction into 3D keypoint detection is
under-explored. We argue that this is restricted by former problem
formulations. To this end, a novel unsupervised paradigm named SNAKE is
proposed, which is short for shape-aware neural 3D keypoint field. Similar to
recent coordinate-based radiance or distance field, our network takes 3D
coordinates as inputs and predicts implicit shape indicators and keypoint
saliency simultaneously, thus naturally entangling 3D keypoint detection and
shape reconstruction. We achieve superior performance on various public
benchmarks, including standalone object datasets ModelNet40, KeypointNet, SMPL
meshes and scene-level datasets 3DMatch and Redwood. Intrinsic shape awareness
brings several advantages as follows. (1) SNAKE generates 3D keypoints
consistent with human semantic annotation, even without such supervision. (2)
SNAKE outperforms counterparts in terms of repeatability, especially when the
input point clouds are down-sampled. (3) the generated keypoints allow accurate
geometric registration, notably in a zero-shot setting. Codes are available at
https://github.com/zhongcl-thu/SNAKE
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GASP, a generalized framework for agglomerative clustering of signed graphs and its application to Instance Segmentation. (arXiv:1906.11713v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.11713">
<div class="article-summary-box-inner">
<span><p>We propose a theoretical framework that generalizes simple and fast
algorithms for hierarchical agglomerative clustering to weighted graphs with
both attractive and repulsive interactions between the nodes. This framework
defines GASP, a Generalized Algorithm for Signed graph Partitioning, and allows
us to explore many combinations of different linkage criteria and cannot-link
constraints. We prove the equivalence of existing clustering methods to some of
those combinations and introduce new algorithms for combinations that have not
been studied before. We study both theoretical and empirical properties of
these combinations and prove that some of these define an ultrametric on the
graph. We conduct a systematic comparison of various instantiations of GASP on
a large variety of both synthetic and existing signed clustering problems, in
terms of accuracy but also efficiency and robustness to noise. Lastly, we show
that some of the algorithms included in our framework, when combined with the
predictions from a CNN model, result in a simple bottom-up instance
segmentation pipeline. Going all the way from pixels to final segments with a
simple procedure, we achieve state-of-the-art accuracy on the CREMI 2016 EM
segmentation benchmark without requiring domain-specific superpixels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmented Equivariant Attention Networks for Microscopy Image Reconstruction. (arXiv:2011.03633v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.03633">
<div class="article-summary-box-inner">
<span><p>It is time-consuming and expensive to take high-quality or high-resolution
electron microscopy (EM) and fluorescence microscopy (FM) images. Taking these
images could be even invasive to samples and may damage certain subtleties in
the samples after long or intense exposures, often necessary for achieving
high-quality or high resolution in the first place. Advances in deep learning
enable us to perform image-to-image transformation tasks for various types of
microscopy image reconstruction, computationally producing high-quality images
from the physically acquired low-quality ones. When training image-to-image
transformation models on pairs of experimentally acquired microscopy images,
prior models suffer from performance loss due to their inability to capture
inter-image dependencies and common features shared among images. Existing
methods that take advantage of shared features in image classification tasks
cannot be properly applied to image reconstruction tasks because they fail to
preserve the equivariance property under spatial permutations, something
essential in image-to-image transformation. To address these limitations, we
propose the augmented equivariant attention networks (AEANets) with better
capability to capture inter-image dependencies, while preserving the
equivariance property. The proposed AEANets captures inter-image dependencies
and shared features via two augmentations on the attention mechanism, which are
the shared references and the batch-aware attention during training. We
theoretically derive the equivariance property of the proposed augmented
attention model and experimentally demonstrate its consistent superiority in
both quantitative and visual results over the baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Transformer Based Semantic Segmentation Scheme for Fine-Resolution Remote Sensing Images. (arXiv:2104.12137v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12137">
<div class="article-summary-box-inner">
<span><p>The fully convolutional network (FCN) with an encoder-decoder architecture
has been the standard paradigm for semantic segmentation. The encoder-decoder
architecture utilizes an encoder to capture multilevel feature maps, which are
incorporated into the final prediction by a decoder. As the context is crucial
for precise segmentation, tremendous effort has been made to extract such
information in an intelligent fashion, including employing dilated/atrous
convolutions or inserting attention modules. However, these endeavors are all
based on the FCN architecture with ResNet or other backbones, which cannot
fully exploit the context from the theoretical concept. By contrast, we
introduce the Swin Transformer as the backbone to extract the context
information and design a novel decoder of densely connected feature aggregation
module (DCFAM) to restore the resolution and produce the segmentation map. The
experimental results on two remotely sensed semantic segmentation datasets
demonstrate the effectiveness of the proposed scheme.Code is available at
https://github.com/WangLibo1995/GeoSeg
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Volumetric Image Segmentation with Deformed Templates. (arXiv:2106.03987v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03987">
<div class="article-summary-box-inner">
<span><p>There are many approaches to weakly-supervised training of networks to
segment 2D images. By contrast, existing approaches to segmenting volumetric
images rely on full-supervision of a subset of 2D slices of the 3D volume. We
propose an approach to volume segmentation that is truly weakly-supervised in
the sense that we only need to provide a sparse set of 3D points on the surface
of target objects instead of detailed 2D masks. We use the 3D points to deform
a 3D template so that it roughly matches the target object outlines and we
introduce an architecture that exploits the supervision it provides to train a
network to find accurate boundaries. We evaluate our approach on Computed
Tomography (CT), Magnetic Resonance Imagery (MRI) and Electron Microscopy (EM)
image datasets and show that it substantially reduces the required amount of
effort.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Source Data-Free Cross-Domain Semantic Segmentation: Align, Teach and Propagate. (arXiv:2106.11653v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11653">
<div class="article-summary-box-inner">
<span><p>Benefiting from considerable pixel-level annotations collected from a
specific situation (source), the trained semantic segmentation model performs
quite well but fails in a new situation (target). To mitigate the domain gap,
previous cross-domain semantic segmentation methods always assume the
co-existence of source data and target data during domain alignment. However,
accessing source data in the real scenario may raise privacy concerns and
violate intellectual property. To tackle this problem, we focus on an
interesting and challenging cross-domain semantic segmentation task where only
the trained source model is provided to the target domain. Specifically, we
propose a unified framework called \textbf{ATP}, which consists of three
schemes, i.e., feature \textbf{A}lignment, bidirectional \textbf{T}eaching, and
information \textbf{P}ropagation. First, considering explicit alignment is
infeasible due to no source data, we devise a curriculum-style entropy
minimization objective to implicitly align the target features with unseen
source features via the provided source model. Second, besides positive pseudo
labels in vanilla self-training, we introduce negative pseudo labels to this
field and develop a bidirectional self-training strategy to enhance the
representation learning in the target domain. It is the first work to use
negative pseudo labels during self-training for domain adaptation. Finally, the
information propagation scheme is employed to further reduce the intra-domain
discrepancy within the target domain via pseudo-semi-supervised learning, which
is the first step by providing a simple and effective post-process for the
domain adaptation field. Furthermore, we also extend the proposed to the more
challenging black-box source-model scenario where only the source model's
prediction is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiband VAE: Latent Space Alignment for Knowledge Consolidation in Continual Learning. (arXiv:2106.12196v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.12196">
<div class="article-summary-box-inner">
<span><p>We propose a new method for unsupervised generative continual learning
through realignment of Variational Autoencoder's latent space. Deep generative
models suffer from catastrophic forgetting in the same way as other neural
structures. Recent generative continual learning works approach this problem
and try to learn from new data without forgetting previous knowledge. However,
those methods usually focus on artificial scenarios where examples share almost
no similarity between subsequent portions of data - an assumption not realistic
in the real-life applications of continual learning. In this work, we identify
this limitation and posit the goal of generative continual learning as a
knowledge accumulation task. We solve it by continuously aligning latent
representations of new data that we call bands in additional latent space where
examples are encoded independently of their source task. In addition, we
introduce a method for controlled forgetting of past data that simplifies this
process. On top of the standard continual learning benchmarks, we propose a
novel challenging knowledge consolidation scenario and show that the proposed
approach outperforms state-of-the-art by up to twofold across all experiments
and the additional real-life evaluation. To our knowledge, Multiband VAE is the
first method to show forward and backward knowledge transfer in generative
continual learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer Meets Convolution: A Bilateral Awareness Network for Semantic Segmentation of Very Fine Resolution Urban Scene Images. (arXiv:2106.12413v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.12413">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation from very fine resolution (VFR) urban scene images
plays a significant role in several application scenarios including autonomous
driving, land cover classification, and urban planning, etc. However, the
tremendous details contained in the VFR image, especially the considerable
variations in scale and appearance of objects, severely limit the potential of
the existing deep learning approaches. Addressing such issues represents a
promising research field in the remote sensing community, which paves the way
for scene-level landscape pattern analysis and decision making. In this paper,
we propose a Bilateral Awareness Network which contains a dependency path and a
texture path to fully capture the long-range relationships and fine-grained
details in VFR images. Specifically, the dependency path is conducted based on
the ResT, a novel Transformer backbone with memory-efficient multi-head
self-attention, while the texture path is built on the stacked convolution
operation. Besides, using the linear attention mechanism, a feature aggregation
module is designed to effectively fuse the dependency features and texture
features. Extensive experiments conducted on the three large-scale urban scene
image segmentation datasets, i.e., ISPRS Vaihingen dataset, ISPRS Potsdam
dataset, and UAVid dataset, demonstrate the effectiveness of our BANet.
Specifically, a 64.6% mIoU is achieved on the UAVid dataset. Code is available
at https://github.com/WangLibo1995/GeoSeg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Cervical Whole Slide Image Analysis Framework Based on Multi-scale Semantic and Location Deep Features. (arXiv:2106.15113v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15113">
<div class="article-summary-box-inner">
<span><p>Digital gigapixel whole slide image (WSI) is widely used in clinical
diagnosis, and automated WSI analysis is key for computer-aided diagnosis.
Currently, analyzing the integrated descriptor of probabilities or feature maps
from massive local patches encoded by ResNet classifier is the main manner for
WSI-level prediction. Feature representations of the sparse and tiny lesion
cells in cervical slides, however, are still challenging, while the unused
location representations are available to supply the semantics classification.
This study designs a novel and efficient framework with a new module InCNet
constructed lightweight model YOLCO (You Only Look Cytology Once). It directly
extracts feature inside the single cell (cluster) instead of the traditional
way that from image tile with a fixed size. The InCNet (Inline Connection
Network) enriches the multi-scale connectivity without efficiency loss. The
proposal allows the input size enlarged to megapixel that can stitch the WSI by
the average repeats decreased from $10^3\sim10^4$ to $10^1\sim10^2$ for
collecting features and predictions at two scales. Based on Transformer for
classifying the integrated multi-scale multi-task WSI features, the
experimental results appear $0.872$ AUC score better than the best conventional
model on our dataset ($n$=2,019) from four scanners. The code is available at
https://github.com/Chrisa142857/You-Only-Look-Cytopathology-Once , where the
deployment version has the speed $\sim$70 s/WSI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NanoBatch Privacy: Enabling fast Differentially Private learning on the IPU. (arXiv:2109.12191v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12191">
<div class="article-summary-box-inner">
<span><p>Differentially private SGD (DPSGD) has recently shown promise in deep
learning. However, compared to non-private SGD, the DPSGD algorithm places
computational overheads that can undo the benefit of batching in GPUs.
Micro-batching is a common method to alleviate this and is fully supported in
the TensorFlow Privacy library (TFDP). However, it degrades accuracy. We
propose NanoBatch Privacy, a lightweight add-on to TFDP to be used on Graphcore
IPUs by leveraging batch size of 1 (without microbatching) and gradient
accumulation. This allows us to achieve large total batch sizes with minimal
impacts to throughput. Second, we illustrate using Cifar-10 how larger batch
sizes are not necessarily optimal from a privacy versus utility perspective. On
ImageNet, we achieve more than 15x speedup over TFDP versus 8x A100s and
significant speedups even across libraries such as Opacus. We also provide two
extensions: 1) DPSGD for pipelined models and 2) per-layer clipping that is 15x
faster than the Opacus implementation on 8x A100s. Finally as an application
case study, we apply NanoBatch training for use on private Covid-19 chest CT
prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D. (arXiv:2109.13410v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13410">
<div class="article-summary-box-inner">
<span><p>For the last few decades, several major subfields of artificial intelligence
including computer vision, graphics, and robotics have progressed largely
independently from each other. Recently, however, the community has realized
that progress towards robust intelligent systems such as self-driving cars
requires a concerted effort across the different fields. This motivated us to
develop KITTI-360, successor of the popular KITTI dataset. KITTI-360 is a
suburban driving dataset which comprises richer input modalities, comprehensive
semantic instance annotations and accurate localization to facilitate research
at the intersection of vision, graphics and robotics. For efficient annotation,
we created a tool to label 3D scenes with bounding primitives and developed a
model that transfers this information into the 2D image domain, resulting in
over 150k images and 1B 3D points with coherent semantic instance annotations
across 2D and 3D. Moreover, we established benchmarks and baselines for several
tasks relevant to mobile perception, encompassing problems from computer
vision, graphics, and robotics on the same dataset, e.g., semantic scene
understanding, novel view synthesis and semantic SLAM. KITTI-360 will enable
progress at the intersection of these research areas and thus contribute
towards solving one of today's grand challenges: the development of fully
autonomous self-driving systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction. (arXiv:2111.11215v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11215">
<div class="article-summary-box-inner">
<span><p>We present a super-fast convergence approach to reconstructing the per-scene
radiance field from a set of images that capture the scene with known poses.
This task, which is often applied to novel view synthesis, is recently
revolutionized by Neural Radiance Field (NeRF) for its state-of-the-art quality
and flexibility. However, NeRF and its variants require a lengthy training time
ranging from hours to days for a single scene. In contrast, our approach
achieves NeRF-comparable quality and converges rapidly from scratch in less
than 15 minutes with a single GPU. We adopt a representation consisting of a
density voxel grid for scene geometry and a feature voxel grid with a shallow
network for complex view-dependent appearance. Modeling with explicit and
discretized volume representations is not new, but we propose two simple yet
non-trivial techniques that contribute to fast convergence speed and
high-quality output. First, we introduce the post-activation interpolation on
voxel density, which is capable of producing sharp surfaces in lower grid
resolution. Second, direct voxel density optimization is prone to suboptimal
geometry solutions, so we robustify the optimization process by imposing
several priors. Finally, evaluation on five inward-facing benchmarks shows that
our method matches, if not surpasses, NeRF's quality, yet it only takes about
15 minutes to train from scratch for a new scene.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAE-DET: Revisiting Maximum Entropy Principle in Zero-Shot NAS for Efficient Object Detection. (arXiv:2111.13336v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13336">
<div class="article-summary-box-inner">
<span><p>In object detection, the detection backbone consumes more than half of the
overall inference cost. Recent researches attempt to reduce this cost by
optimizing the backbone architecture with the help of Neural Architecture
Search (NAS). However, existing NAS methods for object detection require
hundreds to thousands of GPU hours of searching, making them impractical in
fast-paced research and development. In this work, we propose a novel zero-shot
NAS method to address this issue. The proposed method, named MAE-DET,
automatically designs efficient detection backbones via the Maximum Entropy
Principle without training network parameters, reducing the architecture design
cost to nearly zero yet delivering the state-of-the-art (SOTA) performance.
Under the hood, MAE-DET maximizes the differential entropy of detection
backbones, leading to a better feature extractor for object detection under the
same computational budgets. After merely one GPU day of fully automatic design,
MAE-DET innovates SOTA detection backbones on multiple detection benchmark
datasets with little human intervention. Comparing to ResNet-50 backbone,
MAE-DET is $+2.0\%$ better in mAP when using the same amount of
FLOPs/parameters, and is $1.54$ times faster on NVIDIA V100 at the same mAP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PTCT: Patches with 3D-Temporal Convolutional Transformer Network for Precipitation Nowcasting. (arXiv:2112.01085v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01085">
<div class="article-summary-box-inner">
<span><p>Precipitation nowcasting is to predict the future rainfall intensity over a
short period of time, which mainly relies on the prediction of radar echo
sequences. Though convolutional neural network (CNN) and recurrent neural
network (RNN) are widely used to generate radar echo frames, they suffer from
inductive bias (i.e., translation invariance and locality) and seriality,
respectively. Recently, Transformer-based methods also gain much attention due
to the great potential of Transformer structure, whereas short-term
dependencies and autoregressive characteristic are ignored. In this paper, we
propose a variant of Transformer named patches with 3D-temporal convolutional
Transformer network (PTCT), where original frames are split into multiple
patches to remove the constraint of inductive bias and 3D-temporal convolution
is employed to capture short-term dependencies efficiently. After training, the
inference of PTCT is performed in an autoregressive way to ensure the quality
of generated radar echo frames. To validate our algorithm, we conduct
experiments on two radar echo dataset: Radar Echo Guangzhou and HKO-7. The
experimental results show that PTCT achieves state-of-the-art (SOTA)
performance compared with existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods. (arXiv:2112.04417v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04417">
<div class="article-summary-box-inner">
<span><p>A multitude of explainability methods and associated fidelity performance
metrics have been proposed to help better understand how modern AI systems make
decisions. However, much of the current work has remained theoretical --
without much consideration for the human end-user. In particular, it is not yet
known (1) how useful current explainability methods are in practice for more
real-world scenarios and (2) how well associated performance metrics accurately
predict how much knowledge individual explanations contribute to a human
end-user trying to understand the inner-workings of the system. To fill this
gap, we conducted psychophysics experiments at scale to evaluate the ability of
human participants to leverage representative attribution methods for
understanding the behavior of different image classifiers representing three
real-world scenarios: identifying bias in an AI system, characterizing the
visual strategy it uses for tasks that are too difficult for an untrained
non-expert human observer as well as understanding its failure cases. Our
results demonstrate that the degree to which individual attribution methods
help human participants better understand an AI system varied widely across
these scenarios. This suggests a critical need for the field to move past
quantitative improvements of current attribution methods towards the
development of complementary approaches that provide qualitatively different
sources of information to human end-users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MVSS-Net: Multi-View Multi-Scale Supervised Networks for Image Manipulation Detection. (arXiv:2112.08935v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08935">
<div class="article-summary-box-inner">
<span><p>The emergence of powerful image editing software has substantially
facilitated digital image tampering, leading to many security issues. Hence, it
is urgent to identify tampered images and localize tampered regions. Although
much attention has been devoted to image tampering localization in recent
years, it is still challenging to perform tampering localization in practical
forensic applications. The reasons include the difficulty of learning
discriminative representations of tampering traces and the lack of realistic
tampered images for training. Since Photoshop is widely used for image
tampering in practice, this paper attempts to address the issue of tampering
localization by focusing on the detection of commonly used editing tools and
operations in Photoshop. In order to well capture tampering traces, a fully
convolutional encoder-decoder architecture is designed, where dense connections
and dilated convolutions are adopted for achieving better localization
performance. In order to effectively train a model in the case of insufficient
tampered images, we design a training data generation strategy by resorting to
Photoshop scripting, which can imitate human manipulations and generate
large-scale training samples. Extensive experimental results show that the
proposed approach outperforms state-of-the-art competitors when the model is
trained with only generated images or fine-tuned with a small amount of
realistic tampered images. The proposed method also has good robustness against
some common post-processing operations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Framework to Jointly Compress and Index Remote Sensing Images for Efficient Content-Based Retrieval. (arXiv:2201.06459v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06459">
<div class="article-summary-box-inner">
<span><p>Remote sensing (RS) images are usually stored in compressed format to reduce
the storage size of the archives. Thus, existing content-based image retrieval
(CBIR) systems in RS require decoding images before applying CBIR (which is
computationally demanding in the case of large-scale CBIR problems). To address
this problem, in this paper, we present a joint framework that simultaneously
learns RS image compression and indexing. Thus, it eliminates the need for
decoding RS images before applying CBIR. The proposed framework is made up of
two modules. The first module compresses RS images based on an auto-encoder
architecture. The second module produces hash codes with a high discrimination
capability by employing soft pairwise, bit-balancing and classification loss
functions. We also introduce a two stage learning strategy with gradient
manipulation techniques to obtain image representations that are compatible
with both RS image indexing and compression. Experimental results show the
efficacy of the proposed framework when compared to widely used approaches in
RS. The code of the proposed framework is available at
https://git.tu-berlin.de/rsim/RS-JCIF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Deep Contrastive Learning via Coordinate-wise Optimization. (arXiv:2201.12680v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12680">
<div class="article-summary-box-inner">
<span><p>We show that Contrastive Learning (CL) under a broad family of loss functions
(including InfoNCE) has a unified formulation of coordinate-wise optimization
on the network parameter $\boldsymbol{\theta}$ and pairwise importance
$\alpha$, where the \emph{max player} $\boldsymbol{\theta}$ learns
representation for contrastiveness, and the \emph{min player} $\alpha$ puts
more weights on pairs of distinct samples that share similar representations.
The resulting formulation, called $\alpha$-CL, unifies not only various
existing contrastive losses, which differ by how sample-pair importance
$\alpha$ is constructed, but also is able to extrapolate to give novel
contrastive losses beyond popular ones, opening a new avenue of contrastive
loss design. These novel losses yield comparable (or better) performance on
CIFAR10 and STL-10 than classic InfoNCE. Furthermore, we also analyze the max
player in detail: we prove that with fixed $\alpha$, max player is equivalent
to Principal Component Analysis (PCA) for deep linear network, and almost all
local minima are global and rank-1, recovering optimal PCA solutions. Finally,
we extend our analysis on max player to 2-layer ReLU networks, showing that its
fixed points can have higher ranks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional Scene Representation Learning via Reconstruction: A Survey. (arXiv:2202.07135v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07135">
<div class="article-summary-box-inner">
<span><p>Visual scene representation learning is an important research problem in the
field of computer vision. The performance of artificial intelligence systems on
vision tasks could be improved if more suitable representations are learned for
visual scenes. Complex visual scenes are composed of relatively simple visual
concepts, and have the property of combinatorial explosion. Compared with
directly representing the entire visual scene, extracting compositional scene
representations can better cope with the diverse combinations of background and
objects. Because compositional scene representations abstract the concept of
objects, performing visual scene analysis and understanding based on these
representations could be easier and more interpretable. Moreover, learning via
reconstruction can greatly reduce the need for training data annotations.
Therefore, reconstruction-based compositional scene representation learning has
important research significance. In this survey, we first outline the current
progress on this research topic, including development history and
categorizations of existing methods from the perspectives of modeling of visual
scenes and inference of scene representations; then provide benchmarks,
including an open source toolbox to reproduce the benchmark experiments, of
representative methods that consider the most extensively studied problem
setting and form the foundation for other methods; and finally discuss the
future directions of this research topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UncertaINR: Uncertainty Quantification of End-to-End Implicit Neural Representations for Computed Tomography. (arXiv:2202.10847v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10847">
<div class="article-summary-box-inner">
<span><p>Implicit neural representations (INRs) have achieved impressive results for
scene reconstruction and computer graphics, where their performance has
primarily been assessed on reconstruction accuracy. As INRs make their way into
other domains, where model predictions inform high-stakes decision-making,
uncertainty quantification of INR inference is becoming critical. To that end,
we study a Bayesian reformulation of INRs, UncertaINR, in the context of
computed tomography, and evaluate several Bayesian deep learning
implementations in terms of accuracy and calibration. We find that they achieve
well-calibrated uncertainty, while retaining accuracy competitive with other
classical, INR-based, and CNN-based reconstruction techniques. In contrast to
the best-performing prior approaches, UncertaINR does not require a large
training dataset, but only a handful of validation images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Edge Augmentation for Large-Scale Sketch Recognition without Sketches. (arXiv:2202.13164v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13164">
<div class="article-summary-box-inner">
<span><p>This work addresses scaling up the sketch classification task into a large
number of categories. Collecting sketches for training is a slow and tedious
process that has so far precluded any attempts to large-scale sketch
recognition. We overcome the lack of training sketch data by exploiting labeled
collections of natural images that are easier to obtain. To bridge the domain
gap we present a novel augmentation technique that is tailored to the task of
learning sketch recognition from a training set of natural images.
Randomization is introduced in the parameters of edge detection and edge
selection. Natural images are translated to a pseudo-novel domain called
"randomized Binary Thin Edges" (rBTE), which is used as a training domain
instead of natural images. The ability to scale up is demonstrated by training
CNN-based sketch recognition of more than 2.5 times larger number of categories
than used previously. For this purpose, a dataset of natural images from 874
categories is constructed by combining a number of popular computer vision
datasets. The categories are selected to be suitable for sketch recognition. To
estimate the performance, a subset of 393 categories with sketches is also
collected.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth-SIMS: Semi-Parametric Image and Depth Synthesis. (arXiv:2203.03405v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03405">
<div class="article-summary-box-inner">
<span><p>In this paper we present a compositing image synthesis method that generates
RGB canvases with well aligned segmentation maps and sparse depth maps, coupled
with an in-painting network that transforms the RGB canvases into high quality
RGB images and the sparse depth maps into pixel-wise dense depth maps. We
benchmark our method in terms of structural alignment and image quality,
showing an increase in mIoU over SOTA by 3.7 percentage points and a highly
competitive FID. Furthermore, we analyse the quality of the generated data as
training data for semantic segmentation and depth completion, and show that our
approach is more suited for this purpose than other methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards End-to-End Unified Scene Text Detection and Layout Analysis. (arXiv:2203.15143v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15143">
<div class="article-summary-box-inner">
<span><p>Scene text detection and document layout analysis have long been treated as
two separate tasks in different image domains. In this paper, we bring them
together and introduce the task of unified scene text detection and layout
analysis. The first hierarchical scene text dataset is introduced to enable
this novel research task. We also propose a novel method that is able to
simultaneously detect scene text and form text clusters in a unified way.
Comprehensive experiments show that our unified model achieves better
performance than multiple well-designed baseline methods. Additionally, this
model achieves state-of-the-art results on multiple scene text detection
datasets without the need of complex post-processing. Dataset and code:
https://github.com/google-research-datasets/hiertext and
https://github.com/tensorflow/models/tree/master/official/projects/unified_detector.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Document Recognition and Understanding with Dessurt. (arXiv:2203.16618v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16618">
<div class="article-summary-box-inner">
<span><p>We introduce Dessurt, a relatively simple document understanding transformer
capable of being fine-tuned on a greater variety of document tasks than prior
methods. It receives a document image and task string as input and generates
arbitrary text autoregressively as output. Because Dessurt is an end-to-end
architecture that performs text recognition in addition to the document
understanding, it does not require an external recognition model as prior
methods do. Dessurt is a more flexible model than prior methods and is able to
handle a variety of document domains and tasks. We show that this model is
effective at 9 different dataset-task combinations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Visual Prompts for Adapting Large-Scale Models. (arXiv:2203.17274v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17274">
<div class="article-summary-box-inner">
<span><p>We investigate the efficacy of visual prompting to adapt large-scale models
in vision. Following the recent approach from prompt tuning and adversarial
reprogramming, we learn a single image perturbation such that a frozen model
prompted with this perturbation performs a new task. Through comprehensive
experiments, we demonstrate that visual prompting is particularly effective for
CLIP and robust to distribution shift, achieving performance competitive with
standard linear probes. We further analyze properties of the downstream
dataset, prompt design, and output transformation in regard to adaptation
performance. The surprising effectiveness of visual prompting provides a new
perspective on adapting pre-trained models in vision. Code is available at
<a href="http://hjbahng.github.io/visual_prompting">this http URL</a> .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Powering Finetuning in Few-Shot Learning: Domain-Agnostic Bias Reduction with Selected Sampling. (arXiv:2204.03749v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03749">
<div class="article-summary-box-inner">
<span><p>In recent works, utilizing a deep network trained on meta-training set serves
as a strong baseline in few-shot learning. In this paper, we move forward to
refine novel-class features by finetuning a trained deep network. Finetuning is
designed to focus on reducing biases in novel-class feature distributions,
which we define as two aspects: class-agnostic and class-specific biases.
Class-agnostic bias is defined as the distribution shifting introduced by
domain difference, which we propose Distribution Calibration Module(DCM) to
reduce. DCM owes good property of eliminating domain difference and fast
feature adaptation during optimization. Class-specific bias is defined as the
biased estimation using a few samples in novel classes, which we propose
Selected Sampling(SS) to reduce. Without inferring the actual class
distribution, SS is designed by running sampling using proposal distributions
around support-set samples. By powering finetuning with DCM and SS, we achieve
state-of-the-art results on Meta-Dataset with consistent performance boosts
over ten datasets from different domains. We believe our simple yet effective
method demonstrates its possibility to be applied on practical few-shot
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding. (arXiv:2204.08129v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08129">
<div class="article-summary-box-inner">
<span><p>Understanding animals' behaviors is significant for a wide range of
applications. However, existing animal behavior datasets have limitations in
multiple aspects, including limited numbers of animal classes, data samples and
provided tasks, and also limited variations in environmental conditions and
viewpoints. To address these limitations, we create a large and diverse
dataset, Animal Kingdom, that provides multiple annotated tasks to enable a
more thorough understanding of natural animal behaviors. The wild animal
footages used in our dataset record different times of the day in extensive
range of environments containing variations in backgrounds, viewpoints,
illumination and weather conditions. More specifically, our dataset contains 50
hours of annotated videos to localize relevant animal behavior segments in long
videos for the video grounding task, 30K video sequences for the fine-grained
multi-label action recognition task, and 33K frames for the pose estimation
task, which correspond to a diverse range of animals with 850 species across 6
major animal classes. Such a challenging and comprehensive dataset shall be
able to facilitate the community to develop, adapt, and evaluate various types
of advanced methods for animal behavior analysis. Moreover, we propose a
Collaborative Action Recognition (CARe) model that learns general and specific
features for action recognition with unseen new animals. This method achieves
promising performance in our experiments. Our dataset can be found at
https://sutdcv.github.io/Animal-Kingdom.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HierAttn: Effectively Learn Representations from Stage Attention and Branch Attention for Skin Lesions Diagnosis. (arXiv:2205.04326v5 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04326">
<div class="article-summary-box-inner">
<span><p>Accurate and unbiased examinations of skin lesions are critical for the early
diagnosis and treatment of skin conditions and disorders. Visual features of
skin lesions vary significantly because the images are collected from patients
with different lesion colours and morphologies by using dissimilar imaging
equipment. Recent studies have reported ensembled convolutional neural networks
(CNNs) to classify the images for early diagnosis of skin disorders. However,
the practical use of these ensembled CNNs is limited because they are
heavyweight and inadequate for using contextual information. Although
lightweight networks (e.g., MobileNetV3 and EfficientNet) were developed to
achieve parameters reduction for implementing deep neural networks on mobile
devices, insufficient depth of feature representation restricts the
performance. To address the existing limitations, we introduce a new lite and
effective neural network, namely HierAttn. The HierAttn applies a novel
strategy to learn the local and global features by using multi-stage and
multi-branch attention mechanisms. The efficacy of HierAttn was evaluated by
using the dermoscopy images dataset ISIC2019 and smartphone photos dataset
PAD-UFES-20 (PAD20). The experimental results show that HierAttn achieves the
best accuracy and AUC among the state-of-the-art lightweight networks. The code
is available at https://github.com/anthonyweidai/HierAttn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gender and Racial Bias in Visual Question Answering Datasets. (arXiv:2205.08148v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08148">
<div class="article-summary-box-inner">
<span><p>Vision-and-language tasks have increasingly drawn more attention as a means
to evaluate human-like reasoning in machine learning models. A popular task in
the field is visual question answering (VQA), which aims to answer questions
about images. However, VQA models have been shown to exploit language bias by
learning the statistical correlations between questions and answers without
looking into the image content: e.g., questions about the color of a banana are
answered with yellow, even if the banana in the image is green. If societal
bias (e.g., sexism, racism, ableism, etc.) is present in the training data,
this problem may be causing VQA models to learn harmful stereotypes. For this
reason, we investigate gender and racial bias in five VQA datasets. In our
analysis, we find that the distribution of answers is highly different between
questions about women and men, as well as the existence of detrimental
gender-stereotypical samples. Likewise, we identify that specific race-related
attributes are underrepresented, whereas potentially discriminatory samples
appear in the analyzed datasets. Our findings suggest that there are dangers
associated to using VQA datasets without considering and dealing with the
potentially harmful stereotypes. We conclude the paper by proposing solutions
to alleviate the problem before, during, and after the dataset collection
process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning with Boosted Memorization. (arXiv:2205.12693v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12693">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning has achieved a great success in the representation
learning of visual and textual data. However, the current methods are mainly
validated on the well-curated datasets, which do not exhibit the real-world
long-tailed distribution. Recent attempts to consider self-supervised
long-tailed learning are made by rebalancing in the loss perspective or the
model perspective, resembling the paradigms in the supervised long-tailed
learning. Nevertheless, without the aid of labels, these explorations have not
shown the expected significant promise due to the limitation in tail sample
discovery or the heuristic structure design. Different from previous works, we
explore this direction from an alternative perspective, i.e., the data
perspective, and propose a novel Boosted Contrastive Learning (BCL) method.
Specifically, BCL leverages the memorization effect of deep neural networks to
automatically drive the information discrepancy of the sample views in
contrastive learning, which is more efficient to enhance the long-tailed
learning in the label-unaware context. Extensive experiments on a range of
benchmark datasets demonstrate the effectiveness of BCL over several
state-of-the-art methods. Our code is available at
https://github.com/MediaBrain-SJTU/BCL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning-based Lung and Colon Cancer Detection using Deep Feature Extraction and Ensemble Learning. (arXiv:2206.01088v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01088">
<div class="article-summary-box-inner">
<span><p>Cancer is a fatal disease caused by a combination of genetic diseases and a
variety of biochemical abnormalities. Lung and colon cancer have emerged as two
of the leading causes of death and disability in humans. The histopathological
detection of such malignancies is usually the most important component in
determining the best course of action. Early detection of the ailment on either
front considerably decreases the likelihood of mortality. Machine learning and
deep learning techniques can be utilized to speed up such cancer detection,
allowing researchers to study a large number of patients in a much shorter
amount of time and at a lower cost. In this research work, we introduced a
hybrid ensemble feature extraction model to efficiently identify lung and colon
cancer. It integrates deep feature extraction and ensemble learning with
high-performance filtering for cancer image datasets. The model is evaluated on
histopathological (LC25000) lung and colon datasets. According to the study
findings, our hybrid model can detect lung, colon, and (lung and colon) cancer
with accuracy rates of 99.05%, 100%, and 99.30%, respectively. The study's
findings show that our proposed strategy outperforms existing models
significantly. Thus, these models could be applicable in clinics to support the
doctor in the diagnosis of cancers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives. (arXiv:2206.01136v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01136">
<div class="article-summary-box-inner">
<span><p>Transformer, the latest technological advance of deep learning, has gained
prevalence in natural language processing or computer vision. Since medical
imaging bear some resemblance to computer vision, it is natural to inquire
about the status quo of Transformers in medical imaging and ask the question:
can the Transformer models transform medical imaging? In this paper, we attempt
to make a response to the inquiry. After a brief introduction of the
fundamentals of Transformers, especially in comparison with convolutional
neural networks (CNNs), and highlighting key defining properties that
characterize the Transformers, we offer a comprehensive review of the
state-of-the-art Transformer-based approaches for medical imaging and exhibit
current research progresses made in the areas of medical image segmentation,
recognition, detection, registration, reconstruction, enhancement, etc. In
particular, what distinguishes our review lies in its organization based on the
Transformer's key defining properties, which are mostly derived from comparing
the Transformer and CNN, and its type of architecture, which specifies the
manner in which the Transformer and CNN are combined, all helping the readers
to best understand the rationale behind the reviewed approaches. We conclude
with discussions of future perspectives.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-06-06 23:08:16.933599020 UTC">2022-06-06 23:08:16 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>