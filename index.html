<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-06-30T01:30:00Z">06-30</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple and Effective Knowledge-Driven Query Expansion for QA-Based Product Attribute Extraction. (arXiv:2206.14264v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14264">
<div class="article-summary-box-inner">
<span><p>A key challenge in attribute value extraction (AVE) from e-commerce sites is
how to handle a large number of attributes for diverse products. Although this
challenge is partially addressed by a question answering (QA) approach which
finds a value in product data for a given query (attribute), it does not work
effectively for rare and ambiguous queries. We thus propose simple
knowledge-driven query expansion based on possible answers (values) of a query
(attribute) for QA-based AVE. We retrieve values of a query (attribute) from
the training data to expand the query. We train a model with two tricks,
knowledge dropout and knowledge token mixing, which mimic the imperfection of
the value knowledge in testing. Experimental results on our cleaned version of
AliExpress dataset show that our method improves the performance of AVE (+6.08
macro F1), especially for rare and ambiguous attributes (+7.82 and +6.86 macro
F1, respectively).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BertNet: Harvesting Knowledge Graphs from Pretrained Language Models. (arXiv:2206.14268v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14268">
<div class="article-summary-box-inner">
<span><p>Symbolic knowledge graphs (KGs) have been constructed either by expensive
human crowdsourcing or with domain-specific complex information extraction
pipelines. The emerging large pretrained language models (LMs), such as Bert,
have shown to implicitly encode massive knowledge which can be queried with
properly designed prompts. However, compared to the explicit KGs, the implict
knowledge in the black-box LMs is often difficult to access or edit and lacks
explainability. In this work, we aim at harvesting symbolic KGs from the LMs, a
new framework for automatic KG construction empowered by the neural LMs'
flexibility and scalability. Compared to prior works that often rely on large
human annotated data or existing massive KGs, our approach requires only the
minimal definition of relations as inputs, and hence is suitable for extracting
knowledge of rich new relations not available before.The approach automatically
generates diverse prompts, and performs efficient knowledge search within a
given LM for consistent and extensive outputs. The harvested knowledge with our
approach is substantially more accurate than with previous methods, as shown in
both automatic and human evaluation. As a result, we derive from diverse LMs a
family of new KGs (e.g., BertNet and RoBERTaNet) that contain a richer set of
commonsense relations, including complex ones (e.g., "A is capable of but not
good at B"), than the human-annotated KGs (e.g., ConceptNet). Besides, the
resulting KGs also serve as a vehicle to interpret the respective source LMs,
leading to new insights into the varying knowledge capability of different LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collecting high-quality adversarial data for machine reading comprehension tasks with humans and models in the loop. (arXiv:2206.14272v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14272">
<div class="article-summary-box-inner">
<span><p>We present our experience as annotators in the creation of high-quality,
adversarial machine-reading-comprehension data for extractive QA for Task 1 of
the First Workshop on Dynamic Adversarial Data Collection (DADC). DADC is an
emergent data collection paradigm with both models and humans in the loop. We
set up a quasi-experimental annotation design and perform quantitative analyses
across groups with different numbers of annotators focusing on successful
adversarial attacks, cost analysis, and annotator confidence correlation. We
further perform a qualitative analysis of our perceived difficulty of the task
given the different topics of the passages in our dataset and conclude with
recommendations and suggestions that might be of value to people working on
future DADC tasks and related annotation interfaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bottleneck Low-rank Transformers for Low-resource Spoken Language Understanding. (arXiv:2206.14318v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14318">
<div class="article-summary-box-inner">
<span><p>End-to-end spoken language understanding (SLU) systems benefit from
pretraining on large corpora, followed by fine-tuning on application-specific
data. The resulting models are too large for on-edge applications. For
instance, BERT-based systems contain over 110M parameters. Observing the model
is overparameterized, we propose lean transformer structure where the dimension
of the attention mechanism is automatically reduced using group sparsity. We
propose a variant where the learned attention subspace is transferred to an
attention bottleneck layer. In a low-resource setting and without pre-training,
the resulting compact SLU model achieves accuracies competitive with
pre-trained large models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Can Secondary Predictions Tell Us? An Exploration on Question-Answering with SQuAD-v2.0. (arXiv:2206.14348v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14348">
<div class="article-summary-box-inner">
<span><p>Performance in natural language processing, and specifically for the
question-answer task, is typically measured by comparing a model\'s most
confident (primary) prediction to golden answers (the ground truth). We are
making the case that it is also useful to quantify how close a model came to
predicting a correct answer even for examples that failed. We define the Golden
Rank (GR) of an example as the rank of its most confident prediction that
exactly matches a ground truth, and show why such a match always exists. For
the 16 transformer models we analyzed, the majority of exactly matched golden
answers in secondary prediction space hover very close to the top rank. We
refer to secondary predictions as those ranking above 0 in descending
confidence probability order. We demonstrate how the GR can be used to classify
questions and visualize their spectrum of difficulty, from persistent near
successes to persistent extreme failures. We derive a new aggregate statistic
over entire test sets, named the Golden Rank Interpolated Median (GRIM) that
quantifies the proximity of failed predictions to the top choice made by the
model. To develop some intuition and explore the applicability of these metrics
we use the Stanford Question Answering Dataset (SQuAD-2) and a few popular
transformer models from the Hugging Face hub. We first demonstrate that the
GRIM is not directly correlated with the F1 and exact match (EM) scores. We
then calculate and visualize these scores for various transformer
architectures, probe their applicability in error analysis by clustering failed
predictions, and compare how they relate to other training diagnostics such as
the EM and F1 scores. We finally suggest various research goals, such as
broadening data collection for these metrics and their possible use in
adversarial training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EBMs vs. CL: Exploring Self-Supervised Visual Pretraining for Visual Question Answering. (arXiv:2206.14355v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14355">
<div class="article-summary-box-inner">
<span><p>The availability of clean and diverse labeled data is a major roadblock for
training models on complex tasks such as visual question answering (VQA). The
extensive work on large vision-and-language models has shown that
self-supervised learning is effective for pretraining multimodal interactions.
In this technical report, we focus on visual representations. We review and
evaluate self-supervised methods to leverage unlabeled images and pretrain a
model, which we then fine-tune on a custom VQA task that allows controlled
evaluation and diagnosis. We compare energy-based models (EBMs) with
contrastive learning (CL). While EBMs are growing in popularity, they lack an
evaluation on downstream tasks. We find that both EBMs and CL can learn
representations from unlabeled images that enable training a VQA model on very
little annotated data. In a simple setting similar to CLEVR, we find that CL
representations also improve systematic generalization, and even match the
performance of representations from a larger, supervised, ImageNet-pretrained
model. However, we find EBMs to be difficult to train because of instabilities
and high variability in their results. Although EBMs prove useful for OOD
detection, other results on supervised energy-based training and uncertainty
calibration are largely negative. Overall, CL currently seems a preferable
option over EBMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Twitter Data to Understand Public Perceptions of Approved versus Off-label Use for COVID-19-related Medications. (arXiv:2206.14358v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14358">
<div class="article-summary-box-inner">
<span><p>Understanding public discourse on emergency use of unproven therapeutics is
essential to monitor safe use and combat misinformation. We developed a natural
language processing (NLP)-based pipeline to understand public perceptions of
and stances on COVID-19-related drugs on Twitter across time. This
retrospective study included 609,189 US-based tweets between January 29th, 2020
and November 30th, 2021 on four drugs that gained wide public attention during
the COVID-19 pandemic: 1) Hydroxychloroquine and Ivermectin, drug therapies
with anecdotal evidence; and 2) Molnupiravir and Remdesivir, FDA-approved
treatment options for eligible patients. Time-trend analysis was used to
understand the popularity and related events. Content and demographic analyses
were conducted to explore potential rationales of people's stances on each
drug. Time-trend analysis revealed that Hydroxychloroquine and Ivermectin
received much more discussion than Molnupiravir and Remdesivir, particularly
during COVID-19 surges. Hydroxychloroquine and Ivermectin were highly
politicized, related to conspiracy theories, hearsay, celebrity effects, etc.
The distribution of stance between the two major US political parties was
significantly different (p&lt;0.001); Republicans were much more likely to support
Hydroxychloroquine (+55%) and Ivermectin (+30%) than Democrats. People with
healthcare backgrounds tended to oppose Hydroxychloroquine (+7%) more than the
general population; in contrast, the general population was more likely to
support Ivermectin (+14%). We make all the data, code, and models available at
https://github.com/ningkko/COVID-drug.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Distillation of Transformer-based Language Models Revisited. (arXiv:2206.14366v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14366">
<div class="article-summary-box-inner">
<span><p>In the past few years, transformer-based pre-trained language models have
achieved astounding success in both industry and academia. However, the large
model size and high run-time latency are serious impediments to applying them
in practice, especially on mobile phones and Internet of Things (IoT) devices.
To compress the model, considerable literature has grown up around the theme of
knowledge distillation (KD) recently. Nevertheless, how KD works in
transformer-based models is still unclear. We tease apart the components of KD
and propose a unified KD framework. Through the framework, systematic and
extensive experiments that spent over 23,000 GPU hours render a comprehensive
analysis from the perspectives of knowledge types, matching strategies,
width-depth trade-off, initialization, model size, etc. Our empirical results
shed light on the distillation in the pre-train language model and with
relative significant improvement over previous state-of-the-arts(SOTA).
Finally, we provide a best-practice guideline for the KD in transformer-based
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chinese Word Sense Embedding with SememeWSD and Synonym Set. (arXiv:2206.14388v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14388">
<div class="article-summary-box-inner">
<span><p>Word embedding is a fundamental natural language processing task which can
learn feature of words. However, most word embedding methods assign only one
vector to a word, even if polysemous words have multi-senses. To address this
limitation, we propose SememeWSD Synonym (SWSDS) model to assign a different
vector to every sense of polysemous words with the help of word sense
disambiguation (WSD) and synonym set in OpenHowNet. We use the SememeWSD model,
an unsupervised word sense disambiguation model based on OpenHowNet, to do word
sense disambiguation and annotate the polysemous word with sense id. Then, we
obtain top 10 synonyms of the word sense from OpenHowNet and calculate the
average vector of synonyms as the vector of the word sense. In experiments, We
evaluate the SWSDS model on semantic similarity calculation with Gensim's
wmdistance method. It achieves improvement of accuracy. We also examine the
SememeWSD model on different BERT models to find the more effective model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GERNERMED++: Transfer Learning in German Medical NLP. (arXiv:2206.14504v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14504">
<div class="article-summary-box-inner">
<span><p>We present a statistical model for German medical natural language processing
trained for named entity recognition (NER) as an open, publicly available
model. The work serves as a refined successor to our first GERNERMED model
which is substantially outperformed by our work. We demonstrate the
effectiveness of combining multiple techniques in order to achieve strong
results in entity recognition performance by the means of transfer-learning on
pretrained deep language models (LM), word-alignment and neural machine
translation. Due to the sparse situation on open, public medical entity
recognition models for German texts, this work offers benefits to the German
research community on medical NLP as a baseline model. Since our model is based
on public English data, its weights are provided without legal restrictions on
usage and distribution. The sample code and the statistical model is available
at: https://github.com/frankkramer-lab/GERNERMED-pp
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Graph Fusion for Language Model Fine-tuning. (arXiv:2206.14574v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14574">
<div class="article-summary-box-inner">
<span><p>Language Models such as BERT have grown in popularity due to their ability to
be pre-trained and perform robustly on a wide range of Natural Language
Processing tasks. Often seen as an evolution over traditional word embedding
techniques, they can produce semantic representations of text, useful for tasks
such as semantic similarity. However, state-of-the-art models often have high
computational requirements and lack global context or domain knowledge which is
required for complete language understanding. To address these limitations, we
investigate the benefits of knowledge incorporation into the fine-tuning stages
of BERT. An existing K-BERT model, which enriches sentences with triplets from
a Knowledge Graph, is adapted for the English language and extended to inject
contextually relevant information into sentences. As a side-effect, changes
made to K-BERT for accommodating the English language also extend to other
word-based languages. Experiments conducted indicate that injected knowledge
introduces noise. We see statistically significant improvements for
knowledge-driven tasks when this noise is minimised. We show evidence that,
given the appropriate task, modest injection with relevant, high-quality
knowledge is most performant.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why Robust Natural Language Understanding is a Challenge. (arXiv:2206.14575v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14575">
<div class="article-summary-box-inner">
<span><p>With the proliferation of Deep Machine Learning into real-life applications,
a particular property of this technology has been brought to attention: Neural
Networks notoriously present low robustness and can be highly sensitive to
small input perturbations. Recently, many methods for verifying networks'
general properties of robustness have been proposed, but they are mostly
applied in Computer Vision. In this paper we propose a Verification method for
Natural Language Understanding classification based on larger regions of
interest, and we discuss the challenges of such task. We observe that, although
the data is almost linearly separable, the verifier does not output positive
results and we explain the problems and implications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using cognitive psychology to understand GPT-3. (arXiv:2206.14576v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14576">
<div class="article-summary-box-inner">
<span><p>We study GPT-3, a recent large language model, using tools from cognitive
psychology. More specifically, we assess GPT-3's decision-making, information
search, deliberation, and causal reasoning abilities on a battery of canonical
experiments from the literature. We find that much of GPT-3's behavior is
impressive: it solves vignette-based tasks similarly or better than human
subjects, is able to make decent decisions from descriptions, outperforms
humans in a multi-armed bandit task, and shows signatures of model-based
reinforcement learning. Yet we also find that small perturbations to
vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures
of directed exploration, and that it fails miserably in a causal reasoning
task. These results enrich our understanding of current large language models
and pave the way for future investigations using tools from cognitive
psychology to study increasingly capable and opaque artificial agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Generative Patent Language Models. (arXiv:2206.14578v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14578">
<div class="article-summary-box-inner">
<span><p>This research aims to build generative language models in the patent domain
and to evaluate the models from a human-centric perspective. The evaluation
metric is to calculate the ratio of keystrokes that can be saved for a user in
an autocomplete context based on the prediction of the generative models. The
performance of models in different sizes can also be evaluated in such a metric
by measuring a number of newly granted patents. On the basis of the metric, it
is found that the largest model is not necessarily the best. Several models are
pre-trained from scratch with patent corpus and are released. The experiments
in this manuscript focus on patent claims, but the ideas and implementation can
be applied to other parts of a patent document. Furthermore, this research is
motivated to measure how close the pre-trained language model can generate a
newly granted patent claim. Or, conversely, the task is to measure the
probabilities for the model to generate each token text given the newly granted
patent claim. In addition, this manuscript raises several legal implications on
patent law for potential interdisciplinary research in the future. In
particular, can the metric based on model prediction be a metric to measure the
nonobviousness requirement in the patent law?
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Competence-based Multimodal Curriculum Learning for Medical Report Generation. (arXiv:2206.14579v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14579">
<div class="article-summary-box-inner">
<span><p>Medical report generation task, which targets to produce long and coherent
descriptions of medical images, has attracted growing research interests
recently. Different from the general image captioning tasks, medical report
generation is more challenging for data-driven neural models. This is mainly
due to 1) the serious data bias and 2) the limited medical data. To alleviate
the data bias and make best use of available data, we propose a
Competence-based Multimodal Curriculum Learning framework (CMCL). Specifically,
CMCL simulates the learning process of radiologists and optimizes the model in
a step by step manner. Firstly, CMCL estimates the difficulty of each training
instance and evaluates the competence of current model; Secondly, CMCL selects
the most suitable batch of training instances considering current model
competence. By iterating above two steps, CMCL can gradually improve the
model's performance. The experiments on the public IU-Xray and MIMIC-CXR
datasets show that CMCL can be incorporated into existing models to improve
their performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-specific Characteristic Assistance for Code-switching Speech Recognition. (arXiv:2206.14580v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14580">
<div class="article-summary-box-inner">
<span><p>Dual-encoder structure successfully utilizes two language-specific encoders
(LSEs) for code-switching speech recognition. Because LSEs are initialized by
two pre-trained language-specific models (LSMs), the dual-encoder structure can
exploit sufficient monolingual data and capture the individual language
attributes. However, existing methods have no language constraints on LSEs and
underutilize language-specific knowledge of LSMs. In this paper, we propose a
language-specific characteristic assistance (LSCA) method to mitigate the above
problems. Specifically, during training, we introduce two language-specific
losses as language constraints and generate corresponding language-specific
targets for them. During decoding, we take the decoding abilities of LSMs into
account by combining the output probabilities of two LSMs and the mixture model
to obtain the final predictions. Experiments show that either the training or
decoding method of LSCA can improve the model's performance. Furthermore, the
best result can obtain up to 15.4% relative error reduction on the
code-switching test set by combining the training and decoding methods of LSCA.
Moreover, the system can process code-switching speech recognition tasks well
without extra shared parameters or even retraining based on two pre-trained
LSMs by using our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finstreder: Simple and fast Spoken Language Understanding with Finite State Transducers using modern Speech-to-Text models. (arXiv:2206.14589v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14589">
<div class="article-summary-box-inner">
<span><p>In Spoken Language Understanding (SLU) the task is to extract important
information from audio commands, like the intent of what a user wants the
system to do and special entities like locations or numbers. This paper
presents a simple method for embedding intents and entities into Finite State
Transducers, and, in combination with a pretrained general-purpose
Speech-to-Text model, allows building SLU-models without any additional
training. Building those models is very fast and only takes a few seconds. It
is also completely language independent. With a comparison on different
benchmarks it is shown that this method can outperform multiple other, more
resource demanding SLU approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NERDA-Con: Extending NER models for Continual Learning -- Integrating Distinct Tasks and Updating Distribution Shifts. (arXiv:2206.14607v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14607">
<div class="article-summary-box-inner">
<span><p>With increasing applications in areas such as biomedical information
extraction pipelines and social media analytics, Named Entity Recognition (NER)
has become an indispensable tool for knowledge extraction. However, with the
gradual shift in language structure and vocabulary, NERs are plagued with
distribution shifts, making them redundant or not as profitable without
re-training. Re-training NERs based on Large Language Models (LLMs) from
scratch over newly acquired data poses economic disadvantages. In contrast,
re-training only with newly acquired data will result in Catastrophic
Forgetting of previously acquired knowledge. Therefore, we propose NERDA-Con, a
pipeline for training NERs with LLM bases by incorporating the concept of
Elastic Weight Consolidation (EWC) into the NER fine-tuning NERDA pipeline. As
we believe our work has implications to be utilized in the pipeline of
continual learning and NER, we open-source our code as well as provide the
fine-tuning library of the same name NERDA-Con at
https://github.com/SupritiVijay/NERDA-Con and
https://pypi.org/project/NERDA-Con/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multilingual Dataset of COVID-19 Vaccination Attitudes on Twitter. (arXiv:2206.14619v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14619">
<div class="article-summary-box-inner">
<span><p>Vaccine hesitancy is considered as one main cause of the stagnant uptake
ratio of COVID-19 vaccines in Europe and the US where vaccines are sufficiently
supplied. Fast and accurate grasp of public attitudes toward vaccination is
critical to address vaccine hesitancy, and social media platforms have proved
to be an effective source of public opinions. In this paper, we describe the
collection and release of a dataset of tweets related to COVID-19 vaccines.
This dataset consists of the IDs of 2,198,090 tweets collected from Western
Europe, 17,934 of which are annotated with the originators' vaccination
stances. Our annotation will facilitate using and developing data-driven models
to extract vaccination attitudes from social media posts and thus further
confirm the power of social media in public health surveillance. To lay the
groundwork for future research, we not only perform statistical analysis and
visualisation of our dataset, but also evaluate and compare the performance of
established text-based benchmarks in vaccination stance extraction. We
demonstrate one potential use of our data in practice in tracking the temporal
changes of public COVID-19 vaccination attitudes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Weighted Finite Automata from Recurrent Neural Networks for Natural Languages. (arXiv:2206.14621v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14621">
<div class="article-summary-box-inner">
<span><p>Recurrent Neural Networks (RNNs) have achieved tremendous success in
sequential data processing. However, it is quite challenging to interpret and
verify RNNs' behaviors directly. To this end, many efforts have been made to
extract finite automata from RNNs. Existing approaches such as exact learning
are effective in extracting finite-state models to characterize the state
dynamics of RNNs for formal languages, but are limited in the scalability to
process natural languages. Compositional approaches that are scablable to
natural languages fall short in extraction precision. In this paper, we
identify the transition sparsity problem that heavily impacts the extraction
precision. To address this problem, we propose a transition rule extraction
approach, which is scalable to natural language processing models and effective
in improving extraction precision. Specifically, we propose an empirical method
to complement the missing rules in the transition diagram. In addition, we
further adjust the transition matrices to enhance the context-aware ability of
the extracted weighted finite automaton (WFA). Finally, we propose two data
augmentation tactics to track more dynamic behaviors of the target RNN.
Experiments on two popular natural language datasets show that our method can
extract WFA from RNN for natural language processing with better precision than
existing approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple and Effective Multi-sentence TTS with Expressive and Coherent Prosody. (arXiv:2206.14643v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14643">
<div class="article-summary-box-inner">
<span><p>Generating expressive and contextually appropriate prosody remains a
challenge for modern text-to-speech (TTS) systems. This is particularly evident
for long, multi-sentence inputs. In this paper, we examine simple extensions to
a Transformer-based FastSpeech-like system, with the goal of improving prosody
for multi-sentence TTS. We find that long context, powerful text features, and
training on multi-speaker data all improve prosody. More interestingly, they
result in synergies. Long context disambiguates prosody, improves coherence,
and plays to the strengths of Transformers. Fine-tuning word-level features
from a powerful language model, such as BERT, appears to profit from more
training data, readily available in a multi-speaker setting. We look into
objective metrics on pausing and pacing and perform thorough subjective
evaluations for speech naturalness. Our main system, which incorporates all the
extensions, achieves consistently strong results, including statistically
significant improvements in speech naturalness over all its competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-Based Audio Retrieval with Converging Tied Layers and Contrastive Loss. (arXiv:2206.14659v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14659">
<div class="article-summary-box-inner">
<span><p>In this paper, we tackle the new Language-Based Audio Retrieval task proposed
in DCASE 2022. Firstly, we introduce a simple, scalable architecture which ties
both the audio and text encoder together. Secondly, we show that using this
architecture along with contrastive loss allows the model to significantly beat
the performance of the baseline model. Finally, in addition to having an
extremely low training memory requirement, we are able to use pretrained models
as it is without needing to finetune them. We test our methods and show that
using a combination of our methods beats the baseline scores significantly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The THUEE System Description for the IARPA OpenASR21 Challenge. (arXiv:2206.14660v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14660">
<div class="article-summary-box-inner">
<span><p>This paper describes the THUEE team's speech recognition system for the IARPA
Open Automatic Speech Recognition Challenge (OpenASR21), with further
experiment explorations. We achieve outstanding results under both the
Constrained and Constrained-plus training conditions. For the Constrained
training condition, we construct our basic ASR system based on the standard
hybrid architecture. To alleviate the Out-Of-Vocabulary (OOV) problem, we
extend the pronunciation lexicon using Grapheme-to-Phoneme (G2P) techniques for
both OOV and potential new words. Standard acoustic model structures such as
CNN-TDNN-F and CNN-TDNN-F-A are adopted. In addition, multiple data
augmentation techniques are applied. For the Constrained-plus training
condition, we use the self-supervised learning framework wav2vec2.0. We
experiment with various fine-tuning techniques with the Connectionist Temporal
Classification (CTC) criterion on top of the publicly available pre-trained
model XLSR-53. We find that the frontend feature extractor plays an important
role when applying the wav2vec2.0 pre-trained model to the encoder-decoder
based CTC/Attention ASR architecture. Extra improvements can be achieved by
using the CTC model finetuned in the target language as the frontend feature
extractor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Data-Driven Requirements Engineering Approach: Automatic Analysis of User Reviews. (arXiv:2206.14669v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14669">
<div class="article-summary-box-inner">
<span><p>We are concerned by Data Driven Requirements Engineering, and in particular
the consideration of user's reviews. These online reviews are a rich source of
information for extracting new needs and improvement requests. In this work, we
provide an automated analysis using CamemBERT, which is a state-of-the-art
language model in French. We created a multi-label classification dataset of
6000 user reviews from three applications in the Health &amp; Fitness field. The
results are encouraging and suggest that it's possible to identify
automatically the reviews concerning requests for new features.
</p>
<p>Dataset is available at:
https://github.com/Jl-wei/APIA2022-French-user-reviews-classification-dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is it possible not to cheat on the Turing Test: Exploring the potential and challenges for true natural language 'understanding' by computers. (arXiv:2206.14672v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14672">
<div class="article-summary-box-inner">
<span><p>Recent hype surrounding the increasing sophistication of language processing
models has renewed optimism regarding machines achieving a human-like command
of natural language. The area of natural language understanding in artificial
intelligence claims to have been making great strides in this area, however,
the lack of conceptual clarity in how 'understanding' is used in this and other
disciplines have made it difficult to discern how close we actually are. A
comprehensive, interdisciplinary overview of current approaches and remaining
challenges is yet to be carried out. Beyond linguistic knowledge, this requires
considering our species-specific capabilities to categorize, memorize, label
and communicate our (sufficiently similar) embodied and situated experiences.
Moreover, gauging the practical constraints requires critically analyzing the
technical capabilities of current models, as well as deeper philosophical
reflection on theoretical possibilities and limitations. In this paper, I unite
all of these perspectives -- the philosophical, cognitive-linguistic, and
technical -- to unpack the challenges involved in reaching true (human-like)
language understanding. By unpacking the theoretical assumptions inherent in
current approaches, I hope to illustrate how far we actually are from achieving
this goal, if indeed it is the goal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Deliberation by Text-Only and Semi-Supervised Training. (arXiv:2206.14716v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14716">
<div class="article-summary-box-inner">
<span><p>Text-only and semi-supervised training based on audio-only data has gained
popularity recently due to the wide availability of unlabeled text and speech
data. In this work, we propose incorporating text-only and semi-supervised
training into an attention-based deliberation model. By incorporating text-only
data in training a bidirectional encoder representation from transformer (BERT)
for the deliberation text encoder, and large-scale text-to-speech and
audio-only utterances using joint acoustic and text decoder (JATD) and
semi-supervised training, we achieved 4%-12% WER reduction for various tasks
compared to the baseline deliberation. Compared to a state-of-the-art language
model (LM) rescoring method, the deliberation model reduces the Google Voice
Search WER by 11% relative. We show that the deliberation model also achieves a
positive human side-by-side evaluation compared to the state-of-the-art LM
rescorer with reasonable endpointer latencies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trial2Vec: Zero-Shot Clinical Trial Document Similarity Search using Self-Supervision. (arXiv:2206.14719v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14719">
<div class="article-summary-box-inner">
<span><p>Clinical trials are essential for drug development but are extremely
expensive and time-consuming to conduct. It is beneficial to study similar
historical trials when designing a clinical trial. However, lengthy trial
documents and lack of labeled data make trial similarity search difficult. We
propose a zero-shot clinical trial retrieval method, Trial2Vec, which learns
through self-supervision without annotating similar clinical trials.
Specifically, the meta-structure of trial documents (e.g., title, eligibility
criteria, target disease) along with clinical knowledge (e.g., UMLS knowledge
base https://www.nlm.nih.gov/research/umls/index.html) are leveraged to
automatically generate contrastive samples. Besides, Trial2Vec encodes trial
documents considering meta-structure thus producing compact embeddings
aggregating multi-aspect information from the whole document. We show that our
method yields medically interpretable embeddings by visualization and it gets a
15% average improvement over the best baselines on precision/recall for trial
retrieval, which is evaluated on our labeled 1600 trial pairs. In addition, we
prove the pre-trained embeddings benefit the downstream trial outcome
prediction task over 240k trials.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">longhorns at DADC 2022: How many linguists does it take to fool a Question Answering model? A systematic approach to adversarial attacks. (arXiv:2206.14729v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14729">
<div class="article-summary-box-inner">
<span><p>Developing methods to adversarially challenge NLP systems is a promising
avenue for improving both model performance and interpretability. Here, we
describe the approach of the team "longhorns" on Task 1 of the The First
Workshop on Dynamic Adversarial Data Collection (DADC), which asked teams to
manually fool a model on an Extractive Question Answering task. Our team
finished first, with a model error rate of 62%. We advocate for a systematic,
linguistically informed approach to formulating adversarial questions, and we
describe the results of our pilot experiments, as well as our official
submission.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TweetNLP: Cutting-Edge Natural Language Processing for Social Media. (arXiv:2206.14774v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14774">
<div class="article-summary-box-inner">
<span><p>In this paper we present TweetNLP, an integrated platform for Natural
Language Processing (NLP) in social media. TweetNLP supports a diverse set of
NLP tasks, including generic focus areas such as sentiment analysis and named
entity recognition, as well as social media-specific tasks such as emoji
prediction and offensive language identification. Task-specific systems are
powered by reasonably-sized Transformer-based language models specialized on
social media text (in particular, Twitter) which can be run without the need
for dedicated hardware or cloud services. The main contributions of TweetNLP
are: (1) an integrated Python library for a modern toolkit supporting social
media analysis using our various task-specific models adapted to the social
domain; (2) an interactive online demo for codeless experimentation using our
models; and (3) a tutorial covering a wide variety of typical social media
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-based Method. (arXiv:2206.14796v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14796">
<div class="article-summary-box-inner">
<span><p>Most works on modeling the conversation history in Conversational Question
Answering (CQA) report a single main result on a common CQA benchmark. While
existing models show impressive results on CQA leaderboards, it remains unclear
whether they are robust to shifts in setting (sometimes to more realistic
ones), training data size (e.g. from large to small sets) and domain. In this
work, we design and conduct the first large-scale robustness study of history
modeling approaches for CQA. We find that high benchmark scores do not
necessarily translate to strong robustness, and that various methods can
perform extremely differently under different settings. Equipped with the
insights from our study, we design a novel prompt-based history modeling
approach, and demonstrate its strong robustness across various settings. Our
approach is inspired by existing methods that highlight historic answers in the
passage. However, instead of highlighting by modifying the passage token
embeddings, we add textual prompts directly in the passage text. Our approach
is simple, easy-to-plug into practically any model, and highly effective, thus
we recommend it as a starting point for future model developers. We also hope
that our study and insights will raise awareness to the importance of
robustness-focused evaluation, in addition to obtaining high leaderboard
scores, leading to better CQA systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structural Persistence in Language Models: Priming as a Window into Abstract Language Representations. (arXiv:2109.14989v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14989">
<div class="article-summary-box-inner">
<span><p>We investigate the extent to which modern, neural language models are
susceptible to structural priming, the phenomenon whereby the structure of a
sentence makes the same structure more probable in a follow-up sentence. We
explore how priming can be used to study the potential of these models to learn
abstract structural information, which is a prerequisite for good performance
on tasks that require natural language understanding skills. We introduce a
novel metric and release Prime-LM, a large corpus where we control for various
linguistic factors which interact with priming strength. We find that
Transformer models indeed show evidence of structural priming, but also that
the generalisations they learned are to some extent modulated by semantic
information. Our experiments also show that the representations acquired by the
models may not only encode abstract sequential structure but involve certain
level of hierarchical syntactic information. More generally, our study shows
that the priming paradigm is a useful, additional tool for gaining insights
into the capacities of language models and opens the door to future
priming-based investigations that probe the model's internal states.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoCA-MDD: A Coupled Cross-Attention based Framework for Streaming Mispronunciation Detection and Diagnosis. (arXiv:2111.08191v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08191">
<div class="article-summary-box-inner">
<span><p>Mispronunciation detection and diagnosis (MDD) is a popular research focus in
computer-aided pronunciation training (CAPT) systems. End-to-end (e2e)
approaches are becoming dominant in MDD. However an e2e MDD model usually
requires entire speech utterances as input context, which leads to significant
time latency especially for long paragraphs. We propose a streaming e2e MDD
model called CoCA-MDD. We utilize conv-transformer structure to encode input
speech in a streaming manner. A coupled cross-attention (CoCA) mechanism is
proposed to integrate frame-level acoustic features with encoded reference
linguistic features. CoCA also enables our model to perform mispronunciation
classification with whole utterances. The proposed model allows system fusion
between the streaming output and mispronunciation classification output for
further performance enhancement. We evaluate CoCA-MDD on publicly available
corpora. CoCA-MDD achieves F1 scores of 57.03% and 60.78% for streaming and
fusion modes respectively on L2-ARCTIC. For phone-level pronunciation scoring,
CoCA-MDD achieves 0.58 Pearson correlation coefficient (PCC) value on
SpeechOcean762.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reducing language context confusion for end-to-end code-switching automatic speech recognition. (arXiv:2201.12155v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12155">
<div class="article-summary-box-inner">
<span><p>Code-switching deals with alternative languages in communication process.
Training end-to-end (E2E) automatic speech recognition (ASR) systems for
code-switching is especially challenging as code-switching training data are
always insufficient to combat the increased multilingual context confusion due
to the presence of more than one language. We propose a language-related
attention mechanism to reduce multilingual context confusion for the E2E
code-switching ASR model based on the Equivalence Constraint (EC) Theory. The
linguistic theory requires that any monolingual fragment that occurs in the
code-switching sentence must occur in one of the monolingual sentences. The
theory establishes a bridge between monolingual data and code-switching data.
We leverage this linguistics theory to design the code-switching E2E ASR model.
The proposed model efficiently transfers language knowledge from rich
monolingual data to improve the performance of the code-switching ASR model. We
evaluate our model on ASRU 2019 Mandarin-English code-switching challenge
dataset. Compared to the baseline model, our proposed model achieves a 17.12%
relative error reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling speech recognition and synthesis simultaneously: Encoding and decoding lexical and sublexical semantic information into speech with no direct access to speech data. (arXiv:2203.11476v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11476">
<div class="article-summary-box-inner">
<span><p>Human speakers encode information into raw speech which is then decoded by
the listeners. This complex relationship between encoding (production) and
decoding (perception) is often modeled separately. Here, we test how encoding
and decoding of lexical semantic information can emerge automatically from raw
speech in unsupervised generative deep convolutional networks that combine the
production and perception principles of speech. We introduce, to our knowledge,
the most challenging objective in unsupervised lexical learning: a network that
must learn unique representations for lexical items with no direct access to
training data. We train several models (ciwGAN and fiwGAN <a href="/abs/2006.02951">arXiv:2006.02951</a>) and
test how the networks classify acoustic lexical items in unobserved test data.
Strong evidence in favor of lexical learning and a causal relationship between
latent codes and meaningful sublexical units emerge. The architecture that
combines the production and perception principles is thus able to learn to
decode unique information from raw acoustic data without accessing real
training data directly. We propose a technique to explore lexical (holistic)
and sublexical (featural) learned representations in the classifier network.
The results bear implications for unsupervised speech technology, as well as
for unsupervised semantic modeling as language models increasingly bypass text
and operate from raw acoustics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GateNLP-UShef at SemEval-2022 Task 8: Entity-Enriched Siamese Transformer for Multilingual News Article Similarity. (arXiv:2205.15812v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15812">
<div class="article-summary-box-inner">
<span><p>This paper describes the second-placed system on the leaderboard of
SemEval-2022 Task 8: Multilingual News Article Similarity. We propose an
entity-enriched Siamese Transformer which computes news article similarity
based on different sub-dimensions, such as the shared narrative, entities,
location and time of the event discussed in the news article. Our system
exploits a Siamese network architecture using a Transformer encoder to learn
document-level representations for the purpose of capturing the narrative
together with the auxiliary entity-based features extracted from the news
articles. The intuition behind using all these features together is to capture
the similarity between news articles at different granularity levels and to
assess the extent to which different news outlets write about "the same
events". Our experimental results and detailed ablation study demonstrate the
effectiveness and the validity of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery. (arXiv:2206.11706v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11706">
<div class="article-summary-box-inner">
<span><p>Latent Dirichlet allocation (LDA) is widely used for unsupervised topic
modelling on sets of documents. No temporal information is used in the model.
However, there is often a relationship between the corresponding topics of
consecutive tokens. In this paper, we present an extension to LDA that uses a
Markov chain to model temporal information. We use this new model for acoustic
unit discovery from speech. As input tokens, the model takes a discretised
encoding of speech from a vector quantised (VQ) neural network with 512 codes.
The goal is then to map these 512 VQ codes to 50 phone-like units (topics) in
order to more closely resemble true phones. In contrast to the base LDA, which
only considers how VQ codes co-occur within utterances (documents), the Markov
chain LDA additionally captures how consecutive codes follow one another. This
extension leads to an increase in cluster quality and phone segmentation
results compared to the base LDA. Compared to a recent vector quantised neural
network approach that also learns 50 units, the extended LDA model performs
better in phone segmentation but worse in mutual information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-resource Accent Classification in Geographically-proximate Settings: A Forensic and Sociophonetics Perspective. (arXiv:2206.12759v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12759">
<div class="article-summary-box-inner">
<span><p>Accented speech recognition and accent classification are relatively
under-explored research areas in speech technology. Recently, deep
learning-based methods and Transformer-based pretrained models have achieved
superb performances in both areas. However, most accent classification tasks
focused on classifying different kinds of English accents and little attention
was paid to geographically-proximate accent classification, especially under a
low-resource setting where forensic speech science tasks usually encounter. In
this paper, we explored three main accent modelling methods combined with two
different classifiers based on 105 speaker recordings retrieved from five urban
varieties in Northern England. Although speech representations generated from
pretrained models generally have better performances in downstream
classification, traditional methods like Mel Frequency Cepstral Coefficients
(MFCCs) and formant measurements are equipped with specific strengths. These
results suggest that in forensic phonetics scenario where data are relatively
scarce, a simple modelling method and classifier could be competitive with
state-of-the-art pretrained speech models as feature extractors, which could
enhance a sooner estimation for the accent information in practices. Besides,
our findings also cross-validated a new methodology in quantifying
sociophonetic changes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bengali Common Voice Speech Dataset for Automatic Speech Recognition. (arXiv:2206.14053v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14053">
<div class="article-summary-box-inner">
<span><p>Bengali is one of the most spoken languages in the world with over 300
million speakers globally. Despite its popularity, research into the
development of Bengali speech recognition systems is hindered due to the lack
of diverse open-source datasets. As a way forward, we have crowdsourced the
Bengali Common Voice Speech Dataset, which is a sentence-level automatic speech
recognition corpus. Collected on the Mozilla Common Voice platform, the dataset
is part of an ongoing campaign that has led to the collection of over 400 hours
of data in 2 months and is growing rapidly. Our analysis shows that this
dataset has more speaker, phoneme, and environmental diversity compared to the
OpenSLR Bengali ASR dataset, the largest existing open-source Bengali speech
dataset. We present insights obtained from the dataset and discuss key
linguistic challenges that need to be addressed in future versions.
Additionally, we report the current performance of a few Automatic Speech
Recognition (ASR) algorithms and set a benchmark for future research.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked World Models for Visual Control. (arXiv:2206.14244v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14244">
<div class="article-summary-box-inner">
<span><p>Visual model-based reinforcement learning (RL) has the potential to enable
sample-efficient robot learning from visual observations. Yet the current
approaches typically train a single model end-to-end for learning both visual
representations and dynamics, making it difficult to accurately model the
interaction between robots and small objects. In this work, we introduce a
visual model-based RL framework that decouples visual representation learning
and dynamics learning. Specifically, we train an autoencoder with convolutional
layers and vision transformers (ViT) to reconstruct pixels given masked
convolutional features, and learn a latent dynamics model that operates on the
representations from the autoencoder. Moreover, to encode task-relevant
information, we introduce an auxiliary reward prediction objective for the
autoencoder. We continually update both autoencoder and dynamics model using
online samples collected from environment interaction. We demonstrate that our
decoupling approach achieves state-of-the-art performance on a variety of
visual robotic tasks from Meta-world and RLBench, e.g., we achieve 81.7%
success rate on 50 visual robotic manipulation tasks from Meta-world, while the
baseline achieves 67.9%. Code is available on the project website:
https://sites.google.com/view/mwm-rl.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SImProv: Scalable Image Provenance Framework for Robust Content Attribution. (arXiv:2206.14245v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14245">
<div class="article-summary-box-inner">
<span><p>We present SImProv - a scalable image provenance framework to match a query
image back to a trusted database of originals and identify possible
manipulations on the query. SImProv consists of three stages: a scalable search
stage for retrieving top-k most similar images; a re-ranking and
near-duplicated detection stage for identifying the original among the
candidates; and finally a manipulation detection and visualization stage for
localizing regions within the query that may have been manipulated to differ
from the original. SImProv is robust to benign image transformations that
commonly occur during online redistribution, such as artifacts due to noise and
recompression degradation, as well as out-of-place transformations due to image
padding, warping, and changes in size and shape. Robustness towards
out-of-place transformations is achieved via the end-to-end training of a
differentiable warping module within the comparator architecture. We
demonstrate effective retrieval and manipulation detection over a dataset of
100 million images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAN-based Intrinsic Exploration For Sample Efficient Reinforcement Learning. (arXiv:2206.14256v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14256">
<div class="article-summary-box-inner">
<span><p>In this study, we address the problem of efficient exploration in
reinforcement learning. Most common exploration approaches depend on random
action selection, however these approaches do not work well in environments
with sparse or no rewards. We propose Generative Adversarial Network-based
Intrinsic Reward Module that learns the distribution of the observed states and
sends an intrinsic reward that is computed as high for states that are out of
distribution, in order to lead agent to unexplored states. We evaluate our
approach in Super Mario Bros for a no reward setting and in Montezuma's Revenge
for a sparse reward setting and show that our approach is indeed capable of
exploring efficiently. We discuss a few weaknesses and conclude by discussing
future works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZoDIAC: Zoneout Dropout Injection Attention Calculation. (arXiv:2206.14263v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14263">
<div class="article-summary-box-inner">
<span><p>Recently the use of self-attention has yielded to state-of-the-art results in
vision-language tasks such as image captioning as well as natural language
understanding and generation (NLU and NLG) tasks and computer vision tasks such
as image classification. This is since self-attention maps the internal
interactions among the elements of input source and target sequences. Although
self-attention successfully calculates the attention values and maps the
relationships among the elements of input source and target sequence, yet there
is no mechanism to control the intensity of attention. In real world, when
communicating with each other face to face or vocally, we tend to express
different visual and linguistic context with various amounts of intensity. Some
words might carry (be spoken with) more stress and weight indicating the
importance of that word in the context of the whole sentence. Based on this
intuition, we propose Zoneout Dropout Injection Attention Calculation (ZoDIAC)
in which the intensities of attention values in the elements of the input
sequence are calculated with respect to the context of the elements of input
sequence. The results of our experiments reveal that employing ZoDIAC leads to
better performance in comparison with the self-attention module in the
Transformer model. The ultimate goal is to find out if we could modify
self-attention module in the Transformer model with a method that is
potentially extensible to other models that leverage on self-attention at their
core. Our findings suggest that this particular goal deserves further attention
and investigation by the research community.
</p>
<p>The code for ZoDIAC is available on www.github.com/zanyarz/zodiac .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforcement Learning in Medical Image Analysis: Concepts, Applications, Challenges, and Future Directions. (arXiv:2206.14302v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14302">
<div class="article-summary-box-inner">
<span><p>Motivation: Medical image analysis involves tasks to assist physicians in
qualitative and quantitative analysis of lesions or anatomical structures,
significantly improving the accuracy and reliability of diagnosis and
prognosis. Traditionally, these tasks are finished by physicians or medical
physicists and lead to two major problems: (i) low efficiency; (ii) biased by
personal experience. In the past decade, many machine learning methods have
been applied to accelerate and automate the image analysis process. Compared to
the enormous deployments of supervised and unsupervised learning models,
attempts to use reinforcement learning in medical image analysis are scarce.
This review article could serve as the stepping-stone for related research.
Significance: From our observation, though reinforcement learning has gradually
gained momentum in recent years, many researchers in the medical analysis field
find it hard to understand and deploy in clinics. One cause is lacking
well-organized review articles targeting readers lacking professional computer
science backgrounds. Rather than providing a comprehensive list of all
reinforcement learning models in medical image analysis, this paper may help
the readers to learn how to formulate and solve their medical image analysis
research as reinforcement learning problems. Approach &amp; Results: We selected
published articles from Google Scholar and PubMed. Considering the scarcity of
related articles, we also included some outstanding newest preprints. The
papers are carefully reviewed and categorized according to the type of image
analysis task. We first review the basic concepts and popular models of
reinforcement learning. Then we explore the applications of reinforcement
learning models in landmark detection. Finally, we conclude the article by
discussing the reviewed reinforcement learning approaches' limitations and
possible improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multistep Automated Data Labelling Procedure (MADLaP) for Thyroid Nodules on Ultrasound: An Artificial Intelligence Approach for Automating Image Annotation. (arXiv:2206.14305v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14305">
<div class="article-summary-box-inner">
<span><p>Machine learning (ML) for diagnosis of thyroid nodules on ultrasound is an
active area of research. However, ML tools require large, well-labelled
datasets, the curation of which is time-consuming and labor-intensive. The
purpose of our study was to develop and test a deep-learning-based tool to
facilitate and automate the data annotation process for thyroid nodules; we
named our tool Multistep Automated Data Labelling Procedure (MADLaP). MADLaP
was designed to take multiple inputs included pathology reports, ultrasound
images, and radiology reports. Using multiple step-wise modules including
rule-based natural language processing, deep-learning-based imaging
segmentation, and optical character recognition, MADLaP automatically
identified images of a specific thyroid nodule and correctly assigned a
pathology label. The model was developed using a training set of 378 patients
across our health system and tested on a separate set of 93 patients. Ground
truths for both sets were selected by an experienced radiologist. Performance
metrics including yield (how many labeled images the model produced) and
accuracy (percentage correct) were measured using the test set. MADLaP achieved
a yield of 63% and an accuracy of 83%. The yield progressively increased as the
input data moved through each module, while accuracy peaked part way through.
Error analysis showed that inputs from certain examination sites had lower
accuracy (40%) than the other sites (90%, 100%). MADLaP successfully created
curated datasets of labeled ultrasound images of thyroid nodules. While
accurate, the relatively suboptimal yield of MADLaP exposed some challenges
when trying to automatically label radiology images from heterogeneous sources.
The complex task of image curation and annotation could be automated, allowing
for enrichment of larger datasets for use in machine learning development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Neural Articulated Radiance Fields. (arXiv:2206.14314v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14314">
<div class="article-summary-box-inner">
<span><p>Unsupervised learning of 3D-aware generative adversarial networks (GANs)
using only collections of single-view 2D photographs has very recently made
much progress. These 3D GANs, however, have not been demonstrated for human
bodies and the generated radiance fields of existing frameworks are not
directly editable, limiting their applicability in downstream tasks. We propose
a solution to these challenges by developing a 3D GAN framework that learns to
generate radiance fields of human bodies or faces in a canonical pose and warp
them using an explicit deformation field into a desired body pose or facial
expression. Using our framework, we demonstrate the first high-quality radiance
field generation results for human bodies. Moreover, we show that our
deformation-aware training procedure significantly improves the quality of
generated bodies or faces when editing their poses or facial expressions
compared to a 3D GAN that is not trained with explicit deformations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Adjacency Matrix Configuration in GCN-based Models for Skeleton-based Action Recognition. (arXiv:2206.14344v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14344">
<div class="article-summary-box-inner">
<span><p>Human skeleton data has received increasing attention in action recognition
due to its background robustness and high efficiency. In skeleton-based action
recognition, graph convolutional network (GCN) has become the mainstream
method. This paper analyzes the fundamental factor for GCN-based models -- the
adjacency matrix. We notice that most GCN-based methods conduct their adjacency
matrix based on the human natural skeleton structure. Based on our former work
and analysis, we propose that the human natural skeleton structure adjacency
matrix is not proper for skeleton-based action recognition. We propose a new
adjacency matrix that abandons all rigid neighbor connections but lets the
model adaptively learn the relationships of joints. We conduct extensive
experiments and analysis with a validation model on two skeleton-based action
recognition datasets (NTURGBD60 and FineGYM). Comprehensive experimental
results and analysis reveals that 1) the most widely used human natural
skeleton structure adjacency matrix is unsuitable in skeleton-based action
recognition; 2) The proposed adjacency matrix is superior in model performance,
noise robustness and transferability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Neural Network Based Partial Face Detection. (arXiv:2206.14350v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14350">
<div class="article-summary-box-inner">
<span><p>Due to the massive explanation of artificial intelligence, machine learning
technology is being used in various areas of our day-to-day life. In the world,
there are a lot of scenarios where a simple crime can be prevented before it
may even happen or find the person responsible for it. A face is one
distinctive feature that we have and can differentiate easily among many other
species. But not just different species, it also plays a significant role in
determining someone from the same species as us, humans. Regarding this
critical feature, a single problem occurs most often nowadays. When the camera
is pointed, it cannot detect a person's face, and it becomes a poor image. On
the other hand, where there was a robbery and a security camera installed, the
robber's identity is almost indistinguishable due to the low-quality camera.
But just making an excellent algorithm to work and detecting a face reduces the
cost of hardware, and it doesn't cost that much to focus on that area. Facial
recognition, widget control, and such can be done by detecting the face
correctly. This study aims to create and enhance a machine learning model that
correctly recognizes faces. Total 627 Data have been collected from different
Bangladeshi people's faces on four angels. In this work, CNN, Harr Cascade,
Cascaded CNN, Deep CNN &amp; MTCNN are these five machine learning approaches
implemented to get the best accuracy of our dataset. After creating and running
the model, Multi-Task Convolutional Neural Network (MTCNN) achieved 96.2% best
model accuracy with training data rather than other machine learning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EBMs vs. CL: Exploring Self-Supervised Visual Pretraining for Visual Question Answering. (arXiv:2206.14355v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14355">
<div class="article-summary-box-inner">
<span><p>The availability of clean and diverse labeled data is a major roadblock for
training models on complex tasks such as visual question answering (VQA). The
extensive work on large vision-and-language models has shown that
self-supervised learning is effective for pretraining multimodal interactions.
In this technical report, we focus on visual representations. We review and
evaluate self-supervised methods to leverage unlabeled images and pretrain a
model, which we then fine-tune on a custom VQA task that allows controlled
evaluation and diagnosis. We compare energy-based models (EBMs) with
contrastive learning (CL). While EBMs are growing in popularity, they lack an
evaluation on downstream tasks. We find that both EBMs and CL can learn
representations from unlabeled images that enable training a VQA model on very
little annotated data. In a simple setting similar to CLEVR, we find that CL
representations also improve systematic generalization, and even match the
performance of representations from a larger, supervised, ImageNet-pretrained
model. However, we find EBMs to be difficult to train because of instabilities
and high variability in their results. Although EBMs prove useful for OOD
detection, other results on supervised energy-based training and uncertainty
calibration are largely negative. Overall, CL currently seems a preferable
option over EBMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Formalizing and Evaluating Requirements of Perception Systems for Automated Vehicles using Spatio-Temporal Perception Logic. (arXiv:2206.14372v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14372">
<div class="article-summary-box-inner">
<span><p>Automated vehicles (AV) heavily depend on robust perception systems. Current
methods for evaluating vision systems focus mainly on frame-by-frame
performance. Such evaluation methods appear to be inadequate in assessing the
performance of a perception subsystem when used within an AV. In this paper, we
present a logic -- referred to as Spatio-Temporal Perception Logic (STPL) --
which utilizes both spatial and temporal modalities. STPL enables reasoning
over perception data using spatial and temporal relations. One major advantage
of STPL is that it facilitates basic sanity checks on the real-time performance
of the perception system, even without ground-truth data in some cases. We
identify a fragment of STPL which is efficiently monitorable offline in
polynomial time. Finally, we present a range of specifications for AV
perception systems to highlight the types of requirements that can be expressed
and analyzed through offline monitoring with STPL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Semantic Role Contextualized Video Features for Multi-Instance Text-Video Retrieval EPIC-KITCHENS-100 Multi-Instance Retrieval Challenge 2022. (arXiv:2206.14381v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14381">
<div class="article-summary-box-inner">
<span><p>In this report, we present our approach for EPIC-KITCHENS-100 Multi-Instance
Retrieval Challenge 2022. We first parse sentences into semantic roles
corresponding to verbs and nouns; then utilize self-attentions to exploit
semantic role contextualized video features along with textual features via
triplet losses in multiple embedding spaces. Our method overpasses the strong
baseline in normalized Discounted Cumulative Gain (nDCG), which is more
valuable for semantic similarity. Our submission is ranked 3rd for nDCG and
ranked 4th for mAP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">C2FTrans: Coarse-to-Fine Transformers for Medical Image Segmentation. (arXiv:2206.14409v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14409">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNN), the most prevailing architecture for
deep-learning based medical image analysis, are still functionally limited by
their intrinsic inductive biases and inadequate receptive fields. Transformer,
born to address this issue, has drawn explosive attention in natural language
processing and computer vision due to its remarkable ability in capturing
long-range dependency. However, most recent transformer-based methods for
medical image segmentation directly apply vanilla transformers as an auxiliary
module in CNN-based methods, resulting in severe detail loss due to the rigid
patch partitioning scheme in transformers. To address this problem, we propose
C2FTrans, a novel multi-scale architecture that formulates medical image
segmentation as a coarse-to-fine procedure. C2FTrans mainly consists of a
cross-scale global transformer (CGT) which addresses local contextual
similarity in CNN and a boundary-aware local transformer (BLT) which overcomes
boundary uncertainty brought by rigid patch partitioning in transformers.
Specifically, CGT builds global dependency across three different small-scale
feature maps to obtain rich global semantic features with an acceptable
computational cost, while BLT captures mid-range dependency by adaptively
generating windows around boundaries under the guidance of entropy to reduce
computational complexity and minimize detail loss based on large-scale feature
maps. Extensive experimental results on three public datasets demonstrate the
superior performance of C2FTrans against state-of-the-art CNN-based and
transformer-based methods with fewer parameters and lower FLOPs. We believe the
design of C2FTrans would further inspire future work on developing efficient
and lightweight transformers for medical image segmentation. The source code of
this paper is publicly available at https://github.com/xianlin7/C2FTrans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Lighter The Better: Rethinking Transformers in Medical Image Segmentation Through Adaptive Pruning. (arXiv:2206.14413v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14413">
<div class="article-summary-box-inner">
<span><p>Vision transformers have recently set off a new wave in the field of medical
image analysis due to their remarkable performance on various computer vision
tasks. However, recent hybrid-/transformer-based approaches mainly focus on the
benefits of transformers in capturing long-range dependency while ignoring the
issues of their daunting computational complexity, high training costs, and
redundant dependency. In this paper, we propose to employ adaptive pruning to
transformers for medical image segmentation and propose a lightweight and
effective hybrid network APFormer. To our best knowledge, this is the first
work on transformer pruning for medical image analysis tasks. The key features
of APFormer mainly are self-supervised self-attention (SSA) to improve the
convergence of dependency establishment, Gaussian-prior relative position
embedding (GRPE) to foster the learning of position information, and adaptive
pruning to eliminate redundant computations and perception information.
Specifically, SSA and GRPE consider the well-converged dependency distribution
and the Gaussian heatmap distribution separately as the prior knowledge of
self-attention and position embedding to ease the training of transformers and
lay a solid foundation for the following pruning operation. Then, adaptive
transformer pruning, both query-wise and dependency-wise, is performed by
adjusting the gate control parameters for both complexity reduction and
performance improvement. Extensive experiments on two widely-used datasets
demonstrate the prominent segmentation performance of APFormer against the
state-of-the-art methods with much fewer parameters and lower GFLOPs. More
importantly, we prove, through ablation studies, that adaptive pruning can work
as a plug-n-play module for performance improvement on other
hybrid-/transformer-based methods. Code is available at
https://github.com/xianlin7/APFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MaNi: Maximizing Mutual Information for Nuclei Cross-Domain Unsupervised Segmentation. (arXiv:2206.14437v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14437">
<div class="article-summary-box-inner">
<span><p>In this work, we propose a mutual information (MI) based unsupervised domain
adaptation (UDA) method for the cross-domain nuclei segmentation. Nuclei vary
substantially in structure and appearances across different cancer types,
leading to a drop in performance of deep learning models when trained on one
cancer type and tested on another. This domain shift becomes even more critical
as accurate segmentation and quantification of nuclei is an essential
histopathology task for the diagnosis/ prognosis of patients and annotating
nuclei at the pixel level for new cancer types demands extensive effort by
medical experts. To address this problem, we maximize the MI between labeled
source cancer type data and unlabeled target cancer type data for transferring
nuclei segmentation knowledge across domains. We use the Jensen-Shanon
divergence bound, requiring only one negative pair per positive pair for MI
maximization. We evaluate our set-up for multiple modeling frameworks and on
different datasets comprising of over 20 cancer-type domain shifts and
demonstrate competitive performance. All the recently proposed approaches
consist of multiple components for improving the domain adaptation, whereas our
proposed module is light and can be easily incorporated into other methods
(Implementation: https://github.com/YashSharma/MaNi ).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SRCN3D: Sparse R-CNN 3D Surround-View Camera Object Detection and Tracking for Autonomous Driving. (arXiv:2206.14451v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14451">
<div class="article-summary-box-inner">
<span><p>Detection And Tracking of Moving Objects (DATMO) is an essential component in
environmental perception for autonomous driving. While 3D detectors using
surround-view cameras are just flourishing, there is a growing tendency of
using different transformer-based methods to learn queries in 3D space from 2D
feature maps of perspective view. This paper proposes Sparse R-CNN 3D (SRCN3D),
a novel two-stage fully-convolutional mapping pipeline for surround-view camera
detection and tracking. SRCN3D adopts a cascade structure with twin-track
update of both fixed number of proposal boxes and proposal latent features.
Proposal boxes are projected to perspective view so as to aggregate Region of
Interest (RoI) local features. Based on that, proposal features are refined via
a dynamic instance interactive head, which then generates classification and
the offsets applied to original bounding boxes. Compared to prior arts, our
sparse feature sampling module only utilizes local 2D features for adjustment
of each corresponding 3D proposal box, leading to a complete sparse paradigm.
The proposal features and appearance features are both taken in data
association process in a multi-hypotheses 3D multi-object tracking approach.
Extensive experiments on nuScenes dataset demonstrate the effectiveness of our
proposed SRCN3D detector and tracker. Code is available at
https://github.com/synsin0/SRCN3D.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single-domain Generalization in Medical Image Segmentation via Test-time Adaptation from Shape Dictionary. (arXiv:2206.14467v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14467">
<div class="article-summary-box-inner">
<span><p>Domain generalization typically requires data from multiple source domains
for model learning. However, such strong assumption may not always hold in
practice, especially in medical field where the data sharing is highly
concerned and sometimes prohibitive due to privacy issue. This paper studies
the important yet challenging single domain generalization problem, in which a
model is learned under the worst-case scenario with only one source domain to
directly generalize to different unseen target domains. We present a novel
approach to address this problem in medical image segmentation, which extracts
and integrates the semantic shape prior information of segmentation that are
invariant across domains and can be well-captured even from single domain data
to facilitate segmentation under distribution shifts. Besides, a test-time
adaptation strategy with dual-consistency regularization is further devised to
promote dynamic incorporation of these shape priors under each unseen domain to
improve model generalizability. Extensive experiments on two medical image
segmentation tasks demonstrate the consistent improvements of our method across
various unseen domains, as well as its superiority over state-of-the-art
approaches in addressing domain generalization under the worst-case scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Siamese Contrastive Embedding Network for Compositional Zero-Shot Learning. (arXiv:2206.14475v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14475">
<div class="article-summary-box-inner">
<span><p>Compositional Zero-Shot Learning (CZSL) aims to recognize unseen compositions
formed from seen state and object during training. Since the same state may be
various in the visual appearance while entangled with different objects, CZSL
is still a challenging task. Some methods recognize state and object with two
trained classifiers, ignoring the impact of the interaction between object and
state; the other methods try to learn the joint representation of the
state-object compositions, leading to the domain gap between seen and unseen
composition sets. In this paper, we propose a novel Siamese Contrastive
Embedding Network (SCEN) (Code: https://github.com/XDUxyLi/SCEN-master) for
unseen composition recognition. Considering the entanglement between state and
object, we embed the visual feature into a Siamese Contrastive Space to capture
prototypes of them separately, alleviating the interaction between state and
object. In addition, we design a State Transition Module (STM) to increase the
diversity of training compositions, improving the robustness of the recognition
model. Extensive experiments indicate that our method significantly outperforms
the state-of-the-art approaches on three challenging benchmark datasets,
including the recent proposed C-QGA dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond neural scaling laws: beating power law scaling via data pruning. (arXiv:2206.14486v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14486">
<div class="article-summary-box-inner">
<span><p>Widely observed neural scaling laws, in which error falls off as a power of
the training set size, model size, or both, have driven substantial performance
improvements in deep learning. However, these improvements through scaling
alone require considerable costs in compute and energy. Here we focus on the
scaling of error with dataset size and show how both in theory and practice we
can break beyond power law scaling and reduce it to exponential scaling instead
if we have access to a high-quality data pruning metric that ranks the order in
which training examples should be discarded to achieve any pruned dataset size.
We then test this new exponential scaling prediction with pruned dataset size
empirically, and indeed observe better than power law scaling performance on
ResNets trained on CIFAR-10, SVHN, and ImageNet. Given the importance of
finding high-quality pruning metrics, we perform the first large-scale
benchmarking study of ten different data pruning metrics on ImageNet. We find
most existing high performing metrics scale poorly to ImageNet, while the best
are computationally intensive and require labels for every image. We therefore
developed a new simple, cheap and scalable self-supervised pruning metric that
demonstrates comparable performance to the best supervised metrics. Overall,
our work suggests that the discovery of good data-pruning metrics may provide a
viable path forward to substantially improved neural scaling laws, thereby
reducing the resource costs of modern deep learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RegMixup: Mixup as a Regularizer Can Surprisingly Improve Accuracy and Out Distribution Robustness. (arXiv:2206.14502v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14502">
<div class="article-summary-box-inner">
<span><p>We show that the effectiveness of the well celebrated Mixup [Zhang et al.,
2018] can be further improved if instead of using it as the sole learning
objective, it is utilized as an additional regularizer to the standard
cross-entropy loss. This simple change not only provides much improved accuracy
but also significantly improves the quality of the predictive uncertainty
estimation of Mixup in most cases under various forms of covariate shifts and
out-of-distribution detection experiments. In fact, we observe that Mixup
yields much degraded performance on detecting out-of-distribution samples
possibly, as we show empirically, because of its tendency to learn models that
exhibit high-entropy throughout; making it difficult to differentiate
in-distribution samples from out-distribution ones. To show the efficacy of our
approach (RegMixup), we provide thorough analyses and experiments on vision
datasets (ImageNet &amp; CIFAR-10/100) and compare it with a suite of recent
approaches for reliable uncertainty estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Procrustes Analysis with Deformations: A Closed-Form Solution by Eigenvalue Decomposition. (arXiv:2206.14528v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14528">
<div class="article-summary-box-inner">
<span><p>Generalized Procrustes Analysis (GPA) is the problem of bringing multiple
shapes into a common reference by estimating transformations. GPA has been
extensively studied for the Euclidean and affine transformations. We introduce
GPA with deformable transformations, which forms a much wider and difficult
problem. We specifically study a class of transformations called the Linear
Basis Warps (LBWs), which contains the affine transformation and most of the
usual deformation models, such as the Thin-Plate Spline (TPS). GPA with
deformations is a nonconvex underconstrained problem. We resolve the
fundamental ambiguities of deformable GPA using two shape constraints requiring
the eigenvalues of the shape covariance. These eigenvalues can be computed
independently as a prior or posterior. We give a closed-form and optimal
solution to deformable GPA based on an eigenvalue decomposition. This solution
handles regularization, favoring smooth deformation fields. It requires the
transformation model to satisfy a fundamental property of free-translations,
which asserts that the model can implement any translation. We show that this
property fortunately holds true for most common transformation models,
including the affine and TPS models. For the other models, we give another
closed-form solution to GPA, which agrees exactly with the first solution for
models with free-translation. We give pseudo-code for computing our solution,
leading to the proposed DefGPA method, which is fast, globally optimal and
widely applicable. We validate our method and compare it to previous work on
six diverse 2D and 3D datasets, with special care taken to choose the
hyperparameters from cross-validation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">vMFNet: Compositionality Meets Domain-generalised Segmentation. (arXiv:2206.14538v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14538">
<div class="article-summary-box-inner">
<span><p>Training medical image segmentation models usually requires a large amount of
labeled data. By contrast, humans can quickly learn to accurately recognise
anatomy of interest from medical (e.g. MRI and CT) images with some limited
guidance. Such recognition ability can easily generalise to new images from
different clinical centres. This rapid and generalisable learning ability is
mostly due to the compositional structure of image patterns in the human brain,
which is less incorporated in medical image segmentation. In this paper, we
model the compositional components (i.e. patterns) of human anatomy as
learnable von-Mises-Fisher (vMF) kernels, which are robust to images collected
from different domains (e.g. clinical centres). The image features can be
decomposed to (or composed by) the components with the composing operations,
i.e. the vMF likelihoods. The vMF likelihoods tell how likely each anatomical
part is at each position of the image. Hence, the segmentation mask can be
predicted based on the vMF likelihoods. Moreover, with a reconstruction module,
unlabeled data can also be used to learn the vMF kernels and likelihoods by
recombining them to reconstruct the input image. Extensive experiments show
that the proposed vMFNet achieves improved generalisation performance on two
benchmarks, especially when annotations are limited. Code is publicly available
at: https://github.com/vios-s/vMFNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why patient data cannot be easily forgotten?. (arXiv:2206.14541v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14541">
<div class="article-summary-box-inner">
<span><p>Rights provisioned within data protection regulations, permit patients to
request that knowledge about their information be eliminated by data holders.
With the advent of AI learned on data, one can imagine that such rights can
extent to requests for forgetting knowledge of patient's data within AI models.
However, forgetting patients' imaging data from AI models, is still an
under-explored problem. In this paper, we study the influence of patient data
on model performance and formulate two hypotheses for a patient's data: either
they are common and similar to other patients or form edge cases, i.e. unique
and rare cases. We show that it is not possible to easily forget patient data.
We propose a targeted forgetting approach to perform patient-wise forgetting.
Extensive experiments on the benchmark Automated Cardiac Diagnosis Challenge
dataset showcase the improved performance of the proposed targeted forgetting
approach as opposed to a state-of-the-art method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-aware Panoptic Segmentation. (arXiv:2206.14554v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14554">
<div class="article-summary-box-inner">
<span><p>Reliable scene understanding is indispensable for modern autonomous systems.
Current learning-based methods typically try to maximize their performance
based on segmentation metrics that only consider the quality of the
segmentation. However, for the safe operation of a system in the real world it
is crucial to consider the uncertainty in the prediction as well. In this work,
we introduce the novel task of uncertainty-aware panoptic segmentation, which
aims to predict per-pixel semantic and instance segmentations, together with
per-pixel uncertainty estimates. We define two novel metrics to facilitate its
quantitative analysis, the uncertainty-aware Panoptic Quality (uPQ) and the
panoptic Expected Calibration Error (pECE). We further propose the novel
top-down Evidential Panoptic Segmentation Network (EvPSNet) to solve this task.
Our architecture employs a simple yet effective probabilistic fusion module
that leverages the predicted uncertainties. Additionally, we propose a new
Lov\'asz evidential loss function to optimize the IoU for the segmentation
utilizing the probabilities provided by deep evidential learning. Furthermore,
we provide several strong baselines combining state-of-the-art panoptic
segmentation networks with sampling-free uncertainty estimation techniques.
Extensive evaluations show that our EvPSNet achieves the new state-of-the-art
for the standard Panoptic Quality (PQ), as well as for our uncertainty-aware
panoptic metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Technical Report for CVPR 2022 LOVEU AQTC Challenge. (arXiv:2206.14555v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14555">
<div class="article-summary-box-inner">
<span><p>This technical report presents the 2nd winning model for AQTC, a task newly
introduced in CVPR 2022 LOng-form VidEo Understanding (LOVEU) challenges. This
challenge faces difficulties with multi-step answers, multi-modal, and diverse
and changing button representations in video. We address this problem by
proposing a new context ground module attention mechanism for more effective
feature mapping. In addition, we also perform the analysis over the number of
buttons and ablation study of different step networks and video features. As a
result, we achieved the overall 2nd place in LOVEU competition track 3,
specifically the 1st place in two out of four evaluation metrics. Our code is
available at https://github.com/jaykim9870/ CVPR-22_LOVEU_unipyler.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Competence-based Multimodal Curriculum Learning for Medical Report Generation. (arXiv:2206.14579v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14579">
<div class="article-summary-box-inner">
<span><p>Medical report generation task, which targets to produce long and coherent
descriptions of medical images, has attracted growing research interests
recently. Different from the general image captioning tasks, medical report
generation is more challenging for data-driven neural models. This is mainly
due to 1) the serious data bias and 2) the limited medical data. To alleviate
the data bias and make best use of available data, we propose a
Competence-based Multimodal Curriculum Learning framework (CMCL). Specifically,
CMCL simulates the learning process of radiologists and optimizes the model in
a step by step manner. Firstly, CMCL estimates the difficulty of each training
instance and evaluates the competence of current model; Secondly, CMCL selects
the most suitable batch of training instances considering current model
competence. By iterating above two steps, CMCL can gradually improve the
model's performance. The experiments on the public IU-Xray and MIMIC-CXR
datasets show that CMCL can be incorporated into existing models to improve
their performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On-device Synaptic Memory Consolidation using Fowler-Nordheim Quantum-tunneling. (arXiv:2206.14581v1 [cs.ET])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14581">
<div class="article-summary-box-inner">
<span><p>Synaptic memory consolidation has been heralded as one of the key mechanisms
for supporting continual learning in neuromorphic Artificial Intelligence (AI)
systems. Here we report that a Fowler-Nordheim (FN) quantum-tunneling device
can implement synaptic memory consolidation similar to what can be achieved by
algorithmic consolidation models like the cascade and the elastic weight
consolidation (EWC) models. The proposed FN-synapse not only stores the
synaptic weight but also stores the synapse's historical usage statistic on the
device itself. We also show that the operation of the FN-synapse is
near-optimal in terms of the synaptic lifetime and we demonstrate that a
network comprising FN-synapses outperforms a comparable EWC network for a small
benchmark continual learning task. With an energy footprint of femtojoules per
synaptic update, we believe that the proposed FN-synapse provides an
ultra-energy-efficient approach for implementing both synaptic memory
consolidation and persistent learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perspective (In)consistency of Paint by Text. (arXiv:2206.14617v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14617">
<div class="article-summary-box-inner">
<span><p>Type "a sea otter with a pearl earring by Johannes Vermeer" or "a photo of a
teddy bear on a skateboard in Times Square" into OpenAI's DALL-E-2
paint-by-text synthesis engine and you will not be disappointed by the
delightful and eerily pertinent results. The ability to synthesize highly
realistic images -- with seemingly no limitation other than our imagination --
is sure to yield many exciting and creative applications. These images are also
likely to pose new challenges to the photo-forensic community. Motivated by the
fact that paint by text is not based on explicit geometric modeling, and the
human visual system's often obliviousness to even glaring geometric
inconsistencies, we provide an initial exploration of the perspective
consistency of DALL-E-2 synthesized images to determine if geometric-based
forensic analyses will prove fruitful in detecting this new breed of synthetic
media.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BoT-SORT: Robust Associations Multi-Pedestrian Tracking. (arXiv:2206.14651v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14651">
<div class="article-summary-box-inner">
<span><p>The goal of multi-object tracking (MOT) is detecting and tracking all the
objects in a scene, while keeping a unique identifier for each object. In this
paper, we present a new robust state-of-the-art tracker, which can combine the
advantages of motion and appearance information, along with camera-motion
compensation, and a more accurate Kalman filter state vector. Our new trackers
BoT-SORT, and BoT-SORT-ReID rank first in the datasets of MOTChallenge [29, 11]
on both MOT17 and MOT20 test sets, in terms of all the main MOT metrics: MOTA,
IDF1, and HOTA. For MOT17: 80.5 MOTA, 80.2 IDF1, and 65.0 HOTA are achieved.
The source code and the pre-trained models are available at
https://github.com/NirAharon/BOT-SORT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cut Inner Layers: A Structured Pruning Strategy for Efficient U-Net GANs. (arXiv:2206.14658v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14658">
<div class="article-summary-box-inner">
<span><p>Pruning effectively compresses overparameterized models. Despite the success
of pruning methods for discriminative models, applying them for generative
models has been relatively rarely approached. This study conducts structured
pruning on U-Net generators of conditional GANs. A per-layer sensitivity
analysis confirms that many unnecessary filters exist in the innermost layers
near the bottleneck and can be substantially pruned. Based on this observation,
we prune these filters from multiple inner layers or suggest alternative
architectures by completely eliminating the layers. We evaluate our approach
with Pix2Pix for image-to-image translation and Wav2Lip for speech-driven
talking face generation. Our method outperforms global pruning baselines,
demonstrating the importance of properly considering where to prune for U-Net
generators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiometryNet: Landmark-based Fetal Biometry Estimation from Standard Ultrasound Planes. (arXiv:2206.14678v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14678">
<div class="article-summary-box-inner">
<span><p>Fetal growth assessment from ultrasound is based on a few biometric
measurements that are performed manually and assessed relative to the expected
gestational age. Reliable biometry estimation depends on the precise detection
of landmarks in standard ultrasound planes. Manual annotation can be
time-consuming and operator dependent task, and may results in high
measurements variability. Existing methods for automatic fetal biometry rely on
initial automatic fetal structure segmentation followed by geometric landmark
detection. However, segmentation annotations are time-consuming and may be
inaccurate, and landmark detection requires developing measurement-specific
geometric methods. This paper describes BiometryNet, an end-to-end landmark
regression framework for fetal biometry estimation that overcomes these
limitations. It includes a novel Dynamic Orientation Determination (DOD) method
for enforcing measurement-specific orientation consistency during network
training. DOD reduces variabilities in network training, increases landmark
localization accuracy, thus yields accurate and robust biometric measurements.
To validate our method, we assembled a dataset of 3,398 ultrasound images from
1,829 subjects acquired in three clinical sites with seven different ultrasound
devices. Comparison and cross-validation of three different biometric
measurements on two independent datasets shows that BiometryNet is robust and
yields accurate measurements whose errors are lower than the clinically
permissible errors, outperforming other existing automated biometry estimation
methods. Code is available at
https://github.com/netanellavisdris/fetalbiometry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-scale Physical Representations for Approximating PDE Solutions with Graph Neural Operators. (arXiv:2206.14687v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14687">
<div class="article-summary-box-inner">
<span><p>Representing physical signals at different scales is among the most
challenging problems in engineering. Several multi-scale modeling tools have
been developed to describe physical systems governed by \emph{Partial
Differential Equations} (PDEs). These tools are at the crossroad of principled
physical models and numerical schema. Recently, data-driven models have been
introduced to speed-up the approximation of PDE solutions compared to numerical
solvers. Among these recent data-driven methods, neural integral operators are
a class that learn a mapping between function spaces. These functions are
discretized on graphs (meshes) which are appropriate for modeling interactions
in physical phenomena. In this work, we study three multi-resolution schema
with integral kernel operators that can be approximated with \emph{Message
Passing Graph Neural Networks} (MPGNNs). To validate our study, we make
extensive MPGNNs experiments with well-chosen metrics considering steady and
unsteady PDEs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interventional Contrastive Learning with Meta Semantic Regularizer. (arXiv:2206.14702v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14702">
<div class="article-summary-box-inner">
<span><p>Contrastive learning (CL)-based self-supervised learning models learn visual
representations in a pairwise manner. Although the prevailing CL model has
achieved great progress, in this paper, we uncover an ever-overlooked
phenomenon: When the CL model is trained with full images, the performance
tested in full images is better than that in foreground areas; when the CL
model is trained with foreground areas, the performance tested in full images
is worse than that in foreground areas. This observation reveals that
backgrounds in images may interfere with the model learning semantic
information and their influence has not been fully eliminated. To tackle this
issue, we build a Structural Causal Model (SCM) to model the background as a
confounder. We propose a backdoor adjustment-based regularization method,
namely Interventional Contrastive Learning with Meta Semantic Regularizer
(ICL-MSR), to perform causal intervention towards the proposed SCM. ICL-MSR can
be incorporated into any existing CL methods to alleviate background
distractions from representation learning. Theoretically, we prove that ICL-MSR
achieves a tighter error bound. Empirically, our experiments on multiple
benchmark datasets demonstrate that ICL-MSR is able to improve the performances
of different state-of-the-art CL methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An extensible Benchmarking Graph-Mesh dataset for studying Steady-State Incompressible Navier-Stokes Equations. (arXiv:2206.14709v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14709">
<div class="article-summary-box-inner">
<span><p>Recent progress in \emph{Geometric Deep Learning} (GDL) has shown its
potential to provide powerful data-driven models. This gives momentum to
explore new methods for learning physical systems governed by \emph{Partial
Differential Equations} (PDEs) from Graph-Mesh data. However, despite the
efforts and recent achievements, several research directions remain unexplored
and progress is still far from satisfying the physical requirements of
real-world phenomena. One of the major impediments is the absence of
benchmarking datasets and common physics evaluation protocols. In this paper,
we propose a 2-D graph-mesh dataset to study the airflow over airfoils at high
Reynolds regime (from $10^6$ and beyond). We also introduce metrics on the
stress forces over the airfoil in order to evaluate GDL models on important
physical quantities. Moreover, we provide extensive GDL baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CONVIQT: Contrastive Video Quality Estimator. (arXiv:2206.14713v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14713">
<div class="article-summary-box-inner">
<span><p>Perceptual video quality assessment (VQA) is an integral component of many
streaming and video sharing platforms. Here we consider the problem of learning
perceptually relevant video quality representations in a self-supervised
manner. Distortion type identification and degradation level determination is
employed as an auxiliary task to train a deep learning model containing a deep
Convolutional Neural Network (CNN) that extracts spatial features, as well as a
recurrent unit that captures temporal information. The model is trained using a
contrastive loss and we therefore refer to this training framework and
resulting model as CONtrastive VIdeo Quality EstimaTor (CONVIQT). During
testing, the weights of the trained model are frozen, and a linear regressor
maps the learned features to quality scores in a no-reference (NR) setting. We
conduct comprehensive evaluations of the proposed model on multiple VQA
databases by analyzing the correlations between model predictions and
ground-truth quality ratings, and achieve competitive performance when compared
to state-of-the-art NR-VQA models, even though it is not trained on those
databases. Our ablation experiments demonstrate that the learned
representations are highly robust and generalize well across synthetic and
realistic distortions. Our results indicate that compelling representations
with perceptual bearing can be obtained using self-supervised learning. The
implementations used in this work have been made available at
https://github.com/pavancm/CONVIQT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LViT: Language meets Vision Transformer in Medical Image Segmentation. (arXiv:2206.14718v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14718">
<div class="article-summary-box-inner">
<span><p>Deep learning has been widely used in medical image segmentation and other
aspects. However, the performance of existing medical image segmentation models
has been limited by the challenge of obtaining sufficient number of
high-quality data with the high cost of data annotation. To overcome the
limitation, we propose a new vision-language medical image segmentation model
LViT (Language meets Vision Transformer). In our model, medical text annotation
is introduced to compensate for the quality deficiency in image data. In
addition, the text information can guide the generation of pseudo labels to a
certain extent and further guarantee the quality of pseudo labels in
semi-supervised learning. We also propose the Exponential Pseudo label
Iteration mechanism (EPI) to help extend the semi-supervised version of LViT
and the Pixel-Level Attention Module (PLAM) to preserve local features of
images. In our model, LV (Language-Vision) loss is designed to supervise the
training of unlabeled images using text information directly. To validate the
performance of LViT, we construct multimodal medical segmentation datasets
(image + text) containing pathological images, X-rays,etc. Experimental results
show that our proposed LViT has better segmentation performance in both fully
and semi-supervised conditions. Code and datasets are available at
https://github.com/HUANGLIZI/LViT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GO-Surf: Neural Feature Grid Optimization for Fast, High-Fidelity RGB-D Surface Reconstruction. (arXiv:2206.14735v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14735">
<div class="article-summary-box-inner">
<span><p>We present GO-Surf, a direct feature grid optimization method for accurate
and fast surface reconstruction from RGB-D sequences. We model the underlying
scene with a learned hierarchical feature voxel grid that encapsulates
multi-level geometric and appearance local information. Feature vectors are
directly optimized such that after being tri-linearly interpolated, decoded by
two shallow MLPs into signed distance and radiance values, and rendered via
surface volume rendering, the discrepancy between synthesized and observed
RGB/depth values is minimized. Our supervision signals -- RGB, depth and
approximate SDF -- can be obtained directly from input images without any need
for fusion or post-processing. We formulate a novel SDF gradient regularization
term that encourages surface smoothness and hole filling while maintaining high
frequency details. GO-Surf can optimize sequences of $1$-$2$K frames in
$15$-$45$ minutes, a speedup of $\times60$ over NeuralRGB-D, the most related
approach based on an MLP representation, while maintaining on par performance
on standard benchmarks. Project page: https://jingwenwang95.github.io/go_surf/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Placenta Segmentation in Ultrasound Imaging: Addressing Sources of Uncertainty and Limited Field-of-View. (arXiv:2206.14746v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14746">
<div class="article-summary-box-inner">
<span><p>Automatic segmentation of the placenta in fetal ultrasound (US) is
challenging due to the (i) high diversity of placenta appearance, (ii) the
restricted quality in US resulting in highly variable reference annotations,
and (iii) the limited field-of-view of US prohibiting whole placenta assessment
at late gestation. In this work, we address these three challenges with a
multi-task learning approach that combines the classification of placental
location (e.g., anterior, posterior) and semantic placenta segmentation in a
single convolutional neural network. Through the classification task the model
can learn from larger and more diverse datasets while improving the accuracy of
the segmentation task in particular in limited training set conditions. With
this approach we investigate the variability in annotations from multiple
raters and show that our automatic segmentations (Dice of 0.86 for anterior and
0.83 for posterior placentas) achieve human-level performance as compared to
intra- and inter-observer variability. Lastly, our approach can deliver whole
placenta segmentation using a multi-view US acquisition pipeline consisting of
three stages: multi-probe image acquisition, image fusion and image
segmentation. This results in high quality segmentation of larger structures
such as the placenta in US with reduced image artifacts which are beyond the
field-of-view of single probes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D-Aware Video Generation. (arXiv:2206.14797v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14797">
<div class="article-summary-box-inner">
<span><p>Generative models have emerged as an essential building block for many image
synthesis and editing tasks. Recent advances in this field have also enabled
high-quality 3D or video content to be generated that exhibits either
multi-view or temporal consistency. With our work, we explore 4D generative
adversarial networks (GANs) that learn unconditional generation of 3D-aware
videos. By combining neural implicit representations with time-aware
discriminator, we develop a GAN framework that synthesizes 3D video supervised
only with monocular videos. We show that our method learns a rich embedding of
decomposable 3D structures and motions that enables new visual effects of
spatio-temporal renderings while producing imagery with quality comparable to
that of existing 3D or video GANs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SoloGAN: Multi-domain Multimodal Unpaired Image-to-Image Translation via a Single Generative Adversarial Network. (arXiv:2008.01681v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.01681">
<div class="article-summary-box-inner">
<span><p>Despite significant advances in image-to-image (I2I) translation with
generative adversarial networks (GANs), it remains challenging to effectively
translate an image to a set of diverse images in multiple target domains using
a single pair of generator and discriminator. Existing I2I translation methods
adopt multiple domain-specific content encoders for different domains, where
each domain-specific content encoder is trained with images from the same
domain only. Nevertheless, we argue that the content (domain-invariance)
features should be learned from images among all of the domains. Consequently,
each domain-specific content encoder of existing schemes fails to extract the
domain-invariant features efficiently. To address this issue, we present a
flexible and general SoloGAN model for efficient multimodal I2I translation
among multiple domains with unpaired data. In contrast to existing methods, the
SoloGAN algorithm uses a single projection discriminator with an additional
auxiliary classifier and shares the encoder and generator for all domains.
Consequently, the SoloGAN can be trained effectively with images from all
domains such that the domain-invariance content representation can be
efficiently extracted. Qualitative and quantitative results over a wide range
of datasets against several counterparts and variants of the SoloGAN
demonstrate the merits of the method, especially for challenging I2I
translation datasets, i.e., datasets involving extreme shape variations or need
to keep the complex backgrounds unchanged after translations. Furthermore, we
demonstrate the contribution of each component in SoloGAN by ablation studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zoom-to-Inpaint: Image Inpainting with High-Frequency Details. (arXiv:2012.09401v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.09401">
<div class="article-summary-box-inner">
<span><p>Although deep learning has enabled a huge leap forward in image inpainting,
current methods are often unable to synthesize realistic high-frequency
details. In this paper, we propose applying super-resolution to coarsely
reconstructed outputs, refining them at high resolution, and then downscaling
the output to the original resolution. By introducing high-resolution images to
the refinement network, our framework is able to reconstruct finer details that
are usually smoothed out due to spectral bias - the tendency of neural networks
to reconstruct low frequencies better than high frequencies. To assist training
the refinement network on large upscaled holes, we propose a progressive
learning technique in which the size of the missing regions increases as
training progresses. Our zoom-in, refine and zoom-out strategy, combined with
high-resolution supervision and progressive learning, constitutes a
framework-agnostic approach for enhancing high-frequency details that can be
applied to any CNN-based inpainting method. We provide qualitative and
quantitative evaluations along with an ablation analysis to show the
effectiveness of our approach. This seemingly simple, yet powerful approach,
outperforms state-of-the-art inpainting methods. Our code is available in
https://github.com/google/zoom-to-inpaint
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Session Visual SLAM for Illumination Invariant Re-Localization in Indoor Environments. (arXiv:2103.03827v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03827">
<div class="article-summary-box-inner">
<span><p>For robots navigating using only a camera, illumination changes in indoor
environments can cause re-localization failures during autonomous navigation.
In this paper, we present a multi-session visual SLAM approach to create a map
made of multiple variations of the same locations in different illumination
conditions. The multi-session map can then be used at any hour of the day for
improved re-localization capability. The approach presented is independent of
the visual features used, and this is demonstrated by comparing re-localization
performance between multi-session maps created using the RTAB-Map library with
SURF, SIFT, BRIEF, BRISK, KAZE, DAISY and SuperPoint visual features. The
approach is tested on six mapping and six localization sessions recorded at 30
minute intervals during sunset using a Google Tango phone in a real apartment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoMoGAN: continuous model-guided image-to-image translation. (arXiv:2103.06879v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06879">
<div class="article-summary-box-inner">
<span><p>CoMoGAN is a continuous GAN relying on the unsupervised reorganization of the
target data on a functional manifold. To that matter, we introduce a new
Functional Instance Normalization layer and residual mechanism, which together
disentangle image content from position on target manifold. We rely on naive
physics-inspired models to guide the training while allowing private
model/translations features. CoMoGAN can be used with any GAN backbone and
allows new types of image translation, such as cyclic image translation like
timelapse generation, or detached linear translation. On all datasets, it
outperforms the literature. Our code is available at
<a href="http://github.com/cv-rits/CoMoGAN">this http URL</a> .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometry-based Distance Decomposition for Monocular 3D Object Detection. (arXiv:2104.03775v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03775">
<div class="article-summary-box-inner">
<span><p>Monocular 3D object detection is of great significance for autonomous driving
but remains challenging. The core challenge is to predict the distance of
objects in the absence of explicit depth information. Unlike regressing the
distance as a single variable in most existing methods, we propose a novel
geometry-based distance decomposition to recover the distance by its factors.
The decomposition factors the distance of objects into the most representative
and stable variables, i.e. the physical height and the projected visual height
in the image plane. Moreover, the decomposition maintains the self-consistency
between the two heights, leading to robust distance prediction when both
predicted heights are inaccurate. The decomposition also enables us to trace
the causes of the distance uncertainty for different scenarios. Such
decomposition makes the distance prediction interpretable, accurate, and
robust. Our method directly predicts 3D bounding boxes from RGB images with a
compact architecture, making the training and inference simple and efficient.
The experimental results show that our method achieves the state-of-the-art
performance on the monocular 3D Object Detection and Birds Eye View tasks of
the KITTI dataset, and can generalize to images with different camera
intrinsics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead. (arXiv:2105.09121v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09121">
<div class="article-summary-box-inner">
<span><p>Deploying deep learning models in time-critical applications with limited
computational resources, for instance in edge computing systems and IoT
networks, is a challenging task that often relies on dynamic inference methods
such as early exiting. In this paper, we introduce a novel architecture for
early exiting based on the vision transformer architecture, as well as a
fine-tuning strategy that significantly increase the accuracy of early exit
branches compared to conventional approaches while introducing less overhead.
Through extensive experiments on image and audio classification as well as
audiovisual crowd counting, we show that our method works for both
classification and regression problems, and in both single- and multi-modal
settings. Additionally, we introduce a novel method for integrating audio and
visual modalities within early exits in audiovisual data analysis, that can
lead to a more fine-grained dynamic inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Latent Space of Autoencoders with Interventional Assays. (arXiv:2106.16091v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.16091">
<div class="article-summary-box-inner">
<span><p>Autoencoders exhibit impressive abilities to embed the data manifold into a
low-dimensional latent space, making them a staple of representation learning
methods. However, without explicit supervision, which is often unavailable, the
representation is usually uninterpretable, making analysis and principled
progress challenging. We propose a framework, called latent responses, which
exploits the locally contractive behavior exhibited by variational autoencoders
to explore the learned manifold. More specifically, we develop tools to probe
the representation using interventions in the latent space to quantify the
relationships between latent variables. We extend the notion of disentanglement
to take the learned generative process into account and consequently avoid the
limitations of existing metrics that may rely on spurious correlations. Our
analyses underscore the importance of studying the causal structure of the
representation to improve performance on downstream tasks such as generation,
interpolation, and inference of the factors of variation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DOVE: Learning Deformable 3D Objects by Watching Videos. (arXiv:2107.10844v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10844">
<div class="article-summary-box-inner">
<span><p>Learning deformable 3D objects from 2D images is often an ill-posed problem.
Existing methods rely on explicit supervision to establish multi-view
correspondences, such as template shape models and keypoint annotations, which
restricts their applicability on objects "in the wild". A more natural way of
establishing correspondences is by watching videos of objects moving around. In
this paper, we present DOVE, a method that learns textured 3D models of
deformable object categories from monocular videos available online, without
keypoint, viewpoint or template shape supervision. By resolving
symmetry-induced pose ambiguities and leveraging temporal correspondences in
videos, the model automatically learns to factor out 3D shape, articulated pose
and texture from each individual RGB frame, and is ready for single-image
inference at test time. In the experiments, we show that existing methods fail
to learn sensible 3D shapes without additional keypoint or template
supervision, whereas our method produces temporally consistent 3D models, which
can be animated and rendered from arbitrary viewpoints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physics-informed Guided Disentanglement in Generative Networks. (arXiv:2107.14229v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14229">
<div class="article-summary-box-inner">
<span><p>Image-to-image translation (i2i) networks suffer from entanglement effects in
presence of physics-related phenomena in target domain (such as occlusions,
fog, etc), lowering altogether the translation quality, controllability and
variability. In this paper, we build upon collection of simple physics models
and present a comprehensive method for disentangling visual traits in target
images, guiding the process with a physical model that renders some of the
target traits, and learning the remaining ones. Because it allows explicit and
interpretable outputs, our physical models (optimally regressed on target)
allows generating unseen scenarios in a controllable manner. We also extend our
framework, showing versatility to neural-guided disentanglement. The results
show our disentanglement strategies dramatically increase performances
qualitatively and quantitatively in several challenging scenarios for image
translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-imaging real-time detection and tracking of fast-moving objects using a single-pixel detector. (arXiv:2108.06009v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06009">
<div class="article-summary-box-inner">
<span><p>Detection and tracking of fast-moving objects have widespread utility in many
fields. However, fulfilling this demand for fast and efficient detecting and
tracking using image-based techniques is problematic, owing to the complex
calculations and limited data processing capabilities. To tackle this problem,
we propose an image-free method to achieve real-time detection and tracking of
fast-moving objects. It employs the Hadamard pattern to illuminate the
fast-moving object by a spatial light modulator, in which the resulting light
signal is collected by a single-pixel detector. The single-pixel measurement
values are directly used to reconstruct the position information without image
reconstruction. Furthermore, a new sampling method is used to optimize the
pattern projection way for achieving an ultra-low sampling rate. Compared with
the state-of-the-art methods, our approach is not only capable of handling
real-time detection and tracking, but also it has a small amount of calculation
and high efficiency. We experimentally demonstrate that the proposed method,
using a 22kHz digital micro-mirror device, can implement a 105fps frame rate at
a 1.28% sampling rate when tracked. Our method breaks through the traditional
tracking ways, which can implement the object real-time tracking without image
reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Image Region Mining with Region Prototypical Network for Weakly Supervised Segmentation. (arXiv:2108.07413v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07413">
<div class="article-summary-box-inner">
<span><p>Weakly supervised image segmentation trained with image-level labels usually
suffers from inaccurate coverage of object areas during the generation of the
pseudo groundtruth. This is because the object activation maps are trained with
the classification objective and lack the ability to generalize. To improve the
generality of the objective activation maps, we propose a region prototypical
network RPNet to explore the cross-image object diversity of the training set.
Similar object parts across images are identified via region feature
comparison. Object confidence is propagated between regions to discover new
object areas while background regions are suppressed. Experiments show that the
proposed method generates more complete and accurate pseudo object masks, while
achieving state-of-the-art performance on PASCAL VOC 2012 and MS COCO. In
addition, we investigate the robustness of the proposed method on reduced
training sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Segmentation with Optimal Transport Matching and Message Flow. (arXiv:2108.08518v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08518">
<div class="article-summary-box-inner">
<span><p>We tackle the challenging task of few-shot segmentation in this work. It is
essential for few-shot semantic segmentation to fully utilize the support
information. Previous methods typically adopt masked average pooling over the
support feature to extract the support clues as a global vector, usually
dominated by the salient part and lost certain essential clues. In this work,
we argue that every support pixel's information is desired to be transferred to
all query pixels and propose a Correspondence Matching Network (CMNet) with an
Optimal Transport Matching module to mine out the correspondence between the
query and support images. Besides, it is critical to fully utilize both local
and global information from the annotated support images. To this end, we
propose a Message Flow module to propagate the message along the inner-flow
inside the same image and cross-flow between support and query images, which
greatly helps enhance the local feature representations. Experiments on PASCAL
VOC 2012, MS COCO, and FSS-1000 datasets show that our network achieves new
state-of-the-art few-shot segmentation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REFLACX, a dataset of reports and eye-tracking data for localization of abnormalities in chest x-rays. (arXiv:2109.14187v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14187">
<div class="article-summary-box-inner">
<span><p>Deep learning has shown recent success in classifying anomalies in chest
x-rays, but datasets are still small compared to natural image datasets.
Supervision of abnormality localization has been shown to improve trained
models, partially compensating for dataset sizes. However, explicitly labeling
these anomalies requires an expert and is very time-consuming. We propose a
potentially scalable method for collecting implicit localization data using an
eye tracker to capture gaze locations and a microphone to capture a dictation
of a report, imitating the setup of a reading room. The resulting REFLACX
(Reports and Eye-Tracking Data for Localization of Abnormalities in Chest
X-rays) dataset was labeled across five radiologists and contains 3,032
synchronized sets of eye-tracking data and timestamped report transcriptions
for 2,616 chest x-rays from the MIMIC-CXR dataset. We also provide auxiliary
annotations, including bounding boxes around lungs and heart and validation
labels consisting of ellipses localizing abnormalities and image-level labels.
Furthermore, a small subset of the data contains readings from all
radiologists, allowing for the calculation of inter-rater scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TranSalNet: Towards perceptually relevant visual saliency prediction. (arXiv:2110.03593v3 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03593">
<div class="article-summary-box-inner">
<span><p>Visual saliency prediction using transformers - Convolutional neural networks
(CNNs) have significantly advanced computational modelling for saliency
prediction. However, accurately simulating the mechanisms of visual attention
in the human cortex remains an academic challenge. It is critical to integrate
properties of human vision into the design of CNN architectures, leading to
perceptually more relevant saliency prediction. Due to the inherent inductive
biases of CNN architectures, there is a lack of sufficient long-range
contextual encoding capacity. This hinders CNN-based saliency models from
capturing properties that emulate viewing behaviour of humans. Transformers
have shown great potential in encoding long-range information by leveraging the
self-attention mechanism. In this paper, we propose a novel saliency model that
integrates transformer components to CNNs to capture the long-range contextual
visual information. Experimental results show that the transformers provide
added value to saliency prediction, enhancing its perceptual relevance in the
performance. Our proposed saliency model using transformers has achieved
superior results on public benchmarks and competitions for saliency prediction
models.
</p>
<p>The source code of our proposed saliency model TranSalNet is available at:
https://github.com/LJOVO/TranSalNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fire Together Wire Together: A Dynamic Pruning Approach with Self-Supervised Mask Prediction. (arXiv:2110.08232v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08232">
<div class="article-summary-box-inner">
<span><p>Dynamic model pruning is a recent direction that allows for the inference of
a different sub-network for each input sample during deployment. However,
current dynamic methods rely on learning a continuous channel gating through
regularization by inducing sparsity loss. This formulation introduces
complexity in balancing different losses (e.g task loss, regularization loss).
In addition, regularization based methods lack transparent tradeoff
hyperparameter selection to realize a computational budget. Our contribution is
two-fold: 1) decoupled task and pruning losses. 2) Simple hyperparameter
selection that enables FLOPs reduction estimation before training. Inspired by
the Hebbian theory in Neuroscience: "neurons that fire together wire together",
we propose to predict a mask to process k filters in a layer based on the
activation of its previous layer. We pose the problem as a self-supervised
binary classification problem. Each mask predictor module is trained to predict
if the log-likelihood for each filter in the current layer belongs to the top-k
activated filters. The value k is dynamically estimated for each input based on
a novel criterion using the mass of heatmaps. We show experiments on several
neural architectures, such as VGG, ResNet and MobileNet on CIFAR and ImageNet
datasets. On CIFAR, we reach similar accuracy to SOTA methods with 15% and 24%
higher FLOPs reduction. Similarly in ImageNet, we achieve lower drop in
accuracy with up to 13% improvement in FLOPs reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention W-Net: Improved Skip Connections for better Representations. (arXiv:2110.08811v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08811">
<div class="article-summary-box-inner">
<span><p>Segmentation of macro and microvascular structures in fundoscopic retinal
images plays a crucial role in the detection of multiple retinal and systemic
diseases, yet it is a difficult problem to solve. Most neural network
approaches face several issues such as lack of enough parameters, overfitting
and/or incompatibility between internal feature-spaces. We propose Attention
W-Net, a new U-Net based architecture for retinal vessel segmentation to
address these problems. In this architecture, we have two main contributions:
Attention Block and regularisation measures. Our Attention Block uses attention
between encoder and decoder features, resulting in higher compatibility upon
addition. Our regularisation measures include augmentation and modifications to
the ResNet Block used, which greatly prevent overfitting. We observe an F1 and
AUC of 0.8407 and 0.9833 on the DRIVE and 0.8174 and 0.9865 respectively on the
CHASE-DB1 datasets - a sizeable improvement over its backbone as well as
competitive performance among contemporary state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Joint Modelling Based on Hierarchical Transformer for Co-summarization. (arXiv:2112.13478v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13478">
<div class="article-summary-box-inner">
<span><p>Video summarization aims to automatically generate a summary (storyboard or
video skim) of a video, which can facilitate large-scale video retrieval and
browsing. Most of the existing methods perform video summarization on
individual videos, which neglects the correlations among similar videos. Such
correlations, however, are also informative for video understanding and video
summarization. To address this limitation, we propose Video Joint Modelling
based on Hierarchical Transformer (VJMHT) for co-summarization, which takes
into consideration the semantic dependencies across videos. Specifically, VJMHT
consists of two layers of Transformer: the first layer extracts semantic
representation from individual shots of similar videos, while the second layer
performs shot-level video joint modelling to aggregate cross-video semantic
information. By this means, complete cross-video high-level patterns are
explicitly modelled and learned for the summarization of individual videos.
Moreover, Transformer-based video representation reconstruction is introduced
to maximize the high-level similarity between the summary and the original
video. Extensive experiments are conducted to verify the effectiveness of the
proposed modules and the superiority of VJMHT in terms of F-measure and
rank-based evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Centroid-Encoder: A Nonlinear Model for Feature Selection. (arXiv:2201.12910v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12910">
<div class="article-summary-box-inner">
<span><p>Autoencoders have been widely used as a nonlinear tool for data
dimensionality reduction. While autoencoders don't utilize the label
information, Centroid-Encoders (CE)\cite{ghosh2022supervised} use the class
label in their learning process. In this study, we propose a sparse
optimization using the Centroid-Encoder architecture to determine a minimal set
of features that discriminate between two or more classes. The resulting
algorithm, Sparse Centroid-Encoder (SCE), extracts discriminatory features in
groups using a sparsity inducing $\ell_1$-norm while mapping a point to its
class centroid. One key attribute of SCE is that it can extract informative
features from a multi-modal data set, i.e., data sets whose classes appear to
have multiple clusters. The algorithm is applied to a wide variety of real
world data sets, including single-cell data, high dimensional biological data,
image data, speech data, and accelerometer sensor data. We compared our method
to various state-of-the-art feature selection techniques, including supervised
Concrete Autoencoders (SCAE), Feature Selection Network (FsNet), deep feature
selection (DFS), Stochastic Gate (STG), and LassoNet. We empirically showed
that SCE features often produced better classification accuracy than other
methods on sequester test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Monocular Depth Estimation and Uncertainty Quantification using Classification Approaches for Regression. (arXiv:2202.12369v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12369">
<div class="article-summary-box-inner">
<span><p>Monocular depth is important in many tasks, such as 3D reconstruction and
autonomous driving. Deep learning based models achieve state-of-the-art
performance in this field. A set of novel approaches for estimating monocular
depth consists of transforming the regression task into a classification one.
However, there is a lack of detailed descriptions and comparisons for
Classification Approaches for Regression (CAR) in the community and no in-depth
exploration of their potential for uncertainty estimation. To this end, this
paper will introduce a taxonomy and summary of CAR approaches, a new
uncertainty estimation solution for CAR, and a set of experiments on depth
accuracy and uncertainty quantification for CAR-based models on KITTI dataset.
The experiments reflect the differences in the portability of various CAR
methods on two backbones. Meanwhile, the newly proposed method for uncertainty
estimation can outperform the ensembling method with only one forward
propagation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Triangulation as a Form of Self-Supervision for 3D Human Pose Estimation. (arXiv:2203.15865v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15865">
<div class="article-summary-box-inner">
<span><p>Supervised approaches to 3D pose estimation from single images are remarkably
effective when labeled data is abundant. However, as the acquisition of
ground-truth 3D labels is labor intensive and time consuming, recent attention
has shifted towards semi- and weakly-supervised learning. Generating an
effective form of supervision with little annotations still poses major
challenge in crowded scenes. In this paper we propose to impose multi-view
geometrical constraints by means of a weighted differentiable triangulation and
use it as a form of self-supervision when no labels are available. We therefore
train a 2D pose estimator in such a way that its predictions correspond to the
re-projection of the triangulated 3D pose and train an auxiliary network on
them to produce the final 3D poses. We complement the triangulation with a
weighting mechanism that alleviates the impact of noisy predictions caused by
self-occlusion or occlusion from other subjects. We demonstrate the
effectiveness of our semi-supervised approach on Human3.6M and MPI-INF-3DHP
datasets, as well as on a new multi-view multi-person dataset that features
occlusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Generalizable Dexterous Manipulation from Human Grasp Affordance. (arXiv:2204.02320v4 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02320">
<div class="article-summary-box-inner">
<span><p>Dexterous manipulation with a multi-finger hand is one of the most
challenging problems in robotics. While recent progress in imitation learning
has largely improved the sample efficiency compared to Reinforcement Learning,
the learned policy can hardly generalize to manipulate novel objects, given
limited expert demonstrations. In this paper, we propose to learn dexterous
manipulation using large-scale demonstrations with diverse 3D objects in a
category, which are generated from a human grasp affordance model. This
generalizes the policy to novel object instances within the same category. To
train the policy, we propose a novel imitation learning objective jointly with
a geometric representation learning objective using our demonstrations. By
experimenting with relocating diverse objects in simulation, we show that our
approach outperforms baselines with a large margin when manipulating novel
objects. We also ablate the importance on 3D object representation learning for
manipulation. We include videos, code, and additional information on the
project website - https://kristery.github.io/ILAD/ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Learnable Variational Model for Joint Multimodal MRI Reconstruction and Synthesis. (arXiv:2204.03804v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03804">
<div class="article-summary-box-inner">
<span><p>Generating multi-contrasts/modal MRI of the same anatomy enriches diagnostic
information but is limited in practice due to excessive data acquisition time.
In this paper, we propose a novel deep-learning model for joint reconstruction
and synthesis of multi-modal MRI using incomplete k-space data of several
source modalities as inputs. The output of our model includes reconstructed
images of the source modalities and high-quality image synthesized in the
target modality. Our proposed model is formulated as a variational problem that
leverages several learnable modality-specific feature extractors and a
multimodal synthesis module. We propose a learnable optimization algorithm to
solve this model, which induces a multi-phase network whose parameters can be
trained using multi-modal MRI data. Moreover, a bilevel-optimization framework
is employed for robust parameter training. We demonstrate the effectiveness of
our approach using extensive numerical experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepCore: A Comprehensive Library for Coreset Selection in Deep Learning. (arXiv:2204.08499v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08499">
<div class="article-summary-box-inner">
<span><p>Coreset selection, which aims to select a subset of the most informative
training samples, is a long-standing learning problem that can benefit many
downstream tasks such as data-efficient learning, continual learning, neural
architecture search, active learning, etc. However, many existing coreset
selection methods are not designed for deep learning, which may have high
complexity and poor generalization performance. In addition, the recently
proposed methods are evaluated on models, datasets, and settings of different
complexities. To advance the research of coreset selection in deep learning, we
contribute a comprehensive code library, namely DeepCore, and provide an
empirical study on popular coreset selection methods on CIFAR10 and ImageNet
datasets. Extensive experiments on CIFAR10 and ImageNet datasets verify that,
although various methods have advantages in certain experiment settings, random
selection is still a strong baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Copy Motion From One to Another: Fake Motion Video Generation. (arXiv:2205.01373v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01373">
<div class="article-summary-box-inner">
<span><p>One compelling application of artificial intelligence is to generate a video
of a target person performing arbitrary desired motion (from a source person).
While the state-of-the-art methods are able to synthesize a video demonstrating
similar broad stroke motion details, they are generally lacking in texture
details. A pertinent manifestation appears as distorted face, feet, and hands,
and such flaws are very sensitively perceived by human observers. Furthermore,
current methods typically employ GANs with a L2 loss to assess the authenticity
of the generated videos, inherently requiring a large amount of training
samples to learn the texture details for adequate video generation. In this
work, we tackle these challenges from three aspects: 1) We disentangle each
video frame into foreground (the person) and background, focusing on generating
the foreground to reduce the underlying dimension of the network output. 2) We
propose a theoretically motivated Gromov-Wasserstein loss that facilitates
learning the mapping from a pose to a foreground image. 3) To enhance texture
details, we encode facial features with geometric guidance and employ local
GANs to refine the face, feet, and hands. Extensive experiments show that our
method is able to generate realistic target person videos, faithfully copying
complex motions from a source person. Our code and datasets are released at
https://github.com/Sifann/FakeMotion
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guided Diffusion Model for Adversarial Purification. (arXiv:2205.14969v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14969">
<div class="article-summary-box-inner">
<span><p>With wider application of deep neural networks (DNNs) in various algorithms
and frameworks, security threats have become one of the concerns. Adversarial
attacks disturb DNN-based image classifiers, in which attackers can
intentionally add imperceptible adversarial perturbations on input images to
fool the classifiers. In this paper, we propose a novel purification approach,
referred to as guided diffusion model for purification (GDMP), to help protect
classifiers from adversarial attacks. The core of our approach is to embed
purification into the diffusion denoising process of a Denoised Diffusion
Probabilistic Model (DDPM), so that its diffusion process could submerge the
adversarial perturbations with gradually added Gaussian noises, and both of
these noises can be simultaneously removed following a guided denoising
process. On our comprehensive experiments across various datasets, the proposed
GDMP is shown to reduce the perturbations raised by adversarial attacks to a
shallow range, thereby significantly improving the correctness of
classification. GDMP improves the robust accuracy by 5%, obtaining 90.1% under
PGD attack on the CIFAR10 dataset. Moreover, GDMP achieves 70.94% robustness on
the challenging ImageNet dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning for Building Damage Assessment from Large-scale xBD Satellite Imagery Benchmark Datasets. (arXiv:2205.15688v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15688">
<div class="article-summary-box-inner">
<span><p>In the field of post-disaster assessment, for timely and accurate rescue and
localization after a disaster, people need to know the location of damaged
buildings. In deep learning, some scholars have proposed methods to make
automatic and highly accurate building damage assessments by remote sensing
images, which are proved to be more efficient than assessment by domain
experts. However, due to the lack of a large amount of labeled data, these
kinds of tasks can suffer from being able to do an accurate assessment, as the
efficiency of deep learning models relies highly on labeled data. Although
existing semi-supervised and unsupervised studies have made breakthroughs in
this area, none of them has completely solved this problem. Therefore, we
propose adopting a self-supervised comparative learning approach to address the
task without the requirement of labeled data. We constructed a novel asymmetric
twin network architecture and tested its performance on the xBD dataset.
Experiment results of our model show the improvement compared to baseline and
commonly used methods. We also demonstrated the potential of self-supervised
methods for building damage recognition awareness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Distribution Discrepancy for Anomaly Detection in Chest X-Rays. (arXiv:2206.03935v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03935">
<div class="article-summary-box-inner">
<span><p>Chest X-ray (CXR) is the most typical radiological exam for diagnosis of
various diseases. Due to the expensive and time-consuming annotations,
detecting anomalies in CXRs in an unsupervised fashion is very promising.
However, almost all of the existing methods consider anomaly detection as a
one-class classification (OCC) problem. They model the distribution of only
known normal images during training and identify the samples not conforming to
normal profile as anomalies in the testing phase. A large number of unlabeled
images containing anomalies are thus ignored in the training phase, although
they are easy to obtain in clinical practice. In this paper, we propose a novel
strategy, Dual-distribution Discrepancy for Anomaly Detection (DDAD), utilizing
both known normal images and unlabeled images. The proposed method consists of
two modules. During training, one module takes both known normal and unlabeled
images as inputs, capturing anomalous features from unlabeled images in some
way, while the other one models the distribution of only known normal images.
Subsequently, inter-discrepancy between the two modules, and intra-discrepancy
inside the module that is trained on only normal images are designed as anomaly
scores to indicate anomalies. Experiments on three CXR datasets demonstrate
that the proposed DDAD achieves consistent, significant gains and outperforms
state-of-the-art methods. Code is available at
https://github.com/caiyu6666/DDAD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GSmooth: Certified Robustness against Semantic Transformations via Generalized Randomized Smoothing. (arXiv:2206.04310v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04310">
<div class="article-summary-box-inner">
<span><p>Certified defenses such as randomized smoothing have shown promise towards
building reliable machine learning systems against $\ell_p$-norm bounded
attacks. However, existing methods are insufficient or unable to provably
defend against semantic transformations, especially those without closed-form
expressions (such as defocus blur and pixelate), which are more common in
practice and often unrestricted. To fill up this gap, we propose generalized
randomized smoothing (GSmooth), a unified theoretical framework for certifying
robustness against general semantic transformations via a novel dimension
augmentation strategy. Under the GSmooth framework, we present a scalable
algorithm that uses a surrogate image-to-image network to approximate the
complex transformation. The surrogate model provides a powerful tool for
studying the properties of semantic transformations and certifying robustness.
Experimental results on several datasets demonstrate the effectiveness of our
approach for robustness certification against multiple kinds of semantic
transformations and corruptions, which is not achievable by the alternative
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpA-Former: Transformer image shadow detection and removal via spatial attention. (arXiv:2206.10910v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10910">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an end-to-end SpA-Former to recover a shadow-free
image from a single shaded image. Unlike traditional methods that require two
steps for shadow detection and then shadow removal, the SpA-Former unifies
these steps into one, which is a one-stage network capable of directly learning
the mapping function between shadows and no shadows, it does not require a
separate shadow detection. Thus, SpA-former is adaptable to real image
de-shadowing for shadows projected on different semantic regions. SpA-Former
consists of transformer layer and a series of joint Fourier transform residual
blocks and two-wheel joint spatial attention. The network in this paper is able
to handle the task while achieving a very fast processing efficiency.
</p>
<p>Our code is relased on https://github.com/
zhangbaijin/Spatial-Transformer-shadow-removal
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated GI tract segmentation using deep learning. (arXiv:2206.11048v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11048">
<div class="article-summary-box-inner">
<span><p>The job of Radiation oncologists is to deliver x-ray beams pointed toward the
tumor and at the same time avoid the stomach and intestines. With MR-Linacs
(magnetic resonance imaging and linear accelerator systems), oncologists can
visualize the position of the tumor and allow for precise dose according to
tumor cell presence which can vary from day to day. The current job of
outlining the position of the stomach and intestines to adjust the X-ray beams
direction for the dose delivery to the tumor while avoiding the organs. This is
a time-consuming and labor-intensive process that can easily prolong treatments
from 15 minutes to an hour a day unless deep learning methods can automate the
segmentation process. This paper discusses an automated segmentation process
using deep learning to make this process faster and allow more patients to get
effective treatment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Manifold Topology Divergence: a Framework for Comparing Data Manifolds. (arXiv:2106.04024v2 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04024">
<div class="article-summary-box-inner">
<span><p>We develop a framework for comparing data manifolds, aimed, in particular,
towards the evaluation of deep generative models. We describe a novel tool,
Cross-Barcode(P,Q), that, given a pair of distributions in a high-dimensional
space, tracks multiscale topology spacial discrepancies between manifolds on
which the distributions are concentrated. Based on the Cross-Barcode, we
introduce the Manifold Topology Divergence score (MTop-Divergence) and apply it
to assess the performance of deep generative models in various domains: images,
3D-shapes, time-series, and on different datasets: MNIST, Fashion MNIST, SVHN,
CIFAR10, FFHQ, chest X-ray images, market stock data, ShapeNet. We demonstrate
that the MTop-Divergence accurately detects various degrees of mode-dropping,
intra-mode collapse, mode invention, and image disturbance. Our algorithm
scales well (essentially linearly) with the increase of the dimension of the
ambient high-dimensional space. It is one of the first TDA-based practical
methodologies that can be applied universally to datasets of different sizes
and dimensions, including the ones on which the most recent GANs in the visual
domain are trained. The proposed method is domain agnostic and does not rely on
pre-trained networks.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-06-30 23:08:17.257789175 UTC">2022-06-30 23:08:17 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>