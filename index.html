<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-05-02T01:30:00Z">05-02</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">HiNER: A Large Hindi Named Entity Recognition Dataset. (arXiv:2204.13743v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13743">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) is a foundational NLP task that aims to
provide class labels like Person, Location, Organisation, Time, and Number to
words in free text. Named Entities can also be multi-word expressions where the
additional I-O-B annotation information helps label them during the NER
annotation process. While English and European languages have considerable
annotated data for the NER task, Indian languages lack on that front -- both in
terms of quantity and following annotation standards. This paper releases a
significantly sized standard-abiding Hindi NER dataset containing 109,146
sentences and 2,220,856 tokens, annotated with 11 tags. We discuss the dataset
statistics in all their essential detail and provide an in-depth analysis of
the NER tag-set used with our data. The statistics of tag-set in our dataset
show a healthy per-tag distribution, especially for prominent classes like
Person, Location and Organisation. Since the proof of resource-effectiveness is
in building models with the resource and testing the model on benchmark data
and against the leader-board entries in shared tasks, we do the same with the
aforesaid data. We use different language models to perform the sequence
labelling task for NER and show the efficacy of our data by performing a
comparative evaluation with models trained on another dataset available for the
Hindi NER task. Our dataset helps achieve a weighted F1 score of 88.78 with all
the tags and 92.22 when we collapse the tag-set, as discussed in the paper. To
the best of our knowledge, no available dataset meets the standards of volume
(amount) and variability (diversity), as far as Hindi NER is concerned. We fill
this gap through this work, which we hope will significantly help NLP for
Hindi. We release this dataset with our code and models at
https://github.com/cfiltnlp/HiNER
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAVES: A Dataset to facilitate Explainable Classification and Summarization of Concerns towards COVID Vaccines. (arXiv:2204.13746v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13746">
<div class="article-summary-box-inner">
<span><p>Convincing people to get vaccinated against COVID-19 is a key societal
challenge in the present times. As a first step towards this goal, many prior
works have relied on social media analysis to understand the specific concerns
that people have towards these vaccines, such as potential side-effects,
ineffectiveness, political factors, and so on. Though there are datasets that
broadly classify social media posts into Anti-vax and Pro-Vax labels, there is
no dataset (to our knowledge) that labels social media posts according to the
specific anti-vaccine concerns mentioned in the posts. In this paper, we have
curated CAVES, the first large-scale dataset containing about 10k COVID-19
anti-vaccine tweets labelled into various specific anti-vaccine concerns in a
multi-label setting. This is also the first multi-label classification dataset
that provides explanations for each of the labels. Additionally, the dataset
also provides class-wise summaries of all the tweets. We also perform
preliminary experiments on the dataset and show that this is a very challenging
dataset for multi-label explainable classification and tweet summarization, as
is evident by the moderate scores achieved by some state-of-the-art models. Our
dataset and codes are available at: https://github.com/sohampoddar26/caves-data
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Split for Automatic Bias Detection. (arXiv:2204.13749v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13749">
<div class="article-summary-box-inner">
<span><p>Classifiers are biased when trained on biased datasets. As a remedy, we
propose Learning to Split (ls), an algorithm for automatic bias detection.
Given a dataset with input-label pairs, ls learns to split this dataset so that
predictors trained on the training split generalize poorly to the testing
split. This performance gap provides a proxy for measuring the degree of bias
in the learned features and can therefore be used to reduce biases. Identifying
non-generalizable splits is challenging as we don't have any explicit
annotations about how to split. In this work, we show that the prediction
correctness of the testing example can be used as a source of weak supervision:
generalization performance will drop if we move examples that are predicted
correctly away from the testing split, leaving only those that are
mispredicted. We evaluate our approach on Beer Review, Waterbirds, CelebA and
MNLI. Empirical results show that ls is able to generate astonishingly
challenging splits that correlate with human-identified biases. Moreover, we
demonstrate that combining robust learning algorithms (such as group DRO) with
splits identified by ls enables automatic de-biasing. Compared with previous
state-of-the-arts, we substantially improves the worst-group performance (23.4%
on average) when the source of biases is unknown during training and
validation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Faithful to the Document or to the World? Mitigating Hallucinations via Entity-linked Knowledge in Abstractive Summarization. (arXiv:2204.13761v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13761">
<div class="article-summary-box-inner">
<span><p>Despite recent advances in abstractive summarization, current summarization
systems still suffer from content hallucinations where models generate text
that is either irrelevant or contradictory to the source document. However,
prior work has been predicated on the assumption that any generated facts not
appearing explicitly in the source are undesired hallucinations. Methods have
been proposed to address this scenario by ultimately improving `faithfulness'
to the source document, but in reality, there is a large portion of entities in
the gold reference targets that are not directly in the source. In this work,
we show that these entities are not aberrations, but they instead require
utilizing external world knowledge to infer reasoning paths from entities in
the source. We show that by utilizing an external knowledge base, we can
improve the faithfulness of summaries without simply making them more
extractive, and additionally, we show that external knowledge bases linked from
the source can benefit the factuality of generated summaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inferring Implicit Relations with Language Models. (arXiv:2204.13778v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13778">
<div class="article-summary-box-inner">
<span><p>A prominent challenge for modern language understanding systems is the
ability to answer implicit reasoning questions, where the required reasoning
steps for answering the question are not mentioned in the text explicitly. In
this work, we investigate why current models struggle with implicit reasoning
question answering (QA) tasks, by decoupling inference of reasoning steps from
their execution. We define a new task of implicit relation inference and
construct a benchmark, IMPLICITRELATIONS, where given a question, a model
should output a list of concept-relation pairs, where the relations describe
the implicit reasoning steps required for answering the question. Using
IMPLICITRELATIONS, we evaluate models from the GPT-3 family and find that,
while these models struggle on the implicit reasoning QA task, they often
succeed at inferring implicit relations. This suggests that the bottleneck for
answering implicit reasoning questions is in the ability of language models to
retrieve and reason over information rather than to plan an accurate reasoning
strategy
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instilling Type Knowledge in Language Models via Multi-Task QA. (arXiv:2204.13796v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13796">
<div class="article-summary-box-inner">
<span><p>Understanding human language often necessitates understanding entities and
their place in a taxonomy of knowledge -- their types. Previous methods to
learn entity types rely on training classifiers on datasets with coarse, noisy,
and incomplete labels. We introduce a method to instill fine-grained type
knowledge in language models with text-to-text pre-training on type-centric
questions leveraging knowledge base documents and knowledge graphs. We create
the WikiWiki dataset: entities and passages from 10M Wikipedia articles linked
to the Wikidata knowledge graph with 41K types. Models trained on WikiWiki
achieve state-of-the-art performance in zero-shot dialog state tracking
benchmarks, accurately infer entity types in Wikipedia articles, and can
discover new types deemed useful by human judges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating writing style as a contributor to gender gaps in science and technology. (arXiv:2204.13805v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13805">
<div class="article-summary-box-inner">
<span><p>While universalism is a foundational principle of science, a growing stream
of research finds that scientific contributions are evaluated differently
depending on the gender of the author, with women tending to receive fewer
citations relative to men, even for work of comparable quality. Strikingly,
research also suggests that these gender gaps are visible even under blinded
review, wherein the evaluator is not aware of the gender of the author. In this
article, we consider whether gender differences in writing styles -- how men
and women communicate their work -- may contribute to these observed gender
gaps. We ground our investigation in a previously established framework for
characterizing the linguistic style of written text, which distinguishes
between two sets of features -- informational (i.e., features that emphasize
facts) and involved (i.e., features that emphasize relationships). Using a
large, matched sample of academic papers and patents, we find significant
differences in writing style by gender; women use more involved features in
their writing, a pattern that holds universally across fields. The magnitude of
the effect varies across fields, with larger gender differences observed in the
social sciences and arts humanities and smaller gaps in the physical sciences
and technology. Subsequently, we show that gender differences in writing style
may have parallels in reading preferences; papers and patents with more
informational features tend to be cited more by men, while those with more
involved features tend to be cited more by women, even after controlling for
the gender of the author, inventor, and patent attorney. Our findings suggest
that formal written text is not devoid of personal character, which could
contribute to bias in evaluation, thereby compromising the norm of
universalism.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Repro: An Open-Source Library for Improving the Reproducibility and Usability of Publicly Available Research Code. (arXiv:2204.13848v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13848">
<div class="article-summary-box-inner">
<span><p>We introduce Repro, an open-source library which aims at improving the
reproducibility and usability of research code. The library provides a
lightweight Python API for running software released by researchers within
Docker containers which contain the exact required runtime configuration and
dependencies for the code. Because the environment setup for each package is
handled by Docker, users do not have to do any configuration themselves. Once
Repro is installed, users can run the code for the 30+ papers currently
supported by the library. We hope researchers see the value provided to others
by including their research code in Repro and consider adding support for their
own research code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Textual Adversarial Examples Based on Distributional Characteristics of Data Representations. (arXiv:2204.13853v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13853">
<div class="article-summary-box-inner">
<span><p>Although deep neural networks have achieved state-of-the-art performance in
various machine learning tasks, adversarial examples, constructed by adding
small non-random perturbations to correctly classified inputs, successfully
fool highly expressive deep classifiers into incorrect predictions. Approaches
to adversarial attacks in natural language tasks have boomed in the last five
years using character-level, word-level, phrase-level, or sentence-level
textual perturbations. While there is some work in NLP on defending against
such attacks through proactive methods, like adversarial training, there is to
our knowledge no effective general reactive approaches to defence via detection
of textual adversarial examples such as is found in the image processing
literature. In this paper, we propose two new reactive methods for NLP to fill
this gap, which unlike the few limited application baselines from NLP are based
entirely on distribution characteristics of learned representations: we adapt
one from the image processing literature (Local Intrinsic Dimensionality
(LID)), and propose a novel one (MultiDistance Representation Ensemble Method
(MDRE)). Adapted LID and MDRE obtain state-of-the-art results on
character-level, word-level, and phrase-level attacks on the IMDB dataset as
well as on the later two with respect to the MultiNLI dataset. For future
research, we publish our code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-Language Pre-Training for Boosting Scene Text Detectors. (arXiv:2204.13867v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13867">
<div class="article-summary-box-inner">
<span><p>Recently, vision-language joint representation learning has proven to be
highly effective in various scenarios. In this paper, we specifically adapt
vision-language joint learning for scene text detection, a task that
intrinsically involves cross-modal interaction between the two modalities:
vision and language, since text is the written form of language. Concretely, we
propose to learn contextualized, joint representations through vision-language
pre-training, for the sake of enhancing the performance of scene text
detectors. Towards this end, we devise a pre-training architecture with an
image encoder, a text encoder and a cross-modal encoder, as well as three
pretext tasks: image-text contrastive learning (ITC), masked language modeling
(MLM) and word-in-image prediction (WIP). The pre-trained model is able to
produce more informative representations with richer semantics, which could
readily benefit existing scene text detectors (such as EAST and PSENet) in the
down-stream text detection task. Extensive experiments on standard benchmarks
demonstrate that the proposed paradigm can significantly improve the
performance of various representative text detectors, outperforming previous
pre-training approaches. The code and pre-trained models will be publicly
released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Por Qu\'e N\~ao Utiliser Alla Spr{\aa}k? Mixed Training with Gradient Optimization in Few-Shot Cross-Lingual Transfer. (arXiv:2204.13869v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13869">
<div class="article-summary-box-inner">
<span><p>The current state-of-the-art for few-shot cross-lingual transfer learning
first trains on abundant labeled data in the source language and then
fine-tunes with a few examples on the target language, termed target-adapting.
Though this has been demonstrated to work on a variety of tasks, in this paper
we show some deficiencies of this approach and propose a one-step mixed
training method that trains on both source and target data with
\textit{stochastic gradient surgery}, a novel gradient-level optimization.
Unlike the previous studies that focus on one language at a time when
target-adapting, we use one model to handle all target languages simultaneously
to avoid excessively language-specific models. Moreover, we discuss the
unreality of utilizing large target development sets for model selection in
previous literature. We further show that our method is both development-free
for target languages, and is also able to escape from overfitting issues. We
conduct a large-scale experiment on 4 diverse NLP tasks across up to 48
languages. Our proposed method achieves state-of-the-art performance on all
tasks and outperforms target-adapting by a large margin, especially for
languages that are linguistically distant from the source language, e.g., 7.36%
F1 absolute gain on average for the NER task, up to 17.60% on Punjabi.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OA-Mine: Open-World Attribute Mining for E-Commerce Products with Weak Supervision. (arXiv:2204.13874v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13874">
<div class="article-summary-box-inner">
<span><p>Automatic extraction of product attributes from their textual descriptions is
essential for online shopper experience. One inherent challenge of this task is
the emerging nature of e-commerce products -- we see new types of products with
their unique set of new attributes constantly. Most prior works on this matter
mine new values for a set of known attributes but cannot handle new attributes
that arose from constantly changing data. In this work, we study the attribute
mining problem in an open-world setting to extract novel attributes and their
values. Instead of providing comprehensive training data, the user only needs
to provide a few examples for a few known attribute types as weak supervision.
We propose a principled framework that first generates attribute value
candidates and then groups them into clusters of attributes. The candidate
generation step probes a pre-trained language model to extract phrases from
product titles. Then, an attribute-aware fine-tuning method optimizes a
multitask objective and shapes the language model representation to be
attribute-discriminative. Finally, we discover new attributes and values
through the self-ensemble of our framework, which handles the open-world
challenge. We run extensive experiments on a large distantly annotated
development set and a gold standard human-annotated test set that we collected.
Our model significantly outperforms strong baselines and can generalize to
unseen attributes and product types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leaner and Faster: Two-Stage Model Compression for Lightweight Text-Image Retrieval. (arXiv:2204.13913v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13913">
<div class="article-summary-box-inner">
<span><p>Current text-image approaches (e.g., CLIP) typically adopt dual-encoder
architecture using pre-trained vision-language representation. However, these
models still pose non-trivial memory requirements and substantial incremental
indexing time, which makes them less practical on mobile devices. In this
paper, we present an effective two-stage framework to compress large
pre-trained dual-encoder for lightweight text-image retrieval. The resulting
model is smaller (39% of the original), faster (1.6x/2.9x for processing
image/text respectively), yet performs on par with or better than the original
full model on Flickr30K and MSCOCO benchmarks. We also open-source an
accompanying realistic mobile image search application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Czech Dataset for Cross-lingual Subjectivity Classification. (arXiv:2204.13915v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13915">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a new Czech subjectivity dataset of 10k manually
annotated subjective and objective sentences from movie reviews and
descriptions. Our prime motivation is to provide a reliable dataset that can be
used with the existing English dataset as a benchmark to test the ability of
pre-trained multilingual models to transfer knowledge between Czech and English
and vice versa. Two annotators annotated the dataset reaching 0.83 of the
Cohen's \k{appa} inter-annotator agreement. To the best of our knowledge, this
is the first subjectivity dataset for the Czech language. We also created an
additional dataset that consists of 200k automatically labeled sentences. Both
datasets are freely available for research purposes. Furthermore, we fine-tune
five pre-trained BERT-like models to set a monolingual baseline for the new
dataset and we achieve 93.56% of accuracy. We fine-tune models on the existing
English dataset for which we obtained results that are on par with the current
state-of-the-art results. Finally, we perform zero-shot cross-lingual
subjectivity classification between Czech and English to verify the usability
of our dataset as the cross-lingual benchmark. We compare and discuss the
cross-lingual and monolingual results and the ability of multilingual models to
transfer knowledge between languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QRelScore: Better Evaluating Generated Questions with Deeper Understanding of Context-aware Relevance. (arXiv:2204.13921v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13921">
<div class="article-summary-box-inner">
<span><p>Existing metrics for assessing question generation not only require costly
human reference but also fail to take into account the input context of
generation, rendering the lack of deep understanding of the relevance between
the generated questions and input contexts. As a result, they may wrongly
penalize a legitimate and reasonable candidate question when it (i) involves
complicated reasoning with the context or (ii) can be grounded by multiple
evidences in the context. In this paper, we propose $\textbf{QRelScore}$, a
context-aware $\underline{\textbf{Rel}}$evance evaluation metric for
$\underline{\textbf{Q}}$uestion Generation. Based on off-the-shelf language
models such as BERT and GPT2, QRelScore employs both word-level hierarchical
matching and sentence-level prompt-based generation to cope with the
complicated reasoning and diverse generation from multiple evidences,
respectively. Compared with existing metrics, our experiments demonstrate that
QRelScore is able to achieve a higher correlation with human judgments while
being much more robust to adversarial samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KERMIT -- A Transformer-Based Approach for Knowledge Graph Matching. (arXiv:2204.13931v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13931">
<div class="article-summary-box-inner">
<span><p>One of the strongest signals for automated matching of knowledge graphs and
ontologies are textual concept descriptions. With the rise of transformer-based
language models, text comparison based on meaning (rather than lexical
features) is available to researchers. However, performing pairwise comparisons
of all textual descriptions of concepts in two knowledge graphs is expensive
and scales quadratically (or even worse if concepts have more than one
description). To overcome this problem, we follow a two-step approach: we first
generate matching candidates using a pre-trained sentence transformer (so
called bi-encoder). In a second step, we use fine-tuned transformer
cross-encoders to generate the best candidates. We evaluate our approach on
multiple datasets and show that it is feasible and produces competitive
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">User Experience Design for Automatic Credibility Assessment of News Content About COVID-19. (arXiv:2204.13943v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13943">
<div class="article-summary-box-inner">
<span><p>The increasingly rapid spread of information about COVID-19 on the web calls
for automatic measures of quality assurance. In that context, we check the
credibility of news content using selected linguistic features. We present two
empirical studies to evaluate the usability of graphical interfaces that offer
such credibility assessment. In a moderated qualitative interview with six
participants, we identify rating scale, sub-criteria and algorithm authorship
as important predictors of the usability. A subsequent quantitative online
survey with 50 participants reveals a conflict between transparency and
conciseness in the interface design, as well as a perceived hierarchy of
metadata: the authorship of a news text is more important than the authorship
of the credibility algorithm used to assess the content quality. Finally, we
make suggestions for future research, such as proactively documenting
credibility-related metadata for Natural Language Processing and Language
Technology services and establishing an explicit hierarchical taxonomy of
usability predictors for automatic credibility assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"My nose is running.""Are you also coughing?": Building A Medical Diagnosis Agent with Interpretable Inquiry Logics. (arXiv:2204.13953v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13953">
<div class="article-summary-box-inner">
<span><p>With the rise of telemedicine, the task of developing Dialogue Systems for
Medical Diagnosis (DSMD) has received much attention in recent years. Different
from early researches that needed to rely on extra human resources and
expertise to help construct the system, recent researches focused on how to
build DSMD in a purely data-driven manner. However, the previous data-driven
DSMD methods largely overlooked the system interpretability, which is critical
for a medical application, and they also suffered from the data sparsity issue
at the same time. In this paper, we explore how to bring interpretability to
data-driven DSMD. Specifically, we propose a more interpretable decision
process to implement the dialogue manager of DSMD by reasonably mimicking real
doctors' inquiry logics, and we devise a model with highly transparent
components to conduct the inference. Moreover, we collect a new DSMD dataset,
which has a much larger scale, more diverse patterns and is of higher quality
than the existing ones. The experiments show that our method obtains 7.7%,
10.0%, 3.0% absolute improvement in diagnosis accuracy respectively on three
datasets, demonstrating the effectiveness of its rational decision process and
model design. Our codes and the GMD-12 dataset are available at
https://github.com/lwgkzl/BR-Agent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PIE: a Parameter and Inference Efficient Solution for Large Scale Knowledge Graph Embedding Reasoning. (arXiv:2204.13957v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13957">
<div class="article-summary-box-inner">
<span><p>Knowledge graph (KG) embedding methods which map entities and relations to
unique embeddings in the KG have shown promising results on many reasoning
tasks. However, the same embedding dimension for both dense entities and sparse
entities will cause either over parameterization (sparse entities) or under
fitting (dense entities). Normally, a large dimension is set to get better
performance. Meanwhile, the inference time grows log-linearly with the number
of entities for all entities are traversed and compared. Both the parameter and
inference become challenges when working with huge amounts of entities. Thus,
we propose PIE, a \textbf{p}arameter and \textbf{i}nference \textbf{e}fficient
solution. Inspired from tensor decomposition methods, we find that decompose
entity embedding matrix into low rank matrices can reduce more than half of the
parameters while maintaining comparable performance. To accelerate model
inference, we propose a self-supervised auxiliary task, which can be seen as
fine-grained entity typing. By randomly masking and recovering entities'
connected relations, the task learns the co-occurrence of entity and relations.
Utilizing the fine grained typing, we can filter unrelated entities during
inference and get targets with possibly sub-linear time requirement.
Experiments on link prediction benchmarks demonstrate the proposed key
capabilities. Moreover, we prove effectiveness of the proposed solution on the
Open Graph Benchmark large scale challenge dataset WikiKG90Mv2 and achieve the
state of the art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making sense of violence risk predictions using clinical notes. (arXiv:2204.13976v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13976">
<div class="article-summary-box-inner">
<span><p>Violence risk assessment in psychiatric institutions enables interventions to
avoid violence incidents. Clinical notes written by practitioners and available
in electronic health records (EHR) are valuable resources that are seldom used
to their full potential. Previous studies have attempted to assess violence
risk in psychiatric patients using such notes, with acceptable performance.
However, they do not explain why classification works and how it can be
improved. We explore two methods to better understand the quality of a
classifier in the context of clinical note analysis: random forests using topic
models, and choice of evaluation metric. These methods allow us to understand
both our data and our methodology more profoundly, setting up the groundwork to
work on improved models that build upon this understanding. This is
particularly important when it comes to the generalizability of evaluated
classifiers to new data, a trustworthiness problem that is of great interest
due to the increased availability of new data in electronic format.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ExaASC: A General Target-Based Stance Detection Corpus in Arabic Language. (arXiv:2204.13979v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13979">
<div class="article-summary-box-inner">
<span><p>Target-based Stance Detection is the task of finding a stance toward a
target. Twitter is one of the primary sources of political discussions in
social media and one of the best resources to analyze Stance toward entities.
This work proposes a new method toward Target-based Stance detection by using
the stance of replies toward a most important and arguing target in source
tweet. This target is detected with respect to the source tweet itself and not
limited to a set of pre-defined targets which is the usual approach of the
current state-of-the-art methods. Our proposed new attitude resulted in a new
corpus called ExaASC for the Arabic Language, one of the low resource languages
in this field. In the end, we used BERT to evaluate our corpus and reached a
70.69 Macro F-score. This shows that our data and model can work in a general
Target-base Stance Detection system. The corpus is publicly available1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt Engineering for Text-Based Generative Art. (arXiv:2204.13988v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13988">
<div class="article-summary-box-inner">
<span><p>Text-based generative art has seen an explosion of interest in 2021. Online
communities around text-based generative art as a novel digital medium have
quickly emerged. This short paper identifies five types of prompt modifiers
used by practitioners in the community of text-based generative art based on a
3-month ethnographic study on Twitter. The novel taxonomy of prompt modifiers
provides researchers a conceptual starting point for investigating the
practices of text-based generative art, but also may help practitioners of
text-based generative art improve their images. The paper concludes with a
discussion of research opportunities in the space of text-based generative art
and the broader implications of prompt engineering from the perspective of
human-AI interaction in future applications beyond the use case of text-based
generative art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Backdoor Attacks in Federated Learning by Rare Embeddings and Gradient Ensembling. (arXiv:2204.14017v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14017">
<div class="article-summary-box-inner">
<span><p>Recent advances in federated learning have demonstrated its promising
capability to learn on decentralized datasets. However, a considerable amount
of work has raised concerns due to the potential risks of adversaries
participating in the framework to poison the global model for an adversarial
purpose. This paper investigates the feasibility of model poisoning for
backdoor attacks through \textit{rare word embeddings of NLP models} in text
classification and sequence-to-sequence tasks. In text classification, less
than 1\% of adversary clients suffices to manipulate the model output without
any drop in the performance of clean sentences. For a less complex dataset, a
mere 0.1\% of adversary clients is enough to poison the global model
effectively. We also propose a technique specialized in the federated learning
scheme called gradient ensemble, which enhances the backdoor performance in all
experimental settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot learning for medical text: A systematic review. (arXiv:2204.14081v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14081">
<div class="article-summary-box-inner">
<span><p>Objective: Few-shot learning (FSL) methods require small numbers of labeled
instances for training. As many medical topics have limited annotated textual
data in practical settings, FSL-based natural language processing (NLP) methods
hold substantial promise. We aimed to conduct a systematic review to explore
the state of FSL methods for medical NLP. Materials and Methods: We searched
for articles published between January 2016 and August 2021 using
PubMed/Medline, Embase, ACL Anthology, and IEEE Xplore Digital Library. To
identify the latest relevant methods, we also searched other sources such as
preprint servers (eg., medRxiv) via Google Scholar. We included all articles
that involved FSL and any type of medical text. We abstracted articles based on
data source(s), aim(s), training set size(s), primary method(s)/approach(es),
and evaluation method(s). Results: 31 studies met our inclusion criteria-all
published after 2018; 22 (71%) since 2020. Concept extraction/named entity
recognition was the most frequently addressed task (13/31; 42%), followed by
text classification (10/31; 32%). Twenty-one (68%) studies reconstructed
existing datasets to create few-shot scenarios synthetically, and MIMIC-III was
the most frequently used dataset (7/31; 23%). Common methods included FSL with
attention mechanisms (12/31; 39%), prototypical networks (8/31; 26%), and
meta-learning (6/31; 19%). Discussion: Despite the potential for FSL in
biomedical NLP, progress has been limited compared to domain-independent FSL.
This may be due to the paucity of standardized, public datasets, and the
relative underperformance of FSL methods on biomedical topics. Creation and
release of specialized datasets for biomedical FSL may aid method development
by enabling comparative analyses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Climate and Weather: Inspecting Depression Detection via Emotion Recognition. (arXiv:2204.14099v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14099">
<div class="article-summary-box-inner">
<span><p>Automatic depression detection has attracted increasing amount of attention
but remains a challenging task. Psychological research suggests that depressive
mood is closely related with emotion expression and perception, which motivates
the investigation of whether knowledge of emotion recognition can be
transferred for depression detection. This paper uses pretrained features
extracted from the emotion recognition model for depression detection, further
fuses emotion modality with audio and text to form multimodal depression
detection. The proposed emotion transfer improves depression detection
performance on DAIC-WOZ as well as increases the training stability. The
analysis of how the emotion expressed by depressed individuals is further
perceived provides clues for further understanding of the relationship between
depression and emotion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TEMOS: Generating diverse human motions from textual descriptions. (arXiv:2204.14109v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14109">
<div class="article-summary-box-inner">
<span><p>We address the problem of generating diverse 3D human motions from textual
descriptions. This challenging task requires joint modeling of both modalities:
understanding and extracting useful human-centric information from the text,
and then generating plausible and realistic sequences of human poses. In
contrast to most previous work which focuses on generating a single,
deterministic, motion from a textual description, we design a variational
approach that can produce multiple diverse human motions. We propose TEMOS, a
text-conditioned generative model leveraging variational autoencoder (VAE)
training with human motion data, in combination with a text encoder that
produces distribution parameters compatible with the VAE latent space. We show
that TEMOS framework can produce both skeleton-based animations as in prior
work, as well more expressive SMPL body motions. We evaluate our approach on
the KIT Motion-Language benchmark and, despite being relatively
straightforward, demonstrate significant improvements over the state of the
art. Code and models are available on our project page.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Developmental Negation Processing in Transformer Language Models. (arXiv:2204.14114v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14114">
<div class="article-summary-box-inner">
<span><p>Reasoning using negation is known to be difficult for transformer-based
language models. While previous studies have used the tools of
psycholinguistics to probe a transformer's ability to reason over negation,
none have focused on the types of negation studied in developmental psychology.
We explore how well transformers can process such categories of negation, by
framing the problem as a natural language inference (NLI) task. We curate a set
of diagnostic questions for our target categories from popular NLI datasets and
evaluate how well a suite of models reason over them. We find that models
perform consistently better only on certain categories, suggesting clear
distinctions in how they are processed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Natural Language Feedback. (arXiv:2204.14146v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14146">
<div class="article-summary-box-inner">
<span><p>Pretrained language models often do not perform tasks in ways that are in
line with our preferences, e.g., generating offensive text or factually
incorrect summaries. Recent work approaches the above issue by learning from a
simple form of human evaluation: comparisons between pairs of model-generated
task outputs. Comparison feedback conveys limited information about human
preferences per human evaluation. Here, we propose to learn from natural
language feedback, which conveys more information per human evaluation. We
learn from language feedback on model outputs using a three-step learning
algorithm. First, we condition the language model on the initial output and
feedback to generate many refinements. Second, we choose the refinement with
the highest similarity to the feedback. Third, we finetune a language model to
maximize the likelihood of the chosen refinement given the input. In synthetic
experiments, we first evaluate whether language models accurately incorporate
feedback to produce refinements, finding that only large language models (175B
parameters) do so. Using only 100 samples of human-written feedback, our
learning algorithm finetunes a GPT-3 model to roughly human-level
summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPERA:Operation-Pivoted Discrete Reasoning over Text. (arXiv:2204.14166v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14166">
<div class="article-summary-box-inner">
<span><p>Machine reading comprehension (MRC) that requires discrete reasoning
involving symbolic operations, e.g., addition, sorting, and counting, is a
challenging task. According to this nature, semantic parsing-based methods
predict interpretable but complex logical forms. However, logical form
generation is nontrivial and even a little perturbation in a logical form will
lead to wrong answers. To alleviate this issue, multi-predictor -based methods
are proposed to directly predict different types of answers and achieve
improvements. However, they ignore the utilization of symbolic operations and
encounter a lack of reasoning ability and interpretability. To inherit the
advantages of these two types of methods, we propose OPERA, an
operation-pivoted discrete reasoning framework, where lightweight symbolic
operations (compared with logical forms) as neural modules are utilized to
facilitate the reasoning ability and interpretability. Specifically, operations
are first selected and then softly executed to simulate the answer reasoning
procedure. Extensive experiments on both DROP and RACENum datasets show the
reasoning ability of OPERA. Moreover, further analysis verifies its
interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models. (arXiv:2204.14211v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14211">
<div class="article-summary-box-inner">
<span><p>Language Models (LMs) become outdated as the world changes; they often fail
to perform tasks requiring recent factual information which was absent or
different during training, a phenomenon called temporal misalignment. This is
especially a challenging problem because the research community still lacks a
coherent dataset for assessing the adaptability of LMs to frequently-updated
knowledge corpus such as Wikipedia. To this end, we introduce TemporalWiki, a
lifelong benchmark for ever-evolving LMs that utilizes the difference between
consecutive snapshots of English Wikipedia and English Wikidata for training
and evaluation, respectively. The benchmark hence allows researchers to
periodically track an LM's ability to retain previous knowledge and acquire
updated/new knowledge at each point in time. We also find that training an LM
on the diff data through continual learning methods achieves similar or better
perplexity than on the entire snapshot in our benchmark with 12 times less
computational cost, which verifies that factual knowledge in LMs can be safely
updated with minimal training data via continual learning. The dataset and the
code are available at https://github.com/joeljang/temporalwiki .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modular Domain Adaptation. (arXiv:2204.14213v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14213">
<div class="article-summary-box-inner">
<span><p>Off-the-shelf models are widely used by computational social science
researchers to measure properties of text, such as sentiment. However, without
access to source data it is difficult to account for domain shift, which
represents a threat to validity. Here, we treat domain adaptation as a modular
process that involves separate model producers and model consumers, and show
how they can independently cooperate to facilitate more accurate measurements
of text. We introduce two lightweight techniques for this scenario, and
demonstrate that they reliably increase out-of-domain accuracy on four
multi-domain text classification datasets when used with linear and contextual
embedding models. We conclude with recommendations for model producers and
consumers, and release models and replication code to accompany this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Naturalized Semantic Parsers with Very Little Data. (arXiv:2204.14243v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14243">
<div class="article-summary-box-inner">
<span><p>Semantic parsing is an important NLP problem, particularly for voice
assistants such as Alexa and Google Assistant. State-of-the-art (SOTA) semantic
parsers are seq2seq architectures based on large language models that have been
pretrained on vast amounts of text. To better leverage that pretraining, recent
work has explored a reformulation of semantic parsing whereby the output
sequences are themselves natural language sentences, but in a controlled
fragment of natural language. This approach delivers strong results,
particularly for few-shot semantic parsing, which is of key importance in
practice and the focus of our paper. We push this line of work forward by
introducing an automated methodology that delivers very significant additional
improvements by utilizing modest amounts of unannotated data, which is
typically easy to obtain. Our method is based on a novel synthesis of four
techniques: joint training with auxiliary unsupervised tasks; constrained
decoding; self-training; and paraphrasing. We show that this method delivers
new SOTA few-shot performance on the Overnight dataset, particularly in very
low-resource settings, and very compelling few-shot results on a new semantic
parsing dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Handling and Presenting Harmful Text. (arXiv:2204.14256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14256">
<div class="article-summary-box-inner">
<span><p>Textual data can pose a risk of serious harm. These harms can be categorised
along three axes: (1) the harm type (e.g. misinformation, hate speech or racial
stereotypes) (2) whether it is \textit{elicited} as a feature of the research
design from directly studying harmful content (e.g. training a hate speech
classifier or auditing unfiltered large-scale datasets) versus
\textit{spuriously} invoked from working on unrelated problems (e.g. language
generation or part of speech tagging) but with datasets that nonetheless
contain harmful content, and (3) who it affects, from the humans
(mis)represented in the data to those handling or labelling the data to readers
and reviewers of publications produced from the data. It is an unsolved problem
in NLP as to how textual harms should be handled, presented, and discussed;
but, stopping work on content which poses a risk of harm is untenable.
Accordingly, we provide practical advice and introduce \textsc{HarmCheck}, a
resource for reflecting on research into textual harms. We hope our work
encourages ethical, responsible, and respectful research in the NLP community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polyglot Prompt: Multilingual Multitask PrompTraining. (arXiv:2204.14264v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14264">
<div class="article-summary-box-inner">
<span><p>This paper aims for a potential architectural breakthrough for multilingual
learning and asks: could different tasks from different languages be modeled in
a monolithic framework (without any task/language-specific module)? The benefit
of achieving this is not only that systems trained on low resources scenario
can be assisted by more other languages and tasks, but opening new doors for
future multilingual research. We approach this goal by developing a learning
framework Polyglot Prompt, where prompting methods are introduced to learn a
unified semantic space for different languages and tasks after proper
multilingual prompt engineering. Experimentally, we perform a comprehensive
evaluation on 6 tasks (topic classification, sentiment classification, named
entity recognition, question answering, natural language inference,
summarization), 24 datasets, and 49 languages, which shows the efficacy of
multilingual multitask prompting training and suggests several interesting
observations. e.g., English prompts are polyglots since directly applying them
to task samples in other languages could result in a better improvement. We
also present an interpretable multilingual evaluation methodology and show how
the proposed framework, multilingual multitask prompt training, works. We
release all datasets prompted in the best setting and will release our code
soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?. (arXiv:2204.14268v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14268">
<div class="article-summary-box-inner">
<span><p>A multilingual tokenizer is a fundamental component of multilingual neural
machine translation. It is trained from a multilingual corpus. Since a skewed
data distribution is considered to be harmful, a sampling strategy is usually
used to balance languages in the corpus. However, few works have systematically
answered how language imbalance in tokenizer training affects downstream
performance. In this work, we analyze how translation performance changes as
the data ratios among languages vary in the tokenizer training corpus. We find
that while relatively better performance is often observed when languages are
more equally sampled, the downstream performance is more robust to language
imbalance than we usually expected. Two features, UNK rate and closeness to the
character level, can warn of poor downstream performance before performing the
task. We also distinguish language sampling for tokenizer training from
sampling for model training and show that the model is more sensitive to the
latter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Spoken Conversational Question Answering: Task, Dataset and Model. (arXiv:2204.14272v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14272">
<div class="article-summary-box-inner">
<span><p>In spoken question answering, the systems are designed to answer questions
from contiguous text spans within the related speech transcripts. However, the
most natural way that human seek or test their knowledge is via human
conversations. Therefore, we propose a new Spoken Conversational Question
Answering task (SCQA), aiming at enabling the systems to model complex dialogue
flows given the speech documents. In this task, our main objective is to build
the system to deal with conversational questions based on the audio recordings,
and to explore the plausibility of providing more cues from different
modalities with systems in information gathering. To this end, instead of
directly adopting automatically generated speech transcripts with highly noisy
data, we propose a novel unified data distillation approach, DDNet, which
effectively ingests cross-modal information to achieve fine-grained
representations of the speech and language modalities. Moreover, we propose a
simple and novel mechanism, termed Dual Attention, by encouraging better
alignments between audio and text to ease the process of knowledge transfer. To
evaluate the capacity of SCQA systems in a dialogue-style interaction, we
assemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with
more than 40k question-answer pairs from 4k conversations. The performance of
the existing state-of-the-art methods significantly degrade on our dataset,
hence demonstrating the necessity of cross-modal information integration. Our
experimental results demonstrate that our proposed method achieves superior
performance in spoken conversational question answering tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Makes a Good and Useful Summary? Incorporating Users in Automatic Summarization Research. (arXiv:2012.07619v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.07619">
<div class="article-summary-box-inner">
<span><p>Automatic text summarization has enjoyed great progress over the years and is
used in numerous applications, impacting the lives of many. Despite this
development, there is little research that meaningfully investigates how the
current research focus in automatic summarization aligns with users' needs. To
bridge this gap, we propose a survey methodology that can be used to
investigate the needs of users of automatically generated summaries.
Importantly, these needs are dependent on the target group. Hence, we design
our survey in such a way that it can be easily adjusted to investigate
different user groups. In this work we focus on university students, who make
extensive use of summaries during their studies. We find that the current
research directions of the automatic summarization community do not fully align
with students' needs. Motivated by our findings, we present ways to mitigate
this mismatch in future research on automatic summarization: we propose
research directions that impact the design, the development and the evaluation
of automatically generated summaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Machine-Paraphrased Plagiarism. (arXiv:2103.11909v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11909">
<div class="article-summary-box-inner">
<span><p>Employing paraphrasing tools to conceal plagiarized text is a severe threat
to academic integrity. To enable the detection of machine-paraphrased text, we
evaluate the effectiveness of five pre-trained word embedding models combined
with machine learning classifiers and state-of-the-art neural language models.
We analyze preprints of research papers, graduation theses, and Wikipedia
articles, which we paraphrased using different configurations of the tools
SpinBot and SpinnerChief. The best performing technique, Longformer, achieved
an average F1 score of 80.99% (F1=99.68% for SpinBot and F1=71.64% for
SpinnerChief cases), while human evaluators achieved F1=78.4% for SpinBot and
F1=65.6% for SpinnerChief cases. We show that the automated classification
alleviates shortcomings of widely-used text-matching systems, such as Turnitin
and PlagScan. To facilitate future research, all data, code, and two web
applications showcasing our contributions are openly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Neural Language Models Good Plagiarists? A Benchmark for Neural Paraphrase Detection. (arXiv:2103.12450v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12450">
<div class="article-summary-box-inner">
<span><p>The rise of language models such as BERT allows for high-quality text
paraphrasing. This is a problem to academic integrity, as it is difficult to
differentiate between original and machine-generated content. We propose a
benchmark consisting of paraphrased articles using recent language models
relying on the Transformer architecture. Our contribution fosters future
research of paraphrase detection systems as it offers a large collection of
aligned original and paraphrased documents, a study regarding its structure,
classification experiments with state-of-the-art systems, and we make our
findings publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linguistic Dependencies and Statistical Dependence. (arXiv:2104.08685v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08685">
<div class="article-summary-box-inner">
<span><p>Are pairs of words that tend to occur together also likely to stand in a
linguistic dependency? This empirical question is motivated by a long history
of literature in cognitive science, psycholinguistics, and NLP. In this work we
contribute an extensive analysis of the relationship between linguistic
dependencies and statistical dependence between words. Improving on previous
work, we introduce the use of large pretrained language models to compute
contextualized estimates of the pointwise mutual information between words
(CPMI). For multiple models and languages, we extract dependency trees which
maximize CPMI, and compare to gold standard linguistic dependencies. Overall,
we find that CPMI dependencies achieve an unlabelled undirected attachment
score of at most $\approx 0.5$. While far above chance, and consistently above
a non-contextualized PMI baseline, this score is generally comparable to a
simple baseline formed by connecting adjacent words. We analyze which kinds of
linguistic dependencies are best captured in CPMI dependencies, and also find
marked differences between the estimates of the large pretrained language
models, illustrating how their different training schemes affect the type of
dependencies they capture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Post-hoc Interpretability for Neural NLP: A Survey. (arXiv:2108.04840v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04840">
<div class="article-summary-box-inner">
<span><p>Neural networks for NLP are becoming increasingly complex and widespread, and
there is a growing concern if these models are responsible to use. Explaining
models helps to address the safety and ethical concerns and is essential for
accountability. Interpretability serves to provide these explanations in terms
that are understandable to humans. Additionally, post-hoc methods provide
explanations after a model is learned and are generally model-agnostic. This
survey provides a categorization of how recent post-hoc interpretability
methods communicate explanations to humans, it discusses each method in-depth,
and how they are validated, as the latter is often a common concern.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsolved Problems in ML Safety. (arXiv:2109.13916v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13916">
<div class="article-summary-box-inner">
<span><p>Machine learning (ML) systems are rapidly increasing in size, are acquiring
new capabilities, and are increasingly deployed in high-stakes settings. As
with other powerful technologies, safety for ML should be a leading research
priority. In response to emerging safety challenges in ML, such as those
introduced by recent large-scale models, we provide a new roadmap for ML Safety
and refine the technical problems that the field needs to address. We present
four problems ready for research, namely withstanding hazards ("Robustness"),
identifying hazards ("Monitoring"), reducing inherent model hazards
("Alignment"), and reducing systemic hazards ("Systemic Safety"). Throughout,
we clarify each problem's motivation and provide concrete research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Impact of Temporal Representations on Metaphor Detection. (arXiv:2111.03320v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.03320">
<div class="article-summary-box-inner">
<span><p>State-of-the-art approaches for metaphor detection compare their literal - or
core - meaning and their contextual meaning using metaphor classifiers based on
neural networks. However, metaphorical expressions evolve over time due to
various reasons, such as cultural and societal impact. Metaphorical expressions
are known to co-evolve with language and literal word meanings, and even drive,
to some extent, this evolution. This poses the question of whether different,
possibly time-specific, representations of literal meanings may impact the
metaphor detection task. To the best of our knowledge, this is the first study
that examines the metaphor detection task with a detailed exploratory analysis
where different temporal and static word embeddings are used to account for
different representations of literal meanings. Our experimental analysis is
based on three popular benchmarks used for metaphor detection and word
embeddings extracted from different corpora and temporally aligned using
different state-of-the-art approaches. The results suggest that the usage of
different static word embedding methods does impact the metaphor detection task
and some temporal word embeddings slightly outperform static methods. However,
the results also suggest that temporal word embeddings may provide
representations of the core meaning of the metaphor even too close to their
contextual meaning, thus confusing the classifier. Overall, the interaction
between temporal language evolution and metaphor detection appears tiny in the
benchmark datasets used in our experiments. This suggests that future work for
the computational analysis of this important linguistic phenomenon should first
start by creating a new dataset where this interaction is better represented.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AnswerSumm: A Manually-Curated Dataset and Pipeline for Answer Summarization. (arXiv:2111.06474v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06474">
<div class="article-summary-box-inner">
<span><p>Community Question Answering (CQA) fora such as Stack Overflow and Yahoo!
Answers contain a rich resource of answers to a wide range of community-based
questions. Each question thread can receive a large number of answers with
different perspectives. One goal of answer summarization is to produce a
summary that reflects the range of answer perspectives. A major obstacle for
this task is the absence of a dataset to provide supervision for producing such
summaries. Recent works propose heuristics to create such data, but these are
often noisy and do not cover all answer perspectives present. This work
introduces a novel dataset of 4,631 CQA threads for answer summarization
curated by professional linguists. Our pipeline gathers annotations for all
subtasks of answer summarization, including relevant answer sentence selection,
grouping these sentences based on perspectives, summarizing each perspective,
and producing an overall summary. We analyze and benchmark state-of-the-art
models on these subtasks and introduce a novel unsupervised approach for
multi-perspective data augmentation that boosts summarization performance
according to automatic evaluation. Finally, we propose reinforcement learning
rewards to improve factual consistency and answer coverage and analyze areas
for improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Testing the Generalization of Neural Language Models for COVID-19 Misinformation Detection. (arXiv:2111.07819v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07819">
<div class="article-summary-box-inner">
<span><p>A drastic rise in potentially life-threatening misinformation has been a
by-product of the COVID-19 pandemic. Computational support to identify false
information within the massive body of data on the topic is crucial to prevent
harm. Researchers proposed many methods for flagging online misinformation
related to COVID-19. However, these methods predominantly target specific
content types (e.g., news) or platforms (e.g., Twitter). The methods'
capabilities to generalize were largely unclear so far. We evaluate fifteen
Transformer-based models on five COVID-19 misinformation datasets that include
social media posts, news articles, and scientific papers to fill this gap. We
show tokenizers and models tailored to COVID-19 data do not provide a
significant advantage over general-purpose ones. Our study provides a realistic
assessment of models for detecting COVID-19 misinformation. We expect that
evaluating a broad spectrum of datasets and models will benefit future research
in developing misinformation detection systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ITA: Image-Text Alignments for Multi-Modal Named Entity Recognition. (arXiv:2112.06482v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06482">
<div class="article-summary-box-inner">
<span><p>Recently, Multi-modal Named Entity Recognition (MNER) has attracted a lot of
attention. Most of the work utilizes image information through region-level
visual representations obtained from a pretrained object detector and relies on
an attention mechanism to model the interactions between image and text
representations. However, it is difficult to model such interactions as image
and text representations are trained separately on the data of their respective
modality and are not aligned in the same space. As text representations take
the most important role in MNER, in this paper, we propose {\bf I}mage-{\bf
t}ext {\bf A}lignments (ITA) to align image features into the textual space, so
that the attention mechanism in transformer-based pretrained textual embeddings
can be better utilized. ITA first aligns the image into regional object tags,
image-level captions and optical characters as visual contexts, concatenates
them with the input texts as a new cross-modal input, and then feeds it into a
pretrained textual embedding model. This makes it easier for the attention
module of a pretrained textual embedding model to model the interaction between
the two modalities since they are both represented in the textual space. ITA
further aligns the output distributions predicted from the cross-modal input
and textual input views so that the MNER model can be more practical in dealing
with text-only inputs and robust to noises from images. In our experiments, we
show that ITA models can achieve state-of-the-art accuracy on multi-modal Named
Entity Recognition datasets, even without image information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks. (arXiv:2112.07475v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07475">
<div class="article-summary-box-inner">
<span><p>Labelled data is the foundation of most natural language processing tasks.
However, labelling data is difficult and there often are diverse valid beliefs
about what the correct data labels should be. So far, dataset creators have
acknowledged annotator subjectivity, but rarely actively managed it in the
annotation process. This has led to partly-subjective datasets that fail to
serve a clear downstream use. To address this issue, we propose two contrasting
paradigms for data annotation. The descriptive paradigm encourages annotator
subjectivity, whereas the prescriptive paradigm discourages it. Descriptive
annotation allows for the surveying and modelling of different beliefs, whereas
prescriptive annotation enables the training of models that consistently apply
one belief. We discuss benefits and challenges in implementing both paradigms,
and argue that dataset creators should explicitly aim for one or the other to
facilitate the intended use of their dataset. Lastly, we conduct an annotation
experiment using hate speech data that illustrates the contrast between the two
paradigms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization. (arXiv:2112.08542v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08542">
<div class="article-summary-box-inner">
<span><p>Factual consistency is an essential quality of text summarization models in
practical settings. Existing work in evaluating this dimension can be broadly
categorized into two lines of research, entailment-based and question answering
(QA)-based metrics, and different experimental setups often lead to contrasting
conclusions as to which paradigm performs the best. In this work, we conduct an
extensive comparison of entailment and QA-based metrics, demonstrating that
carefully choosing the components of a QA-based metric, especially question
generation and answerability classification, is critical to performance.
Building on those insights, we propose an optimized metric, which we call
QAFactEval, that leads to a 14% average improvement over previous QA-based
metrics on the SummaC factual consistency benchmark, and also outperforms the
best-performing entailment-based metric. Moreover, we find that QA-based and
entailment-based metrics can offer complementary signals and be combined into a
single metric for a further performance boost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing the Limits of Self-Supervision in Handling Bias in Language. (arXiv:2112.08637v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08637">
<div class="article-summary-box-inner">
<span><p>Prompting inputs with natural language task descriptions has emerged as a
popular mechanism to elicit reasonably accurate outputs from large-scale
generative language models with little to no in-context supervision. This also
helps gain insight into how well language models capture the semantics of a
wide range of downstream tasks purely from self-supervised pre-training on
massive corpora of unlabeled text. Such models have naturally also been exposed
to a lot of undesirable content like racist and sexist language and there is
limited work on awareness of models along these dimensions. In this paper, we
define and comprehensively evaluate how well such language models capture the
semantics of four tasks for bias: diagnosis, identification, extraction and
rephrasing. We define three broad classes of task descriptions for these tasks:
statement, question, and completion, with numerous lexical variants within each
class. We study the efficacy of prompting for each task using these classes and
the null task description across several decoding methods and few-shot
examples. Our analyses indicate that language models are capable of performing
these tasks to widely varying degrees across different bias dimensions, such as
gender and political affiliation. We believe our work is an important step
towards unbiased language models by quantifying the limits of current
self-supervision objectives at accomplishing such sociologically challenging
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of HTR models without Ground Truth Material. (arXiv:2201.06170v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06170">
<div class="article-summary-box-inner">
<span><p>The evaluation of Handwritten Text Recognition (HTR) models during their
development is straightforward: because HTR is a supervised problem, the usual
data split into training, validation, and test data sets allows the evaluation
of models in terms of accuracy or error rates. However, the evaluation process
becomes tricky as soon as we switch from development to application. A
compilation of a new (and forcibly smaller) ground truth (GT) from a sample of
the data that we want to apply the model on and the subsequent evaluation of
models thereon only provides hints about the quality of the recognised text, as
do confidence scores (if available) the models return. Moreover, if we have
several models at hand, we face a model selection problem since we want to
obtain the best possible result during the application phase. This calls for
GT-free metrics to select the best model, which is why we (re-)introduce and
compare different metrics, from simple, lexicon-based to more elaborate ones
using standard language models and masked language models (MLM). We show that
MLM-based evaluation can compete with lexicon-based methods, with the advantage
that large and multilingual transformers are readily available, thus making
compiling lexical resources for other metrics superfluous.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Table Pre-training: A Survey on Model Architectures, Pre-training Objectives, and Downstream Tasks. (arXiv:2201.09745v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09745">
<div class="article-summary-box-inner">
<span><p>Since a vast number of tables can be easily collected from web pages,
spreadsheets, PDFs, and various other document types, a flurry of table
pre-training frameworks have been proposed following the success of text and
images, and they have achieved new state-of-the-arts on various tasks such as
table question answering, table type recognition, column relation
classification, table search, formula prediction, etc. To fully use the
supervision signals in unlabeled tables, a variety of pre-training objectives
have been designed and evaluated, for example, denoising cell values,
predicting numerical relationships, and implicitly executing SQLs. And to best
leverage the characteristics of (semi-)structured tables, various tabular
language models, particularly with specially-designed attention mechanisms,
have been explored. Since tables usually appear and interact with free-form
text, table pre-training usually takes the form of table-text joint
pre-training, which attracts significant research interests from multiple
domains. This survey aims to provide a comprehensive review of different model
designs, pre-training objectives, and downstream tasks for table pre-training,
and we further share our thoughts and vision on existing challenges and future
opportunities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Broad Coverage Named Entity Resource: A Data-Efficient Approach for Many Diverse Languages. (arXiv:2201.12219v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12219">
<div class="article-summary-box-inner">
<span><p>Parallel corpora are ideal for extracting a multilingual named entity (MNE)
resource, i.e., a dataset of names translated into multiple languages. Prior
work on extracting MNE datasets from parallel corpora required resources such
as large monolingual corpora or word aligners that are unavailable or perform
poorly for underresourced languages. We present CLC-BN, a new method for
creating an MNE resource, and apply it to the Parallel Bible Corpus, a corpus
of more than 1000 languages. CLC-BN learns a neural transliteration model from
parallel-corpus statistics, without requiring any other bilingual resources,
word aligners, or seed data. Experimental results show that CLC-BN clearly
outperforms prior work. We release an MNE resource for 1340 languages and
demonstrate its effectiveness in two downstream tasks: knowledge graph
augmentation and bilingual lexicon induction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Benchmark Corpus for the Detection of Automatically Generated Text in Academic Publications. (arXiv:2202.02013v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02013">
<div class="article-summary-box-inner">
<span><p>Automatic text generation based on neural language models has achieved
performance levels that make the generated text almost indistinguishable from
those written by humans. Despite the value that text generation can have in
various applications, it can also be employed for malicious tasks. The
diffusion of such practices represent a threat to the quality of academic
publishing. To address these problems, we propose in this paper two datasets
comprised of artificially generated research content: a completely synthetic
dataset and a partial text substitution dataset. In the first case, the content
is completely generated by the GPT-2 model after a short prompt extracted from
original papers. The partial or hybrid dataset is created by replacing several
sentences of abstracts with sentences that are generated by the Arxiv-NLP
model. We evaluate the quality of the datasets comparing the generated texts to
aligned original texts using fluency metrics such as BLEU and ROUGE. The more
natural the artificial texts seem, the more difficult they are to detect and
the better is the benchmark. We also evaluate the difficulty of the task of
distinguishing original from generated text by using state-of-the-art
classification models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Generalizable Semantic Product Search by Text Similarity Pre-training on Search Click Logs. (arXiv:2204.05231v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05231">
<div class="article-summary-box-inner">
<span><p>Recently, semantic search has been successfully applied to e-commerce product
search and the learned semantic space(s) for query and product encoding are
expected to generalize to unseen queries or products. Yet, whether
generalization can conveniently emerge has not been thoroughly studied in the
domain thus far. In this paper, we examine several general-domain and
domain-specific pre-trained Roberta variants and discover that general-domain
fine-tuning does not help generalization, which aligns with the discovery of
prior art. Proper domain-specific fine-tuning with clickstream data can lead to
better model generalization, based on a bucketed analysis of a publicly
available manual annotated query-product pair da
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beam Decoding with Controlled Patience. (arXiv:2204.05424v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05424">
<div class="article-summary-box-inner">
<span><p>Text generation with beam search has proven successful in a wide range of
applications. The commonly-used implementation of beam decoding follows a first
come, first served heuristic: it keeps a set of already completed sequences
over time steps and stops when the size of this set reaches the beam size. We
introduce a patience factor, a simple modification to this decoding algorithm,
that generalizes the stopping criterion and provides flexibility to the depth
of search. Extensive empirical results demonstrate that the patience factor
improves decoding performance of strong pretrained models on news text
summarization and machine translation over diverse language pairs, with a
negligible inference slowdown. Our approach only modifies one line of code and
can be thus readily incorporated in any implementation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding. (arXiv:2204.07316v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07316">
<div class="article-summary-box-inner">
<span><p>Transformer-based models are widely used in natural language understanding
(NLU) tasks, and multimodal transformers have been effective in visual-language
tasks. This study explores distilling visual information from pretrained
multimodal transformers to pretrained language encoders. Our framework is
inspired by cross-modal encoders' success in visual-language tasks while we
alter the learning objective to cater to the language-heavy characteristics of
NLU. After training with a small number of extra adapting steps and finetuned,
the proposed XDBERT (cross-modal distilled BERT) outperforms pretrained-BERT in
general language understanding evaluation (GLUE), situations with adversarial
generations (SWAG) benchmarks, and readability benchmarks. We analyze the
performance of XDBERT on GLUE to show that the improvement is likely visually
grounded.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation. (arXiv:2204.07675v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07675">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have demonstrated superior performance in various
natural language processing tasks. However, these models usually contain
hundreds of millions of parameters, which limits their practicality because of
latency requirements in real-world applications. Existing methods train small
compressed models via knowledge distillation. However, performance of these
small models drops significantly compared with the pre-trained models due to
their reduced model capacity. We propose MoEBERT, which uses a
Mixture-of-Experts structure to increase model capacity and inference speed. We
initialize MoEBERT by adapting the feed-forward neural networks in a
pre-trained model into multiple experts. As such, representation power of the
pre-trained model is largely retained. During inference, only one of the
experts is activated, such that speed can be improved. We also propose a
layer-wise distillation method to train MoEBERT. We validate the efficiency and
effectiveness of MoEBERT on natural language understanding and question
answering tasks. Results show that the proposed method outperforms existing
task-specific distillation algorithms. For example, our method outperforms
previous approaches by over 2% on the MNLI (mismatched) dataset. Our code is
publicly available at https://github.com/SimiaoZuo/MoEBERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WikiOmnia: generative QA corpus on the whole Russian Wikipedia. (arXiv:2204.08009v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08009">
<div class="article-summary-box-inner">
<span><p>The General QA field has been developing the methodology referencing the
Stanford Question answering dataset (SQuAD) as the significant benchmark.
However, compiling factual questions is accompanied by time- and
labour-consuming annotation, limiting the training data's potential size. We
present the WikiOmnia dataset, a new publicly available set of QA-pairs and
corresponding Russian Wikipedia article summary sections, composed with a fully
automated generative pipeline. The dataset includes every available article
from Wikipedia for the Russian language. The WikiOmnia pipeline is available
open-source and is also tested for creating SQuAD-formatted QA on other
domains, like news texts, fiction, and social media. The resulting dataset
includes two parts: raw data on the whole Russian Wikipedia (7,930,873 QA pairs
with paragraphs for ruGPT-3 XL and 7,991,040 QA pairs with paragraphs for
ruT5-large) and cleaned data with strict automatic verification (over 160,000
QA pairs with paragraphs for ruGPT-3 XL and over 3,400,000 QA pairs with
paragraphs for ruT5-large).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation. (arXiv:2204.08401v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08401">
<div class="article-summary-box-inner">
<span><p>Knowledge graph embedding (KGE) aims to learn continuous vectors of relations
and entities in knowledge graph. Recently, transition-based KGE methods have
achieved promising performance, where the single relation vector learns to
translate head entity to tail entity. However, this scoring pattern is not
suitable for complex scenarios where the same entity pair has different
relations. Previous models usually focus on the improvement of entity
representation for 1-to-N, N-to-1 and N-to-N relations, but ignore the single
relation vector. In this paper, we propose a novel transition-based method,
TranS, for knowledge graph embedding. The single relation vector in traditional
scoring patterns is replaced with synthetic relation representation, which can
solve these issues effectively and efficiently. Experiments on a large
knowledge graph dataset, ogbl-wikikg2, show that our model achieves
state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval Enhanced Data Augmentation for Question Answering on Privacy Policies. (arXiv:2204.08952v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08952">
<div class="article-summary-box-inner">
<span><p>Prior studies in privacy policies frame the question answering (QA) tasks as
identifying the most relevant text segment or a list of sentences from the
policy document for a user query. However, annotating such a dataset is
challenging as it requires specific domain expertise (e.g., law academics).
Even if we manage a small-scale one, a bottleneck that remains is that the
labeled data are heavily imbalanced (only a few segments are relevant)
--limiting the gain in this domain. Therefore, in this paper, we develop a
novel data augmentation framework based on ensembling retriever models that
captures the relevant text segments from unlabeled policy documents and expand
the positive examples in the training set. In addition, to improve the
diversity and quality of the augmented data, we leverage multiple pre-trained
language models (LMs) and cascaded them with noise reduction oracles. Using our
augmented data on the PrivacyQA benchmark, we elevate the existing baseline by
a large margin (10% F1) and achieve a new state-of-the-art F1 score of 50%. Our
ablation studies provide further insights into the effectiveness of our
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating User Radicalization: A Novel Dataset for Identifying Fine-Grained Temporal Shifts in Opinion. (arXiv:2204.10190v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10190">
<div class="article-summary-box-inner">
<span><p>There is an increasing need for the ability to model fine-grained opinion
shifts of social media users, as concerns about the potential polarizing social
effects increase. However, the lack of publicly available datasets that are
suitable for the task presents a major challenge. In this paper, we introduce
an innovative annotated dataset for modeling subtle opinion fluctuations and
detecting fine-grained stances. The dataset includes a sufficient amount of
stance polarity and intensity labels per user over time and within entire
conversational threads, thus making subtle opinion fluctuations detectable both
in long term and in short term. All posts are annotated by non-experts and a
significant portion of the data is also annotated by experts. We provide a
strategy for recruiting suitable non-experts. Our analysis of the
inter-annotator agreements shows that the resulting annotations obtained from
the majority vote of the non-experts are of comparable quality to the
annotations of the experts. We provide analyses of the stance evolution in
short term and long term levels, a comparison of language usage between users
with vacillating and resolute attitudes, and fine-grained stance detection
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MuCGEC: a Multi-Reference Multi-Source Evaluation Dataset for Chinese Grammatical Error Correction. (arXiv:2204.10994v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10994">
<div class="article-summary-box-inner">
<span><p>This paper presents MuCGEC, a multi-reference multi-source evaluation dataset
for Chinese Grammatical Error Correction (CGEC), consisting of 7,063 sentences
collected from three different Chinese-as-a-Second-Language (CSL) learner
sources. Each sentence has been corrected by three annotators, and their
corrections are meticulously reviewed by an expert, resulting in 2.3 references
per sentence. We conduct experiments with two mainstream CGEC models, i.e., the
sequence-to-sequence (Seq2Seq) model and the sequence-to-edit (Seq2Edit) model,
both enhanced with large pretrained language models (PLMs), achieving
competitive benchmark performance on previous and our datasets. We also discuss
CGEC evaluation methodologies, including the effect of multiple references and
using a char-based metric. Our annotation guidelines, data, and code are
available at \url{https://github.com/HillZhang1999/MuCGEC}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PLOD: An Abbreviation Detection Dataset for Scientific Documents. (arXiv:2204.12061v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12061">
<div class="article-summary-box-inner">
<span><p>The detection and extraction of abbreviations from unstructured texts can
help to improve the performance of Natural Language Processing tasks, such as
machine translation and information retrieval. However, in terms of publicly
available datasets, there is not enough data for training
deep-neural-networks-based models to the point of generalising well over data.
This paper presents PLOD, a large-scale dataset for abbreviation detection and
extraction that contains 160k+ segments automatically annotated with
abbreviations and their long forms. We performed manual validation over a set
of instances and a complete automatic validation for this dataset. We then used
it to generate several baseline models for detecting abbreviations and long
forms. The best models achieved an F1-score of 0.92 for abbreviations and 0.89
for detecting their corresponding long forms. We release this dataset along
with our code and all the models publicly in
https://github.com/surrey-nlp/PLOD-AbbreviationDetection
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Science Checker: Extractive-Boolean Question Answering For Scientific Fact Checking. (arXiv:2204.12263v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12263">
<div class="article-summary-box-inner">
<span><p>With the explosive growth of scientific publications, making the synthesis of
scientific knowledge and fact checking becomes an increasingly complex task. In
this paper, we propose a multi-task approach for verifying the scientific
questions based on a joint reasoning from facts and evidence in research
articles. We propose an intelligent combination of (1) an automatic information
summarization and (2) a Boolean Question Answering which allows to generate an
answer to a scientific question from only extracts obtained after
summarization. Thus on a given topic, our proposed approach conducts structured
content modeling based on paper abstracts to answer a scientific question while
highlighting texts from paper that discuss the topic. We based our final system
on an end-to-end Extractive Question Answering (EQA) combined with a three
outputs classification model to perform in-depth semantic understanding of a
question to illustrate the aggregation of multiple responses. With our light
and fast proposed architecture, we achieved an average error rate of 4% and a
F1-score of 95.6%. Our results are supported via experiments with two QA models
(BERT, RoBERTa) over 3 Million Open Access (OA) articles in the medical and
health domains on Europe PMC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Label Search for Zero-Shot Multi-Lingual Extractive Summarization. (arXiv:2204.13512v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13512">
<div class="article-summary-box-inner">
<span><p>In zero-shot multilingual extractive text summarization, a model is typically
trained on English summarization dataset and then applied on summarization
datasets of other languages. Given English gold summaries and documents,
sentence-level labels for extractive summarization are usually generated using
heuristics. However, these monolingual labels created on English datasets may
not be optimal on datasets of other languages, for that there is the syntactic
or semantic discrepancy between different languages. In this way, it is
possible to translate the English dataset to other languages and obtain
different sets of labels again using heuristics. To fully leverage the
information of these different sets of labels, we propose NLSSum (Neural Label
Search for Summarization), which jointly learns hierarchical weights for these
different sets of labels together with our summarization model. We conduct
multilingual zero-shot summarization experiments on MLSUM and WikiLingua
datasets, and we achieve state-of-the-art results using both human and
automatic evaluations across these two datasets.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Learning: Balancing the Thin Line Between Data Intelligence and Privacy. (arXiv:2204.13697v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13697">
<div class="article-summary-box-inner">
<span><p>Federated learning holds great promise in learning from fragmented sensitive
data and has revolutionized how machine learning models are trained. This
article provides a systematic overview and detailed taxonomy of federated
learning. We investigate the existing security challenges in federated learning
and provide a comprehensive overview of established defense techniques for data
poisoning, inference attacks, and model poisoning attacks. The work also
presents an overview of current training challenges for federated learning,
focusing on handling non-i.i.d. data, high dimensionality issues, and
heterogeneous architecture, and discusses several solutions for the associated
challenges. Finally, we discuss the remaining challenges in managing federated
learning training and suggest focused research directions to address the open
questions. Potential candidate areas for federated learning, including IoT
ecosystem, healthcare applications, are discussed with a particular focus on
banking and financial domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Channel Pruned YOLOv5-based Deep Learning Approach for Rapid and Accurate Outdoor Obstacles Detection. (arXiv:2204.13699v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13699">
<div class="article-summary-box-inner">
<span><p>One-stage algorithm have been widely used in target detection systems that
need to be trained with massive data. Most of them perform well both in
real-time and accuracy. However, due to their convolutional structure, they
need more computing power and greater memory consumption. Hence, we applied
pruning strategy to target detection networks to reduce the number of
parameters and the size of model. To demonstrate the practicality of the
pruning method, we select the YOLOv5 model for experiments and provide a data
set of outdoor obstacles to show the effect of model. In this specific data
set, in the best circumstances, the volume of the network model is reduced by
49.7% compared with the original model, and the reasoning time is reduced by
52.5%. Meanwhile, it also uses data processing methods to compensate for the
drop in accuracy caused by pruning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coupling Deep Imputation with Multitask Learning for Downstream Tasks on Genomics Data. (arXiv:2204.13705v1 [q-bio.GN])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13705">
<div class="article-summary-box-inner">
<span><p>Genomics data such as RNA gene expression, methylation and micro RNA
expression are valuable sources of information for various clinical predictive
tasks. For example, predicting survival outcomes, cancer histology type and
other patients' related information is possible using not only clinical data
but molecular data as well. Moreover, using these data sources together, for
example in multitask learning, can boost the performance. However, in practice,
there are many missing data points which leads to significantly lower patient
numbers when analysing full cases, which in our setting refers to all
modalities being present.
</p>
<p>In this paper we investigate how imputing data with missing values using deep
learning coupled with multitask learning can help to reach state-of-the-art
performance results using combined genomics modalities, RNA, micro RNA and
methylation. We propose a generalised deep imputation method to impute values
where a patient has all modalities present except one. Interestingly enough,
deep imputation alone outperforms multitask learning alone for the
classification and regression tasks across most combinations of modalities. In
contrast, when using all modalities for survival prediction we observe that
multitask learning alone outperforms deep imputation alone with statistical
significance (adjusted p-value 0.03). Thus, both approaches are complementary
when optimising performance for downstream predictive tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning cosmology and clustering with cosmic graphs. (arXiv:2204.13713v1 [astro-ph.CO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13713">
<div class="article-summary-box-inner">
<span><p>We train deep learning models on thousands of galaxy catalogues from the
state-of-the-art hydrodynamic simulations of the CAMELS project to perform
regression and inference. We employ Graph Neural Networks (GNNs), architectures
designed to work with irregular and sparse data, like the distribution of
galaxies in the Universe. We first show that GNNs can learn to compute the
power spectrum of galaxy catalogues with a few percent accuracy. We then train
GNNs to perform likelihood-free inference at the galaxy-field level. Our models
are able to infer the value of $\Omega_{\rm m}$ with a $\sim12\%-13\%$ accuracy
just from the positions of $\sim1000$ galaxies in a volume of $(25~h^{-1}{\rm
Mpc})^3$ at $z=0$ while accounting for astrophysical uncertainties as modelled
in CAMELS. Incorporating information from galaxy properties, such as stellar
mass, stellar metallicity, and stellar radius, increases the accuracy to
$4\%-8\%$. Our models are built to be translational and rotational invariant,
and they can extract information from any scale larger than the minimum
distance between two galaxies. However, our models are not completely robust:
testing on simulations run with a different subgrid physics than the ones used
for training does not yield as accurate results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Model to Synthesize Them All: Multi-contrast Multi-scale Transformer for Missing Data Imputation. (arXiv:2204.13738v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13738">
<div class="article-summary-box-inner">
<span><p>Multi-contrast magnetic resonance imaging (MRI) is widely used in clinical
practice as each contrast provides complementary information. However, the
availability of each contrast may vary amongst patients in reality. This poses
challenges to both radiologists and automated image analysis algorithms. A
general approach for tackling this problem is missing data imputation, which
aims to synthesize the missing contrasts from existing ones. While several
convolutional neural network (CNN) based algorithms have been proposed, they
suffer from the fundamental limitations of CNN models, such as requirement for
fixed numbers of input and output channels, inability to capture long-range
dependencies, and lack of interpretability. In this paper, we formulate missing
data imputation as a sequence-to-sequence learning problem and propose a
multi-contrast multi-scale Transformer (MMT), which can take any subset of
input contrasts and synthesize those that are missing. MMT consists of a
multi-scale Transformer encoder that builds hierarchical representations of
inputs combined with a multi-scale Transformer decoder that generates the
outputs in a coarse-to-fine fashion. Thanks to the proposed multi-contrast Swin
Transformer blocks, it can efficiently capture intra- and inter-contrast
dependencies for accurate image synthesis. Moreover, MMT is inherently
interpretable. It allows us to understand the importance of each input contrast
in different regions by analyzing the in-built attention maps of Transformer
blocks in the decoder. Extensive experiments on two large-scale multi-contrast
MRI datasets demonstrate that MMT outperforms the state-of-the-art methods
quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Split for Automatic Bias Detection. (arXiv:2204.13749v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13749">
<div class="article-summary-box-inner">
<span><p>Classifiers are biased when trained on biased datasets. As a remedy, we
propose Learning to Split (ls), an algorithm for automatic bias detection.
Given a dataset with input-label pairs, ls learns to split this dataset so that
predictors trained on the training split generalize poorly to the testing
split. This performance gap provides a proxy for measuring the degree of bias
in the learned features and can therefore be used to reduce biases. Identifying
non-generalizable splits is challenging as we don't have any explicit
annotations about how to split. In this work, we show that the prediction
correctness of the testing example can be used as a source of weak supervision:
generalization performance will drop if we move examples that are predicted
correctly away from the testing split, leaving only those that are
mispredicted. We evaluate our approach on Beer Review, Waterbirds, CelebA and
MNLI. Empirical results show that ls is able to generate astonishingly
challenging splits that correlate with human-identified biases. Moreover, we
demonstrate that combining robust learning algorithms (such as group DRO) with
splits identified by ls enables automatic de-biasing. Compared with previous
state-of-the-arts, we substantially improves the worst-group performance (23.4%
on average) when the source of biases is unknown during training and
validation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth Estimation with Simplified Transformer. (arXiv:2204.13791v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13791">
<div class="article-summary-box-inner">
<span><p>Transformer and its variants have shown state-of-the-art results in many
vision tasks recently, ranging from image classification to dense prediction.
Despite of their success, limited work has been reported on improving the model
efficiency for deployment in latency-critical applications, such as autonomous
driving and robotic navigation. In this paper, we aim at improving upon the
existing transformers in vision, and propose a method for self-supervised
monocular Depth Estimation with Simplified Transformer (DEST), which is
efficient and particularly suitable for deployment on GPU-based platforms.
Through strategic design choices, our model leads to significant reduction in
model size, complexity, as well as inference latency, while achieving superior
accuracy as compared to state-of-the-art. We also show that our design
generalize well to other dense prediction task without bells and whistles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A very preliminary analysis of DALL-E 2. (arXiv:2204.13807v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13807">
<div class="article-summary-box-inner">
<span><p>The DALL-E 2 system generates original synthetic images corresponding to an
input text as caption. We report here on the outcome of fourteen tests of this
system designed to assess its common sense, reasoning and ability to understand
complex texts. All of our prompts were intentionally much more challenging than
the typical ones that have been showcased in recent weeks. Nevertheless, for 5
out of the 14 prompts, at least one of the ten images fully satisfied our
requests. On the other hand, on no prompt did all of the ten images satisfy our
requests.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysing the Influence of Attack Configurations on the Reconstruction of Medical Images in Federated Learning. (arXiv:2204.13808v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13808">
<div class="article-summary-box-inner">
<span><p>The idea of federated learning is to train deep neural network models
collaboratively and share them with multiple participants without exposing
their private training data to each other. This is highly attractive in the
medical domain due to patients' privacy records. However, a recently proposed
method called Deep Leakage from Gradients enables attackers to reconstruct data
from shared gradients. This study shows how easy it is to reconstruct images
for different data initialization schemes and distance measures. We show how
data and model architecture influence the optimal choice of initialization
scheme and distance measure configurations when working with single images. We
demonstrate that the choice of initialization scheme and distance measure can
significantly increase convergence speed and quality. Furthermore, we find that
the optimal attack configuration depends largely on the nature of the target
image distribution and the complexity of the model architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning-based Automatic Player Identification and Logging in American Football Videos. (arXiv:2204.13809v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13809">
<div class="article-summary-box-inner">
<span><p>American football games attract significant worldwide attention every year.
Game analysis systems generate crucial information that can help analyze the
games by providing fans and coaches with a convenient means to track and
evaluate player performance. Identifying participating players in each play is
also important for the video indexing of player participation per play.
Processing football game video presents challenges such as crowded setting,
distorted objects, and imbalanced data for identifying players, especially
jersey numbers. In this work, we propose a deep learning-based football video
analysis system to automatically track players and index their participation
per play. It is a multi-stage network design to highlight area of interest and
identify jersey number information with high accuracy. First, we utilize an
object detection network, a detection transformer, to tackle the player
detection problem in crowded context. Second, we identify players using jersey
number recognition with a secondary convolutional neural network, then
synchronize it with a game clock subsystem. Finally, the system outputs a
complete log in a database for play indexing. We demonstrate the effectiveness
and reliability of player identification and the logging system by analyzing
the qualitative and quantitative results on football videos. The proposed
system shows great potential for implementation in and analysis of football
broadcast video.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding the impact of image and input resolution on deep digital pathology patch classifiers. (arXiv:2204.13829v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13829">
<div class="article-summary-box-inner">
<span><p>We consider annotation efficient learning in Digital Pathology (DP), where
expert annotations are expensive and thus scarce. We explore the impact of
image and input resolution on DP patch classification performance. We use two
cancer patch classification datasets PCam and CRC, to validate the results of
our study. Our experiments show that patch classification performance can be
improved by manipulating both the image and input resolution in
annotation-scarce and annotation-rich environments. We show a positive
correlation between the image and input resolution and the patch classification
accuracy on both datasets. By exploiting the image and input resolution, our
final model trained on &lt; 1% of data performs equally well compared to the model
trained on 100% of data in the original image resolution on the PCam dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noise-reducing attention cross fusion learning transformer for histological image classification of osteosarcoma. (arXiv:2204.13838v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13838">
<div class="article-summary-box-inner">
<span><p>The degree of malignancy of osteosarcoma and its tendency to
metastasize/spread mainly depend on the pathological grade (determined by
observing the morphology of the tumor under a microscope). The purpose of this
study is to use artificial intelligence to classify osteosarcoma histological
images and to assess tumor survival and necrosis, which will help doctors
reduce their workload, improve the accuracy of osteosarcoma cancer detection,
and make a better prognosis for patients. The study proposes a typical
transformer image classification framework by integrating noise reduction
convolutional autoencoder and feature cross fusion learning (NRCA-FCFL) to
classify osteosarcoma histological images. Noise reduction convolutional
autoencoder could well denoise histological images of osteosarcoma, resulting
in more pure images for osteosarcoma classification. Moreover, we introduce
feature cross fusion learning, which integrates two scale image patches, to
sufficiently explore their interactions by using additional classification
tokens. As a result, a refined fusion feature is generated, which is fed to the
residual neural network for label predictions. We conduct extensive experiments
to evaluate the performance of the proposed approach. The experimental results
demonstrate that our method outperforms the traditional and deep learning
approaches on various evaluation metrics, with an accuracy of 99.17% to support
osteosarcoma diagnosis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GenDR: A Generalized Differentiable Renderer. (arXiv:2204.13845v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13845">
<div class="article-summary-box-inner">
<span><p>In this work, we present and study a generalized family of differentiable
renderers. We discuss from scratch which components are necessary for
differentiable rendering and formalize the requirements for each component. We
instantiate our general differentiable renderer, which generalizes existing
differentiable renderers like SoftRas and DIB-R, with an array of different
smoothing distributions to cover a large spectrum of reasonable settings. We
evaluate an array of differentiable renderer instantiations on the popular
ShapeNet 3D reconstruction benchmark and analyze the implications of our
results. Surprisingly, the simple uniform distribution yields the best overall
results when averaged over 13 classes; in general, however, the optimal choice
of distribution heavily depends on the task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Goldilocks-curriculum Domain Randomization and Fractal Perlin Noise with Application to Sim2Real Pneumonia Lesion Detection. (arXiv:2204.13849v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13849">
<div class="article-summary-box-inner">
<span><p>A computer-aided detection (CAD) system based on machine learning is expected
to assist radiologists in making a diagnosis. It is desirable to build CAD
systems for the various types of diseases accumulating daily in a hospital. An
obstacle in developing a CAD system for a disease is that the number of medical
images is typically too small to improve the performance of the machine
learning model. In this paper, we aim to explore ways to address this problem
through a sim2real transfer approach in medical image fields. To build a
platform to evaluate the performance of sim2real transfer methods in the field
of medical imaging, we construct a benchmark dataset that consists of $101$
chest X-images with difficult-to-identify pneumonia lesions judged by an
experienced radiologist and a simulator based on fractal Perlin noise and the
X-ray principle for generating pseudo pneumonia lesions. We then develop a
novel domain randomization method, called Goldilocks-curriculum domain
randomization (GDR) and evaluate our method in this platform.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVID-Net US-X: Enhanced Deep Neural Network for Detection of COVID-19 Patient Cases from Convex Ultrasound Imaging Through Extended Linear-Convex Ultrasound Augmentation Learning. (arXiv:2204.13851v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13851">
<div class="article-summary-box-inner">
<span><p>As the global population continues to face significant negative impact by the
on-going COVID-19 pandemic, there has been an increasing usage of point-of-care
ultrasound (POCUS) imaging as a low-cost and effective imaging modality of
choice in the COVID-19 clinical workflow. A major barrier with widespread
adoption of POCUS in the COVID-19 clinical workflow is the scarcity of expert
clinicians that can interpret POCUS examinations, leading to considerable
interest in deep learning-driven clinical decision support systems to tackle
this challenge. A major challenge to building deep neural networks for COVID-19
screening using POCUS is the heterogeneity in the types of probes used to
capture ultrasound images (e.g., convex vs. linear probes), which can lead to
very different visual appearances. In this study, we explore the impact of
leveraging extended linear-convex ultrasound augmentation learning on producing
enhanced deep neural networks for COVID-19 assessment, where we conduct data
augmentation on convex probe data alongside linear probe data that have been
transformed to better resemble convex probe data. Experimental results using an
efficient deep columnar anti-aliased convolutional neural network designed via
a machined-driven design exploration strategy (which we name COVID-Net US-X)
show that the proposed extended linear-convex ultrasound augmentation learning
significantly increases performance, with a gain of 5.1% in test accuracy and
13.6% in AUC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Equine radiograph classification using deep convolutional neural networks. (arXiv:2204.13857v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13857">
<div class="article-summary-box-inner">
<span><p>Purpose: To assess the capability of deep convolutional neural networks to
classify anatomical location and projection from a series of 48 standard views
of racehorse limbs.
</p>
<p>Materials and Methods: 9504 equine pre-import radiographs were used to train,
validate, and test six deep learning architectures available as part of the
open source machine learning framework PyTorch.
</p>
<p>Results: ResNet-34 achieved a top-1 accuracy of 0.8408 and the majority (88%)
of misclassification was because of wrong laterality. Class activation maps
indicated that joint morphology drove the model decision.
</p>
<p>Conclusion: Deep convolutional neural networks are capable of classifying
equine pre-import radiographs into the 48 standard views including moderate
discrimination of laterality independent of side marker presence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Where in the World is this Image? Transformer-based Geo-localization in the Wild. (arXiv:2204.13861v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13861">
<div class="article-summary-box-inner">
<span><p>Predicting the geographic location (geo-localization) from a single
ground-level RGB image taken anywhere in the world is a very challenging
problem. The challenges include huge diversity of images due to different
environmental scenarios, drastic variation in the appearance of the same
location depending on the time of the day, weather, season, and more
importantly, the prediction is made from a single image possibly having only a
few geo-locating cues. For these reasons, most existing works are restricted to
specific cities, imagery, or worldwide landmarks. In this work, we focus on
developing an efficient solution to planet-scale single-image geo-localization.
To this end, we propose TransLocator, a unified dual-branch transformer network
that attends to tiny details over the entire image and produces robust feature
representation under extreme appearance variations. TransLocator takes an RGB
image and its semantic segmentation map as inputs, interacts between its two
parallel branches after each transformer layer, and simultaneously performs
geo-localization and scene recognition in a multi-task fashion. We evaluate
TransLocator on four benchmark datasets - Im2GPS, Im2GPS3k, YFCC4k, YFCC26k and
obtain 5.5%, 14.1%, 4.9%, 9.9% continent-level accuracy improvement over the
state-of-the-art. TransLocator is also validated on real-world test images and
found to be more effective than previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-Language Pre-Training for Boosting Scene Text Detectors. (arXiv:2204.13867v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13867">
<div class="article-summary-box-inner">
<span><p>Recently, vision-language joint representation learning has proven to be
highly effective in various scenarios. In this paper, we specifically adapt
vision-language joint learning for scene text detection, a task that
intrinsically involves cross-modal interaction between the two modalities:
vision and language, since text is the written form of language. Concretely, we
propose to learn contextualized, joint representations through vision-language
pre-training, for the sake of enhancing the performance of scene text
detectors. Towards this end, we devise a pre-training architecture with an
image encoder, a text encoder and a cross-modal encoder, as well as three
pretext tasks: image-text contrastive learning (ITC), masked language modeling
(MLM) and word-in-image prediction (WIP). The pre-trained model is able to
produce more informative representations with richer semantics, which could
readily benefit existing scene text detectors (such as EAST and PSENet) in the
down-stream text detection task. Extensive experiments on standard benchmarks
demonstrate that the proposed paradigm can significantly improve the
performance of various representative text detectors, outperforming previous
pre-training approaches. The code and pre-trained models will be publicly
released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiple Degradation and Reconstruction Network for Single Image Denoising via Knowledge Distillation. (arXiv:2204.13873v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13873">
<div class="article-summary-box-inner">
<span><p>Single image denoising (SID) has achieved significant breakthroughs with the
development of deep learning. However, the proposed methods are often
accompanied by plenty of parameters, which greatly limits their application
scenarios. Different from previous works that blindly increase the depth of the
network, we explore the degradation mechanism of the noisy image and propose a
lightweight Multiple Degradation and Reconstruction Network (MDRN) to
progressively remove noise. Meanwhile, we propose two novel Heterogeneous
Knowledge Distillation Strategies (HMDS) to enable MDRN to learn richer and
more accurate features from heterogeneous models, which make it possible to
reconstruct higher-quality denoised images under extreme conditions. Extensive
experiments show that our MDRN achieves favorable performance against other SID
models with fewer parameters. Meanwhile, plenty of ablation studies demonstrate
that the introduced HMDS can improve the performance of tiny models or the
model under high noise levels, which is extremely useful for related
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Struct-MDC: Mesh-Refined Unsupervised Depth Completion Leveraging Structural Regularities from Visual SLAM. (arXiv:2204.13877v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13877">
<div class="article-summary-box-inner">
<span><p>Feature-based visual simultaneous localization and mapping (SLAM) methods
only estimate the depth of extracted features, generating a sparse depth map.
To solve this sparsity problem, depth completion tasks that estimate a dense
depth from a sparse depth have gained significant importance in robotic
applications like exploration. Existing methodologies that use sparse depth
from visual SLAM mainly employ point features. However, point features have
limitations in preserving structural regularities owing to texture-less
environments and sparsity problems. To deal with these issues, we perform depth
completion with visual SLAM using line features, which can better contain
structural regularities than point features. The proposed methodology creates a
convex hull region by performing constrained Delaunay triangulation with depth
interpolation using line features. However, the generated depth includes
low-frequency information and is discontinuous at the convex hull boundary.
Therefore, we propose a mesh depth refinement (MDR) module to address this
problem. The MDR module effectively transfers the high-frequency details of an
input image to the interpolated depth and plays a vital role in bridging the
conventional and deep learning-based approaches. The Struct-MDC outperforms
other state-of-the-art algorithms on public and our custom datasets, and even
outperforms supervised methodologies for some metrics. In addition, the
effectiveness of the proposed MDR module is verified by a rigorous ablation
study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Adaptive Warping for Real-World Rolling Shutter Correction. (arXiv:2204.13886v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13886">
<div class="article-summary-box-inner">
<span><p>This paper proposes the first real-world rolling shutter (RS) correction
dataset, BS-RSC, and a corresponding model to correct the RS frames in a
distorted video. Mobile devices in the consumer market with CMOS-based sensors
for video capture often result in rolling shutter effects when relative
movements occur during the video acquisition process, calling for RS effect
removal techniques. However, current state-of-the-art RS correction methods
often fail to remove RS effects in real scenarios since the motions are various
and hard to model. To address this issue, we propose a real-world RS correction
dataset BS-RSC. Real distorted videos with corresponding ground truth are
recorded simultaneously via a well-designed beam-splitter-based acquisition
system. BS-RSC contains various motions of both camera and objects in dynamic
scenes. Further, an RS correction model with adaptive warping is proposed. Our
model can warp the learned RS features into global shutter counterparts
adaptively with predicted multiple displacement fields. These warped features
are aggregated and then reconstructed into high-quality global shutter frames
in a coarse-to-fine strategy. Experimental results demonstrate the
effectiveness of the proposed method, and our dataset can improve the model's
ability to remove the RS effects in the real world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SideRT: A Real-time Pure Transformer Architecture for Single Image Depth Estimation. (arXiv:2204.13892v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13892">
<div class="article-summary-box-inner">
<span><p>Since context modeling is critical for estimating depth from a single image,
researchers put tremendous effort into obtaining global context. Many global
manipulations are designed for traditional CNN-based architectures to overcome
the locality of convolutions. Attention mechanisms or transformers originally
designed for capturing long-range dependencies might be a better choice, but
usually complicates architectures and could lead to a decrease in inference
speed. In this work, we propose a pure transformer architecture called SideRT
that can attain excellent predictions in real-time. In order to capture better
global context, Cross-Scale Attention (CSA) and Multi-Scale Refinement (MSR)
modules are designed to work collaboratively to fuse features of different
scales efficiently. CSA modules focus on fusing features of high semantic
similarities, while MSR modules aim to fuse features at corresponding
positions. These two modules contain a few learnable parameters without
convolutions, based on which a lightweight yet effective model is built. This
architecture achieves state-of-the-art performances in real-time (51.3 FPS) and
becomes much faster with a reasonable performance drop on a smaller backbone
Swin-T (83.1 FPS). Furthermore, its performance surpasses the previous
state-of-the-art by a large margin, improving AbsRel metric 6.9% on KITTI and
9.7% on NYU. To the best of our knowledge, this is the first work to show that
transformer-based networks can attain state-of-the-art performance in real-time
in the single image depth estimation field. Code will be made available soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leaner and Faster: Two-Stage Model Compression for Lightweight Text-Image Retrieval. (arXiv:2204.13913v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13913">
<div class="article-summary-box-inner">
<span><p>Current text-image approaches (e.g., CLIP) typically adopt dual-encoder
architecture using pre-trained vision-language representation. However, these
models still pose non-trivial memory requirements and substantial incremental
indexing time, which makes them less practical on mobile devices. In this
paper, we present an effective two-stage framework to compress large
pre-trained dual-encoder for lightweight text-image retrieval. The resulting
model is smaller (39% of the original), faster (1.6x/2.9x for processing
image/text respectively), yet performs on par with or better than the original
full model on Flickr30K and MSCOCO benchmarks. We also open-source an
accompanying realistic mobile image search application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy-Preserving Model Upgrades with Bidirectional Compatible Training in Image Retrieval. (arXiv:2204.13919v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13919">
<div class="article-summary-box-inner">
<span><p>The task of privacy-preserving model upgrades in image retrieval desires to
reap the benefits of rapidly evolving new models without accessing the raw
gallery images. A pioneering work introduced backward-compatible training,
where the new model can be directly deployed in a backfill-free manner, i.e.,
the new query can be directly compared to the old gallery features. Despite a
possible solution, its improvement in sequential model upgrades is gradually
limited by the fixed and under-quality old gallery embeddings. To this end, we
propose a new model upgrade paradigm, termed Bidirectional Compatible Training
(BiCT), which will upgrade the old gallery embeddings by forward-compatible
training towards the embedding space of the backward-compatible new model. We
conduct comprehensive experiments to verify the prominent improvement by BiCT
and interestingly observe that the inconspicuous loss weight of backward
compatibility actually plays an essential role for both backward and forward
retrieval performance. To summarize, we introduce a new and valuable problem
named privacy-preserving model upgrades, with a proper solution BiCT. Several
intriguing insights are further proposed to get the most out of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Gradient of a Regularizer for Plug-and-Play Gradient Descent. (arXiv:2204.13940v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13940">
<div class="article-summary-box-inner">
<span><p>The Plug-and-Play (PnP) framework allows integrating advanced image denoising
priors into optimization algorithms, to efficiently solve a variety of image
restoration tasks. The Plug-and-Play alternating direction method of
multipliers (ADMM) and the Regularization by Denoising (RED) algorithms are two
examples of such methods that made a breakthrough in image restoration.
However, while the former method only applies to proximal algorithms, it has
recently been shown that there exists no regularization that explains the RED
algorithm when the denoisers lack Jacobian symmetry, which happen to be the
case of most practical denoisers. To the best of our knowledge, there exists no
method for training a network that directly represents the gradient of a
regularizer, which can be directly used in Plug-and-Play gradient-based
algorithms. We show that it is possible to train a denoiser along with a
network that corresponds to the gradient of its regularizer. We use this
gradient of the regularizer in gradient-based optimization methods and obtain
better results comparing to other generic Plug-and-Play approaches. We also
show that the regularizer can be used as a pre-trained network for unrolled
gradient descent. Lastly, we show that the resulting denoiser allows for a
quick convergence of the Plug-and-Play ADMM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Geometry Post-Processing for Decompressed Point Clouds. (arXiv:2204.13952v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13952">
<div class="article-summary-box-inner">
<span><p>Point cloud compression plays a crucial role in reducing the huge cost of
data storage and transmission. However, distortions can be introduced into the
decompressed point clouds due to quantization. In this paper, we propose a
novel learning-based post-processing method to enhance the decompressed point
clouds. Specifically, a voxelized point cloud is first divided into small
cubes. Then, a 3D convolutional network is proposed to predict the occupancy
probability for each location of a cube. We leverage both local and global
contexts by generating multi-scale probabilities. These probabilities are
progressively summed to predict the results in a coarse-to-fine manner.
Finally, we obtain the geometry-refined point clouds based on the predicted
probabilities. Different from previous methods, we deal with decompressed point
clouds with huge variety of distortions using a single model. Experimental
results show that the proposed method can significantly improve the quality of
the decompressed point clouds, achieving 9.30dB BDPSNR gain on three
representative datasets on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCS-Co: Self-Consistent Style Contrastive Learning for Image Harmonization. (arXiv:2204.13962v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13962">
<div class="article-summary-box-inner">
<span><p>Image harmonization aims to achieve visual consistency in composite images by
adapting a foreground to make it compatible with a background. However,
existing methods always only use the real image as the positive sample to guide
the training, and at most introduce the corresponding composite image as a
single negative sample for an auxiliary constraint, which leads to limited
distortion knowledge, and further causes a too large solution space, making the
generated harmonized image distorted. Besides, none of them jointly constrain
from the foreground self-style and foreground-background style consistency,
which exacerbates this problem. Moreover, recent region-aware adaptive instance
normalization achieves great success but only considers the global background
feature distribution, making the aligned foreground feature distribution
biased. To address these issues, we propose a self-consistent style contrastive
learning scheme (SCS-Co). By dynamically generating multiple negative samples,
our SCS-Co can learn more distortion knowledge and well regularize the
generated harmonized image in the style representation space from two aspects
of the foreground self-style and foreground-background style consistency,
leading to a more photorealistic visual result. In addition, we propose a
background-attentional adaptive instance normalization (BAIN) to achieve an
attention-weighted background feature distribution according to the
foreground-background feature similarity. Experiments demonstrate the
superiority of our method over other state-of-the-art methods in both
quantitative comparison and visual analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using 3D Shadows to Detect Object Hiding Attacks on Autonomous Vehicle Perception. (arXiv:2204.13973v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13973">
<div class="article-summary-box-inner">
<span><p>Autonomous Vehicles (AVs) are mostly reliant on LiDAR sensors which enable
spatial perception of their surroundings and help make driving decisions.
Recent works demonstrated attacks that aim to hide objects from AV perception,
which can result in severe consequences. 3D shadows, are regions void of
measurements in 3D point clouds which arise from occlusions of objects in a
scene. 3D shadows were proposed as a physical invariant valuable for detecting
spoofed or fake objects. In this work, we leverage 3D shadows to locate
obstacles that are hidden from object detectors. We achieve this by searching
for void regions and locating the obstacles that cause these shadows. Our
proposed methodology can be used to detect an object that has been hidden by an
adversary as these objects, while hidden from 3D object detectors, still induce
shadow artifacts in 3D point clouds, which we use for obstacle detection. We
show that using 3D shadows for obstacle detection can achieve high accuracy in
matching shadows to their object and provide precise prediction of an
obstacle's distance from the ego-vehicle.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaInt: Learning Adaptive Intervals for 3D Lookup Tables on Real-time Image Enhancement. (arXiv:2204.13983v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13983">
<div class="article-summary-box-inner">
<span><p>The 3D Lookup Table (3D LUT) is a highly-efficient tool for real-time image
enhancement tasks, which models a non-linear 3D color transform by sparsely
sampling it into a discretized 3D lattice. Previous works have made efforts to
learn image-adaptive output color values of LUTs for flexible enhancement but
neglect the importance of sampling strategy. They adopt a sub-optimal uniform
sampling point allocation, limiting the expressiveness of the learned LUTs
since the (tri-)linear interpolation between uniform sampling points in the LUT
transform might fail to model local non-linearities of the color transform.
Focusing on this problem, we present AdaInt (Adaptive Intervals Learning), a
novel mechanism to achieve a more flexible sampling point allocation by
adaptively learning the non-uniform sampling intervals in the 3D color space.
In this way, a 3D LUT can increase its capability by conducting dense sampling
in color ranges requiring highly non-linear transforms and sparse sampling for
near-linear transforms. The proposed AdaInt could be implemented as a compact
and efficient plug-and-play module for a 3D LUT-based method. To enable the
end-to-end learning of AdaInt, we design a novel differentiable operator called
AiLUT-Transform (Adaptive Interval LUT Transform) to locate input colors in the
non-uniform 3D LUT and provide gradients to the sampling intervals. Experiments
demonstrate that methods equipped with AdaInt can achieve state-of-the-art
performance on two public benchmark datasets with a negligible overhead
increase. Our source code is available at https://github.com/ImCharlesY/AdaInt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning High-DOF Reaching-and-Grasping via Dynamic Representation of Gripper-Object Interaction. (arXiv:2204.13998v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13998">
<div class="article-summary-box-inner">
<span><p>We approach the problem of high-DOF reaching-and-grasping via learning joint
planning of grasp and motion with deep reinforcement learning. To resolve the
sample efficiency issue in learning the high-dimensional and complex control of
dexterous grasping, we propose an effective representation of grasping state
characterizing the spatial interaction between the gripper and the target
object. To represent gripper-object interaction, we adopt Interaction Bisector
Surface (IBS) which is the Voronoi diagram between two close by 3D geometric
objects and has been successfully applied in characterizing spatial relations
between 3D objects. We found that IBS is surprisingly effective as a state
representation since it well informs the fine-grained control of each finger
with spatial relation against the target object. This novel grasp
representation, together with several technical contributions including a fast
IBS approximation, a novel vector-based reward and an effective training
strategy, facilitate learning a strong control model of high-DOF grasping with
good sample efficiency, dynamic adaptability, and cross-category generality.
Experiments show that it generates high-quality dexterous grasp for complex
shapes with smooth grasping motions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Searching for Efficient Neural Architectures for On-Device ML on Edge TPUs. (arXiv:2204.14007v1 [cs.DC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14007">
<div class="article-summary-box-inner">
<span><p>On-device ML accelerators are becoming a standard in modern mobile
system-on-chips (SoC). Neural architecture search (NAS) comes to the rescue for
efficiently utilizing the high compute throughput offered by these
accelerators. However, existing NAS frameworks have several practical
limitations in scaling to multiple tasks and different target platforms. In
this work, we provide a two-pronged approach to this challenge: (i) a
NAS-enabling infrastructure that decouples model cost evaluation, search space
design, and the NAS algorithm to rapidly target various on-device ML tasks, and
(ii) search spaces crafted from group convolution based inverted bottleneck
(IBN) variants that provide flexible quality/performance trade-offs on ML
accelerators, complementing the existing full and depthwise convolution based
IBNs. Using this approach we target a state-of-the-art mobile platform, Google
Tensor SoC, and demonstrate neural architectures that improve the
quality-performance pareto frontier for various computer vision
(classification, detection, segmentation) as well as natural language
processing tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Implicit Representations for Physical Parameter Inference from a Single Video. (arXiv:2204.14030v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14030">
<div class="article-summary-box-inner">
<span><p>Neural networks have recently been used to analyze diverse physical systems
and to identify the underlying dynamics. While existing methods achieve
impressive results, they are limited by their strong demand for training data
and their weak generalization abilities to out-of-distribution data. To
overcome these limitations, in this work we propose to combine neural implicit
representations for appearance modeling with neural ordinary differential
equations (ODEs) for modelling physical phenomena to obtain a dynamic scene
representation that can be identified directly from visual observations. Our
proposed model combines several unique advantages: (i) Contrary to existing
approaches that require large training datasets, we are able to identify
physical parameters from only a single video. (ii) The use of neural implicit
representations enables the processing of high-resolution videos and the
synthesis of photo-realistic images. (iii) The embedded neural ODE has a known
parametric form that allows for the identification of interpretable physical
parameters, and (iv) long-term prediction in state space. (v) Furthermore, the
photo-realistic rendering of novel scenes with modified physical parameters
becomes possible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Challenging Benchmark of Anime Style Recognition. (arXiv:2204.14034v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14034">
<div class="article-summary-box-inner">
<span><p>Given two images of different anime roles, anime style recognition (ASR) aims
to learn abstract painting style to determine whether the two images are from
the same work, which is an interesting but challenging problem. Unlike
biometric recognition, such as face recognition, iris recognition, and person
re-identification, ASR suffers from a much larger semantic gap but receives
less attention. In this paper, we propose a challenging ASR benchmark. Firstly,
we collect a large-scale ASR dataset (LSASRD), which contains 20,937 images of
190 anime works and each work at least has ten different roles. In addition to
the large-scale, LSASRD contains a list of challenging factors, such as complex
illuminations, various poses, theatrical colors and exaggerated compositions.
Secondly, we design a cross-role protocol to evaluate ASR performance, in which
query and gallery images must come from different roles to validate an ASR
model is to learn abstract painting style rather than learn discriminative
features of roles. Finally, we apply two powerful person re-identification
methods, namely, AGW and TransReID, to construct the baseline performance on
LSASRD. Surprisingly, the recent transformer model (i.e., TransReID) only
acquires a 42.24% mAP on LSASRD. Therefore, we believe that the ASR task of a
huge semantic gap deserves deep and long-term research. We will open our
dataset and code at https://github.com/nkjcqvcpi/ASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">C3-STISR: Scene Text Image Super-resolution with Triple Clues. (arXiv:2204.14044v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14044">
<div class="article-summary-box-inner">
<span><p>Scene text image super-resolution (STISR) has been regarded as an important
pre-processing task for text recognition from low-resolution scene text images.
Most recent approaches use the recognizer's feedback as clues to guide
super-resolution. However, directly using recognition clue has two problems: 1)
Compatibility. It is in the form of probability distribution, has an obvious
modal gap with STISR - a pixel-level task; 2) Inaccuracy. it usually contains
wrong information, thus will mislead the main task and degrade super-resolution
performance. In this paper, we present a novel method C3-STISR that jointly
exploits the recognizer's feedback, visual and linguistical information as
clues to guide super-resolution. Here, visual clue is from the images of texts
predicted by the recognizer, which is informative and more compatible with the
STISR task; while linguistical clue is generated by a pre-trained
character-level language model, which is able to correct the predicted texts.
We design effective extraction and fusion mechanisms for the triple cross-modal
clues to generate a comprehensive and unified guidance for super-resolution.
Extensive experiments on TextZoom show that C3-STISR outperforms the SOTA
methods in fidelity and recognition performance. Code is available in
https://github.com/zhaominyiz/C3-STISR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Learning based No-reference Quality Assessment Model for UGC Videos. (arXiv:2204.14047v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14047">
<div class="article-summary-box-inner">
<span><p>Quality assessment for User Generated Content (UGC) videos plays an important
role in ensuring the viewing experience of end-users. Previous UGC video
quality assessment (VQA) studies either use the image recognition model or the
image quality assessment (IQA) models to extract frame-level features of UGC
videos for quality regression, which are regarded as the sub-optimal solutions
because of the domain shifts between these tasks and the UGC VQA task. In this
paper, we propose a very simple but effective UGC VQA model, which tries to
address this problem by training an end-to-end spatial feature extraction
network to directly learn the quality-aware spatial feature representation from
raw pixels of the video frames. We also extract the motion features to measure
the temporal-related distortions that the spatial features cannot model. The
proposed model utilizes very sparse frames to extract spatial features and
dense frames (i.e. the video chunk) with a very low spatial resolution to
extract motion features, which thereby has low computational complexity. With
the better quality-aware features, we only use the simple multilayer perception
layer (MLP) network to regress them into the chunk-level quality scores, and
then the temporal average pooling strategy is adopted to obtain the video-level
quality score. We further introduce a multi-scale quality fusion strategy to
solve the problem of VQA across different spatial resolutions, where the
multi-scale weights are obtained from the contrast sensitivity function of the
human visual system. The experimental results show that the proposed model
achieves the best performance on five popular UGC VQA databases, which
demonstrates the effectiveness of the proposed model. The code will be publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast. (arXiv:2204.14057v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14057">
<div class="article-summary-box-inner">
<span><p>We present an approach to learn voice-face representations from the talking
face videos, without any identity labels. Previous works employ cross-modal
instance discrimination tasks to establish the correlation of voice and face.
These methods neglect the semantic content of different videos, introducing
false-negative pairs as training noise. Furthermore, the positive pairs are
constructed based on the natural correlation between audio clips and visual
frames. However, this correlation might be weak or inaccurate in a large amount
of real-world data, which leads to deviating positives into the contrastive
paradigm. To address these issues, we propose the cross-modal prototype
contrastive learning (CMPC), which takes advantage of contrastive methods and
resists adverse effects of false negatives and deviate positives. On one hand,
CMPC could learn the intra-class invariance by constructing semantic-wise
positives via unsupervised clustering in different modalities. On the other
hand, by comparing the similarities of cross-modal instances from that of
cross-modal prototypes, we dynamically recalibrate the unlearnable instances'
contribution to overall loss. Experiments show that the proposed approach
outperforms state-of-the-art unsupervised methods on various voice-face
association evaluation protocols. Additionally, in the low-shot supervision
setting, our method also has a significant improvement compared to previous
instance-wise contrastive learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fix the Noise: Disentangling Source Feature for Transfer Learning of StyleGAN. (arXiv:2204.14079v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14079">
<div class="article-summary-box-inner">
<span><p>Transfer learning of StyleGAN has recently shown great potential to solve
diverse tasks, especially in domain translation. Previous methods utilized a
source model by swapping or freezing weights during transfer learning, however,
they have limitations on visual quality and controlling source features. In
other words, they require additional models that are computationally demanding
and have restricted control steps that prevent a smooth transition. In this
paper, we propose a new approach to overcome these limitations. Instead of
swapping or freezing, we introduce a simple feature matching loss to improve
generation quality. In addition, to control the degree of source features, we
train a target model with the proposed strategy, FixNoise, to preserve the
source features only in a disentangled subspace of a target feature space.
Owing to the disentangled feature space, our method can smoothly control the
degree of the source features in a single model. Extensive experiments
demonstrate that the proposed method can generate more consistent and realistic
images than previous works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Localization-aware Target Confidence for Siamese Visual Tracking. (arXiv:2204.14093v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14093">
<div class="article-summary-box-inner">
<span><p>Siamese tracking paradigm has achieved great success, providing effective
appearance discrimination and size estimation by the classification and
regression. While such a paradigm typically optimizes the classification and
regression independently, leading to task misalignment (accurate prediction
boxes have no high target confidence scores). In this paper, to alleviate this
misalignment, we propose a novel tracking paradigm, called SiamLA. Within this
paradigm, a series of simple, yet effective localization-aware components are
introduced, to generate localization-aware target confidence scores.
Specifically, with the proposed localization-aware dynamic label (LADL) loss
and localization-aware label smoothing (LALS) strategy, collaborative
optimization between the classification and regression is achieved, enabling
classification scores to be aware of location state, not just appearance
similarity. Besides, we propose a separate localization branch, centered on a
localization-aware feature aggregation (LAFA) module, to produce location
quality scores to further modify the classification scores. Consequently, the
resulting target confidence scores, are more discriminative for the location
state, allowing accurate prediction boxes tend to be predicted as high scores.
Extensive experiments are conducted on six challenging benchmarks, including
GOT-10k, TrackingNet, LaSOT, TNL2K, OTB100 and VOT2018. Our SiamLA achieves
state-of-the-art performance in terms of both accuracy and efficiency.
Furthermore, a stability analysis reveals that our tracking paradigm is
relatively stable, implying the paradigm is potential to real-world
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model Pretraining. (arXiv:2204.14095v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14095">
<div class="article-summary-box-inner">
<span><p>Large-scale vision-language pre-training has achieved promising results on
downstream tasks. Existing methods highly rely on the assumption that the
image-text pairs crawled from the Internet are in perfect one-to-one
correspondence. However, in real scenarios, this assumption can be difficult to
hold: the text description, obtained by crawling the affiliated metadata of the
image, often suffer from semantic mismatch and mutual compatibility. To address
these issues, here we introduce PyramidCLIP, which constructs an input pyramid
with different semantic levels, and aligns visual elements and linguistic
elements in the form of hierarchy via intra-level semantics alignment and
cross-level relation alignment. Furthermore, we adjust the objective function
by softening the loss of negative samples (unpaired samples) so as to weaken
the strict constraint during the pre-training stage, thus mitigating the risk
of the model being over-confident. Experiments on three downstream tasks,
including zero-shot image classification, zero-shot image-text retrieval and
image object detection, verify the effectiveness of the proposed PyramidCLIP.
In particular, with the same amount of pre-training data of 15 millions
image-text pairs, PyramidCLIP exceeds CLIP by 19.2%/18.5%/19.6% respectively,
with the image encoder being ResNet-50/ViT-B32/ViT-B16 on ImageNet zero-shot
classification top-1 accuracy. When scaling to larger datasets, the results of
PyramidCLIP only trained for 8 epochs using 128M image-text pairs are very
close to that of CLIP trained for 32 epochs using 400M training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Distortion Learning for Medical Image Denoising. (arXiv:2204.14100v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14100">
<div class="article-summary-box-inner">
<span><p>We present a novel adversarial distortion learning (ADL) for denoising two-
and three-dimensional (2D/3D) biomedical image data. The proposed ADL consists
of two auto-encoders: a denoiser and a discriminator. The denoiser removes
noise from input data and the discriminator compares the denoised result to its
noise-free counterpart. This process is repeated until the discriminator cannot
differentiate the denoised data from the reference. Both the denoiser and the
discriminator are built upon a proposed auto-encoder called Efficient-Unet.
Efficient-Unet has a light architecture that uses the residual blocks and a
novel pyramidal approach in the backbone to efficiently extract and re-use
feature maps. During training, the textural information and contrast are
controlled by two novel loss functions. The architecture of Efficient-Unet
allows generalizing the proposed method to any sort of biomedical data. The 2D
version of our network was trained on ImageNet and tested on biomedical
datasets whose distribution is completely different from ImageNet; so, there is
no need for re-training. Experimental results carried out on magnetic resonance
imaging (MRI), dermatoscopy, electron microscopy and X-ray datasets show that
the proposed method achieved the best on each benchmark. Our implementation and
pre-trained models are available at https://github.com/mogvision/ADL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TEMOS: Generating diverse human motions from textual descriptions. (arXiv:2204.14109v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14109">
<div class="article-summary-box-inner">
<span><p>We address the problem of generating diverse 3D human motions from textual
descriptions. This challenging task requires joint modeling of both modalities:
understanding and extracting useful human-centric information from the text,
and then generating plausible and realistic sequences of human poses. In
contrast to most previous work which focuses on generating a single,
deterministic, motion from a textual description, we design a variational
approach that can produce multiple diverse human motions. We propose TEMOS, a
text-conditioned generative model leveraging variational autoencoder (VAE)
training with human motion data, in combination with a text encoder that
produces distribution parameters compatible with the VAE latent space. We show
that TEMOS framework can produce both skeleton-based animations as in prior
work, as well more expressive SMPL body motions. We evaluate our approach on
the KIT Motion-Language benchmark and, despite being relatively
straightforward, demonstrate significant improvements over the state of the
art. Code and models are available on our project page.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seeing without Looking: Analysis Pipeline for Child Sexual Abuse Datasets. (arXiv:2204.14110v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14110">
<div class="article-summary-box-inner">
<span><p>The online sharing and viewing of Child Sexual Abuse Material (CSAM) are
growing fast, such that human experts can no longer handle the manual
inspection. However, the automatic classification of CSAM is a challenging
field of research, largely due to the inaccessibility of target data that is -
and should forever be - private and in sole possession of law enforcement
agencies. To aid researchers in drawing insights from unseen data and safely
providing further understanding of CSAM images, we propose an analysis template
that goes beyond the statistics of the dataset and respective labels. It
focuses on the extraction of automatic signals, provided both by pre-trained
machine learning models, e.g., object categories and pornography detection, as
well as image metrics such as luminance and sharpness. Only aggregated
statistics of sparse signals are provided to guarantee the anonymity of
children and adolescents victimized. The pipeline allows filtering the data by
applying thresholds to each specified signal and provides the distribution of
such signals within the subset, correlations between signals, as well as a bias
evaluation. We demonstrated our proposal on the Region-based annotated Child
Pornography Dataset (RCPD), one of the few CSAM benchmarks in the literature,
composed of over 2000 samples among regular and CSAM images, produced in
partnership with Brazil's Federal Police. Although noisy and limited in several
senses, we argue that automatic signals can highlight important aspects of the
overall distribution of data, which is valuable for databases that can not be
disclosed. Our goal is to safely publicize the characteristics of CSAM
datasets, encouraging researchers to join the field and perhaps other
institutions to provide similar reports on their benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study of Meter Detection Methods for Automated Infrastructure Inspection. (arXiv:2204.14117v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14117">
<div class="article-summary-box-inner">
<span><p>In order to read meter values from a camera on an autonomous inspection robot
with positional errors, it is necessary to detect meter regions from the image.
In this study, we developed shape-based, texture-based, and background
information-based methods as meter area detection techniques and compared their
effectiveness for meters of different shapes and sizes. As a result, we
confirmed that the background information-based method can detect the farthest
meters regardless of the shape and number of meters, and can stably detect
meters with a diameter of 40px.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Automatic Parsing of Structured Visual Content through the Use of Synthetic Data. (arXiv:2204.14136v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14136">
<div class="article-summary-box-inner">
<span><p>Structured Visual Content (SVC) such as graphs, flow charts, or the like are
used by authors to illustrate various concepts. While such depictions allow the
average reader to better understand the contents, images containing SVCs are
typically not machine-readable. This, in turn, not only hinders automated
knowledge aggregation, but also the perception of displayed in-formation for
visually impaired people. In this work, we propose a synthetic dataset,
containing SVCs in the form of images as well as ground truths. We show the
usage of this dataset by an application that automatically extracts a graph
representation from an SVC image. This is done by training a model via common
supervised learning methods. As there currently exist no large-scale public
datasets for the detailed analysis of SVC, we propose the Synthetic SVC (SSVC)
dataset comprising 12,000 images with respective bounding box annotations and
detailed graph representations. Our dataset enables the development of strong
models for the interpretation of SVCs while skipping the time-consuming dense
data annotation. We evaluate our model on both synthetic and manually annotated
data and show the transferability of synthetic to real via various metrics,
given the presented application. Here, we evaluate that this proof of concept
is possible to some extend and lay down a solid baseline for this task. We
discuss the limitations of our approach for further improvements. Our utilized
metrics can be used as a tool for future comparisons in this domain. To enable
further research on this task, the dataset is publicly available at
https://bit.ly/3jN1pJJ
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmentation of kidney stones in endoscopic video feeds. (arXiv:2204.14175v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14175">
<div class="article-summary-box-inner">
<span><p>Image segmentation has been increasingly applied in medical settings as
recent developments have skyrocketed the potential applications of deep
learning. Urology, specifically, is one field of medicine that is primed for
the adoption of a real-time image segmentation system with the long-term aim of
automating endoscopic stone treatment. In this project, we explored supervised
deep learning models to annotate kidney stones in surgical endoscopic video
feeds. In this paper, we describe how we built a dataset from the raw videos
and how we developed a pipeline to automate as much of the process as possible.
For the segmentation task, we adapted and analyzed three baseline deep learning
models -- U-Net, U-Net++, and DenseNet -- to predict annotations on the frames
of the endoscopic videos with the highest accuracy above 90\%. To show clinical
potential for real-time use, we also confirmed that our best trained model can
accurately annotate new videos at 30 frames per second. Our results demonstrate
that the proposed method justifies continued development and study of image
segmentation to annotate ureteroscopic video feeds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Oracle Guided Image Synthesis with Relative Queries. (arXiv:2204.14189v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14189">
<div class="article-summary-box-inner">
<span><p>Isolating and controlling specific features in the outputs of generative
models in a user-friendly way is a difficult and open-ended problem. We develop
techniques that allow an oracle user to generate an image they are envisioning
in their head by answering a sequence of relative queries of the form
\textit{"do you prefer image $a$ or image $b$?"} Our framework consists of a
Conditional VAE that uses the collected relative queries to partition the
latent space into preference-relevant features and non-preference-relevant
features. We then use the user's responses to relative queries to determine the
preference-relevant features that correspond to their envisioned output image.
Additionally, we develop techniques for modeling the uncertainty in images'
predicted preference-relevant features, allowing our framework to generalize to
scenarios in which the relative query training set contains noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Transferability for Domain Adaptive Detection Transformers. (arXiv:2204.14195v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14195">
<div class="article-summary-box-inner">
<span><p>DETR-style detectors stand out amongst in-domain scenarios, but their
properties in domain shift settings are under-explored. This paper aims to
build a simple but effective baseline with a DETR-style detector on domain
shift settings based on two findings. For one, mitigating the domain shift on
the backbone and the decoder output features excels in getting favorable
results. For another, advanced domain alignment methods in both parts further
enhance the performance. Thus, we propose the Object-Aware Alignment (OAA)
module and the Optimal Transport based Alignment (OTA) module to achieve
comprehensive domain alignment on the outputs of the backbone and the detector.
The OAA module aligns the foreground regions identified by pseudo-labels in the
backbone outputs, leading to domain-invariant based features. The OTA module
utilizes sliced Wasserstein distance to maximize the retention of location
information while minimizing the domain gap in the decoder outputs. We
implement the findings and the alignment modules into our adaptation method,
and it benchmarks the DETR-style detector on the domain shift settings.
Experiments on various domain adaptive scenarios validate the effectiveness of
our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flamingo: a Visual Language Model for Few-Shot Learning. (arXiv:2204.14198v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14198">
<div class="article-summary-box-inner">
<span><p>Building models that can be rapidly adapted to numerous tasks using only a
handful of annotated examples is an open challenge for multimodal machine
learning research. We introduce Flamingo, a family of Visual Language Models
(VLM) with this ability. Flamingo models include key architectural innovations
to: (i) bridge powerful pretrained vision-only and language-only models, (ii)
handle sequences of arbitrarily interleaved visual and textual data, and (iii)
seamlessly ingest images or videos as inputs. Thanks to their flexibility,
Flamingo models can be trained on large-scale multimodal web corpora containing
arbitrarily interleaved text and images, which is key to endow them with
in-context few-shot learning capabilities. We perform a thorough evaluation of
the proposed Flamingo models, exploring and measuring their ability to rapidly
adapt to a variety of image and video understanding benchmarks. These include
open-ended tasks such as visual question-answering, where the model is prompted
with a question which it has to answer, captioning tasks, which evaluate the
ability to describe a scene or an event, and close-ended tasks such as multiple
choice visual question-answering. For tasks lying anywhere on this spectrum, we
demonstrate that a single Flamingo model can achieve a new state of the art for
few-shot learning, simply by prompting the model with task-specific examples.
On many of these benchmarks, Flamingo actually surpasses the performance of
models that are fine-tuned on thousands of times more task-specific data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preoperative brain tumor imaging: models and software for segmentation and standardized reporting. (arXiv:2204.14199v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14199">
<div class="article-summary-box-inner">
<span><p>For patients suffering from brain tumor, prognosis estimation and treatment
decisions are made by a multidisciplinary team based on a set of preoperative
MR scans. Currently, the lack of standardized and automatic methods for tumor
detection and generation of clinical reports represents a major hurdle. In this
study, we investigate glioblastomas, lower grade gliomas, meningiomas, and
metastases, through four cohorts of up to 4000 patients. Tumor segmentation
models were trained using the AGU-Net architecture with different preprocessing
steps and protocols. Segmentation performances were assessed in-depth using a
wide-range of voxel and patient-wise metrics covering volume, distance, and
probabilistic aspects. Finally, two software solutions have been developed,
enabling an easy use of the trained models and standardized generation of
clinical reports: Raidionics and Raidionics-Slicer. Segmentation performances
were quite homogeneous across the four different brain tumor types, with an
average true positive Dice ranging between 80% and 90%, patient-wise recall
between 88% and 98%, and patient-wise precision around 95%. With our Raidionics
software, running on a desktop computer with CPU support, tumor segmentation
can be performed in 16 to 54 seconds depending on the dimensions of the MRI
volume. For the generation of a standardized clinical report, including the
tumor segmentation and features computation, 5 to 15 minutes are necessary. All
trained models have been made open-access together with the source code for
both software solutions and validation metrics computation. In the future, an
automatic classification of the brain tumor type would be necessary to replace
manual user input. Finally, the inclusion of post-operative segmentation in
both software solutions will be key for generating complete post-operative
standardized clinical reports.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers. (arXiv:2204.14217v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14217">
<div class="article-summary-box-inner">
<span><p>The development of the transformer-based text-to-image models are impeded by
its slow generation and complexity for high-resolution images. In this work, we
put forward a solution based on hierarchical transformers and local parallel
auto-regressive generation. We pretrain a 6B-parameter transformer with a
simple and flexible self-supervised task, Cross-modal general language model
(CogLM), and finetune it for fast super-resolution. The new text-to-image
system, CogView2, shows very competitive generation compared to concurrent
state-of-the-art DALL-E-2, and naturally supports interactive text-guided
editing on images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Application of machine learning methods to detect and classify Core images using GAN and texture recognition. (arXiv:2204.14224v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14224">
<div class="article-summary-box-inner">
<span><p>During exploration campaigns, oil companies rely heavily on drill core
samples as they provide valuable geological information that helps them find
important oil deposits. Traditional core logging techniques are laborious and
subjective. Core imaging, a new technique in the oil industry, is used to
supplement analysis by rapidly characterising large quantities of drill cores
in a nondestructive and noninvasive manner. In this paper, we will present the
problem of core detection and classification. The first problem is detecting
the cores and segmenting the holes in images by using Faster RCNN and Mask RCNN
models respectively. The second problem is filling the hole in the core image
by applying the Generative adversarial network(GAN) technique and using
Contextual Residual Aggregation(CRA) which creates high frequency residual for
missing contents in images. And finally applying Texture recognition models for
the classification of core images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recommendations on test datasets for evaluating AI solutions in pathology. (arXiv:2204.14226v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14226">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence (AI) solutions that automatically extract information
from digital histology images have shown great promise for improving
pathological diagnosis. Prior to routine use, it is important to evaluate their
predictive performance and obtain regulatory approval. This assessment requires
appropriate test datasets. However, compiling such datasets is challenging and
specific recommendations are missing.
</p>
<p>A committee of various stakeholders, including commercial AI developers,
pathologists, and researchers, discussed key aspects and conducted extensive
literature reviews on test datasets in pathology. Here, we summarize the
results and derive general recommendations for the collection of test datasets.
</p>
<p>We address several questions: Which and how many images are needed? How to
deal with low-prevalence subsets? How can potential bias be detected? How
should datasets be reported? What are the regulatory requirements in different
countries?
</p>
<p>The recommendations are intended to help AI developers demonstrate the
utility of their products and to help regulatory agencies and end users verify
reported performance measures. Further research is needed to formulate criteria
for sufficiently representative test datasets so that AI solutions can operate
with less user intervention and better support diagnostic workflows in the
future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hardware Trojan Detection Using Unsupervised Deep Learning on Quantum Diamond Microscope Magnetic Field Images. (arXiv:2204.14228v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14228">
<div class="article-summary-box-inner">
<span><p>This paper presents a method for hardware trojan detection in integrated
circuits. Unsupervised deep learning is used to classify wide field-of-view
(4x4 mm$^2$), high spatial resolution magnetic field images taken using a
Quantum Diamond Microscope (QDM). QDM magnetic imaging is enhanced using
quantum control techniques and improved diamond material to increase magnetic
field sensitivity by a factor of 4 and measurement speed by a factor of 16 over
previous demonstrations. These upgrades facilitate the first demonstration of
QDM magnetic field measurement for hardware trojan detection. Unsupervised
convolutional neural networks and clustering are used to infer trojan presence
from unlabeled data sets of 600x600 pixel magnetic field images without human
bias. This analysis is shown to be more accurate than principal component
analysis for distinguishing between field programmable gate arrays configured
with trojan free and trojan inserted logic. This framework is tested on a set
of scalable trojans that we developed and measured with the QDM. Scalable and
TrustHub trojans are detectable down to a minimum trojan trigger size of 0.5%
of the total logic. The trojan detection framework can be used for golden-chip
free detection, since knowledge of the chips' identities is only used to
evaluate detection accuracy
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EndoMapper dataset of complete calibrated endoscopy procedures. (arXiv:2204.14240v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14240">
<div class="article-summary-box-inner">
<span><p>Computer-assisted systems are becoming broadly used in medicine. In
endoscopy, most research focuses on automatic detection of polyps or other
pathologies, but localization and navigation of the endoscope is completely
performed manually by physicians. To broaden this research and bring spatial
Artificial Intelligence to endoscopies, data from complete procedures are
needed. This data will be used to build a 3D mapping and localization systems
that can perform special task like, for example, detect blind zones during
exploration, provide automatic polyp measurements, guide doctors to a polyp
found in a previous exploration and retrieve previous images of the same area
aligning them for easy comparison. These systems will provide an improvement in
the quality and precision of the procedures while lowering the burden on the
physicians. This paper introduces the Endomapper dataset, the first collection
of complete endoscopy sequences acquired during regular medical practice,
including slow and careful screening explorations, making secondary use of
medical data. Its original purpose is to facilitate the development and
evaluation of VSLAM (Visual Simultaneous Localization and Mapping) methods in
real endoscopy data. The first release of the dataset is composed of 59
sequences with more than 15 hours of video. It is also the first endoscopic
dataset that includes both the computed geometric and photometric endoscope
calibration with the original calibration videos. Meta-data and annotations
associated to the dataset varies from anatomical landmark and description of
the procedure labeling, tools segmentation masks, COLMAP 3D reconstructions,
simulated sequences with groundtruth and meta-data related to special cases,
such as sequences from the same patient. This information will improve the
research in endoscopic VSLAM, as well as other research lines, and create new
research lines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP-Art: Contrastive Pre-training for Fine-Grained Art Classification. (arXiv:2204.14244v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14244">
<div class="article-summary-box-inner">
<span><p>Existing computer vision research in artwork struggles with artwork's
fine-grained attributes recognition and lack of curated annotated datasets due
to their costly creation. To the best of our knowledge, we are one of the first
methods to use CLIP (Contrastive Language-Image Pre-Training) to train a neural
network on a variety of artwork images and text descriptions pairs. CLIP is
able to learn directly from free-form art descriptions, or, if available,
curated fine-grained labels. Model's zero-shot capability allows predicting
accurate natural language description for a given image, without directly
optimizing for the task. Our approach aims to solve 2 challenges: instance
retrieval and fine-grained artwork attribute recognition. We use the iMet
Dataset, which we consider the largest annotated artwork dataset. In this
benchmark we achieved competitive results using only self-supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OSSGAN: Open-Set Semi-Supervised Image Generation. (arXiv:2204.14249v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14249">
<div class="article-summary-box-inner">
<span><p>We introduce a challenging training scheme of conditional GANs, called
open-set semi-supervised image generation, where the training dataset consists
of two parts: (i) labeled data and (ii) unlabeled data with samples belonging
to one of the labeled data classes, namely, a closed-set, and samples not
belonging to any of the labeled data classes, namely, an open-set. Unlike the
existing semi-supervised image generation task, where unlabeled data only
contain closed-set samples, our task is more general and lowers the data
collection cost in practice by allowing open-set samples to appear. Thanks to
entropy regularization, the classifier that is trained on labeled data is able
to quantify sample-wise importance to the training of cGAN as confidence,
allowing us to use all samples in unlabeled data. We design OSSGAN, which
provides decision clues to the discriminator on the basis of whether an
unlabeled image belongs to one or none of the classes of interest, smoothly
integrating labeled and unlabeled data during training. The results of
experiments on Tiny ImageNet and ImageNet show notable improvements over
supervised BigGAN and semi-supervised methods. Our code is available at
https://github.com/raven38/OSSGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VehicleNet: Learning Robust Visual Representation for Vehicle Re-identification. (arXiv:2004.06305v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.06305">
<div class="article-summary-box-inner">
<span><p>One fundamental challenge of vehicle re-identification (re-id) is to learn
robust and discriminative visual representation, given the significant
intra-class vehicle variations across different camera views. As the existing
vehicle datasets are limited in terms of training images and viewpoints, we
propose to build a unique large-scale vehicle dataset (called VehicleNet) by
harnessing four public vehicle datasets, and design a simple yet effective
two-stage progressive approach to learning more robust visual representation
from VehicleNet. The first stage of our approach is to learn the generic
representation for all domains (i.e., source vehicle datasets) by training with
the conventional classification loss. This stage relaxes the full alignment
between the training and testing domains, as it is agnostic to the target
vehicle domain. The second stage is to fine-tune the trained model purely based
on the target vehicle set, by minimizing the distribution discrepancy between
our VehicleNet and any target domain. We discuss our proposed multi-source
dataset VehicleNet and evaluate the effectiveness of the two-stage progressive
representation learning through extensive experiments. We achieve the
state-of-art accuracy of 86.07% mAP on the private test set of AICity
Challenge, and competitive results on two other public vehicle re-id datasets,
i.e., VeRi-776 and VehicleID. We hope this new VehicleNet dataset and the
learned robust representations can pave the way for vehicle re-id in the
real-world environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Data Generating Networks: Shape Reconstruction via Bi-level Optimization. (arXiv:2010.08276v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.08276">
<div class="article-summary-box-inner">
<span><p>We propose a novel 3d shape representation for 3d shape reconstruction from a
single image. Rather than predicting a shape directly, we train a network to
generate a training set which will be fed into another learning algorithm to
define the shape. The nested optimization problem can be modeled by bi-level
optimization. Specifically, the algorithms for bi-level optimization are also
being used in meta learning approaches for few-shot learning. Our framework
establishes a link between 3D shape analysis and few-shot learning. We combine
training data generating networks with bi-level optimization algorithms to
obtain a complete framework for which all components can be jointly trained. We
improve upon recent work on standard benchmarks for 3d shape reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Duality-Gated Mutual Condition Network for RGBT Tracking. (arXiv:2011.07188v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.07188">
<div class="article-summary-box-inner">
<span><p>Low-quality modalities contain not only a lot of noisy information but also
some discriminative features in RGBT tracking. However, the potentials of
low-quality modalities are not well explored in existing RGBT tracking
algorithms. In this work, we propose a novel duality-gated mutual condition
network to fully exploit the discriminative information of all modalities while
suppressing the effects of data noise. In specific, we design a mutual
condition module, which takes the discriminative information of a modality as
the condition to guide feature learning of target appearance in another
modality. Such module can effectively enhance target representations of all
modalities even in the presence of low-quality modalities. To improve the
quality of conditions and further reduce data noise, we propose a duality-gated
mechanism and integrate it into the mutual condition module. To deal with the
tracking failure caused by sudden camera motion, which often occurs in RGBT
tracking, we design a resampling strategy based on optical flow algorithms. It
does not increase much computational cost since we perform optical flow
calculation only when the model prediction is unreliable and then execute
resampling when the sudden camera motion is detected. Extensive experiments on
four RGBT tracking benchmark datasets show that our method performs favorably
against the state-of-the-art tracking algorithms
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAMP-CNN: A Novel Neural Network for Enhanced Automotive Radar Object Recognition. (arXiv:2011.08981v2 [eess.SP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.08981">
<div class="article-summary-box-inner">
<span><p>Millimeter-wave radars are being increasingly integrated into commercial
vehicles to support new advanced driver-assistance systems by enabling robust
and high-performance object detection, localization, as well as recognition - a
key component of new environmental perception. In this paper, we propose a
novel radar multiple-perspectives convolutional neural network (RAMP-CNN) that
extracts the location and class of objects based on further processing of the
range-velocity-angle (RVA) heatmap sequences. To bypass the complexity of 4D
convolutional neural networks (NN), we propose to combine several
lower-dimension NN models within our RAMP-CNN model that nonetheless approaches
the performance upper-bound with lower complexity. The extensive experiments
show that the proposed RAMP-CNN model achieves better average recall and
average precision than prior works in all testing scenarios. Besides, the
RAMP-CNN model is validated to work robustly under nighttime, which enables
low-cost radars as a potential substitute for pure optical sensing under severe
conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrackFormer: Multi-Object Tracking with Transformers. (arXiv:2101.02702v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.02702">
<div class="article-summary-box-inner">
<span><p>The challenging task of multi-object tracking (MOT) requires simultaneous
reasoning about track initialization, identity, and spatio-temporal
trajectories. We formulate this task as a frame-to-frame set prediction problem
and introduce TrackFormer, an end-to-end trainable MOT approach based on an
encoder-decoder Transformer architecture. Our model achieves data association
between frames via attention by evolving a set of track predictions through a
video sequence. The Transformer decoder initializes new tracks from static
object queries and autoregressively follows existing tracks in space and time
with the conceptually new and identity preserving track queries. Both query
types benefit from self- and encoder-decoder attention on global frame-level
features, thereby omitting any additional graph optimization or modeling of
motion and/or appearance. TrackFormer introduces a new tracking-by-attention
paradigm and while simple in its design is able to achieve state-of-the-art
performance on the task of multi-object tracking (MOT17 and MOT20) and
segmentation (MOTS20). The code is available at
https://github.com/timmeinhardt/trackformer .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Multitask Neural Network for Face Alignment, Head Pose Estimation and Face Tracking. (arXiv:2103.07615v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07615">
<div class="article-summary-box-inner">
<span><p>While Convolutional Neural Networks (CNNs) have significantly boosted the
performance of face related algorithms, maintaining accuracy and efficiency
simultaneously in practical use remains challenging. The state-of-the-art
methods employ deeper networks for better performance, which makes it less
practical for mobile applications because of more parameters and higher
computational complexity. Therefore, we propose an efficient multitask neural
network, Alignment &amp; Tracking &amp; Pose Network (ATPN) for face alignment, face
tracking and head pose estimation. Specifically, to achieve better performance
with fewer layers for face alignment, we introduce a shortcut connection
between shallow-layer and deep-layer features. We find the shallow-layer
features are highly correspond to facial boundaries that can provide the
structural information of face and it is crucial for face alignment. Moreover,
we generate a cheap heatmap based on the face alignment result and fuse it with
features to improve the performance of the other two tasks. Based on the
heatmap, the network can utilize both geometric information of landmarks and
appearance information for head pose estimation. The heatmap also provides
attention clues for face tracking. The face tracking task also saves us the
face detection procedure for each frame, which also significantly boost the
real-time capability for video-based tasks. We experimentally validate ATPN on
four benchmark datasets, WFLW, 300VW, WIDER Face and 300W-LP. The experimental
results demonstrate that it achieves better performance with much less
parameters and lower computational complexity compared to other light models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The GIST and RIST of Iterative Self-Training for Semi-Supervised Segmentation. (arXiv:2103.17105v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.17105">
<div class="article-summary-box-inner">
<span><p>We consider the task of semi-supervised semantic segmentation, where we aim
to produce pixel-wise semantic object masks given only a small number of
human-labeled training examples. We focus on iterative self-training methods in
which we explore the behavior of self-training over multiple refinement stages.
We show that iterative self-training leads to performance degradation if done
na\"ively with a fixed ratio of human-labeled to pseudo-labeled training
examples. We propose Greedy Iterative Self-Training (GIST) and Random Iterative
Self-Training (RIST) strategies that alternate between training on either
human-labeled data or pseudo-labeled data at each refinement stage, resulting
in a performance boost rather than degradation. We further show that GIST and
RIST can be combined with existing semi-supervised learning methods to boost
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pyramid Medical Transformer for Medical Image Segmentation. (arXiv:2104.14702v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14702">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have been a prevailing technique in the field of medical
image processing. However, the most popular convolutional neural networks
(CNNs) based methods for medical image segmentation are imperfect because they
model long-range dependencies by stacking layers or enlarging filters.
Transformers and the self-attention mechanism are recently proposed to
effectively learn long-range dependencies by modeling all pairs of word-to-word
attention regardless of their positions. The idea has also been extended to the
computer vision field by creating and treating image patches as embeddings.
Considering the computation complexity for whole image self-attention, current
transformer-based models settle for a rigid partitioning scheme that
potentially loses informative relations. Besides, current medical transformers
model global context on full resolution images, leading to unnecessary
computation costs. To address these issues, we developed a novel method to
integrate multi-scale attention and CNN feature extraction using a pyramidal
network architecture, namely Pyramid Medical Transformer (PMTrans). The PMTrans
captured multi-range relations by working on multi-resolution images. An
adaptive partitioning scheme was implemented to retain informative relations
and to access different receptive fields efficiently. Experimental results on
three medical image datasets (gland segmentation, MoNuSeg, and HECKTOR
datasets) showed that PMTrans outperformed the latest CNN-based and
transformer-based models for medical image segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smaller Is Better: An Analysis of Instance Quantity/Quality Trade-off in Rehearsal-based Continual Learning. (arXiv:2105.14106v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14106">
<div class="article-summary-box-inner">
<span><p>The design of machines and algorithms capable of learning in a dynamically
changing environment has become an increasingly topical problem with the
increase of the size and heterogeneity of data available to learning systems.
As a consequence, the key issue of Continual Learning has become that of
addressing the stability-plasticity dilemma of connectionist systems, as they
need to adapt their model without forgetting previously acquired knowledge.
Within this context, rehearsal-based methods i.e., solutions in where the
learner exploits memory to revisit past data, has proven to be very effective,
leading to performance at the state-of-the-art. In our study, we propose an
analysis of the memory quantity/quality trade-off adopting various data
reduction approaches to increase the number of instances storable in memory. In
particular, we investigate complex instance compression techniques such as deep
encoders, but also trivial approaches such as image resizing and linear
dimensionality reduction. Our findings suggest that the optimal trade-off is
severely skewed toward instance quantity, where rehearsal approaches with
several heavily compressed instances easily outperform state-of-the-art
approaches with the same amount of memory at their disposal. Further, in high
memory configurations, deep approaches extracting spatial structure combined
with extreme resizing (of the order of $8\times8$ images) yield the best
results, while in memory-constrained configurations where deep approaches
cannot be used due to their memory requirement in training, Extreme Learning
Machines (ELM) offer a clear advantage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DETReg: Unsupervised Pretraining with Region Priors for Object Detection. (arXiv:2106.04550v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04550">
<div class="article-summary-box-inner">
<span><p>Recent self-supervised pretraining methods for object detection largely focus
on pretraining the backbone of the object detector, neglecting key parts of
detection architecture. Instead, we introduce DETReg, a new self-supervised
method that pretrains the entire object detection network, including the object
localization and embedding components. During pretraining, DETReg predicts
object localizations to match the localizations from an unsupervised region
proposal generator and simultaneously aligns the corresponding feature
embeddings with embeddings from a self-supervised image encoder. We implement
DETReg using the DETR family of detectors and show that it improves over
competitive baselines when finetuned on COCO, PASCAL VOC, and Airbus Ship
benchmarks. In low-data regimes, including semi-supervised and few-shot
learning settings, DETReg establishes many state-of-the-art results, e.g., on
COCO we see a +6.0 AP improvement for 10-shot detection and over 2 AP
improvements when training with only 1\% of the labels. For code and pretrained
models, visit the project page at https://amirbar.net/detreg
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to See by Looking at Noise. (arXiv:2106.05963v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05963">
<div class="article-summary-box-inner">
<span><p>Current vision systems are trained on huge datasets, and these datasets come
with costs: curation is expensive, they inherit human biases, and there are
concerns over privacy and usage rights. To counter these costs, interest has
surged in learning from cheaper data sources, such as unlabeled images. In this
paper we go a step further and ask if we can do away with real image datasets
entirely, instead learning from noise processes. We investigate a suite of
image generation models that produce images from simple random processes. These
are then used as training data for a visual representation learner with a
contrastive loss. We study two types of noise processes, statistical image
models and deep generative models under different random initializations. Our
findings show that it is important for the noise to capture certain structural
properties of real data but that good performance can be achieved even with
processes that are far from realistic. We also find that diversity is a key
property to learn good representations. Datasets, models, and code are
available at https://mbaradad.github.io/learning_with_noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLASS: Geometric Latent Augmentation for Shape Spaces. (arXiv:2108.03225v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03225">
<div class="article-summary-box-inner">
<span><p>We investigate the problem of training generative models on a very sparse
collection of 3D models. We use geometrically motivated energies to augment and
thus boost a sparse collection of example (training) models. We analyze the
Hessian of the as-rigid-as-possible (ARAP) energy to sample from and project to
the underlying (local) shape space, and use the augmented dataset to train a
variational autoencoder (VAE). We iterate the process of building latent spaces
of VAE and augmenting the associated dataset, to progressively reveal a richer
and more expressive generative space for creating geometrically and
semantically valid samples. Our framework allows us to train generative 3D
models even with a small set of good quality 3D models, which are typically
hard to curate. We extensively evaluate our method against a set of strong
baselines, provide ablation studies and demonstrate application towards
establishing shape correspondences. We present multiple examples of interesting
and meaningful shape variations even when starting from as few as 3-10 training
shapes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Re-using Adversarial Mask Discriminators for Test-time Training under Distribution Shifts. (arXiv:2108.11926v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11926">
<div class="article-summary-box-inner">
<span><p>Thanks to their ability to learn flexible data-driven losses, Generative
Adversarial Networks (GANs) are an integral part of many semi- and
weakly-supervised methods for medical image segmentation. GANs jointly optimise
a generator and an adversarial discriminator on a set of training data. After
training is complete, the discriminator is usually discarded, and only the
generator is used for inference. But should we discard discriminators? In this
work, we argue that training stable discriminators produces expressive loss
functions that we can re-use at inference to detect and \textit{correct}
segmentation mistakes. First, we identify key challenges and suggest possible
solutions to make discriminators re-usable at inference. Then, we show that we
can combine discriminators with image reconstruction costs (via decoders) to
endow a causal perspective to test-time training and further improve the model.
Our method is simple and improves the test-time performance of pre-trained
GANs. Moreover, we show that it is compatible with standard post-processing
techniques and it has the potential to be used for Online Continual Learning.
With our work, we open new research avenues for re-using adversarial
discriminators at inference. Our code is available at
https://vios-s.github.io/adversarial-test-time-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supervised Contrastive Learning for Detecting Anomalous Driving Behaviours from Multimodal Videos. (arXiv:2109.04021v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04021">
<div class="article-summary-box-inner">
<span><p>Distracted driving is one of the major reasons for vehicle accidents.
Therefore, detecting distracted driving behaviors is of paramount importance to
reduce the millions of deaths and injuries occurring worldwide. Distracted or
anomalous driving behaviors are deviations from 'normal' driving that need to
be identified correctly to alert the driver. However, these driving behaviors
do not comprise one specific type of driving style and their distribution can
be different during the training and test phases of a classifier. We formulate
this problem as a supervised contrastive learning approach to learn a visual
representation to detect normal, and seen and unseen anomalous driving
behaviors. We made a change to the standard contrastive loss function to adjust
the similarity of negative pairs to aid the optimization. Normally, in a (self)
supervised contrastive framework, the projection head layers are omitted during
the test phase as the encoding layers are considered to contain general visual
representative information. However, we assert that for a video-based
supervised contrastive learning task, including a projection head can be
beneficial. We showed our results on a driver anomaly detection dataset that
contains 783 minutes of video recordings of normal and anomalous driving
behaviors of 31 drivers from the various top and front cameras (both depth and
infrared). Out of 9 video modalities combinations, our proposed contrastive
approach improved the ROC AUC on 6 in comparison to the baseline models (from
4.23% to 8.91% for different modalities). We performed statistical tests that
showed evidence that our proposed method performs better than the baseline
contrastive learning setup. Finally, the results showed that the fusion of
depth and infrared modalities from the top and front views achieved the best
AUC ROC of 0.9738 and AUC PR of 0.9772.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ROS-X-Habitat: Bridging the ROS Ecosystem with Embodied AI. (arXiv:2109.07703v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07703">
<div class="article-summary-box-inner">
<span><p>We introduce ROS-X-Habitat, a software interface that bridges the AI Habitat
platform for embodied learning-based agents with other robotics resources via
ROS. This interface not only offers standardized communication protocols
between embodied agents and simulators, but also enables physically and
photorealistic simulation that benefits the training and/or testing of
vision-based embodied agents. With this interface, roboticists can evaluate
their own Habitat RL agents in another ROS-based simulator or use Habitat Sim
v2 as the test bed for their own robotic algorithms. Through in silico
experiments, we demonstrate that ROS-X-Habitat has minimal impact on the
navigation performance and simulation speed of a Habitat RGBD agent; that a
standard set of ROS mapping, planning and navigation tools can run in Habitat
Sim v2; and that a Habitat agent can run in the standard ROS simulator Gazebo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsolved Problems in ML Safety. (arXiv:2109.13916v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13916">
<div class="article-summary-box-inner">
<span><p>Machine learning (ML) systems are rapidly increasing in size, are acquiring
new capabilities, and are increasingly deployed in high-stakes settings. As
with other powerful technologies, safety for ML should be a leading research
priority. In response to emerging safety challenges in ML, such as those
introduced by recent large-scale models, we provide a new roadmap for ML Safety
and refine the technical problems that the field needs to address. We present
four problems ready for research, namely withstanding hazards ("Robustness"),
identifying hazards ("Monitoring"), reducing inherent model hazards
("Alignment"), and reducing systemic hazards ("Systemic Safety"). Throughout,
we clarify each problem's motivation and provide concrete research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation. (arXiv:2110.02624v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02624">
<div class="article-summary-box-inner">
<span><p>Generating shapes using natural language can enable new ways of imagining and
creating the things around us. While significant recent progress has been made
in text-to-image generation, text-to-shape generation remains a challenging
problem due to the unavailability of paired text and shape data at a large
scale. We present a simple yet effective method for zero-shot text-to-shape
generation that circumvents such data scarcity. Our proposed method, named
CLIP-Forge, is based on a two-stage training process, which only depends on an
unlabelled shape dataset and a pre-trained image-text network such as CLIP. Our
method has the benefits of avoiding expensive inference time optimization, as
well as the ability to generate multiple shapes for a given text. We not only
demonstrate promising zero-shot generalization of the CLIP-Forge model
qualitatively and quantitatively, but also provide extensive comparative
evaluations to better understand its behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To Trust or Not To Trust Prediction Scores for Membership Inference Attacks. (arXiv:2111.09076v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09076">
<div class="article-summary-box-inner">
<span><p>Membership inference attacks (MIAs) aim to determine whether a specific
sample was used to train a predictive model. Knowing this may indeed lead to a
privacy breach. Most MIAs, however, make use of the model's prediction scores -
the probability of each output given some input - following the intuition that
the trained model tends to behave differently on its training data. We argue
that this is a fallacy for many modern deep network architectures.
Consequently, MIAs will miserably fail since overconfidence leads to high
false-positive rates not only on known domains but also on out-of-distribution
data and implicitly acts as a defense against MIAs. Specifically, using
generative adversarial networks, we are able to produce a potentially infinite
number of samples falsely classified as part of the training data. In other
words, the threat of MIAs is overestimated, and less information is leaked than
previously assumed. Moreover, there is actually a trade-off between the
overconfidence of models and their susceptibility to MIAs: the more classifiers
know when they do not know, making low confidence predictions, the more they
reveal the training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ice hockey player identification via transformers and weakly supervised learning. (arXiv:2111.11535v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11535">
<div class="article-summary-box-inner">
<span><p>Identifying players in video is a foundational step in computer vision-based
sports analytics. Obtaining player identities is essential for analyzing the
game and is used in downstream tasks such as game event recognition.
Transformers are the existing standard in Natural Language Processing (NLP) and
are swiftly gaining traction in computer vision. Motivated by the increasing
success of transformers in computer vision, in this paper, we introduce a
transformer network for recognizing players through their jersey numbers in
broadcast National Hockey League (NHL) videos. The transformer takes temporal
sequences of player frames (also called player tracklets) as input and outputs
the probabilities of jersey numbers present in the frames. The proposed network
performs better than the previous benchmark on the dataset used. We implement a
weakly-supervised training approach by generating approximate frame-level
labels for jersey number presence and use the frame-level labels for faster
training. We also utilize player shifts available in the NHL play-by-play data
by reading the game time using optical character recognition (OCR) to get the
players on the ice rink at a certain game time. Using player shifts improved
the player identification accuracy by 6%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intuitive Shape Editing in Latent Space. (arXiv:2111.12488v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12488">
<div class="article-summary-box-inner">
<span><p>The use of autoencoders for shape editing or generation through latent space
manipulation suffers from unpredictable changes in the output shape. Our
autoencoder-based method enables intuitive shape editing in latent space by
disentangling latent sub-spaces into style variables and control points on the
surface that can be manipulated independently. The key idea is adding a
Lipschitz-type constraint to the loss function, i.e. bounding the change of the
output shape proportionally to the change in latent space, leading to
interpretable latent space representations. The control points on the surface
that are part of the latent code of an object can then be freely moved,
allowing for intuitive shape editing directly in latent space. We evaluate our
method by comparing to state-of-the-art data-driven shape editing methods. We
further demonstrate the expressiveness of our learned latent space by
leveraging it for unsupervised part segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation. (arXiv:2111.12707v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12707">
<div class="article-summary-box-inner">
<span><p>Estimating 3D human poses from monocular videos is a challenging task due to
depth ambiguity and self-occlusion. Most existing works attempt to solve both
issues by exploiting spatial and temporal relationships. However, those works
ignore the fact that it is an inverse problem where multiple feasible solutions
(i.e., hypotheses) exist. To relieve this limitation, we propose a
Multi-Hypothesis Transformer (MHFormer) that learns spatio-temporal
representations of multiple plausible pose hypotheses. In order to effectively
model multi-hypothesis dependencies and build strong relationships across
hypothesis features, the task is decomposed into three stages: (i) Generate
multiple initial hypothesis representations; (ii) Model self-hypothesis
communication, merge multiple hypotheses into a single converged representation
and then partition it into several diverged hypotheses; (iii) Learn
cross-hypothesis communication and aggregate the multi-hypothesis features to
synthesize the final 3D pose. Through the above processes, the final
representation is enhanced and the synthesized pose is much more accurate.
Extensive experiments show that MHFormer achieves state-of-the-art results on
two challenging datasets: Human3.6M and MPI-INF-3DHP. Without bells and
whistles, its performance surpasses the previous best result by a large margin
of 3% on Human3.6M. Code and models are available at
\url{https://github.com/Vegetebird/MHFormer}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PokeBNN: A Binary Pursuit of Lightweight Accuracy. (arXiv:2112.00133v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00133">
<div class="article-summary-box-inner">
<span><p>Optimization of Top-1 ImageNet promotes enormous networks that may be
impractical in inference settings. Binary neural networks (BNNs) have the
potential to significantly lower the compute intensity but existing models
suffer from low quality. To overcome this deficiency, we propose PokeConv, a
binary convolution block which improves quality of BNNs by techniques such as
adding multiple residual paths, and tuning the activation function. We apply it
to ResNet-50 and optimize ResNet's initial convolutional layer which is hard to
binarize. We name the resulting network family PokeBNN. These techniques are
chosen to yield favorable improvements in both top-1 accuracy and the network's
cost. In order to enable joint optimization of the cost together with accuracy,
we define arithmetic computation effort (ACE), a hardware- and energy-inspired
cost metric for quantized and binarized networks. We also identify a need to
optimize an under-explored hyper-parameter controlling the binarization
gradient approximation.
</p>
<p>We establish a new, strong state-of-the-art (SOTA) on top-1 accuracy together
with commonly-used CPU64 cost, ACE cost and network size metrics.
ReActNet-Adam, the previous SOTA in BNNs, achieved a 70.5% top-1 accuracy with
7.9 ACE. A small variant of PokeBNN achieves 70.5% top-1 with 2.6 ACE, more
than 3x reduction in cost; a larger PokeBNN achieves 75.6% top-1 with 7.8 ACE,
more than 5% improvement in accuracy without increasing the cost. PokeBNN
implementation in JAX/Flax and reproduction instructions are available in AQT
repository: https://github.com/google/aqt
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GANORCON: Are Generative Models Useful for Few-shot Segmentation?. (arXiv:2112.00854v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00854">
<div class="article-summary-box-inner">
<span><p>Advances in generative modeling based on GANs has motivated the community to
find their use beyond image generation and editing tasks. In particular,
several recent works have shown that GAN representations can be re-purposed for
discriminative tasks such as part segmentation, especially when training data
is limited. But how do these improvements stack-up against recent advances in
self-supervised learning? Motivated by this we present an alternative approach
based on contrastive learning and compare their performance on standard
few-shot part segmentation benchmarks. Our experiments reveal that not only do
the GAN-based approach offer no significant performance advantage, their
multi-step training is complex, nearly an order-of-magnitude slower, and can
introduce additional bias. These experiments suggest that the inductive biases
of generative models, such as their ability to disentangle shape and texture,
are well captured by standard feed-forward networks trained using contrastive
learning. These experiments suggest that the inductive biases present in
current generative models, such as their ability to disentangle shape and
texture, are well captured by standard feed-forward networks trained using
contrastive learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAVER: Cross-Modal View-Mixed Transformer for Bi-Modal Salient Object Detection. (arXiv:2112.02363v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02363">
<div class="article-summary-box-inner">
<span><p>Most of the existing bi-modal (RGB-D and RGB-T) salient object detection
methods utilize the convolution operation and construct complex interweave
fusion structures to achieve cross-modal information integration. The inherent
local connectivity of the convolution operation constrains the performance of
the convolution-based methods to a ceiling. In this work, we rethink these
tasks from the perspective of global information alignment and transformation.
Specifically, the proposed \underline{c}ross-mod\underline{a}l
\underline{v}iew-mixed transform\underline{er} (CAVER) cascades several
cross-modal integration units to construct a top-down transformer-based
information propagation path. CAVER treats the multi-scale and multi-modal
feature integration as a sequence-to-sequence context propagation and update
process built on a novel view-mixed attention mechanism. Besides, considering
the quadratic complexity w.r.t. the number of input tokens, we design a
parameter-free patch-wise token re-embedding strategy to simplify operations.
Extensive experimental results on RGB-D and RGB-T SOD datasets demonstrate that
such a simple two-stream encoder-decoder framework can surpass recent
state-of-the-art methods when it is equipped with the proposed components.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GUNNEL: Guided Mixup Augmentation and Multi-View Fusion for Aquatic Animal Segmentation. (arXiv:2112.06193v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06193">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed great advances in object segmentation research.
In addition to generic objects, aquatic animals have attracted research
attention. Deep learning-based methods are widely used for aquatic animal
segmentation and have achieved promising performance. However, there is a lack
of challenging datasets for benchmarking. In this work, we build a new dataset
dubbed "Aquatic Animal Species." We also devise a novel GUided mixup
augmeNtatioN and multi-viEw fusion for aquatic animaL segmentation (GUNNEL)
that leverages the advantages of multiple view segmentation models to
effectively segment aquatic animals and improves the training performance by
synthesizing hard samples. Extensive experiments demonstrated the superiority
of our proposed framework over existing state-of-the-art instance segmentation
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepAdversaries: Examining the Robustness of Deep Learning Models for Galaxy Morphology Classification. (arXiv:2112.14299v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14299">
<div class="article-summary-box-inner">
<span><p>Data processing and analysis pipelines in cosmological survey experiments
introduce data perturbations that can significantly degrade the performance of
deep learning-based models. Given the increased adoption of supervised deep
learning methods for processing and analysis of cosmological survey data, the
assessment of data perturbation effects and the development of methods that
increase model robustness are increasingly important. In the context of
morphological classification of galaxies, we study the effects of perturbations
in imaging data. In particular, we examine the consequences of using neural
networks when training on baseline data and testing on perturbed data. We
consider perturbations associated with two primary sources: 1) increased
observational noise as represented by higher levels of Poisson noise and 2)
data processing noise incurred by steps such as image compression or telescope
errors as represented by one-pixel adversarial attacks. We also test the
efficacy of domain adaptation techniques in mitigating the perturbation-driven
errors. We use classification accuracy, latent space visualizations, and latent
space distance to assess model robustness. Without domain adaptation, we find
that processing pixel-level errors easily flip the classification into an
incorrect class and that higher observational noise makes the model trained on
low-noise data unable to classify galaxy morphologies. On the other hand, we
show that training with domain adaptation improves model robustness and
mitigates the effects of these perturbations, improving the classification
accuracy by 23% on data with higher observational noise. Domain adaptation also
increases by a factor of ~2.3 the latent space distance between the baseline
and the incorrectly classified one-pixel perturbed image, making the model more
robust to inadvertent perturbations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2. (arXiv:2112.14683v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14683">
<div class="article-summary-box-inner">
<span><p>Videos show continuous events, yet most $-$ if not all $-$ video synthesis
frameworks treat them discretely in time. In this work, we think of videos of
what they should be $-$ time-continuous signals, and extend the paradigm of
neural representations to build a continuous-time video generator. For this, we
first design continuous motion representations through the lens of positional
embeddings. Then, we explore the question of training on very sparse videos and
demonstrate that a good generator can be learned by using as few as 2 frames
per clip. After that, we rethink the traditional image + video discriminators
pair and design a holistic discriminator that aggregates temporal information
by simply concatenating frames' features. This decreases the training cost and
provides richer learning signal to the generator, making it possible to train
directly on 1024$^2$ videos for the first time. We build our model on top of
StyleGAN2 and it is just ${\approx}5\%$ more expensive to train at the same
resolution while achieving almost the same image quality. Moreover, our latent
space features similar properties, enabling spatial manipulations that our
method can propagate in time. We can generate arbitrarily long videos at
arbitrary high frame rate, while prior work struggles to generate even 64
frames at a fixed rate. Our model is tested on four modern 256$^2$ and one
1024$^2$-resolution video synthesis benchmarks. In terms of sheer metrics, it
performs on average ${\approx}30\%$ better than the closest runner-up. Project
website: https://universome.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Generative Data-free Knowledge Distillation. (arXiv:2112.15358v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15358">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation has made remarkable achievements in model compression.
However, most existing methods require the original training data, which is
usually unavailable due to privacy and security issues. In this paper, we
propose a conditional generative data-free knowledge distillation (CGDD)
framework for training lightweight networks without any training data. This
method realizes efficient knowledge distillation based on conditional image
generation. Specifically, we treat the preset labels as ground truth to train a
conditional generator in a semi-supervised manner. The trained generator can
produce specified classes of training images. For training the student network,
we force it to extract the knowledge hidden in teacher feature maps, which
provide crucial cues for the learning process. Moreover, an adversarial
training framework for promoting distillation performance is constructed by
designing several loss functions. This framework helps the student model to
explore larger data space. To demonstrate the effectiveness of the proposed
method, we conduct extensive experiments on different datasets. Compared with
other data-free works, our work obtains state-of-the-art results on CIFAR100,
Caltech101, and different versions of ImageNet datasets. The codes will be
released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP-Event: Connecting Text and Images with Event Structures. (arXiv:2201.05078v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05078">
<div class="article-summary-box-inner">
<span><p>Vision-language (V+L) pretraining models have achieved great success in
supporting multimedia applications by understanding the alignments between
images and text. While existing vision-language pretraining models primarily
focus on understanding objects in images or entities in text, they often ignore
the alignment at the level of events and their argument structures. In this
work, we propose a contrastive learning framework to enforce vision-language
pretraining models to comprehend events and associated argument (participant)
roles. To achieve this, we take advantage of text information extraction
technologies to obtain event structural knowledge, and utilize multiple prompt
functions to contrast difficult negative descriptions by manipulating event
structures. We also design an event graph alignment loss based on optimal
transport to capture event argument structures. In addition, we collect a large
event-rich dataset (106,875 images) for pretraining, which provides a more
challenging image retrieval benchmark to assess the understanding of
complicated lengthy sentences. Experiments show that our zero-shot CLIP-Event
outperforms the state-of-the-art supervised model in argument extraction on
Multimedia Event Extraction, achieving more than 5% absolute F-score gain in
event extraction, as well as significant improvements on a variety of
downstream tasks under zero-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive and Selective Hidden Embeddings for Medical Image Segmentation. (arXiv:2201.08779v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08779">
<div class="article-summary-box-inner">
<span><p>Medical image segmentation has been widely recognized as a pivot procedure
for clinical diagnosis, analysis, and treatment planning. However, the
laborious and expensive annotation process lags down the speed of further
advances. Contrastive learning-based weight pre-training provides an
alternative by leveraging unlabeled data to learn a good representation. In
this paper, we investigate how contrastive learning benefits the general
supervised medical segmentation tasks. To this end, patch-dragsaw contrastive
regularization (PDCR) is proposed to perform patch-level tugging and repulsing
with the extent controlled by a continuous affinity score. And a new structure
dubbed uncertainty-aware feature selection block (UAFS) is designed to perform
the feature selection process, which can handle the learning target shift
caused by minority features with high uncertainty. By plugging the proposed 2
modules into the existing segmentation architecture, we achieve
state-of-the-art results across 8 public datasets from 6 domains. Newly
designed modules further decrease the amount of training data to a quarter
while achieving comparable, if not better, performances. From this perspective,
we take the opposite direction of the original self/un-supervised contrastive
learning by further excavating information contained within the label.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero Experience Required: Plug & Play Modular Transfer Learning for Semantic Visual Navigation. (arXiv:2202.02440v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02440">
<div class="article-summary-box-inner">
<span><p>In reinforcement learning for visual navigation, it is common to develop a
model for each new task, and train that model from scratch with task-specific
interactions in 3D environments. However, this process is expensive; massive
amounts of interactions are needed for the model to generalize well. Moreover,
this process is repeated whenever there is a change in the task type or the
goal modality. We present a unified approach to visual navigation using a novel
modular transfer learning model. Our model can effectively leverage its
experience from one source task and apply it to multiple target tasks (e.g.,
ObjectNav, RoomNav, ViewNav) with various goal modalities (e.g., image, sketch,
audio, label). Furthermore, our model enables zero-shot experience learning,
whereby it can solve the target tasks without receiving any task-specific
interactive training. Our experiments on multiple photorealistic datasets and
challenging tasks show that our approach learns faster, generalizes better, and
outperforms SoTA models by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion Sickness Modeling with Visual Vertical Estimation and Its Application to Autonomous Personal Mobility Vehicles. (arXiv:2202.06299v3 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06299">
<div class="article-summary-box-inner">
<span><p>Passengers (drivers) of level 3-5 autonomous personal mobility vehicles
(APMV) and cars can perform non-driving tasks, such as reading books and
smartphones, while driving. It has been pointed out that such activities may
increase motion sickness. Many studies have been conducted to build
countermeasures, of which various computational motion sickness models have
been developed. Many of these are based on subjective vertical conflict (SVC)
theory, which describes vertical changes in direction sensed by human sensory
organs vs. those expected by the central nervous system. Such models are
expected to be applied to autonomous driving scenarios. However, no current
computational model can integrate visual vertical information with vestibular
sensations.
</p>
<p>We proposed a 6 DoF SVC-VV model which add a visually perceived vertical
block into a conventional six-degrees-of-freedom SVC model to predict VV
directions from image data simulating the visual input of a human. Hence, a
simple image-based VV estimation method is proposed.
</p>
<p>As the validation of the proposed model, this paper focuses on describing the
fact that the motion sickness increases as a passenger reads a book while using
an AMPV, assuming that visual vertical (VV) plays an important role. In the
static experiment, it is demonstrated that the estimated VV by the proposed
method accurately described the gravitational acceleration direction with a low
mean absolute deviation. In addition, the results of the driving experiment
using an APMV demonstrated that the proposed 6 DoF SVC-VV model could describe
that the increased motion sickness experienced when the VV and gravitational
acceleration directions were different.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Common Corruptions and Data Augmentation. (arXiv:2203.01441v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01441">
<div class="article-summary-box-inner">
<span><p>We introduce a set of image transformations that can be used as corruptions
to evaluate the robustness of models as well as data augmentation mechanisms
for training neural networks. The primary distinction of the proposed
transformations is that, unlike existing approaches such as Common Corruptions,
the geometry of the scene is incorporated in the transformations -- thus
leading to corruptions that are more likely to occur in the real world. We also
introduce a set of semantic corruptions (e.g. natural object occlusions). We
show these transformations are `efficient' (can be computed on-the-fly),
`extendable' (can be applied on most image datasets), expose vulnerability of
existing models, and can effectively make models more robust when employed as
`3D data augmentation' mechanisms. The evaluations on several tasks and
datasets suggest incorporating 3D information into benchmarking and training
opens up a promising direction for robustness research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stochastic Video Prediction with Structure and Motion. (arXiv:2203.10528v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10528">
<div class="article-summary-box-inner">
<span><p>While stochastic video prediction models enable future prediction under
uncertainty, they mostly fail to model the complex dynamics of real-world
scenes. For example, they cannot provide reliable predictions for scenes with a
moving camera and independently moving foreground objects in driving scenarios.
The existing methods fail to fully capture the dynamics of the structured world
by only focusing on changes in pixels. In this paper, we assume that there is
an underlying process creating observations in a video and propose to factorize
it into static and dynamic components. We model the static part based on the
scene structure and the ego-motion of the vehicle, and the dynamic part based
on the remaining motion of the dynamic objects. By learning separate
distributions of changes in foreground and background, we can decompose the
scene into static and dynamic parts and separately model the change in each.
Our experiments demonstrate that disentangling structure and motion helps
stochastic video prediction, leading to better future predictions in complex
driving scenarios on two real-world driving datasets, KITTI and Cityscapes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Road Layout Parsing with Graph Auto-Encoding. (arXiv:2203.11000v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11000">
<div class="article-summary-box-inner">
<span><p>Aiming for higher-level scene understanding, this work presents a neural
network approach that takes a road-layout map in bird's-eye-view as input, and
predicts a human-interpretable graph that represents the road's topological
layout. Our approach elevates the understanding of road layouts from pixel
level to the level of graphs. To achieve this goal, an image-graph-image
auto-encoder is utilized. The network is designed to learn to regress the graph
representation at its auto-encoder bottleneck. This learning is self-supervised
by an image reconstruction loss, without needing any external manual
annotations. We create a synthetic dataset containing common road layout
patterns and use it for training of the auto-encoder in addition to the
real-world Argoverse dataset. By using this additional synthetic dataset, which
conceptually captures human knowledge of road layouts and makes this available
to the network for training, we are able to stabilize and further improve the
performance of topological road layout understanding on the real-world
Argoverse dataset. The evaluation shows that our approach exhibits comparable
performance to a strong fully-supervised baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Exemplar-Free Continual Learning in Vision Transformers: an Account of Attention, Functional and Weight Regularization. (arXiv:2203.13167v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13167">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate the continual learning of Vision Transformers
(ViT) for the challenging exemplar-free scenario, with special focus on how to
efficiently distill the knowledge of its crucial self-attention mechanism
(SAM). Our work takes an initial step towards a surgical investigation of SAM
for designing coherent continual learning methods in ViTs. We first carry out
an evaluation of established continual learning regularization techniques. We
then examine the effect of regularization when applied to two key enablers of
SAM: (a) the contextualized embedding layers, for their ability to capture
well-scaled representations with respect to the values, and (b) the prescaled
attention maps, for carrying value-independent global contextual information.
We depict the perks of each distilling strategy on two image recognition
benchmarks (CIFAR100 and ImageNet-32) -- while (a) leads to a better overall
accuracy, (b) helps enhance the rigidity by maintaining competitive
performances. Furthermore, we identify the limitation imposed by the symmetric
nature of regularization losses. To alleviate this, we propose an asymmetric
variant and apply it to the pooled output distillation (POD) loss adapted for
ViTs. Our experiments confirm that introducing asymmetry to POD boosts its
plasticity while retaining stability across (a) and (b). Moreover, we
acknowledge low forgetting measures for all the compared methods, indicating
that ViTs might be naturally inclined continual learner
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-N-Out Generative Learning for Dense Unsupervised Video Segmentation. (arXiv:2203.15312v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15312">
<div class="article-summary-box-inner">
<span><p>In this paper, we focus on unsupervised learning for Video Object
Segmentation (VOS) which learns visual correspondence (i.e., the similarity
between pixel-level features) from unlabeled videos. Previous methods are
mainly based on the contrastive learning paradigm, which optimize either in
image level or pixel level. Image-level optimization (e.g., the spatially
pooled feature of ResNet) learns robust high-level semantics but is sub-optimal
since the pixel-level features are optimized implicitly. By contrast,
pixel-level optimization is more explicit, however, it is sensitive to the
visual quality of training data and is not robust to object deformation. To
complementarily perform these two levels of optimization in a unified
framework, we propose the In-aNd-Out (INO) generative learning from a purely
generative perspective with the help of naturally designed class tokens and
patch tokens in Vision Transformer (ViT). Specifically, for image-level
optimization, we force the out-view imagination from local to global views on
class tokens, which helps capture high-level semantics, and we name it as
out-generative learning. As to pixel-level optimization, we perform in-view
masked image modeling on patch tokens, which recovers the corrupted parts of an
image via inferring its fine-grained structure, and we term it as in-generative
learning. To discover the temporal information better, we additionally force
the inter-frame consistency from both feature and affinity matrix levels.
Extensive experiments on DAVIS-2017 val and YouTube-VOS 2018 val show that our
INO outperforms previous state-of-the-art methods by significant margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monitoring social distancing with single image depth estimation. (arXiv:2204.01693v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01693">
<div class="article-summary-box-inner">
<span><p>The recent pandemic emergency raised many challenges regarding the
countermeasures aimed at containing the virus spread, and constraining the
minimum distance between people resulted in one of the most effective
strategies. Thus, the implementation of autonomous systems capable of
monitoring the so-called social distance gained much interest. In this paper,
we aim to address this task leveraging a single RGB frame without additional
depth sensors. In contrast to existing single-image alternatives failing when
ground localization is not available, we rely on single image depth estimation
to perceive the 3D structure of the observed scene and estimate the distance
between people. During the setup phase, a straightforward calibration
procedure, leveraging a scale-aware SLAM algorithm available even on consumer
smartphones, allows us to address the scale ambiguity affecting single image
depth estimation. We validate our approach through indoor and outdoor images
employing a calibrated LiDAR + RGB camera asset. Experimental results highlight
that our proposal enables sufficiently reliable estimation of the
inter-personal distance to monitor social distancing effectively. This fact
confirms that despite its intrinsic ambiguity, if appropriately driven single
image depth estimation can be a viable alternative to other depth perception
techniques, more expensive and not always feasible in practical applications.
Our evaluation also highlights that our framework can run reasonably fast and
comparably to competitors, even on pure CPU systems. Moreover, its practical
deployment on low-power systems is around the corner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Audio Strikes Back: Boosting Augmentations Towards An Efficient Audio Classification Network. (arXiv:2204.11479v4 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11479">
<div class="article-summary-box-inner">
<span><p>While efficient architectures and a plethora of augmentations for end-to-end
image classification tasks have been suggested and heavily investigated,
state-of-the-art techniques for audio classifications still rely on numerous
representations of the audio signal together with large architectures,
fine-tuned from large datasets. By utilizing the inherited lightweight nature
of audio and novel audio augmentations, we were able to present an efficient
end-to-end network with strong generalization ability. Experiments on a variety
of sound classification sets demonstrate the effectiveness and robustness of
our approach, by achieving state-of-the-art results in various settings. Public
code will be available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deeper Insights into ViTs Robustness towards Common Corruptions. (arXiv:2204.12143v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12143">
<div class="article-summary-box-inner">
<span><p>Recent literature have shown design strategies from Convolutions Neural
Networks (CNNs) benefit Vision Transformers (ViTs) in various vision tasks.
However, it remains unclear how these design choices impact on robustness when
transferred to ViTs. In this paper, we make the first attempt to investigate
how CNN-like architectural designs and CNN-based data augmentation strategies
impact on ViTs' robustness towards common corruptions through an extensive and
rigorous benchmarking. We demonstrate that overlapping patch embedding and
convolutional Feed-Forward Network (FFN) boost performance on robustness.
Furthermore, adversarial noise training is powerful on ViTs while
fourier-domain augmentation fails. Moreover, we introduce a novel conditional
method enabling input-varied augmentations from two angles: (1) Generating
dynamic augmentation parameters conditioned on input images. It conduces to
state-of-the-art performance on robustness through conditional convolutions;
(2) Selecting most suitable augmentation strategy by an extra predictor helps
to achieve the best trade-off between clean accuracy and robustness.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-05-02 23:08:45.625191757 UTC">2022-05-02 23:08:45 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>