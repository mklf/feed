<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-20T01:30:00Z">09-20</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast-Slow Transformer for Visually Grounding Speech. (arXiv:2109.08186v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08186">
<div class="article-summary-box-inner">
<span><p>We present Fast-Slow Transformer for Visually Grounding Speech, or FaST-VGS.
FaST-VGS is a Transformer-based model for learning the associations between raw
speech waveforms and visual images. The model unifies dual-encoder and
cross-attention architectures into a single model, reaping the superior
retrieval speed of the former along with the accuracy of the latter. FaST-VGS
achieves state-of-the-art speech-image retrieval accuracy on benchmark
datasets, and its learned representations exhibit strong performance on the
ZeroSpeech 2021 phonetic and semantic tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Numerical reasoning in machine reading comprehension tasks: are we there yet?. (arXiv:2109.08207v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08207">
<div class="article-summary-box-inner">
<span><p>Numerical reasoning based machine reading comprehension is a task that
involves reading comprehension along with using arithmetic operations such as
addition, subtraction, sorting, and counting. The DROP benchmark (Dua et al.,
2019) is a recent dataset that has inspired the design of NLP models aimed at
solving this task. The current standings of these models in the DROP
leaderboard, over standard metrics, suggest that the models have achieved
near-human performance. However, does this mean that these models have learned
to reason? In this paper, we present a controlled study on some of the
top-performing model architectures for the task of numerical reasoning. Our
observations suggest that the standard metrics are incapable of measuring
progress towards such tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Control of Situated Agents through Natural Language. (arXiv:2109.08214v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08214">
<div class="article-summary-box-inner">
<span><p>When humans conceive how to perform a particular task, they do so
hierarchically: splitting higher-level tasks into smaller sub-tasks. However,
in the literature on natural language (NL) command of situated agents, most
works have treated the procedures to be executed as flat sequences of simple
actions, or any hierarchies of procedures have been shallow at best. In this
paper, we propose a formalism of procedures as programs, a powerful yet
intuitive method of representing hierarchical procedural knowledge for agent
command and control. We further propose a modeling paradigm of hierarchical
modular networks, which consist of a planner and reactors that convert NL
intents to predictions of executable programs and probe the environment for
information necessary to complete the program execution. We instantiate this
framework on the IQA and ALFRED datasets for NL instruction following. Our
model outperforms reactive baselines by a large margin on both datasets. We
also demonstrate that our framework is more data-efficient, and that it allows
for fast iterative development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Bag of Tricks for Dialogue Summarization. (arXiv:2109.08232v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08232">
<div class="article-summary-box-inner">
<span><p>Dialogue summarization comes with its own peculiar challenges as opposed to
news or scientific articles summarization. In this work, we explore four
different challenges of the task: handling and differentiating parts of the
dialogue belonging to multiple speakers, negation understanding, reasoning
about the situation, and informal language understanding. Using a pretrained
sequence-to-sequence language model, we explore speaker name substitution,
negation scope highlighting, multi-task learning with relevant tasks, and
pretraining on in-domain data. Our experiments show that our proposed
techniques indeed improve summarization performance, outperforming strong
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regularized Training of Nearest Neighbor Language Models. (arXiv:2109.08249v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08249">
<div class="article-summary-box-inner">
<span><p>Including memory banks in a natural language processing architecture
increases model capacity by equipping it with additional data at inference
time. In this paper, we build upon $k$NN-LM \citep{khandelwal20generalization},
which uses a pre-trained language model together with an exhaustive $k$NN
search through the training data (memory bank) to achieve state-of-the-art
results. We investigate whether we can improve the $k$NN-LM performance by
instead training a LM with the knowledge that we will be using a $k$NN
post-hoc. We achieved significant improvement using our method on language
modeling tasks on \texttt{WIKI-2} and \texttt{WIKI-103}. The main phenomenon
that we encounter is that adding a simple L2 regularization on the activations
(not weights) of the model, a transformer, improves the post-hoc $k$NN
classification performance. We explore some possible reasons for this
improvement. In particular, we find that the added L2 regularization seems to
improve the performance for high-frequency words without deteriorating the
performance for low frequency ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balancing out Bias: Achieving Fairness Through Training Reweighting. (arXiv:2109.08253v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08253">
<div class="article-summary-box-inner">
<span><p>Bias in natural language processing arises primarily from models learning
characteristics of the author such as gender and race when modelling tasks such
as sentiment and syntactic parsing. This problem manifests as disparities in
error rates across author demographics, typically disadvantaging minority
groups. Existing methods for mitigating and measuring bias do not directly
account for correlations between author demographics and linguistic variables.
Moreover, evaluation of bias has been inconsistent in previous work, in terms
of dataset balance and evaluation methods. This paper introduces a very simple
but highly effective method for countering bias using instance reweighting,
based on the frequency of both task labels and author demographics. We extend
the method in the form of a gated model which incorporates the author
demographic as an input, and show that while it is highly vulnerable to input
data bias, it provides debiased predictions through demographic input
perturbation, and outperforms all other bias mitigation techniques when
combined with instance reweighting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ethics Sheet for Automatic Emotion Recognition and Sentiment Analysis. (arXiv:2109.08256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08256">
<div class="article-summary-box-inner">
<span><p>The importance and pervasiveness of emotions in our lives makes affective
computing a tremendously important and vibrant line of work. Systems for
automatic emotion recognition (AER) and sentiment analysis can be facilitators
of enormous progress (e.g., in improving public health and commerce) but also
enablers of great harm (e.g., for suppressing dissidents and manipulating
voters). Thus, it is imperative that the affective computing community actively
engage with the ethical ramifications of their creations. In this paper, I have
synthesized and organized information from AI Ethics and Emotion Recognition
literature to present fifty ethical considerations relevant to AER. Notably,
the sheet fleshes out assumptions hidden in how AER is commonly framed, and in
the choices often made regarding the data, method, and evaluation. Special
attention is paid to the implications of AER on privacy and social groups. The
objective of the sheet is to facilitate and encourage more thoughtfulness on
why to automate, how to automate, and how to judge success well before the
building of AER systems. Additionally, the sheet acts as a useful introductory
document on emotion recognition (complementing survey articles).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-training with Few-shot Rationalization: Teacher Explanations Aid Student in Few-shot NLU. (arXiv:2109.08259v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08259">
<div class="article-summary-box-inner">
<span><p>While pre-trained language models have obtained state-of-the-art performance
for several natural language understanding tasks, they are quite opaque in
terms of their decision-making process. While some recent works focus on
rationalizing neural predictions by highlighting salient concepts in the text
as justifications or rationales, they rely on thousands of labeled training
examples for both task labels as well as an-notated rationales for every
instance. Such extensive large-scale annotations are infeasible to obtain for
many tasks. To this end, we develop a multi-task teacher-student framework
based on self-training language models with limited task-specific labels and
rationales, and judicious sample selection to learn from informative
pseudo-labeled examples1. We study several characteristics of what constitutes
a good rationale and demonstrate that the neural model performance can be
significantly improved by making it aware of its rationalized predictions,
particularly in low-resource settings. Extensive experiments in several
bench-mark datasets demonstrate the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models as a Knowledge Source for Cognitive Agents. (arXiv:2109.08270v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08270">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) are sentence-completion engines trained on massive
corpora. LMs have emerged as a significant breakthrough in natural-language
processing, providing capabilities that go far beyond sentence completion
including question answering, summarization, and natural-language inference.
While many of these capabilities have potential application to cognitive
systems, exploiting language models as a source of task knowledge, especially
for task learning, offers significant, near-term benefits. We introduce
language models and the various tasks to which they have been applied and then
review methods of knowledge extraction from language models. The resulting
analysis outlines both the challenges and opportunities for using language
models as a new knowledge source for cognitive systems. It also identifies
possible ways to improve knowledge extraction from language models using the
capabilities provided by cognitive systems. Central to success will be the
ability of a cognitive agent to itself learn an abstract model of the knowledge
implicit in the LM as well as methods to extract high-quality knowledge
effectively and efficiently. To illustrate, we introduce a hypothetical robot
agent and describe how language models could extend its task knowledge and
improve its performance and the kinds of knowledge and methods the agent can
use to exploit the knowledge within a language model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SentiPrompt: Sentiment Knowledge Enhanced Prompt-Tuning for Aspect-Based Sentiment Analysis. (arXiv:2109.08306v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08306">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) is an emerging fine-grained sentiment
analysis task that aims to extract aspects, classify corresponding sentiment
polarities and find opinions as the causes of sentiment. The latest research
tends to solve the ABSA task in a unified way with end-to-end frameworks. Yet,
these frameworks get fine-tuned from downstream tasks without any task-adaptive
modification. Specifically, they do not use task-related knowledge well or
explicitly model relations between aspect and opinion terms, hindering them
from better performance. In this paper, we propose SentiPrompt to use sentiment
knowledge enhanced prompts to tune the language model in the unified framework.
We inject sentiment knowledge regarding aspects, opinions, and polarities into
prompt and explicitly model term relations via constructing consistency and
polarity judgment templates from the ground truth triplets. Experimental
results demonstrate that our approach can outperform strong baselines on
Triplet Extraction, Pair Extraction, and Aspect Term Extraction with Sentiment
Classification by a notable margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multimodal Sentiment Dataset for Video Recommendation. (arXiv:2109.08333v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08333">
<div class="article-summary-box-inner">
<span><p>Recently, multimodal sentiment analysis has seen remarkable advance and a lot
of datasets are proposed for its development. In general, current multimodal
sentiment analysis datasets usually follow the traditional system of
sentiment/emotion, such as positive, negative and so on. However, when applied
in the scenario of video recommendation, the traditional sentiment/emotion
system is hard to be leveraged to represent different contents of videos in the
perspective of visual senses and language understanding. Based on this, we
propose a multimodal sentiment analysis dataset, named baiDu Video Sentiment
dataset (DuVideoSenti), and introduce a new sentiment system which is designed
to describe the sentimental style of a video on recommendation scenery.
Specifically, DuVideoSenti consists of 5,630 videos which displayed on Baidu,
each video is manually annotated with a sentimental style label which describes
the user's real feeling of a video. Furthermore, we propose UNIMO as our
baseline for DuVideoSenti. Experimental results show that DuVideoSenti brings
new challenges to multimodal sentiment analysis, and could be used as a new
benchmark for evaluating approaches designed for video understanding and
multimodal fusion. We also expect our proposed DuVideoSenti could further
improve the development of multimodal sentiment analysis and its application to
video recommendations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-adaptive Pre-training of Language Models with Word Embedding Regularization. (arXiv:2109.08354v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08354">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PTLMs) acquire domain-independent linguistic
knowledge through pre-training with massive textual resources. Additional
pre-training is effective in adapting PTLMs to domains that are not well
covered by the pre-training corpora. Here, we focus on the static word
embeddings of PTLMs for domain adaptation to teach PTLMs domain-specific
meanings of words. We propose a novel fine-tuning process: task-adaptive
pre-training with word embedding regularization (TAPTER). TAPTER runs
additional pre-training by making the static word embeddings of a PTLM close to
the word embeddings obtained in the target domain with fastText. TAPTER
requires no additional corpus except for the training data of the downstream
task. We confirmed that TAPTER improves the performance of the standard
fine-tuning and the task-adaptive pre-training on BioASQ (question answering in
the biomedical domain) and on SQuAD (the Wikipedia domain) when their
pre-training corpora were not dominated by in-domain data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Linguistic Context for Language Model Compression. (arXiv:2109.08359v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08359">
<div class="article-summary-box-inner">
<span><p>A computationally expensive and memory intensive neural network lies behind
the recent success of language representation learning. Knowledge distillation,
a major technique for deploying such a vast language model in resource-scarce
environments, transfers the knowledge on individual word representations
learned without restrictions. In this paper, inspired by the recent
observations that language representations are relatively positioned and have
more semantic knowledge as a whole, we present a new knowledge distillation
objective for language representation learning that transfers the contextual
knowledge via two types of relationships across representations: Word Relation
and Layer Transforming Relation. Unlike other recent distillation techniques
for the language models, our contextual distillation does not have any
restrictions on architectural changes between teacher and student. We validate
the effectiveness of our method on challenging benchmarks of language
understanding tasks, not only in architectures of various sizes, but also in
combination with DynaBERT, the recently proposed adaptive size pruning method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeQA: A Question Answering Dataset for Source Code Comprehension. (arXiv:2109.08365v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08365">
<div class="article-summary-box-inner">
<span><p>We propose CodeQA, a free-form question answering dataset for the purpose of
source code comprehension: given a code snippet and a question, a textual
answer is required to be generated. CodeQA contains a Java dataset with 119,778
question-answer pairs and a Python dataset with 70,085 question-answer pairs.
To obtain natural and faithful questions and answers, we implement syntactic
rules and semantic analysis to transform code comments into question-answer
pairs. We present the construction process and conduct systematic analysis of
our dataset. Experiment results achieved by several neural baselines on our
dataset are shown and discussed. While research on question-answering and
machine reading comprehension develops rapidly, few prior work has drawn
attention to code question answering. This new dataset can serve as a useful
research benchmark for source code comprehension.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To be Closer: Learning to Link up Aspects with Opinions. (arXiv:2109.08382v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08382">
<div class="article-summary-box-inner">
<span><p>Dependency parse trees are helpful for discovering the opinion words in
aspect-based sentiment analysis (ABSA). However, the trees obtained from
off-the-shelf dependency parsers are static, and could be sub-optimal in ABSA.
This is because the syntactic trees are not designed for capturing the
interactions between opinion words and aspect words. In this work, we aim to
shorten the distance between aspects and corresponding opinion words by
learning an aspect-centric tree structure. The aspect and opinion words are
expected to be closer along such tree structure compared to the standard
dependency parse tree. The learning process allows the tree structure to
adaptively correlate the aspect and opinion words, enabling us to better
identify the polarity in the ABSA task. We conduct experiments on five
aspect-based sentiment datasets, and the proposed model significantly
outperforms recent strong baselines. Furthermore, our thorough analysis
demonstrates the average distance between aspect and opinion words are
shortened by at least 19% on the standard SemEval Restaurant14 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">reproducing "ner and pos when nothing is capitalized". (arXiv:2109.08396v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08396">
<div class="article-summary-box-inner">
<span><p>Capitalization is an important feature in many NLP tasks such as Named Entity
Recognition (NER) or Part of Speech Tagging (POS). We are trying to reproduce
results of paper which shows how to mitigate a significant performance drop
when casing is mismatched between training and testing data. In particular we
show that lowercasing 50% of the dataset provides the best performance,
matching the claims of the original paper. We also show that we got slightly
lower performance in almost all experiments we have tried to reproduce,
suggesting that there might be some hidden factors impacting our performance.
Lastly, we make all of our work available in a public github repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers. (arXiv:2109.08406v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08406">
<div class="article-summary-box-inner">
<span><p>Despite the success of fine-tuning pretrained language encoders like BERT for
downstream natural language understanding (NLU) tasks, it is still poorly
understood how neural networks change after fine-tuning. In this work, we use
centered kernel alignment (CKA), a method for comparing learned
representations, to measure the similarity of representations in task-tuned
models across layers. In experiments across twelve NLU tasks, we discover a
consistent block diagonal structure in the similarity of representations within
fine-tuned RoBERTa and ALBERT models, with strong similarity within clusters of
earlier and later layers, but not between them. The similarity of later layer
representations implies that later layers only marginally contribute to task
performance, and we verify in experiments that the top few layers of fine-tuned
Transformers can be discarded without hurting performance, even with no further
tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Role-Selected Sharing Network for Joint Machine-Human Chatting Handoff and Service Satisfaction Analysis. (arXiv:2109.08412v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08412">
<div class="article-summary-box-inner">
<span><p>Chatbot is increasingly thriving in different domains, however, because of
unexpected discourse complexity and training data sparseness, its potential
distrust hatches vital apprehension. Recently, Machine-Human Chatting Handoff
(MHCH), predicting chatbot failure and enabling human-algorithm collaboration
to enhance chatbot quality, has attracted increasing attention from industry
and academia. In this study, we propose a novel model, Role-Selected Sharing
Network (RSSN), which integrates both dialogue satisfaction estimation and
handoff prediction in one multi-task learning framework. Unlike prior efforts
in dialog mining, by utilizing local user satisfaction as a bridge, global
satisfaction detector and handoff predictor can effectively exchange critical
information. Specifically, we decouple the relation and interaction between the
two tasks by the role information after the shared encoder. Extensive
experiments on two public datasets demonstrate the effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">New Students on Sesame Street: What Order-Aware Matrix Embeddings Can Learn from BERT. (arXiv:2109.08449v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08449">
<div class="article-summary-box-inner">
<span><p>Large-scale pretrained language models (PreLMs) are revolutionizing natural
language processing across all benchmarks. However, their sheer size is
prohibitive in low-resource or large-scale applications. While common
approaches reduce the size of PreLMs via same-architecture distillation or
pruning, we explore distilling PreLMs into more efficient order-aware embedding
models. Our results on the GLUE benchmark show that embedding-centric students,
which have learned from BERT, yield scores comparable to DistilBERT on QQP and
RTE, often match or exceed the scores of ELMo, and only fall behind on
detecting linguistic acceptability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Unification for Logic Reasoning over Natural Language. (arXiv:2109.08460v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08460">
<div class="article-summary-box-inner">
<span><p>Automated Theorem Proving (ATP) deals with the development of computer
programs being able to show that some conjectures (queries) are a logical
consequence of a set of axioms (facts and rules). There exists several
successful ATPs where conjectures and axioms are formally provided (e.g.
formalised as First Order Logic formulas). Recent approaches, such as (Clark et
al., 2020), have proposed transformer-based architectures for deriving
conjectures given axioms expressed in natural language (English). The
conjecture is verified through a binary text classifier, where the transformers
model is trained to predict the truth value of a conjecture given the axioms.
The RuleTaker approach of (Clark et al., 2020) achieves appealing results both
in terms of accuracy and in the ability to generalize, showing that when the
model is trained with deep enough queries (at least 3 inference steps), the
transformers are able to correctly answer the majority of queries (97.6%) that
require up to 5 inference steps. In this work we propose a new architecture,
namely the Neural Unifier, and a relative training procedure, which achieves
state-of-the-art results in term of generalisation, showing that mimicking a
well-known inference procedure, the backward chaining, it is possible to answer
deep queries even when the model is trained only on shallow ones. The approach
is demonstrated in experiments using a diverse set of benchmark data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GoG: Relation-aware Graph-over-Graph Network for Visual Dialog. (arXiv:2109.08475v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08475">
<div class="article-summary-box-inner">
<span><p>Visual dialog, which aims to hold a meaningful conversation with humans about
a given image, is a challenging task that requires models to reason the complex
dependencies among visual content, dialog history, and current questions. Graph
neural networks are recently applied to model the implicit relations between
objects in an image or dialog. However, they neglect the importance of 1)
coreference relations among dialog history and dependency relations between
words for the question representation; and 2) the representation of the image
based on the fully represented question. Therefore, we propose a novel
relation-aware graph-over-graph network (GoG) for visual dialog. Specifically,
GoG consists of three sequential graphs: 1) H-Graph, which aims to capture
coreference relations among dialog history; 2) History-aware Q-Graph, which
aims to fully understand the question through capturing dependency relations
between words based on coreference resolution on the dialog history; and 3)
Question-aware I-Graph, which aims to capture the relations between objects in
an image based on fully question representation. As an additional feature
representation module, we add GoG to the existing visual dialogue model.
Experimental results show that our model outperforms the strong baseline in
both generative and discriminative settings by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue Generation. (arXiv:2109.08478v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08478">
<div class="article-summary-box-inner">
<span><p>Visual dialogue is a challenging task since it needs to answer a series of
coherent questions on the basis of understanding the visual environment.
Previous studies focus on the implicit exploration of multimodal co-reference
by implicitly attending to spatial image features or object-level image
features but neglect the importance of locating the objects explicitly in the
visual content, which is associated with entities in the textual content.
Therefore, in this paper we propose a {\bf M}ultimodal {\bf I}ncremental {\bf
T}ransformer with {\bf V}isual {\bf G}rounding, named MITVG, which consists of
two key parts: visual grounding and multimodal incremental transformer. Visual
grounding aims to explicitly locate related objects in the image guided by
textual entities, which helps the model exclude the visual content that does
not need attention. On the basis of visual grounding, the multimodal
incremental transformer encodes the multi-turn dialogue history combined with
visual scene step by step according to the order of the dialogue and then
generates a contextually and visually coherent response. Experimental results
on the VisDial v0.9 and v1.0 datasets demonstrate the superiority of the
proposed model, which achieves comparable performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple Entity-Centric Questions Challenge Dense Retrievers. (arXiv:2109.08535v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08535">
<div class="article-summary-box-inner">
<span><p>Open-domain question answering has exploded in popularity recently due to the
success of dense retrieval models, which have surpassed sparse models using
only a few supervised training examples. However, in this paper, we demonstrate
current dense models are not yet the holy grail of retrieval. We first
construct EntityQuestions, a set of simple, entity-rich questions based on
facts from Wikidata (e.g., "Where was Arve Furset born?"), and observe that
dense retrievers drastically underperform sparse methods. We investigate this
issue and uncover that dense retrievers can only generalize to common entities
unless the question pattern is explicitly observed during training. We discuss
two simple solutions towards addressing this critical problem. First, we
demonstrate that data augmentation is unable to fix the generalization problem.
Second, we argue a more robust passage encoder helps facilitate better question
adaptation using specialized question encoders. We hope our work can shed light
on the challenges in creating a robust, universal dense retriever that works
well across different input distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversational Multi-Hop Reasoning with Neural Commonsense Knowledge and Symbolic Logic Rules. (arXiv:2109.08544v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08544">
<div class="article-summary-box-inner">
<span><p>One of the challenges faced by conversational agents is their inability to
identify unstated presumptions of their users' commands, a task trivial for
humans due to their common sense. In this paper, we propose a zero-shot
commonsense reasoning system for conversational agents in an attempt to achieve
this. Our reasoner uncovers unstated presumptions from user commands satisfying
a general template of if-(state), then-(action), because-(goal). Our reasoner
uses a state-of-the-art transformer-based generative commonsense knowledge base
(KB) as its source of background knowledge for reasoning. We propose a novel
and iterative knowledge query mechanism to extract multi-hop reasoning chains
from the neural KB which uses symbolic logic rules to significantly reduce the
search space. Similar to any KBs gathered to date, our commonsense KB is prone
to missing knowledge. Therefore, we propose to conversationally elicit the
missing knowledge from human users with our novel dynamic question generation
strategy, which generates and presents contextualized queries to human users.
We evaluate the model with a user study with human users that achieves a 35%
higher success rate compared to SOTA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Slot Filling for Biomedical Information Extraction. (arXiv:2109.08564v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08564">
<div class="article-summary-box-inner">
<span><p>Information Extraction (IE) from text refers to the task of extracting
structured knowledge from unstructured text. The task typically consists of a
series of sub-tasks such as Named Entity Recognition and Relation Extraction.
Sourcing entity and relation type specific training data is a major bottleneck
in the above sub-tasks.In this work we present a slot filling approach to the
task of biomedical IE, effectively replacing the need for entity and
relation-specific training data, allowing to deal with zero-shot settings. We
follow the recently proposed paradigm of coupling a Tranformer-based
bi-encoder, Dense Passage Retrieval, with a Transformer-based reader model to
extract relations from biomedical text. We assemble a biomedical slot filling
dataset for both retrieval and reading comprehension and conduct a series of
experiments demonstrating that our approach outperforms a number of simpler
baselines. We also evaluate our approach end-to-end for standard as well as
zero-shot settings. Our work provides a fresh perspective on how to solve
biomedical IE tasks, in the absence of relevant training data. Our code, models
and pretrained data are available at
https://github.com/healx/biomed-slot-filling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Multitask Learning for Low-Resource AbstractiveSummarization. (arXiv:2109.08565v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08565">
<div class="article-summary-box-inner">
<span><p>This paper explores the effect of using multitask learning for abstractive
summarization in the context of small training corpora. In particular, we
incorporate four different tasks (extractive summarization, language modeling,
concept detection, and paraphrase detection) both individually and in
combination, with the goal of enhancing the target task of abstractive
summarization via multitask learning. We show that for many task combinations,
a model trained in a multitask setting outperforms a model trained only for
abstractive summarization, with no additional summarization data introduced.
Additionally, we do a comprehensive search and find that certain tasks (e.g.
paraphrase detection) consistently benefit abstractive summarization, not only
when combined with other tasks but also when using different architectures and
training corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Data Scarceness through Data Synthesis, Augmentation and Curriculum for Abstractive Summarization. (arXiv:2109.08569v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08569">
<div class="article-summary-box-inner">
<span><p>This paper explores three simple data manipulation techniques (synthesis,
augmentation, curriculum) for improving abstractive summarization models
without the need for any additional data. We introduce a method of data
synthesis with paraphrasing, a data augmentation technique with sample mixing,
and curriculum learning with two new difficulty metrics based on specificity
and abstractiveness. We conduct experiments to show that these three techniques
can help improve abstractive summarization across two summarization models and
two different small datasets. Furthermore, we show that these techniques can
improve performance when applied in isolation and when combined.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchy-Aware T5 with Path-Adaptive Mask Mechanism for Hierarchical Text Classification. (arXiv:2109.08585v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08585">
<div class="article-summary-box-inner">
<span><p>Hierarchical Text Classification (HTC), which aims to predict text labels
organized in hierarchical space, is a significant task lacking in investigation
in natural language processing. Existing methods usually encode the entire
hierarchical structure and fail to construct a robust label-dependent model,
making it hard to make accurate predictions on sparse lower-level labels and
achieving low Macro-F1. In this paper, we propose a novel PAMM-HiA-T5 model for
HTC: a hierarchy-aware T5 model with path-adaptive mask mechanism that not only
builds the knowledge of upper-level labels into low-level ones but also
introduces path dependency information in label prediction. Specifically, we
generate a multi-level sequential label structure to exploit hierarchical
dependency across different levels with Breadth-First Search (BFS) and T5
model. To further improve label dependency prediction within each path, we then
propose an original path-adaptive mask mechanism (PAMM) to identify the label's
path information, eliminating sources of noises from other paths. Comprehensive
experiments on three benchmark datasets show that our novel PAMM-HiA-T5 model
greatly outperforms all state-of-the-art HTC approaches especially in Macro-F1.
The ablation studies show that the improvements mainly come from our innovative
approach instead of T5.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Commonsense help in detecting Sarcasm?. (arXiv:2109.08588v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08588">
<div class="article-summary-box-inner">
<span><p>Sarcasm detection is important for several NLP tasks such as sentiment
identification in product reviews, user feedback, and online forums. It is a
challenging task requiring a deep understanding of language, context, and world
knowledge. In this paper, we investigate whether incorporating commonsense
knowledge helps in sarcasm detection. For this, we incorporate commonsense
knowledge into the prediction process using a graph convolution network with
pre-trained language model embeddings as input. Our experiments with three
sarcasm detection datasets indicate that the approach does not outperform the
baseline model. We perform an exhaustive set of experiments to analyze where
commonsense support adds value and where it hurts classification. Our
implementation is publicly available at:
https://github.com/brcsomnath/commonsense-sarcasm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Transformers for Job Expression Extraction and Classification in a Low-Resource Setting. (arXiv:2109.08597v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08597">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore possible improvements of transformer models in a
low-resource setting. In particular, we present our approaches to tackle the
first two of three subtasks of the MEDDOPROF competition, i.e., the extraction
and classification of job expressions in Spanish clinical texts. As neither
language nor domain experts, we experiment with the multilingual XLM-R
transformer model and tackle these low-resource information extraction tasks as
sequence-labeling problems. We explore domain- and language-adaptive
pretraining, transfer learning and strategic datasplits to boost the
transformer model. Our results show strong improvements using these methods by
up to 5.3 F1 points compared to a fine-tuned XLM-R model. Our best models
achieve 83.2 and 79.3 F1 for the first two tasks, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The futility of STILTs for the classification of lexical borrowings in Spanish. (arXiv:2109.08607v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08607">
<div class="article-summary-box-inner">
<span><p>The first edition of the IberLEF 2021 shared task on automatic detection of
borrowings (ADoBo) focused on detecting lexical borrowings that appeared in the
Spanish press and that have recently been imported into the Spanish language.
In this work, we tested supplementary training on intermediate labeled-data
tasks (STILTs) from part of speech (POS), named entity recognition (NER),
code-switching, and language identification approaches to the classification of
borrowings at the token level using existing pre-trained transformer-based
language models. Our extensive experimental results suggest that STILTs do not
provide any improvement over direct fine-tuning of multilingual models.
However, multilingual models trained on small subsets of languages perform
reasonably better than multilingual BERT but not as good as multilingual
RoBERTa for the given dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Scrubbing of Demographic Information for Text Classification. (arXiv:2109.08613v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08613">
<div class="article-summary-box-inner">
<span><p>Contextual representations learned by language models can often encode
undesirable attributes, like demographic associations of the users, while being
trained for an unrelated target task. We aim to scrub such undesirable
attributes and learn fair representations while maintaining performance on the
target task. In this paper, we present an adversarial learning framework
"Adversarial Scrubber" (ADS), to debias contextual representations. We perform
theoretical analysis to show that our framework converges without leaking
demographic information under certain conditions. We extend previous evaluation
techniques by evaluating debiasing performance using Minimum Description Length
(MDL) probing. Experimental evaluations on 8 datasets show that ADS generates
representations with minimal information about demographic attributes while
being maximally informative about the target task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CKMorph: A Comprehensive Morphological Analyzer for Central Kurdish. (arXiv:2109.08615v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08615">
<div class="article-summary-box-inner">
<span><p>A morphological analyzer, which is a significant component of many natural
language processing applications especially for morphologically rich languages,
divides an input word into all its composing morphemes and identifies their
morphological roles. In this paper, we introduce a comprehensive morphological
analyzer for Central Kurdish (CK), a low-resourced language with a rich
morphology. Building upon the limited existing literature, we first assembled
and systematically categorized a comprehensive collection of the morphological
and morphophonological rules of the language. Additionally, we collected and
manually labeled a generative lexicon containing nearly 10,000 verb, noun and
adjective stems, named entities, and other types of word stems. We used these
rule sets and resources to implement CKMorph Analyzer based on finite-state
transducers. In order to provide a benchmark for future research, we collected,
manually labeled, and publicly shared test sets for evaluating accuracy and
coverage of the analyzer. CKMorph was able to correctly analyze 95.9% of the
accuracy test set, containing 1,000 CK words morphologically analyzed according
to the context. Moreover, CKMorph gave at least one analysis for 95.5% of 4.22M
CK tokens of the coverage test set. The demonstration of the application and
resources including CK verb database and test sets are openly accessible at
https://github.com/CKMorph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification-based Quality Estimation: Small and Efficient Models for Real-world Applications. (arXiv:2109.08627v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08627">
<div class="article-summary-box-inner">
<span><p>Sentence-level Quality estimation (QE) of machine translation is
traditionally formulated as a regression task, and the performance of QE models
is typically measured by Pearson correlation with human labels. Recent QE
models have achieved previously-unseen levels of correlation with human
judgments, but they rely on large multilingual contextualized language models
that are computationally expensive and make them infeasible for real-world
applications. In this work, we evaluate several model compression techniques
for QE and find that, despite their popularity in other NLP tasks, they lead to
poor performance in this regression setting. We observe that a full model
parameterization is required to achieve SoTA results in a regression task.
However, we argue that the level of expressiveness of a model in a continuous
range is unnecessary given the downstream applications of QE, and show that
reframing QE as a classification problem and evaluating QE models using
classification metrics would better reflect their actual performance in
real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounding Natural Language Instructions: Can Large Language Models Capture Spatial Information?. (arXiv:2109.08634v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08634">
<div class="article-summary-box-inner">
<span><p>Models designed for intelligent process automation are required to be capable
of grounding user interface elements. This task of interface element grounding
is centred on linking instructions in natural language to their target
referents. Even though BERT and similar pre-trained language models have
excelled in several NLP tasks, their use has not been widely explored for the
UI grounding domain. This work concentrates on testing and probing the
grounding abilities of three different transformer-based models: BERT, RoBERTa
and LayoutLM. Our primary focus is on these models' spatial reasoning skills,
given their importance in this domain. We observe that LayoutLM has a promising
advantage for applications in this domain, even though it was created for a
different original purpose (representing scanned documents): the learned
spatial features appear to be transferable to the UI grounding setting,
especially as they demonstrate the ability to discriminate between target
directions in natural language instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Measuring of Readability to Improve Documents Accessibility for Arabic Language Learners. (arXiv:2109.08648v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08648">
<div class="article-summary-box-inner">
<span><p>This paper presents an approach based on supervised machine learning methods
to build a classifier that can identify text complexity in order to present
Arabic language learners with texts suitable to their levels. The approach is
based on machine learning classification methods to discriminate between the
different levels of difficulty in reading and understanding a text. Several
models were trained on a large corpus mined from online Arabic websites and
manually annotated. The model uses both Count and TF-IDF representations and
applies five machine learning algorithms; Multinomial Naive Bayes, Bernoulli
Naive Bayes, Logistic Regression, Support Vector Machine and Random Forest,
using unigrams and bigrams features. With the goal of extracting the text
complexity, the problem is usually addressed by formulating the level
identification as a classification task. Experimental results showed that
n-gram features could be indicative of the reading level of a text and could
substantially improve performance, and showed that SVM and Multinomial Naive
Bayes are the most accurate in predicting the complexity level. Best results
were achieved using TF-IDF Vectors trained by a combination of word-based
unigrams and bigrams with an overall accuracy of 87.14% over four classes of
complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Handling Unconstrained User Preferences in Dialogue. (arXiv:2109.08650v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08650">
<div class="article-summary-box-inner">
<span><p>A user input to a schema-driven dialogue information navigation system, such
as venue search, is typically constrained by the underlying database which
restricts the user to specify a predefined set of preferences, or slots,
corresponding to the database fields. We envision a more natural information
navigation dialogue interface where a user has flexibility to specify
unconstrained preferences that may not match a predefined schema. We propose to
use information retrieval from unstructured knowledge to identify entities
relevant to a user request. We update the Cambridge restaurants database with
unstructured knowledge snippets (reviews and information from the web) for each
of the restaurants and annotate a set of query-snippet pairs with a relevance
label. We use the annotated dataset to train and evaluate snippet relevance
classifiers, as a proxy to evaluating recommendation accuracy. We show that
with a pretrained transformer model as an encoder, an unsupervised/supervised
classifier achieves a weighted F1 of .661/.856.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Primer: Searching for Efficient Transformers for Language Modeling. (arXiv:2109.08668v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08668">
<div class="article-summary-box-inner">
<span><p>Large Transformer models have been central to recent advances in natural
language processing. The training and inference costs of these models, however,
have grown rapidly and become prohibitively expensive. Here we aim to reduce
the costs of Transformers by searching for a more efficient variant. Compared
to previous approaches, our search is performed at a lower level, over the
primitives that define a Transformer TensorFlow program. We identify an
architecture, named Primer, that has a smaller training cost than the original
Transformer and other variants for auto-regressive language modeling. Primer's
improvements can be mostly attributed to two simple modifications: squaring
ReLU activations and adding a depthwise convolution layer after each Q, K, and
V projection in self-attention.
</p>
<p>Experiments show Primer's gains over Transformer increase as compute scale
grows and follow a power law with respect to quality at optimal model sizes. We
also verify empirically that Primer can be dropped into different codebases to
significantly speed up training without additional tuning. For example, at a
500M parameter size, Primer improves the original T5 architecture on C4
auto-regressive language modeling, reducing the training cost by 4X.
Furthermore, the reduced training cost means Primer needs much less compute to
reach a target one-shot performance. For instance, in a 1.9B parameter
configuration similar to GPT-3 XL, Primer uses 1/3 of the training compute to
achieve the same one-shot performance as Transformer. We open source our models
and several comparisons in T5 to help with reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RnG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering. (arXiv:2109.08678v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08678">
<div class="article-summary-box-inner">
<span><p>Existing KBQA approaches, despite achieving strong performance on i.i.d. test
data, often struggle in generalizing to questions involving unseen KB schema
items. Prior ranking-based approaches have shown some success in
generalization, but suffer from the coverage issue. We present RnG-KBQA, a
Rank-and-Generate approach for KBQA, which remedies the coverage issue with a
generation model while preserving a strong generalization capability. Our
approach first uses a contrastive ranker to rank a set of candidate logical
forms obtained by searching over the knowledge graph. It then introduces a
tailored generation model conditioned on the question and the top-ranked
candidates to compose the final logical form. We achieve new state-of-the-art
results on GrailQA and WebQSP datasets. In particular, our method surpasses the
prior state-of-the-art by a large margin on the GrailQA leaderboard. In
addition, RnG-KBQA outperforms all prior approaches on the popular WebQSP
benchmark, even including the ones that use the oracle entity linking. The
experimental results demonstrate the effectiveness of the interplay between
ranking and generation, which leads to the superior performance of our proposed
approach across all settings with especially strong improvements in zero-shot
generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Capturing Global Informativeness in Open Domain Keyphrase Extraction. (arXiv:2004.13639v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.13639">
<div class="article-summary-box-inner">
<span><p>Open-domain KeyPhrase Extraction (KPE) aims to extract keyphrases from
documents without domain or quality restrictions, e.g., web pages with variant
domains and qualities. Recently, neural methods have shown promising results in
many KPE tasks due to their powerful capacity for modeling contextual semantics
of the given documents. However, we empirically show that most neural KPE
methods prefer to extract keyphrases with good phraseness, such as short and
entity-style n-grams, instead of globally informative keyphrases from
open-domain documents. This paper presents JointKPE, an open-domain KPE
architecture built on pre-trained language models, which can capture both local
phraseness and global informativeness when extracting keyphrases. JointKPE
learns to rank keyphrases by estimating their informativeness in the entire
document and is jointly trained on the keyphrase chunking task to guarantee the
phraseness of keyphrase candidates. Experiments on two large KPE datasets with
diverse domains, OpenKP and KP20k, demonstrate the effectiveness of JointKPE on
different pre-trained variants in open-domain scenarios. Further analyses
reveal the significant advantages of JointKPE in predicting long and non-entity
keyphrases, which are challenging for previous neural KPE methods. Our code is
publicly available at https://github.com/thunlp/BERT-KPE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. (arXiv:2007.15779v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.15779">
<div class="article-summary-box-inner">
<span><p>Pretraining large neural language models, such as BERT, has led to impressive
gains on many natural language processing (NLP) tasks. However, most
pretraining efforts focus on general domain corpora, such as newswire and Web.
A prevailing assumption is that even domain-specific pretraining can benefit by
starting from general-domain language models. In this paper, we challenge this
assumption by showing that for domains with abundant unlabeled text, such as
biomedicine, pretraining language models from scratch results in substantial
gains over continual pretraining of general-domain language models. To
facilitate this investigation, we compile a comprehensive biomedical NLP
benchmark from publicly-available datasets. Our experiments show that
domain-specific pretraining serves as a solid foundation for a wide range of
biomedical NLP tasks, leading to new state-of-the-art results across the board.
Further, in conducting a thorough evaluation of modeling choices, both for
pretraining and task-specific fine-tuning, we discover that some common
practices are unnecessary with BERT models, such as using complex tagging
schemes in named entity recognition (NER). To help accelerate research in
biomedical NLP, we have released our state-of-the-art pretrained and
task-specific models for the community, and created a leaderboard featuring our
BLURB benchmark (short for Biomedical Language Understanding &amp; Reasoning
Benchmark) at https://aka.ms/BLURB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Document Clustering Based on BERT with Data Augment. (arXiv:2011.08523v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.08523">
<div class="article-summary-box-inner">
<span><p>Contrastive learning is a promising approach to unsupervised learning, as it
inherits the advantages of well-studied deep models without a dedicated and
complex model design. In this paper, based on bidirectional encoder
representations from transformers, we propose self-supervised contrastive
learning (SCL) as well as few-shot contrastive learning (FCL) with unsupervised
data augmentation (UDA) for text clustering. SCL outperforms state-of-the-art
unsupervised clustering approaches for short texts and those for long texts in
terms of several clustering evaluation measures. FCL achieves performance close
to supervised learning, and FCL with UDA further improves the performance for
short texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To what extent do human explanations of model behavior align with actual model behavior?. (arXiv:2012.13354v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.13354">
<div class="article-summary-box-inner">
<span><p>Given the increasingly prominent role NLP models (will) play in our lives, it
is important for human expectations of model behavior to align with actual
model behavior. Using Natural Language Inference (NLI) as a case study, we
investigate the extent to which human-generated explanations of models'
inference decisions align with how models actually make these decisions. More
specifically, we define three alignment metrics that quantify how well natural
language explanations align with model sensitivity to input words, as measured
by integrated gradients. Then, we evaluate eight different models (the base and
large versions of BERT, RoBERTa and ELECTRA, as well as anRNN and bag-of-words
model), and find that the BERT-base model has the highest alignment with
human-generated explanations, for all alignment metrics. Focusing in on
transformers, we find that the base versions tend to have higher alignment with
human-generated explanations than their larger counterparts, suggesting that
increasing the number of model parameters leads, in some cases, to worse
alignment with human explanations. Finally, we find that a model's alignment
with human explanations is not predicted by the model's accuracy, suggesting
that accuracy and alignment are complementary ways to evaluate models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning. (arXiv:2012.15283v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15283">
<div class="article-summary-box-inner">
<span><p>While pre-trained language models (PTLMs) have achieved noticeable success on
many NLP tasks, they still struggle for tasks that require event temporal
reasoning, which is essential for event-centric applications. We present a
continual pre-training approach that equips PTLMs with targeted knowledge about
event temporal relations. We design self-supervised learning objectives to
recover masked-out event and temporal indicators and to discriminate sentences
from their corrupted counterparts (where event or temporal indicators got
replaced). By further pre-training a PTLM with these objectives jointly, we
reinforce its attention to event and temporal information, yielding enhanced
capability on event temporal reasoning. This effective continual pre-training
framework for event temporal reasoning (ECONET) improves the PTLMs' fine-tuning
performances across five relation extraction and question answering tasks and
achieves new or on-par state-of-the-art performances in most of our downstream
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora. (arXiv:2012.15674v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15674">
<div class="article-summary-box-inner">
<span><p>Recent studies have demonstrated that pre-trained cross-lingual models
achieve impressive performance in downstream cross-lingual tasks. This
improvement benefits from learning a large amount of monolingual and parallel
corpora. Although it is generally acknowledged that parallel corpora are
critical for improving the model performance, existing methods are often
constrained by the size of parallel corpora, especially for low-resource
languages. In this paper, we propose ERNIE-M, a new training method that
encourages the model to align the representation of multiple languages with
monolingual corpora, to overcome the constraint that the parallel corpus size
places on the model performance. Our key insight is to integrate
back-translation into the pre-training process. We generate pseudo-parallel
sentence pairs on a monolingual corpus to enable the learning of semantic
alignments between different languages, thereby enhancing the semantic modeling
of cross-lingual models. Experimental results show that ERNIE-M outperforms
existing cross-lingual models and delivers new state-of-the-art results in
various cross-lingual downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Politics via Contextualized Discourse Processing. (arXiv:2012.15784v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15784">
<div class="article-summary-box-inner">
<span><p>Politicians often have underlying agendas when reacting to events. Arguments
in contexts of various events reflect a fairly consistent set of agendas for a
given entity. In spite of recent advances in Pretrained Language Models (PLMs),
those text representations are not designed to capture such nuanced patterns.
In this paper, we propose a Compositional Reader model consisting of encoder
and composer modules, that attempts to capture and leverage such information to
generate more effective representations for entities, issues, and events. These
representations are contextualized by tweets, press releases, issues, news
articles, and participating entities. Our model can process several documents
at once and generate composed representations for multiple entities over
several issues or events. Via qualitative and quantitative empirical analysis,
we show that these representations are meaningful and effective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Everything in Order? A Simple Way to Order Sentences. (arXiv:2104.07064v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07064">
<div class="article-summary-box-inner">
<span><p>The task of organizing a shuffled set of sentences into a coherent text has
been used to evaluate a machine's understanding of causal and temporal
relations. We formulate the sentence ordering task as a conditional
text-to-marker generation problem. We present Reorder-BART (Re-BART) that
leverages a pre-trained Transformer-based model to identify a coherent order
for a given set of shuffled sentences. The model takes a set of shuffled
sentences with sentence-specific markers as input and generates a sequence of
position markers of the sentences in the ordered text. Re-BART achieves the
state-of-the-art performance across 7 datasets in Perfect Match Ratio (PMR) and
Kendall's tau ($\tau$). We perform evaluations in a zero-shot setting,
showcasing that our model is able to generalize well across other datasets. We
additionally perform several experiments to understand the functioning and
limitations of our framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Learning for Generation with Long Source Sequences. (arXiv:2104.07545v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07545">
<div class="article-summary-box-inner">
<span><p>One of the challenges for current sequence to sequence (seq2seq) models is
processing long sequences, such as those in summarization and document level
machine translation tasks. These tasks require the model to reason at the token
level as well as the sentence and paragraph level. We design and study a new
Hierarchical Attention Transformer-based architecture (HAT) that outperforms
standard Transformers on several sequence to sequence tasks. Furthermore, our
model achieves state-of-the-art ROUGE scores on four summarization tasks,
including PubMed, arXiv, CNN/DM, SAMSum, and AMI. Our model outperforms
document-level machine translation baseline on the WMT20 English to German
translation task. We investigate what the hierarchical layers learn by
visualizing the hierarchical encoder-decoder attention. Finally, we study
hierarchical learning on encoder-only pre-training and analyze its performance
on classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions. (arXiv:2106.04484v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04484">
<div class="article-summary-box-inner">
<span><p>Deep learning algorithms have shown promising results in visual question
answering (VQA) tasks, but a more careful look reveals that they often do not
understand the rich signal they are being fed with. To understand and better
measure the generalization capabilities of VQA systems, we look at their
robustness to counterfactually augmented data. Our proposed augmentations are
designed to make a focused intervention on a specific property of the question
such that the answer changes. Using these augmentations, we propose a new
robustness measure, Robustness to Augmented Data (RAD), which measures the
consistency of model predictions between original and augmented examples.
Through extensive experimentation, we show that RAD, unlike classical accuracy
measures, can quantify when state-of-the-art systems are not robust to
counterfactuals. We find substantial failure cases which reveal that current
VQA systems are still brittle. Finally, we connect between robustness and
generalization, demonstrating the predictive power of RAD for performance on
unseen augmentations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literature. (arXiv:2106.13375v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13375">
<div class="article-summary-box-inner">
<span><p>Information overload is a prevalent challenge in many high-value domains. A
prominent case in point is the explosion of the biomedical literature on
COVID-19, which swelled to hundreds of thousands of papers in a matter of
months. In general, biomedical literature expands by two papers every minute,
totalling over a million new papers every year. Search in the biomedical realm,
and many other vertical domains is challenging due to the scarcity of direct
supervision from click logs. Self-supervised learning has emerged as a
promising direction to overcome the annotation bottleneck. We propose a general
approach for vertical search based on domain-specific pretraining and present a
case study for the biomedical domain. Despite being substantially simpler and
not using any relevance labels for training or development, our method performs
comparably or better than the best systems in the official TREC-COVID
evaluation, a COVID-related biomedical search competition. Using distributed
computing in modern cloud infrastructure, our system can scale to tens of
millions of articles on PubMed and has been deployed as Microsoft Biomedical
Search, a new search experience for biomedical literature:
https://aka.ms/biomedsearch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ethics Sheets for AI Tasks. (arXiv:2107.01183v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01183">
<div class="article-summary-box-inner">
<span><p>Recent innovations such as Datasheets for Datasets and Model Cards for Model
Reporting have made useful contributions to furthering ethical research. Yet,
several high-profile events, such as the mass testing of emotion recognition
systems on vulnerable sub-populations, have highlighted how technology will
often lead to more adverse outcomes for those that are already marginalized. In
this paper, I will make a case for thinking about ethical considerations not
just at the level of individual models and datasets, but also at the level of
AI tasks. I will present a new form of such an effort, Ethics Sheets for AI
Tasks, dedicated to fleshing out the assumptions and ethical considerations
hidden in how a task is commonly framed and in the choices we make regarding
the data, method, and evaluation. Finally, I will provide an example ethics
sheet for automatic emotion recognition. Ethics sheets are a mechanism to
document ethical considerations \textit{before} building datasets and systems.
Such pre-production activities (e.g., ethics analyses) and associated artifacts
(e.g., accessible documentation) are crucial for responsible AI: for
communicating risks to all stakeholders, to help decision and policy making,
and for developing more effective post-production documents such as Data Sheets
and Model Cards.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pattern-based Acquisition of Scientific Entities from Scholarly Article Titles. (arXiv:2109.00199v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00199">
<div class="article-summary-box-inner">
<span><p>We describe a rule-based approach for the automatic acquisition of salient
scientific entities from Computational Linguistics (CL) scholarly article
titles. Two observations motivated the approach: (i) noting salient aspects of
an article's contribution in its title; and (ii) pattern regularities capturing
the salient terms that could be expressed in a set of rules. Only those
lexico-syntactic patterns were selected that were easily recognizable, occurred
frequently, and positionally indicated a scientific entity type. The rules were
developed on a collection of 50,237 CL titles covering all articles in the ACL
Anthology. In total, 19,799 research problems, 18,111 solutions, 20,033
resources, 1,059 languages, 6,878 tools, and 21,687 methods were extracted at
an average precision of 75%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00590">
<div class="article-summary-box-inner">
<span><p>Web search is fundamentally multimodal and multihop. Often, even before
asking a question we choose to go directly to image search to find our answers.
Further, rarely do we find an answer from a single source but aggregate
information and reason through implications. Despite the frequency of this
everyday occurrence, at present, there is no unified question answering
benchmark that requires a single model to answer long-form natural language
questions from text and open-ended visual sources -- akin to a human's
experience. We propose to bridge this gap between the natural language and
computer vision communities with WebQA. We show that A. our multihop text
queries are difficult for a large-scale transformer model, and B. existing
multi-modal transformers and visual representations do not perform well on
open-domain visual queries. Our challenge for the community is to create a
unified multimodal reasoning model that seamlessly transitions and reasons
regardless of the source modality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tree-constrained Pointer Generator for End-to-end Contextual Speech Recognition. (arXiv:2109.00627v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00627">
<div class="article-summary-box-inner">
<span><p>Contextual knowledge is important for real-world automatic speech recognition
(ASR) applications. In this paper, a novel tree-constrained pointer generator
(TCPGen) component is proposed that incorporates such knowledge as a list of
biasing words into both attention-based encoder-decoder and transducer
end-to-end ASR models in a neural-symbolic way. TCPGen structures the biasing
words into an efficient prefix tree to serve as its symbolic input and creates
a neural shortcut between the tree and the final ASR output distribution to
facilitate recognising biasing words during decoding. Systems were trained and
evaluated on the Librispeech corpus where biasing words were extracted at the
scales of an utterance, a chapter, or a book to simulate different application
scenarios. Experimental results showed that TCPGen consistently improved word
error rates (WERs) compared to the baselines, and in particular, achieved
significant WER reductions on the biasing words. TCPGen is highly efficient: it
can handle 5,000 biasing words and distractors and only add a small overhead to
memory use and computation cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biomedical and Clinical Language Models for Spanish: On the Benefits of Domain-Specific Pretraining in a Mid-Resource Scenario. (arXiv:2109.03570v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03570">
<div class="article-summary-box-inner">
<span><p>This work presents biomedical and clinical language models for Spanish by
experimenting with different pretraining choices, such as masking at word and
subword level, varying the vocabulary size and testing with domain data,
looking for better language representations. Interestingly, in the absence of
enough clinical data to train a model from scratch, we applied mixed-domain
pretraining and cross-domain transfer approaches to generate a performant
bio-clinical model suitable for real-world clinical data. We evaluated our
models on Named Entity Recognition (NER) tasks for biomedical documents and
challenging hospital discharge reports. When compared against the competitive
mBERT and BETO models, we outperform them in all NER tasks by a significant
margin. Finally, we studied the impact of the model's vocabulary on the NER
performances by offering an interesting vocabulary-centric analysis. The
results confirm that domain-specific pretraining is fundamental to achieving
higher performances in downstream NER tasks, even within a mid-resource
scenario. To the best of our knowledge, we provide the first biomedical and
clinical transformer-based pretrained language models for Spanish, intending to
boost native Spanish NLP applications in biomedicine. Our best models are
freely available in the HuggingFace hub: https://huggingface.co/BSC-TeMU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fixing exposure bias with imitation learning needs powerful oracles. (arXiv:2109.04114v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04114">
<div class="article-summary-box-inner">
<span><p>We apply imitation learning (IL) to tackle the NMT exposure bias problem with
error-correcting oracles, and evaluate an SMT lattice-based oracle which,
despite its excellent performance in an unconstrained oracle translation task,
turned out to be too pruned and idiosyncratic to serve as the oracle for IL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cartography Active Learning. (arXiv:2109.04282v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04282">
<div class="article-summary-box-inner">
<span><p>We propose Cartography Active Learning (CAL), a novel Active Learning (AL)
algorithm that exploits the behavior of the model on individual instances
during training as a proxy to find the most informative instances for labeling.
CAL is inspired by data maps, which were recently proposed to derive insights
into dataset quality (Swayamdipta et al., 2020). We compare our method on
popular text classification tasks to commonly used AL strategies, which instead
rely on post-training behavior. We demonstrate that CAL is competitive to other
common AL methods, showing that training dynamics derived from small seed data
can be successfully used for AL. We provide insights into our new AL method by
analyzing batch-level statistics utilizing the data maps. Our results further
show that CAL results in a more data-efficient learning strategy, achieving
comparable or better results with considerably less training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Asking Questions Like Educational Experts: Automatically Generating Question-Answer Pairs on Real-World Examination Data. (arXiv:2109.05179v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05179">
<div class="article-summary-box-inner">
<span><p>Generating high quality question-answer pairs is a hard but meaningful task.
Although previous works have achieved great results on answer-aware question
generation, it is difficult to apply them into practical application in the
education field. This paper for the first time addresses the question-answer
pair generation task on the real-world examination data, and proposes a new
unified framework on RACE. To capture the important information of the input
passage we first automatically generate(rather than extracting) keyphrases,
thus this task is reduced to keyphrase-question-answer triplet joint
generation. Accordingly, we propose a multi-agent communication model to
generate and optimize the question and keyphrases iteratively, and then apply
the generated question and keyphrases to guide the generation of answers. To
establish a solid benchmark, we build our model on the strong generative
pre-training model. Experimental results show that our model makes great
breakthroughs in the question-answer pair generation task. Moreover, we make a
comprehensive analysis on our model, suggesting new directions for this
challenging task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RankNAS: Efficient Neural Architecture Search by Pairwise Ranking. (arXiv:2109.07383v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07383">
<div class="article-summary-box-inner">
<span><p>This paper addresses the efficiency challenge of Neural Architecture Search
(NAS) by formulating the task as a ranking problem. Previous methods require
numerous training examples to estimate the accurate performance of
architectures, although the actual goal is to find the distinction between
"good" and "bad" candidates. Here we do not resort to performance predictors.
Instead, we propose a performance ranking method (RankNAS) via pairwise
ranking. It enables efficient architecture search using much fewer training
examples. Moreover, we develop an architecture selection method to prune the
search space and concentrate on more promising candidates. Extensive
experiments on machine translation and language modeling tasks show that
RankNAS can design high-performance architectures while being orders of
magnitude faster than state-of-the-art NAS systems.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">TANet: A new Paradigm for Global Face Super-resolution via Transformer-CNN Aggregation Network. (arXiv:2109.08174v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08174">
<div class="article-summary-box-inner">
<span><p>Recently, face super-resolution (FSR) methods either feed whole face image
into convolutional neural networks (CNNs) or utilize extra facial priors (e.g.,
facial parsing maps, facial landmarks) to focus on facial structure, thereby
maintaining the consistency of the facial structure while restoring facial
details. However, the limited receptive fields of CNNs and inaccurate facial
priors will reduce the naturalness and fidelity of the reconstructed face. In
this paper, we propose a novel paradigm based on the self-attention mechanism
(i.e., the core of Transformer) to fully explore the representation capacity of
the facial structure feature. Specifically, we design a Transformer-CNN
aggregation network (TANet) consisting of two paths, in which one path uses
CNNs responsible for restoring fine-grained facial details while the other
utilizes a resource-friendly Transformer to capture global information by
exploiting the long-distance visual relation modeling. By aggregating the
features from the above two paths, the consistency of global facial structure
and fidelity of local facial detail restoration are strengthened
simultaneously. Experimental results of face reconstruction and recognition
verify that the proposed method can significantly outperform the
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KATANA: Simple Post-Training Robustness Using Test Time Augmentations. (arXiv:2109.08191v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08191">
<div class="article-summary-box-inner">
<span><p>Although Deep Neural Networks (DNNs) achieve excellent performance on many
real-world tasks, they are highly vulnerable to adversarial attacks. A leading
defense against such attacks is adversarial training, a technique in which a
DNN is trained to be robust to adversarial attacks by introducing adversarial
noise to its input. This procedure is effective but must be done during the
training phase. In this work, we propose a new simple and easy-to-use
technique, KATANA, for robustifying an existing pretrained DNN without
modifying its weights. For every image, we generate N randomized Test Time
Augmentations (TTAs) by applying diverse color, blur, noise, and geometric
transforms. Next, we utilize the DNN's logits output to train a simple random
forest classifier to predict the real class label. Our strategy achieves
state-of-the-art adversarial robustness on diverse attacks with minimal
compromise on the natural images' classification. We test KATANA also against
two adaptive white-box attacks and it shows excellent results when combined
with adversarial training. Code is available in
https://github.com/giladcohen/KATANA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Torch.manual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision. (arXiv:2109.08203v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08203">
<div class="article-summary-box-inner">
<span><p>In this paper I investigate the effect of random seed selection on the
accuracy when using popular deep learning architectures for computer vision. I
scan a large amount of seeds (up to $10^4$) on CIFAR 10 and I also scan fewer
seeds on Imagenet using pre-trained models to investigate large scale datasets.
The conclusions are that even if the variance is not very large, it is
surprisingly easy to find an outlier that performs much better or much worse
than the average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Divide-and-Merge Point Cloud Clustering Algorithm for LiDAR Panoptic Segmentation. (arXiv:2109.08224v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08224">
<div class="article-summary-box-inner">
<span><p>Clustering objects from the LiDAR point cloud is an important research
problem with many applications such as autonomous driving. To meet the
real-time requirement, existing research proposed to apply the
connected-component-labeling (CCL) technique on LiDAR spherical range image
with a heuristic condition to check if two neighbor points are connected.
However, LiDAR range image is different from a binary image which has a
deterministic condition to tell if two pixels belong to the same component. The
heuristic condition used on the LiDAR range image only works empirically, which
suggests the LiDAR clustering algorithm should be robust to potential failures
of the empirical heuristic condition. To overcome this challenge, this paper
proposes a divide-and-merge LiDAR clustering algorithm. This algorithm firstly
conducts clustering in each evenly divided local region, then merges the local
clustered small components by voting on edge point pairs. Assuming there are
$N$ LiDAR points of objects in total with $m$ divided local regions, the time
complexity of the proposed algorithm is $O(N)+O(m^2)$. A smaller $m$ means the
voting will involve more neighbor points, but the time complexity will become
larger. So the $m$ controls the trade-off between the time complexity and the
clustering accuracy. A proper $m$ helps the proposed algorithm work in
real-time as well as maintain good performance. We evaluate the
divide-and-merge clustering algorithm on the SemanticKITTI panoptic
segmentation benchmark by cascading it with a state-of-the-art semantic
segmentation model. The final performance evaluated through the leaderboard
achieves the best among all published methods. The proposed algorithm is
implemented with C++ and wrapped as a python function. It can be easily used
with the modern deep learning framework in python.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic Surgery. (arXiv:2109.08227v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08227">
<div class="article-summary-box-inner">
<span><p>We introduce the task of stereo video reconstruction or, equivalently,
2D-to-3D video conversion for minimally invasive surgical video. We design and
implement a series of end-to-end U-Net-based solutions for this task by varying
the input (single frame vs. multiple consecutive frames), loss function (MSE,
MAE, or perceptual losses), and network architecture. We evaluate these
solutions by surveying ten experts - surgeons who routinely perform endoscopic
surgery. We run two separate reader studies: one evaluating individual frames
and the other evaluating fully reconstructed 3D video played on a VR headset.
In the first reader study, a variant of the U-Net that takes as input multiple
consecutive video frames and outputs the missing view performs best. We draw
two conclusions from this outcome. First, motion information coming from
multiple past frames is crucial in recreating stereo vision. Second, the
proposed U-Net variant can indeed exploit such motion information for solving
this task. The result from the second study further confirms the effectiveness
of the proposed U-Net variant. The surgeons reported that they could
successfully perceive depth from the reconstructed 3D video clips. They also
expressed a clear preference for the reconstructed 3D video over the original
2D video. These two reader studies strongly support the usefulness of the
proposed task of stereo reconstruction for minimally invasive surgical video
and indicate that deep learning is a promising approach to this task. Finally,
we identify two automatic metrics, LPIPS and DISTS, that are strongly
correlated with expert judgement and that could serve as proxies for the latter
in future studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI. (arXiv:2109.08238v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08238">
<div class="article-summary-box-inner">
<span><p>We present the Habitat-Matterport 3D (HM3D) dataset. HM3D is a large-scale
dataset of 1,000 building-scale 3D reconstructions from a diverse set of
real-world locations. Each scene in the dataset consists of a textured 3D mesh
reconstruction of interiors such as multi-floor residences, stores, and other
private indoor spaces.
</p>
<p>HM3D surpasses existing datasets available for academic research in terms of
physical scale, completeness of the reconstruction, and visual fidelity. HM3D
contains 112.5k m^2 of navigable space, which is 1.4 - 3.7x larger than other
building-scale datasets such as MP3D and Gibson. When compared to existing
photorealistic 3D datasets such as Replica, MP3D, Gibson, and ScanNet, images
rendered from HM3D have 20 - 85% higher visual fidelity w.r.t. counterpart
images captured with real cameras, and HM3D meshes have 34 - 91% fewer
artifacts due to incomplete surface reconstruction.
</p>
<p>The increased scale, fidelity, and diversity of HM3D directly impacts the
performance of embodied AI agents trained using it. In fact, we find that HM3D
is `pareto optimal' in the following sense -- agents trained to perform
PointGoal navigation on HM3D achieve the highest performance regardless of
whether they are evaluated on HM3D, Gibson, or MP3D. No similar claim can be
made about training on other datasets. HM3D-trained PointNav agents achieve
100% performance on Gibson-test dataset, suggesting that it might be time to
retire that episode dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A computationally efficient framework for vector representation of persistence diagrams. (arXiv:2109.08239v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08239">
<div class="article-summary-box-inner">
<span><p>In Topological Data Analysis, a common way of quantifying the shape of data
is to use a persistence diagram (PD). PDs are multisets of points in
$\mathbb{R}^2$ computed using tools of algebraic topology. However, this
multi-set structure limits the utility of PDs in applications. Therefore, in
recent years efforts have been directed towards extracting informative and
efficient summaries from PDs to broaden the scope of their use for machine
learning tasks. We propose a computationally efficient framework to convert a
PD into a vector in $\mathbb{R}^n$, called a vectorized persistence block
(VPB). We show that our representation possesses many of the desired properties
of vector-based summaries such as stability with respect to input noise, low
computational cost and flexibility. Through simulation studies, we demonstrate
the effectiveness of VPBs in terms of performance and computational cost within
various learning tasks, namely clustering, classification and change point
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards agricultural autonomy: crop row detection under varying field conditions using deep learning. (arXiv:2109.08247v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08247">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel metric to evaluate the robustness of deep
learning based semantic segmentation approaches for crop row detection under
different field conditions encountered by a field robot. A dataset with ten
main categories encountered under various field conditions was used for
testing. The effect on these conditions on the angular accuracy of crop row
detection was compared. A deep convolutional encoder decoder network is
implemented to predict crop row masks using RGB input images. The predicted
mask is then sent to a post processing algorithm to extract the crop rows. The
deep learning model was found to be robust against shadows and growth stages of
the crop while the performance was reduced under direct sunlight, increasing
weed density, tramlines and discontinuities in crop rows when evaluated with
the novel metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are we ready for beyond-application high-volume data? The Reeds robot perception benchmark dataset. (arXiv:2109.08250v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08250">
<div class="article-summary-box-inner">
<span><p>This paper presents a dataset, called Reeds, for research on robot perception
algorithms. The dataset aims to provide demanding benchmark opportunities for
algorithms, rather than providing an environment for testing
application-specific solutions. A boat was selected as a logging platform in
order to provide highly dynamic kinematics. The sensor package includes six
high-performance vision sensors, two long-range lidars, radar, as well as GNSS
and an IMU. The spatiotemporal resolution of sensors were maximized in order to
provide large variations and flexibility in the data, offering evaluation at a
large number of different resolution presets based on the resolution found in
other datasets. Reeds also provides means of a fair and reproducible comparison
of algorithms, by running all evaluations on a common server backend. As the
dataset contains massive-scale data, the evaluation principle also serves as a
way to avoid moving data unnecessarily.
</p>
<p>It was also found that naive evaluation of algorithms, where each evaluation
is computed sequentially, was not practical as the fetch and decode task of
each frame would not scale well. Instead, each frame is only decoded once and
then fed to all algorithms in parallel, including for GPU-based algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Network Based Lidar Gesture Recognition for Realtime Robot Teleoperation. (arXiv:2109.08263v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08263">
<div class="article-summary-box-inner">
<span><p>We propose a novel low-complexity lidar gesture recognition system for mobile
robot control robust to gesture variation. Our system uses a modular approach,
consisting of a pose estimation module and a gesture classifier. Pose estimates
are predicted from lidar scans using a Convolutional Neural Network trained
using an existing stereo-based pose estimation system. Gesture classification
is accomplished using a Long Short-Term Memory network and uses a sequence of
estimated body poses as input to predict a gesture. Breaking down the pipeline
into two modules reduces the dimensionality of the input, which could be lidar
scans, stereo imagery, or any other modality from which body keypoints can be
extracted, making our system lightweight and suitable for mobile robot control
with limited computing power. The use of lidar contributes to the robustness of
the system, allowing it to operate in most outdoor conditions, to be
independent of lighting conditions, and for input to be detected 360 degrees
around the robot. The lidar-based pose estimator and gesture classifier use
data augmentation and automated labeling techniques, requiring a minimal amount
of data collection and avoiding the need for manual labeling. We report
experimental results for each module of our system and demonstrate its
effectiveness by testing it in a real-world robot teleoperation setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Level Visual Similarity Based Personalized Tourist Attraction Recommendation Using Geo-Tagged Photos. (arXiv:2109.08275v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08275">
<div class="article-summary-box-inner">
<span><p>Geo-tagged photo based tourist attraction recommendation can discover users'
travel preferences from their taken photos, so as to recommend suitable tourist
attractions to them. However, existing visual content based methods cannot
fully exploit the user and tourist attraction information of photos to extract
visual features, and do not differentiate the significances of different
photos. In this paper, we propose multi-level visual similarity based
personalized tourist attraction recommendation using geo-tagged photos (MEAL).
MEAL utilizes the visual contents of photos and interaction behavior data to
obtain the final embeddings of users and tourist attractions, which are then
used to predict the visit probabilities. Specifically, by crossing the user and
tourist attraction information of photos, we define four visual similarity
levels and introduce a corresponding quintuplet loss to embed the visual
contents of photos. In addition, to capture the significances of different
photos, we exploit the self-attention mechanism to obtain the visual
representations of users and tourist attractions. We conducted experiments on a
dataset crawled from Flickr, and the experimental results proved the advantage
of this method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Hierarchical Dual Consistency for Semi-Supervised Left Atrium Segmentation on Cross-Domain Data. (arXiv:2109.08311v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08311">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning provides great significance in left atrium (LA)
segmentation model learning with insufficient labelled data. Generalising
semi-supervised learning to cross-domain data is of high importance to further
improve model robustness. However, the widely existing distribution difference
and sample mismatch between different data domains hinder the generalisation of
semi-supervised learning. In this study, we alleviate these problems by
proposing an Adaptive Hierarchical Dual Consistency (AHDC) for the
semi-supervised LA segmentation on cross-domain data. The AHDC mainly consists
of a Bidirectional Adversarial Inference module (BAI) and a Hierarchical Dual
Consistency learning module (HDC). The BAI overcomes the difference of
distributions and the sample mismatch between two different domains. It mainly
learns two mapping networks adversarially to obtain two matched domains through
mutual adaptation. The HDC investigates a hierarchical dual learning paradigm
for cross-domain semi-supervised segmentation based on the obtained matched
domains. It mainly builds two dual-modelling networks for mining the
complementary information in both intra-domain and inter-domain. For the
intra-domain learning, a consistency constraint is applied to the
dual-modelling targets to exploit the complementary modelling information. For
the inter-domain learning, a consistency constraint is applied to the LAs
modelled by two dual-modelling networks to exploit the complementary knowledge
among different data domains. We demonstrated the performance of our proposed
AHDC on four 3D late gadolinium enhancement cardiac MR (LGE-CMR) datasets from
different centres and a 3D CT dataset. Compared to other state-of-the-art
methods, our proposed AHDC achieved higher segmentation accuracy, which
indicated its capability in the cross-domain semi-supervised LA segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mass Segmentation in Automated 3-D Breast Ultrasound Using Dual-Path U-net. (arXiv:2109.08330v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08330">
<div class="article-summary-box-inner">
<span><p>Automated 3-D breast ultrasound (ABUS) is a newfound system for breast
screening that has been proposed as a supplementary modality to mammography for
breast cancer detection. While ABUS has better performance in dense breasts,
reading ABUS images is exhausting and time-consuming. So, a computer-aided
detection system is necessary for interpretation of these images. Mass
segmentation plays a vital role in the computer-aided detection systems and it
affects the overall performance. Mass segmentation is a challenging task
because of the large variety in size, shape, and texture of masses. Moreover,
an imbalanced dataset makes segmentation harder. A novel mass segmentation
approach based on deep learning is introduced in this paper. The deep network
that is used in this study for image segmentation is inspired by U-net, which
has been used broadly for dense segmentation in recent years. The system's
performance was determined using a dataset of 50 masses including 38 malign and
12 benign lesions. The proposed segmentation method attained a mean Dice of
0.82 which outperformed a two-stage supervised edge-based method with a mean
Dice of 0.74 and an adaptive region growing method with a mean Dice of 0.65.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoGG3D-Net: Locally Guided Global Descriptor Learning for 3D Place Recognition. (arXiv:2109.08336v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08336">
<div class="article-summary-box-inner">
<span><p>Retrieval-based place recognition is an efficient and effective solution for
enabling re-localization within a pre-built map or global data association for
Simultaneous Localization and Mapping (SLAM). The accuracy of such an approach
is heavily dependent on the quality of the extracted scene-level
representation. While end-to-end solutions, which learn a global descriptor
from input point clouds, have demonstrated promising results, such approaches
are limited in their ability to enforce desirable properties at the local
feature level. In this paper, we demonstrate that the inclusion of an
additional training signal (local consistency loss) can guide the network to
learning local features which are consistent across revisits, hence leading to
more repeatable global descriptors resulting in an overall improvement in place
recognition performance. We formulate our approach in an end-to-end trainable
architecture called LoGG3D-Net. Experiments on two large-scale public
benchmarks (KITTI and MulRan) show that our method achieves mean $F1_{max}$
scores of $0.939$ and $0.968$ on KITTI and MulRan, respectively while operating
in near real-time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraFormer: Graph Convolution Transformer for 3D Pose Estimation. (arXiv:2109.08364v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08364">
<div class="article-summary-box-inner">
<span><p>Exploiting relations among 2D joints plays a crucial role yet remains
semi-developed in 2D-to-3D pose estimation. To alleviate this issue, we propose
GraFormer, a novel transformer architecture combined with graph convolution for
3D pose estimation. The proposed GraFormer comprises two repeatedly stacked
core modules, GraAttention and ChebGConv block. GraAttention enables all 2D
joints to interact in global receptive field without weakening the graph
structure information of joints, which introduces vital features for later
modules. Unlike vanilla graph convolutions that only model the apparent
relationship of joints, ChebGConv block enables 2D joints to interact in the
high-order sphere, which formulates their hidden implicit relations. We
empirically show the superiority of GraFormer through conducting extensive
experiments across popular benchmarks. Specifically, GraFormer outperforms
state of the art on Human3.6M dataset while using 18$\%$ parameters. The code
is available at https://github.com/Graformer/GraFormer .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bio-Inspired Audio-Visual Cues Integration for Visual Attention Prediction. (arXiv:2109.08371v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08371">
<div class="article-summary-box-inner">
<span><p>Visual Attention Prediction (VAP) methods simulates the human selective
attention mechanism to perceive the scene, which is significant and imperative
in many vision tasks. Most existing methods only consider visual cues, while
neglect the accompanied audio information, which can provide complementary
information for the scene understanding. In fact, there exists a strong
relation between auditory and visual cues, and humans generally perceive the
surrounding scene by simultaneously sensing these cues. Motivated by this, a
bio-inspired audio-visual cues integration method is proposed for the VAP task,
which explores the audio modality to better predict the visual attention map by
assisting vision modality. The proposed method consists of three parts: 1)
audio-visual encoding, 2) audio-visual location, and 3) multi-cues aggregation
parts. Firstly, a refined SoundNet architecture is adopted to encode audio
modality for obtaining corresponding features, and a modified 3D ResNet-50
architecture is employed to learn visual features, containing both spatial
location and temporal motion information. Secondly, an audio-visual location
part is devised to locate the sound source in the visual scene by learning the
correspondence between audio-visual information. Thirdly, a multi-cues
aggregation part is devised to adaptively aggregate audio-visual information
and center-bias prior to generate the final visual attention map. Extensive
experiments are conducted on six challenging audiovisual eye-tracking datasets,
including DIEM, AVAD, Coutrot1, Coutrot2, SumMe, and ETMD, which shows
significant superiority over state-of-the-art visual attention models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PIRenderer: Controllable Portrait Image Generation via Semantic Neural Rendering. (arXiv:2109.08379v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08379">
<div class="article-summary-box-inner">
<span><p>Generating portrait images by controlling the motions of existing faces is an
important task of great consequence to social media industries. For easy use
and intuitive control, semantically meaningful and fully disentangled
parameters should be used as modifications. However, many existing techniques
do not provide such fine-grained controls or use indirect editing methods i.e.
mimic motions of other individuals. In this paper, a Portrait Image Neural
Renderer (PIRenderer) is proposed to control the face motions with the
parameters of three-dimensional morphable face models (3DMMs). The proposed
model can generate photo-realistic portrait images with accurate movements
according to intuitive modifications. Experiments on both direct and indirect
editing tasks demonstrate the superiority of this model. Meanwhile, we further
extend this model to tackle the audio-driven facial reenactment task by
extracting sequential motions from audio inputs. We show that our model can
generate coherent videos with convincing movements from only a single reference
image and a driving audio stream. Our source code is available at
https://github.com/RenYurui/PIRender.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Snapping for Guided Multi-View Visualization Design. (arXiv:2109.08384v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08384">
<div class="article-summary-box-inner">
<span><p>Visual information displays are typically composed of multiple visualizations
that are used to facilitate an understanding of the underlying data. A common
example are dashboards, which are frequently used in domains such as finance,
process monitoring and business intelligence. However, users may not be aware
of existing guidelines and lack expert design knowledge when composing such
multi-view visualizations. In this paper, we present semantic snapping, an
approach to help non-expert users design effective multi-view visualizations
from sets of pre-existing views. When a particular view is placed on a canvas,
it is "aligned" with the remaining views -- not with respect to its geometric
layout, but based on aspects of the visual encoding itself, such as how data
dimensions are mapped to channels. Our method uses an on-the-fly procedure to
detect and suggest resolutions for conflicting, misleading, or ambiguous
designs, as well as to provide suggestions for alternative presentations. With
this approach, users can be guided to avoid common pitfalls encountered when
composing visualizations. Our provided examples and case studies demonstrate
the usefulness and validity of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expression Snippet Transformer for Robust Video-based Facial Expression Recognition. (arXiv:2109.08409v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08409">
<div class="article-summary-box-inner">
<span><p>The recent success of Transformer has provided a new direction to various
visual understanding tasks, including video-based facial expression recognition
(FER). By modeling visual relations effectively, Transformer has shown its
power for describing complicated patterns. However, Transformer still performs
unsatisfactorily to notice subtle facial expression movements, because the
expression movements of many videos can be too small to extract meaningful
spatial-temporal relations and achieve robust performance. To this end, we
propose to decompose each video into a series of expression snippets, each of
which contains a small number of facial movements, and attempt to augment the
Transformer's ability for modeling intra-snippet and inter-snippet visual
relations, respectively, obtaining the Expression snippet Transformer (EST). In
particular, for intra-snippet modeling, we devise an attention-augmented
snippet feature extractor (AA-SFE) to enhance the encoding of subtle facial
movements of each snippet by gradually attending to more salient information.
In addition, for inter-snippet modeling, we introduce a shuffled snippet order
prediction (SSOP) head and a corresponding loss to improve the modeling of
subtle motion changes across subsequent snippets by training the Transformer to
identify shuffled snippet orders. Extensive experiments on four challenging
datasets (i.e., BU-3DFE, MMI, AFEW, and DFEW) demonstrate that our EST is
superior to other CNN-based methods, obtaining state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross Modification Attention Based Deliberation Model for Image Captioning. (arXiv:2109.08411v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08411">
<div class="article-summary-box-inner">
<span><p>The conventional encoder-decoder framework for image captioning generally
adopts a single-pass decoding process, which predicts the target descriptive
sentence word by word in temporal order. Despite the great success of this
framework, it still suffers from two serious disadvantages. Firstly, it is
unable to correct the mistakes in the predicted words, which may mislead the
subsequent prediction and result in error accumulation problem. Secondly, such
a framework can only leverage the already generated words but not the possible
future words, and thus lacks the ability of global planning on linguistic
information. To overcome these limitations, we explore a universal two-pass
decoding framework, where a single-pass decoding based model serving as the
Drafting Model first generates a draft caption according to an input image, and
a Deliberation Model then performs the polishing process to refine the draft
caption to a better image description. Furthermore, inspired from the
complementarity between different modalities, we propose a novel Cross
Modification Attention (CMA) module to enhance the semantic expression of the
image features and filter out error information from the draft captions. We
integrate CMA with the decoder of our Deliberation Model and name it as Cross
Modification Attention based Deliberation Model (CMA-DM). We train our proposed
framework by jointly optimizing all trainable components from scratch with a
trade-off coefficient. Experiments on MS COCO dataset demonstrate that our
approach obtains significant improvements over single-pass decoding baselines
and achieves competitive performances compared with other state-of-the-art
two-pass decoding based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-Unet: Raw Image Processing with Unet. (arXiv:2109.08417v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08417">
<div class="article-summary-box-inner">
<span><p>Medical image segmentation have drawn massive attention as it is important in
biomedical image analysis. Good segmentation results can assist doctors with
their judgement and further improve patients' experience. Among many available
pipelines in medical image analysis, Unet is one of the most popular neural
networks as it keeps raw features by adding concatenation between encoder and
decoder, which makes it still widely used in industrial field. In the mean
time, as a popular model which dominates natural language process tasks,
transformer is now introduced to computer vision tasks and have seen promising
results in object detection, image classification and semantic segmentation
tasks. Therefore, the combination of transformer and Unet is supposed to be
more efficient than both methods working individually. In this article, we
propose Transformer-Unet by adding transformer modules in raw images instead of
feature maps in Unet and test our network in CT82 datasets for Pancreas
segmentation accordingly. We form an end-to-end network and gain segmentation
results better than many previous Unet based algorithms in our experiment. We
demonstrate our network and show our experimental results in this paper
accordingly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Messing Up 3D Virtual Environments: Transferable Adversarial 3D Objects. (arXiv:2109.08465v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08465">
<div class="article-summary-box-inner">
<span><p>In the last few years, the scientific community showed a remarkable and
increasing interest towards 3D Virtual Environments, training and testing
Machine Learning-based models in realistic virtual worlds. On one hand, these
environments could also become a mean to study the weaknesses of Machine
Learning algorithms, or to simulate training settings that allow Machine
Learning models to gain robustness to 3D adversarial attacks. On the other
hand, their growing popularity might also attract those that aim at creating
adversarial conditions to invalidate the benchmarking process, especially in
the case of public environments that allow the contribution from a large
community of people. Most of the existing Adversarial Machine Learning
approaches are focused on static images, and little work has been done in
studying how to deal with 3D environments and how a 3D object should be altered
to fool a classifier that observes it. In this paper, we study how to craft
adversarial 3D objects by altering their textures, using a tool chain composed
of easily accessible elements. We show that it is possible, and indeed simple,
to create adversarial objects using off-the-shelf limited surrogate renderers
that can compute gradients with respect to the parameters of the rendering
process, and, to a certain extent, to transfer the attacks to more advanced 3D
engines. We propose a saliency-based attack that intersects the two classes of
renderers in order to focus the alteration to those texture elements that are
estimated to be effective in the target engine, evaluating its impact in
popular neural classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LOF: Structure-Aware Line Tracking based on Optical Flow. (arXiv:2109.08466v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08466">
<div class="article-summary-box-inner">
<span><p>Lines provide the significantly richer geometric structural information about
the environment than points, so lines are widely used in recent Visual Odometry
(VO) works. Since VO with lines use line tracking results to locate and map,
line tracking is a crucial component in VO. Although the state-of-the-art line
tracking methods have made great progress, they are still heavily dependent on
line detection or the predicted line segments. In order to relieve the
dependencies described above to track line segments completely, accurately, and
robustly at higher computational efficiency, we propose a structure-aware Line
tracking algorithm based entirely on Optical Flow (LOF). Firstly, we propose a
gradient-based strategy to sample pixels on lines that are suitable for line
optical flow calculation. Then, in order to align the lines by fully using the
structural relationship between the sampled points on it and effectively
removing the influence of sampled points on it occluded by other objects, we
propose a two-step structure-aware line segment alignment method. Furthermore,
we propose a line refinement method to refine the orientation, position, and
endpoints of the aligned line segments. Extensive experimental results
demonstrate that the proposed LOF outperforms the state-of-the-art performance
in line tracking accuracy, robustness, and efficiency, which also improves the
location accuracy and robustness of VO system with lines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ActionCLIP: A New Paradigm for Video Action Recognition. (arXiv:2109.08472v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08472">
<div class="article-summary-box-inner">
<span><p>The canonical approach to video action recognition dictates a neural model to
do a classic and standard 1-of-N majority vote task. They are trained to
predict a fixed set of predefined categories, limiting their transferable
ability on new datasets with unseen concepts. In this paper, we provide a new
perspective on action recognition by attaching importance to the semantic
information of label texts rather than simply mapping them into numbers.
Specifically, we model this task as a video-text matching problem within a
multimodal learning framework, which strengthens the video representation with
more semantic language supervision and enables our model to do zero-shot action
recognition without any further labeled data or parameters requirements.
Moreover, to handle the deficiency of label texts and make use of tremendous
web data, we propose a new paradigm based on this multimodal learning framework
for action recognition, which we dub "pre-train, prompt and fine-tune". This
paradigm first learns powerful representations from pre-training on a large
amount of web image-text or video-text data. Then it makes the action
recognition task to act more like pre-training problems via prompt engineering.
Finally, it end-to-end fine-tunes on target datasets to obtain strong
performance. We give an instantiation of the new paradigm, ActionCLIP, which
not only has superior and flexible zero-shot/few-shot transfer ability but also
reaches a top performance on general action recognition task, achieving 83.8%
top-1 accuracy on Kinetics-400 with a ViT-B/16 as the backbone. Code is
available at https://github.com/sallymmx/ActionCLIP.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GoG: Relation-aware Graph-over-Graph Network for Visual Dialog. (arXiv:2109.08475v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08475">
<div class="article-summary-box-inner">
<span><p>Visual dialog, which aims to hold a meaningful conversation with humans about
a given image, is a challenging task that requires models to reason the complex
dependencies among visual content, dialog history, and current questions. Graph
neural networks are recently applied to model the implicit relations between
objects in an image or dialog. However, they neglect the importance of 1)
coreference relations among dialog history and dependency relations between
words for the question representation; and 2) the representation of the image
based on the fully represented question. Therefore, we propose a novel
relation-aware graph-over-graph network (GoG) for visual dialog. Specifically,
GoG consists of three sequential graphs: 1) H-Graph, which aims to capture
coreference relations among dialog history; 2) History-aware Q-Graph, which
aims to fully understand the question through capturing dependency relations
between words based on coreference resolution on the dialog history; and 3)
Question-aware I-Graph, which aims to capture the relations between objects in
an image based on fully question representation. As an additional feature
representation module, we add GoG to the existing visual dialogue model.
Experimental results show that our model outperforms the strong baseline in
both generative and discriminative settings by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Including Keyword Position in Image-based Models for Act Segmentation of Historical Registers. (arXiv:2109.08477v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08477">
<div class="article-summary-box-inner">
<span><p>The segmentation of complex images into semantic regions has seen a growing
interest these last years with the advent of Deep Learning. Until recently,
most existing methods for Historical Document Analysis focused on the visual
appearance of documents, ignoring the rich information that textual content can
offer. However, the segmentation of complex documents into semantic regions is
sometimes impossible relying only on visual features and recent models embed
both visual and textual information. In this paper, we focus on the use of both
visual and textual information for segmenting historical registers into
structured and meaningful units such as acts. An act is a text recording
containing valuable knowledge such as demographic information (baptism,
marriage or death) or royal decisions (donation or pardon). We propose a simple
pipeline to enrich document images with the position of text lines containing
key-phrases and show that running a standard image-based layout analysis system
on these images can lead to significant gains. Our experiments show that the
detection of acts increases from 38 % of mAP to 74 % when adding textual
information, in real use-case conditions where text lines positions and content
are extracted with an automatic recognition system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue Generation. (arXiv:2109.08478v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08478">
<div class="article-summary-box-inner">
<span><p>Visual dialogue is a challenging task since it needs to answer a series of
coherent questions on the basis of understanding the visual environment.
Previous studies focus on the implicit exploration of multimodal co-reference
by implicitly attending to spatial image features or object-level image
features but neglect the importance of locating the objects explicitly in the
visual content, which is associated with entities in the textual content.
Therefore, in this paper we propose a {\bf M}ultimodal {\bf I}ncremental {\bf
T}ransformer with {\bf V}isual {\bf G}rounding, named MITVG, which consists of
two key parts: visual grounding and multimodal incremental transformer. Visual
grounding aims to explicitly locate related objects in the image guided by
textual entities, which helps the model exclude the visual content that does
not need attention. On the basis of visual grounding, the multimodal
incremental transformer encodes the multi-turn dialogue history combined with
visual scene step by step according to the order of the dialogue and then
generates a contextually and visually coherent response. Experimental results
on the VisDial v0.9 and v1.0 datasets demonstrate the superiority of the
proposed model, which achieves comparable performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CardiSort: a convolutional neural network for cross vendor automated sorting of cardiac MR images. (arXiv:2109.08479v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08479">
<div class="article-summary-box-inner">
<span><p>Objectives: To develop an image-based automatic deep learning method to
classify cardiac MR images by sequence type and imaging plane for improved
clinical post-processing efficiency. Methods: Multi-vendor cardiac MRI studies
were retrospectively collected from 4 centres and 3 vendors. A two-head
convolutional neural network ('CardiSort') was trained to classify 35 sequences
by imaging sequence (n=17) and plane (n=10). Single vendor training (SVT) on
single centre images (n=234 patients) and multi-vendor training (MVT) with
multicentre images (n = 479 patients, 3 centres) was performed. Model accuracy
was compared to manual ground truth labels by an expert radiologist on a
hold-out test set for both SVT and MVT. External validation of MVT
(MVTexternal) was performed on data from 3 previously unseen magnet systems
from 2 vendors (n=80 patients). Results: High sequence and plane accuracies
were observed for SVT (85.2% and 93.2% respectively), and MVT (96.5% and 98.1%
respectively) on the hold-out test set. MVTexternal yielded sequence accuracy
of 92.7% and plane accuracy of 93.0%. There was high accuracy for common
sequences and conventional cardiac planes. Poor accuracy was observed for
underrepresented classes and sequences where there was greater variability in
acquisition parameters across centres, such as perfusion imaging. Conclusions:
A deep learning network was developed on multivendor data to classify MRI
studies into component sequences and planes, with external validation. With
refinement, it has potential to improve workflow by enabling automated sequence
selection, an important first step in completely automated post-processing
pipelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What we see and What we don't see: Imputing Occluded Crowd Structures from Robot Sensing. (arXiv:2109.08494v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08494">
<div class="article-summary-box-inner">
<span><p>We consider the navigation of mobile robots in crowded environments, for
which onboard sensing of the crowd is typically limited by occlusions. We
address the problem of inferring the human occupancy in the space around the
robot, in blind spots, beyond the range of its sensing capabilities. This
problem is rather unexplored in spite of the important impact it has on the
robot crowd navigation efficiency and safety, which requires the estimation and
the prediction of the crowd state around it. In this work, we propose the first
solution to sample predictions of possible human presence based on the state of
a fewer set of sensed people around the robot as well as previous observations
of the crowd activity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focus on Impact: Indoor Exploration with Intrinsic Motivation. (arXiv:2109.08521v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08521">
<div class="article-summary-box-inner">
<span><p>Exploration of indoor environments has recently experienced a significant
interest, also thanks to the introduction of deep neural agents built in a
hierarchical fashion and trained with Deep Reinforcement Learning (DRL) on
simulated environments. Current state-of-the-art methods employ a dense
extrinsic reward that requires the complete a priori knowledge of the layout of
the training environment to learn an effective exploration policy. However,
such information is expensive to gather in terms of time and resources. In this
work, we propose to train the model with a purely intrinsic reward signal to
guide exploration, which is based on the impact of the robot's actions on the
environment. So far, impact-based rewards have been employed for simple tasks
and in procedurally generated synthetic environments with countable states.
Since the number of states observable by the agent in realistic indoor
environments is non-countable, we include a neural-based density model and
replace the traditional count-based regularization with an estimated
pseudo-count of previously visited states. The proposed exploration approach
outperforms DRL-based competitors relying on intrinsic rewards and surpasses
the agents trained with a dense extrinsic reward computed with the environment
layouts. We also show that a robot equipped with the proposed approach
seamlessly adapts to point-goal navigation and real-world deployment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pointly-supervised 3D Scene Parsing with Viewpoint Bottleneck. (arXiv:2109.08553v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08553">
<div class="article-summary-box-inner">
<span><p>Semantic understanding of 3D point clouds is important for various robotics
applications. Given that point-wise semantic annotation is expensive, in this
paper, we address the challenge of learning models with extremely sparse
labels. The core problem is how to leverage numerous unlabeled points. To this
end, we propose a self-supervised 3D representation learning framework named
viewpoint bottleneck. It optimizes a mutual-information based objective, which
is applied on point clouds under different viewpoints. A principled analysis
shows that viewpoint bottleneck leads to an elegant surrogate loss function
that is suitable for large-scale point cloud data. Compared with former arts
based upon contrastive learning, viewpoint bottleneck operates on the feature
dimension instead of the sample dimension. This paradigm shift has several
advantages: It is easy to implement and tune, does not need negative samples
and performs better on our goal down-streaming task. We evaluate our method on
the public benchmark ScanNet, under the pointly-supervised setting. We achieve
the best quantitative results among comparable solutions. Meanwhile we provide
an extensive qualitative inspection on various challenging scenes. They
demonstrate that our models can produce fairly good scene parsing results for
robotics applications. Our code, data and models will be made public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Neural Architecture Search for Imbalanced Datasets. (arXiv:2109.08580v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08580">
<div class="article-summary-box-inner">
<span><p>Neural Architecture Search (NAS) provides state-of-the-art results when
trained on well-curated datasets with annotated labels. However, annotating
data or even having balanced number of samples can be a luxury for
practitioners from different scientific fields, e.g., in the medical domain. To
that end, we propose a NAS-based framework that bears the threefold
contributions: (a) we focus on the self-supervised scenario, i.e., where no
labels are required to determine the architecture, and (b) we assume the
datasets are imbalanced, (c) we design each component to be able to run on a
resource constrained setup, i.e., on a single GPU (e.g. Google Colab). Our
components build on top of recent developments in self-supervised
learning~\citep{zbontar2021barlow}, self-supervised NAS~\citep{kaplan2020self}
and extend them for the case of imbalanced datasets. We conduct experiments on
an (artificially) imbalanced version of CIFAR-10 and we demonstrate our
proposed method outperforms standard neural networks, while using $27\times$
less parameters. To validate our assumption on a naturally imbalanced dataset,
we also conduct experiments on ChestMNIST and COVID-19 X-ray. The results
demonstrate how the proposed method can be used in imbalanced datasets, while
it can be fully run on a single GPU. Code is available
\href{https://github.com/TimofeevAlex/ssnas_imbalanced}{here}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse Generation from a Single Video Made Possible. (arXiv:2109.08591v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08591">
<div class="article-summary-box-inner">
<span><p>Most advanced video generation and manipulation methods train on a large
collection of videos. As such, they are restricted to the types of video
dynamics they train on. To overcome this limitation, GANs trained on a single
video were recently proposed. While these provide more flexibility to a wide
variety of video dynamics, they require days to train on a single tiny input
video, rendering them impractical. In this paper we present a fast and
practical method for video generation and manipulation from a single natural
video, which generates diverse high-quality video outputs within seconds (for
benchmark videos). Our method can be further applied to Full-HD video clips
within minutes. Our approach is inspired by a recent advanced
patch-nearest-neighbor based approach [Granot et al. 2021], which was shown to
significantly outperform single-image GANs, both in run-time and in visual
quality. Here we generalize this approach from images to videos, by casting
classical space-time patch-based methods as a new generative video model. We
adapt the generative image patch nearest neighbor approach to efficiently cope
with the huge number of space-time patches in a single video. Our method
generates more realistic and higher quality results than single-video GANs
(confirmed by quantitative and qualitative evaluations). Moreover, it is
disproportionally faster (runtime reduced from several days to seconds). Other
than diverse video generation, we demonstrate several other challenging video
applications, including spatio-temporal video retargeting, video structural
analogies and conditional video-inpainting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A review of deep learning methods for MRI reconstruction. (arXiv:2109.08618v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08618">
<div class="article-summary-box-inner">
<span><p>Following the success of deep learning in a wide range of applications,
neural network-based machine-learning techniques have received significant
interest for accelerating magnetic resonance imaging (MRI) acquisition and
reconstruction strategies. A number of ideas inspired by deep learning
techniques for computer vision and image processing have been successfully
applied to nonlinear image reconstruction in the spirit of compressed sensing
for accelerated MRI. Given the rapidly growing nature of the field, it is
imperative to consolidate and summarize the large number of deep learning
methods that have been reported in the literature, to obtain a better
understanding of the field in general. This article provides an overview of the
recent developments in neural-network based approaches that have been proposed
specifically for improving parallel imaging. A general background and
introduction to parallel MRI is also given from a classical view of k-space
based reconstruction methods. Image domain based techniques that introduce
improved regularizers are covered along with k-space based methods which focus
on better interpolation strategies using neural networks. While the field is
rapidly evolving with thousands of papers published each year, in this review,
we attempt to cover broad categories of methods that have shown good
performance on publicly available data sets. Limitations and open problems are
also discussed and recent efforts for producing open data sets and benchmarks
for the community are examined.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autonomous Vision-based UAV Landing with Collision Avoidance using Deep Learning. (arXiv:2109.08628v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08628">
<div class="article-summary-box-inner">
<span><p>There is a risk of collision when multiple UAVs land simultaneously without
communication on the same platform. This work accomplishes vision-based
autonomous landing and uses a deep-learning-based method to realize collision
avoidance during the landing process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monitoring Indoor Activity of Daily Living Using Thermal Imaging: A Case Study. (arXiv:2109.08672v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08672">
<div class="article-summary-box-inner">
<span><p>Monitoring indoor activities of daily living (ADLs) of a person is neither an
easy nor an accurate process. It is subjected to dependency on sensor type,
power supply stability, and connectivity stability without mentioning artifacts
introduced by the person himself. Multiple challenges have to be overcome in
this field, such as; monitoring the precise spatial location of the person, and
estimating vital signs like an individuals average temperature. Privacy is
another domain of the problem to be thought of with care. Identifying the
persons posture without a camera is another challenge. Posture identification
assists in the persons fall detection. Thermal imaging could be a proper
solution for most of the mentioned challenges. It provides monitoring both the
persons average temperature and spatial location while maintaining privacy. In
this research, we propose an IoT system for monitoring an indoor ADL using
thermal sensor array (TSA). Three classes of ADLs are introduced, which are
daily activity, sleeping activity and no-activity respectively. Estimating
person average temperature using TSAs is introduced as well in this paper.
Results have shown that the three activity classes can be identified as well as
the persons average temperature during day and night. The persons spatial
location can be determined while his/her privacy is maintained as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Realistic PointGoal Navigation via Auxiliary Losses and Information Bottleneck. (arXiv:2109.08677v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08677">
<div class="article-summary-box-inner">
<span><p>We propose a novel architecture and training paradigm for training realistic
PointGoal Navigation -- navigating to a target coordinate in an unseen
environment under actuation and sensor noise without access to ground-truth
localization. Specifically, we find that the primary challenge under this
setting is learning localization -- when stripped of idealized localization,
agents fail to stop precisely at the goal despite reliably making progress
towards it. To address this we introduce a set of auxiliary losses to help the
agent learn localization. Further, we explore the idea of treating the precise
location of the agent as privileged information -- it is unavailable during
test time, however, it is available during training time in simulation. We
grant the agent restricted access to ground-truth localization readings during
training via an information bottleneck. Under this setting, the agent incurs a
penalty for using this privileged information, encouraging the agent to only
leverage this information when it is crucial to learning. This enables the
agent to first learn navigation and then learn localization instead of
conflating these two objectives in training. We evaluate our proposed method
both in a semi-idealized (noiseless simulation without Compass+GPS) and
realistic (addition of noisy simulation) settings. Specifically, our method
outperforms existing baselines on the semi-idealized setting by 18\%/21\%
SPL/Success and by 15\%/20\% SPL in the realistic setting. Our improved Success
and SPL metrics indicate our agent's improved ability to accurately
self-localize while maintaining a strong navigation policy. Our implementation
can be found at https://github.com/NicoGrande/habitat-pointnav-via-ib.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust Performance. (arXiv:1909.10837v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.10837">
<div class="article-summary-box-inner">
<span><p>Spiking neural network (SNN) is interesting both theoretically and
practically because of its strong bio-inspiration nature and potentially
outstanding energy efficiency. Unfortunately, its development has fallen far
behind the conventional deep neural network (DNN), mainly because of difficult
training and lack of widely accepted hardware experiment platforms. In this
paper, we show that a deep temporal-coded SNN can be trained easily and
directly over the benchmark datasets CIFAR10 and ImageNet, with testing
accuracy within 1% of the DNN of equivalent size and architecture. Training
becomes similar to DNN thanks to the closed-form solution to the spiking
waveform dynamics. Considering that SNNs should be implemented in practical
neuromorphic hardwares, we train the deep SNN with weights quantized to 8, 4, 2
bits and with weights perturbed by random noise to demonstrate its robustness
in practical applications. In addition, we develop a phase-domain signal
processing circuit schematic to implement our spiking neuron with 90% gain of
energy efficiency over existing work. This paper demonstrates that the
temporal-coded deep SNN is feasible for applications with high performance and
high energy efficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VideoDG: Generalizing Temporal Relations in Videos to Novel Domains. (arXiv:1912.03716v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.03716">
<div class="article-summary-box-inner">
<span><p>This paper introduces video domain generalization where most video
classification networks degenerate due to the lack of exposure to the target
domains of divergent distributions. We observe that the global temporal
features are less generalizable, due to the temporal domain shift that videos
from other unseen domains may have an unexpected absence or misalignment of the
temporal relations. This finding has motivated us to solve video domain
generalization by effectively learning the local-relation features of different
timescales that are more generalizable, and exploiting them along with the
global-relation features to maintain the discriminability. This paper presents
the VideoDG framework with two technical contributions. The first is a new deep
architecture named the Adversarial Pyramid Network, which improves the
generalizability of video features by capturing the local-relation,
global-relation, and cross-relation features progressively. On the basis of
pyramid features, the second contribution is a new and robust approach of
adversarial data augmentation that can bridge different video domains by
improving the diversity and quality of augmented data. We construct three video
domain generalization benchmarks in which domains are divided according to
different datasets, different consequences of actions, or different camera
views, respectively. VideoDG consistently outperforms the combinations of
previous video classification models and existing domain generalization methods
on all benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TSS: Transformation-Specific Smoothing for Robustness Certification. (arXiv:2002.12398v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.12398">
<div class="article-summary-box-inner">
<span><p>As machine learning (ML) systems become pervasive, safeguarding their
security is critical. However, recently it has been demonstrated that motivated
adversaries are able to mislead ML systems by perturbing test data using
semantic transformations. While there exists a rich body of research providing
provable robustness guarantees for ML models against $\ell_p$ norm bounded
adversarial perturbations, guarantees against semantic perturbations remain
largely underexplored. In this paper, we provide TSS -- a unified framework for
certifying ML robustness against general adversarial semantic transformations.
First, depending on the properties of each transformation, we divide common
transformations into two categories, namely resolvable (e.g., Gaussian blur)
and differentially resolvable (e.g., rotation) transformations. For the former,
we propose transformation-specific randomized smoothing strategies and obtain
strong robustness certification. The latter category covers transformations
that involve interpolation errors, and we propose a novel approach based on
stratified sampling to certify the robustness. Our framework TSS leverages
these certification strategies and combines with consistency-enhanced training
to provide rigorous certification of robustness. We conduct extensive
experiments on over ten types of challenging semantic transformations and show
that TSS significantly outperforms the state of the art. Moreover, to the best
of our knowledge, TSS is the first approach that achieves nontrivial certified
robustness on the large-scale ImageNet dataset. For instance, our framework
achieves 30.4% certified robust accuracy against rotation attack (within $\pm
30^\circ$) on ImageNet. Moreover, to consider a broader range of
transformations, we show TSS is also robust against adaptive attacks and
unforeseen image corruptions such as CIFAR-10-C and ImageNet-C.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Standardised convolutional filtering for radiomics. (arXiv:2006.05470v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.05470">
<div class="article-summary-box-inner">
<span><p>The Image Biomarker Standardisation Initiative (IBSI) aims to improve
reproducibility of radiomics studies by standardising the computational process
of extracting image biomarkers (features) from images. We have previously
established reference values for 169 commonly used features, created a standard
radiomics image processing scheme, and developed reporting guidelines for
radiomic studies. However, several aspects are not standardised.
</p>
<p>Here we present a preliminary version of a reference manual on the use of
convolutional image filters in radiomics. Filters, such as wavelets or
Laplacian of Gaussian filters, play an important part in emphasising specific
image characteristics such as edges and blobs. Features derived from filter
response maps have been found to be poorly reproducible. This reference manual
forms the basis of ongoing work on standardising convolutional filters in
radiomics, and will be updated as this work progresses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rescaling Egocentric Vision. (arXiv:2006.13256v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.13256">
<div class="article-summary-box-inner">
<span><p>This paper introduces the pipeline to extend the largest dataset in
egocentric vision, EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a
collection of 100 hours, 20M frames, 90K actions in 700 variable-length videos,
capturing long-term unscripted activities in 45 environments, using
head-mounted cameras. Compared to its previous version, EPIC-KITCHENS-100 has
been annotated using a novel pipeline that allows denser (54% more actions per
minute) and more complete annotations of fine-grained actions (+128% more
action segments). This collection enables new challenges such as action
detection and evaluating the "test of time" - i.e. whether models trained on
data collected in 2018 can generalise to new footage collected two years later.
The dataset is aligned with 6 challenges: action recognition (full and weak
supervision), action detection, action anticipation, cross-modal retrieval
(from captions), as well as unsupervised domain adaptation for action
recognition. For each challenge, we define the task, provide baselines and
evaluation metrics
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimating Example Difficulty Using Variance of Gradients. (arXiv:2008.11600v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.11600">
<div class="article-summary-box-inner">
<span><p>In machine learning, a question of great interest is understanding what
examples are challenging for a model to classify. Identifying atypical examples
ensures the safe deployment of models, isolates samples that require further
human inspection and provides interpretability into model behavior. In this
work, we propose Variance of Gradients (VoG) as a valuable and efficient metric
to rank data by difficulty and to surface a tractable subset of the most
challenging examples for human-in-the-loop auditing. We show that data points
with high VoG scores are far more difficult for the model to learn and
over-index on corrupted or memorized examples. Further, restricting the
evaluation to the test set instances with the lowest VoG improves the model's
generalization performance. Finally, we show that VoG is a valuable and
efficient ranking for out-of-distribution detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minimal Adversarial Examples for Deep Learning on 3D Point Clouds. (arXiv:2008.12066v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.12066">
<div class="article-summary-box-inner">
<span><p>With recent developments of convolutional neural networks, deep learning for
3D point clouds has shown significant progress in various 3D scene
understanding tasks, e.g., object recognition, semantic segmentation. In a
safety-critical environment, it is however not well understood how such deep
learning models are vulnerable to adversarial examples. In this work, we
explore adversarial attacks for point cloud-based neural networks. We propose a
unified formulation for adversarial point cloud generation that can generalise
two different attack strategies. Our method generates adversarial examples by
attacking the classification ability of point cloud-based networks while
considering the perceptibility of the examples and ensuring the minimal level
of point manipulations. Experimental results show that our method achieves the
state-of-the-art performance with higher than 89% and 90% of attack success
rate on synthetic and real-world data respectively, while manipulating only
about 4% of the total points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProCAN: Progressive Growing Channel Attentive Non-Local Network for Lung Nodule Classification. (arXiv:2010.15417v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.15417">
<div class="article-summary-box-inner">
<span><p>Lung cancer classification in screening computed tomography (CT) scans is one
of the most crucial tasks for early detection of this disease. Many lives can
be saved if we are able to accurately classify malignant/cancerous lung
nodules. Consequently, several deep learning based models have been proposed
recently to classify lung nodules as malignant or benign. Nevertheless, the
large variation in the size and heterogeneous appearance of the nodules makes
this task an extremely challenging one. We propose a new Progressive Growing
Channel Attentive Non-Local (ProCAN) network for lung nodule classification.
The proposed method addresses this challenge from three different aspects.
First, we enrich the Non-Local network by adding channel-wise attention
capability to it. Second, we apply Curriculum Learning principles, whereby we
first train our model on easy examples before hard ones. Third, as the
classification task gets harder during the Curriculum learning, our model is
progressively grown to increase its capability of handling the task at hand. We
examined our proposed method on two different public datasets and compared its
performance with state-of-the-art methods in the literature. The results show
that the ProCAN model outperforms state-of-the-art methods and achieves an AUC
of 98.05% and an accuracy of 95.28% on the LIDC-IDRI dataset. Moreover, we
conducted extensive ablation studies to analyze the contribution and effects of
each new component of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LEAN: graph-based pruning for convolutional neural networks by extracting longest chains. (arXiv:2011.06923v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.06923">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have proven to be highly successful at a
range of image-to-image tasks. CNNs can be prohibitively computationally
expensive for real time use, which can limit their applicability in practice.
Model pruning can improve computational efficiency by sparsifying trained
networks. Common methods for pruning CNNs determine what convolutional filters
to remove by ranking filters on an individual basis. However, filters are not
independent, as CNNs consist of chains of convolutions, which can result in
sub-optimal filter selection.
</p>
<p>We propose a novel pruning method, LongEst-chAiN (LEAN) pruning, which takes
the interdependency between the convolution operations into account. We propose
to prune CNNs by using graph-based algorithms to select relevant chains of
convolutions. A CNN is interpreted as a graph, with the operator norm of each
operator as distance metric for the edges. LEAN pruning iteratively extracts
the highest value path from the graph to keep. In our experiments, we test LEAN
pruning for several image-to-image tasks, including the well-known CamVid
dataset, and a real-world dynamic X-ray CT dataset. When pruning CNNs with
LEAN, we achieve a higher accuracy than pruning filters individually, and
different pruned substructures emerge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian Triplet Loss: Uncertainty Quantification in Image Retrieval. (arXiv:2011.12663v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12663">
<div class="article-summary-box-inner">
<span><p>Uncertainty quantification in image retrieval is crucial for downstream
decisions, yet it remains a challenging and largely unexplored problem. Current
methods for estimating uncertainties are poorly calibrated, computationally
expensive, or based on heuristics. We present a new method that views image
embeddings as stochastic features rather than deterministic features. Our two
main contributions are (1) a likelihood that matches the triplet constraint and
that evaluates the probability of an anchor being closer to a positive than a
negative; and (2) a prior over the feature space that justifies the
conventional l2 normalization. To ensure computational efficiency, we derive a
variational approximation of the posterior, called the Bayesian triplet loss,
that produces state-of-the-art uncertainty estimates and matches the predictive
performance of current state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Learning for Sparsely-Labeled Sequential Data: Application to Healthcare Video Processing. (arXiv:2011.14101v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14101">
<div class="article-summary-box-inner">
<span><p>Labeled data is a critical resource for training and evaluating machine
learning models. However, many real-life datasets are only partially labeled.
We propose a semi-supervised machine learning training strategy to improve
event detection performance on sequential data, such as video recordings, when
only sparse labels are available, such as event start times without their
corresponding end times. Our method uses noisy guesses of the events' end times
to train event detection models. Depending on how conservative these guesses
are, mislabeled false positives may be introduced into the training set (i.e.,
negative sequences mislabeled as positives). We further propose a mathematical
model for estimating how many inaccurate labels a model is exposed to, based on
how noisy the end time guesses are. Finally, we show that neural networks can
improve their detection performance by leveraging more training data with less
conservative approximations despite the higher proportion of incorrect labels.
We adapt sequential versions of MNIST and CIFAR-10 to empirically evaluate our
method, and find that our risk-tolerant strategy outperforms conservative
estimates by 12 points of mean average precision for MNIST, and 3.5 points for
CIFAR. Then, we leverage the proposed training strategy to tackle a real-life
application: processing continuous video recordings of epilepsy patients to
improve seizure detection, and show that our method outperforms baseline
labeling methods by 10 points of average precision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AGRNet: Adaptive Graph Representation Learning and Reasoning for Face Parsing. (arXiv:2101.07034v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07034">
<div class="article-summary-box-inner">
<span><p>Face parsing infers a pixel-wise label to each facial component, which has
drawn much attention recently. Previous methods have shown their success in
face parsing, which however overlook the correlation among facial components.
As a matter of fact, the component-wise relationship is a critical clue in
discriminating ambiguous pixels in facial area. To address this issue, we
propose adaptive graph representation learning and reasoning over facial
components, aiming to learn representative vertices that describe each
component, exploit the component-wise relationship and thereby produce accurate
parsing results against ambiguity. In particular, we devise an adaptive and
differentiable graph abstraction method to represent the components on a graph
via pixel-to-vertex projection under the initial condition of a predicted
parsing map, where pixel features within a certain facial region are aggregated
onto a vertex. Further, we explicitly incorporate the image edge as a prior in
the model, which helps to discriminate edge and non-edge pixels during the
projection, thus leading to refined parsing results along the edges. Then, our
model learns and reasons over the relations among components by propagating
information across vertices on the graph. Finally, the refined vertex features
are projected back to pixel grids for the prediction of the final parsing map.
To train our model, we propose a discriminative loss to penalize small
distances between vertices in the feature space, which leads to distinct
vertices with strong semantics. Experimental results show the superior
performance of the proposed model on multiple face parsing datasets, along with
the validation on the human parsing task to demonstrate the generalizability of
our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Post-OCR Paragraph Recognition by Graph Convolutional Networks. (arXiv:2101.12741v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.12741">
<div class="article-summary-box-inner">
<span><p>We propose a new approach for paragraph recognition in document images by
spatial graph convolutional networks (GCN) applied on OCR text boxes. Two
steps, namely line splitting and line clustering, are performed to extract
paragraphs from the lines in OCR results. Each step uses a beta-skeleton graph
constructed from bounding boxes, where the graph edges provide efficient
support for graph convolution operations. With only pure layout input features,
the GCN model size is 3~4 orders of magnitude smaller compared to R-CNN based
models, while achieving comparable or better accuracies on PubLayNet and other
datasets. Furthermore, the GCN models show good generalization from synthetic
training data to real-world images, and good adaptivity for variable document
styles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Techniques for Quantizing Deep Networks with Adaptive Bit-Widths. (arXiv:2103.01435v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01435">
<div class="article-summary-box-inner">
<span><p>Quantizing deep networks with adaptive bit-widths is a promising technique
for efficient inference across many devices and resource constraints. In
contrast to static methods that repeat the quantization process and train
different models for different constraints, adaptive quantization enables us to
flexibly adjust the bit-widths of a single deep network during inference for
instant adaptation in different scenarios. While existing research shows
encouraging results on common image classification benchmarks, this paper
investigates how to train such adaptive networks more effectively.
Specifically, we present two novel techniques for quantizing deep neural
networks with adaptive bit-widths of weights and activations. First, we propose
a collaborative strategy to choose a high-precision teacher for transferring
knowledge to the low-precision student while jointly optimizing the model with
all bit-widths. Second, to effectively transfer knowledge, we develop a dynamic
block swapping method by randomly replacing the blocks in the lower-precision
student network with the corresponding blocks in the higher-precision teacher
network. Extensive experiments on multiple image classification datasets
including video classification benchmarks for the first time, well demonstrate
the efficacy of our approach over state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effect of Pre-Training Scale on Intra- and Inter-Domain Full and Few-Shot Transfer Learning for Natural and Medical X-Ray Chest Images. (arXiv:2106.00116v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00116">
<div class="article-summary-box-inner">
<span><p>Transfer learning aims to exploit pre-trained models for more efficient
follow-up training on wide range of downstream tasks and datasets, enabling
successful training also on small data. Recently, strong improvement was shown
for transfer learning and model generalization when increasing model, data and
compute budget scale in the pre-training. To compare effect of scale both in
intra- and inter-domain full and few-shot transfer, in this study we combine
for the first time large openly available medical X-Ray chest imaging datasets
to reach a dataset scale comparable to ImageNet-1k. We then conduct
pre-training and transfer to different natural or medical targets while varying
network size and source data scale and domain, being either large natural
(ImageNet-1k/21k) or large medical chest X-Ray datasets. We observe strong
improvement due to larger pre-training scale for intra-domain natural-natural
and medical-medical transfer. For inter-domain natural-medical transfer, we
find improvements due to larger pre-training scale on larger X-Ray targets in
full shot regime, while for smaller targets and for few-shot regime the
improvement is not visible. Remarkably, large networks pre-trained on very
large natural ImageNet-21k are as good or better than networks pre-trained on
largest available medical X-Ray data when performing transfer to large X-Ray
targets. We conclude that high quality models for inter-domain transfer can be
also obtained by substantially increasing scale of model and generic natural
source data, removing necessity for large domain-specific medical source data
in the pre-training. Code is available at:
\url{https://github.com/SLAMPAI/large-scale-pretraining-transfer}}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions. (arXiv:2106.04484v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04484">
<div class="article-summary-box-inner">
<span><p>Deep learning algorithms have shown promising results in visual question
answering (VQA) tasks, but a more careful look reveals that they often do not
understand the rich signal they are being fed with. To understand and better
measure the generalization capabilities of VQA systems, we look at their
robustness to counterfactually augmented data. Our proposed augmentations are
designed to make a focused intervention on a specific property of the question
such that the answer changes. Using these augmentations, we propose a new
robustness measure, Robustness to Augmented Data (RAD), which measures the
consistency of model predictions between original and augmented examples.
Through extensive experimentation, we show that RAD, unlike classical accuracy
measures, can quantify when state-of-the-art systems are not robust to
counterfactuals. We find substantial failure cases which reveal that current
VQA systems are still brittle. Finally, we connect between robustness and
generalization, demonstrating the predictive power of RAD for performance on
unseen augmentations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is this Harmful? Learning to Predict Harmfulness Ratings from Video. (arXiv:2106.08323v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08323">
<div class="article-summary-box-inner">
<span><p>Automatically identifying harmful content in video is an important task with
a wide range of applications. However, due to the difficulty of collecting
high-quality labels as well as demanding computational requirements, the task
has not yet had a fully general approach. Typically, only small subsets of the
problem are considered, such as identifying violent content. In cases where the
general problem is tackled, approximations and simplifications are made to deal
with the lack of labels and computational complexity. In this work, we identify
and tackle some of the main obstacles. First, we create an open dataset of 3589
video clips from film trailers and annotated by professionals in the field.
Second, we perform an analysis of our constructed dataset, investigating among
other things the relation between clip and trailer level annotations. Lastly,
we train audiovisual models on our dataset and conduct an in-depth study on our
modeling choices. We find that results greatly improve by combining the visual
and audio modality and that pre-training on large-scale video recognition
datasets as well as class balanced sampling further improves performance.
Further details of our dataset is available at this webpage:
https://vidharm.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GKNet: grasp keypoint network for grasp candidates detection. (arXiv:2106.08497v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08497">
<div class="article-summary-box-inner">
<span><p>Contemporary grasp detection approaches employ deep learning to achieve
robustness to sensor and object model uncertainty. The two dominant approaches
design either grasp-quality scoring or anchor-based grasp recognition networks.
This paper presents a different approach to grasp detection by treating it as
keypoint detection. The deep network detects each grasp candidate as a pair of
keypoints, convertible to the grasp representation g = {x, y, w, {\theta}}^T,
rather than a triplet or quartet of corner points. Decreasing the detection
difficulty by grouping keypoints into pairs boosts performance. To further
promote dependencies between keypoints, the general non-local module is
incorporated into the proposed learning framework. A final filtering strategy
based on discrete and continuous orientation prediction removes false
correspondences and further improves grasp detection performance. GKNet, the
approach presented here, achieves the best balance of accuracy and speed on the
Cornell and the abridged Jacquard dataset (96.9% and 98.39% at 41.67 and 23.26
fps). Follow-up experiments on a manipulator evaluate GKNet using 4 types of
grasping experiments reflecting different nuisance sources: static grasping,
dynamic grasping, grasping at varied camera angles, and bin picking. GKNet
outperforms reference baselines in static and dynamic grasping experiments
while showing robustness to varied camera viewpoints and bin picking
experiments. The results confirm the hypothesis that grasp keypoints are an
effective output representation for deep grasp networks that provide robustness
to expected nuisance factors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-N-Out: Towards Good Initialization for Inpainting and Outpainting. (arXiv:2106.13953v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13953">
<div class="article-summary-box-inner">
<span><p>In computer vision, recovering spatial information by filling in masked
regions, e.g., inpainting, has been widely investigated for its usability and
wide applicability to other various applications: image inpainting, image
extrapolation, and environment map estimation. Most of them are studied
separately depending on the applications. Our focus, however, is on
accommodating the opposite task, e.g., image outpainting, which would benefit
the target applications, e.g., image inpainting. Our self-supervision method,
In-N-Out, is summarized as a training approach that leverages the knowledge of
the opposite task into the target model. We empirically show that In-N-Out --
which explores the complementary information -- effectively takes advantage
over the traditional pipelines where only task-specific learning takes place in
training. In experiments, we compare our method to the traditional procedure
and analyze the effectiveness of our method on different applications: image
inpainting, image extrapolation, and environment map estimation. For these
tasks, we demonstrate that In-N-Out consistently improves the performance of
the recent works with In-N-Out self-supervision to their training procedure.
Also, we show that our approach achieves better results than an existing
training approach for outpainting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MSN: Multi-Style Network for Trajectory Prediction. (arXiv:2107.00932v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00932">
<div class="article-summary-box-inner">
<span><p>It is essential to predict future trajectories of various agents in complex
scenes. Whether it is internal personality factors of agents, interactive
behavior of the neighborhood, or the influence of surroundings, it will have an
impact on their future plannings. It means that even for the same physical type
of agents, there are huge differences in their behavior styles. We concentrate
on the problem of modeling agents' multi-style characteristics when predicting
their trajectories. We propose the Multi-Style Network (MSN) to focus on this
problem by dividing agents' behaviors into several hidden behavior categories
adaptively, and then train each category's prediction network jointly, thus
giving agents multiple styles of predictions simultaneously. Experiments show
that MSN outperforms current state-of-the-art methods with 10\% - 20\%
performance improvement on two widely used datasets, and presents better
multi-style characteristics in predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hepatocellular Carcinoma Segmentation from Digital Subtraction Angiography Videos using Learnable Temporal Difference. (arXiv:2107.04306v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.04306">
<div class="article-summary-box-inner">
<span><p>Automatic segmentation of hepatocellular carcinoma (HCC) in Digital
Subtraction Angiography (DSA) videos can assist radiologists in efficient
diagnosis of HCC and accurate evaluation of tumors in clinical practice. Few
studies have investigated HCC segmentation from DSA videos. It shows great
challenging due to motion artifacts in filming, ambiguous boundaries of tumor
regions and high similarity in imaging to other anatomical tissues. In this
paper, we raise the problem of HCC segmentation in DSA videos, and build our
own DSA dataset. We also propose a novel segmentation network called
DSA-LTDNet, including a segmentation sub-network, a temporal difference
learning (TDL) module and a liver region segmentation (LRS) sub-network for
providing additional guidance. DSA-LTDNet is preferable for learning the latent
motion information from DSA videos proactively and boosting segmentation
performance. All of experiments are conducted on our self-collected dataset.
Experimental results show that DSA-LTDNet increases the DICE score by nearly 4%
compared to the U-Net baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Locally Enhanced Self-Attention: Rethinking Self-Attention as Local and Context Terms. (arXiv:2107.05637v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05637">
<div class="article-summary-box-inner">
<span><p>Self-Attention has become prevalent in computer vision models. Inspired by
fully connected Conditional Random Fields (CRFs), we decompose it into local
and context terms. They correspond to the unary and binary terms in CRF and are
implemented by attention mechanisms with projection matrices. We observe that
the unary terms only make small contributions to the outputs, and meanwhile
standard CNNs that rely solely on the unary terms achieve great performances on
a variety of tasks. Therefore, we propose Locally Enhanced Self-Attention
(LESA), which enhances the unary term by incorporating it with convolutions,
and utilizes a fusion module to dynamically couple the unary and binary
operations. In our experiments, we replace the self-attention modules with
LESA. The results on ImageNet and COCO show the superiority of LESA over
convolution and self-attention baselines for the tasks of image recognition,
object detection, and instance segmentation. The code is made publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Abnormal Event Detection by Learning to Complete Visual Cloze Tests. (arXiv:2108.02356v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02356">
<div class="article-summary-box-inner">
<span><p>Although deep neural networks (DNNs) enable great progress in video abnormal
event detection (VAD), existing solutions typically suffer from two issues: (1)
The localization of video events cannot be both precious and comprehensive. (2)
The semantics and temporal context are under-explored. To tackle those issues,
we are motivated by the prevalent cloze test in education and propose a novel
approach named Visual Cloze Completion (VCC), which conducts VAD by learning to
complete "visual cloze tests" (VCTs). Specifically, VCC first localizes each
video event and encloses it into a spatio-temporal cube (STC). To achieve both
precise and comprehensive localization, appearance and motion are used as
complementary cues to mark the object region associated with each event. For
each marked region, a normalized patch sequence is extracted from current and
adjacent frames and stacked into a STC. With each patch and the patch sequence
of a STC compared to a visual "word" and "sentence" respectively, we
deliberately erase a certain "word" (patch) to yield a VCT. Then, the VCT is
completed by training DNNs to infer the erased patch and its optical flow via
video semantics. Meanwhile, VCC fully exploits temporal context by
alternatively erasing each patch in temporal context and creating multiple
VCTs. Furthermore, we propose localization-level, event-level, model-level and
decision-level solutions to enhance VCC, which can further exploit VCC's
potential and produce significant performance improvement gain. Extensive
experiments demonstrate that VCC achieves state-of-the-art VAD performance. Our
codes and results are open at https://github.com/yuguangnudt/VEC_VAD/tree/VCC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development. (arXiv:2108.04308v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04308">
<div class="article-summary-box-inner">
<span><p>Data is a crucial component of machine learning. The field is reliant on data
to train, validate, and test models. With increased technical capabilities,
machine learning research has boomed in both academic and industry settings,
and one major focus has been on computer vision. Computer vision is a popular
domain of machine learning increasingly pertinent to real-world applications,
from facial recognition in policing to object detection for autonomous
vehicles. Given computer vision's propensity to shape machine learning research
and impact human life, we seek to understand disciplinary practices around
dataset documentation - how data is collected, curated, annotated, and packaged
into datasets for computer vision researchers and practitioners to use for
model tuning and development. Specifically, we examine what dataset
documentation communicates about the underlying values of vision data and the
larger practices and goals of computer vision as a field. To conduct this
study, we collected a corpus of about 500 computer vision datasets, from which
we sampled 114 dataset publications across different vision tasks. Through both
a structured and thematic content analysis, we document a number of values
around accepted data practices, what makes desirable data, and the treatment of
humans in the dataset construction process. We discuss how computer vision
datasets authors value efficiency at the expense of care; universality at the
expense of contextuality; impartiality at the expense of positionality; and
model work at the expense of data work. Many of the silenced values we identify
sit in opposition with social computing practices. We conclude with suggestions
on how to better incorporate silenced values into the dataset creation and
curation process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overfitting the Data: Compact Neural Video Delivery via Content-aware Feature Modulation. (arXiv:2108.08202v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08202">
<div class="article-summary-box-inner">
<span><p>Internet video delivery has undergone a tremendous explosion of growth over
the past few years. However, the quality of video delivery system greatly
depends on the Internet bandwidth. Deep Neural Networks (DNNs) are utilized to
improve the quality of video delivery recently. These methods divide a video
into chunks, and stream LR video chunks and corresponding content-aware models
to the client. The client runs the inference of models to super-resolve the LR
chunks. Consequently, a large number of models are streamed in order to deliver
a video. In this paper, we first carefully study the relation between models of
different chunks, then we tactfully design a joint training framework along
with the Content-aware Feature Modulation (CaFM) layer to compress these models
for neural video delivery. {\bf With our method, each video chunk only requires
less than $1\% $ of original parameters to be streamed, achieving even better
SR performance.} We conduct extensive experiments across various SR backbones,
video time length, and scaling factors to demonstrate the advantages of our
method. Besides, our method can be also viewed as a new approach of video
coding. Our primary experiments achieve better video quality compared with the
commercial H.264 and H.265 standard under the same storage cost, showing the
great potential of the proposed method. Code is available
at:\url{https://github.com/Neural-video-delivery/CaFM-Pytorch-ICCV2021}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LOKI: Long Term and Key Intentions for Trajectory Prediction. (arXiv:2108.08236v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08236">
<div class="article-summary-box-inner">
<span><p>Recent advances in trajectory prediction have shown that explicit reasoning
about agents' intent is important to accurately forecast their motion. However,
the current research activities are not directly applicable to intelligent and
safety critical systems. This is mainly because very few public datasets are
available, and they only consider pedestrian-specific intents for a short
temporal horizon from a restricted egocentric view. To this end, we propose
LOKI (LOng term and Key Intentions), a novel large-scale dataset that is
designed to tackle joint trajectory and intention prediction for heterogeneous
traffic agents (pedestrians and vehicles) in an autonomous driving setting. The
LOKI dataset is created to discover several factors that may affect intention,
including i) agent's own will, ii) social interactions, iii) environmental
constraints, and iv) contextual information. We also propose a model that
jointly performs trajectory and intention prediction, showing that recurrently
reasoning about intention can assist with trajectory prediction. We show our
method outperforms state-of-the-art trajectory prediction methods by upto
$27\%$ and also provide a baseline for frame-wise intention estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">External Knowledge Augmented Text Visual Question Answering. (arXiv:2108.09717v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09717">
<div class="article-summary-box-inner">
<span><p>The open-ended question answering task of Text-VQA requires reading and
reasoning about local, often previously unseen, scene-text content of an image
to generate answers. In this work, we propose the generalized use of external
knowledge to augment our understanding of the said scene-text. We design a
framework to extract, validate, and reason with knowledge using a standard
multimodal transformer for vision language understanding tasks. Through
empirical evidence and qualitative results, we demonstrate how external
knowledge can highlight instance-only cues and thus help deal with training
data bias, improve answer entity type correctness, and detect multiword named
entities. We generate results comparable to the state-of-the-art on two
publicly available datasets, under the constraints of similar upstream OCR
systems and training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LUAI Challenge 2021 on Learning to Understand Aerial Images. (arXiv:2108.13246v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13246">
<div class="article-summary-box-inner">
<span><p>This report summarizes the results of Learning to Understand Aerial Images
(LUAI) 2021 challenge held on ICCV 2021, which focuses on object detection and
semantic segmentation in aerial images. Using DOTA-v2.0 and GID-15 datasets,
this challenge proposes three tasks for oriented object detection, horizontal
object detection, and semantic segmentation of common categories in aerial
images. This challenge received a total of 146 registrations on the three
tasks. Through the challenge, we hope to draw attention from a wide range of
communities and call for more efforts on the problems of learning to understand
aerial images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00590">
<div class="article-summary-box-inner">
<span><p>Web search is fundamentally multimodal and multihop. Often, even before
asking a question we choose to go directly to image search to find our answers.
Further, rarely do we find an answer from a single source but aggregate
information and reason through implications. Despite the frequency of this
everyday occurrence, at present, there is no unified question answering
benchmark that requires a single model to answer long-form natural language
questions from text and open-ended visual sources -- akin to a human's
experience. We propose to bridge this gap between the natural language and
computer vision communities with WebQA. We show that A. our multihop text
queries are difficult for a large-scale transformer model, and B. existing
multi-modal transformers and visual representations do not perform well on
open-domain visual queries. Our challenge for the community is to create a
unified multimodal reasoning model that seamlessly transitions and reasons
regardless of the source modality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of GAN-synthesized street videos. (arXiv:2109.04991v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04991">
<div class="article-summary-box-inner">
<span><p>Research on the detection of AI-generated videos has focused almost
exclusively on face videos, usually referred to as deepfakes. Manipulations
like face swapping, face reenactment and expression manipulation have been the
subject of an intense research with the development of a number of efficient
tools to distinguish artificial videos from genuine ones. Much less attention
has been paid to the detection of artificial non-facial videos. Yet, new tools
for the generation of such kind of videos are being developed at a fast pace
and will soon reach the quality level of deepfake videos. The goal of this
paper is to investigate the detectability of a new kind of AI-generated videos
framing driving street sequences (here referred to as DeepStreets videos),
which, by their nature, can not be analysed with the same tools used for facial
deepfakes. Specifically, we present a simple frame-based detector, achieving
very good performance on state-of-the-art DeepStreets videos generated by the
Vid2vid architecture. Noticeably, the detector retains very good performance on
compressed videos, even when the compression level used during training does
not match that used for the test videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Learning-Based Unified Framework for Red Lesions Detection on Retinal Fundus Images. (arXiv:2109.05021v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05021">
<div class="article-summary-box-inner">
<span><p>Red-lesions, i.e., microaneurysms (MAs) and hemorrhages (HMs), are the early
signs of diabetic retinopathy (DR). The automatic detection of MAs and HMs on
retinal fundus images is a challenging task. Most of the existing methods
detect either only MAs or only HMs because of the difference in their texture,
sizes, and morphology. Though some methods detect both MAs and HMs, they suffer
from the curse of dimensionality of shape and colors features and fail to
detect all shape variations of HMs such as flame-shaped HM. Leveraging the
progress in deep learning, we proposed a two-stream red lesions detection
system dealing simultaneously with small and large red lesions. For this
system, we introduced a new ROIs candidates generation method for large red
lesions fundus images; it is based on blood vessel segmentation and
morphological operations, and reduces the computational complexity, and
enhances the detection accuracy by generating a small number of potential
candidates. For detection, we adapted the Faster RCNN framework with two
streams. We used pre-trained VGGNet as a bone model and carried out several
extensive experiments to tune it for vessels segmentation and candidates
generation, and finally learning the appropriate mapping, which yields better
detection of the red lesions comparing with the state-of-the-art methods. The
experimental results validated the effectiveness of the system in the detection
of both MAs and HMs; the method yields higher performance for per lesion
detection according to sensitivity under 4 FPIs on DiaretDB1-MA and
DiaretDB1-HM datasets, and 1 FPI on e-ophtha and ROCh datasets than the state
of the art methods w.r.t. various evaluation metrics. For DR screening, the
system outperforms other methods on DiaretDB1-MA, DiaretDB1-HM, and e-ophtha
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Tensor Completion via Element-wise Weighted Low-rank Tensor Train with Overlapping Ket Augmentation. (arXiv:2109.05736v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05736">
<div class="article-summary-box-inner">
<span><p>In recent years, there have been an increasing number of applications of
tensor completion based on the tensor train (TT) format because of its
efficiency and effectiveness in dealing with higher-order tensor data. However,
existing tensor completion methods using TT decomposition have two obvious
drawbacks. One is that they only consider mode weights according to the degree
of mode balance, even though some elements are recovered better in an
unbalanced mode. The other is that serious blocking artifacts appear when the
missing element rate is relatively large. To remedy such two issues, in this
work, we propose a novel tensor completion approach via the element-wise
weighted technique. Accordingly, a novel formulation for tensor completion and
an effective optimization algorithm, called as tensor completion by parallel
weighted matrix factorization via tensor train (TWMac-TT), is proposed. In
addition, we specifically consider the recovery quality of edge elements from
adjacent blocks. Different from traditional reshaping and ket augmentation, we
utilize a new tensor augmentation technique called overlapping ket
augmentation, which can further avoid blocking artifacts. We then conduct
extensive performance evaluations on synthetic data and several real image data
sets. Our experimental results demonstrate that the proposed algorithm TWMac-TT
outperforms several other competing tensor completion methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication. (arXiv:2109.07644v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07644">
<div class="article-summary-box-inner">
<span><p>Employing Vehicle-to-Vehicle communication to enhance perception performance
in self-driving technology has attracted considerable attention recently;
however, the absence of a suitable open dataset for benchmarking algorithms has
made it difficult to develop and assess cooperative perception technologies. To
this end, we present the first large-scale open simulated dataset for
Vehicle-to-Vehicle perception. It contains over 70 interesting scenes, 11,464
frames, and 232,913 annotated 3D vehicle bounding boxes, collected from 8 towns
in CARLA and a digital town of Culver City, Los Angeles. We then construct a
comprehensive benchmark with a total of 16 implemented models to evaluate
several information fusion strategies~(i.e. early, late, and intermediate
fusion) with state-of-the-art LiDAR detection algorithms. Moreover, we propose
a new Attentive Intermediate Fusion pipeline to aggregate information from
multiple connected vehicles. Our experiments show that the proposed pipeline
can be easily integrated with existing 3D LiDAR detectors and achieve
outstanding performance even with large compression rates. To encourage more
researchers to investigate Vehicle-to-Vehicle perception, we will release the
dataset, benchmark methods, and all related codes in
https://mobility-lab.seas.ucla.edu/opv2v/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ROS-X-Habitat: Bridging the ROS Ecosystem with Embodied AI. (arXiv:2109.07703v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07703">
<div class="article-summary-box-inner">
<span><p>We introduce ROS-X-Habitat, a software interface that bridges the AI Habitat
platform for embodied reinforcement learning agents with other robotics
resources via ROS. This interface not only offers standardized communication
protocols between embodied agents and simulators, but also enables
physics-based simulation. With this interface, roboticists are able to train
their own Habitat RL agents in another simulation environment or to develop
their own robotic algorithms inside Habitat Sim. Through in silico experiments,
we demonstrate that ROS-X-Habitat has minimal impact on the navigation
performance and simulation speed of Habitat agents; that a standard set of ROS
mapping, planning and navigation tools can run in the Habitat simulator, and
that a Habitat agent can run in the standard ROS simulator Gazebo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Temporal Sentence Grounding in Videos. (arXiv:2109.08039v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08039">
<div class="article-summary-box-inner">
<span><p>Temporal sentence grounding in videos(TSGV), which aims to localize one
target segment from an untrimmed video with respect to a given sentence query,
has drawn increasing attentions in the research community over the past few
years. Different from the task of temporal action localization, TSGV is more
flexible since it can locate complicated activities via natural languages,
without restrictions from predefined action categories. Meanwhile, TSGV is more
challenging since it requires both textual and visual understanding for
semantic alignment between two modalities(i.e., text and video). In this
survey, we give a comprehensive overview for TSGV, which i) summarizes the
taxonomy of existing methods, ii) provides a detailed description of the
evaluation protocols(i.e., datasets and metrics) to be used in TSGV, and iii)
in-depth discusses potential problems of current benchmarking designs and
research directions for further investigations. To the best of our knowledge,
this is the first systematic survey on temporal sentence grounding. More
specifically, we first discuss existing TSGV approaches by grouping them into
four categories, i.e., two-stage methods, end-to-end methods, reinforcement
learning-based methods, and weakly supervised methods. Then we present the
benchmark datasets and evaluation metrics to assess current research progress.
Finally, we discuss some limitations in TSGV through pointing out potential
problems improperly resolved in the current evaluation protocols, which may
push forwards more cutting edge research in TSGV. Besides, we also share our
insights on several promising directions, including three typical tasks with
new and practical settings based on TSGV.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Machine Learning Framework for Automatic Prediction of Human Semen Motility. (arXiv:2109.08049v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08049">
<div class="article-summary-box-inner">
<span><p>In this paper, human semen samples from the visem dataset collected by the
Simula Research Laboratory are automatically assessed with machine learning
methods for their quality in respect to sperm motility. Several regression
models are trained to automatically predict the percentage (0 to 100) of
progressive, non-progressive, and immotile spermatozoa in a given sample. The
video samples are adopted for three different feature extraction methods, in
particular custom movement statistics, displacement features, and motility
specific statistics have been utilised. Furthermore, four machine learning
models, including linear Support Vector Regressor (SVR), Multilayer Perceptron
(MLP), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN),
have been trained on the extracted features for the task of automatic motility
prediction. Best results for predicting motility are achieved by using the
Crocker-Grier algorithm to track sperm cells in an unsupervised way and
extracting individual mean squared displacement features for each detected
track. These features are then aggregated into a histogram representation
applying a Bag-of-Words approach. Finally, a linear SVR is trained on this
feature representation. Compared to the best submission of the Medico
Multimedia for Medicine challenge, which used the same dataset and splits, the
Mean Absolute Error (MAE) could be reduced from 8.83 to 7.31. For the sake of
reproducibility, we provide the source code for our experiments on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sign-MAML: Efficient Model-Agnostic Meta-Learning by SignSGD. (arXiv:2109.07497v1 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07497">
<div class="article-summary-box-inner">
<span><p>We propose a new computationally-efficient first-order algorithm for
Model-Agnostic Meta-Learning (MAML). The key enabling technique is to interpret
MAML as a bilevel optimization (BLO) problem and leverage the sign-based
SGD(signSGD) as a lower-level optimizer of BLO. We show that MAML, through the
lens of signSGD-oriented BLO, naturally yields an alternating optimization
scheme that just requires first-order gradients of a learned meta-model. We
term the resulting MAML algorithm Sign-MAML. Compared to the conventional
first-order MAML (FO-MAML) algorithm, Sign-MAML is theoretically-grounded as it
does not impose any assumption on the absence of second-order derivatives
during meta training. In practice, we show that Sign-MAML outperforms FO-MAML
in various few-shot image classification tasks, and compared to MAML, it
achieves a much more graceful tradeoff between classification accuracy and
computation efficiency.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-20 23:01:45.152562381 UTC">2021-09-20 23:01:45 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>