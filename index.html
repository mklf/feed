<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-01-04T01:30:00Z">01-04</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">How do lexical semantics affect translation? An empirical study. (arXiv:2201.00075v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00075">
<div class="article-summary-box-inner">
<span><p>Neural machine translation (NMT) systems aim to map text from one language
into another. While there are a wide variety of applications of NMT, one of the
most important is translation of natural language. A distinguishing factor of
natural language is that words are typically ordered according to the rules of
the grammar of a given language. Although many advances have been made in
developing NMT systems for translating natural language, little research has
been done on understanding how the word ordering of and lexical similarity
between the source and target language affect translation performance. Here, we
investigate these relationships on a variety of low-resource language pairs
from the OpenSubtitles2016 database, where the source language is English, and
find that the more similar the target language is to English, the greater the
translation performance. In addition, we study the impact of providing NMT
models with part of speech of words (POS) in the English sequence and find
that, for Transformer-based models, the more dissimilar the target language is
from English, the greater the benefit provided by POS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Fake News Detection using cross-checking with reliable sources. (arXiv:2201.00083v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00083">
<div class="article-summary-box-inner">
<span><p>Over the past decade, fake news and misinformation have turned into a major
problem that has impacted different aspects of our lives, including politics
and public health. Inspired by natural human behavior, we present an approach
that automates the detection of fake news. Natural human behavior is to
cross-check new information with reliable sources. We use Natural Language
Processing (NLP) and build a machine learning (ML) model that automates the
process of cross-checking new information with a set of predefined reliable
sources. We implement this for Twitter and build a model that flags fake
tweets. Specifically, for a given tweet, we use its text to find relevant news
from reliable news agencies. We then train a Random Forest model that checks if
the textual content of the tweet is aligned with the trusted news. If it is
not, the tweet is classified as fake. This approach can be generally applied to
any kind of information and is not limited to a specific news story or a
category of information. Our implementation of this approach gives a $70\%$
accuracy which outperforms other generic fake-news classification models. These
results pave the way towards a more sensible and natural approach to fake news
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Search for Large Scale Clinical Ontologies. (arXiv:2201.00118v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00118">
<div class="article-summary-box-inner">
<span><p>Finding concepts in large clinical ontologies can be challenging when queries
use different vocabularies. A search algorithm that overcomes this problem is
useful in applications such as concept normalisation and ontology matching,
where concepts can be referred to in different ways, using different synonyms.
In this paper, we present a deep learning based approach to build a semantic
search system for large clinical ontologies. We propose a Triplet-BERT model
and a method that generates training data directly from the ontologies. The
model is evaluated using five real benchmark data sets and the results show
that our approach achieves high results on both free text to concept and
concept to concept searching tasks, and outperforms all baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot Commonsense Question Answering with Cloze Translation and Consistency Optimization. (arXiv:2201.00136v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00136">
<div class="article-summary-box-inner">
<span><p>Commonsense question answering (CQA) aims to test if models can answer
questions regarding commonsense knowledge that everyone knows. Prior works that
incorporate external knowledge bases have shown promising results, but
knowledge bases are expensive to construct and are often limited to a fixed set
of relations. In this paper, we instead focus on better utilizing the
\textit{implicit knowledge} stored in pre-trained language models. While
researchers have found that the knowledge embedded in pre-trained language
models can be extracted by having them fill in the blanks of carefully designed
prompts for relation extraction and text classification, it remains unclear if
we can adopt this paradigm in CQA where the inputs and outputs take much more
flexible forms. To this end, we investigate four translation methods that can
translate natural questions into cloze-style sentences to better solicit
commonsense knowledge from language models, including a syntactic-based model,
an unsupervised neural model, and two supervised neural models. In addition, to
combine the different translation methods, we propose to encourage consistency
among model predictions on different translated questions with unlabeled data.
We demonstrate the effectiveness of our methods on three CQA datasets in
zero-shot settings. We show that our methods are complementary to a knowledge
base improved model, and combining them can lead to state-of-the-art zero-shot
performance. Analyses also reveal distinct characteristics of the different
cloze translation methods and provide insights on why combining them can lead
to great improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges of sampling and how phylogenetic comparative methods help: With a case study of the Pama-Nyungan laminal contrast. (arXiv:2201.00195v1 [q-bio.PE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00195">
<div class="article-summary-box-inner">
<span><p>Phylogenetic comparative methods are new in our field and are shrouded, for
most linguists, in at least a little mystery. Yet the path that led to their
discovery in comparative biology is so similar to the methodological history of
balanced sampling, that it is only an accident of history that they were not
discovered by a typologist. Here we clarify the essential logic behind
phylogenetic comparative methods and their fundamental relatedness to a deep
intellectual tradition focussed on sampling. Then we introduce concepts,
methods and tools which will enable typologists to use these methods in
everyday typological research. The key commonality of phylogenetic comparative
methods and balanced sampling is that they attempt to deal with statistical
non-independence due to genealogy. Whereas sampling can never achieve
independence and requires most comparative data to be discarded, phylogenetic
comparative methods achieve independence while retaining and using all data. We
discuss the essential notions of phylogenetic signal; uncertainty about trees;
typological averages and proportions that are sensitive to genealogy;
comparison across language families; and the effects of areality. Extensive
supplementary materials illustrate computational tools for practical analysis
and we illustrate the methods discussed with a typological case study of the
laminal contrast in Pama-Nyungan.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Informed Multi-context Entity Alignment. (arXiv:2201.00304v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00304">
<div class="article-summary-box-inner">
<span><p>Entity alignment is a crucial step in integrating knowledge graphs (KGs) from
multiple sources. Previous attempts at entity alignment have explored different
KG structures, such as neighborhood-based and path-based contexts, to learn
entity embeddings, but they are limited in capturing the multi-context
features. Moreover, most approaches directly utilize the embedding similarity
to determine entity alignment without considering the global interaction among
entities and relations. In this work, we propose an Informed Multi-context
Entity Alignment (IMEA) model to address these issues. In particular, we
introduce Transformer to flexibly capture the relation, path, and neighborhood
contexts, and design holistic reasoning to estimate alignment probabilities
based on both embedding similarity and the relation/entity functionality. The
alignment evidence obtained from holistic reasoning is further injected back
into the Transformer via the proposed soft label editing to inform embedding
learning. Experimental results on several benchmark datasets demonstrate the
superiority of our IMEA model compared with existing state-of-the-art entity
alignment methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Sensitivity of Deep Learning Based Text Classification Algorithms to Practical Input Perturbations. (arXiv:2201.00318v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00318">
<div class="article-summary-box-inner">
<span><p>Text classification is a fundamental Natural Language Processing task that
has a wide variety of applications, where deep learning approaches have
produced state-of-the-art results. While these models have been heavily
criticized for their black-box nature, their robustness to slight perturbations
in input text has been a matter of concern. In this work, we carry out a
data-focused study evaluating the impact of systematic practical perturbations
on the performance of the deep learning based text classification models like
CNN, LSTM, and BERT-based algorithms. The perturbations are induced by the
addition and removal of unwanted tokens like punctuation and stop-words that
are minimally associated with the final performance of the model. We show that
these deep learning approaches including BERT are sensitive to such legitimate
input perturbations on four standard benchmark datasets SST2, TREC-6, BBC News,
and tweet_eval. We observe that BERT is more susceptible to the removal of
tokens as compared to the addition of tokens. Moreover, LSTM is slightly more
sensitive to input perturbations as compared to CNN based model. The work also
serves as a practical guide to assessing the impact of discrepancies in
train-test conditions on the final performance of models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Establishing Strong Baselines for TripClick Health Retrieval. (arXiv:2201.00365v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00365">
<div class="article-summary-box-inner">
<span><p>We present strong Transformer-based re-ranking and dense retrieval baselines
for the recently released TripClick health ad-hoc retrieval collection. We
improve the - originally too noisy - training data with a simple negative
sampling policy. We achieve large gains over BM25 in the re-ranking task of
TripClick, which were not achieved with the original baselines. Furthermore, we
study the impact of different domain-specific pre-trained models on TripClick.
Finally, we show that dense retrieval outperforms BM25 by considerable margins,
even with simple training procedures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topical Classification of Food Safety Publications with a Knowledge Base. (arXiv:2201.00374v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00374">
<div class="article-summary-box-inner">
<span><p>The vast body of scientific publications presents an increasing challenge of
finding those that are relevant to a given research question, and making
informed decisions on their basis. This becomes extremely difficult without the
use of automated tools. Here, one possible area for improvement is automatic
classification of publication abstracts according to their topic. This work
introduces a novel, knowledge base-oriented publication classifier. The
proposed method focuses on achieving scalability and easy adaptability to other
domains. Classification speed and accuracy are shown to be satisfactory, in the
very demanding field of food safety. Further development and evaluation of the
method is needed, as the proposed approach shows much potential.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Actor-Critic Network for Q&A in an Adversarial Environment. (arXiv:2201.00455v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00455">
<div class="article-summary-box-inner">
<span><p>Significant work has been placed in the Q&amp;A NLP space to build models that
are more robust to adversarial attacks. Two key areas of focus are in
generating adversarial data for the purposes of training against these
situations or modifying existing architectures to build robustness within. This
paper introduces an approach that joins these two ideas together to train a
critic model for use in an almost reinforcement learning framework. Using the
Adversarial SQuAD "Add One Sent" dataset we show that there are some promising
signs for this method in protecting against Adversarial attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning with Latent Structures in Natural Language Processing: A Survey. (arXiv:2201.00490v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00490">
<div class="article-summary-box-inner">
<span><p>While end-to-end learning with fully differentiable models has enabled
tremendous success in natural language process (NLP) and machine learning,
there have been significant recent interests in learning with latent discrete
structures to incorporate better inductive biases for improved end-task
performance and better interpretability. This paradigm, however, is not
straightforwardly amenable to the mainstream gradient-based optimization
methods. This work surveys three main families of methods to learn such models:
surrogate gradients, continuous relaxation, and marginal likelihood
maximization via sampling. We conclude with a review of applications of these
methods and an inspection of the learned latent structure that they induce.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Which Student is Best? A Comprehensive Knowledge Distillation Exam for Task-Specific BERT Models. (arXiv:2201.00558v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00558">
<div class="article-summary-box-inner">
<span><p>We perform knowledge distillation (KD) benchmark from task-specific BERT-base
teacher models to various student models: BiLSTM, CNN, BERT-Tiny, BERT-Mini,
and BERT-Small. Our experiment involves 12 datasets grouped in two tasks: text
classification and sequence labeling in the Indonesian language. We also
compare various aspects of distillations including the usage of word embeddings
and unlabeled data augmentation. Our experiments show that, despite the rising
popularity of Transformer-based models, using BiLSTM and CNN student models
provide the best trade-off between performance and computational resource (CPU,
RAM, and storage) compared to pruned BERT models. We further propose some quick
wins on performing KD to produce small NLP models via efficient KD training
mechanisms involving simple choices of loss functions, word embeddings, and
unlabeled data preparation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toxicity Detection for Indic Multilingual Social Media Content. (arXiv:2201.00598v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00598">
<div class="article-summary-box-inner">
<span><p>Toxic content is one of the most critical issues for social media platforms
today. India alone had 518 million social media users in 2020. In order to
provide a good experience to content creators and their audience, it is crucial
to flag toxic comments and the users who post that. But the big challenge is
identifying toxicity in low resource Indic languages because of the presence of
multiple representations of the same text. Moreover, the posts/comments on
social media do not adhere to a particular format, grammar or sentence
structure; this makes the task of abuse detection even more challenging for
multilingual social media platforms. This paper describes the system proposed
by team 'Moj Masti' using the data provided by ShareChat/Moj in \emph{IIIT-D
Multilingual Abusive Comment Identification} challenge. We focus on how we can
leverage multilingual transformer based pre-trained and fine-tuned models to
approach code-mixed/code-switched classification tasks. Our best performing
system was an ensemble of XLM-RoBERTa and MuRIL which achieved a Mean F-1 score
of 0.9 on the test data/leaderboard. We also observed an increase in the
performance by adding transliterated data. Furthermore, using weak metadata,
ensembling and some post-processing techniques boosted the performance of our
system, thereby placing us 1st on the leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Stance Detection of Tweets Via Distant Network Supervision. (arXiv:2201.00614v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00614">
<div class="article-summary-box-inner">
<span><p>Detecting and labeling stance in social media text is strongly motivated by
hate speech detection, poll prediction, engagement forecasting, and concerted
propaganda detection. Today's best neural stance detectors need large volumes
of training data, which is difficult to curate given the fast-changing
landscape of social media text and issues on which users opine. Homophily
properties over the social network provide strong signal of coarse-grained
user-level stance. But semi-supervised approaches for tweet-level stance
detection fail to properly leverage homophily. In light of this, We present
SANDS, a new semi-supervised stance detector. SANDS starts from very few
labeled tweets. It builds multiple deep feature views of tweets. It also uses a
distant supervision signal from the social network to provide a surrogate loss
signal to the component learners. We prepare two new tweet datasets comprising
over 236,000 politically tinted tweets from two demographics (US and India)
posted by over 87,000 users, their follower-followee graph, and over 8,000
tweets annotated by linguists. SANDS achieves a macro-F1 score of 0.55 (0.49)
on US (India)-based datasets, outperforming 17 baselines (including variants of
SANDS) substantially, particularly for minority stance labels and noisy text.
Numerous ablation experiments on SANDS disentangle the dynamics of textual and
network-propagated stance signals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Entity Tagging with Multimodal Knowledge Base. (arXiv:2201.00693v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00693">
<div class="article-summary-box-inner">
<span><p>To enhance research on multimodal knowledge base and multimodal information
processing, we propose a new task called multimodal entity tagging (MET) with a
multimodal knowledge base (MKB). We also develop a dataset for the problem
using an existing MKB. In an MKB, there are entities and their associated texts
and images. In MET, given a text-image pair, one uses the information in the
MKB to automatically identify the related entity in the text-image pair. We
solve the task by using the information retrieval paradigm and implement
several baselines using state-of-the-art methods in NLP and CV. We conduct
extensive experiments and make analyses on the experimental results. The
results show that the task is challenging, but current technologies can achieve
relatively high performance. We will release the dataset, code, and models for
future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Natural Language Processing: Recent Advances, Challenges, and Future Directions. (arXiv:2201.00768v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00768">
<div class="article-summary-box-inner">
<span><p>Recent natural language processing (NLP) techniques have accomplished high
performance on benchmark datasets, primarily due to the significant improvement
in the performance of deep learning. The advances in the research community
have led to great enhancements in state-of-the-art production systems for NLP
tasks, such as virtual assistants, speech recognition, and sentiment analysis.
However, such NLP systems still often fail when tested with adversarial
attacks. The initial lack of robustness exposed troubling gaps in current
models' language understanding capabilities, creating problems when NLP systems
are deployed in real life. In this paper, we present a structured overview of
NLP robustness research by summarizing the literature in a systemic way across
various dimensions. We then take a deep-dive into the various dimensions of
robustness, across techniques, metrics, embeddings, and benchmarks. Finally, we
argue that robustness should be multi-dimensional, provide insights into
current research, identify gaps in the literature to suggest directions worth
pursuing to address these gaps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods. (arXiv:1907.09358v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.09358">
<div class="article-summary-box-inner">
<span><p>Interest in Artificial Intelligence (AI) and its applications has seen
unprecedented growth in the last few years. This success can be partly
attributed to the advancements made in the sub-fields of AI such as machine
learning, computer vision, and natural language processing. Much of the growth
in these fields has been made possible with deep learning, a sub-area of
machine learning that uses artificial neural networks. This has created
significant interest in the integration of vision and language. In this survey,
we focus on ten prominent tasks that integrate language and vision by
discussing their problem formulation, methods, existing datasets, evaluation
measures, and compare the results obtained with corresponding state-of-the-art
methods. Our efforts go beyond earlier surveys which are either task-specific
or concentrate only on one type of visual content, i.e., image or video.
Furthermore, we also provide some potential future directions in this field of
research with an anticipation that this survey stimulates innovative thoughts
and ideas to address the existing challenges and build new applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characterizing Speech Adversarial Examples Using Self-Attention U-Net Enhancement. (arXiv:2003.13917v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.13917">
<div class="article-summary-box-inner">
<span><p>Recent studies have highlighted adversarial examples as ubiquitous threats to
the deep neural network (DNN) based speech recognition systems. In this work,
we present a U-Net based attention model, U-Net$_{At}$, to enhance adversarial
speech signals. Specifically, we evaluate the model performance by
interpretable speech recognition metrics and discuss the model performance by
the augmented adversarial training. Our experiments show that our proposed
U-Net$_{At}$ improves the perceptual evaluation of speech quality (PESQ) from
1.13 to 2.78, speech transmission index (STI) from 0.65 to 0.75, short-term
objective intelligibility (STOI) from 0.83 to 0.96 on the task of speech
enhancement with adversarial speech examples. We conduct experiments on the
automatic speech recognition (ASR) task with adversarial audio attacks. We find
that (i) temporal features learned by the attention network are capable of
enhancing the robustness of DNN based ASR models; (ii) the generalization power
of DNN based ASR model could be enhanced by applying adversarial training with
an additive adversarial data augmentation. The ASR metric on word-error-rates
(WERs) shows that there is an absolute 2.22 $\%$ decrease under gradient-based
perturbation, and an absolute 2.03 $\%$ decrease, under evolutionary-optimized
perturbation, which suggests that our enhancement models with adversarial
training can further secure a resilient ASR system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Layer-Wise Multi-View Decoding for Improved Natural Language Generation. (arXiv:2005.08081v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.08081">
<div class="article-summary-box-inner">
<span><p>In sequence-to-sequence learning, e.g., natural language generation, the
decoder relies on the attention mechanism to efficiently extract information
from the encoder. While it is common practice to draw information from only the
last encoder layer, recent work has proposed to use representations from
different encoder layers for diversified levels of information. Nonetheless,
the decoder still obtains only a single view of the source sequences, which
might lead to insufficient training of the encoder layer stack due to the
hierarchy bypassing problem. In this work, we propose layer-wise multi-view
decoding, where for each decoder layer, together with the representations from
the last encoder layer, which serve as a global view, those from other encoder
layers are supplemented for a stereoscopic view of the source sequences.
Systematic experiments and analyses show that we successfully address the
hierarchy bypassing problem, require almost negligible parameter increase, and
substantially improve the performance of sequence-to-sequence learning with
deep representations on five diverse tasks, i.e., machine translation,
abstractive summarization, image captioning, video captioning, and medical
report generation. In particular, our approach achieves new state-of-the-art
results on eight benchmark datasets, including a low-resource machine
translation dataset and two low-resource medical report generation datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Rediscovery Hypothesis: Language Models Need to Meet Linguistics. (arXiv:2103.01819v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01819">
<div class="article-summary-box-inner">
<span><p>There is an ongoing debate in the NLP community whether modern language
models contain linguistic knowledge, recovered through so-called probes. In
this paper, we study whether linguistic knowledge is a necessary condition for
the good performance of modern language models, which we call the
\textit{rediscovery hypothesis}. In the first place, we show that language
models that are significantly compressed but perform well on their pretraining
objectives retain good scores when probed for linguistic structures. This
result supports the rediscovery hypothesis and leads to the second contribution
of our paper: an information-theoretic framework that relates language modeling
objectives with linguistic information. This framework also provides a metric
to measure the impact of linguistic information on the word prediction task. We
reinforce our analytical results with various experiments, both on synthetic
and on real NLP tasks in English.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Graph Augmented Political Perspective Detection in News Media. (arXiv:2108.03861v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03861">
<div class="article-summary-box-inner">
<span><p>Identifying political perspectives in news media has become an important task
due to the rapid growth of political commentary and the increasingly polarized
political ideologies. Previous approaches focus on textual content and leave
out the rich social and political context that is essential in the argument
mining process. To address this limitation, we propose a political perspective
detection method that incorporates external domain knowledge. Specifically, we
construct a political knowledge graph to serve as domain-specific external
knowledge. We then leverage heterogeneous information networks to represent
news documents, which jointly model news text and external knowledge. Finally,
we adopt relational graph neural networks and conduct political perspective
detection as graph-level classification. Extensive experiments demonstrate that
our method consistently achieves the best performance on two real-world
perspective detection benchmarks. Ablation studies further bear out the
necessity of external knowledge and the effectiveness of our graph-based
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Legislator Representation Learning with Social Context and Expert Knowledge. (arXiv:2108.03881v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03881">
<div class="article-summary-box-inner">
<span><p>Modeling the ideological perspectives of political actors is an essential
task in computational political science with applications in many downstream
tasks. Existing approaches are generally limited to textual data and voting
records, while they neglect the rich social context and valuable expert
knowledge for holistic evaluation. In this paper, we propose a representation
learning framework of political actors that jointly leverages social context
and expert knowledge. Specifically, we retrieve and extract factual statements
about legislators to leverage social context information. We then construct a
heterogeneous information network to incorporate social context and use
relational graph neural networks to learn legislator representations. Finally,
we train our model with three objectives to align representation learning with
expert knowledge, model ideological stance consistency, and simulate the echo
chamber phenomenon. Extensive experiments demonstrate that our learned
representations successfully advance the state-of-the-art in three downstream
tasks. Further analysis proves the correlation between learned legislator
representations and various socio-political factors, as well as bearing out the
necessity of social context and expert knowledge in modeling political actors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond NED: Fast and Effective Search Space Reduction for Complex Question Answering over Knowledge Bases. (arXiv:2108.08597v7 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08597">
<div class="article-summary-box-inner">
<span><p>Answering complex questions over knowledge bases (KB-QA) faces huge input
data with billions of facts, involving millions of entities and thousands of
predicates. For efficiency, QA systems first reduce the answer search space by
identifying a set of facts that is likely to contain all answers and relevant
cues. The most common technique or doing this is to apply named entity
disambiguation (NED) systems to the question, and retrieve KB facts for the
disambiguated entities. This work presents CLOCQ, an efficient method that
prunes irrelevant parts of the search space using KB-aware signals. CLOCQ uses
a top-k query processor over score-ordered lists of KB items that combine
signals about lexical matching, relevance to the question, coherence among
candidate items, and connectivity in the KB graph. Experiments with two recent
QA benchmarks for complex questions demonstrate the superiority of CLOCQ over
state-of-the-art baselines with respect to answer presence, size of the search
space, and runtimes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese. (arXiv:2109.09701v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09701">
<div class="article-summary-box-inner">
<span><p>In this paper, we present BARTpho with two versions BARTpho-syllable and
BARTpho-word, which are the first public large-scale monolingual
sequence-to-sequence models pre-trained for Vietnamese. BARTpho uses the
"large" architecture and the pre-training scheme of the sequence-to-sequence
denoising autoencoder BART, thus especially suitable for generative NLP tasks.
We conduct experiments to compare our BARTpho with its competitor mBART on a
downstream task of Vietnamese text summarization and show that: in both
automatic and human evaluations, BARTpho outperforms the strong baseline mBART
and improves the state-of-the-art. We release BARTpho to facilitate future
research and applications of generative Vietnamese NLP tasks. Our BARTpho
models are publicly available at: https://github.com/VinAIResearch/BARTpho
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clustering Vietnamese Conversations From Facebook Page To Build Training Dataset For Chatbot. (arXiv:2112.15338v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15338">
<div class="article-summary-box-inner">
<span><p>The biggest challenge of building chatbots is training data. The required
data must be realistic and large enough to train chatbots. We create a tool to
get actual training data from Facebook messenger of a Facebook page. After text
preprocessing steps, the newly obtained dataset generates FVnC and Sample
dataset. We use the Retraining of BERT for Vietnamese (PhoBERT) to extract
features of our text data. K-Means and DBSCAN clustering algorithms are used
for clustering tasks based on output embeddings from PhoBERT$_{base}$. We apply
V-measure score and Silhouette score to evaluate the performance of clustering
algorithms. We also demonstrate the efficiency of PhoBERT compared to other
models in feature extraction on the Sample dataset and wiki dataset. A
GridSearch algorithm that combines both clustering evaluations is also proposed
to find optimal parameters. Thanks to clustering such a number of
conversations, we save a lot of time and effort to build data and storylines
for training chatbot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hypers at ComMA@ICON: Modelling Aggressiveness, Gender Bias and Communal Bias Identification. (arXiv:2112.15417v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15417">
<div class="article-summary-box-inner">
<span><p>Due to the exponentially increasing reach of social media, it is essential to
focus on its negative aspects as it can potentially divide society and incite
people into violence. In this paper, we present our system description of work
on the shared task ComMA@ICON, where we have to classify how aggressive the
sentence is and if the sentence is gender-biased or communal biased. These
three could be the primary reasons to cause significant problems in society. As
team Hypers we have proposed an approach that utilizes different pretrained
models with Attention and mean pooling methods. We were able to get Rank 3 with
0.223 Instance F1 score on Bengali, Rank 2 with 0.322 Instance F1 score on
Multi-lingual set, Rank 4 with 0.129 Instance F1 score on Meitei and Rank 5
with 0.336 Instance F1 score on Hindi. The source code and the pretrained
models of this work can be found here.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Using Gaze Behaviour for Natural Language Processing. (arXiv:2112.15471v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15471">
<div class="article-summary-box-inner">
<span><p>Gaze behaviour has been used as a way to gather cognitive information for a
number of years. In this paper, we discuss the use of gaze behaviour in solving
different tasks in natural language processing (NLP) without having to record
it at test time. This is because the collection of gaze behaviour is a costly
task, both in terms of time and money. Hence, in this paper, we focus on
research done to alleviate the need for recording gaze behaviour at run time.
We also mention different eye tracking corpora in multiple languages, which are
currently available and can be used in natural language processing. We conclude
our paper by discussing applications in a domain - education - and how learning
gaze behaviour can help in solving the tasks of complex word identification and
automatic essay grading.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Dimensional Model Compression of Vision Transformer. (arXiv:2201.00043v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00043">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViT) have recently attracted considerable attentions,
but the huge computational cost remains an issue for practical deployment.
Previous ViT pruning methods tend to prune the model along one dimension
solely, which may suffer from excessive reduction and lead to sub-optimal model
quality. In contrast, we advocate a multi-dimensional ViT compression paradigm,
and propose to harness the redundancy reduction from attention head, neuron and
sequence dimensions jointly. We firstly propose a statistical dependence based
pruning criterion that is generalizable to different dimensions for identifying
deleterious components. Moreover, we cast the multi-dimensional compression as
an optimization, learning the optimal pruning policy across the three
dimensions that maximizes the compressed model's accuracy under a computational
budget. The problem is solved by our adapted Gaussian process search with
expected improvement. Experimental results show that our method effectively
reduces the computational cost of various ViT models. For example, our method
reduces 40\% FLOPs without top-1 accuracy loss for DeiT and T2T-ViT models,
outperforming previous state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iCaps: Iterative Category-level Object Pose and Shape Estimation. (arXiv:2201.00059v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00059">
<div class="article-summary-box-inner">
<span><p>This paper proposes a category-level 6D object pose and shape estimation
approach iCaps, which allows tracking 6D poses of unseen objects in a category
and estimating their 3D shapes. We develop a category-level auto-encoder
network using depth images as input, where feature embeddings from the
auto-encoder encode poses of objects in a category. The auto-encoder can be
used in a particle filter framework to estimate and track 6D poses of objects
in a category. By exploiting an implicit shape representation based on signed
distance functions, we build a LatentNet to estimate a latent representation of
the 3D shape given the estimated pose of an object. Then the estimated pose and
shape can be used to update each other in an iterative way. Our category-level
6D object pose and shape estimation pipeline only requires 2D detection and
segmentation for initialization. We evaluate our approach on a publicly
available dataset and demonstrate its effectiveness. In particular, our method
achieves comparably high accuracy on shape estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Croesus: Multi-Stage Processing and Transactions for Video-Analytics in Edge-Cloud Systems. (arXiv:2201.00063v1 [eess.SY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00063">
<div class="article-summary-box-inner">
<span><p>Emerging edge applications require both a fast response latency and complex
processing. This is infeasible without expensive hardware that can process
complex operations -- such as object detection -- within a short time. Many
approach this problem by addressing the complexity of the models -- via model
compression, pruning and quantization -- or compressing the input. In this
paper, we propose a different perspective when addressing the performance
challenges. Croesus is a multi-stage approach to edge-cloud systems that
provides the ability to find the balance between accuracy and performance.
Croesus consists of two stages (that can be generalized to multiple stages): an
initial and a final stage. The initial stage performs the computation in
real-time using approximate/best-effort computation at the edge. The final
stage performs the full computation at the cloud, and uses the results to
correct any errors made at the initial stage. In this paper, we demonstrate the
implications of such an approach on a video analytics use-case and show how
multi-stage processing yields a better balance between accuracy and
performance. Moreover, we study the safety of multi-stage transactions via two
proposals: multi-stage serializability (MS-SR) and multi-stage invariant
confluence with Apologies (MS-IA).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PatchTrack: Multiple Object Tracking Using Frame Patches. (arXiv:2201.00080v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00080">
<div class="article-summary-box-inner">
<span><p>Object motion and object appearance are commonly used information in multiple
object tracking (MOT) applications, either for associating detections across
frames in tracking-by-detection methods or direct track predictions for
joint-detection-and-tracking methods. However, not only are these two types of
information often considered separately, but also they do not help optimize the
usage of visual information from the current frame of interest directly. In
this paper, we present PatchTrack, a Transformer-based
joint-detection-and-tracking system that predicts tracks using patches of the
current frame of interest. We use the Kalman filter to predict the locations of
existing tracks in the current frame from the previous frame. Patches cropped
from the predicted bounding boxes are sent to the Transformer decoder to infer
new tracks. By utilizing both object motion and object appearance information
encoded in patches, the proposed method pays more attention to where new tracks
are more likely to occur. We show the effectiveness of PatchTrack on recent MOT
benchmarks, including MOT16 (MOTA 73.71%, IDF1 65.77%) and MOT17 (MOTA 73.59%,
IDF1 65.23%). The results are published on
https://motchallenge.net/method/MOT=4725&amp;chl=10.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Performance Comparison of Deep Learning Architectures for Artifact Removal in Gastrointestinal Endoscopic Imaging. (arXiv:2201.00084v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00084">
<div class="article-summary-box-inner">
<span><p>Endoscopic images typically contain several artifacts. The artifacts
significantly impact image analysis result in computer-aided diagnosis.
Convolutional neural networks (CNNs), a type of deep learning, can removes such
artifacts. Various architectures have been proposed for the CNNs, and the
accuracy of artifact removal varies depending on the choice of architecture.
Therefore, it is necessary to determine the artifact removal accuracy,
depending on the selected architecture. In this study, we focus on endoscopic
surgical instruments as artifacts, and determine and discuss the artifact
removal accuracy using seven different CNN architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computer Vision Based Parking Optimization System. (arXiv:2201.00095v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00095">
<div class="article-summary-box-inner">
<span><p>An improvement in technology is linearly related to time and time-relevant
problems. It has been seen that as time progresses, the number of problems
humans face also increases. However, technology to resolve these problems tends
to improve as well. One of the earliest existing problems which started with
the invention of vehicles was parking. The ease of resolving this problem using
technology has evolved over the years but the problem of parking still remains
unsolved. The main reason behind this is that parking does not only involve one
problem but it consists of a set of problems within itself. One of these
problems is the occupancy detection of the parking slots in a distributed
parking ecosystem. In a distributed system, users would find preferable parking
spaces as opposed to random parking spaces. In this paper, we propose a
web-based application as a solution for parking space detection in different
parking spaces. The solution is based on Computer Vision (CV) and is built
using the Django framework written in Python 3.0. The solution works to resolve
the occupancy detection problem along with providing the user the option to
determine the block based on availability and his preference. The evaluation
results for our proposed system are promising and efficient. The proposed
system can also be integrated with different systems and be used for solving
other relevant parking problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SalyPath360: Saliency and Scanpath Prediction Framework for Omnidirectional Images. (arXiv:2201.00096v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00096">
<div class="article-summary-box-inner">
<span><p>This paper introduces a new framework to predict visual attention of
omnidirectional images. The key setup of our architecture is the simultaneous
prediction of the saliency map and a corresponding scanpath for a given
stimulus. The framework implements a fully encoder-decoder convolutional neural
network augmented by an attention module to generate representative saliency
maps. In addition, an auxiliary network is employed to generate probable
viewport center fixation points through the SoftArgMax function. The latter
allows to derive fixation points from feature maps. To take advantage of the
scanpath prediction, an adaptive joint probability distribution model is then
applied to construct the final unbiased saliency map by leveraging the encoder
decoder-based saliency map and the scanpath-based saliency heatmap. The
proposed framework was evaluated in terms of saliency and scanpath prediction,
and the results were compared to state-of-the-art methods on Salient360!
dataset. The results showed the relevance of our framework and the benefits of
such architecture for further omnidirectional visual attention prediction
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Attack via Dual-Stage Network Erosion. (arXiv:2201.00097v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00097">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are vulnerable to adversarial examples, which can fool
deep models by adding subtle perturbations. Although existing attacks have
achieved promising results, it still leaves a long way to go for generating
transferable adversarial examples under the black-box setting. To this end,
this paper proposes to improve the transferability of adversarial examples, and
applies dual-stage feature-level perturbations to an existing model to
implicitly create a set of diverse models. Then these models are fused by the
longitudinal ensemble during the iterations. The proposed method is termed
Dual-Stage Network Erosion (DSNE). We conduct comprehensive experiments both on
non-residual and residual networks, and obtain more transferable adversarial
examples with the computational cost similar to the state-of-the-art method. In
particular, for the residual networks, the transferability of the adversarial
examples can be significantly improved by biasing the residual block
information to the skip connections. Our work provides new insights into the
architectural vulnerability of neural networks and presents new challenges to
the robustness of neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting RGB-D Saliency Detection by Leveraging Unlabeled RGB Images. (arXiv:2201.00100v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00100">
<div class="article-summary-box-inner">
<span><p>Training deep models for RGB-D salient object detection (SOD) often requires
a large number of labeled RGB-D images. However, RGB-D data is not easily
acquired, which limits the development of RGB-D SOD techniques. To alleviate
this issue, we present a Dual-Semi RGB-D Salient Object Detection Network
(DS-Net) to leverage unlabeled RGB images for boosting RGB-D saliency
detection. We first devise a depth decoupling convolutional neural network
(DDCNN), which contains a depth estimation branch and a saliency detection
branch. The depth estimation branch is trained with RGB-D images and then used
to estimate the pseudo depth maps for all unlabeled RGB images to form the
paired data. The saliency detection branch is used to fuse the RGB feature and
depth feature to predict the RGB-D saliency. Then, the whole DDCNN is assigned
as the backbone in a teacher-student framework for semi-supervised learning.
Moreover, we also introduce a consistency loss on the intermediate attention
and saliency maps for the unlabeled data, as well as a supervised depth and
saliency loss for labeled data. Experimental results on seven widely-used
benchmark datasets demonstrate that our DDCNN outperforms state-of-the-art
methods both quantitatively and qualitatively. We also demonstrate that our
semi-supervised DS-Net can further improve the performance, even when using an
RGB image with the pseudo depth map.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Region Feature Synthesizer for Zero-Shot Object Detection. (arXiv:2201.00103v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00103">
<div class="article-summary-box-inner">
<span><p>Zero-shot object detection aims at incorporating class semantic vectors to
realize the detection of (both seen and) unseen classes given an unconstrained
test image. In this study, we reveal the core challenges in this research area:
how to synthesize robust region features (for unseen objects) that are as
intra-class diverse and inter-class separable as the real samples, so that
strong unseen object detectors can be trained upon them. To address these
challenges, we build a novel zero-shot object detection framework that contains
an Intra-class Semantic Diverging component and an Inter-class Structure
Preserving component. The former is used to realize the one-to-more mapping to
obtain diverse visual features from each class semantic vector, preventing
miss-classifying the real unseen objects as image backgrounds. While the latter
is used to avoid the synthesized features too scattered to mix up the
inter-class and foreground-background relationship. To demonstrate the
effectiveness of the proposed approach, comprehensive experiments on PASCAL
VOC, COCO, and DIOR datasets are conducted. Notably, our approach achieves the
new state-of-the-art performance on PASCAL VOC and COCO and it is the first
study to carry out zero-shot object detection in remote sensing imagery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quality-aware Part Models for Occluded Person Re-identification. (arXiv:2201.00107v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00107">
<div class="article-summary-box-inner">
<span><p>Occlusion poses a major challenge for person re-identification (ReID).
Existing approaches typically rely on outside tools to infer visible body
parts, which may be suboptimal in terms of both computational efficiency and
ReID accuracy. In particular, they may fail when facing complex occlusions,
such as those between pedestrians. Accordingly, in this paper, we propose a
novel method named Quality-aware Part Models (QPM) for occlusion-robust ReID.
First, we propose to jointly learn part features and predict part quality
scores. As no quality annotation is available, we introduce a strategy that
automatically assigns low scores to occluded body parts, thereby weakening the
impact of occluded body parts on ReID results. Second, based on the predicted
part quality scores, we propose a novel identity-aware spatial attention (ISA)
module. In this module, a coarse identity-aware feature is utilized to
highlight pixels of the target pedestrian, so as to handle the occlusion
between pedestrians. Third, we design an adaptive and efficient approach for
generating global features from common non-occluded regions with respect to
each image pair. This design is crucial, but is often ignored by existing
methods. QPM has three key advantages: 1) it does not rely on any outside tools
in either the training or inference stages; 2) it handles occlusions caused by
both objects and other pedestrians;3) it is highly computationally efficient.
Experimental results on four popular databases for occluded ReID demonstrate
that QPM consistently outperforms state-of-the-art methods by significant
margins. The code of QPM will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SurfGen: Adversarial 3D Shape Synthesis with Explicit Surface Discriminators. (arXiv:2201.00112v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00112">
<div class="article-summary-box-inner">
<span><p>Recent advances in deep generative models have led to immense progress in 3D
shape synthesis. While existing models are able to synthesize shapes
represented as voxels, point-clouds, or implicit functions, these methods only
indirectly enforce the plausibility of the final 3D shape surface. Here we
present a 3D shape synthesis framework (SurfGen) that directly applies
adversarial training to the object surface. Our approach uses a differentiable
spherical projection layer to capture and represent the explicit zero
isosurface of an implicit 3D generator as functions defined on the unit sphere.
By processing the spherical representation of 3D object surfaces with a
spherical CNN in an adversarial setting, our generator can better learn the
statistics of natural shape surfaces. We evaluate our model on large-scale
shape datasets, and demonstrate that the end-to-end trained model is capable of
generating high fidelity 3D shapes with diverse topology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAFL: A Self-Attention Scene Text Recognizer with Focal Loss. (arXiv:2201.00132v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00132">
<div class="article-summary-box-inner">
<span><p>In the last decades, scene text recognition has gained worldwide attention
from both the academic community and actual users due to its importance in a
wide range of applications. Despite achievements in optical character
recognition, scene text recognition remains challenging due to inherent
problems such as distortions or irregular layout. Most of the existing
approaches mainly leverage recurrence or convolution-based neural networks.
However, while recurrent neural networks (RNNs) usually suffer from slow
training speed due to sequential computation and encounter problems as
vanishing gradient or bottleneck, CNN endures a trade-off between complexity
and performance. In this paper, we introduce SAFL, a self-attention-based
neural network model with the focal loss for scene text recognition, to
overcome the limitation of the existing approaches. The use of focal loss
instead of negative log-likelihood helps the model focus more on low-frequency
samples training. Moreover, to deal with the distortions and irregular texts,
we exploit Spatial TransformerNetwork (STN) to rectify text before passing to
the recognition network. We perform experiments to compare the performance of
the proposed model with seven benchmarks. The numerical results show that our
model achieves the best performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Feature Uncertainty in Stochastic Neural Networks for Adversarial Robustness. (arXiv:2201.00148v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00148">
<div class="article-summary-box-inner">
<span><p>It is well-known that deep neural networks (DNNs) have shown remarkable
success in many fields. However, when adding an imperceptible magnitude
perturbation on the model input, the model performance might get rapid
decrease. To address this issue, a randomness technique has been proposed
recently, named Stochastic Neural Networks (SNNs). Specifically, SNNs inject
randomness into the model to defend against unseen attacks and improve the
adversarial robustness. However, existed studies on SNNs mainly focus on
injecting fixed or learnable noises to model weights/activations. In this
paper, we find that the existed SNNs performances are largely bottlenecked by
the feature representation ability. Surprisingly, simply maximizing the
variance per dimension of the feature distribution leads to a considerable
boost beyond all previous methods, which we named maximize feature distribution
variance stochastic neural network (MFDV-SNN). Extensive experiments on
well-known white- and black-box attacks show that MFDV-SNN achieves a
significant improvement over existing methods, which indicates that it is a
simple but effective method to improve model robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Single Image Deblurring. (arXiv:2201.00155v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00155">
<div class="article-summary-box-inner">
<span><p>This paper tackles the problem of dynamic scene deblurring. Although
end-to-end fully convolutional designs have recently advanced the
state-of-the-art in non-uniform motion deblurring, their performance-complexity
trade-off is still sub-optimal. Existing approaches achieve a large receptive
field by a simple increment in the number of generic convolution layers,
kernel-size, which comes with the burden of the increase in model size and
inference speed. In this work, we propose an efficient pixel adaptive and
feature attentive design for handling large blur variations within and across
different images. We also propose an effective content-aware global-local
filtering module that significantly improves the performance by considering not
only the global dependencies of the pixel but also dynamically using the
neighboring pixels. We use a patch hierarchical attentive architecture composed
of the above module that implicitly discover the spatial variations in the blur
present in the input image and in turn perform local and global modulation of
intermediate features. Extensive qualitative and quantitative comparisons with
prior art on deblurring benchmarks demonstrate the superiority of the proposed
network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Development of Diabetic Foot Ulcer Datasets: An Overview. (arXiv:2201.00163v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00163">
<div class="article-summary-box-inner">
<span><p>This paper provides conceptual foundation and procedures used in the
development of diabetic foot ulcer datasets over the past decade, with a
timeline to demonstrate progress. We conduct a survey on data capturing methods
for foot photographs, an overview of research in developing private and public
datasets, the related computer vision tasks (detection, segmentation and
classification), the diabetic foot ulcer challenges and the future direction of
the development of the datasets. We report the distribution of dataset users by
country and year. Our aim is to share the technical challenges that we
encountered together with good practices in dataset development, and provide
motivation for other researchers to participate in data sharing in this domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-attention Multi-view Representation Learning with Diversity-promoting Complementarity. (arXiv:2201.00168v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00168">
<div class="article-summary-box-inner">
<span><p>Multi-view learning attempts to generate a model with a better performance by
exploiting the consensus and/or complementarity among multi-view data. However,
in terms of complementarity, most existing approaches only can find
representations with single complementarity rather than complementary
information with diversity. In this paper, to utilize both complementarity and
consistency simultaneously, give free rein to the potential of deep learning in
grasping diversity-promoting complementarity for multi-view representation
learning, we propose a novel supervised multi-view representation learning
algorithm, called Self-Attention Multi-View network with Diversity-Promoting
Complementarity (SAMVDPC), which exploits the consistency by a group of
encoders, uses self-attention to find complementary information entailing
diversity. Extensive experiments conducted on eight real-world datasets have
demonstrated the effectiveness of our proposed method, and show its superiority
over several baseline methods, which only consider single complementary
information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Scene Video Deblurring using Non-Local Attention. (arXiv:2201.00169v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00169">
<div class="article-summary-box-inner">
<span><p>This paper tackles the challenging problem of video deblurring. Most of the
existing works depend on implicit or explicit alignment for temporal
information fusion which either increase the computational cost or result in
suboptimal performance due to wrong alignment. In this study, we propose a
factorized spatio-temporal attention to perform non-local operations across
space and time to fully utilize the available information without depending on
alignment. It shows superior performance compared to existing fusion techniques
while being much efficient. Extensive experiments on multiple datasets
demonstrate the superiority of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-view Subspace Adaptive Learning via Autoencoder and Attention. (arXiv:2201.00171v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00171">
<div class="article-summary-box-inner">
<span><p>Multi-view learning can cover all features of data samples more
comprehensively, so multi-view learning has attracted widespread attention.
Traditional subspace clustering methods, such as sparse subspace clustering
(SSC) and low-ranking subspace clustering (LRSC), cluster the affinity matrix
for a single view, thus ignoring the problem of fusion between views. In our
article, we propose a new Multiview Subspace Adaptive Learning based on
Attention and Autoencoder (MSALAA). This method combines a deep autoencoder and
a method for aligning the self-representations of various views in Multi-view
Low-Rank Sparse Subspace Clustering (MLRSSC), which can not only increase the
capability to non-linearity fitting, but also can meets the principles of
consistency and complementarity of multi-view learning. We empirically observe
significant improvement over existing baseline methods on six real-life
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Image Inpainting. (arXiv:2201.00177v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00177">
<div class="article-summary-box-inner">
<span><p>Image inpainting methods have shown significant improvements by using deep
neural networks recently. However, many of these techniques often create
distorted structures or blurry textures inconsistent with surrounding areas.
The problem is rooted in the encoder layers' ineffectiveness in building a
complete and faithful embedding of the missing regions. To address this
problem, two-stage approaches deploy two separate networks for a coarse and
fine estimate of the inpainted image. Some approaches utilize handcrafted
features like edges or contours to guide the reconstruction process. These
methods suffer from huge computational overheads owing to multiple generator
networks, limited ability of handcrafted features, and sub-optimal utilization
of the information present in the ground truth. Motivated by these
observations, we propose a distillation based approach for inpainting, where we
provide direct feature level supervision for the encoder layers in an adaptive
manner. We deploy cross and self distillation techniques and discuss the need
for a dedicated completion-block in encoder to achieve the distillation target.
We conduct extensive evaluations on multiple datasets to validate our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Restoration using Feature-guidance. (arXiv:2201.00187v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00187">
<div class="article-summary-box-inner">
<span><p>Image restoration is the task of recovering a clean image from a degraded
version. In most cases, the degradation is spatially varying, and it requires
the restoration network to both localize and restore the affected regions. In
this paper, we present a new approach suitable for handling the image-specific
and spatially-varying nature of degradation in images affected by practically
occurring artifacts such as blur, rain-streaks. We decompose the restoration
task into two stages of degradation localization and degraded region-guided
restoration, unlike existing methods which directly learn a mapping between the
degraded and clean images. Our premise is to use the auxiliary task of
degradation mask prediction to guide the restoration process. We demonstrate
that the model trained for this auxiliary task contains vital region knowledge,
which can be exploited to guide the restoration network's training using
attentive knowledge distillation technique. Further, we propose mask-guided
convolution and global context aggregation module that focuses solely on
restoring the degraded regions. The proposed approach's effectiveness is
demonstrated by achieving significant improvement over strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Turath-150K: Image Database of Arab Heritage. (arXiv:2201.00220v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00220">
<div class="article-summary-box-inner">
<span><p>Large-scale image databases remain largely biased towards objects and
activities encountered in a select few cultures. This absence of
culturally-diverse images, which we refer to as the hidden tail, limits the
applicability of pre-trained neural networks and inadvertently excludes
researchers from under-represented regions. To begin remedying this issue, we
curate Turath-150K, a database of images of the Arab world that reflect
objects, activities, and scenarios commonly found there. In the process, we
introduce three benchmark databases, Turath Standard, Art, and UNESCO,
specialised subsets of the Turath dataset. After demonstrating the limitations
of existing networks pre-trained on ImageNet when deployed on such benchmarks,
we train and evaluate several networks on the task of image classification. As
a consequence of Turath, we hope to engage machine learning researchers in
under-represented regions, and to inspire the release of additional
culture-focused databases. The database can be accessed here:
danikiyasseh.github.io/Turath.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Applications for Lung Cancer Diagnosis: A systematic review. (arXiv:2201.00227v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00227">
<div class="article-summary-box-inner">
<span><p>Lung cancer has been one of the most prevalent disease in recent years.
According to the research of this field, more than 200,000 cases are identified
each year in the US. Uncontrolled multiplication and growth of the lung cells
result in malignant tumour formation. Recently, deep learning algorithms,
especially Convolutional Neural Networks (CNN), have become a superior way to
automatically diagnose disease. The purpose of this article is to review
different models that lead to different accuracy and sensitivity in the
diagnosis of early-stage lung cancer and to help physicians and researchers in
this field. The main purpose of this work is to identify the challenges that
exist in lung cancer based on deep learning. The survey is systematically
written that combines regular mapping and literature review to review 32
conference and journal articles in the field from 2016 to 2021. After analysing
and reviewing the articles, the questions raised in the articles are being
answered. This research is superior to other review articles in this field due
to the complete review of relevant articles and systematic write up.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SporeAgent: Reinforced Scene-level Plausibility for Object Pose Refinement. (arXiv:2201.00239v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00239">
<div class="article-summary-box-inner">
<span><p>Observational noise, inaccurate segmentation and ambiguity due to symmetry
and occlusion lead to inaccurate object pose estimates. While depth- and
RGB-based pose refinement approaches increase the accuracy of the resulting
pose estimates, they are susceptible to ambiguity in the observation as they
consider visual alignment. We propose to leverage the fact that we often
observe static, rigid scenes. Thus, the objects therein need to be under
physically plausible poses. We show that considering plausibility reduces
ambiguity and, in consequence, allows poses to be more accurately predicted in
cluttered environments. To this end, we extend a recent RL-based registration
approach towards iterative refinement of object poses. Experiments on the
LINEMOD and YCB-VIDEO datasets demonstrate the state-of-the-art performance of
our depth-based refinement approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subspace modeling for fast and high-sensitivity X-ray chemical imaging. (arXiv:2201.00259v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00259">
<div class="article-summary-box-inner">
<span><p>Resolving morphological chemical phase transformations at the nanoscale is of
vital importance to many scientific and industrial applications across various
disciplines. The TXM-XANES imaging technique, by combining full field
transmission X-ray microscopy (TXM) and X-ray absorption near edge structure
(XANES), has been an emerging tool which operates by acquiring a series of
microscopy images with multi-energy X-rays and fitting to obtain the chemical
map. Its capability, however, is limited by the poor signal-to-noise ratios due
to the system errors and low exposure illuminations for fast acquisition. In
this work, by exploiting the intrinsic properties and subspace modeling of the
TXM-XANES imaging data, we introduce a simple and robust denoising approach to
improve the image quality, which enables fast and high-sensitivity chemical
imaging. Extensive experiments on both synthetic and real datasets demonstrate
the superior performance of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Cross-dataset Generalization for License Plate Recognition. (arXiv:2201.00267v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00267">
<div class="article-summary-box-inner">
<span><p>Automatic License Plate Recognition (ALPR) systems have shown remarkable
performance on license plates (LPs) from multiple regions due to advances in
deep learning and the increasing availability of datasets. The evaluation of
deep ALPR systems is usually done within each dataset; therefore, it is
questionable if such results are a reliable indicator of generalization
ability. In this paper, we propose a traditional-split versus
leave-one-dataset-out experimental setup to empirically assess the
cross-dataset generalization of 12 Optical Character Recognition (OCR) models
applied to LP recognition on nine publicly available datasets with a great
variety in several aspects (e.g., acquisition settings, image resolution, and
LP layouts). We also introduce a public dataset for end-to-end ALPR that is the
first to contain images of vehicles with Mercosur LPs and the one with the
highest number of motorcycle images. The experimental results shed light on the
limitations of the traditional-split protocol for evaluating approaches in the
ALPR context, as there are significant drops in performance for most datasets
when training and testing the models in a leave-one-dataset-out fashion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents. (arXiv:2201.00308v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00308">
<div class="article-summary-box-inner">
<span><p>Diffusion Probabilistic models have been shown to generate state-of-the-art
results on several competitive image synthesis benchmarks but lack a
low-dimensional, interpretable latent space, and are slow at generation. On the
other hand, Variational Autoencoders (VAEs) typically have access to a
low-dimensional latent space but exhibit poor sample quality. Despite recent
advances, VAEs usually require high-dimensional hierarchies of the latent codes
to generate high-quality samples. We present DiffuseVAE, a novel generative
framework that integrates VAE within a diffusion model framework, and leverage
this to design a novel conditional parameterization for diffusion models. We
show that the resulting model can improve upon the unconditional diffusion
model in terms of sampling efficiency while also equipping diffusion models
with the low-dimensional VAE inferred latent code. Furthermore, we show that
the proposed model can generate high-resolution samples and exhibits synthesis
quality comparable to state-of-the-art models on standard benchmarks. Lastly,
we show that the proposed method can be used for controllable image synthesis
and also exhibits out-of-the-box capabilities for downstream tasks like image
super-resolution and denoising. For reproducibility, our source code is
publicly available at \url{https://github.com/kpandey008/DiffuseVAE}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recurrent Feature Propagation and Edge Skip-Connections for Automatic Abdominal Organ Segmentation. (arXiv:2201.00317v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00317">
<div class="article-summary-box-inner">
<span><p>Automatic segmentation of abdominal organs in computed tomography (CT) images
can support radiation therapy and image-guided surgery workflows. Developing of
such automatic solutions remains challenging mainly owing to complex organ
interactions and blurry boundaries in CT images. To address these issues, we
focus on effective spatial context modeling and explicit edge segmentation
priors. Accordingly, we propose a 3D network with four main components trained
end-to-end including shared encoder, edge detector, decoder with edge
skip-connections (ESCs) and recurrent feature propagation head (RFP-Head). To
capture wide-range spatial dependencies, the RFP-Head propagates and harvests
local features through directed acyclic graphs (DAGs) formulated with recurrent
connections in an efficient slice-wise manner, with regard to spatial
arrangement of image units. To leverage edge information, the edge detector
learns edge prior knowledge specifically tuned for semantic segmentation by
exploiting intermediate features from the encoder with the edge supervision.
The ESCs then aggregate the edge knowledge with multi-level decoder features to
learn a hierarchy of discriminative features explicitly modeling
complementarity between organs' interiors and edges for segmentation. We
conduct extensive experiments on two challenging abdominal CT datasets with
eight annotated organs. Experimental results show that the proposed network
outperforms several state-of-the-art models, especially for the segmentation of
small and complicated structures (gallbladder, esophagus, stomach, pancreas and
duodenum). The code will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">V-LinkNet: Learning Contextual Inpainting Across Latent Space of Generative Adversarial Network. (arXiv:2201.00323v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00323">
<div class="article-summary-box-inner">
<span><p>Deep learning methods outperform traditional methods in image inpainting. In
order to generate contextual textures, researchers are still working to improve
on existing methods and propose models that can extract, propagate, and
reconstruct features similar to ground-truth regions. Furthermore, the lack of
a high-quality feature transfer mechanism in deeper layers contributes to
persistent aberrations on generated inpainted regions. To address these
limitations, we propose the V-LinkNet cross-space learning strategy network. To
improve learning on contextualised features, we design a loss model that
employs both encoders. In addition, we propose a recursive residual transition
layer (RSTL). The RSTL extracts high-level semantic information and propagates
it down layers. Finally, we compare inpainting performance on the same face
with different masks and on different faces with the same masks. To improve
image inpainting reproducibility, we propose a standard protocol to overcome
biases with various masks and images. We investigate the V-LinkNet components
using experimental methods. Our result surpasses the state of the art when
evaluated on the CelebA-HQ with the standard protocol. In addition, our model
can generalise well when evaluated on Paris Street View, and Places2 datasets
with the standard protocol.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Riemannian Nearest-Regularized Subspace Classification for Polarimetric SAR images. (arXiv:2201.00337v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00337">
<div class="article-summary-box-inner">
<span><p>As a representation learning method, nearest regularized subspace(NRS)
algorithm is an effective tool to obtain both accuracy and speed for PolSAR
image classification. However, existing NRS methods use the polarimetric
feature vector but the PolSAR original covariance matrix(known as Hermitian
positive definite(HPD)matrix) as the input. Without considering the matrix
structure, existing NRS-based methods cannot learn correlation among channels.
How to utilize the original covariance matrix to NRS method is a key problem.
To address this limit, a Riemannian NRS method is proposed, which consider the
HPD matrices endow in the Riemannian space. Firstly, to utilize the PolSAR
original data, a Riemannian NRS method(RNRS) is proposed by constructing HPD
dictionary and HPD distance metric. Secondly, a new Tikhonov regularization
term is designed to reduce the differences within the same class. Finally, the
optimal method is developed and the first-order derivation is inferred. During
the experimental test, only T matrix is used in the proposed method, while
multiple of features are utilized for compared methods. Experimental results
demonstrate the proposed method can outperform the state-of-art algorithms even
using less features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detail-Preserving Transformer for Light Field Image Super-Resolution. (arXiv:2201.00346v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00346">
<div class="article-summary-box-inner">
<span><p>Recently, numerous algorithms have been developed to tackle the problem of
light field super-resolution (LFSR), i.e., super-resolving low-resolution light
fields to gain high-resolution views. Despite delivering encouraging results,
these approaches are all convolution-based, and are naturally weak in global
relation modeling of sub-aperture images necessarily to characterize the
inherent structure of light fields. In this paper, we put forth a novel
formulation built upon Transformers, by treating LFSR as a sequence-to-sequence
reconstruction task. In particular, our model regards sub-aperture images of
each vertical or horizontal angular view as a sequence, and establishes
long-range geometric dependencies within each sequence via a spatial-angular
locally-enhanced self-attention layer, which maintains the locality of each
sub-aperture image as well. Additionally, to better recover image details, we
propose a detail-preserving Transformer (termed as DPT), by leveraging gradient
maps of light field to guide the sequence learning. DPT consists of two
branches, with each associated with a Transformer for learning from an original
or gradient image sequence. The two branches are finally fused to obtain
comprehensive feature representations for reconstruction. Evaluations are
conducted on a number of light field datasets, including real-world scenes and
synthetic data. The proposed method achieves superior performance comparing
with other state-of-the-art schemes. Our code is publicly available at:
https://github.com/BITszwang/DPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parkour Spot ID: Feature Matching in Satellite and Street view images using Deep Learning. (arXiv:2201.00377v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00377">
<div class="article-summary-box-inner">
<span><p>How to find places that are not indexed by Google Maps? We propose an
intuitive method and framework to locate places based on their distinctive
spatial features. The method uses satellite and street view images in machine
vision approaches to classify locations. If we can classify locations, we just
need to repeat for non-overlapping locations in our area of interest. We assess
the proposed system in finding Parkour spots in the campus of Arizona State
University. The results are very satisfactory, having found more than 25 new
Parkour spots, with a rate of true positives above 60%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast and High-Quality Image Denoising via Malleable Convolutions. (arXiv:2201.00392v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00392">
<div class="article-summary-box-inner">
<span><p>Many image processing networks apply a single set of static convolutional
kernels across the entire input image, which is sub-optimal for natural images,
as they often consist of heterogeneous visual patterns. Recent work in
classification, segmentation, and image restoration has demonstrated that
dynamic kernels outperform static kernels at modeling local image statistics.
However, these works often adopt per-pixel convolution kernels, which introduce
high memory and computation costs. To achieve spatial-varying processing
without significant overhead, we present \textbf{Malle}able
\textbf{Conv}olution (\textbf{MalleConv}), as an efficient variant of dynamic
convolution. The weights of \ours are dynamically produced by an efficient
predictor network capable of generating content-dependent outputs at specific
spatial locations. Unlike previous works, \ours generates a much smaller set of
spatially-varying kernels from input, which enlarges the network's receptive
field and significantly reduces computational and memory costs. These kernels
are then applied to a full-resolution feature map through an efficient
slice-and-conv operator with minimum memory overhead. We further build a
efficient denoising network using MalleConv, coined as \textbf{MalleNet}. It
achieves high quality results without very deep architecture, \eg, it is
8.91$\times$ faster than the best performed denoising algorithms (SwinIR),
while maintaining similar performance. We also show that a single \ours added
to a standard convolution-based backbones can contribute significantly reduce
the computational cost or boost image quality at similar cost. Project page:
https://yifanjiang.net/MalleConv.html
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MHATC: Autism Spectrum Disorder identification utilizing multi-head attention encoder along with temporal consolidation modules. (arXiv:2201.00404v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00404">
<div class="article-summary-box-inner">
<span><p>Resting-state fMRI is commonly used for diagnosing Autism Spectrum Disorder
(ASD) by using network-based functional connectivity. It has been shown that
ASD is associated with brain regions and their inter-connections. However,
discriminating based on connectivity patterns among imaging data of the control
population and that of ASD patients' brains is a non-trivial task. In order to
tackle said classification task, we propose a novel deep learning architecture
(MHATC) consisting of multi-head attention and temporal consolidation modules
for classifying an individual as a patient of ASD. The devised architecture
results from an in-depth analysis of the limitations of current deep neural
network solutions for similar applications. Our approach is not only robust but
computationally efficient, which can allow its adoption in a variety of other
research and clinical settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Introspective Agent: Interdependence of Strategy, Physiology, and Sensing for Embodied Agents. (arXiv:2201.00411v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00411">
<div class="article-summary-box-inner">
<span><p>The last few years have witnessed substantial progress in the field of
embodied AI where artificial agents, mirroring biological counterparts, are now
able to learn from interaction to accomplish complex tasks. Despite this
success, biological organisms still hold one large advantage over these
simulated agents: adaptation. While both living and simulated agents make
decisions to achieve goals (strategy), biological organisms have evolved to
understand their environment (sensing) and respond to it (physiology). The net
gain of these factors depends on the environment, and organisms have adapted
accordingly. For example, in a low vision aquatic environment some fish have
evolved specific neurons which offer a predictable, but incredibly rapid,
strategy to escape from predators. Mammals have lost these reactive systems,
but they have a much larger fields of view and brain circuitry capable of
understanding many future possibilities. While traditional embodied agents
manipulate an environment to best achieve a goal, we argue for an introspective
agent, which considers its own abilities in the context of its environment. We
show that different environments yield vastly different optimal designs, and
increasing long-term planning is often far less beneficial than other
improvements, such as increased physical ability. We present these findings to
broaden the definition of improvement in embodied AI passed increasingly
complex models. Just as in nature, we hope to reframe strategy as one tool,
among many, to succeed in an environment. Code is available at:
https://github.com/sarahpratt/introspective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FUSeg: The Foot Ulcer Segmentation Challenge. (arXiv:2201.00414v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00414">
<div class="article-summary-box-inner">
<span><p>Acute and chronic wounds with varying etiologies burden the healthcare
systems economically. The advanced wound care market is estimated to reach $22
billion by 2024. Wound care professionals provide proper diagnosis and
treatment with heavy reliance on images and image documentation. Segmentation
of wound boundaries in images is a key component of the care and diagnosis
protocol since it is important to estimate the area of the wound and provide
quantitative measurement for the treatment. Unfortunately, this process is very
time-consuming and requires a high level of expertise. Recently automatic wound
segmentation methods based on deep learning have shown promising performance
but require large datasets for training and it is unclear which methods perform
better. To address these issues, we propose the Foot Ulcer Segmentation
challenge (FUSeg) organized in conjunction with the 2021 International
Conference on Medical Image Computing and Computer Assisted Intervention
(MICCAI). We built a wound image dataset containing 1,210 foot ulcer images
collected over 2 years from 889 patients. It is pixel-wise annotated by wound
care experts and split into a training set with 1010 images and a testing set
with 200 images for evaluation. Teams around the world developed automated
methods to predict wound segmentations on the testing set of which annotations
were kept private. The predictions were evaluated and ranked based on the
average Dice coefficient. The FUSeg challenge remains an open challenge as a
benchmark for wound segmentation after the conference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Splicing ViT Features for Semantic Appearance Transfer. (arXiv:2201.00424v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00424">
<div class="article-summary-box-inner">
<span><p>We present a method for semantically transferring the visual appearance of
one natural image to another. Specifically, our goal is to generate an image in
which objects in a source structure image are "painted" with the visual
appearance of their semantically related objects in a target appearance image.
Our method works by training a generator given only a single
structure/appearance image pair as input. To integrate semantic information
into our framework - a pivotal component in tackling this task - our key idea
is to leverage a pre-trained and fixed Vision Transformer (ViT) model which
serves as an external semantic prior. Specifically, we derive novel
representations of structure and appearance extracted from deep ViT features,
untwisting them from the learned self-attention modules. We then establish an
objective function that splices the desired structure and appearance
representations, interweaving them together in the space of ViT features. Our
framework, which we term "Splice", does not involve adversarial training, nor
does it require any additional input information such as semantic segmentation
or correspondences, and can generate high-resolution results, e.g., work in HD.
We demonstrate high quality results on a variety of in-the-wild image pairs,
under significant variations in the number of objects, their pose and
appearance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Denoising with Control over Deep Network Hallucination. (arXiv:2201.00429v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00429">
<div class="article-summary-box-inner">
<span><p>Deep image denoisers achieve state-of-the-art results but with a hidden cost.
As witnessed in recent literature, these deep networks are capable of
overfitting their training distributions, causing inaccurate hallucinations to
be added to the output and generalizing poorly to varying data. For better
control and interpretability over a deep denoiser, we propose a novel framework
exploiting a denoising network. We call it controllable confidence-based image
denoising (CCID). In this framework, we exploit the outputs of a deep denoising
network alongside an image convolved with a reliable filter. Such a filter can
be a simple convolution kernel which does not risk adding hallucinated
information. We propose to fuse the two components with a frequency-domain
approach that takes into account the reliability of the deep network outputs.
With our framework, the user can control the fusion of the two components in
the frequency domain. We also provide a user-friendly map estimating spatially
the confidence in the output that potentially contains network hallucination.
Results show that our CCID not only provides more interpretability and control,
but can even outperform both the quantitative performance of the deep denoiser
and that of the reliable filter, especially when the test data diverge from the
training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TVNet: Temporal Voting Network for Action Localization. (arXiv:2201.00434v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00434">
<div class="article-summary-box-inner">
<span><p>We propose a Temporal Voting Network (TVNet) for action localization in
untrimmed videos. This incorporates a novel Voting Evidence Module to locate
temporal boundaries, more accurately, where temporal contextual evidence is
accumulated to predict frame-level probabilities of start and end action
boundaries. Our action-independent evidence module is incorporated within a
pipeline to calculate confidence scores and action classes. We achieve an
average mAP of 34.6% on ActivityNet-1.3, particularly outperforming previous
methods with the highest IoU of 0.95. TVNet also achieves mAP of 56.0% when
combined with PGCN and 59.1% with MUSES at 0.5 IoU on THUMOS14 and outperforms
prior work at all thresholds. Our code is available at
https://github.com/hanielwang/TVNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salient Object Detection by LTP Texture Characterization on Opposing Color Pairs under SLICO Superpixel Constraint. (arXiv:2201.00439v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00439">
<div class="article-summary-box-inner">
<span><p>The effortless detection of salient objects by humans has been the subject of
research in several fields, including computer vision as it has many
applications. However, salient object detection remains a challenge for many
computer models dealing with color and textured images. Herein, we propose a
novel and efficient strategy, through a simple model, almost without internal
parameters, which generates a robust saliency map for a natural image. This
strategy consists of integrating color information into local textural patterns
to characterize a color micro-texture. Most models in the literature that use
the color and texture features treat them separately. In our case, it is the
simple, yet powerful LTP (Local Ternary Patterns) texture descriptor applied to
opposing color pairs of a color space that allows us to achieve this end. Each
color micro-texture is represented by vector whose components are from a
superpixel obtained by SLICO (Simple Linear Iterative Clustering with zero
parameter) algorithm which is simple, fast and exhibits state-of-the-art
boundary adherence. The degree of dissimilarity between each pair of color
micro-texture is computed by the FastMap method, a fast version of MDS
(Multi-dimensional Scaling), that considers the color micro-textures
non-linearity while preserving their distances. These degrees of dissimilarity
give us an intermediate saliency map for each RGB, HSL, LUV and CMY color
spaces. The final saliency map is their combination to take advantage of the
strength of each of them. The MAE (Mean Absolute Error) and F$_{\beta}$
measures of our saliency maps, on the complex ECSSD dataset show that our model
is both simple and efficient, outperforming several state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scene Graph Generation: A Comprehensive Survey. (arXiv:2201.00443v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00443">
<div class="article-summary-box-inner">
<span><p>Deep learning techniques have led to remarkable breakthroughs in the field of
generic object detection and have spawned a lot of scene-understanding tasks in
recent years. Scene graph has been the focus of research because of its
powerful semantic representation and applications to scene understanding. Scene
Graph Generation (SGG) refers to the task of automatically mapping an image
into a semantic structural scene graph, which requires the correct labeling of
detected objects and their relationships. Although this is a challenging task,
the community has proposed a lot of SGG approaches and achieved good results.
In this paper, we provide a comprehensive survey of recent achievements in this
field brought about by deep learning techniques. We review 138 representative
works that cover different input modalities, and systematically summarize
existing methods of image-based SGG from the perspective of feature extraction
and fusion. We attempt to connect and systematize the existing visual
relationship detection methods, to summarize, and interpret the mechanisms and
the strategies of SGG in a comprehensive way. Finally, we finish this survey
with deep discussions about current existing problems and future research
directions. This survey will help readers to develop a better understanding of
the current research status and ideas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-Guided Semantic Learning Network for Temporal Sentence Grounding. (arXiv:2201.00454v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00454">
<div class="article-summary-box-inner">
<span><p>Temporal sentence grounding (TSG) is crucial and fundamental for video
understanding. Although the existing methods train well-designed deep networks
with a large amount of data, we find that they can easily forget the rarely
appeared cases in the training stage due to the off-balance data distribution,
which influences the model generalization and leads to undesirable performance.
To tackle this issue, we propose a memory-augmented network, called
Memory-Guided Semantic Learning Network (MGSL-Net), that learns and memorizes
the rarely appeared content in TSG tasks. Specifically, MGSL-Net consists of
three main parts: a cross-modal inter-action module, a memory augmentation
module, and a heterogeneous attention module. We first align the given
video-query pair by a cross-modal graph convolutional network, and then utilize
a memory module to record the cross-modal shared semantic features in the
domain-specific persistent memory. During training, the memory slots are
dynamically associated with both common and rare cases, alleviating the
forgetting issue. In testing, the rare cases can thus be enhanced by retrieving
the stored memories, resulting in better generalization. At last, the
heterogeneous attention module is utilized to integrate the enhanced
multi-modal features in both video and query domains. Experimental results on
three benchmarks show the superiority of our method on both effectiveness and
efficiency, which substantially improves the accuracy not only on the entire
dataset but also on rare cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Motion and Appearance Information for Temporal Sentence Grounding. (arXiv:2201.00457v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00457">
<div class="article-summary-box-inner">
<span><p>This paper addresses temporal sentence grounding. Previous works typically
solve this task by learning frame-level video features and align them with the
textual information. A major limitation of these works is that they fail to
distinguish ambiguous video frames with subtle appearance differences due to
frame-level feature extraction. Recently, a few methods adopt Faster R-CNN to
extract detailed object features in each frame to differentiate the
fine-grained appearance similarities. However, the object-level features
extracted by Faster R-CNN suffer from missing motion analysis since the object
detection model lacks temporal modeling. To solve this issue, we propose a
novel Motion-Appearance Reasoning Network (MARN), which incorporates both
motion-aware and appearance-aware object features to better reason object
relations for modeling the activity among successive frames. Specifically, we
first introduce two individual video encoders to embed the video into
corresponding motion-oriented and appearance-aspect object representations.
Then, we develop separate motion and appearance branches to learn motion-guided
and appearance-guided object relations, respectively. At last, both motion and
appearance information from two branches are associated to generate more
representative features for final grounding. Extensive experiments on two
challenging datasets (Charades-STA and TACoS) show that our proposed MARN
significantly outperforms previous state-of-the-art methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lung-Originated Tumor Segmentation from Computed Tomography Scan (LOTUS) Benchmark. (arXiv:2201.00458v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00458">
<div class="article-summary-box-inner">
<span><p>Lung cancer is one of the deadliest cancers, and in part its effective
diagnosis and treatment depend on the accurate delineation of the tumor.
Human-centered segmentation, which is currently the most common approach, is
subject to inter-observer variability, and is also time-consuming, considering
the fact that only experts are capable of providing annotations. Automatic and
semi-automatic tumor segmentation methods have recently shown promising
results. However, as different researchers have validated their algorithms
using various datasets and performance metrics, reliably evaluating these
methods is still an open challenge. The goal of the Lung-Originated Tumor
Segmentation from Computed Tomography Scan (LOTUS) Benchmark created through
2018 IEEE Video and Image Processing (VIP) Cup competition, is to provide a
unique dataset and pre-defined metrics, so that different researchers can
develop and evaluate their methods in a unified fashion. The 2018 VIP Cup
started with a global engagement from 42 countries to access the competition
data. At the registration stage, there were 129 members clustered into 28 teams
from 10 countries, out of which 9 teams made it to the final stage and 6 teams
successfully completed all the required tasks. In a nutshell, all the
algorithms proposed during the competition, are based on deep learning models
combined with a false positive reduction technique. Methods developed by the
three finalists show promising results in tumor segmentation, however, more
effort should be put into reducing the false positive rate. This competition
manuscript presents an overview of the VIP-Cup challenge, along with the
proposed algorithms and results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biometrics in the Time of Pandemic: 40% Masked Face Recognition Degradation can be Reduced to 2%. (arXiv:2201.00461v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00461">
<div class="article-summary-box-inner">
<span><p>In this study of the face recognition on masked versus unmasked faces
generated using Flickr-Faces-HQ and SpeakingFaces datasets, we report 36.78%
degradation of recognition performance caused by the mask-wearing at the time
of pandemics, in particular, in border checkpoint scenarios. We have achieved
better performance and reduced the degradation to 1.79% using advanced deep
learning approaches in the cross-spectral domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D-Former: A U-shaped Dilated Transformer for 3D Medical Image Segmentation. (arXiv:2201.00462v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00462">
<div class="article-summary-box-inner">
<span><p>Computer-aided medical image segmentation has been applied widely in
diagnosis and treatment to obtain clinically useful information of shapes and
volumes of target organs and tissues. In the past several years, convolutional
neural network (CNN) based methods (e.g., U-Net) have dominated this area, but
still suffered from inadequate long-range information capturing. Hence, recent
work presented computer vision Transformer variants for medical image
segmentation tasks and obtained promising performances. Such Transformers model
long-range dependency by computing pair-wise patch relations. However, they
incur prohibitive computational costs, especially on 3D medical images (e.g.,
CT and MRI). In this paper, we propose a new method called Dilated Transformer,
which conducts self-attention for pair-wise patch relations captured
alternately in local and global scopes. Inspired by dilated convolution
kernels, we conduct the global self-attention in a dilated manner, enlarging
receptive fields without increasing the patches involved and thus reducing
computational costs. Based on this design of Dilated Transformer, we construct
a U-shaped encoder-decoder hierarchical architecture called D-Former for 3D
medical image segmentation. Experiments on the Synapse and ACDC datasets show
that our D-Former model, trained from scratch, outperforms various competitive
CNN-based or Transformer-based segmentation models at a low computational cost
without time-consuming per-training process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RFormer: Transformer-based Generative Adversarial Network for Real Fundus Image Restoration on A New Clinical Benchmark. (arXiv:2201.00466v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00466">
<div class="article-summary-box-inner">
<span><p>Ophthalmologists have used fundus images to screen and diagnose eye diseases.
However, different equipments and ophthalmologists pose large variations to the
quality of fundus images. Low-quality (LQ) degraded fundus images easily lead
to uncertainty in clinical screening and generally increase the risk of
misdiagnosis. Thus, real fundus image restoration is worth studying.
Unfortunately, real clinical benchmark has not been explored for this task so
far. In this paper, we investigate the real clinical fundus image restoration
problem. Firstly, We establish a clinical dataset, Real Fundus (RF), including
120 low- and high-quality (HQ) image pairs. Then we propose a novel
Transformer-based Generative Adversarial Network (RFormer) to restore the real
degradation of clinical fundus images. The key component in our network is the
Window-based Self-Attention Block (WSAB) which captures non-local
self-similarity and long-range dependencies. To produce more visually pleasant
results, a Transformer-based discriminator is introduced. Extensive experiments
on our clinical benchmark show that the proposed RFormer significantly
outperforms the state-of-the-art (SOTA) methods. In addition, experiments of
downstream tasks such as vessel segmentation and optic disc/cup detection
demonstrate that our proposed RFormer benefits clinical fundus image analysis
and applications. The dataset, code, and models will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">maskGRU: Tracking Small Objects in the Presence of Large Background Motions. (arXiv:2201.00467v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00467">
<div class="article-summary-box-inner">
<span><p>We propose a recurrent neural network-based spatio-temporal framework named
maskGRU for the detection and tracking of small objects in videos. While there
have been many developments in the area of object tracking in recent years,
tracking a small moving object amid other moving objects and actors (such as a
ball amid moving players in sports footage) continues to be a difficult task.
Existing spatio-temporal networks, such as convolutional Gated Recurrent Units
(convGRUs), are difficult to train and have trouble accurately tracking small
objects under such conditions. To overcome these difficulties, we developed the
maskGRU framework that uses a weighted sum of the internal hidden state
produced by a convGRU and a 3-channel mask of the tracked object's predicted
bounding box as the hidden state to be used at the next time step of the
underlying convGRU. We believe the technique of incorporating a mask into the
hidden state through a weighted sum has two benefits: controlling the effect of
exploding gradients and introducing an attention-like mechanism into the
network by indicating where in the previous video frame the object is located.
Our experiments show that maskGRU outperforms convGRU at tracking objects that
are small relative to the video resolution even in the presence of other moving
objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Open World Object Detection. (arXiv:2201.00471v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00471">
<div class="article-summary-box-inner">
<span><p>Open World Object Detection (OWOD), simulating the real dynamic world where
knowledge grows continuously, attempts to detect both known and unknown classes
and incrementally learn the identified unknown ones. We find that although the
only previous OWOD work constructively puts forward to the OWOD definition, the
experimental settings are unreasonable with the illogical benchmark, confusing
metric calculation, and inappropriate method. In this paper, we rethink the
OWOD experimental setting and propose five fundamental benchmark principles to
guide the OWOD benchmark construction. Moreover, we design two fair evaluation
protocols specific to the OWOD problem, filling the void of evaluating from the
perspective of unknown classes. Furthermore, we introduce a novel and effective
OWOD framework containing an auxiliary Proposal ADvisor (PAD) and a
Class-specific Expelling Classifier (CEC). The non-parametric PAD could assist
the RPN in identifying accurate unknown proposals without supervision, while
CEC calibrates the over-confident activation boundary and filters out confusing
predictions through a class-specific expelling function. Comprehensive
experiments conducted on our fair benchmark demonstrate that our method
outperforms other state-of-the-art object detection approaches in terms of both
existing and our new metrics.\footnote{Our benchmark and code are available at
https://github.com/RE-OWOD/RE-OWOD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CaFT: Clustering and Filter on Tokens of Transformer for Weakly Supervised Object Localization. (arXiv:2201.00475v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00475">
<div class="article-summary-box-inner">
<span><p>Weakly supervised object localization (WSOL) is a challenging task to
localize the object by only category labels. However, there is contradiction
between classification and localization because accurate classification network
tends to pay attention to discriminative region of objects rather than the
entirety. We propose this discrimination is caused by handcraft threshold
choosing in CAM-based methods. Therefore, we propose Clustering and Filter of
Tokens (CaFT) with Vision Transformer (ViT) backbone to solve this problem in
another way. CaFT first sends the patch tokens of the image split to ViT and
cluster the output tokens to generate initial mask of the object. Secondly,
CaFT considers the initial mask as pseudo labels to train a shallow convolution
head (Attention Filter, AtF) following backbone to directly extract the mask
from tokens. Then, CaFT splits the image into parts, outputs masks respectively
and merges them into one refined mask. Finally, a new AtF is trained on the
refined masks and used to predict the box of object. Experiments verify that
CaFT outperforms previous work and achieves 97.55\% and 69.86\% localization
accuracy with ground-truth class on CUB-200 and ImageNet-1K respectively. CaFT
provides a fresh way to think about the WSOL task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language as Queries for Referring Video Object Segmentation. (arXiv:2201.00487v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00487">
<div class="article-summary-box-inner">
<span><p>Referring video object segmentation (R-VOS) is an emerging cross-modal task
that aims to segment the target object referred by a language expression in all
video frames. In this work, we propose a simple and unified framework built
upon Transformer, termed ReferFormer. It views the language as queries and
directly attends to the most relevant regions in the video frames. Concretely,
we introduce a small set of object queries conditioned on the language as the
input to the Transformer. In this manner, all the queries are obligated to find
the referred objects only. They are eventually transformed into dynamic kernels
which capture the crucial object-level information, and play the role of
convolution filters to generate the segmentation masks from feature maps. The
object tracking is achieved naturally by linking the corresponding queries
across frames. This mechanism greatly simplifies the pipeline and the
end-to-end framework is significantly different from the previous methods.
Extensive experiments on Ref-Youtube-VOS, Ref-DAVIS17, A2D-Sentences and
JHMDB-Sentences show the effectiveness of ReferFormer. On Ref-Youtube-VOS,
Refer-Former achieves 55.6J&amp;F with a ResNet-50 backbone without bells and
whistles, which exceeds the previous state-of-the-art performance by 8.4
points. In addition, with the strong Swin-Large backbone, ReferFormer achieves
the best J&amp;F of 62.4 among all existing methods. The J&amp;F metric can be further
boosted to 63.3 by adopting a simple post-process technique. Moreover, we show
the impressive results of 55.0 mAP and 43.7 mAP on A2D-Sentences
andJHMDB-Sentences respectively, which significantly outperforms the previous
methods by a large margin. Code is publicly available at
https://github.com/wjn922/ReferFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">R-Theta Local Neighborhood Pattern for Unconstrained Facial Image Recognition and Retrieval. (arXiv:2201.00504v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00504">
<div class="article-summary-box-inner">
<span><p>In this paper R-Theta Local Neighborhood Pattern (RTLNP) is proposed for
facial image retrieval. RTLNP exploits relationships amongst the pixels in
local neighborhood of the reference pixel at different angular and radial
widths. The proposed encoding scheme divides the local neighborhood into
sectors of equal angular width. These sectors are again divided into subsectors
of two radial widths. Average grayscales values of these two subsectors are
encoded to generate the micropatterns. Performance of the proposed descriptor
has been evaluated and results are compared with the state of the art
descriptors e.g. LBP, LTP, CSLBP, CSLTP, Sobel-LBP, LTCoP, LMeP, LDP, LTrP,
MBLBP, BRINT and SLBP. The most challenging facial constrained and
unconstrained databases, namely; AT&amp;T, CARIA-Face-V5-Cropped, LFW, and Color
FERET have been used for showing the efficiency of the proposed descriptor.
Proposed descriptor is also tested on near infrared (NIR) face databases; CASIA
NIR-VIS 2.0 and PolyU-NIRFD to explore its potential with respect to NIR facial
images. Better retrieval rates of RTLNP as compared to the existing state of
the art descriptors show the effectiveness of the descriptor
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Gradient Hexa Pattern: A Descriptor for Face Recognition and Retrieval. (arXiv:2201.00509v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00509">
<div class="article-summary-box-inner">
<span><p>Local descriptors used in face recognition are robust in a sense that these
descriptors perform well in varying pose, illumination and lighting conditions.
Accuracy of these descriptors depends on the precision of mapping the
relationship that exists in the local neighborhood of a facial image into
microstructures. In this paper a local gradient hexa pattern (LGHP) is proposed
that identifies the relationship amongst the reference pixel and its
neighboring pixels at different distances across different derivative
directions. Discriminative information exists in the local neighborhood as well
as in different derivative directions. Proposed descriptor effectively
transforms these relationships into binary micropatterns discriminating
interclass facial images with optimal precision. Recognition and retrieval
performance of the proposed descriptor has been compared with state-of-the-art
descriptors namely LDP and LVP over the most challenging and benchmark facial
image databases, i.e. Cropped Extended Yale-B, CMU-PIE, color-FERET, and LFW.
The proposed descriptor has better recognition as well as retrieval rates
compared to state-of-the-art descriptors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Centre Symmetric Quadruple Pattern: A Novel Descriptor for Facial Image Recognition and Retrieval. (arXiv:2201.00511v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00511">
<div class="article-summary-box-inner">
<span><p>Facial features are defined as the local relationships that exist amongst the
pixels of a facial image. Hand-crafted descriptors identify the relationships
of the pixels in the local neighbourhood defined by the kernel. Kernel is a two
dimensional matrix which is moved across the facial image. Distinctive
information captured by the kernel with limited number of pixel achieves
satisfactory recognition and retrieval accuracies on facial images taken under
constrained environment (controlled variations in light, pose, expressions, and
background). To achieve similar accuracies under unconstrained environment
local neighbourhood has to be increased, in order to encode more pixels.
Increasing local neighbourhood also increases the feature length of the
descriptor. In this paper we propose a hand-crafted descriptor namely Centre
Symmetric Quadruple Pattern (CSQP), which is structurally symmetric and encodes
the facial asymmetry in quadruple space. The proposed descriptor efficiently
encodes larger neighbourhood with optimal number of binary bits. It has been
shown using average entropy, computed over feature images encoded with the
proposed descriptor, that the CSQP captures more meaningful information as
compared to state of the art descriptors. The retrieval and recognition
accuracies of the proposed descriptor has been compared with state of the art
hand-crafted descriptors (CSLBP, CSLTP, LDP, LBP, SLBP and LDGP) on bench mark
databases namely; LFW, Colour-FERET, and CASIA-face-v5. Result analysis shows
that the proposed descriptor performs well under controlled as well as
uncontrolled variations in pose, illumination, background and expressions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cascaded Asymmetric Local Pattern: A Novel Descriptor for Unconstrained Facial Image Recognition and Retrieval. (arXiv:2201.00518v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00518">
<div class="article-summary-box-inner">
<span><p>Feature description is one of the most frequently studied areas in the expert
systems and machine learning. Effective encoding of the images is an essential
requirement for accurate matching. These encoding schemes play a significant
role in recognition and retrieval systems. Facial recognition systems should be
effective enough to accurately recognize individuals under intrinsic and
extrinsic variations of the system. The templates or descriptors used in these
systems encode spatial relationships of the pixels in the local neighbourhood
of an image. Features encoded using these hand crafted descriptors should be
robust against variations such as; illumination, background, poses, and
expressions. In this paper a novel hand crafted cascaded asymmetric local
pattern (CALP) is proposed for retrieval and recognition facial image. The
proposed descriptor uniquely encodes relationship amongst the neighbouring
pixels in horizontal and vertical directions. The proposed encoding scheme has
optimum feature length and shows significant improvement in accuracy under
environmental and physiological changes in a facial image. State of the art
hand crafted descriptors namely; LBP, LDGP, CSLBP, SLBP and CSLTP are compared
with the proposed descriptor on most challenging datasets namely; Caltech-face,
LFW, and CASIA-face-v5. Result analysis shows that, the proposed descriptor
outperforms state of the art under uncontrolled variations in expressions,
background, pose and illumination.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformer with Deformable Attention. (arXiv:2201.00520v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00520">
<div class="article-summary-box-inner">
<span><p>Transformers have recently shown superior performances on various vision
tasks. The large, sometimes even global, receptive field endows Transformer
models with higher representation power over their CNN counterparts.
Nevertheless, simply enlarging receptive field also gives rise to several
concerns. On the one hand, using dense attention e.g., in ViT, leads to
excessive memory and computational cost, and features can be influenced by
irrelevant parts which are beyond the region of interests. On the other hand,
the sparse attention adopted in PVT or Swin Transformer is data agnostic and
may limit the ability to model long range relations. To mitigate these issues,
we propose a novel deformable self-attention module, where the positions of key
and value pairs in self-attention are selected in a data-dependent way. This
flexible scheme enables the self-attention module to focus on relevant regions
and capture more informative features. On this basis, we present Deformable
Attention Transformer, a general backbone model with deformable attention for
both image classification and dense prediction tasks. Extensive experiments
show that our models achieve consistently improved results on comprehensive
benchmarks. Code is available at https://github.com/LeapLabTHU/DAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Novelty-based Generalization Evaluation for Traffic Light Detection. (arXiv:2201.00531v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00531">
<div class="article-summary-box-inner">
<span><p>The advent of Convolutional Neural Networks (CNNs) has led to their
application in several domains. One noteworthy application is the perception
system for autonomous driving that relies on the predictions from CNNs.
Practitioners evaluate the generalization ability of such CNNs by calculating
various metrics on an independent test dataset. A test dataset is often chosen
based on only one precondition, i.e., its elements are not a part of the
training data. Such a dataset may contain objects that are both similar and
novel w.r.t. the training dataset. Nevertheless, existing works do not reckon
the novelty of the test samples and treat them all equally for evaluating
generalization. Such novelty-based evaluations are of significance to validate
the fitness of a CNN in autonomous driving applications. Hence, we propose a
CNN generalization scoring framework that considers novelty of objects in the
test dataset. We begin with the representation learning technique to reduce the
image data into a low-dimensional space. It is on this space we estimate the
novelty of the test samples. Finally, we calculate the generalization score as
a combination of the test data prediction performance and novelty. We perform
an experimental study of the same for our traffic light detection application.
In addition, we systematically visualize the results for an interpretable
notion of novelty.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Concept Embeddings for Fuzzy Logic Verification of Deep Neural Networks in Perception Tasks. (arXiv:2201.00572v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00572">
<div class="article-summary-box-inner">
<span><p>One major drawback of deep neural networks (DNNs) for use in sensitive
application domains is their black-box nature. This makes it hard to verify or
monitor complex, symbolic requirements. In this work, we present a simple, yet
effective, approach to verify whether a trained convolutional neural network
(CNN) respects specified symbolic background knowledge. The knowledge may
consist of any fuzzy predicate logic rules. For this, we utilize methods from
explainable artificial intelligence (XAI): First, using concept embedding
analysis, the output of a computer vision CNN is post-hoc enriched by concept
outputs; second, logical rules from prior knowledge are fuzzified to serve as
continuous-valued functions on the concept outputs. These can be evaluated with
little computational overhead. We demonstrate three diverse use-cases of our
method on stateof-the-art object detectors: Finding corner cases, utilizing the
rules for detecting and localizing DNN misbehavior during runtime, and
comparing the logical consistency of DNNs. The latter is used to find related
differences between EfficientDet D1 and Mask R-CNN object detectors. We show
that this approach benefits from fuzziness and calibrating the concept outputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantically Grounded Visual Embeddings for Zero-Shot Learning. (arXiv:2201.00577v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00577">
<div class="article-summary-box-inner">
<span><p>Zero-shot learning methods rely on fixed visual and semantic embeddings,
extracted from independent vision and language models, both pre-trained for
other large-scale tasks. This is a weakness of current zero-shot learning
frameworks as such disjoint embeddings fail to adequately associate visual and
textual information to their shared semantic content. Therefore, we propose to
learn semantically grounded and enriched visual information by computing a
joint image and text model with a two-stream network on a proxy task. To
improve this alignment between image and textual representations, provided by
attributes, we leverage ancillary captions to provide grounded semantic
information. Our method, dubbed joint embeddings for zero-shot learning is
evaluated on several benchmark datasets, improving the performance of existing
state-of-the-art methods in both standard ($+1.6$\% on aPY, $+2.6\%$ on FLO)
and generalized ($+2.1\%$ on AWA$2$, $+2.2\%$ on CUB) zero-shot recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LiDAR Point--to--point Correspondences for Rigorous Registration of Kinematic Scanning in Dynamic Networks. (arXiv:2201.00596v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00596">
<div class="article-summary-box-inner">
<span><p>With the objective of improving the registration of LiDAR point clouds
produced by kinematic scanning systems, we propose a novel trajectory
adjustment procedure that leverages on the automated extraction of selected
reliable 3D point--to--point correspondences between overlapping point clouds
and their joint integration (adjustment) together with all raw inertial and
GNSS observations. This is performed in a tightly coupled fashion using a
Dynamic Network approach that results in an optimally compensated trajectory
through modeling of errors at the sensor, rather than the trajectory, level.
The 3D correspondences are formulated as static conditions within this network
and the registered point cloud is generated with higher accuracy utilizing the
corrected trajectory and possibly other parameters determined within the
adjustment. We first describe the method for selecting correspondences and how
they are inserted into the Dynamic Network as new observation models. We then
describe the experiments conducted to evaluate the performance of the proposed
framework in practical airborne laser scanning scenarios with low-cost MEMS
inertial sensors. In the conducted experiments, the method proposed to
establish 3D correspondences is effective in determining point--to--point
matches across a wide range of geometries such as trees, buildings and cars.
Our results demonstrate that the method improves the point cloud registration
accuracy, that is otherwise strongly affected by errors in the determined
platform attitude or position (in nominal and emulated GNSS outage conditions),
and possibly determine unknown boresight angles using only a fraction of the
total number of 3D correspondences that are established.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An analysis of over-sampling labeled data in semi-supervised learning with FixMatch. (arXiv:2201.00604v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00604">
<div class="article-summary-box-inner">
<span><p>Most semi-supervised learning methods over-sample labeled data when
constructing training mini-batches. This paper studies whether this common
practice improves learning and how. We compare it to an alternative setting
where each mini-batch is uniformly sampled from all the training data, labeled
or not, which greatly reduces direct supervision from true labels in typical
low-label regimes. However, this simpler setting can also be seen as more
general and even necessary in multi-task problems where over-sampling labeled
data would become intractable. Our experiments on semi-supervised CIFAR-10
image classification using FixMatch show a performance drop when using the
uniform sampling approach which diminishes when the amount of labeled data or
the training time increases. Further, we analyse the training dynamics to
understand how over-sampling of labeled data compares to uniform sampling. Our
main finding is that over-sampling is especially beneficial early in training
but gets less important in the later stages when more pseudo-labels become
correct. Nevertheless, we also find that keeping some true labels remains
important to avoid the accumulation of confirmation errors from incorrect
pseudo-labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAT-CADNet: Graph Attention Network for Panoptic Symbol Spotting in CAD Drawings. (arXiv:2201.00625v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00625">
<div class="article-summary-box-inner">
<span><p>Spotting graphical symbols from the computer-aided design (CAD) drawings is
essential to many industrial applications. Different from raster images, CAD
drawings are vector graphics consisting of geometric primitives such as
segments, arcs, and circles. By treating each CAD drawing as a graph, we
propose a novel graph attention network GAT-CADNet to solve the panoptic symbol
spotting problem: vertex features derived from the GAT branch are mapped to
semantic labels, while their attention scores are cascaded and mapped to
instance prediction. Our key contributions are three-fold: 1) the instance
symbol spotting task is formulated as a subgraph detection problem and solved
by predicting the adjacency matrix; 2) a relative spatial encoding (RSE) module
explicitly encodes the relative positional and geometric relation among
vertices to enhance the vertex attention; 3) a cascaded edge encoding (CEE)
module extracts vertex attentions from multiple stages of GAT and treats them
as edge encoding to predict the adjacency matrix. The proposed GAT-CADNet is
intuitive yet effective and manages to solve the panoptic symbol spotting
problem in one consolidated network. Extensive experiments and ablation studies
on the public benchmark show that our graph-based approach surpasses existing
state-of-the-art methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Feature Extraction from Histopathological Images Through A Fine-tuning ImageNet Model. (arXiv:2201.00636v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00636">
<div class="article-summary-box-inner">
<span><p>Due to lack of annotated pathological images, transfer learning has been the
predominant approach in the field of digital pathology.Pre-trained neural
networks based on ImageNet database are often used to extract "off the shelf"
features, achieving great success in predicting tissue types, molecular
features, and clinical outcomes, etc. We hypothesize that fine-tuning the
pre-trained models using histopathological images could further improve feature
extraction, and downstream prediction performance.We used 100,000 annotated HE
image patches for colorectal cancer (CRC) to finetune a pretrained Xception
model via a twostep approach.The features extracted from finetuned Xception
(FTX2048) model and Imagepretrained (IMGNET2048) model were compared through:
(1) tissue classification for HE images from CRC, same image type that was used
for finetuning; (2) prediction of immunerelated gene expression and (3) gene
mutations for lung adenocarcinoma (LUAD).Fivefold cross validation was used for
model performance evaluation. The extracted features from the finetuned FTX2048
exhibited significantly higher accuracy for predicting tisue types of CRC
compared to the off the shelf feature directly from Xception based on ImageNet
database. Particularly, FTX2048 markedly improved the accuracy for stroma from
87% to 94%. Similarly, features from FTX2048 boosted the prediction of
transcriptomic expression of immunerelated genesin LUAD. For the genes that had
signigicant relationships with image fetures, the features fgrom the finetuned
model imprroved the prediction for the majority of the genes. Inaddition,
fetures from FTX2048 improved prediction of mutation for 5 out of 9 most
frequently mutated genes in LUAD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compression-Resistant Backdoor Attack against Deep Neural Networks. (arXiv:2201.00672v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00672">
<div class="article-summary-box-inner">
<span><p>In recent years, many backdoor attacks based on training data poisoning have
been proposed. However, in practice, those backdoor attacks are vulnerable to
image compressions. When backdoor instances are compressed, the feature of
specific backdoor trigger will be destroyed, which could result in the backdoor
attack performance deteriorating. In this paper, we propose a
compression-resistant backdoor attack based on feature consistency training. To
the best of our knowledge, this is the first backdoor attack that is robust to
image compressions. First, both backdoor images and their compressed versions
are input into the deep neural network (DNN) for training. Then, the feature of
each image is extracted by internal layers of the DNN. Next, the feature
difference between backdoor images and their compressed versions are minimized.
As a result, the DNN treats the feature of compressed images as the feature of
backdoor images in feature space. After training, the backdoor attack against
DNN is robust to image compression. Furthermore, we consider three different
image compressions (i.e., JPEG, JPEG2000, WEBP) in feature consistency
training, so that the backdoor attack is robust to multiple image compression
algorithms. Experimental results demonstrate the effectiveness and robustness
of the proposed backdoor attack. When the backdoor instances are compressed,
the attack success rate of common backdoor attack is lower than 10%, while the
attack success rate of our compression-resistant backdoor is greater than 97%.
The compression-resistant attack is still robust even when the backdoor images
are compressed with low compression quality. In addition, extensive experiments
have demonstrated that, our compression-resistant backdoor attack has the
generalization ability to resist image compression which is not used in the
training process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Entity Tagging with Multimodal Knowledge Base. (arXiv:2201.00693v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00693">
<div class="article-summary-box-inner">
<span><p>To enhance research on multimodal knowledge base and multimodal information
processing, we propose a new task called multimodal entity tagging (MET) with a
multimodal knowledge base (MKB). We also develop a dataset for the problem
using an existing MKB. In an MKB, there are entities and their associated texts
and images. In MET, given a text-image pair, one uses the information in the
MKB to automatically identify the related entity in the text-image pair. We
solve the task by using the information retrieval paradigm and implement
several baselines using state-of-the-art methods in NLP and CV. We conduct
extensive experiments and make analyses on the experimental results. The
results show that the task is challenging, but current technologies can achieve
relatively high performance. We will release the dataset, code, and models for
future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiview point cloud registration with anisotropic and space-varying localization noise. (arXiv:2201.00708v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00708">
<div class="article-summary-box-inner">
<span><p>In this paper, we address the problem of registering multiple point clouds
corrupted with high anisotropic localization noise. Our approach follows the
widely used framework of Gaussian mixture model (GMM) reconstruction with an
expectation-maximization (EM) algorithm. Existing methods are based on an
implicit assumption of space-invariant isotropic Gaussian noise. However, this
assumption is violated in practice in applications such as single molecule
localization microscopy (SMLM). To address this issue, we propose to introduce
an explicit localization noise model that decouples shape modeling with the GMM
from noise handling. We design a stochastic EM algorithm that considers
noise-free data as a latent variable, with closed-form solutions at each EM
step. The first advantage of our approach is to handle space-variant and
anisotropic Gaussian noise with arbitrary covariances. The second advantage is
to leverage the explicit noise model to impose prior knowledge about the noise
that may be available from physical sensors. We show on various simulated data
that our noise handling strategy improves significantly the robustness to high
levels of anisotropic noise. We also demonstrate the performance of our method
on real SMLM data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-view Data Classification with a Label-driven Auto-weighted Strategy. (arXiv:2201.00714v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00714">
<div class="article-summary-box-inner">
<span><p>Distinguishing the importance of views has proven to be quite helpful for
semi-supervised multi-view learning models. However, existing strategies cannot
take advantage of semi-supervised information, only distinguishing the
importance of views from a data feature perspective, which is often influenced
by low-quality views then leading to poor performance. In this paper, by
establishing a link between labeled data and the importance of different views,
we propose an auto-weighted strategy to evaluate the importance of views from a
label perspective to avoid the negative impact of unimportant or low-quality
views. Based on this strategy, we propose a transductive semi-supervised
auto-weighted multi-view classification model. The initialization of the
proposed model can be effectively determined by labeled data, which is
practical. The model is decoupled into three small-scale sub-problems that can
efficiently be optimized with a local convergence guarantee. The experimental
results on classification tasks show that the proposed method achieves optimal
or sub-optimal classification accuracy at the lowest computational cost
compared to other related methods, and the weight change experiments show that
our proposed strategy can distinguish view importance more accurately than
other related strategies on multi-view datasets with low-quality views.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BDG-Net: Boundary Distribution Guided Network for Accurate Polyp Segmentation. (arXiv:2201.00767v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00767">
<div class="article-summary-box-inner">
<span><p>Colorectal cancer (CRC) is one of the most common fatal cancer in the world.
Polypectomy can effectively interrupt the progression of adenoma to
adenocarcinoma, thus reducing the risk of CRC development. Colonoscopy is the
primary method to find colonic polyps. However, due to the different sizes of
polyps and the unclear boundary between polyps and their surrounding mucosa, it
is challenging to segment polyps accurately. To address this problem, we design
a Boundary Distribution Guided Network (BDG-Net) for accurate polyp
segmentation. Specifically, under the supervision of the ideal Boundary
Distribution Map (BDM), we use Boundary Distribution Generate Module (BDGM) to
aggregate high-level features and generate BDM. Then, BDM is sent to the
Boundary Distribution Guided Decoder (BDGD) as complementary spatial
information to guide the polyp segmentation. Moreover, a multi-scale feature
interaction strategy is adopted in BDGD to improve the segmentation accuracy of
polyps with different sizes. Extensive quantitative and qualitative evaluations
demonstrate the effectiveness of our model, which outperforms state-of-the-art
models remarkably on five public polyp datasets while maintaining low
computational complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FaceQgen: Semi-Supervised Deep Learning for Face Image Quality Assessment. (arXiv:2201.00770v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00770">
<div class="article-summary-box-inner">
<span><p>In this paper we develop FaceQgen, a No-Reference Quality Assessment approach
for face images based on a Generative Adversarial Network that generates a
scalar quality measure related with the face recognition accuracy. FaceQgen
does not require labelled quality measures for training. It is trained from
scratch using the SCface database. FaceQgen applies image restoration to a face
image of unknown quality, transforming it into a canonical high quality image,
i.e., frontal pose, homogeneous background, etc. The quality estimation is
built as the similarity between the original and the restored images, since low
quality images experience bigger changes due to restoration. We compare three
different numerical quality measures: a) the MSE between the original and the
restored images, b) their SSIM, and c) the output score of the Discriminator of
the GAN. The results demonstrate that FaceQgen's quality measures are good
estimators of face recognition accuracy. Our experiments include a comparison
with other quality assessment methods designed for faces and for general
images, in order to position FaceQgen in the state of the art. This comparison
shows that, even though FaceQgen does not surpass the best existing face
quality assessment methods in terms of face recognition accuracy prediction, it
achieves good enough results to demonstrate the potential of semi-supervised
learning approaches for quality estimation (in particular, data-driven learning
based on a single high quality image per subject), having the capacity to
improve its performance in the future with adequate refinement of the model and
the significant advantage over competing methods of not needing quality labels
for its development. This makes FaceQgen flexible and scalable without
expensive data curation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Autoencoder for Point Cloud Self-supervised Representation Learning. (arXiv:2201.00785v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00785">
<div class="article-summary-box-inner">
<span><p>Many 3D representations (e.g., point clouds) are discrete samples of the
underlying continuous 3D surface. This process inevitably introduces sampling
variations on the underlying 3D shapes. In learning 3D representation, the
variations should be disregarded while transferable knowledge of the underlying
3D shape should be captured. This becomes a grand challenge in existing
representation learning paradigms. This paper studies autoencoding on point
clouds. The standard autoencoding paradigm forces the encoder to capture such
sampling variations as the decoder has to reconstruct the original point cloud
that has sampling variations. We introduce Implicit Autoencoder(IAE), a simple
yet effective method that addresses this challenge by replacing the point cloud
decoder with an implicit decoder. The implicit decoder outputs a continuous
representation that is shared among different point cloud sampling of the same
model. Reconstructing under the implicit representation can prioritize that the
encoder discards sampling variations, introducing more space to learn useful
features. We theoretically justify this claim under a simple linear
autoencoder. Moreover, the implicit decoder offers a rich space to design
suitable implicit representations for different tasks. We demonstrate the
usefulness of IAE across various self-supervised learning tasks for both 3D
objects and 3D scenes. Experimental results show that IAE consistently
outperforms the state-of-the-art in each task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DFA-NeRF: Personalized Talking Head Generation via Disentangled Face Attributes Neural Rendering. (arXiv:2201.00791v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00791">
<div class="article-summary-box-inner">
<span><p>While recent advances in deep neural networks have made it possible to render
high-quality images, generating photo-realistic and personalized talking head
remains challenging. With given audio, the key to tackling this task is
synchronizing lip movement and simultaneously generating personalized
attributes like head movement and eye blink. In this work, we observe that the
input audio is highly correlated to lip motion while less correlated to other
personalized attributes (e.g., head movements). Inspired by this, we propose a
novel framework based on neural radiance field to pursue high-fidelity and
personalized talking head generation. Specifically, neural radiance field takes
lip movements features and personalized attributes as two disentangled
conditions, where lip movements are directly predicted from the audio inputs to
achieve lip-synchronized generation. In the meanwhile, personalized attributes
are sampled from a probabilistic model, where we design a Transformer-based
variational autoencoder sampled from Gaussian Process to learn plausible and
natural-looking head pose and eye blink. Experiments on several benchmarks
demonstrate that our method achieves significantly better results than
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space. (arXiv:2201.00814v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00814">
<div class="article-summary-box-inner">
<span><p>This paper explores the feasibility of finding an optimal sub-model from a
vision transformer and introduces a pure vision transformer slimming (ViT-Slim)
framework that can search such a sub-structure from the original model
end-to-end across multiple dimensions, including the input tokens, MHSA and MLP
modules with state-of-the-art performance. Our method is based on a learnable
and unified l1 sparsity constraint with pre-defined factors to reflect the
global importance in the continuous searching space of different dimensions.
The searching process is highly efficient through a single-shot training
scheme. For instance, on DeiT-S, ViT-Slim only takes ~43 GPU hours for
searching process, and the searched structure is flexible with diverse
dimensionalities in different modules. Then, a budget threshold is employed
according to the requirements of accuracy-FLOPs trade-off on running devices,
and a re-training process is performed to obtain the final models. The
extensive experiments show that our ViT-Slim can compress up to 40% of
parameters and 40% FLOPs on various vision transformers while increasing the
accuracy by ~0.6% on ImageNet. We also demonstrate the advantage of our
searched models on several downstream datasets. Our source code will be
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods. (arXiv:1907.09358v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.09358">
<div class="article-summary-box-inner">
<span><p>Interest in Artificial Intelligence (AI) and its applications has seen
unprecedented growth in the last few years. This success can be partly
attributed to the advancements made in the sub-fields of AI such as machine
learning, computer vision, and natural language processing. Much of the growth
in these fields has been made possible with deep learning, a sub-area of
machine learning that uses artificial neural networks. This has created
significant interest in the integration of vision and language. In this survey,
we focus on ten prominent tasks that integrate language and vision by
discussing their problem formulation, methods, existing datasets, evaluation
measures, and compare the results obtained with corresponding state-of-the-art
methods. Our efforts go beyond earlier surveys which are either task-specific
or concentrate only on one type of visual content, i.e., image or video.
Furthermore, we also provide some potential future directions in this field of
research with an anticipation that this survey stimulates innovative thoughts
and ideas to address the existing challenges and build new applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Feature Fusion for Mitosis Counting. (arXiv:2002.03781v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.03781">
<div class="article-summary-box-inner">
<span><p>Each woman living in the United States has about 1 in 8 chance of developing
invasive breast cancer. The mitotic cell count is one of the most common tests
to assess the aggressiveness or grade of breast cancer. In this prognosis,
histopathology images must be examined by a pathologist using high-resolution
microscopes to count the cells. Unfortunately, can be an exhaustive task with
poor reproducibility, especially for non-experts. Deep learning networks have
recently been adapted to medical applications which are able to automatically
localize these regions of interest. However, these region-based networks lack
the ability to take advantage of the segmentation features produced by a full
image CNN which are often used as a sole method of detection. Therefore, the
proposed method leverages Faster RCNN for object detection while fusing
segmentation features generated by a UNet with RGB image features to achieve an
F-score of 0.508 on the MITOS-ATYPIA 2014 mitosis counting challenge dataset,
outperforming state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedBoost: Federated Learning with Gradient Protected Boosting for Text Recognition. (arXiv:2007.07296v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.07296">
<div class="article-summary-box-inner">
<span><p>Typical machine learning approaches require centralized data for model
training, which may not be possible where restrictions on data sharing are in
place due to, for instance, privacy and gradient protection. The recently
proposed Federated Learning (FL) framework allows learning a shared model
collaboratively without data being centralized or data sharing among data
owners. However, we show in this paper that the generalization ability of the
joint model is poor on Non-Independent and Non-Identically Distributed
(Non-IID) data, particularly when the Federated Averaging (FedAvg) strategy is
used due to the weight divergence phenomenon. We propose a novel boosting
algorithm for FL to address this generalization issue, as well as achieving a
much faster convergence rate in gradient-based optimization. In addition, a
secure gradient sharing protocol using Homomorphic Encryption (HE) and
Differential Privacy (DP) is introduced to defend against gradient leakage
attack. We demonstrate the proposed Federated Boosting (FedBoost) method
achieves significant improvements in both prediction accuracy and run-time
efficiency on text recognition task using public benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Boundary Based Out-of-Distribution Classifier for Generalized Zero-Shot Learning. (arXiv:2008.04872v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.04872">
<div class="article-summary-box-inner">
<span><p>Generalized Zero-Shot Learning (GZSL) is a challenging topic that has
promising prospects in many realistic scenarios. Using a gating mechanism that
discriminates the unseen samples from the seen samples can decompose the GZSL
problem to a conventional Zero-Shot Learning (ZSL) problem and a supervised
classification problem. However, training the gate is usually challenging due
to the lack of data in the unseen domain. To resolve this problem, in this
paper, we propose a boundary based Out-of-Distribution (OOD) classifier which
classifies the unseen and seen domains by only using seen samples for training.
First, we learn a shared latent space on a unit hyper-sphere where the latent
distributions of visual features and semantic attributes are aligned
class-wisely. Then we find the boundary and the center of the manifold for each
class. By leveraging the class centers and boundaries, the unseen samples can
be separated from the seen samples. After that, we use two experts to classify
the seen and unseen samples separately. We extensively validate our approach on
five popular benchmark datasets including AWA1, AWA2, CUB, FLO and SUN. The
experimental results demonstrate the advantages of our approach over
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast and Incremental Loop Closure Detection with Deep Features and Proximity Graphs. (arXiv:2010.11703v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11703">
<div class="article-summary-box-inner">
<span><p>In recent years, the robotics community has extensively examined methods
concerning the place recognition task within the scope of simultaneous
localization and mapping applications.This article proposes an appearance-based
loop closure detection pipeline named ``FILD++" (Fast and Incremental Loop
closure Detection).First, the system is fed by consecutive images and, via
passing them twice through a single convolutional neural network, global and
local deep features are extracted.Subsequently, a hierarchical navigable
small-world graph incrementally constructs a visual database representing the
robot's traversed path based on the computed global features.Finally, a query
image, grabbed each time step, is set to retrieve similar locations on the
traversed route.An image-to-image pairing follows, which exploits local
features to evaluate the spatial information. Thus, in the proposed article, we
propose a single network for global and local feature extraction in contrast to
our previous work (FILD), while an exhaustive search for the verification
process is adopted over the generated deep local features avoiding the
utilization of hash codes. Exhaustive experiments on eleven publicly available
datasets exhibit the system's high performance (achieving the highest recall
score on eight of them) and low execution times (22.05 ms on average in New
College, which is the largest one containing 52480 images) compared to other
state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Contrastive Self-Supervised Learning with False Negative Cancellation. (arXiv:2011.11765v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11765">
<div class="article-summary-box-inner">
<span><p>Self-supervised representation learning has made significant leaps fueled by
progress in contrastive learning, which seeks to learn transformations that
embed positive input pairs nearby, while pushing negative pairs far apart.
While positive pairs can be generated reliably (e.g., as different views of the
same image), it is difficult to accurately establish negative pairs, defined as
samples from different images regardless of their semantic content or visual
features. A fundamental problem in contrastive learning is mitigating the
effects of false negatives. Contrasting false negatives induces two critical
issues in representation learning: discarding semantic information and slow
convergence. In this paper, we propose novel approaches to identify false
negatives, as well as two strategies to mitigate their effect, i.e. false
negative elimination and attraction, while systematically performing rigorous
evaluations to study this problem in detail. Our method exhibits consistent
improvements over existing contrastive learning-based methods. Without labels,
we identify false negatives with 40% accuracy among 1000 semantic classes on
ImageNet, and achieve 5.8% absolute improvement in top-1 accuracy over the
previous state-of-the-art when finetuning with 1% labels. Our code is available
at https://github.com/google-research/fnc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review of Open-World Learning and Steps Toward Open-World Learning Without Labels. (arXiv:2011.12906v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12906">
<div class="article-summary-box-inner">
<span><p>In open-world learning, an agent starts with a set of known classes, detects,
and manages things that it does not know, and learns them over time from a
non-stationary stream of data. Open-world learning is related to but also
distinct from a multitude of other learning problems and this paper briefly
analyzes the key differences between a wide range of problems including
incremental learning, generalized novelty discovery, and generalized zero-shot
learning. This paper formalizes various open-world learning problems including
open-world learning without labels. These open-world problems can be addressed
with modifications to known elements, we present a new framework that enables
agents to combine various modules for novelty-detection,
novelty-characterization, incremental learning, and instance management to
learn new classes from a stream of unlabeled data in an unsupervised manner,
survey how to adapt a few state-of-the-art techniques to fit the framework and
use them to define seven baselines for performance on the open-world learning
without labels problem. We then discuss open-world learning quality and analyze
how that can improve instance management. We also discuss some of the general
ambiguity issues that occur in open-world learning without labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A General Descent Aggregation Framework for Gradient-based Bi-level Optimization. (arXiv:2102.07976v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07976">
<div class="article-summary-box-inner">
<span><p>In recent years, a variety of gradient-based methods have been developed to
solve Bi-Level Optimization (BLO) problems in machine learning and computer
vision areas. However, the theoretical correctness and practical effectiveness
of these existing approaches always rely on some restrictive conditions (e.g.,
Lower-Level Singleton, LLS), which could hardly be satisfied in real-world
applications. Moreover, previous literature only proves theoretical results
based on their specific iteration strategies, thus lack a general recipe to
uniformly analyze the convergence behaviors of different gradient-based BLOs.
In this work, we formulate BLOs from an optimistic bi-level viewpoint and
establish a new gradient-based algorithmic framework, named Bi-level Descent
Aggregation (BDA), to partially address the above issues. Specifically, BDA
provides a modularized structure to hierarchically aggregate both the upper-
and lower-level subproblems to generate our bi-level iterative dynamics.
Theoretically, we establish a general convergence analysis template and derive
a new proof recipe to investigate the essential theoretical properties of
gradient-based BLO methods. Furthermore, this work systematically explores the
convergence behavior of BDA in different optimization scenarios, i.e.,
considering various solution qualities (i.e., global/local/stationary solution)
returned from solving approximation subproblems. Extensive experiments justify
our theoretical results and demonstrate the superiority of the proposed
algorithm for hyper-parameter optimization and meta-learning tasks. Source code
is available at https://github.com/vis-opt-group/BDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Galaxy Zoo DECaLS: Detailed Visual Morphology Measurements from Volunteers and Deep Learning for 314,000 Galaxies. (arXiv:2102.08414v2 [astro-ph.GA] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08414">
<div class="article-summary-box-inner">
<span><p>We present Galaxy Zoo DECaLS: detailed visual morphological classifications
for Dark Energy Camera Legacy Survey images of galaxies within the SDSS DR8
footprint. Deeper DECaLS images (r=23.6 vs. r=22.2 from SDSS) reveal spiral
arms, weak bars, and tidal features not previously visible in SDSS imaging. To
best exploit the greater depth of DECaLS images, volunteers select from a new
set of answers designed to improve our sensitivity to mergers and bars. Galaxy
Zoo volunteers provide 7.5 million individual classifications over 314,000
galaxies. 140,000 galaxies receive at least 30 classifications, sufficient to
accurately measure detailed morphology like bars, and the remainder receive
approximately 5. All classifications are used to train an ensemble of Bayesian
convolutional neural networks (a state-of-the-art deep learning method) to
predict posteriors for the detailed morphology of all 314,000 galaxies. When
measured against confident volunteer classifications, the networks are
approximately 99% accurate on every question. Morphology is a fundamental
feature of every galaxy; our human and machine classifications are an accurate
and detailed resource for understanding how galaxies evolve.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Elsa: Energy-based learning for semi-supervised anomaly detection. (arXiv:2103.15296v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15296">
<div class="article-summary-box-inner">
<span><p>Anomaly detection aims at identifying deviant instances from the normal data
distribution. Many advances have been made in the field, including the
innovative use of unsupervised contrastive learning. However, existing methods
generally assume clean training data and are limited when the data contain
unknown anomalies. This paper presents Elsa, a novel semi-supervised anomaly
detection approach that unifies the concept of energy-based models with
unsupervised contrastive learning. Elsa instills robustness against any data
contamination by a carefully designed fine-tuning step based on the new energy
function that forces the normal data to be divided into classes of prototypes.
Experiments on multiple contamination scenarios show the proposed model
achieves SOTA performance. Extensive analyses also verify the contribution of
each component in the proposed model. Beyond the experiments, we also offer a
theoretical interpretation of why contrastive learning alone cannot detect
anomalies under data contamination.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Vibration Tomography: Estimating Interior Material Properties from Monocular Video. (arXiv:2104.02735v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02735">
<div class="article-summary-box-inner">
<span><p>An object's interior material properties, while invisible to the human eye,
determine motion observed on its surface. We propose an approach that estimates
heterogeneous material properties of an object from a monocular video of its
surface vibrations. Specifically, we show how to estimate Young's modulus and
density throughout a 3D object with known geometry. Knowledge of how these
values change across the object is useful for simulating its motion and
characterizing any defects. Traditional non-destructive testing approaches,
which often require expensive instruments, generally estimate only homogenized
material properties or simply identify the presence of defects. In contrast,
our approach leverages monocular video to (1) identify image-space modes from
an object's sub-pixel motion, and (2) directly infer spatially-varying Young's
modulus and density values from the observed modes. We demonstrate our approach
on both simulated and real videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BM-NAS: Bilevel Multimodal Neural Architecture Search. (arXiv:2104.09379v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09379">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) have shown superior performances on various
multimodal learning problems. However, it often requires huge efforts to adapt
DNNs to individual multimodal tasks by manually engineering unimodal features
and designing multimodal feature fusion strategies. This paper proposes Bilevel
Multimodal Neural Architecture Search (BM-NAS) framework, which makes the
architecture of multimodal fusion models fully searchable via a bilevel
searching scheme. At the upper level, BM-NAS selects the inter/intra-modal
feature pairs from the pretrained unimodal backbones. At the lower level,
BM-NAS learns the fusion strategy for each feature pair, which is a combination
of predefined primitive operations. The primitive operations are elaborately
designed and they can be flexibly combined to accommodate various effective
feature fusion modules such as multi-head attention (Transformer) and Attention
on Attention (AoA). Experimental results on three multimodal tasks demonstrate
the effectiveness and efficiency of the proposed BM-NAS framework. BM-NAS
achieves competitive performances with much less search time and fewer model
parameters in comparison with the existing generalized multimodal NAS methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Momentum Contrastive Voxel-wise Representation Learning for Semi-supervised Volumetric Medical Image Segmentation. (arXiv:2105.07059v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07059">
<div class="article-summary-box-inner">
<span><p>Automated segmentation in medical image analysis is a challenging task that
requires a large amount of manually labeled data. However, manually annotating
medical data is often laborious, and most existing learning-based approaches
fail to accurately delineate object boundaries without effective geometric
constraints. Contrastive learning, a sub-area of self-supervised learning, has
recently been noted as a promising direction in multiple application fields. In
this work, we present a novel Contrastive Voxel-wise Representation
Distillation (CVRD) method with geometric constraints to learn global-local
visual representations for volumetric medical image segmentation with limited
annotations. Our framework can effectively learn global and local features by
capturing 3D spatial context and rich anatomical information. Specifically, we
introduce a voxel-to-volume contrastive algorithm to learn global information
from 3D images, and propose to perform local voxel-to-voxel distillation to
explicitly make use of local cues in the embedding space. Moreover, we
integrate an elastic interaction-based active contour model as a geometric
regularization term to enable fast and reliable object delineations in an
end-to-end learning manner. Results on the Atrial Segmentation Challenge
dataset demonstrate superiority of our proposed scheme, especially in a setting
with a very limited number of annotated data. The code will be available at
https://github.com/charlesyou999648/CVRD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Visual Place Recognition in Dynamics-Invariant Perception Space. (arXiv:2105.07800v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07800">
<div class="article-summary-box-inner">
<span><p>Visual place recognition is one of the essential and challenging problems in
the fields of robotics. In this letter, we for the first time explore the use
of multi-modal fusion of semantic and visual modalities in dynamics-invariant
space to improve place recognition in dynamic environments. We achieve this by
first designing a novel deep learning architecture to generate the static
semantic segmentation and recover the static image directly from the
corresponding dynamic image. We then innovatively leverage the
spatial-pyramid-matching model to encode the static semantic segmentation into
feature vectors. In parallel, the static image is encoded using the popular
Bag-of-words model. On the basis of the above multi-modal features, we finally
measure the similarity between the query image and target landmark by the joint
similarity of their semantic and visual codes. Extensive experiments
demonstrate the effectiveness and robustness of the proposed approach for place
recognition in dynamic environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel lightweight Convolutional Neural Network, ExquisiteNetV2. (arXiv:2105.09008v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09008">
<div class="article-summary-box-inner">
<span><p>In the paper of ExquisiteNetV1, the ability of classification of
ExquisiteNetV1 is worse than DenseNet. In this article, we propose a faster and
better model ExquisiteNetV2. We conduct many experiments to evaluate its
performance. We test ExquisiteNetV2, ExquisiteNetV1 and other 9 well-known
models on 15 credible datasets under the same condition. According to the
experimental results, ExquisiteNetV2 gets the highest classification accuracy
over half of the datasets. Important of all, ExquisiteNetV2 has fewest amounts
of parameters. Besides, in most instances, ExquisiteNetV2 has fastest computing
speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medical Matting: A New Perspective on Medical Segmentation with Uncertainty. (arXiv:2106.09887v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09887">
<div class="article-summary-box-inner">
<span><p>It is difficult to accurately label ambiguous and complex shaped targets
manually by binary masks. The weakness of binary mask under-expression is
highlighted in medical image segmentation, where blurring is prevalent. In the
case of multiple annotations, reaching a consensus for clinicians by binary
masks is more challenging. Moreover, these uncertain areas are related to the
lesions' structure and may contain anatomical information beneficial to
diagnosis. However, current studies on uncertainty mainly focus on the
uncertainty in model training and data labels. None of them investigate the
influence of the ambiguous nature of the lesion itself.Inspired by image
matting, this paper introduces alpha matte as a soft mask to represent
uncertain areas in medical scenes and accordingly puts forward a new
uncertainty quantification method to fill the gap of uncertainty research for
lesion structure. In this work, we introduce a new architecture to generate
binary masks and alpha mattes in a multitasking framework, which outperforms
all state-of-the-art matting algorithms compared. The proposed uncertainty map
is able to highlight the ambiguous regions and a novel multitasking loss
weighting strategy we presented can improve performance further and demonstrate
their concrete benefits. To fully-evaluate the effectiveness of our proposed
method, we first labelled three medical datasets with alpha matte to address
the shortage of available matting datasets in medical scenes and prove the
alpha matte to be a more efficient labeling method than a binary mask from both
qualitative and quantitative aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Novel Visual Category Discovery with Dual Ranking Statistics and Mutual Knowledge Distillation. (arXiv:2107.03358v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03358">
<div class="article-summary-box-inner">
<span><p>In this paper, we tackle the problem of novel visual category discovery,
i.e., grouping unlabelled images from new classes into different semantic
partitions by leveraging a labelled dataset that contains images from other
different but relevant categories. This is a more realistic and challenging
setting than conventional semi-supervised learning. We propose a two-branch
learning framework for this problem, with one branch focusing on local
part-level information and the other branch focusing on overall
characteristics. To transfer knowledge from the labelled data to the
unlabelled, we propose using dual ranking statistics on both branches to
generate pseudo labels for training on the unlabelled data. We further
introduce a mutual knowledge distillation method to allow information exchange
and encourage agreement between the two branches for discovering new
categories, allowing our model to enjoy the benefits of global and local
features. We comprehensively evaluate our method on public benchmarks for
generic object classification, as well as the more challenging datasets for
fine-grained visual recognition, achieving state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modality specific U-Net variants for biomedical image segmentation: A survey. (arXiv:2107.04537v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.04537">
<div class="article-summary-box-inner">
<span><p>With the advent of advancements in deep learning approaches, such as deep
convolution neural network, residual neural network, adversarial network; U-Net
architectures are most widely utilized in biomedical image segmentation to
address the automation in identification and detection of the target regions or
sub-regions. In recent studies, U-Net based approaches have illustrated
state-of-the-art performance in different applications for the development of
computer-aided diagnosis systems for early diagnosis and treatment of diseases
such as brain tumor, lung cancer, alzheimer, breast cancer, etc., using various
modalities. This article contributes in presenting the success of these
approaches by describing the U-Net framework, followed by the comprehensive
analysis of the U-Net variants by performing 1) inter-modality, and 2)
intra-modality categorization to establish better insights into the associated
challenges and solutions. Besides, this article also highlights the
contribution of U-Net based frameworks in the ongoing pandemic, severe acute
respiratory syndrome coronavirus 2 (SARS-CoV-2) also known as COVID-19.
Finally, the strengths and similarities of these U-Net variants are analysed
along with the challenges involved in biomedical image segmentation to uncover
promising future research directions in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LASOR: Learning Accurate 3D Human Pose and Shape Via Synthetic Occlusion-Aware Data and Neural Mesh Rendering. (arXiv:2108.00351v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00351">
<div class="article-summary-box-inner">
<span><p>A key challenge in the task of human pose and shape estimation is occlusion,
including self-occlusions, object-human occlusions, and inter-person
occlusions. The lack of diverse and accurate pose and shape training data
becomes a major bottleneck, especially for scenes with occlusions in the wild.
In this paper, we focus on the estimation of human pose and shape in the case
of inter-person occlusions, while also handling object-human occlusions and
self-occlusion. We propose a novel framework that synthesizes occlusion-aware
silhouette and 2D keypoints data and directly regress to the SMPL pose and
shape parameters. A neural 3D mesh renderer is exploited to enable silhouette
supervision on the fly, which contributes to great improvements in shape
estimation. In addition, keypoints-and-silhouette-driven training data in
panoramic viewpoints are synthesized to compensate for the lack of viewpoint
diversity in any existing dataset. Experimental results show that we are among
the state-of-the-art on the 3DPW and 3DPW-Crowd datasets in terms of pose
estimation accuracy. The proposed method evidently outperforms the rank-1
method in terms of shape estimation. Top performance is also achieved on SSP-3D
in terms of shape prediction accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Acquisition and Preparation for Dual-reference Deep Learning of Image Super-Resolution. (arXiv:2108.02348v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02348">
<div class="article-summary-box-inner">
<span><p>For deep learning methods of real-world image super-resolution, the most
critical issue is whether the paired low and high resolution images for
training accurately reflect the sampling process of real cameras. Low and high
resolution (LR$\sim$HR) image pairs synthesized by existing degradation models
(e.g., bicubic downsampling) deviate from those in reality; thus the
super-resolution CNN trained by these synthesized LR$\sim$HR image pairs does
not perform well when being applied to real images. To address the problem, we
propose a novel data acquisition process to shoot a large set of LR$\sim$HR
image pairs using real cameras. The images are displayed on an ultra-high
quality screen and captured at different resolutions. The resulting LR$\sim$HR
image pairs can be aligned with very high sub-pixel precision by a novel
spatial-frequency dual-domain registration method, and hence they provide high
quality training data for the learning task of super-resolution. Moreover, the
captured HR image and the original digital image offer dual references to
improve the learning performance. Experimental results show that training a
super-resolution CNN by our LR$\sim$HR dataset achieves higher image quality
than training it by other datasets in the literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RCA-IUnet: A residual cross-spatial attention guided inception U-Net model for tumor segmentation in breast ultrasound imaging. (arXiv:2108.02508v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02508">
<div class="article-summary-box-inner">
<span><p>The advancements in deep learning technologies have produced immense
contributions to biomedical image analysis applications. With breast cancer
being the common deadliest disease among women, early detection is the key
means to improve survivability. Medical imaging like ultrasound presents an
excellent visual representation of the functioning of the organs; however, for
any radiologist analysing such scans is challenging and time consuming which
delays the diagnosis process. Although various deep learning based approaches
are proposed that achieved promising results, the present article introduces an
efficient residual cross-spatial attention guided inception U-Net (RCA-IUnet)
model with minimal training parameters for tumor segmentation using breast
ultrasound imaging to further improve the segmentation performance of varying
tumor sizes. The RCA-IUnet model follows U-Net topology with residual inception
depth-wise separable convolution and hybrid pooling (max pooling and spectral
pooling) layers. In addition, cross-spatial attention filters are added to
suppress the irrelevant features and focus on the target structure. The
segmentation performance of the proposed model is validated on two publicly
available datasets using standard segmentation evaluation metrics, where it
outperformed the other state-of-the-art segmentation models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bird's-Eye-View Panoptic Segmentation Using Monocular Frontal View Images. (arXiv:2108.03227v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03227">
<div class="article-summary-box-inner">
<span><p>Bird's-Eye-View (BEV) maps have emerged as one of the most powerful
representations for scene understanding due to their ability to provide rich
spatial context while being easy to interpret and process. Such maps have found
use in many real-world tasks that extensively rely on accurate scene
segmentation as well as object instance identification in the BEV space for
their operation. However, existing segmentation algorithms only predict the
semantics in the BEV space, which limits their use in applications where the
notion of object instances is also critical. In this work, we present the first
BEV panoptic segmentation approach for directly predicting dense panoptic
segmentation maps in the BEV, given a single monocular image in the frontal
view (FV). Our architecture follows the top-down paradigm and incorporates a
novel dense transformer module consisting of two distinct transformers that
learn to independently map vertical and flat regions in the input image from
the FV to the BEV. Additionally, we derive a mathematical formulation for the
sensitivity of the FV-BEV transformation which allows us to intelligently
weight pixels in the BEV space to account for the varying descriptiveness
across the FV image. Extensive evaluations on the KITTI-360 and nuScenes
datasets demonstrate that our approach exceeds the state-of-the-art in the PQ
metric by 3.61 pp and 4.93 pp respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Salient Object Detection with Transformer-based Asymmetric Bilateral U-Net. (arXiv:2108.07851v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07851">
<div class="article-summary-box-inner">
<span><p>Existing salient object detection (SOD) methods mainly rely on CNN-based
U-shaped structures with skip connections to combine the global contexts and
local spatial details that are crucial for locating salient objects and
refining object details, respectively. Despite great successes, the ability of
CNN in learning global contexts is limited. Recently, the vision transformer
has achieved revolutionary progress in computer vision owing to its powerful
modeling of global dependencies. However, directly applying the transformer to
SOD is suboptimal because the transformer lacks the ability to learn local
spatial representations. To this end, this paper explores the combination of
transformer and CNN to learn both global and local representations for SOD. We
propose a transformer-based Asymmetric Bilateral U-Net (ABiU-Net). The
asymmetric bilateral encoder has a transformer path and a lightweight CNN path,
where the two paths communicate at each encoder stage to learn complementary
global contexts and local spatial details, respectively. The asymmetric
bilateral decoder also consists of two paths to process features from the
transformer and CNN encoder paths, with communication at each decoder stage for
decoding coarse salient object locations and find-grained object details,
respectively. Such communication between the two encoder/decoder paths enables
AbiU-Net to learn complementary global and local representations, taking
advantage of the natural properties of transformer and CNN, respectively.
Hence, ABiU-Net provides a new perspective for transformer-based SOD. Extensive
experiments demonstrate that ABiU-Net performs favorably against previous
state-of-the-art SOD methods. The code will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Transferable Adversarial Attacks on Vision Transformers. (arXiv:2109.04176v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04176">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) have demonstrated impressive performance on a
series of computer vision tasks, yet they still suffer from adversarial
examples. % crafted in a similar fashion as CNNs. In this paper, we posit that
adversarial attacks on transformers should be specially tailored for their
architecture, jointly considering both patches and self-attention, in order to
achieve high transferability. More specifically, we introduce a dual attack
framework, which contains a Pay No Attention (PNA) attack and a PatchOut
attack, to improve the transferability of adversarial samples across different
ViTs. We show that skipping the gradients of attention during backpropagation
can generate adversarial examples with high transferability. In addition,
adversarial perturbations generated by optimizing randomly sampled subsets of
patches at each iteration achieve higher attack success rates than attacks
using all patches. We evaluate the transferability of attacks on
state-of-the-art ViTs, CNNs and robustly trained CNNs. The results of these
experiments demonstrate that the proposed dual attack can greatly boost
transferability between ViTs and from ViTs to CNNs. In addition, the proposed
method can easily be combined with existing transfer methods to boost
performance. Code is available at https://github.com/zhipeng-wei/PNA-PatchOut.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantics-Guided Contrastive Network for Zero-Shot Object detection. (arXiv:2109.06062v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06062">
<div class="article-summary-box-inner">
<span><p>Zero-shot object detection (ZSD), the task that extends conventional
detection models to detecting objects from unseen categories, has emerged as a
new challenge in computer vision. Most existing approaches tackle the ZSD task
with a strict mapping-transfer strategy, which may lead to suboptimal ZSD
results: 1) the learning process of those models ignores the available unseen
class information, and thus can be easily biased towards the seen categories;
2) the original visual feature space is not well-structured and lack of
discriminative information. To address these issues, we develop a novel
Semantics-Guided Contrastive Network for ZSD, named ContrastZSD, a detection
framework that first brings contrastive learning mechanism into the realm of
zero-shot detection. Particularly, ContrastZSD incorporates two
semantics-guided contrastive learning subnets that contrast between
region-category and region-region pairs respectively. The pairwise contrastive
tasks take advantage of additional supervision signals derived from both ground
truth label and pre-defined class similarity distribution. Under the guidance
of those explicit semantic supervision, the model can learn more knowledge
about unseen categories to avoid the bias problem to seen concepts, while
optimizing the data structure of visual features to be more discriminative for
better visual-semantic alignment. Extensive experiments are conducted on two
popular benchmarks for ZSD, i.e., PASCAL VOC and MS COCO. Results show that our
method outperforms the previous state-of-the-art on both ZSD and generalized
ZSD tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-Stage Mesh Deep Learning for Automated Tooth Segmentation and Landmark Localization on 3D Intraoral Scans. (arXiv:2109.11941v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11941">
<div class="article-summary-box-inner">
<span><p>Accurately segmenting teeth and identifying the corresponding anatomical
landmarks on dental mesh models are essential in computer-aided orthodontic
treatment. Manually performing these two tasks is time-consuming, tedious, and,
more importantly, highly dependent on orthodontists' experiences due to the
abnormality and large-scale variance of patients' teeth. Some machine
learning-based methods have been designed and applied in the orthodontic field
to automatically segment dental meshes (e.g., intraoral scans). In contrast,
the number of studies on tooth landmark localization is still limited. This
paper proposes a two-stage framework based on mesh deep learning (called
TS-MDL) for joint tooth labeling and landmark identification on raw intraoral
scans. Our TS-MDL first adopts an end-to-end \emph{i}MeshSegNet method (i.e., a
variant of the existing MeshSegNet with both improved accuracy and efficiency)
to label each tooth on the downsampled scan. Guided by the segmentation
outputs, our TS-MDL further selects each tooth's region of interest (ROI) on
the original mesh to construct a light-weight variant of the pioneering
PointNet (i.e., PointNet-Reg) for regressing the corresponding landmark
heatmaps. Our TS-MDL was evaluated on a real-clinical dataset, showing
promising segmentation and localization performance. Specifically,
\emph{i}MeshSegNet in the first stage of TS-MDL reached an averaged Dice
similarity coefficient (DSC) at $0.964\pm0.054$, significantly outperforming
the original MeshSegNet. In the second stage, PointNet-Reg achieved a mean
absolute error (MAE) of $0.597\pm0.761 \, mm$ in distances between the
prediction and ground truth for $66$ landmarks, which is superior compared with
other networks for landmark detection. All these results suggest the potential
usage of our TS-MDL in clinical practices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A General Gaussian Heatmap Label Assignment for Arbitrary-Oriented Object Detection. (arXiv:2109.12848v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12848">
<div class="article-summary-box-inner">
<span><p>Recently, many arbitrary-oriented object detection (AOOD) methods have been
proposed and attracted widespread attention in many fields. However, most of
them are based on anchor-boxes or standard Gaussian heatmaps. Such label
assignment strategy may not only fail to reflect the shape and direction
characteristics of arbitrary-oriented objects, but also have high
parameter-tuning efforts. In this paper, a novel AOOD method called General
Gaussian Heatmap Label Assignment (GGHL) is proposed. Specifically, an
anchor-free object-adaptation label assignment (OLA) strategy is presented to
define the positive candidates based on two-dimensional (2-D) oriented Gaussian
heatmaps, which reflect the shape and direction features of arbitrary-oriented
objects. Based on OLA, an oriented-bounding-box (OBB) representation component
(ORC) is developed to indicate OBBs and adjust the Gaussian center prior
weights to fit the characteristics of different objects adaptively through
neural network learning. Moreover, a joint-optimization loss (JOL) with area
normalization and dynamic confidence weighting is designed to refine the
misalign optimal results of different subtasks. Extensive experiments on public
datasets demonstrate that the proposed GGHL improves the AOOD performance with
low parameter-tuning and time costs. Furthermore, it is generally applicable to
most AOOD methods to improve their performance including lightweight models on
embedded platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting the Certified Robustness of L-infinity Distance Nets. (arXiv:2110.06850v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06850">
<div class="article-summary-box-inner">
<span><p>Recently, Zhang et al.(2021) developed a new neural network architecture
based on $\ell_\infty$-distance functions, which naturally possesses certified
$\ell_\infty$ robustness by its construction. Despite rigorous theoretical
guarantees, the model so far can only achieve comparable performance to
conventional networks. In this paper, we make the following two contributions:
$\mathrm{(i)}$ We demonstrate that $\ell_\infty$-distance nets enjoy a
fundamental advantage in certified robustness over conventional networks (under
typical certification approaches); $\mathrm{(ii)}$ With an improved training
process we are able to significantly boost the certified accuracy of
$\ell_\infty$-distance nets. Our training approach largely alleviates the
optimization problem that arose in the previous training scheme, in particular,
the unexpected large Lipschitz constant due to the use of a crucial trick
called $\ell_p$-relaxation. The core of our training approach is a novel
objective function that combines scaled cross-entropy loss and clipped hinge
loss with a decaying mixing coefficient. Experiments show that using the
proposed training strategy, the certified accuracy of $\ell_\infty$-distance
net can be dramatically improved from 33.30% to 40.06% on CIFAR-10
($\epsilon=8/255$), meanwhile outperforming other approaches in this area by a
large margin. Our results clearly demonstrate the effectiveness and potential
of $\ell_\infty$-distance net for certified robustness. Codes are available at
https://github.com/zbh2047/L_inf-dist-net-v2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Pedestrian Attribute Recognition Using Group Sparsity for Occlusion Videos. (arXiv:2110.08708v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08708">
<div class="article-summary-box-inner">
<span><p>Occlusion processing is a key issue in pedestrian attribute recognition
(PAR). Nevertheless, several existing video-based PAR methods have not yet
considered occlusion handling in depth. In this paper, we formulate finding
non-occluded frames as sparsity-based temporal attention of a crowded video. In
this manner, a model is guided not to pay attention to the occluded frame.
However, temporal sparsity cannot include a correlation between attributes when
occlusion occurs. For example, "boots" and "shoe color" cannot be recognized
when the foot is invisible. To solve the uncorrelated attention issue, we also
propose a novel group sparsity-based temporal attention module. Group sparsity
is applied across attention weights in correlated attributes. Thus, attention
weights in a group are forced to pay attention to the same frames. Experimental
results showed that the proposed method achieved a higher F1-score than the
state-of-the-art methods on two video-based PAR datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salt and pepper noise removal method based on stationary Framelet transform with non-convex sparsity regularization. (arXiv:2110.09113v5 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09113">
<div class="article-summary-box-inner">
<span><p>Salt and pepper noise removal is a common inverse problem in image
processing. Traditional denoising methods have two limitations. First, noise
characteristics are often not described accurately. For example, the noise
location information is often ignored and the sparsity of the salt and pepper
noise is often described by L1 norm, which cannot illustrate the sparse
variables clearly. Second, conventional methods separate the contaminated image
into a recovered image and a noise part, thus resulting in recovering an image
with unsatisfied smooth parts and detail parts. In this study, we introduce a
noise detection strategy to determine the position of the noise, and a
non-convex sparsity regularization depicted by Lp quasi-norm is employed to
describe the sparsity of the noise, thereby addressing the first limitation.
The morphological component analysis framework with stationary Framelet
transform is adopted to decompose the processed image into cartoon, texture,
and noise parts to resolve the second limitation. Then, the alternating
direction method of multipliers (ADMM) is employed to solve the proposed model.
Finally, experiments are conducted to verify the proposed method and compare it
with some current state-of-the-art denoising methods. The experimental results
show that the proposed method can remove salt and pepper noise while preserving
the details of the processed image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SLURP: Side Learning Uncertainty for Regression Problems. (arXiv:2110.11182v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11182">
<div class="article-summary-box-inner">
<span><p>It has become critical for deep learning algorithms to quantify their output
uncertainties to satisfy reliability constraints and provide accurate results.
Uncertainty estimation for regression has received less attention than
classification due to the more straightforward standardized output of the
latter class of tasks and their high importance. However, regression problems
are encountered in a wide range of applications in computer vision. We propose
SLURP, a generic approach for regression uncertainty estimation via a side
learner that exploits the output and the intermediate representations generated
by the main task model. We test SLURP on two critical regression tasks in
computer vision: monocular depth and optical flow estimation. In addition, we
conduct exhaustive benchmarks comprising transfer to different datasets and the
addition of aleatoric noise. The results show that our proposal is generic and
readily applicable to various regression problems and has a low computational
cost with respect to existing solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AugMax: Adversarial Composition of Random Augmentations for Robust Training. (arXiv:2110.13771v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13771">
<div class="article-summary-box-inner">
<span><p>Data augmentation is a simple yet effective way to improve the robustness of
deep neural networks (DNNs). Diversity and hardness are two complementary
dimensions of data augmentation to achieve robustness. For example, AugMix
explores random compositions of a diverse set of augmentations to enhance
broader coverage, while adversarial training generates adversarially hard
samples to spot the weakness. Motivated by this, we propose a data augmentation
framework, termed AugMax, to unify the two aspects of diversity and hardness.
AugMax first randomly samples multiple augmentation operators and then learns
an adversarial mixture of the selected operators. Being a stronger form of data
augmentation, AugMax leads to a significantly augmented input distribution
which makes model training more challenging. To solve this problem, we further
design a disentangled normalization module, termed DuBIN
(Dual-Batch-and-Instance Normalization), that disentangles the instance-wise
feature heterogeneity arising from AugMax. Experiments show that AugMax-DuBIN
leads to significantly improved out-of-distribution robustness, outperforming
prior arts by 3.03%, 3.49%, 1.82% and 0.71% on CIFAR10-C, CIFAR100-C, Tiny
ImageNet-C and ImageNet-C. Codes and pretrained models are available:
https://github.com/VITA-Group/AugMax.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Contrastive Learning Using Negative Samples with Diminished Semantics. (arXiv:2110.14189v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14189">
<div class="article-summary-box-inner">
<span><p>Unsupervised learning has recently made exceptional progress because of the
development of more effective contrastive learning methods. However, CNNs are
prone to depend on low-level features that humans deem non-semantic. This
dependency has been conjectured to induce a lack of robustness to image
perturbations or domain shift. In this paper, we show that by generating
carefully designed negative samples, contrastive learning can learn more robust
representations with less dependence on such features. Contrastive learning
utilizes positive pairs that preserve semantic information while perturbing
superficial features in the training images. Similarly, we propose to generate
negative samples in a reversed way, where only the superfluous instead of the
semantic features are preserved. We develop two methods, texture-based and
patch-based augmentations, to generate negative samples. These samples achieve
better generalization, especially under out-of-domain settings. We also analyze
our method and the generated texture-based samples, showing that texture
features are indispensable in classifying particular ImageNet classes and
especially finer classes. We also show that model bias favors texture and shape
features differently under different test settings. Our code, trained models,
and ImageNet-Texture dataset can be found at
https://github.com/SongweiGe/Contrastive-Learning-with-Non-Semantic-Negatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models. (arXiv:2111.07355v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07355">
<div class="article-summary-box-inner">
<span><p>Wrist fractures are common cases in hospitals, particularly in emergency
services. Physicians need images from various medical devices, and patients
medical history and physical examination to diagnose these fractures correctly
and apply proper treatment. This study aims to perform fracture detection using
deep learning on wrist Xray images to assist physicians not specialized in the
field, working in emergency services in particular, in diagnosis of fractures.
For this purpose, 20 different detection procedures were performed using deep
learning based object detection models on dataset of wrist Xray images obtained
from Gazi University Hospital. DCN, Dynamic R_CNN, Faster R_CNN, FSAF, Libra
R_CNN, PAA, RetinaNet, RegNet and SABL deep learning based object detection
models with various backbones were used herein. To further improve detection
procedures in the study, 5 different ensemble models were developed, which were
later used to reform an ensemble model to develop a detection model unique to
our study, titled wrist fracture detection combo (WFD_C). Based on 26 different
models for fracture detection, the highest result of detection was 0.8639
average precision (AP50) in WFD_C model developed. This study is supported by
Huawei Turkey R&amp;D Center within the scope of the ongoing cooperation project
coded 071813 among Gazi University, Huawei and Medskor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NCVX: A User-Friendly and Scalable Package for Nonconvex Optimization in Machine Learning. (arXiv:2111.13984v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13984">
<div class="article-summary-box-inner">
<span><p>Optimizing nonconvex (NCVX) problems, especially nonsmooth and constrained
ones, is an essential part of machine learning. However, it can be hard to
reliably solve such problems without optimization expertise. Existing
general-purpose NCVX optimization packages are powerful but typically cannot
handle nonsmoothness. GRANSO is among the first optimization solvers targeting
general nonsmooth NCVX problems with nonsmooth constraints, but, as it is
implemented in MATLAB and requires the user to provide analytical gradients,
GRANSO is often not a convenient choice in machine learning (especially deep
learning) applications. To greatly lower the technical barrier, we introduce a
new software package called NCVX, whose initial release contains the solver
PyGRANSO, a PyTorch-enabled port of GRANSO incorporating auto-differentiation,
GPU acceleration, tensor input, and support for new QP solvers. NCVX is built
on freely available and widely used open-source frameworks, and as a highlight,
can solve general constrained deep learning problems, the first of its kind.
NCVX is available at https://ncvx.org, with detailed documentation and numerous
examples from machine learning and other fields.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Deep learning based Document Image Enhancement. (arXiv:2112.02719v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02719">
<div class="article-summary-box-inner">
<span><p>Digitized documents such as scientific articles, tax forms, invoices,
contract papers, historic texts are widely used nowadays. These document images
could be degraded or damaged due to various reasons including poor lighting
conditions, shadow, distortions like noise and blur, aging, ink stain,
bleed-through, watermark, stamp, etc. Document image enhancement plays a
crucial role as a pre-processing step in many automated document analysis and
recognition tasks such as character recognition. With recent advances in deep
learning, many methods are proposed to enhance the quality of these document
images. In this paper, we review deep learning-based methods, datasets, and
metrics for six main document image enhancement tasks, including binarization,
debluring, denoising, defading, watermark removal, and shadow removal. We
summarize the recent works for each task and discuss their features,
challenges, and limitations. We introduce multiple document image enhancement
tasks that have received little to no attention, including over and under
exposure correction, super resolution, and bleed-through removal. We identify
several promising research directions and opportunities for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BT-Unet: A self-supervised learning framework for biomedical image segmentation using Barlow Twins with U-Net models. (arXiv:2112.03916v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03916">
<div class="article-summary-box-inner">
<span><p>Deep learning has brought the most profound contribution towards biomedical
image segmentation to automate the process of delineation in medical imaging.
To accomplish such task, the models are required to be trained using huge
amount of annotated or labelled data that highlights the region of interest
with a binary mask. However, efficient generation of the annotations for such
huge data requires expert biomedical analysts and extensive manual effort. It
is a tedious and expensive task, while also being vulnerable to human error. To
address this problem, a self-supervised learning framework, BT-Unet is proposed
that uses the Barlow Twins approach to pre-train the encoder of a U-Net model
via redundancy reduction in an unsupervised manner to learn data
representation. Later, complete network is fine-tuned to perform actual
segmentation. The BT-Unet framework can be trained with a limited number of
annotated samples while having high number of unannotated samples, which is
mostly the case in real-world problems. This framework is validated over
multiple U-Net models over diverse datasets by generating scenarios of a
limited number of labelled samples using standard evaluation metrics. With
exhaustive experiment trials, it is observed that the BT-Unet framework
enhances the performance of the U-Net models with significant margin under such
circumstances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mimicking the Oracle: An Initial Phase Decorrelation Approach for Class Incremental Learning. (arXiv:2112.04731v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04731">
<div class="article-summary-box-inner">
<span><p>Class Incremental Learning (CIL) aims at learning a multi-class classifier in
a phase-by-phase manner, in which only data of a subset of the classes are
provided at each phase. Previous works mainly focus on mitigating forgetting in
phases after the initial one. However, we find that improving CIL at its
initial phase is also a promising direction. Specifically, we experimentally
show that directly encouraging CIL Learner at the initial phase to output
similar representations as the model jointly trained on all classes can greatly
boost the CIL performance. Motivated by this, we study the difference between a
na\"ively-trained initial-phase model and the oracle model. Specifically, since
one major difference between these two models is the number of training
classes, we investigate how such difference affects the model representations.
We find that, with fewer training classes, the data representations of each
class lie in a long and narrow region; with more training classes, the
representations of each class scatter more uniformly. Inspired by this
observation, we propose Class-wise Decorrelation (CwD) that effectively
regularizes representations of each class to scatter more uniformly, thus
mimicking the model jointly trained with all classes (i.e., the oracle model).
Our CwD is simple to implement and easy to plug into existing methods.
Extensive experiments on various benchmark datasets show that CwD consistently
and significantly improves the performance of existing state-of-the-art methods
by around 1\% to 3\%. Code will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UFPMP-Det: Toward Accurate and Efficient Object Detection on Drone Imagery. (arXiv:2112.10415v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10415">
<div class="article-summary-box-inner">
<span><p>This paper proposes a novel approach to object detection on drone imagery,
namely Multi-Proxy Detection Network with Unified Foreground Packing
(UFPMP-Det). To deal with the numerous instances of very small scales,
different from the common solution that divides the high-resolution input image
into quite a number of chips with low foreground ratios to perform detection on
them each, the Unified Foreground Packing (UFP) module is designed, where the
sub-regions given by a coarse detector are initially merged through clustering
to suppress background and the resulting ones are subsequently packed into a
mosaic for a single inference, thus significantly reducing overall time cost.
Furthermore, to address the more serious confusion between inter-class
similarities and intra-class variations of instances, which deteriorates
detection performance but is rarely discussed, the Multi-Proxy Detection
Network (MP-Det) is presented to model object distributions in a fine-grained
manner by employing multiple proxy learning, and the proxies are enforced to be
diverse by minimizing a Bag-of-Instance-Words (BoIW) guided optimal transport
loss. By such means, UFPMP-Det largely promotes both the detection accuracy and
efficiency. Extensive experiments are carried out on the widely used VisDrone
and UAVDT datasets, and UFPMP-Det reports new state-of-the-art scores at a much
higher speed, highlighting its advantages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HarmoFL: Harmonizing Local and Global Drifts in Federated Learning on Heterogeneous Medical Images. (arXiv:2112.10775v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10775">
<div class="article-summary-box-inner">
<span><p>Multiple medical institutions collaboratively training a model using
federated learning (FL) has become a promising solution for maximizing the
potential of data-driven models, yet the non-independent and identically
distributed (non-iid) data in medical images is still an outstanding challenge
in real-world practice. The feature heterogeneity caused by diverse scanners or
protocols introduces a drift in the learning process, in both local (client)
and global (server) optimizations, which harms the convergence as well as model
performance. Many previous works have attempted to address the non-iid issue by
tackling the drift locally or globally, but how to jointly solve the two
essentially coupled drifts is still unclear. In this work, we concentrate on
handling both local and global drifts and introduce a new harmonizing framework
called HarmoFL. First, we propose to mitigate the local update drift by
normalizing amplitudes of images transformed into the frequency domain to mimic
a unified imaging setting, in order to generate a harmonized feature space
across local clients. Second, based on harmonized features, we design a client
weight perturbation guiding each local model to reach a flat optimum, where a
neighborhood area of the local optimal solution has a uniformly low loss.
Without any extra communication cost, the perturbation assists the global model
to optimize towards a converged optimal solution by aggregating several local
flat optima. We have theoretically analyzed the proposed method and empirically
conducted extensive experiments on three medical image classification and
segmentation tasks, showing that HarmoFL outperforms a set of recent
state-of-the-art methods with promising convergence behavior. Code is available
at https://github.com/med-air/HarmoFL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generation of Synthetic Rat Brain MRI scans with a 3D Enhanced Alpha-GAN. (arXiv:2112.13626v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13626">
<div class="article-summary-box-inner">
<span><p>Translational brain research using Magnetic Resonance Imaging (MRI) is
becoming increasingly popular as animal models are an essential part of
scientific studies and more ultra-high-field scanners are becoming available.
Some disadvantages of MRI are the availability of MRI scanners and the time
required for a full scanning session (it usually takes over 30 minutes).
Privacy laws and the 3Rs ethics rule also make it difficult to create large
datasets for training deep learning models. Generative Adversarial Networks
(GANs) can perform data augmentation with higher quality than other techniques.
In this work, the alpha-GAN architecture is used to test its ability to produce
realistic 3D MRI scans of the rat brain. As far as the authors are aware, this
is the first time that a GAN-based approach has been used for data augmentation
in preclinical data. The generated scans are evaluated using various
qualitative and quantitative metrics. A Turing test conducted by 4 experts has
shown that the generated scans can trick almost any expert. The generated scans
were also used to evaluate their impact on the performance of an existing deep
learning model developed for segmenting the rat brain into white matter, grey
matter and cerebrospinal fluid. The models were compared using the Dice score.
The best results for whole brain and white matter segmentation were obtained
when 174 real scans and 348 synthetic scans were used, with improvements of
0.0172 and 0.0129, respectively. Using 174 real scans and 87 synthetic scans
resulted in improvements of 0.0038 and 0.0764 for grey matter and CSF
segmentation, respectively. Thus, by using the proposed new normalisation layer
and loss functions, it was possible to improve the realism of the generated rat
MRI scans and it was shown that using the generated data improved the
segmentation model more than using the conventional data augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Visual-Auditory Saliency Detection with Multigranularity Perception. (arXiv:2112.13697v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13697">
<div class="article-summary-box-inner">
<span><p>Thanks to the rapid advances in deep learning techniques and the wide
availability of large-scale training sets, the performance of video saliency
detection models has been improving steadily and significantly. However, deep
learning-based visualaudio fixation prediction is still in its infancy. At
present, only a few visual-audio sequences have been furnished, with real
fixations being recorded in real visual-audio environments. Hence, it would be
neither efficient nor necessary to recollect real fixations under the same
visual-audio circumstances. To address this problem, this paper promotes a
novel approach in a weakly supervised manner to alleviate the demand of
large-scale training sets for visual-audio model training. By using only the
video category tags, we propose the selective class activation mapping (SCAM)
and its upgrade (SCAM+). In the spatial-temporal-audio circumstance, the former
follows a coarse-to-fine strategy to select the most discriminative regions,
and these regions are usually capable of exhibiting high consistency with the
real human-eye fixations. The latter equips the SCAM with an additional
multi-granularity perception mechanism, making the whole process more
consistent with that of the real human visual system. Moreover, we distill
knowledge from these regions to obtain complete new spatial-temporal-audio
(STA) fixation prediction (FP) networks, enabling broad applications in cases
where video tags are not available. Without resorting to any real human-eye
fixation, the performances of these STA FP networks are comparable to those of
fully supervised networks. The code and results are publicly available at
https://github.com/guotaowang/STANet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding the Task-Optimal Low-Bit Sub-Distribution in Deep Neural Networks. (arXiv:2112.15139v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15139">
<div class="article-summary-box-inner">
<span><p>Quantized neural networks typically require smaller memory footprints and
lower computation complexity, which is crucial for efficient deployment.
However, quantization inevitably leads to a distribution divergence from the
original network, which generally degrades the performance. To tackle this
issue, massive efforts have been made, but most existing approaches lack
statistical considerations and depend on several manual configurations. In this
paper, we present an adaptive-mapping quantization method to learn an optimal
latent sub-distribution that is inherent within models and smoothly
approximated with a concrete Gaussian Mixture (GM). In particular, the network
weights are projected in compliance with the GM-approximated sub-distribution.
This sub-distribution evolves along with the weight update in a co-tuning
schema guided by the direct task-objective optimization. Sufficient experiments
on image classification and object detection over various modern architectures
demonstrate the effectiveness, generalization property, and transferability of
the proposed method. Besides, an efficient deployment flow for the mobile CPU
is developed, achieving up to 7.46$\times$ inference acceleration on an
octa-core ARM CPU. Codes are publicly released at
https://github.com/RunpeiDong/DGMS.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-01-04 23:07:25.827021556 UTC">2022-01-04 23:07:25 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>