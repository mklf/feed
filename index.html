<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-06-01T01:30:00Z">06-01</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient and Student-Friendly Knowledge Distillation. (arXiv:2205.15308v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15308">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) has been extensively employed to transfer the
knowledge from a large teacher model to the smaller students, where the
parameters of the teacher are fixed (or partially) during training. Recent
studies show that this mode may cause difficulties in knowledge transfer due to
the mismatched model capacities. To alleviate the mismatch problem,
teacher-student joint training methods, e.g., online distillation, have been
proposed, but it always requires expensive computational cost. In this paper,
we present a parameter-efficient and student-friendly knowledge distillation
method, namely PESF-KD, to achieve efficient and sufficient knowledge transfer
by updating relatively few partial parameters. Technically, we first
mathematically formulate the mismatch as the sharpness gap between their
predictive distributions, where we show such a gap can be narrowed with the
appropriate smoothness of the soft label. Then, we introduce an adapter module
for the teacher and only update the adapter to obtain soft labels with
appropriate smoothness. Experiments on a variety of benchmarks show that
PESF-KD can significantly reduce the training cost while obtaining competitive
results compared to advanced online distillation methods. Code will be released
upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleTTS: A Style-Based Generative Model for Natural and Diverse Text-to-Speech Synthesis. (arXiv:2205.15439v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15439">
<div class="article-summary-box-inner">
<span><p>Text-to-Speech (TTS) has recently seen great progress in synthesizing
high-quality speech owing to the rapid development of parallel TTS systems, but
producing speech with naturalistic prosodic variations, speaking styles and
emotional tones remains challenging. Moreover, since duration and speech are
generated separately, parallel TTS models still have problems finding the best
monotonic alignments that are crucial for naturalistic speech synthesis. Here,
we propose StyleTTS, a style-based generative model for parallel TTS that can
synthesize diverse speech with natural prosody from a reference speech
utterance. With novel Transferable Monotonic Aligner (TMA) and
duration-invariant data augmentation schemes, our method significantly
outperforms state-of-the-art models on both single and multi-speaker datasets
in subjective tests of speech naturalness and speaker similarity. Through
self-supervised learning of the speaking styles, our model can synthesize
speech with the same prosodic and emotional tone as any given reference speech
without the need for explicitly labeling these categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Modality Robustness in Multimodal Sentiment Analysis. (arXiv:2205.15465v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15465">
<div class="article-summary-box-inner">
<span><p>Building robust multimodal models are crucial for achieving reliable
deployment in the wild. Despite its importance, less attention has been paid to
identifying and improving the robustness of Multimodal Sentiment Analysis (MSA)
models. In this work, we hope to address that by (i) Proposing simple
diagnostic checks for modality robustness in a trained multimodal model. Using
these checks, we find MSA models to be highly sensitive to a single modality,
which creates issues in their robustness; (ii) We analyze well-known robust
training strategies to alleviate the issues. Critically, we observe that
robustness can be achieved without compromising on the original performance. We
hope our extensive study-performed across five models and two benchmark
datasets-and proposed procedures would make robustness an integral component in
MSA research. Our diagnostic checks and robust training solutions are simple to
implement and available at https://github. com/declare-lab/MSA-Robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FinBERT-MRC: financial named entity recognition using BERT under the machine reading comprehension paradigm. (arXiv:2205.15485v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15485">
<div class="article-summary-box-inner">
<span><p>Financial named entity recognition (FinNER) from literature is a challenging
task in the field of financial text information extraction, which aims to
extract a large amount of financial knowledge from unstructured texts. It is
widely accepted to use sequence tagging frameworks to implement FinNER tasks.
However, such sequence tagging models cannot fully take advantage of the
semantic information in the texts. Instead, we formulate the FinNER task as a
machine reading comprehension (MRC) problem and propose a new model termed
FinBERT-MRC. This formulation introduces significant prior information by
utilizing well-designed queries, and extracts start index and end index of
target entities without decoding modules such as conditional random fields
(CRF). We conduct experiments on a publicly available Chinese financial dataset
ChFinAnn and a real-word bussiness dataset AdminPunish. FinBERT-MRC model
achieves average F1 scores of 92.78% and 96.80% on the two datasets,
respectively, with average F1 gains +3.94% and +0.89% over some sequence
tagging models including BiLSTM-CRF, BERT-Tagger, and BERT-CRF. The source code
is available at https://github.com/zyz0000/FinBERT-MRC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Pre-Trained Language Models to Streamline Natural Language Interaction for Self-Tracking. (arXiv:2205.15503v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15503">
<div class="article-summary-box-inner">
<span><p>Current natural language interaction for self-tracking tools largely depends
on bespoke implementation optimized for a specific tracking theme and data
format, which is neither generalizable nor scalable to a tremendous design
space of self-tracking. However, training machine learning models in the
context of self-tracking is challenging due to the wide variety of tracking
topics and data formats. In this paper, we propose a novel NLP task for
self-tracking that extracts close- and open-ended information from a
retrospective activity log described as a plain text, and a domain-agnostic,
GPT-3-based NLU framework that performs this task. The framework augments the
prompt using synthetic samples to transform the task into 10-shot learning, to
address a cold-start problem in bootstrapping a new tracking topic. Our
preliminary evaluation suggests that our approach significantly outperforms the
baseline QA models. Going further, we discuss future application domains toward
which the NLP and HCI researchers can collaborate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts. (arXiv:2205.15509v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15509">
<div class="article-summary-box-inner">
<span><p>Vision-Language Navigation (VLN) is a challenging task that requires an
embodied agent to perform action-level modality alignment, i.e., make
instruction-asked actions sequentially in complex visual environments. Most
existing VLN agents learn the instruction-path data directly and cannot
sufficiently explore action-level alignment knowledge inside the multi-modal
inputs. In this paper, we propose modAlity-aligneD Action PrompTs (ADAPT),
which provides the VLN agent with action prompts to enable the explicit
learning of action-level modality alignment to pursue successful navigation.
Specifically, an action prompt is defined as a modality-aligned pair of an
image sub-prompt and a text sub-prompt, where the former is a single-view
observation and the latter is a phrase like ''walk past the chair''. When
starting navigation, the instruction-related action prompt set is retrieved
from a pre-built action prompt base and passed through a prompt encoder to
obtain the prompt feature. Then the prompt feature is concatenated with the
original instruction feature and fed to a multi-layer transformer for action
prediction. To collect high-quality action prompts into the prompt base, we use
the Contrastive Language-Image Pretraining (CLIP) model which has powerful
cross-modality alignment ability. A modality alignment loss and a sequential
consistency loss are further introduced to enhance the alignment of the action
prompt and enforce the agent to focus on the related prompt sequentially.
Experimental results on both R2R and RxR show the superiority of ADAPT over
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Event-Level Sentiment Analysis with Structured Arguments. (arXiv:2205.15511v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15511">
<div class="article-summary-box-inner">
<span><p>Previous studies about event-level sentiment analysis (SA) usually model the
event as a topic, a category or target terms, while the structured arguments
(e.g., subject, object, time and location) that have potential effects on the
sentiment are not well studied. In this paper, we redefine the task as
structured event-level SA and propose an End-to-End Event-level Sentiment
Analysis ($\textit{E}^{3}\textit{SA}$) approach to solve this issue.
Specifically, we explicitly extract and model the event structure information
for enhancing event-level SA. Extensive experiments demonstrate the great
advantages of our proposed approach over the state-of-the-art methods. Noting
the lack of the dataset, we also release a large-scale real-world dataset with
event arguments and sentiment labelling for promoting more
researches\footnote{The dataset is available at
https://github.com/zhangqi-here/E3SA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Framework for Emotion Identification and Generation in Dialogues. (arXiv:2205.15513v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15513">
<div class="article-summary-box-inner">
<span><p>Social chatbots have gained immense popularity, and their appeal lies not
just in their capacity to respond to the diverse requests from users, but also
in the ability to develop an emotional connection with users. To further
develop and promote social chatbots, we need to concentrate on increasing user
interaction and take into account both the intellectual and emotional quotient
in the conversational agents. In this paper, we propose a multi-task framework
that jointly identifies the emotion of a given dialogue and generates response
in accordance to the identified emotion. We employ a BERT based network for
creating an empathetic system and use a mixed objective function that trains
the end-to-end network with both the classification and generation loss.
Experimental results show that our proposed framework outperforms current
state-of-the-art models
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Knowledge-Enhanced Adversarial Model for Cross-lingual Structured Sentiment Analysis. (arXiv:2205.15514v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15514">
<div class="article-summary-box-inner">
<span><p>Structured sentiment analysis, which aims to extract the complex semantic
structures such as holders, expressions, targets, and polarities, has obtained
widespread attention from both industry and academia. Unfortunately, the
existing structured sentiment analysis datasets refer to a few languages and
are relatively small, limiting neural network models' performance. In this
paper, we focus on the cross-lingual structured sentiment analysis task, which
aims to transfer the knowledge from the source language to the target one.
Notably, we propose a Knowledge-Enhanced Adversarial Model (\texttt{KEAM}) with
both implicit distributed and explicit structural knowledge to enhance the
cross-lingual transfer. First, we design an adversarial embedding adapter for
learning an informative and robust representation by capturing implicit
semantic information from diverse multi-lingual embeddings adaptively. Then, we
propose a syntax GCN encoder to transfer the explicit semantic information
(e.g., universal dependency tree) among multiple languages. We conduct
experiments on five datasets and compare \texttt{KEAM} with both the supervised
and unsupervised methods. The extensive experimental results show that our
\texttt{KEAM} model outperforms all the unsupervised baselines in various
metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Refining Low-Resource Unsupervised Translation by Language Disentanglement of Multilingual Model. (arXiv:2205.15544v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15544">
<div class="article-summary-box-inner">
<span><p>Numerous recent work on unsupervised machine translation (UMT) implies that
competent unsupervised translations of low-resource and unrelated languages,
such as Nepali or Sinhala, are only possible if the model is trained in a
massive multilingual environment, where theses low-resource languages are mixed
with high-resource counterparts. Nonetheless, while the high-resource languages
greatly help kick-start the target low-resource translation tasks, the language
discrepancy between them may hinder their further improvement. In this work, we
propose a simple refinement procedure to disentangle languages from a
pre-trained multilingual UMT model for it to focus on only the target
low-resource task. Our method achieves the state of the art in the fully
unsupervised translation tasks of English to Nepali, Sinhala, Gujarati,
Latvian, Estonian and Kazakh, with BLEU score gains of 3.5, 3.5, 3.3, 4.1, 4.2,
and 3.3, respectively. Our codebase is available at
https://github.com/nxphi47/refine_unsup_multilingual_mt
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-level Supervised Contrastive Learning Framework for Low-Resource Natural Language Inference. (arXiv:2205.15550v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15550">
<div class="article-summary-box-inner">
<span><p>Natural Language Inference (NLI) is a growingly essential task in natural
language understanding, which requires inferring the relationship between the
sentence pairs (premise and hypothesis). Recently, low-resource natural
language inference has gained increasing attention, due to significant savings
in manual annotation costs and a better fit with real-world scenarios. Existing
works fail to characterize discriminative representations between different
classes with limited training data, which may cause faults in label prediction.
Here we propose a multi-level supervised contrastive learning framework named
MultiSCL for low-resource natural language inference. MultiSCL leverages a
sentence-level and pair-level contrastive learning objective to discriminate
between different classes of sentence pairs by bringing those in one class
together and pushing away those in different classes. MultiSCL adopts a data
augmentation module that generates different views for input samples to better
learn the latent representation. The pair-level representation is obtained from
a cross attention module. We conduct extensive experiments on two public NLI
datasets in low-resource settings, and the accuracy of MultiSCL exceeds other
models by 3.1% on average. Moreover, our method outperforms the previous
state-of-the-art method on cross-domain tasks of text classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">hmBERT: Historical Multilingual Language Models for Named Entity Recognition. (arXiv:2205.15575v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15575">
<div class="article-summary-box-inner">
<span><p>Compared to standard Named Entity Recognition (NER), identifying persons,
locations, and organizations in historical texts forms a big challenge. To
obtain machine-readable corpora, the historical text is usually scanned and
optical character recognition (OCR) needs to be performed. As a result, the
historical corpora contain errors. Also, entities like location or organization
can change over time, which poses another challenge. Overall historical texts
come with several peculiarities that differ greatly from modern texts and large
labeled corpora for training a neural tagger are hardly available for this
domain. In this work, we tackle NER for historical German, English, French,
Swedish, and Finnish by training large historical language models. We
circumvent the need for labeled data by using unlabeled data for pretraining a
language model. hmBERT, a historical multilingual BERT-based language model is
proposed, with different sizes of it being publicly released. Furthermore, we
evaluate the capability of hmBERT by solving downstream NER as part of this
year's HIPE-2022 shared task and provide detailed analysis and insights. For
the Multilingual Classical Commentary coarse-grained NER challenge, our tagger
HISTeria outperforms the other teams' models for two out of three languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preparing an Endangered Language for the Digital Age: The Case of Judeo-Spanish. (arXiv:2205.15599v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15599">
<div class="article-summary-box-inner">
<span><p>We develop machine translation and speech synthesis systems to complement the
efforts of revitalizing Judeo-Spanish, the exiled language of Sephardic Jews,
which survived for centuries, but now faces the threat of extinction in the
digital age. Building on resources created by the Sephardic community of Turkey
and elsewhere, we create corpora and tools that would help preserve this
language for future generations. For machine translation, we first develop a
Spanish to Judeo-Spanish rule-based machine translation system, in order to
generate large volumes of synthetic parallel data in the relevant language
pairs: Turkish, English and Spanish. Then, we train baseline neural machine
translation engines using this synthetic data and authentic parallel data
created from translations by the Sephardic community. For text-to-speech
synthesis, we present a 3.5 hour single speaker speech corpus for building a
neural speech synthesis engine. Resources, model weights and online inference
engines are shared publicly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">APPReddit: a Corpus of Reddit Posts Annotated for Appraisal. (arXiv:2205.15627v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15627">
<div class="article-summary-box-inner">
<span><p>Despite the large number of computational resources for emotion recognition,
there is a lack of data sets relying on appraisal models. According to
Appraisal theories, emotions are the outcome of a multi-dimensional evaluation
of events. In this paper, we present APPReddit, the first corpus of
non-experimental data annotated according to this theory. After describing its
development, we compare our resource with enISEAR, a corpus of events created
in an experimental setting and annotated for appraisal. Results show that the
two corpora can be mapped notwithstanding different typologies of data and
annotations schemes. A SVM model trained on APPReddit predicts four appraisal
dimensions without significant loss. Merging both corpora in a single training
set increases the prediction of 3 out of 4 dimensions. Such findings pave the
way to a better performing classification model for appraisal prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NEWTS: A Corpus for News Topic-Focused Summarization. (arXiv:2205.15661v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15661">
<div class="article-summary-box-inner">
<span><p>Text summarization models are approaching human levels of fidelity. Existing
benchmarking corpora provide concordant pairs of full and abridged versions of
Web, news or, professional content. To date, all summarization datasets operate
under a one-size-fits-all paradigm that may not reflect the full range of
organic summarization needs. Several recently proposed models (e.g., plug and
play language models) have the capacity to condition the generated summaries on
a desired range of themes. These capacities remain largely unused and
unevaluated as there is no dedicated dataset that would support the task of
topic-focused summarization.
</p>
<p>This paper introduces the first topical summarization corpus NEWTS, based on
the well-known CNN/Dailymail dataset, and annotated via online crowd-sourcing.
Each source article is paired with two reference summaries, each focusing on a
different theme of the source document. We evaluate a representative range of
existing techniques and analyze the effectiveness of different prompting
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why are NLP Models Fumbling at Elementary Math? A Survey of Deep Learning based Word Problem Solvers. (arXiv:2205.15683v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15683">
<div class="article-summary-box-inner">
<span><p>From the latter half of the last decade, there has been a growing interest in
developing algorithms for automatically solving mathematical word problems
(MWP). It is a challenging and unique task that demands blending surface level
text pattern recognition with mathematical reasoning. In spite of extensive
research, we are still miles away from building robust representations of
elementary math word problems and effective solutions for the general task. In
this paper, we critically examine the various models that have been developed
for solving word problems, their pros and cons and the challenges ahead. In the
last two years, a lot of deep learning models have recorded competing results
on benchmark datasets, making a critical and conceptual analysis of literature
highly useful at this juncture. We take a step back and analyse why, in spite
of this abundance in scholarly interest, the predominantly used experiment and
dataset designs continue to be a stumbling block. From the vantage point of
having analyzed the literature closely, we also endeavour to provide a road-map
for future math word problem research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Informational Space Based Semantic Analysis for Scientific Texts. (arXiv:2205.15696v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15696">
<div class="article-summary-box-inner">
<span><p>One major problem in Natural Language Processing is the automatic analysis
and representation of human language. Human language is ambiguous and deeper
understanding of semantics and creating human-to-machine interaction have
required an effort in creating the schemes for act of communication and
building common-sense knowledge bases for the 'meaning' in texts. This paper
introduces computational methods for semantic analysis and the quantifying the
meaning of short scientific texts. Computational methods extracting semantic
feature are used to analyse the relations between texts of messages and
'representations of situations' for a newly created large collection of
scientific texts, Leicester Scientific Corpus. The representation of
scientific-specific meaning is standardised by replacing the situation
representations, rather than psychological properties, with the vectors of some
attributes: a list of scientific subject categories that the text belongs to.
First, this paper introduces 'Meaning Space' in which the informational
representation of the meaning is extracted from the occurrence of the word in
texts across the scientific categories, i.e., the meaning of a word is
represented by a vector of Relative Information Gain about the subject
categories. Then, the meaning space is statistically analysed for Leicester
Scientific Dictionary-Core and we investigate 'Principal Components of the
Meaning' to describe the adequate dimensions of the meaning. The research in
this paper conducts the base for the geometric representation of the meaning of
texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Transformers for Product Matching -- Experiments and a New Benchmark in Polish. (arXiv:2205.15712v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15712">
<div class="article-summary-box-inner">
<span><p>Product matching corresponds to the task of matching identical products
across different data sources. It typically employs available product features
which, apart from being multimodal, i.e., comprised of various data types,
might be non-homogeneous and incomplete. The paper shows that pre-trained,
multilingual Transformer models, after fine-tuning, are suitable for solving
the product matching problem using textual features both in English and Polish
languages. We tested multilingual mBERT and XLM-RoBERTa models in English on
Web Data Commons - training dataset and gold standard for large-scale product
matching. The obtained results show that these models perform similarly to the
latest solutions tested on this set, and in some cases, the results were even
better.
</p>
<p>Additionally, we prepared a new dataset -- ProductMatch.pl -- that is
entirely in Polish and based on offers in selected categories obtained from
several online stores for the research purpose. It is the first open dataset
for product matching tasks in Polish, which allows comparing the effectiveness
of the pre-trained models. Thus, we also showed the baseline results obtained
by the fine-tuned mBERT and XLM-RoBERTa models on the Polish datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Forget Cheap Training Signals Before Building Unsupervised Bilingual Word Embeddings. (arXiv:2205.15713v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15713">
<div class="article-summary-box-inner">
<span><p>Bilingual Word Embeddings (BWEs) are one of the cornerstones of cross-lingual
transfer of NLP models. They can be built using only monolingual corpora
without supervision leading to numerous works focusing on unsupervised BWEs.
However, most of the current approaches to build unsupervised BWEs do not
compare their results with methods based on easy-to-access cross-lingual
signals. In this paper, we argue that such signals should always be considered
when developing unsupervised BWE methods. The two approaches we find most
effective are: 1) using identical words as seed lexicons (which unsupervised
approaches incorrectly assume are not available for orthographically distinct
language pairs) and 2) combining such lexicons with pairs extracted by matching
romanized versions of words with an edit distance threshold. We experiment on
thirteen non-Latin languages (and English) and show that such cheap signals
work well and that they outperform using more complex unsupervised methods on
distant language pairs such as Chinese, Japanese, Kannada, Tamil, and Thai. In
addition, they are even competitive with the use of high-quality lexicons in
supervised approaches. Our results show that these training signals should not
be neglected when building BWEs, even for distant languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EMS: Efficient and Effective Massively Multilingual Sentence Representation Learning. (arXiv:2205.15744v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15744">
<div class="article-summary-box-inner">
<span><p>Massively multilingual sentence representation models, e.g., LASER,
SBERT-distill, and LaBSE, help significantly improve cross-lingual downstream
tasks. However, multiple training procedures, the use of a large amount of
data, or inefficient model architectures result in heavy computation to train a
new model according to our preferred languages and domains. To resolve this
issue, we introduce efficient and effective massively multilingual sentence
representation learning (EMS), using cross-lingual sentence reconstruction
(XTR) and sentence-level contrastive learning as training objectives. Compared
with related studies, the proposed model can be efficiently trained using
significantly fewer parallel sentences and GPU computation resources without
depending on large-scale pre-trained models. Empirical results show that the
proposed model significantly yields better or comparable results with regard to
bi-text mining, zero-shot cross-lingual genre classification, and sentiment
classification. Ablative analyses demonstrate the effectiveness of each
component of the proposed model. We release the codes for model training and
the EMS pre-trained model, which supports 62 languages
(https://github.com/Mao-KU/EMS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GateNLP-UShef at SemEval-2022 Task 8: Entity-Enriched Siamese Transformer for Multilingual News Article Similarity. (arXiv:2205.15812v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15812">
<div class="article-summary-box-inner">
<span><p>This paper describes the second-placed system on the leaderboard of
SemEval-2022 Task 8: Multilingual News Article Similarity. We propose an
entity-enriched Siamese Transformer which computes news article similarity
based on different sub-dimensions, such as the shared narrative, entities,
location and time of the event discussed in the news article. Our system
exploits a Siamese network architecture using a Transformer encoder to learn
document-level representations for the purpose of capturing the narrative
together with the auxiliary entity-based features extracted from the news
articles. The intuition behind using all these features together is to capture
the similarity between news articles at different granularity levels and to
assess the extent to which different news outlets write about "the same
events". Our experimental results and detailed ablation study demonstrate the
effectiveness and the validity of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do self-supervised speech models develop human-like perception biases?. (arXiv:2205.15819v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15819">
<div class="article-summary-box-inner">
<span><p>Self-supervised models for speech processing form representational spaces
without using any external labels. Increasingly, they appear to be a feasible
way of at least partially eliminating costly manual annotations, a problem of
particular concern for low-resource languages. But what kind of
representational spaces do these models construct? Human perception specializes
to the sounds of listeners' native languages. Does the same thing happen in
self-supervised models? We examine the representational spaces of three kinds
of state-of-the-art self-supervised models: wav2vec 2.0, HuBERT and contrastive
predictive coding (CPC), and compare them with the perceptual spaces of
French-speaking and English-speaking human listeners, both globally and taking
account of the behavioural differences between the two language groups. We show
that the CPC model shows a small native language effect, but that wav2vec 2.0
and HuBERT seem to develop a universal speech perception space which is not
language specific. A comparison against the predictions of supervised phone
recognisers suggests that all three self-supervised models capture relatively
fine-grained perceptual phenomena, while supervised models are better at
capturing coarser, phone-level, effects of listeners' native language, on
perception.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting non-native speech perception using the Perceptual Assimilation Model and state-of-the-art acoustic models. (arXiv:2205.15823v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15823">
<div class="article-summary-box-inner">
<span><p>Our native language influences the way we perceive speech sounds, affecting
our ability to discriminate non-native sounds. We compare two ideas about the
influence of the native language on speech perception: the Perceptual
Assimilation Model, which appeals to a mental classification of sounds into
native phoneme categories, versus the idea that rich, fine-grained phonetic
representations tuned to the statistics of the native language, are sufficient.
We operationalize this idea using representations from two state-of-the-art
speech models, a Dirichlet process Gaussian mixture model and the more recent
wav2vec 2.0 model. We present a new, open dataset of French- and
English-speaking participants' speech perception behaviour for 61 vowel sounds
from six languages. We show that phoneme assimilation is a better predictor
than fine-grained phonetic modelling, both for the discrimination behaviour as
a whole, and for predicting differences in discriminability associated with
differences in native language background. We also show that wav2vec 2.0, while
not good at capturing the effects of native language on speech perception, is
complementary to information about native phoneme assimilation, and provides a
good model of low-level phonetic representations, supporting the idea that both
categorical and fine-grained perception are used during speech perception.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LEXpander: applying colexification networks to automated lexicon expansion. (arXiv:2205.15850v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15850">
<div class="article-summary-box-inner">
<span><p>Recent approaches to text analysis from social media and other corpora rely
on word lists to detect topics, measure meaning, or to select relevant
documents. These lists are often generated by applying computational lexicon
expansion methods to small, manually-curated sets of root words. Despite the
wide use of this approach, we still lack an exhaustive comparative analysis of
the performance of lexicon expansion methods and how they can be improved with
additional linguistic data. In this work, we present LEXpander, a method for
lexicon expansion that leverages novel data on colexification, i.e. semantic
networks connecting words based on shared concepts and translations to other
languages. We evaluate LEXpander in a benchmark including widely used methods
for lexicon expansion based on various word embedding models and synonym
networks. We find that LEXpander outperforms existing approaches in terms of
both precision and the trade-off between precision and recall of generated word
lists in a variety of tests. Our benchmark includes several linguistic
categories and sentiment variables in English and German. We also show that the
expanded word lists constitute a high-performing text analysis method in
application cases to various corpora. This way, LEXpander poses a systematic
automated solution to expand short lists of words into exhaustive and accurate
word lists that can closely approximate word lists generated by experts in
psychology and linguistics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers. (arXiv:2205.15868v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15868">
<div class="article-summary-box-inner">
<span><p>Large-scale pretrained transformers have created milestones in text (GPT-3)
and text-to-image (DALL-E and CogView) generation. Its application to video
generation is still facing many challenges: The potential huge computation cost
makes the training from scratch unaffordable; The scarcity and weak relevance
of text-video datasets hinder the model understanding complex movement
semantics. In this work, we present 9B-parameter transformer CogVideo, trained
by inheriting a pretrained text-to-image model, CogView2. We also propose
multi-frame-rate hierarchical training strategy to better align text and video
clips. As (probably) the first open-source large-scale pretrained text-to-video
model, CogVideo outperforms all publicly available models at a large margin in
machine and human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uzbek Sentiment Analysis based on local Restaurant Reviews. (arXiv:2205.15930v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15930">
<div class="article-summary-box-inner">
<span><p>Extracting useful information for sentiment analysis and classification
problems from a big amount of user-generated feedback, such as restaurant
reviews, is a crucial task of natural language processing, which is not only
for customer satisfaction where it can give personalized services, but can also
influence the further development of a company. In this paper, we present a
work done on collecting restaurant reviews data as a sentiment analysis dataset
for the Uzbek language, a member of the Turkic family which is heavily affected
by the low-resource constraint, and provide some further analysis of the novel
dataset by evaluation using different techniques, from logistic regression
based models, to support vector machines, and even deep learning models, such
as recurrent neural networks, as well as convolutional neural networks. The
paper includes detailed information on how the data was collected, how it was
pre-processed for better quality optimization, as well as experimental setups
for the evaluation process. The overall evaluation results indicate that by
performing pre-processing steps, such as stemming for agglutinative languages,
the system yields better results, eventually achieving 91% accuracy result in
the best performing model
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hollywood Identity Bias Dataset: A Context Oriented Bias Analysis of Movie Dialogues. (arXiv:2205.15951v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15951">
<div class="article-summary-box-inner">
<span><p>Movies reflect society and also hold power to transform opinions. Social
biases and stereotypes present in movies can cause extensive damage due to
their reach. These biases are not always found to be the need of storyline but
can creep in as the author's bias. Movie production houses would prefer to
ascertain that the bias present in a script is the story's demand. Today, when
deep learning models can give human-level accuracy in multiple tasks, having an
AI solution to identify the biases present in the script at the writing stage
can help them avoid the inconvenience of stalled release, lawsuits, etc. Since
AI solutions are data intensive and there exists no domain specific data to
address the problem of biases in scripts, we introduce a new dataset of movie
scripts that are annotated for identity bias. The dataset contains dialogue
turns annotated for (i) bias labels for seven categories, viz., gender,
race/ethnicity, religion, age, occupation, LGBTQ, and other, which contains
biases like body shaming, personality bias, etc. (ii) labels for sensitivity,
stereotype, sentiment, emotion, emotion intensity, (iii) all labels annotated
with context awareness, (iv) target groups and reason for bias labels and (v)
expert-driven group-validation process for high quality annotations. We also
report various baseline performances for bias identification and category
detection on our dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Graph -- Deep Learning: A Case Study in Question Answering in Aviation Safety Domain. (arXiv:2205.15952v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15952">
<div class="article-summary-box-inner">
<span><p>In the commercial aviation domain, there are a large number of documents,
like, accident reports (NTSB, ASRS) and regulatory directives (ADs). There is a
need for a system to access these diverse repositories efficiently in order to
service needs in the aviation industry, like maintenance, compliance, and
safety. In this paper, we propose a Knowledge Graph (KG) guided Deep Learning
(DL) based Question Answering (QA) system for aviation safety. We construct a
Knowledge Graph from Aircraft Accident reports and contribute this resource to
the community of researchers. The efficacy of this resource is tested and
proved by the aforesaid QA system. Natural Language Queries constructed from
the documents mentioned above are converted into SPARQL (the interface language
of the RDF graph database) queries and answered. On the DL side, we have two
different QA models: (i) BERT QA which is a pipeline of Passage Retrieval
(Sentence-BERT based) and Question Answering (BERT based), and (ii) the
recently released GPT-3. We evaluate our system on a set of queries created
from the accident reports. Our combined QA system achieves 9.3% increase in
accuracy over GPT-3 and 40.3% increase over BERT QA. Thus, we infer that KG-DL
performs better than either singly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages. (arXiv:2205.15960v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15960">
<div class="article-summary-box-inner">
<span><p>Natural language processing (NLP) has a significant impact on society via
technologies such as machine translation and search engines. Despite its
success, NLP technology is only widely available for high-resource languages
such as English and Chinese, while it remains inaccessible to many languages
due to the unavailability of data resources and benchmarks. In this work, we
focus on developing resources for languages in Indonesia. Despite being the
second most linguistically diverse country, most languages in Indonesia are
categorized as endangered and some are even extinct. We develop the first-ever
parallel resource for 10 low-resource languages in Indonesia. Our resource
includes datasets, a multi-task benchmark, and lexicons, as well as a parallel
Indonesian-English dataset. We provide extensive analyses and describe the
challenges when creating such resources. We hope that our work can spark NLP
research on Indonesian and other underrepresented languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cluster-based Evaluation of Automatically Generated Text. (arXiv:2205.16001v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.16001">
<div class="article-summary-box-inner">
<span><p>While probabilistic language generators have improved dramatically over the
last few years, the automatic evaluation metrics used to assess them have not
kept pace with this progress. In the domain of language generation, a good
metric must correlate highly with human judgements. Yet, with few exceptions,
there is a lack of such metrics in the literature. In this work, we analyse the
general paradigm of language generator evaluation. We first discuss the
computational and qualitative issues with using automatic evaluation metrics
that operate on probability distributions over strings, the backbone of most
language generators. We then propose the use of distributions over clusters
instead, where we cluster strings based on their text embeddings (obtained from
a pretrained language model). While we find the biases introduced by this
substitution to be quite strong, we observe that, empirically, this methodology
leads to metric estimators with higher correlation with human judgements, while
simultaneously reducing estimator variance. We finish the paper with a probing
analysis, which leads us to conclude that -- by encoding syntactic- and
coherence-level features of text, while ignoring surface-level features --
these clusters may simply be better equipped to evaluate state-of-the-art
language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Retriever and Go Beyond: A Thesis Proposal. (arXiv:2205.16005v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.16005">
<div class="article-summary-box-inner">
<span><p>Information Retriever (IR) aims to find the relevant documents (e.g.
snippets, passages, and articles) to a given query at large scale. IR plays an
important role in many tasks such as open domain question answering and
dialogue systems, where external knowledge is needed. In the past, searching
algorithms based on term matching have been widely used. Recently, neural-based
algorithms (termed as neural retrievers) have gained more attention which can
mitigate the limitations of traditional methods. Regardless of the success
achieved by neural retrievers, they still face many challenges, e.g. suffering
from a small amount of training data and failing to answer simple
entity-centric questions. Furthermore, most of the existing neural retrievers
are developed for pure-text query. This prevents them from handling
multi-modality queries (i.e. the query is composed of textual description and
images). This proposal has two goals. First, we introduce methods to address
the abovementioned issues of neural retrievers from three angles, new model
architectures, IR-oriented pretraining tasks, and generating large scale
training data. Second, we identify the future research direction and propose
potential corresponding solution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Topic Model via Optimal Transport. (arXiv:2008.13537v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.13537">
<div class="article-summary-box-inner">
<span><p>Recently, Neural Topic Models (NTMs) inspired by variational autoencoders
have obtained increasingly research interest due to their promising results on
text analysis. However, it is usually hard for existing NTMs to achieve good
document representation and coherent/diverse topics at the same time. Moreover,
they often degrade their performance severely on short documents. The
requirement of reparameterisation could also comprise their training quality
and model flexibility. To address these shortcomings, we present a new neural
topic model via the theory of optimal transport (OT). Specifically, we propose
to learn the topic distribution of a document by directly minimising its OT
distance to the document's word distributions. Importantly, the cost matrix of
the OT distance models the weights between topics and words, which is
constructed by the distances between topics and words in an embedding space.
Our proposed model can be trained efficiently with a differentiable loss.
Extensive experiments show that our framework significantly outperforms the
state-of-the-art NTMs on discovering more coherent and diverse topics and
deriving better document representations for both regular and short texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional Generalization in Multilingual Semantic Parsing over Wikidata. (arXiv:2108.03509v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03509">
<div class="article-summary-box-inner">
<span><p>Semantic parsing (SP) allows humans to leverage vast knowledge resources
through natural interaction. However, parsers are mostly designed for and
evaluated on English resources, such as CFQ (Keysers et al., 2020), the current
standard benchmark based on English data generated from grammar rules and
oriented towards Freebase, an outdated knowledge base. We propose a method for
creating a multilingual, parallel dataset of question-query pairs, grounded in
Wikidata. We introduce such a dataset, which we call Multilingual Compositional
Wikidata Questions (MCWQ), and use it to analyze the compositional
generalization of semantic parsers in Hebrew, Kannada, Chinese and English.
While within-language generalization is comparable across languages,
experiments on zero-shot cross-lingual transfer demonstrate that cross-lingual
compositional generalization fails, even with state-of-the-art pretrained
multilingual encoders. Furthermore, our methodology, dataset and results will
facilitate future research on SP in more realistic and diverse settings than
has been possible with existing resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Ground Visual Objects for Visual Dialog. (arXiv:2109.06013v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06013">
<div class="article-summary-box-inner">
<span><p>Visual dialog is challenging since it needs to answer a series of coherent
questions based on understanding the visual environment. How to ground related
visual objects is one of the key problems. Previous studies utilize the
question and history to attend to the image and achieve satisfactory
performance, however these methods are not sufficient to locate related visual
objects without any guidance. The inappropriate grounding of visual objects
prohibits the performance of visual dialog models. In this paper, we propose a
novel approach to Learn to Ground visual objects for visual dialog, which
employs a novel visual objects grounding mechanism where both prior and
posterior distributions over visual objects are used to facilitate visual
objects grounding. Specifically, a posterior distribution over visual objects
is inferred from both context (history and questions) and answers, and it
ensures the appropriate grounding of visual objects during the training
process. Meanwhile, a prior distribution, which is inferred from context only,
is used to approximate the posterior distribution so that appropriate visual
objects can be grounded even without answers during the inference process.
Experimental results on the VisDial v0.9 and v1.0 datasets demonstrate that our
approach improves the previous strong models in both generative and
discriminative settings by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPARQLing Database Queries from Intermediate Question Decompositions. (arXiv:2109.06162v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06162">
<div class="article-summary-box-inner">
<span><p>To translate natural language questions into executable database queries,
most approaches rely on a fully annotated training set. Annotating a large
dataset with queries is difficult as it requires query-language expertise. We
reduce this burden using grounded in databases intermediate question
representations. These representations are simpler to collect and were
originally crowdsourced within the Break dataset (Wolfson et al., 2020). Our
pipeline consists of two parts: a neural semantic parser that converts natural
language questions into the intermediate representations and a non-trainable
transpiler to the SPARQL query language (a standard language for accessing
knowledge graphs and semantic web). We chose SPARQL because its queries are
structurally closer to our intermediate representations (compared to SQL). We
observe that the execution accuracy of queries constructed by our model on the
challenging Spider dataset is comparable with the state-of-the-art text-to-SQL
methods trained with annotated SQL queries. Our code and data are publicly
available (see https://github.com/yandex-research/sparqling-queries).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GoG: Relation-aware Graph-over-Graph Network for Visual Dialog. (arXiv:2109.08475v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08475">
<div class="article-summary-box-inner">
<span><p>Visual dialog, which aims to hold a meaningful conversation with humans about
a given image, is a challenging task that requires models to reason the complex
dependencies among visual content, dialog history, and current questions. Graph
neural networks are recently applied to model the implicit relations between
objects in an image or dialog. However, they neglect the importance of 1)
coreference relations among dialog history and dependency relations between
words for the question representation; and 2) the representation of the image
based on the fully represented question. Therefore, we propose a novel
relation-aware graph-over-graph network (GoG) for visual dialog. Specifically,
GoG consists of three sequential graphs: 1) H-Graph, which aims to capture
coreference relations among dialog history; 2) History-aware Q-Graph, which
aims to fully understand the question through capturing dependency relations
between words based on coreference resolution on the dialog history; and 3)
Question-aware I-Graph, which aims to capture the relations between objects in
an image based on fully question representation. As an additional feature
representation module, we add GoG to the existing visual dialogue model.
Experimental results show that our model outperforms the strong baseline in
both generative and discriminative settings by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MixQG: Neural Question Generation with Mixed Answer Types. (arXiv:2110.08175v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08175">
<div class="article-summary-box-inner">
<span><p>Asking good questions is an essential ability for both human and machine
intelligence. However, existing neural question generation approaches mainly
focus on the short factoid type of answers. In this paper, we propose a neural
question generator, MixQG, to bridge this gap. We combine 9 question answering
datasets with diverse answer types, including yes/no, multiple-choice,
extractive, and abstractive answers, to train a single generative model. We
show with empirical results that our model outperforms existing work in both
seen and unseen domains and can generate questions with different cognitive
levels when conditioned on different answer types. Our code is released and
well-integrated with the Huggingface library to facilitate various downstream
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BitextEdit: Automatic Bitext Editing for Improved Low-Resource Machine Translation. (arXiv:2111.06787v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06787">
<div class="article-summary-box-inner">
<span><p>Mined bitexts can contain imperfect translations that yield unreliable
training signals for Neural Machine Translation (NMT). While filtering such
pairs out is known to improve final model quality, we argue that it is
suboptimal in low-resource conditions where even mined data can be limited. In
our work, we propose instead, to refine the mined bitexts via automatic
editing: given a sentence in a language xf, and a possibly imperfect
translation of it xe, our model generates a revised version xf' or xe' that
yields a more equivalent translation pair (i.e., &lt;xf, xe'&gt; or &lt;xf', xe&gt;). We
use a simple editing strategy by (1) mining potentially imperfect translations
for each sentence in a given bitext, (2) learning a model to reconstruct the
original translations and translate, in a multi-task fashion. Experiments
demonstrate that our approach successfully improves the quality of CCMatrix
mined bitext for 5 low-resource language-pairs and 10 translation directions by
up to ~ 8 BLEU points, in most cases improving upon a competitive
back-translation baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making sense of electrical vehicle discussions using sentiment analysis on closely related news and user comments. (arXiv:2112.12327v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12327">
<div class="article-summary-box-inner">
<span><p>We used a token-wise and document-wise sentiment analysis using both
unsupervised and supervised models applied to both news and user reviews
dataset. And our token-wise sentiment analysis found a statistically
significant difference in sentiment between the two groups (both of which were
very large N), our document-wise supervised sentiment analysis found no
significant difference in sentiment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model-Agnostic Multitask Fine-tuning for Few-shot Vision-Language Transfer Learning. (arXiv:2203.04904v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04904">
<div class="article-summary-box-inner">
<span><p>Despite achieving state-of-the-art zero-shot performance, existing
vision-language models, e.g., CLIP, still fall short of domain-specific
classification tasks, e.g., Fungi Classification. In the context of few-shot
transfer learning, traditional fine-tuning fails to prevent highly expressive
model from exploiting spurious correlations in the training data. On the other
hand, although model-agnostic meta-learning (MAML) presents as a natural
alternative for transfer learning, the expensive computation due to implicit
second-order optimization limits its use in large-scale models and datasets. In
this work we aim to further improve the generalization of existing
vision-language models on unseen tasks via a simple yet efficient fine-tuning
strategy based on uniform task sampling. We term our method as Model-Agnostic
Multitask Fine-tuning (MAMF). Compared with MAML, MAMF discards the bi-level
optimization and uses only first-order gradients, which makes it easily
scalable and computationally efficient. Due to the uniform task sampling
procedure, MAMF consistently outperforms the classical fine-tuning method for
few-shot transfer learning on five benchmark datasets. Empirically, we further
discover that the effectiveness of first-order MAML is highly dependent on the
zero-shot performance of the pretrained model, and our simple algorithm can
outperform first-order MAML on more challenging datasets with low zero-shot
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revise and Resubmit: An Intertextual Model of Text-based Collaboration in Peer Review. (arXiv:2204.10805v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10805">
<div class="article-summary-box-inner">
<span><p>Peer review is a key component of the publishing process in most fields of
science. The increasing submission rates put a strain on reviewing quality and
efficiency, motivating the development of applications to support the reviewing
and editorial work. While existing NLP studies focus on the analysis of
individual texts, editorial assistance often requires modeling interactions
between pairs of texts -- yet general frameworks and datasets to support this
scenario are missing. Relationships between texts are the core object of the
intertextuality theory -- a family of approaches in literary studies not yet
operationalized in NLP. Inspired by prior theoretical work, we propose the
first intertextual model of text-based collaboration, which encompasses three
major phenomena that make up a full iteration of the review-revise-and-resubmit
cycle: pragmatic tagging, linking and long-document version alignment. While
peer review is used across the fields of science and publication formats,
existing datasets solely focus on conference-style review in computer science.
Addressing this, we instantiate our proposed model in the first annotated
multi-domain corpus in journal-style post-publication open peer review, and
provide detailed insights into the practical aspects of intertextual
annotation. Our resource is a major step towards multi-domain, fine-grained
applications of NLP in editorial support for peer review, and our intertextual
framework paves the path for general-purpose modeling of text-based
collaboration. Our corpus and accompanying code are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models Can See: Plugging Visual Controls in Text Generation. (arXiv:2205.02655v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02655">
<div class="article-summary-box-inner">
<span><p>Generative language models (LMs) such as GPT-2/3 can be prompted to generate
text with remarkable quality. While they are designed for text-prompted
generation, it remains an open question how the generation process could be
guided by modalities beyond text such as images. In this work, we propose a
training-free framework, called MAGIC (iMAge-Guided text generatIon with CLIP),
for plugging in visual controls in the generation process and enabling LMs to
perform multimodal tasks (e.g., image captioning) in a zero-shot manner. MAGIC
is a simple yet efficient plug-and-play framework, which directly combines an
off-the-shelf LM (i.e., GPT-2) and an image-text matching model (i.e., CLIP)
for image-grounded text generation. During decoding, MAGIC influences the
generation of the LM by introducing a CLIP-induced score, called magic score,
which regularizes the generated result to be semantically related to a given
image while being coherent to the previously generated context. Notably, the
proposed decoding scheme does not involve any gradient update operation,
therefore being computationally efficient. On the challenging task of zero-shot
image captioning, MAGIC outperforms the state-of-the-art method by notable
margins with a nearly 27 times decoding speedup. MAGIC is a flexible framework
and is theoretically compatible with any text generation tasks that incorporate
image grounding. In the experiments, we showcase that it is also capable of
performing visually grounded story generation given both an image and a text
prompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Use of Transformer-Based Models for Word-Level Transliteration of the Book of the Dean of Lismore. (arXiv:2205.11370v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11370">
<div class="article-summary-box-inner">
<span><p>The Book of the Dean of Lismore (BDL) is a 16th-century Scottish Gaelic
manuscript written in a non-standard orthography. In this work, we outline the
problem of transliterating the text of the BDL into a standardised orthography,
and perform exploratory experiments using Transformer-based models for this
task. In particular, we focus on the task of word-level transliteration, and
achieve a character-level BLEU score of 54.15 with our best model, a BART
architecture pre-trained on the text of Scottish Gaelic Wikipedia and then
fine-tuned on around 2,000 word-level parallel examples. Our initial
experiments give promising results, but we highlight the shortcomings of our
model, and discuss directions for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HiVLP: Hierarchical Vision-Language Pre-Training for Fast Image-Text Retrieval. (arXiv:2205.12105v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12105">
<div class="article-summary-box-inner">
<span><p>In the past few years, the emergence of vision-language pre-training (VLP)
has brought cross-modal retrieval to a new era. However, due to the latency and
computation demand, it is commonly challenging to apply VLP in a real-time
online retrieval system. To alleviate the defect, this paper proposes a
\textbf{Hi}erarchical \textbf{V}ision-\textbf{}Language \textbf{P}re-Training
(\textbf{HiVLP}) for fast Image-Text Retrieval (ITR). Specifically, we design a
novel hierarchical retrieval objective, which uses the representation of
different dimensions for coarse-to-fine ITR, i.e., using low-dimensional
representation for large-scale coarse retrieval and high-dimensional
representation for small-scale fine retrieval. We evaluate our proposed HiVLP
on two popular image-text retrieval benchmarks, i.e., Flickr30k and COCO.
Extensive experiments demonstrate that our HiVLP not only has fast inference
speed but also can be easily scaled to large-scale ITR scenarios. The detailed
results show that HiVLP is $1,427$$\sim$$120,649\times$ faster than the
fusion-based model UNITER and 2$\sim$5 faster than the fastest embedding-based
model LightingDot in different candidate scenarios. It also achieves about +4.9
AR on COCO and +3.8 AR on Flickr30K than LightingDot and achieves comparable
performance with the state-of-the-art (SOTA) fusion-based model METER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Dense Graph Do You Need for Self-Attention?. (arXiv:2205.14014v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14014">
<div class="article-summary-box-inner">
<span><p>Transformers have made progress in miscellaneous tasks, but suffer from
quadratic computational and memory complexities. Recent works propose sparse
Transformers with attention on sparse graphs to reduce complexity and remain
strong performance. While effective, the crucial parts of how dense a graph
needs to be to perform well are not fully explored. In this paper, we propose
Normalized Information Payload (NIP), a graph scoring function measuring
information transfer on graph, which provides an analysis tool for trade-offs
between performance and complexity. Guided by this theoretical analysis, we
present Hypercube Transformer, a sparse Transformer that models token
interactions in a hypercube and shows comparable or even better results with
vanilla Transformer while yielding $O(N\log N)$ complexity with sequence length
$N$. Experiments on tasks requiring various sequence lengths lay validation for
our graph function well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L3Cube-MahaNLP: Marathi Natural Language Processing Datasets, Models, and Library. (arXiv:2205.14728v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14728">
<div class="article-summary-box-inner">
<span><p>Despite being the third most popular language in India, the Marathi language
lacks useful NLP resources. Moreover, popular NLP libraries do not have support
for the Marathi language. With L3Cube-MahaNLP, we aim to build resources and a
library for Marathi natural language processing. We present datasets and
transformer models for supervised tasks like sentiment analysis, named entity
recognition, and hate speech detection. We have also published a monolingual
Marathi corpus for unsupervised language modeling tasks. Overall we present
MahaCorpus, MahaSent, MahaNER, and MahaHate datasets and their corresponding
MahaBERT models fine-tuned on these datasets. We aim to move ahead of benchmark
datasets and prepare useful resources for Marathi. The resources are available
at https://github.com/l3cube-pune/MarathiNLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Duplex Conversation: Towards Human-like Interaction in Spoken Dialogue System. (arXiv:2205.15060v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15060">
<div class="article-summary-box-inner">
<span><p>In this paper, we present Duplex Conversation, a multi-turn, multimodal
spoken dialogue system that enables telephone-based agents to interact with
customers like a human. We use the concept of full-duplex in telecommunication
to demonstrate what a human-like interactive experience should be and how to
achieve smooth turn-taking through three subtasks: user state detection,
backchannel selection, and barge-in detection. Besides, we propose
semi-supervised learning with multimodal data augmentation to leverage
unlabeled data to increase model generalization. Experimental results on three
sub-tasks show that the proposed method achieves consistent improvements
compared with baselines. We deploy the Duplex Conversation to Alibaba
intelligent customer service and share lessons learned in production. Online
A/B experiments show that the proposed system can significantly reduce response
latency by 50%.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient and Student-Friendly Knowledge Distillation. (arXiv:2205.15308v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15308">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) has been extensively employed to transfer the
knowledge from a large teacher model to the smaller students, where the
parameters of the teacher are fixed (or partially) during training. Recent
studies show that this mode may cause difficulties in knowledge transfer due to
the mismatched model capacities. To alleviate the mismatch problem,
teacher-student joint training methods, e.g., online distillation, have been
proposed, but it always requires expensive computational cost. In this paper,
we present a parameter-efficient and student-friendly knowledge distillation
method, namely PESF-KD, to achieve efficient and sufficient knowledge transfer
by updating relatively few partial parameters. Technically, we first
mathematically formulate the mismatch as the sharpness gap between their
predictive distributions, where we show such a gap can be narrowed with the
appropriate smoothness of the soft label. Then, we introduce an adapter module
for the teacher and only update the adapter to obtain soft labels with
appropriate smoothness. Experiments on a variety of benchmarks show that
PESF-KD can significantly reduce the training cost while obtaining competitive
results compared to advanced online distillation methods. Code will be released
upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Searching for the Essence of Adversarial Perturbations. (arXiv:2205.15357v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15357">
<div class="article-summary-box-inner">
<span><p>Neural networks have achieved the state-of-the-art performance on various
machine learning fields, yet the incorporation of malicious perturbations with
input data (adversarial example) is able to fool neural networks' predictions.
This would lead to potential risks in real-world applications, for example,
auto piloting and facial recognition. However, the reason for the existence of
adversarial examples remains controversial. Here we demonstrate that
adversarial perturbations contain human-recognizable information, which is the
key conspirator responsible for a neural network's erroneous prediction. This
concept of human-recognizable information allows us to explain key features
related to adversarial perturbations, which include the existence of
adversarial examples, the transferability among different neural networks, and
the increased neural network interpretability for adversarial training. Two
unique properties in adversarial perturbations that fool neural networks are
uncovered: masking and generation. A special class, the complementary class, is
identified when neural networks classify input images. The human-recognizable
information contained in adversarial perturbations allows researchers to gain
insight on the working principles of neural networks and may lead to develop
techniques that detect/defense adversarial attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Audio Pattern Recognition for Asthma Medication Adherence: Evaluation with the RDA Benchmark Suite. (arXiv:2205.15360v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15360">
<div class="article-summary-box-inner">
<span><p>Asthma is a common, usually long-term respiratory disease with negative
impact on society and the economy worldwide. Treatment involves using medical
devices (inhalers) that distribute medication to the airways, and its
efficiency depends on the precision of the inhalation technique. Health
monitoring systems equipped with sensors and embedded with sound signal
detection enable the recognition of drug actuation and could be powerful tools
for reliable audio content analysis. This paper revisits audio pattern
recognition and machine learning techniques for asthma medication adherence
assessment and presents the Respiratory and Drug Actuation (RDA)
Suite(https://gitlab.com/vvr/monitoring-medication-adherence/rda-benchmark) for
benchmarking and further research. The RDA Suite includes a set of tools for
audio processing, feature extraction and classification and is provided along
with a dataset consisting of respiratory and drug actuation sounds. The
classification models in RDA are implemented based on conventional and advanced
machine learning and deep network architectures. This study provides a
comparative evaluation of the implemented approaches, examines potential
improvements and discusses challenges and future tendencies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TubeFormer-DeepLab: Video Mask Transformer. (arXiv:2205.15361v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15361">
<div class="article-summary-box-inner">
<span><p>We present TubeFormer-DeepLab, the first attempt to tackle multiple core
video segmentation tasks in a unified manner. Different video segmentation
tasks (e.g., video semantic/instance/panoptic segmentation) are usually
considered as distinct problems. State-of-the-art models adopted in the
separate communities have diverged, and radically different approaches dominate
in each task. By contrast, we make a crucial observation that video
segmentation tasks could be generally formulated as the problem of assigning
different predicted labels to video tubes (where a tube is obtained by linking
segmentation masks along the time axis) and the labels may encode different
values depending on the target task. The observation motivates us to develop
TubeFormer-DeepLab, a simple and effective video mask transformer model that is
widely applicable to multiple video segmentation tasks. TubeFormer-DeepLab
directly predicts video tubes with task-specific labels (either pure semantic
categories, or both semantic categories and instance identities), which not
only significantly simplifies video segmentation models, but also advances
state-of-the-art results on multiple video segmentation benchmarks
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dictionary Learning with Accumulator Neurons. (arXiv:2205.15386v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15386">
<div class="article-summary-box-inner">
<span><p>The Locally Competitive Algorithm (LCA) uses local competition between
non-spiking leaky integrator neurons to infer sparse representations, allowing
for potentially real-time execution on massively parallel neuromorphic
architectures such as Intel's Loihi processor. Here, we focus on the problem of
inferring sparse representations from streaming video using dictionaries of
spatiotemporal features optimized in an unsupervised manner for sparse
reconstruction. Non-spiking LCA has previously been used to achieve
unsupervised learning of spatiotemporal dictionaries composed of convolutional
kernels from raw, unlabeled video. We demonstrate how unsupervised dictionary
learning with spiking LCA (\hbox{S-LCA}) can be efficiently implemented using
accumulator neurons, which combine a conventional leaky-integrate-and-fire
(\hbox{LIF}) spike generator with an additional state variable that is used to
minimize the difference between the integrated input and the spiking output. We
demonstrate dictionary learning across a wide range of dynamical regimes, from
graded to intermittent spiking, for inferring sparse representations of both
static images drawn from the CIFAR database as well as video frames captured
from a DVS camera. On a classification task that requires identification of the
suite from a deck of cards being rapidly flipped through as viewed by a DVS
camera, we find essentially no degradation in performance as the LCA model used
to infer sparse spatiotemporal representations migrates from graded to spiking.
We conclude that accumulator neurons are likely to provide a powerful enabling
component of future neuromorphic hardware for implementing online unsupervised
learning of spatiotemporal dictionaries optimized for sparse reconstruction of
streaming video from event based DVS cameras.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VoGE: A Differentiable Volume Renderer using Gaussian Ellipsoids for Analysis-by-Synthesis. (arXiv:2205.15401v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15401">
<div class="article-summary-box-inner">
<span><p>Differentiable rendering allows the application of computer graphics on
vision tasks, e.g. object pose and shape fitting, via analysis-by-synthesis,
where gradients at occluded regions are important when inverting the rendering
process. To obtain those gradients, state-of-the-art (SoTA) differentiable
renderers use rasterization to collect a set of nearest components for each
pixel and aggregate them based on the viewing distance. In this paper, we
propose VoGE, which uses ray tracing to capture nearest components with their
volume density distributions on the rays and aggregates via integral of the
volume densities based on Gaussian ellipsoids, which brings more efficient and
stable gradients. To efficiently render via VoGE, we propose an approximate
close-form solution for the volume density aggregation and a coarse-to-fine
rendering strategy. Finally, we provide a CUDA implementation of VoGE, which
gives a competitive rendering speed in comparison to PyTorch3D. Quantitative
and qualitative experiment results show VoGE outperforms SoTA counterparts when
applied to various vision tasks,e.g., object pose estimation, shape/texture
fitting, and occlusion reasoning. The VoGE library and demos are available at
https://github.com/Angtian/VoGE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gator: Customizable Channel Pruning of Neural Networks with Gating. (arXiv:2205.15404v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15404">
<div class="article-summary-box-inner">
<span><p>The rise of neural network (NN) applications has prompted an increased
interest in compression, with a particular focus on channel pruning, which does
not require any additional hardware. Most pruning methods employ either
single-layer operations or global schemes to determine which channels to remove
followed by fine-tuning of the network. In this paper we present Gator, a
channel-pruning method which temporarily adds learned gating mechanisms for
pruning of individual channels, and which is trained with an additional
auxiliary loss, aimed at reducing the computational cost due to memory,
(theoretical) speedup (in terms of FLOPs), and practical, hardware-specific
speedup. Gator introduces a new formulation of dependencies between NN layers
which, in contrast to most previous methods, enables pruning of non-sequential
parts, such as layers on ResNet's highway, and even removing entire ResNet
blocks. Gator's pruning for ResNet-50 trained on ImageNet produces
state-of-the-art (SOTA) results, such as 50% FLOPs reduction with only
0.4%-drop in top-5 accuracy. Also, Gator outperforms previous pruning models,
in terms of GPU latency by running 1.4 times faster. Furthermore, Gator
achieves improved top-5 accuracy results, compared to MobileNetV2 and
SqueezeNet, for similar runtimes. The source code of this work is available at:
https://github.com/EliPassov/gator.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grid HTM: Hierarchical Temporal Memory for Anomaly Detection in Videos. (arXiv:2205.15407v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15407">
<div class="article-summary-box-inner">
<span><p>The interest for video anomaly detection systems has gained traction for the
past few years. The current approaches use deep learning to perform anomaly
detection in videos, but this approach has multiple problems. For starters,
deep learning in general has issues with noise, concept drift, explainability,
and training data volumes. Additionally, anomaly detection in itself is a
complex task and faces challenges such as unknowness, heterogeneity, and class
imbalance. Anomaly detection using deep learning is therefore mainly
constrained to generative models such as generative adversarial networks and
autoencoders due to their unsupervised nature, but even they suffer from
general deep learning issues and are hard to train properly. In this paper, we
explore the capabilities of the Hierarchical Temporal Memory (HTM) algorithm to
perform anomaly detection in videos, as it has favorable properties such as
noise tolerance and online learning which combats concept drift. We introduce a
novel version of HTM, namely, Grid HTM, which is an HTM-based architecture
specifically for anomaly detection in complex videos such as surveillance
footage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LiDAR-aid Inertial Poser: Large-scale Human Motion Capture by Sparse Inertial and LiDAR Sensors. (arXiv:2205.15410v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15410">
<div class="article-summary-box-inner">
<span><p>We propose a multi-sensor fusion method for capturing challenging 3D human
motions with accurate consecutive local poses and global trajectories in
large-scale scenarios, only using a single LiDAR and 4 IMUs. Specifically, to
fully utilize the global geometry information captured by LiDAR and local
dynamic motions captured by IMUs, we design a two-stage pose estimator in a
coarse-to-fine manner, where point clouds provide the coarse body shape and IMU
measurements optimize the local actions. Furthermore, considering the
translation deviation caused by the view-dependent partial point cloud, we
propose a pose-guided translation corrector. It predicts the offset between
captured points and the real root locations, which makes the consecutive
movements and trajectories more precise and natural. Extensive quantitative and
qualitative experiments demonstrate the capability of our approach for
compelling motion capture in large-scale scenarios, which outperforms other
methods by an obvious margin. We will release our code and captured dataset to
stimulate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PolypConnect: Image inpainting for generating realistic gastrointestinal tract images with polyps. (arXiv:2205.15413v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15413">
<div class="article-summary-box-inner">
<span><p>Early identification of a polyp in the lower gastrointestinal (GI) tract can
lead to prevention of life-threatening colorectal cancer. Developing
computer-aided diagnosis (CAD) systems to detect polyps can improve detection
accuracy and efficiency and save the time of the domain experts called
endoscopists. Lack of annotated data is a common challenge when building CAD
systems. Generating synthetic medical data is an active research area to
overcome the problem of having relatively few true positive cases in the
medical domain. To be able to efficiently train machine learning (ML) models,
which are the core of CAD systems, a considerable amount of data should be
used. In this respect, we propose the PolypConnect pipeline, which can convert
non-polyp images into polyp images to increase the size of training datasets
for training. We present the whole pipeline with quantitative and qualitative
evaluations involving endoscopists. The polyp segmentation model trained using
synthetic data, and real data shows a 5.1% improvement of mean intersection
over union (mIOU), compared to the model trained only using real data. The
codes of all the experiments are available on GitHub to reproduce the results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fitting and recognition of geometric primitives in segmented 3D point clouds using a localized voting procedure. (arXiv:2205.15426v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15426">
<div class="article-summary-box-inner">
<span><p>The automatic creation of geometric models from point clouds has numerous
applications in CAD (e.g., reverse engineering, manufacturing, assembling) and,
more in general, in shape modelling and processing. Given a segmented point
cloud representing a man-made object, we propose a method for recognizing
simple geometric primitives and their interrelationships. Our approach is based
on the Hough transform (HT) for its ability to deal with noise, missing parts
and outliers. In our method we introduce a novel technique for processing
segmented point clouds that, through a voting procedure, is able to provide an
initial estimate of the geometric parameters characterizing each primitive
type. By using these estimates, we localize the search of the optimal solution
in a dimensionally-reduced parameter space thus making it efficient to extend
the HT to more primitives than those that are generally found in the
literature, i.e. planes and spheres. Then, we extract a number of geometric
descriptors that uniquely characterize a segment, and, on the basis of these
descriptors, we show how to aggregate parts of primitives (segments).
Experiments on both synthetic and industrial scans reveal the robustness of the
primitive fitting method and its effectiveness for inferring relations among
segments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmentation Consistency Training: Out-of-Distribution Generalization for Medical Image Segmentation. (arXiv:2205.15428v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15428">
<div class="article-summary-box-inner">
<span><p>Generalizability is seen as one of the major challenges in deep learning, in
particular in the domain of medical imaging, where a change of hospital or in
imaging routines can lead to a complete failure of a model. To tackle this, we
introduce Consistency Training, a training procedure and alternative to data
augmentation based on maximizing models' prediction consistency across
augmented and unaugmented data in order to facilitate better
out-of-distribution generalization. To this end, we develop a novel
region-based segmentation loss function called Segmentation Inconsistency Loss
(SIL), which considers the differences between pairs of augmented and
unaugmented predictions and labels. We demonstrate that Consistency Training
outperforms conventional data augmentation on several out-of-distribution
datasets on polyp segmentation, a popular medical task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Advances in Transformers and CNN for Skin Lesion Diagnosis on Small Datasets. (arXiv:2205.15442v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15442">
<div class="article-summary-box-inner">
<span><p>Skin cancer is one of the most common types of cancer in the world. Different
computer-aided diagnosis systems have been proposed to tackle skin lesion
diagnosis, most of them based in deep convolutional neural networks. However,
recent advances in computer vision achieved state-of-art results in many tasks,
notably Transformer-based networks. We explore and evaluate advances in
computer vision architectures, training methods and multimodal feature fusion
for skin lesion diagnosis task. Experiments show that PiT ($0.800 \pm 0.006$),
CoaT ($0.780 \pm 0.024$) and ViT ($0.771 \pm 0.018$) backbone models with
MetaBlock fusion achieved state-of-art results for balanced accuracy metric in
PAD-UFES-20 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Object Detection: A review of definitions, strategies, and challenges. (arXiv:2205.15445v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15445">
<div class="article-summary-box-inner">
<span><p>The field of Continual Learning investigates the ability to learn consecutive
tasks without losing performance on those previously learned. Its focus has
been mainly on incremental classification tasks. We believe that research in
continual object detection deserves even more attention due to its vast range
of applications in robotics and autonomous vehicles. This scenario is more
complex than conventional classification given the occurrence of instances of
classes that are unknown at the time, but can appear in subsequent tasks as a
new class to be learned, resulting in missing annotations and conflicts with
the background label. In this review, we analyze the current strategies
proposed to tackle the problem of class-incremental object detection. Our main
contributions are: (1) a short and systematic review of the methods that
propose solutions to traditional incremental object detection scenarios; (2) A
comprehensive evaluation of the existing approaches using a new metric to
quantify the stability and plasticity of each technique in a standard way; (3)
an overview of the current trends within continual object detection and a
discussion of possible future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HeatER: An Efficient and Unified Network for Human Reconstruction via Heatmap-based TransformER. (arXiv:2205.15448v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15448">
<div class="article-summary-box-inner">
<span><p>Recently, vision transformers have shown great success in 2D human pose
estimation (2D HPE), 3D human pose estimation (3D HPE), and human mesh
reconstruction (HMR) tasks. In these tasks, heatmap representations of the
human structural information are often extracted first from the image by a CNN,
and then further processed with a transformer architecture to provide the final
HPE or HMR estimation. However, existing transformer architectures are not able
to process these heatmap inputs directly, forcing an unnatural flattening of
the features prior to input. Furthermore, much of the performance benefit in
recent HPE and HMR methods has come at the cost of ever-increasing computation
and memory needs. Therefore, to simultaneously address these problems, we
propose HeatER, a novel transformer design which preserves the inherent
structure of heatmap representations when modeling attention while reducing the
memory and computational costs. Taking advantage of HeatER, we build a unified
and efficient network for 2D HPE, 3D HPE, and HMR tasks. A heatmap
reconstruction module is applied to improve the robustness of the estimated
human pose and mesh. Extensive experiments demonstrate the effectiveness of
HeatER on various human pose and mesh datasets. For instance, HeatER
outperforms the SOTA method MeshGraphormer by requiring 5% of Params and 16% of
MACs on Human3.6M and 3DPW datasets. Code will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MVMO: A Multi-Object Dataset for Wide Baseline Multi-View Semantic Segmentation. (arXiv:2205.15452v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15452">
<div class="article-summary-box-inner">
<span><p>We present MVMO (Multi-View, Multi-Object dataset): a synthetic dataset of
116,000 scenes containing randomly placed objects of 10 distinct classes and
captured from 25 camera locations in the upper hemisphere. MVMO comprises
photorealistic, path-traced image renders, together with semantic segmentation
ground truth for every view. Unlike existing multi-view datasets, MVMO features
wide baselines between cameras and high density of objects, which lead to large
disparities, heavy occlusions and view-dependent object appearance. Single view
semantic segmentation is hindered by self and inter-object occlusions that
could benefit from additional viewpoints. Therefore, we expect that MVMO will
propel research in multi-view semantic segmentation and cross-view semantic
transfer. We also provide baselines that show that new research is needed in
such fields to exploit the complementary information of multi-view setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Registering Image Volumes using 3D SIFT and Discrete SP-Symmetry. (arXiv:2205.15456v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15456">
<div class="article-summary-box-inner">
<span><p>This paper proposes to extend local image features in 3D to include
invariance to discrete symmetry including inversion of spatial axes and image
contrast. A binary feature sign $s \in \{-1,+1\}$ is defined as the sign of the
Laplacian operator $\nabla^2$, and used to obtain a descriptor that is
invariant to image sign inversion $s \rightarrow -s$ and 3D parity transforms
$(x,y,z)\rightarrow(-x,-y,-z)$, i.e. SP-invariant or SP-symmetric. SP-symmetry
applies to arbitrary scalar image fields $I: R^3 \rightarrow R^1$ mapping 3D
coordinates $(x,y,z) \in R^3$ to scalar intensity $I(x,y,z) \in R^1$,
generalizing the well-known charge conjugation and parity symmetry
(CP-symmetry) applying to elementary charged particles. Feature orientation is
modeled as a set of discrete states corresponding to potential axis
reflections, independently of image contrast inversion. Two primary axis
vectors are derived from image observations and potentially subject to
reflection, and a third axis is an axial vector defined by the right-hand rule.
Augmenting local feature properties with sign in addition to standard
(location, scale, orientation) geometry leads to descriptors that are invariant
to coordinate reflections and intensity contrast inversion. Feature properties
are factored in to probabilistic point-based registration as symmetric kernels,
based on a model of binary feature correspondence. Experiments using the
well-known coherent point drift (CPD) algorithm demonstrate that SIFT-CPD
kernels achieve the most accurate and rapid registration of the human brain and
CT chest, including multiple MRI modalities of differing intensity contrast,
and abnormal local variations such as tumors or occlusions. SIFT-CPD image
registration is invariant to global scaling, rotation and translation and image
intensity inversions of the input data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Diffusion Models. (arXiv:2205.15463v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15463">
<div class="article-summary-box-inner">
<span><p>Denoising diffusion probabilistic models (DDPM) are powerful hierarchical
latent variable models with remarkable sample generation quality and training
stability. These properties can be attributed to parameter sharing in the
generative hierarchy, as well as a parameter-free diffusion-based inference
procedure. In this paper, we present Few-Shot Diffusion Models (FSDM), a
framework for few-shot generation leveraging conditional DDPMs. FSDMs are
trained to adapt the generative process conditioned on a small set of images
from a given class by aggregating image patch information using a set-based
Vision Transformer (ViT). At test time, the model is able to generate samples
from previously unseen classes conditioned on as few as 5 samples from that
class. We empirically show that FSDM can perform few-shot generation and
transfer to new datasets. We benchmark variants of our method on complex vision
datasets for few-shot learning and compare to unconditional and conditional
DDPM baselines. Additionally, we show how conditioning the model on patch-based
input set information improves training convergence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GCoNet+: A Stronger Group Collaborative Co-Salient Object Detector. (arXiv:2205.15469v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15469">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel end-to-end group collaborative learning
network, termed GCoNet+, which can effectively and efficiently (250 fps)
identify co-salient objects in natural scenes. The proposed GCoNet+ achieves
the new state-of-the-art performance for co-salient object detection (CoSOD)
through mining consensus representations based on the following two essential
criteria: 1) intra-group compactness to better formulate the consistency among
co-salient objects by capturing their inherent shared attributes using our
novel group affinity module (GAM); 2) inter-group separability to effectively
suppress the influence of noisy objects on the output by introducing our new
group collaborating module (GCM) conditioning on the inconsistent consensus. To
further improve the accuracy, we design a series of simple yet effective
components as follows: i) a recurrent auxiliary classification module (RACM)
promoting the model learning at the semantic level; ii) a confidence
enhancement module (CEM) helping the model to improve the quality of the final
predictions; and iii) a group-based symmetric triplet (GST) loss guiding the
model to learn more discriminative features. Extensive experiments on three
challenging benchmarks, i.e., CoCA, CoSOD3k, and CoSal2015, demonstrate that
our GCoNet+ outperforms the existing 12 cutting-edge models. Code has been
released at https://github.com/ZhengPeng7/GCoNet_plus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Introduction of a tree-based technique for efficient and real-time label retrieval in the object tracking system. (arXiv:2205.15477v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15477">
<div class="article-summary-box-inner">
<span><p>This paper addresses the issue of the real-time tracking quality of moving
objects in large-scale video surveillance systems. During the tracking process,
the system assigns an identifier or label to each tracked object to distinguish
it from other objects. In such a mission, it is essential to keep this
identifier for the same objects, whatever the area, the time of their
appearance, or the detecting camera. This is to conserve as much information
about the tracking object as possible, decrease the number of ID switching
(ID-Sw), and increase the quality of object tracking. To accomplish object
labeling, a massive amount of data collected by the cameras must be searched to
retrieve the most similar (nearest neighbor) object identifier. Although this
task is simple, it becomes very complex in large-scale video surveillance
networks, where the data becomes very large. In this case, the label retrieval
time increases significantly with this increase, which negatively affects the
performance of the real-time tracking system. To avoid such problems, we
propose a new solution to automatically label multiple objects for efficient
real-time tracking using the indexing mechanism. This mechanism organizes the
metadata of the objects extracted during the detection and tracking phase in an
Adaptive BCCF-tree. The main advantage of this structure is: its ability to
index massive metadata generated by multi-cameras, its logarithmic search
complexity, which implicitly reduces the search response time, and its quality
of research results, which ensure coherent labeling of the tracked objects. The
system load is distributed through a new Internet of Video Things
infrastructure-based architecture to improve data processing and real-time
object tracking performance. The experimental evaluation was conducted on a
publicly available dataset generated by multi-camera containing different crowd
activities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Spatial-Temporal and Appearance Modeling with Transformer for Multiple Object Tracking. (arXiv:2205.15495v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15495">
<div class="article-summary-box-inner">
<span><p>The recent trend in multiple object tracking (MOT) is heading towards
leveraging deep learning to boost the tracking performance. In this paper, we
propose a novel solution named TransSTAM, which leverages Transformer to
effectively model both the appearance features of each object and the
spatial-temporal relationships among objects. TransSTAM consists of two major
parts: (1) The encoder utilizes the powerful self-attention mechanism of
Transformer to learn discriminative features for each tracklet; (2) The decoder
adopts the standard cross-attention mechanism to model the affinities between
the tracklets and the detections by taking both spatial-temporal and appearance
features into account. TransSTAM has two major advantages: (1) It is solely
based on the encoder-decoder architecture and enjoys a compact network design,
hence being computationally efficient; (2) It can effectively learn
spatial-temporal and appearance features within one model, hence achieving
better tracking accuracy. The proposed method is evaluated on multiple public
benchmarks including MOT16, MOT17, and MOT20, and it achieves a clear
performance improvement in both IDF1 and HOTA with respect to previous
state-of-the-art approaches on all the benchmarks. Our code is available at
\url{https://github.com/icicle4/TranSTAM}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts. (arXiv:2205.15509v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15509">
<div class="article-summary-box-inner">
<span><p>Vision-Language Navigation (VLN) is a challenging task that requires an
embodied agent to perform action-level modality alignment, i.e., make
instruction-asked actions sequentially in complex visual environments. Most
existing VLN agents learn the instruction-path data directly and cannot
sufficiently explore action-level alignment knowledge inside the multi-modal
inputs. In this paper, we propose modAlity-aligneD Action PrompTs (ADAPT),
which provides the VLN agent with action prompts to enable the explicit
learning of action-level modality alignment to pursue successful navigation.
Specifically, an action prompt is defined as a modality-aligned pair of an
image sub-prompt and a text sub-prompt, where the former is a single-view
observation and the latter is a phrase like ''walk past the chair''. When
starting navigation, the instruction-related action prompt set is retrieved
from a pre-built action prompt base and passed through a prompt encoder to
obtain the prompt feature. Then the prompt feature is concatenated with the
original instruction feature and fed to a multi-layer transformer for action
prediction. To collect high-quality action prompts into the prompt base, we use
the Contrastive Language-Image Pretraining (CLIP) model which has powerful
cross-modality alignment ability. A modality alignment loss and a sequential
consistency loss are further introduced to enhance the alignment of the action
prompt and enforce the agent to focus on the related prompt sequentially.
Experimental results on both R2R and RxR show the superiority of ADAPT over
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IDE-3D: Interactive Disentangled Editing for High-Resolution 3D-aware Portrait Synthesis. (arXiv:2205.15517v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15517">
<div class="article-summary-box-inner">
<span><p>Existing 3D-aware facial generation methods face a dilemma in quality versus
editability: they either generate editable results in low resolution or
high-quality ones with no editing flexibility. In this work, we propose a new
approach that brings the best of both worlds together. Our system consists of
three major components: (1) a 3D-semantics-aware generative model that produces
view-consistent, disentangled face images and semantic masks; (2) a hybrid GAN
inversion approach that initialize the latent codes from the semantic and
texture encoder, and further optimized them for faithful reconstruction; and
(3) a canonical editor that enables efficient manipulation of semantic masks in
canonical view and product high-quality editing results. Our approach is
competent for many applications, e.g. free-view face drawing, editing, and
style control. Both quantitative and qualitative results show that our method
reaches the state-of-the-art in terms of photorealism, faithfulness, and
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variational Transfer Learning using Cross-Domain Latent Modulation. (arXiv:2205.15523v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15523">
<div class="article-summary-box-inner">
<span><p>To successfully apply trained neural network models to new domains, powerful
transfer learning solutions are essential. We propose to introduce a novel
cross-domain latent modulation mechanism to a variational autoencoder framework
so as to achieve effective transfer learning. Our key idea is to procure deep
representations from one data domain and use it to influence the
reparameterization of the latent variable of another domain. Specifically, deep
representations of the source and target domains are first extracted by a
unified inference model and aligned by employing gradient reversal. The learned
deep representations are then cross-modulated to the latent encoding of the
alternative domain, where consistency constraints are also applied. In the
empirical validation that includes a number of transfer learning benchmark
tasks for unsupervised domain adaptation and image-to-image translation, our
model demonstrates competitive performance, which is also supported by evidence
obtained from visualization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo-Data based Self-Supervised Federated Learning for Classification of Histopathological Images. (arXiv:2205.15530v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15530">
<div class="article-summary-box-inner">
<span><p>Computer-aided diagnosis (CAD) can help pathologists improve diagnostic
accuracy together with consistency and repeatability for cancers. However, the
CAD models trained with the histopathological images only from a single center
(hospital) generally suffer from the generalization problem due to the
straining inconsistencies among different centers. In this work, we propose a
pseudo-data based self-supervised federated learning (FL) framework, named
SSL-FT-BT, to improve both the diagnostic accuracy and generalization of CAD
models. Specifically, the pseudo histopathological images are generated from
each center, which contains inherent and specific properties corresponding to
the real images in this center, but does not include the privacy information.
These pseudo images are then shared in the central server for self-supervised
learning (SSL). A multi-task SSL is then designed to fully learn both the
center-specific information and common inherent representation according to the
data characteristics. Moreover, a novel Barlow Twins based FL (FL-BT) algorithm
is proposed to improve the local training for the CAD model in each center by
conducting contrastive learning, which benefits the optimization of the global
model in the FL procedure. The experimental results on three public
histopathological image datasets indicate the effectiveness of the proposed
SSL-FL-BT on both diagnostic accuracy and generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">itKD: Interchange Transfer-based Knowledge Distillation for 3D Object Detection. (arXiv:2205.15531v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15531">
<div class="article-summary-box-inner">
<span><p>Recently, point-cloud based 3D object detectors have achieved remarkable
progress. However, most studies are limited to the development of deep learning
architectures for improving only their accuracy. In this paper, we propose an
autoencoder-style framework comprising channel-wise compression and
decompression via interchange transfer for knowledge distillation. To learn the
map-view feature of a teacher network, the features from a teacher and student
network are independently passed through the shared autoencoder; here, we use a
compressed representation loss that binds the channel-wised compression
knowledge from both the networks as a kind of regularization. The decompressed
features are transferred in opposite directions to reduce the gap in the
interchange reconstructions. Lastly, we present an attentive head loss for
matching the pivotal detection information drawn by the multi-head
self-attention mechanism. Through extensive experiments, we verify that our
method can learn the lightweight model that is well-aligned with the 3D point
cloud detection task and we demonstrate its superiority using the well-known
public datasets Waymo and nuScenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gluing Neural Networks Symbolically Through Hyperdimensional Computing. (arXiv:2205.15534v1 [cs.SC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15534">
<div class="article-summary-box-inner">
<span><p>Hyperdimensional Computing affords simple, yet powerful operations to create
long Hyperdimensional Vectors (hypervectors) that can efficiently encode
information, be used for learning, and are dynamic enough to be modified on the
fly. In this paper, we explore the notion of using binary hypervectors to
directly encode the final, classifying output signals of neural networks in
order to fuse differing networks together at the symbolic level. This allows
multiple neural networks to work together to solve a problem, with little
additional overhead. Output signals just before classification are encoded as
hypervectors and bundled together through consensus summation to train a
classification hypervector. This process can be performed iteratively and even
on single neural networks by instead making a consensus of multiple
classification hypervectors. We find that this outperforms the state of the
art, or is on a par with it, while using very little overhead, as hypervector
operations are extremely fast and efficient in comparison to the neural
networks. This consensus process can learn online and even grow or lose models
in real time. Hypervectors act as memories that can be stored, and even further
bundled together over time, affording life long learning capabilities.
Additionally, this consensus structure inherits the benefits of
Hyperdimensional Computing, without sacrificing the performance of modern
Machine Learning. This technique can be extrapolated to virtually any neural
model, and requires little modification to employ - one simply requires
recording the output signals of networks when presented with a testing example.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepDefacer: Automatic Removal of Facial Features via U-Net Image Segmentation. (arXiv:2205.15536v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15536">
<div class="article-summary-box-inner">
<span><p>Recent advancements in the field of magnetic resonance imaging (MRI) have
enabled large-scale collaboration among clinicians and researchers for
neuroimaging tasks. However, researchers are often forced to use outdated and
slow software to anonymize MRI images for publication. These programs
specifically perform expensive mathematical operations over 3D images that
rapidly slow down anonymization speed as an image's volume increases in size.
In this paper, we introduce DeepDefacer, an application of deep learning to MRI
anonymization that uses a streamlined 3D U-Net network to mask facial regions
in MRI images with a significant increase in speed over traditional
de-identification software. We train DeepDefacer on MRI images from the Brain
Development Organization (IXI) and International Consortium for Brain Mapping
(ICBM) and quantitatively evaluate our model against a baseline 3D U-Net model
with regards to Dice, recall, and precision scores. We also evaluate
DeepDefacer against Pydeface, a traditional defacing application, with regards
to speed on a range of CPU and GPU devices and qualitatively evaluate our
model's defaced output versus the ground truth images produced by Pydeface. We
provide a link to a PyPi program at the end of this manuscript to encourage
further research into the application of deep learning to MRI anonymization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI-based automated Meibomian gland segmentation, classification and reflection correction in infrared Meibography. (arXiv:2205.15543v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15543">
<div class="article-summary-box-inner">
<span><p>Purpose: Develop a deep learning-based automated method to segment meibomian
glands (MG) and eyelids, quantitatively analyze the MG area and MG ratio,
estimate the meiboscore, and remove specular reflections from infrared images.
Methods: A total of 1600 meibography images were captured in a clinical
setting. 1000 images were precisely annotated with multiple revisions by
investigators and graded 6 times by meibomian gland dysfunction (MGD) experts.
Two deep learning (DL) models were trained separately to segment areas of the
MG and eyelid. Those segmentation were used to estimate MG ratio and
meiboscores using a classification-based DL model. A generative adversarial
network was implemented to remove specular reflections from original images.
Results: The mean ratio of MG calculated by investigator annotation and DL
segmentation was consistent 26.23% vs 25.12% in the upper eyelids and 32.34%
vs. 32.29% in the lower eyelids, respectively. Our DL model achieved 73.01%
accuracy for meiboscore classification on validation set and 59.17% accuracy
when tested on images from independent center, compared to 53.44% validation
accuracy by MGD experts. The DL-based approach successfully removes reflection
from the original MG images without affecting meiboscore grading. Conclusions:
DL with infrared meibography provides a fully automated, fast quantitative
evaluation of MG morphology (MG Segmentation, MG area, MG ratio, and
meiboscore) which are sufficiently accurate for diagnosing dry eye disease.
Also, the DL removes specular reflection from images to be used by
ophthalmologists for distraction-free assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mask2Hand: Learning to Predict the 3D Hand Pose and Shape from Shadow. (arXiv:2205.15553v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15553">
<div class="article-summary-box-inner">
<span><p>We present a self-trainable method, Mask2Hand, which learns to solve the
challenging task of predicting 3D hand pose and shape from a 2D binary mask of
hand silhouette/shadow without additional manually-annotated data. Given the
intrinsic camera parameters and the parametric hand model in the camera space,
we adopt the differentiable rendering technique to project 3D estimations onto
the 2D binary silhouette space. By applying a tailored combination of losses
between the rendered silhouette and the input binary mask, we are able to
integrate the self-guidance mechanism into our end-to-end optimization process
for constraining global mesh registration and hand pose estimation. The
experiments show that our method, which takes a single binary mask as the
input, can achieve comparable prediction accuracy on both unaligned and aligned
settings as state-of-the-art methods that require RGB or depth inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iFS-RCNN: An Incremental Few-shot Instance Segmenter. (arXiv:2205.15562v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15562">
<div class="article-summary-box-inner">
<span><p>This paper addresses incremental few-shot instance segmentation, where a few
examples of new object classes arrive when access to training examples of old
classes is not available anymore, and the goal is to perform well on both old
and new classes. We make two contributions by extending the common Mask-RCNN
framework in its second stage -- namely, we specify a new object class
classifier based on the probit function and a new uncertainty-guided
bounding-box predictor. The former leverages Bayesian learning to address a
paucity of training examples of new classes. The latter learns not only to
predict object bounding boxes but also to estimate the uncertainty of the
prediction as guidance for bounding box refinement. We also specify two new
loss functions in terms of the estimated object-class distribution and
bounding-box uncertainty. Our contributions produce significant performance
gains on the COCO dataset over the state of the art -- specifically, the gain
of +6 on the new classes and +16 on the old classes in the AP instance
segmentation metric. Furthermore, we are the first to evaluate the incremental
few-shot setting on the more challenging LVIS dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sub-Image Histogram Equalization using Coot Optimization Algorithm for Segmentation and Parameter Selection. (arXiv:2205.15565v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15565">
<div class="article-summary-box-inner">
<span><p>Contrast enhancement is very important in terms of assessing images in an
objective way. Contrast enhancement is also significant for various algorithms
including supervised and unsupervised algorithms for accurate classification of
samples. Some contrast enhancement algorithms solve this problem by addressing
the low contrast issue. Mean and variance based sub-image histogram
equalization (MVSIHE) algorithm is one of these contrast enhancements methods
proposed in the literature. It has different parameters which need to be tuned
in order to achieve optimum results. With this motivation, in this study, we
employed one of the most recent optimization algorithms, namely, coot
optimization algorithm (COA) for selecting appropriate parameters for the
MVSIHE algorithm. Blind/referenceless image spatial quality evaluator (BRISQUE)
and natural image quality evaluator (NIQE) metrics are used for evaluating
fitness of the coot swarm population. The results show that the proposed method
can be used in the field of biomedical image processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Spherical CNNs with Lifting-based Adaptive Wavelets for Pooling and Unpooling. (arXiv:2205.15571v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15571">
<div class="article-summary-box-inner">
<span><p>Pooling and unpooling are two essential operations in constructing
hierarchical spherical convolutional neural networks (HS-CNNs) for
comprehensive feature learning in the spherical domain. Most existing models
employ downsampling-based pooling, which will inevitably incur information loss
and cannot adapt to different spherical signals and tasks. Besides, the
preserved information after pooling cannot be well restored by the subsequent
unpooling to characterize the desirable features for a task. In this paper, we
propose a novel framework of HS-CNNs with a lifting structure to learn adaptive
spherical wavelets for pooling and unpooling, dubbed LiftHS-CNN, which ensures
a more efficient hierarchical feature learning for both image- and pixel-level
tasks. Specifically, adaptive spherical wavelets are learned with a lifting
structure that consists of trainable lifting operators (i.e., update and
predict operators). With this learnable lifting structure, we can adaptively
partition a signal into two sub-bands containing low- and high-frequency
components, respectively, and thus generate a better down-scaled representation
for pooling by preserving more information in the low-frequency sub-band. The
update and predict operators are parameterized with graph-based attention to
jointly consider the signal's characteristics and the underlying geometries. We
further show that particular properties are promised by the learned wavelets,
ensuring the spatial-frequency localization for better exploiting the signal's
correlation in both spatial and frequency domains. We then propose an unpooling
operation that is invertible to the lifting-based pooling, where an inverse
wavelet transform is performed by using the learned lifting operators to
restore an up-scaled representation. Extensive empirical evaluations on various
spherical domain tasks validate the superiority of the proposed LiftHS-CNN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3PSDF: Three-Pole Signed Distance Function for Learning Surfaces with Arbitrary Topologies. (arXiv:2205.15572v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15572">
<div class="article-summary-box-inner">
<span><p>Recent advances in learning 3D shapes using neural implicit functions have
achieved impressive results by breaking the previous barrier of resolution and
diversity for varying topologies. However, most of such approaches are limited
to closed surfaces as they require the space to be divided into inside and
outside. More recent works based on unsigned distance function have been
proposed to handle complex geometry containing both the open and closed
surfaces. Nonetheless, as their direct outputs are point clouds, robustly
obtaining high-quality meshing results from discrete points remains an open
question. We present a novel learnable implicit representation, called the
three-pole signed distance function (3PSDF), that can represent non-watertight
3D shapes with arbitrary topologies while supporting easy field-to-mesh
conversion using the classic Marching Cubes algorithm. The key to our method is
the introduction of a new sign, the NULL sign, in addition to the conventional
in and out labels. The existence of the null sign could stop the formation of a
closed isosurface derived from the bisector of the in/out regions. Further, we
propose a dedicated learning framework to effectively learn 3PSDF without
worrying about the vanishing gradient due to the null labels. Experimental
results show that our approach outperforms the previous state-of-the-art
methods in a wide range of benchmarks both quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MontageGAN: Generation and Assembly of Multiple Components by GANs. (arXiv:2205.15577v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15577">
<div class="article-summary-box-inner">
<span><p>A multi-layer image is more valuable than a single-layer image from a graphic
designer's perspective. However, most of the proposed image generation methods
so far focus on single-layer images. In this paper, we propose MontageGAN,
which is a Generative Adversarial Networks (GAN) framework for generating
multi-layer images. Our method utilized a two-step approach consisting of local
GANs and global GAN. Each local GAN learns to generate a specific image layer,
and the global GAN learns the placement of each generated image layer. Through
our experiments, we show the ability of our method to generate multi-layer
images and estimate the placement of the generated image layers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Effective Fusion Method to Enhance the Robustness of CNN. (arXiv:2205.15582v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15582">
<div class="article-summary-box-inner">
<span><p>With the development of technology rapidly, applications of convolutional
neural networks have improved the convenience of our life. However, in image
classification field, it has been found that when some perturbations are added
to images, the CNN would misclassify it. Thus various defense methods have been
proposed. The previous approach only considered how to incorporate modules in
the network to improve robustness, but did not focus on the way the modules
were incorporated. In this paper, we design a new fusion method to enhance the
robustness of CNN. We use a dot product-based approach to add the denoising
module to ResNet18 and the attention mechanism to further improve the
robustness of the model. The experimental results on CIFAR10 have shown that
our method is effective and better than the state-of-the-art methods under the
attack of FGSM and PGD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decomposing NeRF for Editing via Feature Field Distillation. (arXiv:2205.15585v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15585">
<div class="article-summary-box-inner">
<span><p>Emerging neural radiance fields (NeRF) are a promising scene representation
for computer graphics, enabling high-quality 3D reconstruction and novel view
synthesis from image observations. However, editing a scene represented by a
NeRF is challenging, as the underlying connectionist representations such as
MLPs or voxel grids are not object-centric or compositional. In particular, it
has been difficult to selectively edit specific regions or objects. In this
work, we tackle the problem of semantic scene decomposition of NeRFs to enable
query-based local editing of the represented 3D scenes. We propose to distill
the knowledge of off-the-shelf, self-supervised 2D image feature extractors
such as CLIP-LSeg or DINO into a 3D feature field optimized in parallel to the
radiance field. Given a user-specified query of various modalities such as
text, an image patch, or a point-and-click selection, 3D feature fields
semantically decompose 3D space without the need for re-training and enable us
to semantically select and edit regions in the radiance field. Our experiments
validate that the distilled feature fields (DFFs) can transfer recent progress
in 2D vision and language foundation models to 3D scene representations,
enabling convincing 3D segmentation and selective editing of emerging neural
graphics representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Novel View Synthesis for High-fidelity Headshot Scenes. (arXiv:2205.15595v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15595">
<div class="article-summary-box-inner">
<span><p>Rendering scenes with a high-quality human face from arbitrary viewpoints is
a practical and useful technique for many real-world applications. Recently,
Neural Radiance Fields (NeRF), a rendering technique that uses neural networks
to approximate classical ray tracing, have been considered as one of the
promising approaches for synthesizing novel views from a sparse set of images.
We find that NeRF can render new views while maintaining geometric consistency,
but it does not properly maintain skin details, such as moles and pores. These
details are important particularly for faces because when we look at an image
of a face, we are much more sensitive to details than when we look at other
objects. On the other hand, 3D Morpable Models (3DMMs) based on traditional
meshes and textures can perform well in terms of skin detail despite that it
has less precise geometry and cannot cover the head and the entire scene with
background. Based on these observations, we propose a method to use both NeRF
and 3DMM to synthesize a high-fidelity novel view of a scene with a face. Our
method learns a Generative Adversarial Network (GAN) to mix a NeRF-synthesized
image and a 3DMM-rendered image and produces a photorealistic scene with a face
preserving the skin details. Experiments with various real-world scenes
demonstrate the effectiveness of our approach. The code will be available on
https://github.com/showlab/headshot .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Aging of Brain Images with Diffeomorphic Registration. (arXiv:2205.15607v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15607">
<div class="article-summary-box-inner">
<span><p>Analyzing and predicting brain aging is essential for early prognosis and
accurate diagnosis of cognitive diseases. The technique of neuroimaging, such
as Magnetic Resonance Imaging (MRI), provides a noninvasive means of observing
the aging process within the brain. With longitudinal image data collection,
data-intensive Artificial Intelligence (AI) algorithms have been used to
examine brain aging. However, existing state-of-the-art algorithms tend to be
restricted to group-level predictions and suffer from unreal predictions. This
paper proposes a methodology for generating longitudinal MRI scans that capture
subject-specific neurodegeneration and retain anatomical plausibility in aging.
The proposed methodology is developed within the framework of diffeomorphic
registration and relies on three key novel technological advances to generate
subject-level anatomically plausible predictions: i) a computationally
efficient and individualized generative framework based on registration; ii) an
aging generative module based on biological linear aging progression; iii) a
quality control module to fit registration for generation task. Our methodology
was evaluated on 2662 T1-weighted (T1-w) MRI scans from 796 participants from
three different cohorts. First, we applied 6 commonly used criteria to
demonstrate the aging simulation ability of the proposed methodology; Secondly,
we evaluated the quality of the synthetic images using quantitative
measurements and qualitative assessment by a neuroradiologist. Overall, the
experimental results show that the proposed method can produce anatomically
plausible predictions that can be used to enhance longitudinal datasets, in
turn enabling data-hungry AI-driven healthcare tools.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-supervised Action Transition Learning for Stochastic Human Motion Prediction. (arXiv:2205.15608v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15608">
<div class="article-summary-box-inner">
<span><p>We introduce the task of action-driven stochastic human motion prediction,
which aims to predict multiple plausible future motions given a sequence of
action labels and a short motion history. This differs from existing works,
which predict motions that either do not respect any specific action category,
or follow a single action label. In particular, addressing this task requires
tackling two challenges: The transitions between the different actions must be
smooth; the length of the predicted motion depends on the action sequence and
varies significantly across samples. As we cannot realistically expect training
data to cover sufficiently diverse action transitions and motion lengths, we
propose an effective training strategy consisting of combining multiple motions
from different actions and introducing a weak form of supervision to encourage
smooth transitions. We then design a VAE-based model conditioned on both the
observed motion and the action label sequence, allowing us to generate multiple
plausible future motions of varying length. We illustrate the generality of our
approach by exploring its use with two different temporal encoding models,
namely RNNs and Transformers. Our approach outperforms baseline models
constructed by adapting state-of-the-art single action-conditioned motion
generation methods and stochastic human motion prediction approaches to our new
task of action-driven stochastic motion prediction. Our code is available at
https://github.com/wei-mao-2019/WAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bag of Tricks for Domain Adaptive Multi-Object Tracking. (arXiv:2205.15609v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15609">
<div class="article-summary-box-inner">
<span><p>In this paper, SIA_Track is presented which is developed by a research team
from SI Analytics. The proposed method was built from pre-existing detector and
tracker under the tracking-by-detection paradigm. The tracker we used is an
online tracker that merely links newly received detections with existing
tracks. The core part of our method is training procedure of the object
detector where synthetic and unlabeled real data were only used for training.
To maximize the performance on real data, we first propose to use
pseudo-labeling that generates imperfect labels for real data using a model
trained with synthetic dataset. After that model soups scheme was applied to
aggregate weights produced during iterative pseudo-labeling. Besides,
cross-domain mixed sampling also helped to increase detection performance on
real data. Our method, SIA_Track, takes the first place on MOTSynth2MOT17 track
at BMTT 2022 challenge. The code is available on
https://github.com/SIAnalytics/BMTT2022_SIA_track.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Centroid Supervision Alleviates Domain Shift in Medical Image Classification. (arXiv:2205.15658v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15658">
<div class="article-summary-box-inner">
<span><p>Deep learning based medical imaging classification models usually suffer from
the domain shift problem, where the classification performance drops when
training data and real-world data differ in imaging equipment manufacturer,
image acquisition protocol, patient populations, etc. We propose Feature
Centroid Contrast Learning (FCCL), which can improve target domain
classification performance by extra supervision during training with
contrastive loss between instance and class centroid. Compared with current
unsupervised domain adaptation and domain generalization methods, FCCL performs
better while only requires labeled image data from a single source domain and
no target domain. We verify through extensive experiments that FCCL can achieve
superior performance on at least three imaging modalities, i.e. fundus
photographs, dermatoscopic images, and H &amp; E tissue images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViT-BEVSeg: A Hierarchical Transformer Network for Monocular Birds-Eye-View Segmentation. (arXiv:2205.15667v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15667">
<div class="article-summary-box-inner">
<span><p>Generating a detailed near-field perceptual model of the environment is an
important and challenging problem in both self-driving vehicles and autonomous
mobile robotics. A Bird Eye View (BEV) map, providing a panoptic
representation, is a commonly used approach that provides a simplified 2D
representation of the vehicle surroundings with accurate semantic level
segmentation for many downstream tasks. Current state-of-the art approaches to
generate BEV-maps employ a Convolutional Neural Network (CNN) backbone to
create feature-maps which are passed through a spatial transformer to project
the derived features onto the BEV coordinate frame. In this paper, we evaluate
the use of vision transformers (ViT) as a backbone architecture to generate BEV
maps. Our network architecture, ViT-BEVSeg, employs standard vision
transformers to generate a multi-scale representation of the input image. The
resulting representation is then provided as an input to a spatial transformer
decoder module which outputs segmentation maps in the BEV grid. We evaluate our
approach on the nuScenes dataset demonstrating a considerable improvement in
the performance relative to state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmentation-Aware Self-Supervision for Data-Efficient GAN Training. (arXiv:2205.15677v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15677">
<div class="article-summary-box-inner">
<span><p>Training generative adversarial networks (GANs) with limited data is valuable
but challenging because discriminators are prone to over-fitting in such
situations. Recently proposed differentiable data augmentation techniques for
discriminators demonstrate improved data efficiency of training GANs. However,
the naive data augmentation introduces undesired invariance to augmentation
into the discriminator. The invariance may degrade the representation learning
ability of the discriminator, thereby affecting the generative modeling
performance of the generator. To mitigate the invariance while inheriting the
benefits of data augmentation, we propose a novel augmentation-aware
self-supervised discriminator that predicts the parameter of augmentation given
the augmented and original data. Moreover, the prediction task is required to
distinguishable between real data and generated data since they are different
during training. We further encourage the generator to learn from the proposed
discriminator by generating augmentation-predictable real data. We compare the
proposed method with state-of-the-arts across the class-conditional BigGAN and
unconditional StyleGAN2 architectures on CIFAR-10/100 and several low-shot
datasets, respectively. Experimental results show a significantly improved
generation performance of our method over competing methods for training
data-efficient GANs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Relation-aware Graph Network Proliferation. (arXiv:2205.15678v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15678">
<div class="article-summary-box-inner">
<span><p>Graph neural architecture search has sparked much attention as Graph Neural
Networks (GNNs) have shown powerful reasoning capability in many relational
tasks. However, the currently used graph search space overemphasizes learning
node features and neglects mining hierarchical relational information.
Moreover, due to diverse mechanisms in the message passing, the graph search
space is much larger than that of CNNs. This hinders the straightforward
application of classical search strategies for exploring complicated graph
search space. We propose Automatic Relation-aware Graph Network Proliferation
(ARGNP) for efficiently searching GNNs with a relation-guided message passing
mechanism. Specifically, we first devise a novel dual relation-aware graph
search space that comprises both node and relation learning operations. These
operations can extract hierarchical node/relational information and provide
anisotropic guidance for message passing on a graph. Second, analogous to cell
proliferation, we design a network proliferation search paradigm to
progressively determine the GNN architectures by iteratively performing network
division and differentiation. The experiments on six datasets for four graph
learning tasks demonstrate that GNNs produced by our method are superior to the
current state-of-the-art hand-crafted and search-based GNNs. Codes are
available at https://github.com/phython96/ARGNP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning for Building Damage Assessment from Large-scale xBD Satellite Imagery Benchmark Datasets. (arXiv:2205.15688v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15688">
<div class="article-summary-box-inner">
<span><p>In the field of post-disaster assessment, for timely and accurate rescue and
localization after a disaster, people need to know the location of damaged
buildings. In deep learning, some scholars have proposed methods to make
automatic and highly accurate building damage assessments by remote sensing
images, which are proved to be more efficient than assessment by domain
experts. However, due to the lack of a large amount of labeled data, these
kinds of tasks can suffer from being able to do an accurate assessment, as the
efficiency of deep learning models relies highly on labeled data. Although
existing semi-supervised and unsupervised studies have made breakthroughs in
this area, none of them has completely solved this problem. Therefore, we
propose adopting a self-supervised comparative learning approach to address the
task without the requirement of labeled data. We constructed a novel asymmetric
twin network architecture and tested its performance on the xBD dataset.
Experiment results of our model show the improvement compared to baseline and
commonly used methods. We also demonstrated the potential of self-supervised
methods for building damage recognition awareness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Multi-scale Consistent Network for Multi-class Fundus Lesion Segmentation. (arXiv:2205.15720v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15720">
<div class="article-summary-box-inner">
<span><p>Effectively integrating multi-scale information is of considerable
significance for the challenging multi-class segmentation of fundus lesions
because different lesions vary significantly in scales and shapes. Several
methods have been proposed to successfully handle the multi-scale object
segmentation. However, two issues are not considered in previous studies. The
first is the lack of interaction between adjacent feature levels, and this will
lead to the deviation of high-level features from low-level features and the
loss of detailed cues. The second is the conflict between the low-level and
high-level features, this occurs because they learn different scales of
features, thereby confusing the model and decreasing the accuracy of the final
prediction. In this paper, we propose a progressive multi-scale consistent
network (PMCNet) that integrates the proposed progressive feature fusion (PFF)
block and dynamic attention block (DAB) to address the aforementioned issues.
Specifically, PFF block progressively integrates multi-scale features from
adjacent encoding layers, facilitating feature learning of each layer by
aggregating fine-grained details and high-level semantics. As features at
different scales should be consistent, DAB is designed to dynamically learn the
attentive cues from the fused features at different scales, thus aiming to
smooth the essential conflicts existing in multi-scale features. The two
proposed PFF and DAB blocks can be integrated with the off-the-shelf backbone
networks to address the two issues of multi-scale and feature inconsistency in
the multi-class segmentation of fundus lesions, which will produce better
feature representation in the feature space. Experimental results on three
public datasets indicate that the proposed method is more effective than recent
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Loss for Quantization: Deep Hashing with Discrete Wasserstein Distributional Matching. (arXiv:2205.15721v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15721">
<div class="article-summary-box-inner">
<span><p>Image hashing is a principled approximate nearest neighbor approach to find
similar items to a query in a large collection of images. Hashing aims to learn
a binary-output function that maps an image to a binary vector. For optimal
retrieval performance, producing balanced hash codes with low-quantization
error to bridge the gap between the learning stage's continuous relaxation and
the inference stage's discrete quantization is important. However, in the
existing deep supervised hashing methods, coding balance and low-quantization
error are difficult to achieve and involve several losses. We argue that this
is because the existing quantization approaches in these methods are
heuristically constructed and not effective to achieve these objectives. This
paper considers an alternative approach to learning the quantization
constraints. The task of learning balanced codes with low quantization error is
re-formulated as matching the learned distribution of the continuous codes to a
pre-defined discrete, uniform distribution. This is equivalent to minimizing
the distance between two distributions. We then propose a computationally
efficient distributional distance by leveraging the discrete property of the
hash functions. This distributional distance is a valid distance and enjoys
lower time and sample complexities. The proposed single-loss quantization
objective can be integrated into any existing supervised hashing method to
improve code balance and quantization error. Experiments confirm that the
proposed approach substantially improves the performance of several
representative hashing~methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeVRF: Fast Deformable Voxel Radiance Fields for Dynamic Scenes. (arXiv:2205.15723v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15723">
<div class="article-summary-box-inner">
<span><p>Modeling dynamic scenes is important for many applications such as virtual
reality and telepresence. Despite achieving unprecedented fidelity for novel
view synthesis in dynamic scenes, existing methods based on Neural Radiance
Fields (NeRF) suffer from slow convergence (i.e., model training time measured
in days). In this paper, we present DeVRF, a novel representation to accelerate
learning dynamic radiance fields. The core of DeVRF is to model both the 3D
canonical space and 4D deformation field of a dynamic, non-rigid scene with
explicit and discrete voxel-based representations. However, it is quite
challenging to train such a representation which has a large number of model
parameters, often resulting in overfitting issues. To overcome this challenge,
we devise a novel static-to-dynamic learning paradigm together with a new data
capture setup that is convenient to deploy in practice. This paradigm unlocks
efficient learning of deformable radiance fields via utilizing the 3D
volumetric canonical space learnt from multi-view static images to ease the
learning of 4D voxel deformation field with only few-view dynamic sequences. To
further improve the efficiency of our DeVRF and its synthesized novel view's
quality, we conduct thorough explorations and identify a set of strategies. We
evaluate DeVRF on both synthetic and real-world dynamic scenes with different
types of deformation. Experiments demonstrate that DeVRF achieves two orders of
magnitude speedup (100x faster) with on-par high-fidelity results compared to
the previous state-of-the-art approaches. The code and dataset will be released
in https://github.com/showlab/DeVRF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers for Multi-Object Tracking on Point Clouds. (arXiv:2205.15730v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15730">
<div class="article-summary-box-inner">
<span><p>We present TransMOT, a novel transformer-based end-to-end trainable online
tracker and detector for point cloud data. The model utilizes a cross- and a
self-attention mechanism and is applicable to lidar data in an automotive
context, as well as other data types, such as radar. Both track management and
the detection of new tracks are performed by the same transformer decoder
module and the tracker state is encoded in feature space. With this approach,
we make use of the rich latent space of the detector for tracking rather than
relying on low-dimensional bounding boxes. Still, we are able to retain some of
the desirable properties of traditional Kalman-filter based approaches, such as
an ability to handle sensor input at arbitrary timesteps or to compensate frame
skips. This is possible due to a novel module that transforms the track
information from one frame to the next on feature-level and thereby fulfills a
similar task as the prediction step of a Kalman filter. Results are presented
on the challenging real-world dataset nuScenes, where the proposed model
outperforms its Kalman filter-based tracking baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Omni-Granular Ego-Semantic Propagation for Self-Supervised Graph Representation Learning. (arXiv:2205.15746v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15746">
<div class="article-summary-box-inner">
<span><p>Unsupervised/self-supervised graph representation learning is critical for
downstream node- and graph-level classification tasks. Global structure of
graphs helps discriminating representations and existing methods mainly utilize
the global structure by imposing additional supervisions. However, their global
semantics are usually invariant for all nodes/graphs and they fail to
explicitly embed the global semantics to enrich the representations. In this
paper, we propose Omni-Granular Ego-Semantic Propagation for Self-Supervised
Graph Representation Learning (OEPG). Specifically, we introduce
instance-adaptive global-aware ego-semantic descriptors, leveraging the first-
and second-order feature differences between each node/graph and hierarchical
global clusters of the entire graph dataset. The descriptors can be explicitly
integrated into local graph convolution as new neighbor nodes. Besides, we
design an omni-granular normalization on the whole scales and hierarchies of
the ego-semantic to assign attentional weight to each descriptor from an
omni-granular perspective. Specialized pretext tasks and cross-iteration
momentum update are further developed for local-global mutual adaptation. In
downstream tasks, OEPG consistently achieves the best performance with a 2%~6%
accuracy gain on multiple datasets cross scales and domains. Notably, OEPG also
generalizes to quantity- and topology-imbalance scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial synthesis based data-augmentation for code-switched spoken language identification. (arXiv:2205.15747v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15747">
<div class="article-summary-box-inner">
<span><p>Spoken Language Identification (LID) is an important sub-task of Automatic
Speech Recognition(ASR) that is used to classify the language(s) in an audio
segment. Automatic LID plays an useful role in multilingual countries. In
various countries, identifying a language becomes hard, due to the multilingual
scenario where two or more than two languages are mixed together during
conversation. Such phenomenon of speech is called as code-mixing or
code-switching. This nature is followed not only in India but also in many
Asian countries. Such code-mixed data is hard to find, which further reduces
the capabilities of the spoken LID. Due to the lack of avalibility of this
code-mixed data, it becomes a minority class in LID task. Hence, this work
primarily addresses this problem using data augmentation as a solution on the
minority code-switched class. This study focuses on Indic language code-mixed
with English. Spoken LID is performed on Hindi, code-mixed with English. This
research proposes Generative Adversarial Network (GAN) based data augmentation
technique performed using Mel spectrograms for audio data. GANs have already
been proven to be accurate in representing the real data distribution in the
image domain. Proposed research exploits these capabilities of GANs in speech
domains such as speech classification, automatic speech recognition,etc. GANs
are trained to generate Mel spectrograms of the minority code-mixed class which
are then used to augment data for the classifier. Utilizing GANs give an
overall improvement on Unweighted Average Recall by an amount of 3.5\% as
compared to a Convolutional Recurrent Neural Network (CRNN) classifier used as
the baseline reference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Iterative Recovery from Nonlinear Observations using Generative Models. (arXiv:2205.15749v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15749">
<div class="article-summary-box-inner">
<span><p>In this paper, we aim to estimate the direction of an underlying signal from
its nonlinear observations following the semi-parametric single index model
(SIM). Unlike conventional compressed sensing where the signal is assumed to be
sparse, we assume that the signal lies in the range of an $L$-Lipschitz
continuous generative model with bounded $k$-dimensional inputs. This is mainly
motivated by the tremendous success of deep generative models in various real
applications. Our reconstruction method is non-iterative (though approximating
the projection step may use an iterative procedure) and highly efficient, and
it is shown to attain the near-optimal statistical rate of order $\sqrt{(k \log
L)/m}$, where $m$ is the number of measurements. We consider two specific
instances of the SIM, namely noisy $1$-bit and cubic measurement models, and
perform experiments on image datasets to demonstrate the efficacy of our
method. In particular, for the noisy $1$-bit measurement model, we show that
our non-iterative method significantly outperforms a state-of-the-art iterative
method in terms of both accuracy and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Role of Image Retrieval for Visual Localization -- An exhaustive benchmark. (arXiv:2205.15761v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15761">
<div class="article-summary-box-inner">
<span><p>Visual localization, i.e., camera pose estimation in a known scene, is a core
component of technologies such as autonomous driving and augmented reality.
State-of-the-art localization approaches often rely on image retrieval
techniques for one of two purposes: (1) provide an approximate pose estimate or
(2) determine which parts of the scene are potentially visible in a given query
image. It is common practice to use state-of-the-art image retrieval algorithms
for both of them. These algorithms are often trained for the goal of retrieving
the same landmark under a large range of viewpoint changes which often differs
from the requirements of visual localization. In order to investigate the
consequences for visual localization, this paper focuses on understanding the
role of image retrieval for multiple visual localization paradigms. First, we
introduce a novel benchmark setup and compare state-of-the-art retrieval
representations on multiple datasets using localization performance as metric.
Second, we investigate several definitions of "ground truth" for image
retrieval. Using these definitions as upper bounds for the visual localization
paradigms, we show that there is still sgnificant room for improvement. Third,
using these tools and in-depth analysis, we show that retrieval performance on
classical landmark retrieval or place recognition tasks correlates only for
some but not all paradigms to localization performance. Finally, we analyze the
effects of blur and dynamic scenes in the images. We conclude that there is a
need for retrieval approaches specifically designed for localization paradigms.
Our benchmark and evaluation protocols are available at
https://github.com/naver/kapture-localization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SymFormer: End-to-end symbolic regression using transformer-based architecture. (arXiv:2205.15764v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15764">
<div class="article-summary-box-inner">
<span><p>Novel view synthesis is a long-standing problem. In this work, we consider a
variant of the problem where we are given only a few context views sparsely
covering a scene or an object. The goal is to predict novel viewpoints in the
scene, which requires learning priors. The current state of the art is based on
Neural Radiance Fields (NeRFs), and while achieving impressive results, the
methods suffer from long training times as they require evaluating thousands of
3D point samples via a deep neural network for each image. We propose a 2D-only
method that maps multiple context views and a query pose to a new image in a
single pass of a neural network. Our model uses a two-stage architecture
consisting of a codebook and a transformer model. The codebook is used to embed
individual images into a smaller latent space, and the transformer solves the
view synthesis task in this more compact space. To train our model efficiently,
we introduce a novel branching attention mechanism that allows us to use the
same model not only for neural rendering but also for camera pose estimation.
Experimental results on real-world scenes show that our approach is competitive
compared to NeRF-based methods while not reasoning in 3D, and it is faster to
train.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections. (arXiv:2205.15768v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15768">
<div class="article-summary-box-inner">
<span><p>Inverse rendering of an object under entirely unknown capture conditions is a
fundamental challenge in computer vision and graphics. Neural approaches such
as NeRF have achieved photorealistic results on novel view synthesis, but they
require known camera poses. Solving this problem with unknown camera poses is
highly challenging as it requires joint optimization over shape, radiance, and
pose. This problem is exacerbated when the input images are captured in the
wild with varying backgrounds and illuminations. Standard pose estimation
techniques fail in such image collections in the wild due to very few estimated
correspondences across images. Furthermore, NeRF cannot relight a scene under
any illumination, as it operates on radiance (the product of reflectance and
illumination). We propose a joint optimization framework to estimate the shape,
BRDF, and per-image camera pose and illumination. Our method works on
in-the-wild online image collections of an object and produces relightable 3D
assets for several use-cases such as AR/VR. To our knowledge, our method is the
first to tackle this severely unconstrained task with minimal user interaction.
Project page: https://markboss.me/publication/2022-samurai/ Video:
https://youtu.be/LlYuGDjXp-8
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Concept-level Debugging of Part-Prototype Networks. (arXiv:2205.15769v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15769">
<div class="article-summary-box-inner">
<span><p>Part-prototype Networks (ProtoPNets) are concept-based classifiers designed
to achieve the same performance as black-box models without compromising
transparency. ProtoPNets compute predictions based on similarity to
class-specific part-prototypes learned to recognize parts of training examples,
making it easy to faithfully determine what examples are responsible for any
target prediction and why. However, like other models, they are prone to
picking up confounds and shortcuts from the data, thus suffering from
compromised prediction accuracy and limited generalization. We propose
ProtoPDebug, an effective concept-level debugger for ProtoPNets in which a
human supervisor, guided by the model's explanations, supplies feedback in the
form of what part-prototypes must be forgotten or kept, and the model is
fine-tuned to align with this supervision. An extensive empirical evaluation on
synthetic and real-world data shows that ProtoPDebug outperforms
state-of-the-art debuggers for a fraction of the annotation cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The hybrid approach -- Convolutional Neural Networks and Expectation Maximization Algorithm -- for Tomographic Reconstruction of Hyperspectral Images. (arXiv:2205.15772v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15772">
<div class="article-summary-box-inner">
<span><p>We present a simple but novel hybrid approach to hyperspectral data cube
reconstruction from computed tomography imaging spectrometry (CTIS) images that
sequentially combines neural networks and the iterative Expectation
Maximization (EM) algorithm. We train and test the ability of the method to
reconstruct data cubes of $100\times100\times25$ and $100\times100\times100$
voxels, corresponding to 25 and 100 spectral channels, from simulated CTIS
images generated by our CTIS simulator. The hybrid approach utilizes the
inherent strength of the Convolutional Neural Network (CNN) with regard to
noise and its ability to yield consistent reconstructions and make use of the
EM algorithm's ability to generalize to spectral images of any object without
training. The hybrid approach achieves better performance than both the CNNs
and EM alone for seen (included in CNN training) and unseen (excluded from CNN
training) cubes for both the 25- and 100-channel cases. For the 25 spectral
channels, the improvements from CNN to the hybrid model (CNN + EM) in terms of
the mean-squared errors are between 14-26%. For 100 spectral channels, the
improvements between 19-40% are attained with the largest improvement of 40%
for the unseen data, to which the CNNs are not exposed during the training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Co-Training for Unsupervised Domain Adaptation of Semantic Segmentation Models. (arXiv:2205.15781v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15781">
<div class="article-summary-box-inner">
<span><p>Semantic image segmentation is addressed by training deep models. Since
supervised training draws to a curse of human-based image labeling, using
synthetic images with automatically generated ground truth together with
unlabeled real-world images is a promising alternative. This implies to address
an unsupervised domain adaptation (UDA) problem. In this paper, we proposed a
new co-training process for synth-to-real UDA of semantic segmentation models.
First, we design a self-training procedure which provides two initial models.
Then, we keep training these models in a collaborative manner for obtaining the
final model. The overall process treats the deep models as black boxes and
drives their collaboration at the level of pseudo-labeled target images, {\ie},
neither modifying loss functions is required, nor explicit feature alignment.
We test our proposal on standard synthetic and real-world datasets. Our
co-training shows improvements of 15-20 percentage points of mIoU over
baselines, so establishing new state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Deep Fake Detection for Trial Courts. (arXiv:2205.15792v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15792">
<div class="article-summary-box-inner">
<span><p>Recently, image manipulation has achieved rapid growth due to the advancement
of sophisticated image editing tools. A recent surge of generated fake imagery
and videos using neural networks is DeepFake. DeepFake algorithms can create
fake images and videos that humans cannot distinguish from authentic ones.
(GANs) have been extensively used for creating realistic images without
accessing the original images. Therefore, it is become essential to detect fake
videos to avoid spreading false information. This paper presents a survey of
methods used to detect DeepFakes and datasets available for detecting DeepFakes
in the literature to date. We present extensive discussions and research trends
related to DeepFake technologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrasting quadratic assignments for set-based representation learning. (arXiv:2205.15814v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15814">
<div class="article-summary-box-inner">
<span><p>The standard approach to contrastive learning is to maximize the agreement
between different views of the data. The views are ordered in pairs, such that
they are either positive, encoding different views of the same object, or
negative, corresponding to views of different objects. The supervisory signal
comes from maximizing the total similarity over positive pairs, while the
negative pairs are needed to avoid collapse. In this work, we note that the
approach of considering individual pairs cannot account for both intra-set and
inter-set similarities when the sets are formed from the views of the data. It
thus limits the information content of the supervisory signal available to
train representations. We propose to go beyond contrasting individual pairs of
objects by focusing on contrasting objects as sets. For this, we use
combinatorial quadratic assignment theory designed to evaluate set and graph
similarities and derive set-contrastive objective as a regularizer for
contrastive learning methods. We conduct experiments and demonstrate that our
method improves learned representations for the tasks of metric learning and
self-supervised classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Image Representation Learning with Deep Latent Particles. (arXiv:2205.15821v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15821">
<div class="article-summary-box-inner">
<span><p>We propose a new representation of visual data that disentangles object
position from appearance. Our method, termed Deep Latent Particles (DLP),
decomposes the visual input into low-dimensional latent ``particles'', where
each particle is described by its spatial location and features of its
surrounding region. To drive learning of such representations, we follow a
VAE-based approach and introduce a prior for particle positions based on a
spatial-softmax architecture, and a modification of the evidence lower bound
loss inspired by the Chamfer distance between particles. We demonstrate that
our DLP representations are useful for downstream tasks such as unsupervised
keypoint (KP) detection, image manipulation, and video prediction for scenes
composed of multiple dynamic objects. In addition, we show that our
probabilistic interpretation of the problem naturally provides uncertainty
estimates for particle locations, which can be used for model selection, among
other tasks. Videos and code are available:
https://taldatech.github.io/deep-latent-particles-web/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface Analysis with Vision Transformers. (arXiv:2205.15836v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15836">
<div class="article-summary-box-inner">
<span><p>The extension of convolutional neural networks (CNNs) to non-Euclidean
geometries has led to multiple frameworks for studying manifolds. Many of those
methods have shown design limitations resulting in poor modelling of long-range
associations, as the generalisation of convolutions to irregular surfaces is
non-trivial. Recent state-of-the-art performance of Vision Transformers (ViTs)
demonstrates that a general-purpose architecture, which implements
self-attention, could replace the local feature learning operations of CNNs.
Motivated by the success of attention-modelling in computer vision, we extend
ViTs to surfaces by reformulating the task of surface learning as a
sequence-to-sequence problem and propose a patching mechanism for surface
meshes. We validate the performance of the proposed Surface Vision Transformer
(SiT) on two brain age prediction tasks in the developing Human Connectome
Project (dHCP) dataset and investigate the impact of pre-training on model
performance. Experiments show that the SiT outperforms many surface CNNs, while
indicating some evidence of general transformation invariance. Code available
at https://github.com/metrics-lab/surface-vision-transformers
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D$^2$NeRF: Self-Supervised Decoupling of Dynamic and Static Objects from a Monocular Video. (arXiv:2205.15838v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15838">
<div class="article-summary-box-inner">
<span><p>Given a monocular video, segmenting and decoupling dynamic objects while
recovering the static environment is a widely studied problem in machine
intelligence. Existing solutions usually approach this problem in the image
domain, limiting their performance and understanding of the environment. We
introduce Decoupled Dynamic Neural Radiance Field (D$^2$NeRF), a
self-supervised approach that takes a monocular video and learns a 3D scene
representation which decouples moving objects, including their shadows, from
the static background. Our method represents the moving objects and the static
background by two separate neural radiance fields with only one allowing for
temporal changes. A naive implementation of this approach leads to the dynamic
component taking over the static one as the representation of the former is
inherently more general and prone to overfitting. To this end, we propose a
novel loss to promote correct separation of phenomena. We further propose a
shadow field network to detect and decouple dynamically moving shadows. We
introduce a new dataset containing various dynamic objects and shadows and
demonstrate that our method can achieve better performance than
state-of-the-art approaches in decoupling dynamic and static 3D objects,
occlusion and shadow removal, and image segmentation for moving objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geo-Neus: Geometry-Consistent Neural Implicit Surfaces Learning for Multi-view Reconstruction. (arXiv:2205.15848v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15848">
<div class="article-summary-box-inner">
<span><p>Recently, neural implicit surfaces learning by volume rendering has become
popular for multi-view reconstruction. However, one key challenge remains:
existing approaches lack explicit multi-view geometry constraints, hence
usually fail to generate geometry consistent surface reconstruction. To address
this challenge, we propose geometry-consistent neural implicit surfaces
learning for multi-view reconstruction. We theoretically analyze that there
exists a gap between the volume rendering integral and point-based signed
distance function (SDF) modeling. To bridge this gap, we directly locate the
zero-level set of SDF networks and explicitly perform multi-view geometry
optimization by leveraging the sparse geometry from structure from motion (SFM)
and photometric consistency in multi-view stereo. This makes our SDF
optimization unbiased and allows the multi-view geometry constraints to focus
on the true surface optimization. Extensive experiments show that our proposed
method achieves high-quality surface reconstruction in both complex thin
structures and large smooth regions, thus outperforming the state-of-the-arts
by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Snapture -- A Novel Neural Architecture for Combined Static and Dynamic Hand Gesture Recognition. (arXiv:2205.15862v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15862">
<div class="article-summary-box-inner">
<span><p>As robots are expected to get more involved in people's everyday lives,
frameworks that enable intuitive user interfaces are in demand. Hand gesture
recognition systems provide a natural way of communication and, thus, are an
integral part of seamless Human-Robot Interaction (HRI). Recent years have
witnessed an immense evolution of computational models powered by deep
learning. However, state-of-the-art models fall short in expanding across
different gesture domains, such as emblems and co-speech. In this paper, we
propose a novel hybrid hand gesture recognition system. Our architecture
enables learning both static and dynamic gestures: by capturing a so-called
"snapshot" of the gesture performance at its peak, we integrate the hand pose
along with the dynamic movement. Moreover, we present a method for analyzing
the motion profile of a gesture to uncover its dynamic characteristics and
which allows regulating a static channel based on the amount of motion. Our
evaluation demonstrates the superiority of our approach on two gesture
benchmarks compared to a CNNLSTM baseline. We also provide an analysis on a
gesture class basis that unveils the potential of our Snapture architecture for
performance improvements. Thanks to its modular implementation, our framework
allows the integration of other multimodal data like facial expressions and
head tracking, which are important cues in HRI scenarios, into one
architecture. Thus, our work contributes both to gesture recognition research
and machine learning applications for non-verbal communication with robots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Braille Letter Reading: A Benchmark for Spatio-Temporal Pattern Recognition on Neuromorphic Hardware. (arXiv:2205.15864v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15864">
<div class="article-summary-box-inner">
<span><p>Spatio-temporal pattern recognition is a fundamental ability of the brain
which is required for numerous real-world applications. Recent deep learning
approaches have reached outstanding accuracy in such tasks, but their
implementation on conventional embedded solutions is still very computationally
and energy expensive. Tactile sensing in robotic applications is a
representative example where real-time processing and energy-efficiency are
required. Following a brain-inspired computing approach, we propose a new
benchmark for spatio-temporal tactile pattern recognition at the edge through
braille letters reading. We recorded a new braille letters dataset based on the
capacitive tactile sensors/fingertip of the iCub robot, then we investigated
the importance of temporal information and the impact of event-based encoding
for spike-based/event-based computation. Afterwards, we trained and compared
feed-forward and recurrent spiking neural networks (SNNs) offline using
back-propagation through time with surrogate gradients, then we deployed them
on the Intel Loihi neuromorphic chip for fast and efficient inference. We
confronted our approach to standard classifiers, in particular to a Long
Short-Term Memory (LSTM) deployed on the embedded Nvidia Jetson GPU in terms of
classification accuracy, power/energy consumption and computational delay. Our
results show that the LSTM outperforms the recurrent SNN in terms of accuracy
by 14%. However, the recurrent SNN on Loihi is 237 times more energy-efficient
than the LSTM on Jetson, requiring an average power of only 31mW. This work
proposes a new benchmark for tactile sensing and highlights the challenges and
opportunities of event-based encoding, neuromorphic hardware and spike-based
computing for spatio-temporal pattern recognition at the edge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review of Mobile Mapping Systems: From Sensors to Applications. (arXiv:2205.15865v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15865">
<div class="article-summary-box-inner">
<span><p>The evolution of mobile mapping systems (MMSs) has gained more attention in
the past few decades. MMSs have been widely used to provide valuable assets in
different applications. This has been facilitated by the wide availability of
low-cost sensors, the advances in computational resources, the maturity of the
mapping algorithms, and the need for accurate and on-demand geographic
information system (GIS) data and digital maps. Many MMSs combine hybrid
sensors to provide a more informative, robust, and stable solution by
complementing each other. In this paper, we present a comprehensive review of
the modern MMSs by focusing on 1) the types of sensors and platforms, where we
discuss their capabilities, limitations, and also provide a comprehensive
overview of recent MMS technologies available in the market, 2) highlighting
the general workflow to process any MMS data, 3) identifying the different use
cases of mobile mapping technology by reviewing some of the common
applications, and 4) presenting a discussion on the benefits, challenges, and
share our views on the potential research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Median Pixel Difference Convolutional Network for Robust Face Recognition. (arXiv:2205.15867v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15867">
<div class="article-summary-box-inner">
<span><p>Face recognition is one of the most active tasks in computer vision and has
been widely used in the real world. With great advances made in convolutional
neural networks (CNN), lots of face recognition algorithms have achieved high
accuracy on various face datasets. However, existing face recognition
algorithms based on CNNs are vulnerable to noise. Noise corrupted image
patterns could lead to false activations, significantly decreasing face
recognition accuracy in noisy situations. To equip CNNs with built-in
robustness to noise of different levels, we proposed a Median Pixel Difference
Convolutional Network (MeDiNet) by replacing some traditional convolutional
layers with the proposed novel Median Pixel Difference Convolutional Layer
(MeDiConv) layer. The proposed MeDiNet integrates the idea of traditional
multiscale median filtering with deep CNNs. The MeDiNet is tested on the four
face datasets (LFW, CA-LFW, CP-LFW, and YTF) with versatile settings on blur
kernels, noise intensities, scales, and JPEG quality factors. Extensive
experiments show that our MeDiNet can effectively remove noisy pixels in the
feature map and suppress the negative impact of noise, leading to achieving
limited accuracy loss under these practical noises compared with the standard
CNN under clean conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers. (arXiv:2205.15868v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15868">
<div class="article-summary-box-inner">
<span><p>Large-scale pretrained transformers have created milestones in text (GPT-3)
and text-to-image (DALL-E and CogView) generation. Its application to video
generation is still facing many challenges: The potential huge computation cost
makes the training from scratch unaffordable; The scarcity and weak relevance
of text-video datasets hinder the model understanding complex movement
semantics. In this work, we present 9B-parameter transformer CogVideo, trained
by inheriting a pretrained text-to-image model, CogView2. We also propose
multi-frame-rate hierarchical training strategy to better align text and video
clips. As (probably) the first open-source large-scale pretrained text-to-video
model, CogVideo outperforms all publicly available models at a large margin in
machine and human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D-model ShapeNet Core Classification using Meta-Semantic Learning. (arXiv:2205.15869v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15869">
<div class="article-summary-box-inner">
<span><p>Understanding 3D point cloud models for learning purposes has become an
imperative challenge for real-world identification such as autonomous driving
systems. A wide variety of solutions using deep learning have been proposed for
point cloud segmentation, object detection, and classification. These methods,
however, often require a considerable number of model parameters and are
computationally expensive. We study a semantic dimension of given 3D data
points and propose an efficient method called Meta-Semantic Learning
(Meta-SeL). Meta-SeL is an integrated framework that leverages two input 3D
local points (input 3D models and part-segmentation labels), providing a time
and cost-efficient, and precise projection model for a number of 3D recognition
tasks. The results indicate that Meta-SeL yields competitive performance in
comparison with other complex state-of-the-art work. Moreover, being random
shuffle invariant, Meta-SeL is resilient to translation as well as jittering
noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FaIRCoP: Facial Image Retrieval using Contrastive Personalization. (arXiv:2205.15870v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15870">
<div class="article-summary-box-inner">
<span><p>Retrieving facial images from attributes plays a vital role in various
systems such as face recognition and suspect identification. Compared to other
image retrieval tasks, facial image retrieval is more challenging due to the
high subjectivity involved in describing a person's facial features. Existing
methods do so by comparing specific characteristics from the user's mental
image against the suggested images via high-level supervision such as using
natural language. In contrast, we propose a method that uses a relatively
simpler form of binary supervision by utilizing the user's feedback to label
images as either similar or dissimilar to the target image. Such supervision
enables us to exploit the contrastive learning paradigm for encapsulating each
user's personalized notion of similarity. For this, we propose a novel loss
function optimized online via user feedback. We validate the efficacy of our
proposed approach using a carefully designed testbed to simulate user feedback
and a large-scale user study. Our experiments demonstrate that our method
iteratively improves personalization, leading to faster convergence and
enhanced recommendation relevance, thereby, improving user satisfaction. Our
proposed framework is also equipped with a user-friendly web interface with a
real-time experience for facial image retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Keypoints to Object Landmarks via Self-Training Correspondence: A novel approach to Unsupervised Landmark Discovery. (arXiv:2205.15895v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15895">
<div class="article-summary-box-inner">
<span><p>This paper proposes a novel paradigm for the unsupervised learning of object
landmark detectors. Contrary to existing methods that build on auxiliary tasks
such as image generation or equivariance, we propose a self-training approach
where, departing from generic keypoints, a landmark detector and descriptor is
trained to improve itself, tuning the keypoints into distinctive landmarks. To
this end, we propose an iterative algorithm that alternates between producing
new pseudo-labels through feature clustering and learning distinctive features
for each pseudo-class through contrastive learning. With a shared backbone for
the landmark detector and descriptor, the keypoint locations progressively
converge to stable landmarks, filtering those less stable. Compared to previous
works, our approach can learn points that are more flexible in terms of
capturing large viewpoint changes. We validate our method on a variety of
difficult datasets, including LS3D, BBCPose, Human3.6M and PennAction,
achieving new state of the art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inferring 3D change detection from bitemporal optical images. (arXiv:2205.15903v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15903">
<div class="article-summary-box-inner">
<span><p>Change detection is one of the most active research areas in Remote Sensing
(RS). Most of the recently developed change detection methods are based on deep
learning (DL) algorithms. This kind of algorithms is generally focused on
generating two-dimensional (2D) change maps, thus only identifying planimetric
changes in land use/land cover (LULC) and not considering nor returning any
information on the corresponding elevation changes. Our work goes one step
further, proposing two novel networks, able to solve simultaneously the 2D and
3D CD tasks, and the 3DCD dataset, a novel and freely available dataset
precisely designed for this multitask. Particularly, the aim of this work is to
lay the foundations for the development of DL algorithms able to automatically
infer an elevation (3D) CD map -- together with a standard 2D CD map --,
starting only from a pair of bitemporal optical images. The proposed
architectures, to perform the task described before, consist of a
transformer-based network, the MultiTask Bitemporal Images Transformer (MTBIT),
and a deep convolutional network, the Siamese ResUNet (SUNet). Particularly,
MTBIT is a transformer-based architecture, based on a semantic tokenizer. SUNet
instead combines, in a siamese encoder, skip connections and residual layers to
learn rich features, capable to solve efficiently the proposed task. These
models are, thus, able to obtain 3D CD maps from two optical images taken at
different time instants, without the need to rely directly on elevation data
during the inference step. Encouraging results, obtained on the novel 3DCD
dataset, are shown. The code and the 3DCD dataset are available at
\url{https://sites.google.com/uniroma1.it/3dchangedetection/home-page}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAR Despeckling Using Overcomplete Convolutional Networks. (arXiv:2205.15906v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15906">
<div class="article-summary-box-inner">
<span><p>Synthetic Aperture Radar (SAR) despeckling is an important problem in remote
sensing as speckle degrades SAR images, affecting downstream tasks like
detection and segmentation. Recent studies show that convolutional neural
networks(CNNs) outperform classical despeckling methods. Traditional CNNs try
to increase the receptive field size as the network goes deeper, thus
extracting global features. However,speckle is relatively small, and increasing
receptive field does not help in extracting speckle features. This study
employs an overcomplete CNN architecture to focus on learning low-level
features by restricting the receptive field. The proposed network consists of
an overcomplete branch to focus on the local structures and an undercomplete
branch that focuses on the global structures. We show that the proposed network
improves despeckling performance compared to recent despeckling methods on
synthetic and real SAR images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Competitive Method for Dog Nose-print Re-identification. (arXiv:2205.15934v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15934">
<div class="article-summary-box-inner">
<span><p>Vision-based pattern identification (such as face, fingerprint, iris etc.)
has been successfully applied in human biometrics for a long history. However,
dog nose-print authentication is a challenging problem since the lack of a
large amount of labeled data. For that, this paper presents our proposed
methods for dog nose-print authentication (Re-ID) task in CVPR 2022 pet
biometric challenge. First, considering the problem that each class only with
few samples in the training set, we propose an automatic offline data
augmentation strategy. Then, for the difference in sample styles between the
training and test datasets, we employ joint cross-entropy, triplet and
pair-wise circle losses function for network optimization. Finally, with
multiple models ensembled adopted, our methods achieve 86.67\% AUC on the test
set. Codes are available at https://github.com/muzishen/Pet-ReID-IMAG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skeleton-based Action Recognition via Temporal-Channel Aggregation. (arXiv:2205.15936v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15936">
<div class="article-summary-box-inner">
<span><p>Skeleton-based action recognition methods are limited by the semantic
extraction of spatio-temporal skeletal maps. However, current methods have
difficulty in effectively combining features from both temporal and spatial
graph dimensions and tend to be thick on one side and thin on the other. In
this paper, we propose a Temporal-Channel Aggregation Graph Convolutional
Networks (TCA-GCN) to learn spatial and temporal topologies dynamically and
efficiently aggregate topological features in different temporal and channel
dimensions for skeleton-based action recognition. We use the Temporal
Aggregation module to learn temporal dimensional features and the Channel
Aggregation module to efficiently combine spatial dynamic topological features
learned using Channel-wise with temporal dynamic topological features. In
addition, we extract multi-scale skeletal features on temporal modeling and
fuse them with priori skeletal knowledge with an attention mechanism. Extensive
experiments show that our model results outperform state-of-the-art methods on
the NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Voxel Field Fusion for 3D Object Detection. (arXiv:2205.15938v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15938">
<div class="article-summary-box-inner">
<span><p>In this work, we present a conceptually simple yet effective framework for
cross-modality 3D object detection, named voxel field fusion. The proposed
approach aims to maintain cross-modality consistency by representing and fusing
augmented image features as a ray in the voxel field. To this end, the
learnable sampler is first designed to sample vital features from the image
plane that are projected to the voxel grid in a point-to-ray manner, which
maintains the consistency in feature representation with spatial context. In
addition, ray-wise fusion is conducted to fuse features with the supplemental
context in the constructed voxel field. We further develop mixed augmentor to
align feature-variant transformations, which bridges the modality gap in data
augmentation. The proposed framework is demonstrated to achieve consistent
gains in various benchmarks and outperforms previous fusion-based methods on
KITTI and nuScenes datasets. Code is made available at
https://github.com/dvlab-research/VFF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-efficient Segmentation of High-resolution Volumetric MicroCT Images. (arXiv:2205.15941v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15941">
<div class="article-summary-box-inner">
<span><p>In recent years, 3D convolutional neural networks have become the dominant
approach for volumetric medical image segmentation. However, compared to their
2D counterparts, 3D networks introduce substantially more training parameters
and higher requirement for the GPU memory. This has become a major limiting
factor for designing and training 3D networks for high-resolution volumetric
images. In this work, we propose a novel memory-efficient network architecture
for 3D high-resolution image segmentation. The network incorporates both global
and local features via a two-stage U-net-based cascaded framework and at the
first stage, a memory-efficient U-net (meU-net) is developed. The features
learnt at the two stages are connected via post-concatenation, which further
improves the information flow. The proposed segmentation method is evaluated on
an ultra high-resolution microCT dataset with typically 250 million voxels per
volume. Experiments show that it outperforms state-of-the-art 3D segmentation
methods in terms of both segmentation accuracy and memory efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-Dimensional Quantum Material Identification via Self-Attention and Soft-labeling in Deep Learning. (arXiv:2205.15948v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15948">
<div class="article-summary-box-inner">
<span><p>In quantum machine field, detecting two-dimensional (2D) materials in Silicon
chips is one of the most critical problems. Instance segmentation can be
considered as a potential approach to solve this problem. However, similar to
other deep learning methods, the instance segmentation requires a large scale
training dataset and high quality annotation in order to achieve a considerable
performance. In practice, preparing the training dataset is a challenge since
annotators have to deal with a large image, e.g 2K resolution, and extremely
dense objects in this problem. In this work, we present a novel method to
tackle the problem of missing annotation in instance segmentation in 2D quantum
material identification. We propose a new mechanism for automatically detecting
false negative objects and an attention based loss strategy to reduce the
negative impact of these objects contributing to the overall loss function. We
experiment on the 2D material detection datasets, and the experiments show our
method outperforms previous works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CropMix: Sampling a Rich Input Distribution via Multi-Scale Cropping. (arXiv:2205.15955v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15955">
<div class="article-summary-box-inner">
<span><p>We present a simple method, CropMix, for the purpose of producing a rich
input distribution from the original dataset distribution. Unlike single random
cropping, which may inadvertently capture only limited information, or
irrelevant information, like pure background, unrelated objects, etc, we crop
an image multiple times using distinct crop scales, thereby ensuring that
multi-scale information is captured. The new input distribution, serving as
training data, useful for a number of vision tasks, is then formed by simply
mixing multiple cropped views. We first demonstrate that CropMix can be
seamlessly applied to virtually any training recipe and neural network
architecture performing classification tasks. CropMix is shown to improve the
performance of image classifiers on several benchmark tasks across-the-board
without sacrificing computational simplicity and efficiency. Moreover, we show
that CropMix is of benefit to both contrastive learning and masked image
modeling towards more powerful representations, where preferable results are
achieved when learned representations are transferred to downstream tasks. Code
is available at GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedHarmony: Unlearning Scanner Bias with Distributed Data. (arXiv:2205.15970v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15970">
<div class="article-summary-box-inner">
<span><p>The ability to combine data across scanners and studies is vital for
neuroimaging, to increase both statistical power and the representation of
biological variability. However, combining datasets across sites leads to two
challenges: first, an increase in undesirable non-biological variance due to
scanner and acquisition differences - the harmonisation problem - and second,
data privacy concerns due to the inherently personal nature of medical imaging
data, meaning that sharing them across sites may risk violation of privacy
laws. To overcome these restrictions, we propose FedHarmony: a harmonisation
framework operating in the federated learning paradigm. We show that to remove
the scanner-specific effects, we only need to share the mean and standard
deviation of the learned features, helping to protect individual subjects'
privacy. We demonstrate our approach across a range of realistic data
scenarios, using real multi-site data from the ABIDE dataset, thus showing the
potential utility of our method for MRI harmonisation across studies. Our code
is available at https://github.com/nkdinsdale/FedHarmony.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text2Human: Text-Driven Controllable Human Image Generation. (arXiv:2205.15996v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15996">
<div class="article-summary-box-inner">
<span><p>Generating high-quality and diverse human images is an important yet
challenging task in vision and graphics. However, existing generative models
often fall short under the high diversity of clothing shapes and textures.
Furthermore, the generation process is even desired to be intuitively
controllable for layman users. In this work, we present a text-driven
controllable framework, Text2Human, for a high-quality and diverse human
generation. We synthesize full-body human images starting from a given human
pose with two dedicated steps. 1) With some texts describing the shapes of
clothes, the given human pose is first translated to a human parsing map. 2)
The final human image is then generated by providing the system with more
attributes about the textures of clothes. Specifically, to model the diversity
of clothing textures, we build a hierarchical texture-aware codebook that
stores multi-scale neural representations for each type of texture. The
codebook at the coarse level includes the structural representations of
textures, while the codebook at the fine level focuses on the details of
textures. To make use of the learned hierarchical codebook to synthesize
desired images, a diffusion-based transformer sampler with mixture of experts
is firstly employed to sample indices from the coarsest level of the codebook,
which then is used to predict the indices of the codebook at finer levels. The
predicted indices at different levels are translated to human images by the
decoder learned accompanied with hierarchical codebooks. The use of
mixture-of-experts allows for the generated image conditioned on the
fine-grained text input. The prediction for finer level indices refines the
quality of clothing textures. Extensive quantitative and qualitative
evaluations demonstrate that our proposed framework can generate more diverse
and realistic human images compared to state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving. (arXiv:2205.15997v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15997">
<div class="article-summary-box-inner">
<span><p>How should we integrate representations from complementary sensors for
autonomous driving? Geometry-based fusion has shown promise for perception
(e.g. object detection, motion forecasting). However, in the context of
end-to-end driving, we find that imitation learning based on existing sensor
fusion methods underperforms in complex driving scenarios with a high density
of dynamic agents. Therefore, we propose TransFuser, a mechanism to integrate
image and LiDAR representations using self-attention. Our approach uses
transformer modules at multiple resolutions to fuse perspective view and bird's
eye view feature maps. We experimentally validate its efficacy on a challenging
new benchmark with long routes and dense traffic, as well as the official
leaderboard of the CARLA urban driving simulator. At the time of submission,
TransFuser outperforms all prior work on the CARLA leaderboard in terms of
driving score by a large margin. Compared to geometry-based fusion, TransFuser
reduces the average collisions per kilometer by 48%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cascade Luminance and Chrominance for Image Retouching: More Like Artist. (arXiv:2205.15999v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15999">
<div class="article-summary-box-inner">
<span><p>Photo retouching aims to adjust the luminance, contrast, and saturation of
the image to make it more human aesthetically desirable. However, artists'
actions in photo retouching are difficult to quantitatively analyze. By
investigating their retouching behaviors, we propose a two-stage network that
brightens images first and then enriches them in the chrominance plane. Six
pieces of useful information from image EXIF are picked as the network's
condition input. Additionally, hue palette loss is added to make the image more
vibrant. Based on the above three aspects, Luminance-Chrominance Cascading
Net(LCCNet) makes the machine learning problem of mimicking artists in photo
retouching more reasonable. Experiments show that our method is effective on
the benchmark MIT-Adobe FiveK dataset, and achieves state-of-the-art
performance for both quantitative and qualitative evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Knowledge Gets Distilled in Knowledge Distillation?. (arXiv:2205.16004v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.16004">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation aims to transfer useful information from a teacher
network to a student network, with the primary goal of improving the student's
performance for the task at hand. Over the years, there has a been a deluge of
novel techniques and use cases of knowledge distillation. Yet, despite the
various improvements, there seems to be a glaring gap in the community's
fundamental understanding of the process. Specifically, what is the knowledge
that gets distilled in knowledge distillation? In other words, in what ways
does the student become similar to the teacher? Does it start to localize
objects in the same way? Does it get fooled by the same adversarial samples?
Does its data invariance properties become similar? Our work presents a
comprehensive study to try to answer these questions and more. Our results,
using image classification as a case study and three state-of-the-art knowledge
distillation techniques, show that knowledge distillation methods can indeed
indirectly distill other kinds of properties beyond improving task performance.
By exploring these questions, we hope for our work to provide a clearer picture
of what happens during knowledge distillation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Vector Quantized Diffusion Models. (arXiv:2205.16007v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.16007">
<div class="article-summary-box-inner">
<span><p>Vector quantized diffusion (VQ-Diffusion) is a powerful generative model for
text-to-image synthesis, but sometimes can still generate low-quality samples
or weakly correlated images with text input. We find these issues are mainly
due to the flawed sampling strategy. In this paper, we propose two important
techniques to further improve the sample quality of VQ-Diffusion. 1) We explore
classifier-free guidance sampling for discrete denoising diffusion model and
propose a more general and effective implementation of classifier-free
guidance. 2) We present a high-quality inference strategy to alleviate the
joint distribution issue in VQ-Diffusion. Finally, we conduct experiments on
various datasets to validate their effectiveness and show that the improved
VQ-Diffusion suppresses the vanilla version by large margins. We achieve an
8.44 FID score on MSCOCO, surpassing VQ-Diffusion by 5.42 FID score. When
trained on ImageNet, we dramatically improve the FID score from 11.89 to 4.83,
demonstrating the superiority of our proposed techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kymatio: Scattering Transforms in Python. (arXiv:1812.11214v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1812.11214">
<div class="article-summary-box-inner">
<span><p>The wavelet scattering transform is an invariant signal representation
suitable for many signal processing and machine learning applications. We
present the Kymatio software package, an easy-to-use, high-performance Python
implementation of the scattering transform in 1D, 2D, and 3D that is compatible
with modern deep learning frameworks. All transforms may be executed on a GPU
(in addition to CPU), offering a considerable speed up over CPU
implementations. The package also has a small memory footprint, resulting
inefficient memory usage. The source code, documentation, and examples are
available undera BSD license at https://www.kymat.io/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PDE-based Group Equivariant Convolutional Neural Networks. (arXiv:2001.09046v6 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.09046">
<div class="article-summary-box-inner">
<span><p>We present a PDE-based framework that generalizes Group equivariant
Convolutional Neural Networks (G-CNNs). In this framework, a network layer is
seen as a set of PDE-solvers where geometrically meaningful PDE-coefficients
become the layer's trainable weights. Formulating our PDEs on homogeneous
spaces allows these networks to be designed with built-in symmetries such as
rotation in addition to the standard translation equivariance of CNNs.
</p>
<p>Having all the desired symmetries included in the design obviates the need to
include them by means of costly techniques such as data augmentation. We will
discuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space
setting while also going into the specifics of our primary case of interest:
roto-translation equivariance.
</p>
<p>We solve the PDE of interest by a combination of linear group convolutions
and non-linear morphological group convolutions with analytic kernel
approximations that we underpin with formal theorems. Our kernel approximations
allow for fast GPU-implementation of the PDE-solvers, we release our
implementation with this article in the form of the LieTorch extension to
PyTorch, available at https://gitlab.com/bsmetsjr/lietorch . Just like for
linear convolution a morphological convolution is specified by a kernel that we
train in our PDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as
max/min-pooling and ReLUs as they are already subsumed by morphological
convolutions.
</p>
<p>We present a set of experiments to demonstrate the strength of the proposed
PDE-G-CNNs in increasing the performance of deep learning based imaging
applications with far fewer parameters than traditional CNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoRe: Color Regression for Multicolor Fashion Garments. (arXiv:2010.02849v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.02849">
<div class="article-summary-box-inner">
<span><p>Developing deep networks that analyze fashion garments has many real-world
applications. Among all fashion attributes, color is one of the most important
yet challenging to detect. Existing approaches are classification-based and
thus cannot go beyond the list of discrete predefined color names. In this
paper, we handle color detection as a regression problem to predict the exact
RGB values. That's why in addition to a first color classifier, we include a
second regression stage for refinement in our newly proposed architecture. This
second step combines two attention models: the first depends on the type of
clothing, the second depends on the color previously detected by the
classifier. Our final prediction is the weighted spatial pooling over the image
pixels RGB values, where the illumination has been corrected. This architecture
is modular and easily expanded to detect the RGBs of all colors in a multicolor
garment. In our experiments, we show the benefits of each component of our
architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Few-shot Semantic Segmentation. (arXiv:2010.05210v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.05210">
<div class="article-summary-box-inner">
<span><p>Training semantic segmentation models requires a large amount of finely
annotated data, making it hard to quickly adapt to novel classes not satisfying
this condition. Few-Shot Segmentation (FS-Seg) tackles this problem with many
constraints. In this paper, we introduce a new benchmark, called Generalized
Few-Shot Semantic Segmentation (GFS-Seg), to analyze the generalization ability
of simultaneously segmenting the novel categories with very few examples and
the base categories with sufficient examples. It is the first study showing
that previous representative state-of-the-art FS-Seg methods fall short in
GFS-Seg and the performance discrepancy mainly comes from the constrained
setting of FS-Seg. To make GFS-Seg tractable, we set up a GFS-Seg baseline that
achieves decent performance without structural change on the original model.
Then, since context is essential for semantic segmentation, we propose the
Context-Aware Prototype Learning (CAPL) that significantly improves performance
by 1) leveraging the co-occurrence prior knowledge from support samples, and 2)
dynamically enriching contextual information to the classifier, conditioned on
the content of each query image. Both two contributions are experimentally
shown to have substantial practical merit. Extensive experiments on Pascal-VOC
and COCO manifest the effectiveness of CAPL, and CAPL generalizes well to
FS-Seg by achieving competitive performance. Code is available at
https://github.com/dvlab-research/GFS-Seg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Outdoor Recognition Performance of Infrared Beacons for Infrastructure-based Localization. (arXiv:2104.09335v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09335">
<div class="article-summary-box-inner">
<span><p>This paper demonstrates a system comprised of infrared beacons and a camera
equipped with an optical band-pass filter. Our system can reliably detect and
identify individual beacons at 100m distance regardless of lighting conditions.
We describe the camera and beacon design as well as the image processing
pipeline in detail. In our experiments, we investigate and demonstrate the
ability of the system to recognize our beacons in both daytime and nighttime
conditions. High precision localization is a key enabler for automated vehicles
but remains unsolved, despite strong recent improvements. Our low-cost,
infrastructure-based approach is a potential step towards solving the
localization problem. All datasets are made available here
https://embedded.rwth-aachen.de/doku.php?id=forschung:mobility:infralocalization:concept.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simulated Adversarial Testing of Face Recognition Models. (arXiv:2106.04569v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04569">
<div class="article-summary-box-inner">
<span><p>Most machine learning models are validated and tested on fixed datasets. This
can give an incomplete picture of the capabilities and weaknesses of the model.
Such weaknesses can be revealed at test time in the real world. The risks
involved in such failures can be loss of profits, loss of time or even loss of
life in certain critical applications. In order to alleviate this issue,
simulators can be controlled in a fine-grained manner using interpretable
parameters to explore the semantic image manifold. In this work, we propose a
framework for learning how to test machine learning algorithms using simulators
in an adversarial manner in order to find weaknesses in the model before
deploying it in critical scenarios. We apply this method in a face recognition
setup. We show that certain weaknesses of models trained on real data can be
discovered using simulated samples. Using our proposed method, we can find
adversarial synthetic faces that fool contemporary face recognition models.
This demonstrates the fact that these models have weaknesses that are not
measured by commonly used validation datasets. We hypothesize that this type of
adversarial examples are not isolated, but usually lie in connected spaces in
the latent space of the simulator. We present a method to find these
adversarial regions as opposed to the typical adversarial points found in the
adversarial example literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the potential of sequential and non-sequential regression models for Sentinel-1-based biomass prediction in Tanzanian miombo forests. (arXiv:2106.15020v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15020">
<div class="article-summary-box-inner">
<span><p>This study derives regression models for above-ground biomass (AGB)
estimation in miombo woodlands of Tanzania that utilise the high availability
and low cost of Sentinel-1 data. The limited forest canopy penetration of
C-band SAR sensors along with the sparseness of available ground truth restrict
their usefulness in traditional AGB regression models. Therefore, we propose to
use AGB predictions based on airborne laser scanning (ALS) data as a surrogate
response variable for SAR data. This dramatically increases the available
training data and opens for flexible regression models that capture fine-scale
AGB dynamics. This becomes a sequential modelling approach, where the first
regression stage has linked in situ data to ALS data and produced the AGB
prediction map; We perform the subsequent stage, where this map is related to
Sentinel-1 data. We develop a traditional, parametric regression model and
alternative non-parametric models for this stage. The latter uses a conditional
generative adversarial network (cGAN) to translate Sentinel-1 images into
ALS-based AGB prediction maps. The convolution filters in the neural networks
make them contextual. We compare the sequential models to traditional,
non-sequential regression models, all trained on limited AGB ground reference
data. Results show that our newly proposed non-sequential Sentinel-1-based
regression model performs better quantitatively than the sequential models, but
achieves less sensitivity to fine-scale AGB dynamics. The contextual cGAN-based
sequential models best reproduce the distribution of ALS-based AGB predictions.
They also reach a lower RMSE against in situ AGB data than the parametric
sequential model, indicating a potential for further development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Creating synthetic night-time visible-light meteorological satellite images using the GAN method. (arXiv:2108.04330v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04330">
<div class="article-summary-box-inner">
<span><p>Meteorology satellite visible light images is critical for meteorology
support and forecast. However, there is no such kind of data during night time.
To overcome this, we propose a method based on deep learning to create
synthetic satellite visible light images during night. Specifically, to produce
more realistic products, we train a Generative Adversarial Networks (GAN) model
to generate visible light images given the corresponding satellite infrared
images and numerical weather prediction(NWP) products. To better model the
nonlinear relationship from infrared data and NWP products to visible light
images, we propose to use the channel-wise attention mechanics, e.g., SEBlock
to quantitative weight the input channels. The experiments based on the ECMWF
NWP products and FY-4A meteorology satellite visible light and infrared
channels date show that the proposed methods can be effective to create
realistic synthetic satellite visible light images during night.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistent Relative Confidence and Label-Free Model Selection for Convolutional Neural Networks. (arXiv:2108.11845v9 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11845">
<div class="article-summary-box-inner">
<span><p>In this paper, we are concerned with image classification with deep
convolutional neural networks (CNNs). We focus on the following question: given
a set of candidate CNN models, how to select the right one with the best
generalization property for the current task? Current model selection methods
all require access to a batch of labeled data for computing a pre-specified
performance metric, such as the cross-entropy loss, the classification error
rate and the negative log-likelihood. In many practical cases, labels are not
available in time as labeling itself is a time-consuming and expensive task. To
this end, we propose an approach to CNN model selection using only unlabeled
data. We develop this method based on a principle termed consistent relative
confidence. Experimental results on benchmark datasets demonstrate the
effectiveness and efficiency of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eyes Tell All: Irregular Pupil Shapes Reveal GAN-generated Faces. (arXiv:2109.00162v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00162">
<div class="article-summary-box-inner">
<span><p>Generative adversary network (GAN) generated high-realistic human faces have
been used as profile images for fake social media accounts and are visually
challenging to discern from real ones. In this work, we show that GAN-generated
faces can be exposed via irregular pupil shapes. This phenomenon is caused by
the lack of physiological constraints in the GAN models. We demonstrate that
such artifacts exist widely in high-quality GAN-generated faces and further
describe an automatic method to extract the pupils from two eyes and analysis
their shapes for exposing the GAN-generated faces. Qualitative and quantitative
evaluations of our method suggest its simplicity and effectiveness in
distinguishing GAN-generated faces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Ground Visual Objects for Visual Dialog. (arXiv:2109.06013v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06013">
<div class="article-summary-box-inner">
<span><p>Visual dialog is challenging since it needs to answer a series of coherent
questions based on understanding the visual environment. How to ground related
visual objects is one of the key problems. Previous studies utilize the
question and history to attend to the image and achieve satisfactory
performance, however these methods are not sufficient to locate related visual
objects without any guidance. The inappropriate grounding of visual objects
prohibits the performance of visual dialog models. In this paper, we propose a
novel approach to Learn to Ground visual objects for visual dialog, which
employs a novel visual objects grounding mechanism where both prior and
posterior distributions over visual objects are used to facilitate visual
objects grounding. Specifically, a posterior distribution over visual objects
is inferred from both context (history and questions) and answers, and it
ensures the appropriate grounding of visual objects during the training
process. Meanwhile, a prior distribution, which is inferred from context only,
is used to approximate the posterior distribution so that appropriate visual
objects can be grounded even without answers during the inference process.
Experimental results on the VisDial v0.9 and v1.0 datasets demonstrate that our
approach improves the previous strong models in both generative and
discriminative settings by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Visual Navigation under Partial Observability. (arXiv:2109.07752v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07752">
<div class="article-summary-box-inner">
<span><p>How can a robot navigate successfully in rich and diverse environments,
indoors or outdoors, along office corridors or trails on the grassland, on the
flat ground or the staircase? To this end, this work aims to address three
challenges: (i) complex visual observations, (ii) partial observability of
local visual sensing, and (iii) multimodal robot behaviors conditioned on both
the local environment and the global navigation objective. We propose to train
a neural network (NN) controller for local navigation via imitation learning.
To tackle complex visual observations, we extract multi-scale spatial
representations through CNNs. To tackle partial observability, we aggregate
multi-scale spatial information over time and encode it in LSTMs. To learn
multimodal behaviors, we use a separate memory module for each behavior mode.
Importantly, we integrate the multiple neural network modules into a unified
controller that achieves robust performance for visual navigation in complex,
partially observable environments. We implemented the controller on the
quadrupedal Spot robot and evaluated it on three challenging tasks: adversarial
pedestrian avoidance, blind-spot obstacle avoidance, and elevator riding. The
experiments show that the proposed NN architecture significantly improves
navigation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GoG: Relation-aware Graph-over-Graph Network for Visual Dialog. (arXiv:2109.08475v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08475">
<div class="article-summary-box-inner">
<span><p>Visual dialog, which aims to hold a meaningful conversation with humans about
a given image, is a challenging task that requires models to reason the complex
dependencies among visual content, dialog history, and current questions. Graph
neural networks are recently applied to model the implicit relations between
objects in an image or dialog. However, they neglect the importance of 1)
coreference relations among dialog history and dependency relations between
words for the question representation; and 2) the representation of the image
based on the fully represented question. Therefore, we propose a novel
relation-aware graph-over-graph network (GoG) for visual dialog. Specifically,
GoG consists of three sequential graphs: 1) H-Graph, which aims to capture
coreference relations among dialog history; 2) History-aware Q-Graph, which
aims to fully understand the question through capturing dependency relations
between words based on coreference resolution on the dialog history; and 3)
Question-aware I-Graph, which aims to capture the relations between objects in
an image based on fully question representation. As an additional feature
representation module, we add GoG to the existing visual dialogue model.
Experimental results show that our model outperforms the strong baseline in
both generative and discriminative settings by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KD-VLP: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation. (arXiv:2109.10504v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10504">
<div class="article-summary-box-inner">
<span><p>Self-supervised vision-and-language pretraining (VLP) aims to learn
transferable multi-modal representations from large-scale image-text data and
to achieve strong performances on a broad scope of vision-language tasks after
finetuning. Previous mainstream VLP approaches typically adopt a two-step
strategy relying on external object detectors to encode images in a multi-modal
Transformer framework, which suffer from restrictive object concept space,
limited image context and inefficient computation. In this paper, we propose an
object-aware end-to-end VLP framework, which directly feeds image grid features
from CNNs into the Transformer and learns the multi-modal representations
jointly. More importantly, we propose to perform object knowledge distillation
to facilitate learning cross-modal alignment at different semantic levels. To
achieve that, we design two novel pretext tasks by taking object features and
their semantic labels from external detectors as supervision: 1.) Object-guided
masked vision modeling task focuses on enforcing object-aware representation
learning in the multi-modal Transformer; 2.) Phrase-region alignment task aims
to improve cross-modal alignment by utilizing the similarities between noun
phrases and object labels in the linguistic space. Extensive experiments on a
wide range of vision-language tasks demonstrate the efficacy of our proposed
framework, and we achieve competitive or superior performances over the
existing pretraining strategies. The code is available in supplementary
materials.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RWN: Robust Watermarking Network for Image Cropping Localization. (arXiv:2110.05687v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05687">
<div class="article-summary-box-inner">
<span><p>Image cropping can be maliciously used to manipulate the layout of an image
and alter the underlying meaning. Previous image crop detection schemes only
predicts whether an image has been cropped, ignoring which part of the image is
cropped. This paper presents a novel robust watermarking network (RWN) for
image crop localization. We train an anti-crop processor (ACP) that embeds a
watermark into a target image. The visually indistinguishable protected image
is then posted on the social network instead of the original image. At the
recipient's side, ACP extracts the watermark from the attacked image, and we
conduct feature matching on the original and extracted watermark to locate the
position of the crop in the original image plane. We further extend our scheme
to detect tampering attack on the attacked image. Besides, we explore a simple
yet efficient method (JPEG-Mixup) to improve the generalization of JPEG
robustness. According to our comprehensive experiments, RWN is the first to
provide high-accuracy and robust image crop localization. Besides, the accuracy
of tamper detection is comparable with many state-of-the-art passive-based
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation. (arXiv:2110.08733v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08733">
<div class="article-summary-box-inner">
<span><p>Deep learning approaches have shown promising results in remote sensing high
spatial resolution (HSR) land-cover mapping. However, urban and rural scenes
can show completely different geographical landscapes, and the inadequate
generalizability of these algorithms hinders city-level or national-level
mapping. Most of the existing HSR land-cover datasets mainly promote the
research of learning semantic representation, thereby ignoring the model
transferability. In this paper, we introduce the Land-cOVEr Domain Adaptive
semantic segmentation (LoveDA) dataset to advance semantic and transferable
learning. The LoveDA dataset contains 5987 HSR images with 166768 annotated
objects from three different cities. Compared to the existing datasets, the
LoveDA dataset encompasses two domains (urban and rural), which brings
considerable challenges due to the: 1) multi-scale objects; 2) complex
background samples; and 3) inconsistent class distributions. The LoveDA dataset
is suitable for both land-cover semantic segmentation and unsupervised domain
adaptation (UDA) tasks. Accordingly, we benchmarked the LoveDA dataset on
eleven semantic segmentation methods and eight UDA methods. Some exploratory
studies including multi-scale architectures and strategies, additional
background supervision, and pseudo-label analysis were also carried out to
address these challenges. The code and data are available at
https://github.com/Junjue-Wang/LoveDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Prediction with Attentive Feature Aggregation. (arXiv:2111.00770v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00770">
<div class="article-summary-box-inner">
<span><p>Aggregating information from features across different layers is an essential
operation for dense prediction models. Despite its limited expressiveness,
feature concatenation dominates the choice of aggregation operations. In this
paper, we introduce Attentive Feature Aggregation (AFA) to fuse different
network layers with more expressive non-linear operations. AFA exploits both
spatial and channel attention to compute weighted average of the layer
activations. Inspired by neural volume rendering, we extend AFA with
Scale-Space Rendering (SSR) to perform late fusion of multi-scale predictions.
AFA is applicable to a wide range of existing network designs. Our experiments
show consistent and significant improvements on challenging semantic
segmentation benchmarks, including Cityscapes, BDD100K, and Mapillary Vistas,
at negligible computational and parameter overhead. In particular, AFA improves
the performance of the Deep Layer Aggregation (DLA) model by nearly 6% mIoU on
Cityscapes. Our experimental analyses show that AFA learns to progressively
refine segmentation maps and to improve boundary details, leading to new
state-of-the-art results on boundary detection benchmarks on BSDS500 and
NYUDv2. Code and video resources are available at <a href="http://vis.xyz/pub/dla-afa.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Representational Alignment with Human Perception Using Identically Represented Inputs. (arXiv:2111.14726v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14726">
<div class="article-summary-box-inner">
<span><p>We contribute to the study of the quality of learned representations. In many
domains, an important evaluation criterion for safe and trustworthy deep
learning is how well the invariances captured by representations of deep neural
networks (DNNs) are shared with humans. We identify challenges in measuring
these invariances. Prior works used gradient-based methods to generate
\textit{identically represented inputs} (IRIs), \ie, inputs which have similar
representations (on a given layer) of a neural network. If these IRIs look
`similar' to humans then a neural network's learned invariances are said to
align with human perception. However, we show that prior studies on the
alignment of invariances between DNNs and humans are `biased' by the specific
loss function used to generate IRIs. We show how different loss functions can
lead to different takeaways about a model's shared invariances with humans. We
show that under an \textit{adversarial} IRI~generation process all models
appear to have very little shared invariance with humans. We conduct an
in-depth investigation of how different components of the deep learning
pipeline contribute to learning models that have good alignment with human's
invariances. We find that architectures with residual connections trained using
a self-supervised contrastive loss with $\ell_p$ ball adversarial data
augmentation tend to learn the most human-like invariances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiview Transformers for Video Recognition. (arXiv:2201.04288v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04288">
<div class="article-summary-box-inner">
<span><p>Video understanding requires reasoning at multiple spatiotemporal resolutions
-- from short fine-grained motions to events taking place over longer
durations. Although transformer architectures have recently advanced the
state-of-the-art, they have not explicitly modelled different spatiotemporal
resolutions. To this end, we present Multiview Transformers for Video
Recognition (MTV). Our model consists of separate encoders to represent
different views of the input video with lateral connections to fuse information
across views. We present thorough ablation studies of our model and show that
MTV consistently performs better than single-view counterparts in terms of
accuracy and computational cost across a range of model sizes. Furthermore, we
achieve state-of-the-art results on six standard datasets, and improve even
further with large-scale pretraining. Code and checkpoints are available at:
https://github.com/google-research/scenic/tree/main/scenic/projects/mtv.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DICP: Doppler Iterative Closest Point Algorithm. (arXiv:2201.11944v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11944">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel algorithm for point cloud registration for
range sensors capable of measuring per-return instantaneous radial velocity:
Doppler ICP. Existing variants of ICP that solely rely on geometry or other
features generally fail to estimate the motion of the sensor correctly in
scenarios that have non-distinctive features and/or repetitive geometric
structures such as hallways, tunnels, highways, and bridges. We propose a new
Doppler velocity objective function that exploits the compatibility of each
point's Doppler measurement and the sensor's current motion estimate. We
jointly optimize the Doppler velocity objective function and the geometric
objective function which sufficiently constrains the point cloud alignment
problem even in feature-denied environments. Furthermore, the correspondence
matches used for the alignment are improved by pruning away the points from
dynamic targets which generally degrade the ICP solution. We evaluate our
method on data collected from real sensors and from simulation. Our results
show that with the added Doppler velocity residual terms, our method achieves a
significant improvement in registration accuracy along with faster convergence,
on average, when compared to classical point-to-plane ICP that solely relies on
geometric residuals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MVP-Net: Multiple View Pointwise Semantic Segmentation of Large-Scale Point Clouds. (arXiv:2201.12769v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12769">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation of 3D point cloud is an essential task for autonomous
driving environment perception. The pipeline of most pointwise point cloud
semantic segmentation methods includes points sampling, neighbor searching,
feature aggregation, and classification. Neighbor searching method like
K-nearest neighbors algorithm, KNN, has been widely applied. However, the
complexity of KNN is always a bottleneck of efficiency. In this paper, we
propose an end-to-end neural architecture, Multiple View Pointwise Net,
MVP-Net, to efficiently and directly infer large-scale outdoor point cloud
without KNN or any complex pre/postprocessing. Instead, assumption-based space
filling curves and multi-rotation of point cloud methods are introduced to
point feature aggregation and receptive field expanding. Numerical experiments
show that the proposed MVP-Net is 11 times faster than the most efficient
pointwise semantic segmentation method RandLA-Net and achieves the same
accuracy on the large-scale benchmark SemanticKITTI dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A General Deep Learning framework for Neuron Instance Segmentation based on Efficient UNet and Morphological Post-processing. (arXiv:2202.08682v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08682">
<div class="article-summary-box-inner">
<span><p>Recent studies have demonstrated the superiority of deep learning in medical
image analysis, especially in cell instance segmentation, a fundamental step
for many biological studies. However, the excellent performance of the neural
networks requires training on large, unbiased dataset and annotations, which is
labor-intensive and expertise-demanding. This paper presents an end-to-end
framework to automatically detect and segment NeuN stained neuronal cells on
histological images using only point annotations. Unlike traditional nuclei
segmentation with point annotation, we propose using point annotation and
binary segmentation to synthesize pixel-level annotations. The synthetic masks
are used as the ground truth to train the neural network, a U-Net-like
architecture with a state-of-the-art network, EfficientNet, as the encoder.
Validation results show the superiority of our model compared to other recent
methods. In addition, we investigated multiple post-processing schemes and
proposed an original strategy to convert the probability map into segmented
instances using ultimate erosion and dynamic reconstruction. This approach is
easy to configure and outperforms other classical post-processing techniques.
This work aims to develop a robust and efficient framework for analyzing
neurons using optical microscopic data, which can be used in preclinical
biological studies and, more specifically, in the context of neurodegenerative
diseases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Pruning is All You Need for Pruning CNNs at Initialization. (arXiv:2203.02549v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02549">
<div class="article-summary-box-inner">
<span><p>Pruning is a popular technique for reducing the model size and computational
cost of convolutional neural networks (CNNs). However, a slow retraining or
fine-tuning procedure is often required to recover the accuracy loss caused by
pruning. Recently, a new research direction on weight pruning,
pruning-at-initialization (PAI), is proposed to directly prune CNNs before
training so that fine-tuning or retraining can be avoided. While PAI has shown
promising results in reducing the model size, existing approaches rely on
fine-grained weight pruning which requires unstructured sparse matrix
computation, making it difficult to achieve real speedup in practice unless the
sparsity is very high. This work is the first to show that fine-grained weight
pruning is in fact not necessary for PAI. Instead, the layerwise compression
ratio is the main critical factor to determine the accuracy of a CNN model
pruned at initialization. Based on this key observation, we propose
PreCropping, a structured hardware-efficient model compression scheme.
PreCropping directly compresses the model at the channel level following the
layerwise compression ratio. Compared to weight pruning, the proposed scheme is
regular and dense in both storage and computation without sacrificing accuracy.
In addition, since PreCropping compresses CNNs at initialization, the
computational and memory costs of CNNs are reduced for both training and
inference on commodity hardware. We empirically demonstrate our approaches on
several modern CNN architectures, including ResNet, ShuffleNet, and MobileNet
for both CIFAR-10 and ImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DrawingInStyles: Portrait Image Generation and Editing with Spatially Conditioned StyleGAN. (arXiv:2203.02762v3 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02762">
<div class="article-summary-box-inner">
<span><p>The research topic of sketch-to-portrait generation has witnessed a boost of
progress with deep learning techniques. The recently proposed StyleGAN
architectures achieve state-of-the-art generation ability but the original
StyleGAN is not friendly for sketch-based creation due to its unconditional
generation nature. To address this issue, we propose a direct conditioning
strategy to better preserve the spatial information under the StyleGAN
framework. Specifically, we introduce Spatially Conditioned StyleGAN
(SC-StyleGAN for short), which explicitly injects spatial constraints to the
original StyleGAN generation process. We explore two input modalities, sketches
and semantic maps, which together allow users to express desired generation
results more precisely and easily. Based on SC-StyleGAN, we present
DrawingInStyles, a novel drawing interface for non-professional users to easily
produce high-quality, photo-realistic face images with precise control, either
from scratch or editing existing ones. Qualitative and quantitative evaluations
show the superior generation ability of our method to existing and alternative
solutions. The usability and expressiveness of our system are confirmed by a
user study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model-Agnostic Multitask Fine-tuning for Few-shot Vision-Language Transfer Learning. (arXiv:2203.04904v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04904">
<div class="article-summary-box-inner">
<span><p>Despite achieving state-of-the-art zero-shot performance, existing
vision-language models, e.g., CLIP, still fall short of domain-specific
classification tasks, e.g., Fungi Classification. In the context of few-shot
transfer learning, traditional fine-tuning fails to prevent highly expressive
model from exploiting spurious correlations in the training data. On the other
hand, although model-agnostic meta-learning (MAML) presents as a natural
alternative for transfer learning, the expensive computation due to implicit
second-order optimization limits its use in large-scale models and datasets. In
this work we aim to further improve the generalization of existing
vision-language models on unseen tasks via a simple yet efficient fine-tuning
strategy based on uniform task sampling. We term our method as Model-Agnostic
Multitask Fine-tuning (MAMF). Compared with MAML, MAMF discards the bi-level
optimization and uses only first-order gradients, which makes it easily
scalable and computationally efficient. Due to the uniform task sampling
procedure, MAMF consistently outperforms the classical fine-tuning method for
few-shot transfer learning on five benchmark datasets. Empirically, we further
discover that the effectiveness of first-order MAML is highly dependent on the
zero-shot performance of the pretrained model, and our simple algorithm can
outperform first-order MAML on more challenging datasets with low zero-shot
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Semantic Segmentation with Error Localization Network. (arXiv:2204.02078v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02078">
<div class="article-summary-box-inner">
<span><p>This paper studies semi-supervised learning of semantic segmentation, which
assumes that only a small portion of training images are labeled and the others
remain unlabeled. The unlabeled images are usually assigned pseudo labels to be
used in training, which however often causes the risk of performance
degradation due to the confirmation bias towards errors on the pseudo labels.
We present a novel method that resolves this chronic issue of pseudo labeling.
At the heart of our method lies error localization network (ELN), an auxiliary
module that takes an image and its segmentation prediction as input and
identifies pixels whose pseudo labels are likely to be wrong. ELN enables
semi-supervised learning to be robust against inaccurate pseudo labels by
disregarding label noises during training and can be naturally integrated with
self-training and contrastive learning. Moreover, we introduce a new learning
strategy for ELN that simulates plausible and diverse segmentation errors
during training of ELN to enhance its generalization. Our method is evaluated
on PASCAL VOC 2012 and Cityscapes, where it outperforms all existing methods in
every evaluation setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAIPI in Practice: Towards Explainable Interactive Medical Image Classification. (arXiv:2204.02661v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02661">
<div class="article-summary-box-inner">
<span><p>Would you trust physicians if they cannot explain their decisions to you?
Medical diagnostics using machine learning gained enormously in importance
within the last decade. However, without further enhancements many
state-of-the-art machine learning methods are not suitable for medical
application. The most important reasons are insufficient data set quality and
the black-box behavior of machine learning algorithms such as Deep Learning
models. Consequently, end-users cannot correct the model's decisions and the
corresponding explanations. The latter is crucial for the trustworthiness of
machine learning in the medical domain. The research field explainable
interactive machine learning searches for methods that address both
shortcomings. This paper extends the explainable and interactive CAIPI
algorithm and provides an interface to simplify human-in-the-loop approaches
for image classification. The interface enables the end-user (1) to investigate
and (2) to correct the model's prediction and explanation, and (3) to influence
the data set quality. After CAIPI optimization with only a single
counterexample per iteration, the model achieves an accuracy of $97.48\%$ on
the Medical MNIST and $95.02\%$ on the Fashion MNIST. This accuracy is
approximately equal to state-of-the-art Deep Learning optimization procedures.
Besides, CAIPI reduces the labeling effort by approximately $80\%$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Permutation-Invariant Relational Network for Multi-person 3D Pose Estimation. (arXiv:2204.04913v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04913">
<div class="article-summary-box-inner">
<span><p>The recovery of multi-person 3D poses from a single RGB image is a severely
ill-conditioned problem due to the inherent 2D-3D depth ambiguity, inter-person
occlusions, and body truncations. To tackle these issues, recent works have
shown promising results by simultaneously reasoning for different people.
However, in most cases this is done by only considering pairwise person
interactions, hindering thus a holistic scene representation able to capture
long-range interactions. This is addressed by approaches that jointly process
all people in the scene, although they require defining one of the individuals
as a reference and a pre-defined person ordering, being sensitive to this
choice. In this paper, we overcome both these limitations, and we propose an
approach for multi-person 3D pose estimation that captures long-range
interactions independently of the input order. For this purpose, we build a
residual-like permutation-invariant network that successfully refines
potentially corrupted initial 3D poses estimated by an off-the-shelf detector.
The residual function is learned via Set Transformer blocks, that model the
interactions among all initial poses, no matter their ordering or number. A
thorough evaluation demonstrates that our approach is able to boost the
performance of the initially estimated 3D poses by large margins, achieving
state-of-the-art results on standardized benchmarks. Additionally, the proposed
module works in a computationally efficient manner and can be potentially used
as a drop-in complement for any 3D pose detector in multi-people scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiEarth 2022 -- Multimodal Learning for Earth and Environment Workshop and Challenge. (arXiv:2204.07649v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07649">
<div class="article-summary-box-inner">
<span><p>The Multimodal Learning for Earth and Environment Challenge (MultiEarth 2022)
will be the first competition aimed at the monitoring and analysis of
deforestation in the Amazon rainforest at any time and in any weather
conditions. The goal of the Challenge is to provide a common benchmark for
multimodal information processing and to bring together the earth and
environmental science communities as well as multimodal representation learning
communities to compare the relative merits of the various multimodal learning
methods to deforestation estimation under well-defined and strictly comparable
conditions. MultiEarth 2022 will have three sub-challenges: 1) matrix
completion, 2) deforestation estimation, and 3) image-to-image translation.
This paper presents the challenge guidelines, datasets, and evaluation metrics
for the three sub-challenges. Our challenge website is available at
https://sites.google.com/view/rainforest-challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weighted Bayesian Gaussian Mixture Model for Roadside LiDAR Object Detection. (arXiv:2204.09804v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09804">
<div class="article-summary-box-inner">
<span><p>Background modeling is widely used for intelligent surveillance systems to
detect the moving targets by subtracting the static background components. Most
roadside LiDAR object detection methods filter out foreground points by
comparing new data points to pre-trained background references based on
descriptive statistics over many frames (e.g., voxel density, slopes, maximum
distance). These solutions are not efficient under heavy traffic, and parameter
values are hard to transfer from one scenario to another. In early studies, the
probabilistic background modeling methods widely used for the video-based
system were considered not suitable for roadside LiDAR surveillance systems due
to the sparse and unstructured point clouds data. In this paper, the raw LiDAR
data were transformed into a structured representation based on the elevation
and azimuth value of each LiDAR point. With this high-order tensor
representation, we break the barrier to allow the efficient Gaussian Mixture
Model (GMM) method for roadside LiDAR background modeling. The Bayesian
NonParametric (BNP) approach integrates the intensity value and 3D measurements
to fully exploit the measurement data using both 3D and intensity info. The
proposed method was compared against two state-of-the-art roadside LiDAR
background models and evaluated at point, object, and path levels,
demonstrating better robustness under heavy traffic and challenging weather.
This multimodal Weighted Bayesian GMM method is capable of handling dynamic
backgrounds with noisy measurements and substantially enhances the
infrastructure-based LiDAR object detection, whereby various 3D modeling for
smart city applications could be created.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models Can See: Plugging Visual Controls in Text Generation. (arXiv:2205.02655v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02655">
<div class="article-summary-box-inner">
<span><p>Generative language models (LMs) such as GPT-2/3 can be prompted to generate
text with remarkable quality. While they are designed for text-prompted
generation, it remains an open question how the generation process could be
guided by modalities beyond text such as images. In this work, we propose a
training-free framework, called MAGIC (iMAge-Guided text generatIon with CLIP),
for plugging in visual controls in the generation process and enabling LMs to
perform multimodal tasks (e.g., image captioning) in a zero-shot manner. MAGIC
is a simple yet efficient plug-and-play framework, which directly combines an
off-the-shelf LM (i.e., GPT-2) and an image-text matching model (i.e., CLIP)
for image-grounded text generation. During decoding, MAGIC influences the
generation of the LM by introducing a CLIP-induced score, called magic score,
which regularizes the generated result to be semantically related to a given
image while being coherent to the previously generated context. Notably, the
proposed decoding scheme does not involve any gradient update operation,
therefore being computationally efficient. On the challenging task of zero-shot
image captioning, MAGIC outperforms the state-of-the-art method by notable
margins with a nearly 27 times decoding speedup. MAGIC is a flexible framework
and is theoretically compatible with any text generation tasks that incorporate
image grounding. In the experiments, we showcase that it is also capable of
performing visually grounded story generation given both an image and a text
prompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmentations: An Insight into their Effectiveness on Convolution Neural Networks. (arXiv:2205.04064v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04064">
<div class="article-summary-box-inner">
<span><p>Augmentations are the key factor in determining the performance of any neural
network as they provide a model with a critical edge in boosting its
performance. Their ability to boost a model's robustness depends on two
factors, viz-a-viz, the model architecture, and the type of augmentations.
Augmentations are very specific to a dataset, and it is not imperative that all
kinds of augmentation would necessarily produce a positive effect on a model's
performance. Hence there is a need to identify augmentations that perform
consistently well across a variety of datasets and also remain invariant to the
type of architecture, convolutions, and the number of parameters used. Hence
there is a need to identify augmentations that perform consistently well across
a variety of datasets and also remain invariant to the type of architecture,
convolutions, and the number of parameters used. This paper evaluates the
effect of parameters using 3x3 and depth-wise separable convolutions on
different augmentation techniques on MNIST, FMNIST, and CIFAR10 datasets.
Statistical Evidence shows that techniques such as Cutouts and Random
horizontal flip were consistent on both parametrically low and high
architectures. Depth-wise separable convolutions outperformed 3x3 convolutions
at higher parameters due to their ability to create deeper networks.
Augmentations resulted in bridging the accuracy gap between the 3x3 and
depth-wise separable convolutions, thus establishing their role in model
generalization. At higher number augmentations did not produce a significant
change in performance. The synergistic effect of multiple augmentations at
higher parameters, with antagonistic effect at lower parameters, was also
evaluated. The work proves that a delicate balance between architectural
supremacy and augmentations needs to be achieved to enhance a model's
performance in any given deep learning task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Impact of Partial Occlusion on Pedestrian Detectability. (arXiv:2205.04812v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04812">
<div class="article-summary-box-inner">
<span><p>Robust detection of vulnerable road users is a safety critical requirement
for the deployment of autonomous vehicles in heterogeneous traffic. One of the
most complex outstanding challenges is that of partial occlusion where a target
object is only partially available to the sensor due to obstruction by another
foreground object. A number of leading pedestrian detection benchmarks provide
annotation for partial occlusion, however each benchmark varies greatly in
their definition of the occurrence and severity of occlusion. Recent research
demonstrates that a high degree of subjectivity is used to classify occlusion
level in these cases and occlusion is typically categorized into 2 to 3 broad
categories such as partially and heavily occluded. This can lead to inaccurate
or inconsistent reporting of pedestrian detection model performance depending
on which benchmark is used. This research introduces a novel, objective
benchmark for partially occluded pedestrian detection to facilitate the
objective characterization of pedestrian detection models. Characterization is
carried out on seven popular pedestrian detection models for a range of
occlusion levels from 0-99%. Results demonstrate that pedestrian detection
performance degrades, and the number of false negative detections increase as
pedestrian occlusion level increases. Of the seven popular pedestrian detection
routines characterized, CenterNet has the greatest overall performance,
followed by SSDlite. RetinaNet has the lowest overall detection performance
across the range of occlusion levels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Objective Method for Pedestrian Occlusion Level Classification. (arXiv:2205.05412v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05412">
<div class="article-summary-box-inner">
<span><p>Pedestrian detection is among the most safety-critical features of driver
assistance systems for autonomous vehicles. One of the most complex detection
challenges is that of partial occlusion, where a target object is only
partially available to the sensor due to obstruction by another foreground
object. A number of current pedestrian detection benchmarks provide annotation
for partial occlusion to assess algorithm performance in these scenarios,
however each benchmark varies greatly in their definition of the occurrence and
severity of occlusion. In addition, current occlusion level annotation methods
contain a high degree of subjectivity by the human annotator. This can lead to
inaccurate or inconsistent reporting of an algorithm's detection performance
for partially occluded pedestrians, depending on which benchmark is used. This
research presents a novel, objective method for pedestrian occlusion level
classification for ground truth annotation. Occlusion level classification is
achieved through the identification of visible pedestrian keypoints and through
the use of a novel, effective method of 2D body surface area estimation.
Experimental results demonstrate that the proposed method reflects the
pixel-wise occlusion level of pedestrians in images and is effective for all
forms of occlusion, including challenging edge cases such as self-occlusion,
truncation and inter-occluding pedestrians.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PillarNet: Real-Time and High-Performance Pillar-based 3D Object Detection. (arXiv:2205.07403v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07403">
<div class="article-summary-box-inner">
<span><p>Real-time and high-performance 3D object detection is of critical importance
for autonomous driving. Recent top-performing 3D object detectors mainly rely
on point-based or 3D voxel-based convolutions, which are both computationally
inefficient for onboard deployment. In contrast, pillar-based methods use
solely 2D convolutions, which consume less computation resources, but they lag
far behind their voxel-based counterparts in detection accuracy. In this paper,
by examining the primary performance gap between pillar- and voxel-based
detectors, we develop a real-time and high-performance pillar-based detector,
dubbed PillarNet. The proposed PillarNet consists of a powerful encoder network
for effective pillar feature learning, a neck network for spatial-semantic
feature fusion and the commonly used detect head. Using only 2D convolutions,
PillarNet is flexible to an optional pillar size and compatible with classical
2D CNN backbones, such as VGGNet and ResNet.Additionally, PillarNet benefits
from our designed orientation-decoupled IoU regression loss along with the
IoU-aware prediction branch. Extensive experimental results on large-scale
nuScenes Dataset and Waymo Open Dataset demonstrate that the proposed PillarNet
performs well over the state-of-the-art 3D detectors in terms of effectiveness
and efficiency. The source code is available at
https://github.com/agent-sgs/PillarNet.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ColonFormer: An Efficient Transformer based Method for Colon Polyp Segmentation. (arXiv:2205.08473v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08473">
<div class="article-summary-box-inner">
<span><p>Identifying polyps is challenging for automatic analysis of endoscopic images
in computer-aided clinical support systems. Models based on convolutional
networks (CNN), transformers, and their combinations have been proposed to
segment polyps with promising results. However, those approaches have
limitations either in modeling the local appearance of the polyps only or lack
of multi-level features for spatial dependency in the decoding process. This
paper proposes a novel network, namely ColonFormer, to address these
limitations. ColonFormer is an encoder-decoder architecture capable of modeling
long-range semantic information at both encoder and decoder branches. The
encoder is a lightweight architecture based on transformers for modeling global
semantic relations at multi scales. The decoder is a hierarchical network
structure designed for learning multi-level features to enrich feature
representation. Besides, a refinement module is added with a new skip
connection technique to refine the boundary of polyp objects in the global map
for accurate segmentation. Extensive experiments have been conducted on five
popular benchmark datasets for polyp segmentation, including Kvasir, CVC-Clinic
DB, CVC-ColonDB, CVC-T, and ETIS-Larib. Experimental results show that our
ColonFormer outperforms other state-of-the-art methods on all benchmark
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian Convolutional Neural Networks for Limited Data Hyperspectral Remote Sensing Image Classification. (arXiv:2205.09250v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09250">
<div class="article-summary-box-inner">
<span><p>Employing deep neural networks for Hyperspectral remote sensing (HSRS) image
classification is a challenging task. HSRS images have high dimensionality and
a large number of channels with substantial redundancy between channels. In
addition, the training data for classifying HSRS images is limited and the
amount of available training data is much smaller compared to other
classification tasks. These factors complicate the training process of deep
neural networks with many parameters and cause them to not perform well even
compared to conventional models. Moreover, convolutional neural networks
produce over-confident predictions, which is highly undesirable considering the
aforementioned problem.
</p>
<p>In this work, we use for HSRS image classification a special class of deep
neural networks, namely a Bayesian neural network (BNN). To the extent of our
knowledge, this is the first time that BNNs are used in HSRS image
classification. BNNs inherently provide a measure for uncertainty. We perform
extensive experiments on the Pavia Centre, Salinas, and Botswana datasets. We
show that a BNN outperforms a standard convolutional neural network (CNN) and
an off-the-shelf Random Forest (RF). Further experiments underline that the BNN
is more stable and robust to model pruning, and that the uncertainty is higher
for samples with higher expected prediction error.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering Dynamic Functional Brain Networks via Spatial and Channel-wise Attention. (arXiv:2205.09576v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09576">
<div class="article-summary-box-inner">
<span><p>Using deep learning models to recognize functional brain networks (FBNs) in
functional magnetic resonance imaging (fMRI) has been attracting increasing
interest recently. However, most existing work focuses on detecting static FBNs
from entire fMRI signals, such as correlation-based functional connectivity.
Sliding-window is a widely used strategy to capture the dynamics of FBNs, but
it is still limited in representing intrinsic functional interactive dynamics
at each time step. And the number of FBNs usually need to be set manually. More
over, due to the complexity of dynamic interactions in brain, traditional
linear and shallow models are insufficient in identifying complex and spatially
overlapped FBNs across each time step. In this paper, we propose a novel
Spatial and Channel-wise Attention Autoencoder (SCAAE) for discovering FBNs
dynamically. The core idea of SCAAE is to apply attention mechanism to FBNs
construction. Specifically, we designed two attention modules: 1) spatial-wise
attention (SA) module to discover FBNs in the spatial domain and 2) a
channel-wise attention (CA) module to weigh the channels for selecting the FBNs
automatically. We evaluated our approach on ADHD200 dataset and our results
indicate that the proposed SCAAE method can effectively recover the dynamic
changes of the FBNs at each fMRI time step, without using sliding windows. More
importantly, our proposed hybrid attention modules (SA and CA) do not enforce
assumptions of linearity and independence as previous methods, and thus provide
a novel approach to better understanding dynamic functional brain networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HiVLP: Hierarchical Vision-Language Pre-Training for Fast Image-Text Retrieval. (arXiv:2205.12105v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12105">
<div class="article-summary-box-inner">
<span><p>In the past few years, the emergence of vision-language pre-training (VLP)
has brought cross-modal retrieval to a new era. However, due to the latency and
computation demand, it is commonly challenging to apply VLP in a real-time
online retrieval system. To alleviate the defect, this paper proposes a
\textbf{Hi}erarchical \textbf{V}ision-\textbf{}Language \textbf{P}re-Training
(\textbf{HiVLP}) for fast Image-Text Retrieval (ITR). Specifically, we design a
novel hierarchical retrieval objective, which uses the representation of
different dimensions for coarse-to-fine ITR, i.e., using low-dimensional
representation for large-scale coarse retrieval and high-dimensional
representation for small-scale fine retrieval. We evaluate our proposed HiVLP
on two popular image-text retrieval benchmarks, i.e., Flickr30k and COCO.
Extensive experiments demonstrate that our HiVLP not only has fast inference
speed but also can be easily scaled to large-scale ITR scenarios. The detailed
results show that HiVLP is $1,427$$\sim$$120,649\times$ faster than the
fusion-based model UNITER and 2$\sim$5 faster than the fastest embedding-based
model LightingDot in different candidate scenarios. It also achieves about +4.9
AR on COCO and +3.8 AR on Flickr30K than LightingDot and achieves comparable
performance with the state-of-the-art (SOTA) fusion-based model METER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Video Deblurring via Lightweight Motion Compensation. (arXiv:2205.12634v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12634">
<div class="article-summary-box-inner">
<span><p>While motion compensation greatly improves video deblurring quality,
separately performing motion compensation and video deblurring demands huge
computational overhead. This paper proposes a real-time video deblurring
framework consisting of a lightweight multi-task unit that supports both video
deblurring and motion compensation in an efficient way. The multi-task unit is
specifically designed to handle large portions of the two tasks using a single
shared network, and consists of a multi-task detail network and simple networks
for deblurring and motion compensation. The multi-task unit minimizes the cost
of incorporating motion compensation into video deblurring and enables
real-time deblurring. Moreover, by stacking multiple multi-task units, our
framework provides flexible control between the cost and deblurring quality. We
experimentally validate the state-of-the-art deblurring quality of our
approach, which runs at a much faster speed compared to previous methods, and
show practical real-time performance (30.99dB@30fps measured in the DVD
dataset).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">V-Doc : Visual questions answers with Documents. (arXiv:2205.13724v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13724">
<div class="article-summary-box-inner">
<span><p>We propose V-Doc, a question-answering tool using document images and PDF,
mainly for researchers and general non-deep learning experts looking to
generate, process, and understand the document visual question answering tasks.
The V-Doc supports generating and using both extractive and abstractive
question-answer pairs using documents images. The extractive QA selects a
subset of tokens or phrases from the document contents to predict the answers,
while the abstractive QA recognises the language in the content and generates
the answer based on the trained model. Both aspects are crucial to
understanding the documents, especially in an image format. We include a
detailed scenario of question generation for the abstractive QA task. V-Doc
supports a wide range of datasets and models, and is highly extensible through
a declarative, framework-agnostic platform.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video2StyleGAN: Disentangling Local and Global Variations in a Video. (arXiv:2205.13996v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13996">
<div class="article-summary-box-inner">
<span><p>Image editing using a pretrained StyleGAN generator has emerged as a powerful
paradigm for facial editing, providing disentangled controls over age,
expression, illumination, etc. However, the approach cannot be directly adopted
for video manipulations. We hypothesize that the main missing ingredient is the
lack of fine-grained and disentangled control over face location, face pose,
and local facial expressions. In this work, we demonstrate that such a
fine-grained control is indeed achievable using pretrained StyleGAN by working
across multiple (latent) spaces (namely, the positional space, the W+ space,
and the S space) and combining the optimization results across the multiple
spaces. Building on this enabling component, we introduce Video2StyleGAN that
takes a target image and driving video(s) to reenact the local and global
locations and expressions from the driving video in the identity of the target
image. We evaluate the effectiveness of our method over multiple challenging
scenarios and demonstrate clear improvements over alternative approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Learning with Multi-query Transformer for Dense Prediction. (arXiv:2205.14354v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14354">
<div class="article-summary-box-inner">
<span><p>Previous multi-task dense prediction studies developed complex pipelines such
as multi-modal distillations in multiple stages or searching for task
relational contexts for each task. The core insight beyond these methods is to
maximize the mutual effects between each task. Inspired by the recent
query-based Transformers, we propose a simpler pipeline named Multi-Query
Transformer (MQTransformer) that is equipped with multiple queries from
different tasks to facilitate the reasoning among multiple tasks and simplify
the cross task pipeline. Instead of modeling the dense per-pixel context among
different tasks, we seek a task-specific proxy to perform cross-task reasoning
via multiple queries where each query encodes the task-related context. The
MQTransformer is composed of three key components: shared encoder, cross task
attention and shared decoder. We first model each task with a task-relevant and
scale-aware query, and then both the image feature output by the feature
extractor and the task-relevant query feature are fed into the shared encoder,
thus encoding the query feature from the image feature. Secondly, we design a
cross task attention module to reason the dependencies among multiple tasks and
feature scales from two perspectives including different tasks of the same
scale and different scales of the same task. Then we use a shared decoder to
gradually refine the image features with the reasoned query features from
different tasks. Extensive experiment results on two dense prediction datasets
(NYUD-v2 and PASCAL-Context) show that the proposed method is an effective
approach and achieves the state-of-the-art result. Code will be available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PSNet: Fast Data Structuring for Hierarchical Deep Learning on Point Cloud. (arXiv:2205.14965v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14965">
<div class="article-summary-box-inner">
<span><p>In order to retain more feature information of local areas on a point cloud,
local grouping and subsampling are the necessary data structuring steps in most
hierarchical deep learning models. Due to the disorder nature of the points in
a point cloud, the significant time cost may be consumed when grouping and
subsampling the points, which consequently results in poor scalability. This
paper proposes a fast data structuring method called PSNet (Point Structuring
Net). PSNet transforms the spatial features of the points and matches them to
the features of local areas in a point cloud. PSNet achieves grouping and
sampling at the same time while the existing methods process sampling and
grouping in two separate steps (such as using FPS plus kNN). PSNet performs
feature transformation pointwise while the existing methods uses the spatial
relationship among the points as the reference for grouping. Thanks to these
features, PSNet has two important advantages: 1) the grouping and sampling
results obtained by PSNet is stable and permutation invariant; and 2) PSNet can
be easily parallelized. PSNet can replace the data structuring methods in the
mainstream point cloud deep learning models in a plug-and-play manner. We have
conducted extensive experiments. The results show that PSNet can improve the
training and inference speed significantly while maintaining the model
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deblurring Photographs of Characters Using Deep Neural Networks. (arXiv:2205.15053v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15053">
<div class="article-summary-box-inner">
<span><p>In this paper, we present our approach for the Helsinki Deblur Challenge
(HDC2021). The task of this challenge is to deblur images of characters without
knowing the point spread function (PSF). The organizers provided a dataset of
pairs of sharp and blurred images. Our method consists of three steps: First,
we estimate a warping transformation of the images to align the sharp images
with the blurred ones. Next, we estimate the PSF using a quasi-Newton method.
The estimated PSF allows to generate additional pairs of sharp and blurred
images. Finally, we train a deep convolutional neural network to reconstruct
the sharp images from the blurred images. Our method is able to successfully
reconstruct images from the first 10 stages of the HDC 2021 data. Our code is
available at https://github.com/hhu-machine-learning/hdc2021-psfnn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model. (arXiv:2205.15278v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15278">
<div class="article-summary-box-inner">
<span><p>Although significant progress has been made to audio-driven talking face
generation, existing methods either neglect facial emotion or cannot be applied
to arbitrary subjects. In this paper, we propose the Emotion-Aware Motion Model
(EAMM) to generate one-shot emotional talking faces by involving an emotion
source video. Specifically, we first propose an Audio2Facial-Dynamics module,
which renders talking faces from audio-driven unsupervised zero- and
first-order key-points motion. Then through exploring the motion model's
properties, we further propose an Implicit Emotion Displacement Learner to
represent emotion-related facial dynamics as linearly additive displacements to
the previously acquired motion representations. Comprehensive experiments
demonstrate that by incorporating the results from both modules, our method can
generate satisfactory talking face results on arbitrary subjects with realistic
emotion patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot and Few-Shot Learning for Lung Cancer Multi-Label Classification using Vision Transformer. (arXiv:2205.15290v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15290">
<div class="article-summary-box-inner">
<span><p>Lung cancer is the leading cause of cancer-related death worldwide. Lung
adenocarcinoma (LUAD) and lung squamous cell carcinoma (LUSC) are the most
common histologic subtypes of non-small-cell lung cancer (NSCLC). Histology is
an essential tool for lung cancer diagnosis. Pathologists make classifications
according to the dominant subtypes. Although morphology remains the standard
for diagnosis, significant tool needs to be developed to elucidate the
diagnosis. In our study, we utilize the pre-trained Vision Transformer (ViT)
model to classify multiple label lung cancer on histologic slices (from dataset
LC25000), in both Zero-Shot and Few-Shot settings. Then we compare the
performance of Zero-Shot and Few-Shot ViT on accuracy, precision, recall,
sensitivity and specificity. Our study show that the pre-trained ViT model has
a good performance in Zero-Shot setting, a competitive accuracy ($99.87\%$) in
Few-Shot setting ({epoch = 1}) and an optimal result ($100.00\%$ on both
validation set and test set) in Few-Shot seeting ({epoch = 5}).
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-06-01 23:08:53.199881011 UTC">2022-06-01 23:08:53 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>