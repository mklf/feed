<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-07-06T01:30:00Z">07-06</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A Cascade Model for Argument Mining in Japanese Political Discussions: the QA Lab-PoliInfo-3 Case Study. (arXiv:2207.01672v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01672">
<div class="article-summary-box-inner">
<span><p>The rVRAIN team tackled the Budget Argument Mining (BAM) task, consisting of
a combination of classification and information retrieval sub-tasks. For the
argument classification (AC), the team achieved its best performing results
with a five-class BERT-based cascade model complemented with some handcrafted
rules. The rules were used to determine if the expression was monetary or not.
Then, each monetary expression was classified as a premise or as a conclusion
in the first level of the cascade model. Finally, each premise was classified
into the three premise classes, and each conclusion into the two conclusion
classes. For the information retrieval (i.e., relation ID detection or RID),
our best results were achieved by a combination of a BERT-based binary
classifier, and the cosine similarity of pairs consisting of the monetary
expression and budget dense embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Location reference recognition from texts: A survey and comparison. (arXiv:2207.01683v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01683">
<div class="article-summary-box-inner">
<span><p>A vast amount of location information exists in unstructured texts, such as
social media posts, news stories, scientific articles, web pages, travel blogs,
and historical archives. Geoparsing refers to the process of recognizing
location references from texts and identifying their geospatial
representations. While geoparsing can benefit many domains, a summary of the
specific applications is still missing. Further, there lacks a comprehensive
review and comparison of existing approaches for location reference
recognition, which is the first and a core step of geoparsing. To fill these
research gaps, this review first summarizes seven typical application domains
of geoparsing: geographic information retrieval, disaster management, disease
surveillance, traffic management, spatial humanities, tourism management, and
crime management. We then review existing approaches for location reference
recognition by categorizing these approaches into four groups based on their
underlying functional principle: rule-based, gazetteer matching-based,
statistical learning-based, and hybrid approaches. Next, we thoroughly evaluate
the correctness and computational efficiency of the 27 most widely used
approaches for location reference recognition based on 26 public datasets with
different types of texts (e.g., social media posts and news stories) containing
39,736 location references across the world. Results from this thorough
evaluation can help inform future methodological developments for location
reference recognition, and can help guide the selection of proper approaches
based on application needs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangled Action Recognition with Knowledge Bases. (arXiv:2207.01708v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01708">
<div class="article-summary-box-inner">
<span><p>Action in video usually involves the interaction of human with objects.
Action labels are typically composed of various combinations of verbs and
nouns, but we may not have training data for all possible combinations. In this
paper, we aim to improve the generalization ability of the compositional action
recognition model to novel verbs or novel nouns that are unseen during training
time, by leveraging the power of knowledge graphs. Previous work utilizes
verb-noun compositional action nodes in the knowledge graph, making it
inefficient to scale since the number of compositional action nodes grows
quadratically with respect to the number of verbs and nouns. To address this
issue, we propose our approach: Disentangled Action Recognition with
Knowledge-bases (DARK), which leverages the inherent compositionality of
actions. DARK trains a factorized model by first extracting disentangled
feature representations for verbs and nouns, and then predicting classification
weights using relations in external knowledge graphs. The type constraint
between verb and noun is extracted from external knowledge bases and finally
applied when composing actions. DARK has better scalability in the number of
objects and verbs, and achieves state-of-the-art performance on the Charades
dataset. We further propose a new benchmark split based on the Epic-kitchen
dataset which is an order of magnitude bigger in the numbers of classes and
samples, and benchmark various models on this benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT, can HE predict contrastive focus? Predicting and controlling prominence in neural TTS using a language model. (arXiv:2207.01718v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01718">
<div class="article-summary-box-inner">
<span><p>Several recent studies have tested the use of transformer language model
representations to infer prosodic features for text-to-speech synthesis (TTS).
While these studies have explored prosody in general, in this work, we look
specifically at the prediction of contrastive focus on personal pronouns. This
is a particularly challenging task as it often requires semantic, discursive
and/or pragmatic knowledge to predict correctly. We collect a corpus of
utterances containing contrastive focus and we evaluate the accuracy of a BERT
model, finetuned to predict quantized acoustic prominence features, on these
samples. We also investigate how past utterances can provide relevant
information for this prediction. Furthermore, we evaluate the controllability
of pronoun prominence in a TTS model conditioned on acoustic prominence
features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing via Prompting. (arXiv:2207.01736v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01736">
<div class="article-summary-box-inner">
<span><p>Probing is a popular method to discern what linguistic information is
contained in the representations of pre-trained language models. However, the
mechanism of selecting the probe model has recently been subject to intense
debate, as it is not clear if the probes are merely extracting information or
modeling the linguistic property themselves. To address this challenge, this
paper introduces a novel model-free approach to probing, by formulating probing
as a prompting task. We conduct experiments on five probing tasks and show that
our approach is comparable or better at extracting information than diagnostic
probes while learning much less on its own. We further combine the probing via
prompting approach with attention head pruning to analyze where the model
stores the linguistic information in its architecture. We then examine the
usefulness of a specific linguistic property for pre-training by removing the
heads that are essential to that property and evaluating the resulting model's
performance on language modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PReGAN: Answer Oriented Passage Ranking with Weakly Supervised GAN. (arXiv:2207.01762v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01762">
<div class="article-summary-box-inner">
<span><p>Beyond topical relevance, passage ranking for open-domain factoid question
answering also requires a passage to contain an answer (answerability). While a
few recent studies have incorporated some reading capability into a ranker to
account for answerability, the ranker is still hindered by the noisy nature of
the training data typically available in this area, which considers any passage
containing an answer entity as a positive sample. However, the answer entity in
a passage is not necessarily mentioned in relation with the given question. To
address the problem, we propose an approach called \ttt{PReGAN} for Passage
Reranking based on Generative Adversarial Neural networks, which incorporates a
discriminator on answerability, in addition to a discriminator on topical
relevance. The goal is to force the generator to rank higher a passage that is
topically relevant and contains an answer. Experiments on five public datasets
show that \ttt{PReGAN} can better rank appropriate passages, which in turn,
boosts the effectiveness of QA systems, and outperforms the existing approaches
without using external data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-and-Language Pretraining. (arXiv:2207.01772v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01772">
<div class="article-summary-box-inner">
<span><p>With the burgeoning amount of data of image-text pairs and diversity of
Vision-and-Language (V&amp;L) tasks, scholars have introduced an abundance of deep
learning models in this research domain. Furthermore, in recent years, transfer
learning has also shown tremendous success in Computer Vision for tasks such as
Image Classification, Object Detection, etc., and in Natural Language
Processing for Question Answering, Machine Translation, etc. Inheriting the
spirit of Transfer Learning, research works in V&amp;L have devised multiple
pretraining techniques on large-scale datasets in order to enhance the
performance of downstream tasks. The aim of this article is to provide a
comprehensive revision of contemporary V&amp;L pretraining models. In particular,
we categorize and delineate pretraining approaches, along with the summary of
state-of-the-art vision-and-language pre-trained models. Moreover, a list of
training datasets and downstream tasks is supplied to further polish the
perspective on V&amp;L pretraining. Lastly, we decided to take a further step to
discuss numerous directions for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning. (arXiv:2207.01780v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01780">
<div class="article-summary-box-inner">
<span><p>Program synthesis or code generation aims to generate a program that
satisfies a problem specification. Recent approaches using large-scale
pretrained language models (LMs) have shown promising results, yet they have
some critical limitations. In particular, they often follow a standard
supervised fine-tuning procedure to train a code generation model only from the
pairs of natural-language problem descriptions and ground-truth programs. Such
paradigm largely ignores some important but potentially useful signals in the
problem specification such as unit tests, which thus often results in poor
performance when solving complex unseen coding tasks. To address the
limitations, we propose "CodeRL", a new framework for program synthesis tasks
through pretrained LMs and deep reinforcement learning (RL). Specifically,
during training, we treat the code-generating LM as an actor network, and
introduce a critic network that is trained to predict the functional
correctness of generated programs and provide dense feedback signals to the
actor. During inference, we introduce a new generation procedure with a
critical sampling strategy that allows a model to automatically regenerate
programs based on feedback from example unit tests and critic scores. For the
model backbones, we extended the encoder-decoder architecture of CodeT5 with
enhanced learning objectives, larger model sizes, and better pretraining data.
Our method not only achieves new SOTA results on the challenging APPS
benchmark, but also shows strong zero-shot transfer capability with new SOTA
results on the simpler MBPP benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scene-Aware Prompt for Multi-modal Dialogue Understanding and Generation. (arXiv:2207.01823v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01823">
<div class="article-summary-box-inner">
<span><p>This paper introduces the schemes of Team LingJing's experiments in
NLPCC-2022-Shared-Task-4 Multi-modal Dialogue Understanding and Generation
(MDUG). The MDUG task can be divided into two phases: multi-modal context
understanding and response generation. To fully leverage the visual information
for both scene understanding and dialogue generation, we propose the
scene-aware prompt for the MDUG task. Specifically, we utilize the
multi-tasking strategy for jointly modelling the scene- and session-
multi-modal understanding. The visual captions are adopted to aware the scene
information, while the fixed-type templated prompt based on the scene- and
session-aware labels are used to further improve the dialogue generation
performance. Extensive experimental results show that the proposed method has
achieved state-of-the-art (SOTA) performance compared with other competitive
methods, where we rank the 1-st in all three subtasks in this MDUG competition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keyword Extraction in Scientific Documents. (arXiv:2207.01888v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01888">
<div class="article-summary-box-inner">
<span><p>The scientific publication output grows exponentially. Therefore, it is
increasingly challenging to keep track of trends and changes. Understanding
scientific documents is an important step in downstream tasks such as knowledge
graph building, text mining, and discipline classification. In this workshop,
we provide a better understanding of keyword and keyphrase extraction from the
abstract of scientific publications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASR-Generated Text for Language Model Pre-training Applied to Speech Tasks. (arXiv:2207.01893v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01893">
<div class="article-summary-box-inner">
<span><p>We aim at improving spoken language modeling (LM) using very large amount of
automatically transcribed speech. We leverage the INA (French National
Audiovisual Institute) collection and obtain 19GB of text after applying ASR on
350,000 hours of diverse TV shows. From this, spoken language models are
trained either by fine-tuning an existing LM (FlauBERT) or through training a
LM from scratch. New models (FlauBERT-Oral) are shared with the community and
evaluated for 3 downstream tasks: spoken language understanding, classification
of TV shows and speech syntactic parsing. Results show that FlauBERT-Oral can
be beneficial compared to its initial FlauBERT version demonstrating that,
despite its inherent noisy nature, ASR-generated text can be used to build
spoken language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Betti numbers of attention graphs is all you really need. (arXiv:2207.01903v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01903">
<div class="article-summary-box-inner">
<span><p>We apply methods of topological analysis to the attention graphs, calculated
on the attention heads of the BERT model ( <a href="/abs/1810.04805">arXiv:1810.04805v2</a> ). Our research
shows that the classifier built upon basic persistent topological features
(namely, Betti numbers) of the trained neural network can achieve
classification results on par with the conventional classification method. We
show the relevance of such topological text representation on three text
classification benchmarks. For the best of our knowledge, it is the first
attempt to analyze the topology of an attention-based neural network, widely
used for Natural Language Processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual QA as a Stepping Stone for Monolingual Open QA in Icelandic. (arXiv:2207.01918v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01918">
<div class="article-summary-box-inner">
<span><p>It can be challenging to build effective open question answering (open QA)
systems for languages other than English, mainly due to a lack of labeled data
for training. We present a data efficient method to bootstrap such a system for
languages other than English. Our approach requires only limited QA resources
in the given language, along with machine-translated data, and at least a
bilingual language model. To evaluate our approach, we build such a system for
the Icelandic language and evaluate performance over trivia style datasets. The
corpora used for training are English in origin but machine translated into
Icelandic. We train a bilingual Icelandic/English language model to embed
English context and Icelandic questions following methodology introduced with
DensePhrases (Lee et al., 2021). The resulting system is an open domain
cross-lingual QA system between Icelandic and English. Finally, the system is
adapted for Icelandic only open QA, demonstrating how it is possible to
efficiently create an open QA system with limited access to curated datasets in
the language of interest.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity Linking in Tabular Data Needs the Right Attention. (arXiv:2207.01937v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01937">
<div class="article-summary-box-inner">
<span><p>Understanding the semantic meaning of tabular data requires Entity Linking
(EL), in order to associate each cell value to a real-world entity in a
Knowledge Base (KB). In this work, we focus on end-to-end solutions for EL on
tabular data that do not rely on fact lookup in the target KB. Tabular data
contains heterogeneous and sparse context, including column headers, cell
values and table captions. We experiment with various models to generate a
vector representation for each cell value to be linked. Our results show that
it is critical to apply an attention mechanism as well as an attention mask, so
that the model can only attend to the most relevant context and avoid
information dilution. The most relevant context includes: same-row cells,
same-column cells, headers and caption. Computational complexity, however,
grows quadratically with the size of tabular data for such a complex model. We
achieve constant memory usage by introducing a Tabular Entity Linking Lite
model (TELL ) that generates vector representation for a cell based only on its
value, the table headers and the table caption. TELL achieves 80.8% accuracy on
Wikipedia tables, which is only 0.1% lower than the state-of-the-art model with
quadratic memory usage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MIA 2022 Shared Task Submission: Leveraging Entity Representations, Dense-Sparse Hybrids, and Fusion-in-Decoder for Cross-Lingual Question Answering. (arXiv:2207.01940v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01940">
<div class="article-summary-box-inner">
<span><p>We describe our two-stage system for the Multi-lingual Information Access
(MIA) 2022 Shared Task on Cross-Lingual Open-Retrieval Question Answering. The
first stage consists of multilingual passage retrieval with a hybrid dense and
sparse retrieval strategy. The second stage consists of a reader which outputs
the answer from the top passages returned by the first stage. We show the
efficacy of using entity representations, sparse retrieval signals to help
dense retrieval, and Fusion-in-Decoder. On the development set, we obtain 43.46
F1 on XOR-TyDi QA and 21.99 F1 on MKQA, for an average F1 score of 32.73. On
the test set, we obtain 40.93 F1 on XOR-TyDi QA and 22.29 F1 on MKQA, for an
average F1 score of 31.61. We improve over the official baseline by over 4 F1
points on both the development and test sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making sense of spoken plurals. (arXiv:2207.01947v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01947">
<div class="article-summary-box-inner">
<span><p>Distributional semantics offers new ways to study the semantics of
morphology. This study focuses on the semantics of noun singulars and their
plural inflectional variants in English. Our goal is to compare two models for
the conceptualization of plurality. One model (FRACSS) proposes that all
singular-plural pairs should be taken into account when predicting plural
semantics from singular semantics. The other model (CCA) argues that
conceptualization for plurality depends primarily on the semantic class of the
base word. We compare the two models on the basis of how well the speech signal
of plural tokens in a large corpus of spoken American English aligns with the
semantic vectors predicted by the two models. Two measures are employed: the
performance of a form-to-meaning mapping and the correlations between form
distances and meaning distances. Results converge on a superior alignment for
CCA. Our results suggest that usage-based approaches to pluralization in which
a given word's own semantic neighborhood is given priority outperform theories
according to which pluralization is conceptualized as a process building on
high-level abstraction. We see that what has often been conceived of as a
highly abstract concept, [+plural], is better captured via a family of
mid-level partial generalizations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantum Circuit Compiler for a Shuttling-Based Trapped-Ion Quantum Computer. (arXiv:2207.01964v1 [quant-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01964">
<div class="article-summary-box-inner">
<span><p>Increasing capabilities of quantum computing hardware and the challenge to
realize deep quantum circuits call for fully automated and efficient tools to
compile quantum circuits. To express arbitrary circuits in a sequence of native
gates pertaining to the specific quantum computer architecture is necessary to
make algorithms portable across the landscape of quantum hardware providers. In
this work, we present a compiler capable of transforming and optimizing a
quantum circuit, targeting a shuttling-based trapped-ion quantum processor. It
consists of custom algorithms set on top of the Cambridge Quantum Computer's
quantum circuit framework Pytket. The performance is evaluated for a wide range
of quantum circuits, showing that the gate counts can be reduced by a factor of
up to 3.6 compared to standard Pytket and up to 2.2 compared to standard Qiskit
compilation, while we achieve similar gate counts as compared to a Pytket
extension targeting the AQT linear-static trapped ion addressing-based
architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Block-SCL: Blocking Matters for Supervised Contrastive Learning in Product Matching. (arXiv:2207.02008v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02008">
<div class="article-summary-box-inner">
<span><p>Product matching is a fundamental step for the global understanding of
consumer behavior in e-commerce. In practice, product matching refers to the
task of deciding if two product offers from different data sources (e.g.
retailers) represent the same product. Standard pipelines use a previous stage
called blocking, where for a given product offer a set of potential matching
candidates are retrieved based on similar characteristics (e.g. same brand,
category, flavor, etc.). From these similar product candidates, those that are
not a match can be considered hard negatives. We present Block-SCL, a strategy
that uses the blocking output to make the most of Supervised Contrastive
Learning (SCL). Concretely, Block-SCL builds enriched batches using the
hard-negatives samples obtained in the blocking stage. These batches provide a
strong training signal leading the model to learn more meaningful sentence
embeddings for product matching. Experimental results in several public
datasets demonstrate that Block-SCL achieves state-of-the-art results despite
only using short product titles as input, no data augmentation, and a lighter
transformer backbone than competing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Networks and the Chomsky Hierarchy. (arXiv:2207.02098v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02098">
<div class="article-summary-box-inner">
<span><p>Reliable generalization lies at the heart of safe ML and AI. However,
understanding when and how neural networks generalize remains one of the most
important unsolved problems in the field. In this work, we conduct an extensive
empirical study (2200 models, 16 tasks) to investigate whether insights from
the theory of computation can predict the limits of neural network
generalization in practice. We demonstrate that grouping tasks according to the
Chomsky hierarchy allows us to forecast whether certain architectures will be
able to generalize to out-of-distribution inputs. This includes negative
results where even extensive amounts of data and training time never led to any
non-trivial generalization, despite models having sufficient capacity to
perfectly fit the training data. Our results show that, for our subset of
tasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can
solve regular and counter-language tasks, and only networks augmented with
structured memory (such as a stack or memory tape) can successfully generalize
on context-free and context-sensitive tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A cross-corpus study on speech emotion recognition. (arXiv:2207.02104v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02104">
<div class="article-summary-box-inner">
<span><p>For speech emotion datasets, it has been difficult to acquire large
quantities of reliable data and acted emotions may be over the top compared to
less expressive emotions displayed in everyday life. Lately, larger datasets
with natural emotions have been created. Instead of ignoring smaller, acted
datasets, this study investigates whether information learnt from acted
emotions is useful for detecting natural emotions. Cross-corpus research has
mostly considered cross-lingual and even cross-age datasets, and difficulties
arise from different methods of annotating emotions causing a drop in
performance. To be consistent, four adult English datasets covering acted,
elicited and natural emotions are considered. A state-of-the-art model is
proposed to accurately investigate the degradation of performance. The system
involves a bi-directional LSTM with an attention mechanism to classify emotions
across datasets. Experiments study the effects of training models in a
cross-corpus and multi-domain fashion and results show the transfer of
information is not successful. Out-of-domain models, followed by adapting to
the missing dataset, and domain adversarial training (DAT) are shown to be more
suitable to generalising to emotions across datasets. This shows positive
information transfer from acted datasets to those with more natural emotions
and the benefits from training on different corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Review of Visual-Textual Sentiment Analysis from Social Media Networks. (arXiv:2207.02160v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02160">
<div class="article-summary-box-inner">
<span><p>Social media networks have become a significant aspect of people's lives,
serving as a platform for their ideas, opinions and emotions. Consequently,
automated sentiment analysis (SA) is critical for recognising people's feelings
in ways that other information sources cannot. The analysis of these feelings
revealed various applications, including brand evaluations, YouTube film
reviews and healthcare applications. As social media continues to develop,
people post a massive amount of information in different forms, including text,
photos, audio and video. Thus, traditional SA algorithms have become limited,
as they do not consider the expressiveness of other modalities. By including
such characteristics from various material sources, these multimodal data
streams provide new opportunities for optimising the expected results beyond
text-based SA. Our study focuses on the forefront field of multimodal SA, which
examines visual and textual data posted on social media networks. Many people
are more likely to utilise this information to express themselves on these
platforms. To serve as a resource for academics in this rapidly growing field,
we introduce a comprehensive overview of textual and visual SA, including data
pre-processing, feature extraction techniques, sentiment benchmark datasets,
and the efficacy of multiple classification methodologies suited to each field.
We also provide a brief introduction of the most frequently utilised data
fusion strategies and a summary of existing research on visual-textual SA.
Finally, we highlight the most significant challenges and investigate several
important sentiment applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLEAR: Improving Vision-Language Navigation with Cross-Lingual, Environment-Agnostic Representations. (arXiv:2207.02185v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02185">
<div class="article-summary-box-inner">
<span><p>Vision-and-Language Navigation (VLN) tasks require an agent to navigate
through the environment based on language instructions. In this paper, we aim
to solve two key challenges in this task: utilizing multilingual instructions
for improved instruction-path grounding and navigating through new environments
that are unseen during training. To address these challenges, we propose CLEAR:
Cross-Lingual and Environment-Agnostic Representations. First, our agent learns
a shared and visually-aligned cross-lingual language representation for the
three languages (English, Hindi and Telugu) in the Room-Across-Room dataset.
Our language representation learning is guided by text pairs that are aligned
by visual information. Second, our agent learns an environment-agnostic visual
representation by maximizing the similarity between semantically-aligned image
pairs (with constraints on object-matching) from different environments. Our
environment agnostic visual representation can mitigate the environment bias
induced by low-level visual information. Empirically, on the Room-Across-Room
dataset, we show that our multilingual agent gets large improvements in all
metrics over the strong baseline model when generalizing to unseen environments
with the cross-lingual language representation and the environment-agnostic
visual representation. Furthermore, we show that our learned language and
visual representations can be successfully transferred to the Room-to-Room and
Cooperative Vision-and-Dialogue Navigation task, and present detailed
qualitative and quantitative generalization and grounding analysis. Our code is
available at https://github.com/jialuli-luka/CLEAR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BlonDe: An Automatic Evaluation Metric for Document-level Machine Translation. (arXiv:2103.11878v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11878">
<div class="article-summary-box-inner">
<span><p>Standard automatic metrics, e.g. BLEU, are not reliable for document-level MT
evaluation. They can neither distinguish document-level improvements in
translation quality from sentence-level ones, nor identify the discourse
phenomena that cause context-agnostic translations. This paper introduces a
novel automatic metric BlonDe to widen the scope of automatic MT evaluation
from sentence to document level. BlonDe takes discourse coherence into
consideration by categorizing discourse-related spans and calculating the
similarity-based F1 measure of categorized spans. We conduct extensive
comparisons on a newly constructed dataset BWB. The experimental results show
that BlonDe possesses better selectivity and interpretability at the
document-level, and is more sensitive to document-level nuances. In a
large-scale human study, BlonDe also achieves significantly higher Pearson's r
correlation with human judgments compared to previous metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MReD: A Meta-Review Dataset for Structure-Controllable Text Generation. (arXiv:2110.07474v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07474">
<div class="article-summary-box-inner">
<span><p>When directly using existing text generation datasets for controllable
generation, we are facing the problem of not having the domain knowledge and
thus the aspects that could be controlled are limited. A typical example is
when using CNN/Daily Mail dataset for controllable text summarization, there is
no guided information on the emphasis of summary sentences. A more useful text
generator should leverage both the input text and the control signal to guide
the generation, which can only be built with a deep understanding of the domain
knowledge. Motivated by this vision, our paper introduces a new text generation
dataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its
45k meta-review sentences are manually annotated with one of the 9 carefully
defined categories, including abstract, strength, decision, etc. We present
experimental results on start-of-the-art summarization models, and propose
methods for structure-controlled generation with both extractive and
abstractive models using our annotated data. By exploring various settings and
analyzing the model behavior with respect to the control signal, we demonstrate
the challenges of our proposed task and the values of our dataset MReD.
Meanwhile, MReD also allows us to have a better understanding of the
meta-review domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Attention-Model Explainability through Faithfulness Violation Test. (arXiv:2201.12114v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12114">
<div class="article-summary-box-inner">
<span><p>Attention mechanisms are dominating the explainability of deep models. They
produce probability distributions over the input, which are widely deemed as
feature-importance indicators. However, in this paper, we find one critical
limitation in attention explanations: weakness in identifying the polarity of
feature impact. This would be somehow misleading -- features with higher
attention weights may not faithfully contribute to model predictions; instead,
they can impose suppression effects. With this finding, we reflect on the
explainability of current attention-based techniques, such as
Attentio$\odot$Gradient and LRP-based attention explanations. We first propose
an actionable diagnostic methodology (henceforth faithfulness violation test)
to measure the consistency between explanation weights and the impact polarity.
Through the extensive experiments, we then show that most tested explanation
methods are unexpectedly hindered by the faithfulness violation issue,
especially the raw attention. Empirical analyses on the factors affecting
violation issues further provide useful observations for adopting explanation
methods in attention models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation in Mandarin. (arXiv:2203.10430v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10430">
<div class="article-summary-box-inner">
<span><p>Polyphone disambiguation is the most crucial task in Mandarin
grapheme-to-phoneme (g2p) conversion. Previous studies have approached this
problem using pre-trained language models, restricted output, and extra
information from Part-Of-Speech (POS) tagging. Inspired by these strategies, we
propose a novel approach, called g2pW, which adapts learnable softmax-weights
to condition the outputs of BERT with the polyphonic character of interest and
its POS tagging. Rather than using the hard mask as in previous works, our
experiments show that learning a soft-weighting function for the candidate
phonemes benefits performance. In addition, our proposed g2pW does not require
extra pre-trained POS tagging models while using POS tags as auxiliary features
since we train the POS tagging model simultaneously with the unified encoder.
Experimental results show that our g2pW outperforms existing methods on the
public CPP dataset. All codes, model weights, and a user-friendly package are
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WeNet 2.0: More Productive End-to-End Speech Recognition Toolkit. (arXiv:2203.15455v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15455">
<div class="article-summary-box-inner">
<span><p>Recently, we made available WeNet, a production-oriented end-to-end speech
recognition toolkit, which introduces a unified two-pass (U2) framework and a
built-in runtime to address the streaming and non-streaming decoding modes in a
single model. To further improve ASR performance and facilitate various
production requirements, in this paper, we present WeNet 2.0 with four
important updates. (1) We propose U2++, a unified two-pass framework with
bidirectional attention decoders, which includes the future contextual
information by a right-to-left attention decoder to improve the representative
ability of the shared encoder and the performance during the rescoring stage.
(2) We introduce an n-gram based language model and a WFST-based decoder into
WeNet 2.0, promoting the use of rich text data in production scenarios. (3) We
design a unified contextual biasing framework, which leverages user-specific
context (e.g., contact lists) to provide rapid adaptation ability for
production and improves ASR accuracy in both with-LM and without-LM scenarios.
(4) We design a unified IO to support large-scale data for effective model
training. In summary, the brand-new WeNet 2.0 achieves up to 10\% relative
recognition performance improvement over the original WeNet on various corpora
and makes available several important production-oriented features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of Language Model Integration for Transducer based Speech Recognition. (arXiv:2203.16776v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16776">
<div class="article-summary-box-inner">
<span><p>Utilizing text-only data with an external language model (ELM) in end-to-end
RNN-Transducer (RNN-T) for speech recognition is challenging. Recently, a class
of methods such as density ratio (DR) and internal language model estimation
(ILME) have been developed, outperforming the classic shallow fusion (SF)
method. The basic idea behind these methods is that RNN-T posterior should
first subtract the implicitly learned internal language model (ILM) prior, in
order to integrate the ELM. While recent studies suggest that RNN-T only learns
some low-order language model information, the DR method uses a well-trained
neural language model with full context, which may be inappropriate for the
estimation of ILM and deteriorate the integration performance. Based on the DR
method, we propose a low-order density ratio method (LODR) by replacing the
estimation with a low-order weak language model. Extensive empirical
experiments are conducted on both in-domain and cross-domain scenarios on
English LibriSpeech &amp; Tedlium-2 and Chinese WenetSpeech &amp; AISHELL-1 datasets.
It is shown that LODR consistently outperforms SF in all tasks, while
performing generally close to ILME and better than DR in most tests.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PanGu-Bot: Efficient Generative Dialogue Pre-training from Pre-trained Language Model. (arXiv:2203.17090v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17090">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce PanGu-Bot, a Chinese pre-trained open-domain
dialogue generation model based on a large pre-trained language model (PLM)
PANGU-alpha (Zeng et al.,2021). Different from other pre-trained dialogue
models trained over a massive amount of dialogue data from scratch, we aim to
build a powerful dialogue model with relatively fewer data and computation
costs by inheriting valuable language capabilities and knowledge from PLMs. To
this end, we train PanGu-Bot from the large PLM PANGU-alpha, which has been
proven well-performed on a variety of Chinese natural language tasks. We
investigate different aspects of responses generated by PanGu-Bot, including
response quality, knowledge, and safety. We show that PanGu-Bot outperforms
state-of-the-art Chinese dialogue systems (CDIALGPT (Wang et al., 2020), EVA
(Zhou et al., 2021), EVA2.0 (Gu et al., 2022)) w.r.t. the above three aspects.
We also demonstrate that PanGu-Bot can be easily deployed to generate emotional
responses without further training. Throughout our empirical analysis, we also
point out that the PanGu-Bot response quality, knowledge correctness, and
safety are still far from perfect, and further explorations are indispensable
to building reliable and smart dialogue systems. Our model and code will be
available at
https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/PanGu-Bot
soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study of Gender Impact in Self-supervised Models for Speech-to-Text Systems. (arXiv:2204.01397v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01397">
<div class="article-summary-box-inner">
<span><p>Self-supervised models for speech processing emerged recently as popular
foundation blocks in speech processing pipelines. These models are pre-trained
on unlabeled audio data and then used in speech processing downstream tasks
such as automatic speech recognition (ASR) or speech translation (ST). Since
these models are now used in research and industrial systems alike, it becomes
necessary to understand the impact caused by some features such as gender
distribution within pre-training data. Using French as our investigation
language, we train and compare gender-specific wav2vec 2.0 models against
models containing different degrees of gender balance in their pre-training
data. The comparison is performed by applying these models to two
speech-to-text downstream tasks: ASR and ST. Results show the type of
downstream integration matters. We observe lower overall performance using
gender-specific pre-training before fine-tuning an end-to-end ASR system.
However, when self-supervised models are used as feature extractors, the
overall ASR and ST results follow more complex patterns in which the balanced
pre-trained model does not necessarily lead to the best results. Lastly, our
crude 'fairness' metric, the relative performance difference measured between
female and male test sets, does not display a strong variation from balanced to
gender-specific pre-trained wav2vec 2.0 models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Semantic Answer Similarity Metrics. (arXiv:2206.12664v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12664">
<div class="article-summary-box-inner">
<span><p>There are several issues with the existing general machine translation or
natural language generation evaluation metrics, and question-answering (QA)
systems are indifferent in that context. To build robust QA systems, we need
the ability to have equivalently robust evaluation systems to verify whether
model predictions to questions are similar to ground-truth annotations. The
ability to compare similarity based on semantics as opposed to pure string
overlap is important to compare models fairly and to indicate more realistic
acceptance criteria in real-life applications. We build upon the first to our
knowledge paper that uses transformer-based model metrics to assess semantic
answer similarity and achieve higher correlations to human judgement in the
case of no lexical overlap. We propose cross-encoder augmented bi-encoder and
BERTScore models for semantic answer similarity, trained on a new dataset
consisting of name pairs of US-American public figures. As far as we are
concerned, we provide the first dataset of co-referent name string pairs along
with their similarities, which can be used for training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-specific Characteristic Assistance for Code-switching Speech Recognition. (arXiv:2206.14580v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14580">
<div class="article-summary-box-inner">
<span><p>Dual-encoder structure successfully utilizes two language-specific encoders
(LSEs) for code-switching speech recognition. Because LSEs are initialized by
two pre-trained language-specific models (LSMs), the dual-encoder structure can
exploit sufficient monolingual data and capture the individual language
attributes. However, existing methods have no language constraints on LSEs and
underutilize language-specific knowledge of LSMs. In this paper, we propose a
language-specific characteristic assistance (LSCA) method to mitigate the above
problems. Specifically, during training, we introduce two language-specific
losses as language constraints and generate corresponding language-specific
targets for them. During decoding, we take the decoding abilities of LSMs into
account by combining the output probabilities of two LSMs and the mixture model
to obtain the final predictions. Experiments show that either the training or
decoding method of LSCA can improve the model's performance. Furthermore, the
best result can obtain up to 15.4% relative error reduction on the
code-switching test set by combining the training and decoding methods of LSCA.
Moreover, the system can process code-switching speech recognition tasks well
without extra shared parameters or even retraining based on two pre-trained
LSMs by using our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations. (arXiv:2206.15462v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.15462">
<div class="article-summary-box-inner">
<span><p>We propose a margin-based loss for vision-language model pretraining that
encourages gradient-based explanations that are consistent with region-level
annotations. We refer to this objective as Attention Mask Consistency (AMC) and
demonstrate that it produces superior visual grounding performance compared to
models that rely instead on region-level annotations for explicitly training an
object detector such as Faster R-CNN. AMC works by encouraging gradient-based
explanation masks that focus their attention scores mostly within annotated
regions of interest for images that contain such annotations. Particularly, a
model trained with AMC on top of standard vision-language modeling objectives
obtains a state-of-the-art accuracy of 86.59% in the Flickr30k visual grounding
benchmark, an absolute improvement of 5.48% when compared to the best previous
model. Our approach also performs exceedingly well on established benchmarks
for referring expression comprehension and offers the added benefit by design
of gradient-based explanations that better align with human annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing the Effects of Hyperparameters on Knowledge Graph Embedding Quality. (arXiv:2207.00473v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00473">
<div class="article-summary-box-inner">
<span><p>Embedding knowledge graphs into low-dimensional spaces is a popular method
for applying approaches, such as link prediction or node classification, to
these databases. This embedding process is very costly in terms of both
computational time and space. Part of the reason for this is the optimisation
of hyperparameters, which involves repeatedly sampling, by random, guided, or
brute-force selection, from a large hyperparameter space and testing the
resulting embeddings for their quality. However, not all hyperparameters in
this search space will be equally important. In fact, with prior knowledge of
the relative importance of the hyperparameters, some could be eliminated from
the search altogether without significantly impacting the overall quality of
the outputted embeddings. To this end, we ran a Sobol sensitivity analysis to
evaluate the effects of tuning different hyperparameters on the variance of
embedding quality. This was achieved by performing thousands of embedding
trials, each time measuring the quality of embeddings produced by different
hyperparameter configurations. We regressed the embedding quality on those
hyperparameter configurations, using this model to generate Sobol sensitivity
indices for each of the hyperparameters. By evaluating the correlation between
Sobol indices, we find substantial variability in the hyperparameter
sensitivities between knowledge graphs, with differing dataset characteristics
being the probable cause of these inconsistencies. As an additional
contribution of this work we identify several relations in the UMLS knowledge
graph that may cause data leakage via inverse relations, and derive and present
UMLS-43, a leakage-robust variant of that graph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attributed Abnormality Graph Embedding for Clinically Accurate X-Ray Report Generation. (arXiv:2207.01208v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01208">
<div class="article-summary-box-inner">
<span><p>Automatic generation of medical reports from X-ray images can assist
radiologists to perform the time-consuming and yet important reporting task.
Yet, achieving clinically accurate generated reports remains challenging.
Modeling the underlying abnormalities using the knowledge graph approach has
been found promising in enhancing the clinical accuracy. In this paper, we
introduce a novel fined-grained knowledge graph structure called an attributed
abnormality graph (ATAG). The ATAG consists of interconnected abnormality nodes
and attribute nodes, allowing it to better capture the abnormality details. In
contrast to the existing methods where the abnormality graph was constructed
manually, we propose a methodology to automatically construct the fine-grained
graph structure based on annotations, medical reports in X-ray datasets, and
the RadLex radiology lexicon. We then learn the ATAG embedding using a deep
model with an encoder-decoder architecture for the report generation. In
particular, graph attention networks are explored to encode the relationships
among the abnormalities and their attributes. A gating mechanism is adopted and
integrated with various decoders for the generation. We carry out extensive
experiments based on the benchmark datasets, and show that the proposed
ATAG-based deep model outperforms the SOTA methods by a large margin and can
improve the clinical accuracy of the generated reports.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Student-AI Creative Writing: Pedagogical Strategies for Applying Natural Language Generation in Schools. (arXiv:2207.01484v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01484">
<div class="article-summary-box-inner">
<span><p>AI natural language generation (NLG) is a process where computer systems
generate human-comprehensible language texts from information. It can become an
integral part of a human's creative writing process. Importantly, youths can
learn to apply NLG in mainstream education and become better prepared for
AI-enhanced writing jobs and other writing endeavors. To explore how students
apply NLG to creative writing, we designed and implemented the 1st Human-AI
Creative Writing Contest in a Hong Kong secondary school. In this contest, each
student participant wrote a short story of up to 500-words using the student's
own words and words generated by a computer and built on open-source language
models. We designed four text generators for the contest as the computer's text
entry. Additionally, using design-based research, we developed seven workshops
where students learned to write with the four text generators and answered
reflection questions. In analyzing four students' short stories and
adjudicators' scores for the stories, we found different strategies in terms of
the number and the type of text generator words that students used. Some
strategies appeared more sophisticated than others. In analyzing students'
reflections, we found students could describe text generator input and output
as units of thought. Besides, students showed preferences for text generators;
and they expressed a range of feelings when writing with text generators. The
findings provide design implications not only for NLG applications in formal
schooling but also suggest pedagogical strategies for AI curriculum.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Egocentric Video-Language Pretraining @ Ego4D Challenge 2022. (arXiv:2207.01622v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01622">
<div class="article-summary-box-inner">
<span><p>In this report, we propose a video-language pretraining (VLP) based solution
\cite{kevin2022egovlp} for four Ego4D challenge tasks, including Natural
Language Query (NLQ), Moment Query (MQ), Object State Change Classification
(OSCC), and PNR Localization (PNR). Especially, we exploit the recently
released Ego4D dataset \cite{grauman2021ego4d} to pioneer Egocentric VLP from
pretraining dataset, pretraining objective, and development set. Based on the
above three designs, we develop a pretrained video-language model that is able
to transfer its egocentric video-text representation or video-only
representation to several video downstream tasks. Our Egocentric VLP achieves
10.46R@1&amp;IoU @0.3 on NLQ, 10.33 mAP on MQ, 74% Acc on OSCC, 0.67 sec error on
PNR. The code is available at https://github.com/showlab/EgoVLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Slice-by-slice deep learning aided oropharyngeal cancer segmentation with adaptive thresholding for spatial uncertainty on FDG PET and CT images. (arXiv:2207.01623v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01623">
<div class="article-summary-box-inner">
<span><p>Tumor segmentation is a fundamental step for radiotherapy treatment planning.
To define an accurate segmentation of the primary tumor (GTVp) of oropharyngeal
cancer patients (OPC), simultaneous assessment of different image modalities is
needed, and each image volume is explored slice-by-slice from different
orientations. Moreover, the manual fixed boundary of segmentation neglects the
spatial uncertainty known to occur in tumor delineation. This study proposes a
novel automatic deep learning (DL) model to assist radiation oncologists in a
slice-by-slice adaptive GTVp segmentation on registered FDG PET/CT images. We
included 138 OPC patients treated with (chemo)radiation in our institute. Our
DL framework exploits both inter and intra-slice context. Sequences of 3
consecutive 2D slices of concatenated FDG PET/CT images and GTVp contours were
used as input. A 3-fold cross validation was performed three times, training on
sequences extracted from the Axial (A), Sagittal (S), and Coronal (C) plane of
113 patients. Since consecutive sequences in a volume contain overlapping
slices, each slice resulted in three outcome predictions that were averaged. In
the A, S, and C planes, the output shows areas with different probabilities of
predicting the tumor. The performance of the models was assessed on 25 patients
at different probability thresholds using the mean Dice Score Coefficient
(DSC). Predictions were the closest to the ground truth at a probability
threshold of 0.9 (DSC of 0.70 in the A, 0.77 in the S, and 0.80 in the C
plane). The promising results of the proposed DL model show that the
probability maps on registered FDG PET/CT images could guide radiation
oncologists in a slice-by-slice adaptive GTVp segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interaction Transformer for Human Reaction Generation. (arXiv:2207.01685v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01685">
<div class="article-summary-box-inner">
<span><p>We address the challenging task of human reaction generation which aims to
generate a corresponding reaction based on an input action. Most of the
existing works do not focus on generating and predicting the reaction and
cannot generate the motion when only the action is given as input. To address
this limitation, we propose a novel interaction Transformer (InterFormer)
consisting of a Transformer network with both temporal and spatial attentions.
Specifically, the temporal attention captures the temporal dependencies of the
motion of both characters and of their interaction, while the spatial attention
learns the dependencies between the different body parts of each character and
those which are part of the interaction. Moreover, we propose using graphs to
increase the performance of the spatial attention via an interaction distance
module that helps focus on nearby joints from both characters. Extensive
experiments on the SBU interaction, K3HI, and DuetDance datasets demonstrate
the effectiveness of InterFormer. Our method is general and can be used to
generate more complex and long-term interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crime scene classification from skeletal trajectory analysis in surveillance settings. (arXiv:2207.01687v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01687">
<div class="article-summary-box-inner">
<span><p>Video anomaly analysis is a core task actively pursued in the field of
computer vision, with applications extending to real-world crime detection in
surveillance footage. In this work, we address the task of human-related crime
classification. In our proposed approach, the human body in video frames,
represented as skeletal joints trajectories, is used as the main source of
exploration. First, we introduce the significance of extending the ground truth
labels for HR-Crime dataset and hence, propose a supervised and unsupervised
methodology to generate trajectory-level ground truth labels. Next, given the
availability of the trajectory-level ground truth, we introduce a
trajectory-based crime classification framework. Ablation studies are conducted
with various architectures and feature fusion strategies for the representation
of the human trajectories. The conducted experiments demonstrate the
feasibility of the task and pave the path for further research in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motions and Texts. (arXiv:2207.01696v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01696">
<div class="article-summary-box-inner">
<span><p>Inspired by the strong ties between vision and language, the two intimate
human sensing and communication modalities, our paper aims to explore the
generation of 3D human full-body motions from texts, as well as its reciprocal
task, shorthanded for text2motion and motion2text, respectively. To tackle the
existing challenges, especially to enable the generation of multiple distinct
motions from the same text, and to avoid the undesirable production of trivial
motionless pose sequences, we propose the use of motion token, a discrete and
compact motion representation. This provides one level playing ground when
considering both motions and text signals, as the motion and text tokens,
respectively. Moreover, our motion2text module is integrated into the inverse
alignment process of our text2motion training pipeline, where a significant
deviation of synthesized text from the input text would be penalized by a large
training loss; empirically this is shown to effectively improve performance.
Finally, the mappings in-between the two modalities of motions and texts are
facilitated by adapting the neural model for machine translation (NMT) to our
context. This autoregressive modeling of the distribution over discrete motion
tokens further enables non-deterministic production of pose sequences, of
variable lengths, from an input text. Our approach is flexible, could be used
for both text2motion and motion2text tasks. Empirical evaluations on two
benchmark datasets demonstrate the superior performance of our approach on both
tasks over a variety of state-of-the-art methods. Project page:
https://ericguo5513.github.io/TM2T/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WPPG Net: A Non-contact Video Based Heart Rate Extraction Network Framework with Compatible Training Capability. (arXiv:2207.01697v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01697">
<div class="article-summary-box-inner">
<span><p>Our facial skin presents subtle color change known as remote
Photoplethysmography (rPPG) signal, from which we could extract the heart rate
of the subject. Recently many deep learning methods and related datasets on
rPPG signal extraction are proposed. However, because of the time consumption
blood flowing through our body and other factors, label waves such as BVP
signals have uncertain delays with real rPPG signals in some datasets, which
results in the difficulty on training of networks which output predicted rPPG
waves directly. In this paper, by analyzing the common characteristics on
rhythm and periodicity of rPPG signals and label waves, we propose a whole set
of training methodology which wraps these networks so that they could remain
efficient when be trained at the presence of frequent uncertain delay in
datasets and gain more precise and robust heart rate prediction results than
other delay-free rPPG extraction methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangled Action Recognition with Knowledge Bases. (arXiv:2207.01708v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01708">
<div class="article-summary-box-inner">
<span><p>Action in video usually involves the interaction of human with objects.
Action labels are typically composed of various combinations of verbs and
nouns, but we may not have training data for all possible combinations. In this
paper, we aim to improve the generalization ability of the compositional action
recognition model to novel verbs or novel nouns that are unseen during training
time, by leveraging the power of knowledge graphs. Previous work utilizes
verb-noun compositional action nodes in the knowledge graph, making it
inefficient to scale since the number of compositional action nodes grows
quadratically with respect to the number of verbs and nouns. To address this
issue, we propose our approach: Disentangled Action Recognition with
Knowledge-bases (DARK), which leverages the inherent compositionality of
actions. DARK trains a factorized model by first extracting disentangled
feature representations for verbs and nouns, and then predicting classification
weights using relations in external knowledge graphs. The type constraint
between verb and noun is extracted from external knowledge bases and finally
applied when composing actions. DARK has better scalability in the number of
objects and verbs, and achieves state-of-the-art performance on the Charades
dataset. We further propose a new benchmark split based on the Epic-kitchen
dataset which is an order of magnitude bigger in the numbers of classes and
samples, and benchmark various models on this benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Fine-Grained Sketch-Based Image Retrieval. (arXiv:2207.01723v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01723">
<div class="article-summary-box-inner">
<span><p>The recent focus on Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) has
shifted towards generalising a model to new categories without any training
data from them. In real-world applications, however, a trained FG-SBIR model is
often applied to both new categories and different human sketchers, i.e.,
different drawing styles. Although this complicates the generalisation problem,
fortunately, a handful of examples are typically available, enabling the model
to adapt to the new category/style. In this paper, we offer a novel perspective
-- instead of asking for a model that generalises, we advocate for one that
quickly adapts, with just very few samples during testing (in a few-shot
manner). To solve this new problem, we introduce a novel model-agnostic
meta-learning (MAML) based framework with several key modifications: (1) As a
retrieval task with a margin-based contrastive loss, we simplify the MAML
training in the inner loop to make it more stable and tractable. (2) The margin
in our contrastive loss is also meta-learned with the rest of the model. (3)
Three additional regularisation losses are introduced in the outer loop, to
make the meta-learned FG-SBIR model more effective for category/style
adaptation. Extensive experiments on public datasets suggest a large gain over
generalisation and zero-shot based approaches, and a few strong few-shot
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Much More Data Do I Need? Estimating Requirements for Downstream Tasks. (arXiv:2207.01725v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01725">
<div class="article-summary-box-inner">
<span><p>Given a small training data set and a learning algorithm, how much more data
is necessary to reach a target validation or test performance? This question is
of critical importance in applications such as autonomous driving or medical
imaging where collecting data is expensive and time-consuming. Overestimating
or underestimating data requirements incurs substantial costs that could be
avoided with an adequate budget. Prior work on neural scaling laws suggest that
the power-law function can fit the validation performance curve and extrapolate
it to larger data set sizes. We find that this does not immediately translate
to the more difficult downstream task of estimating the required data set size
to meet a target performance. In this work, we consider a broad class of
computer vision tasks and systematically investigate a family of functions that
generalize the power-law function to allow for better estimation of data
requirements. Finally, we show that incorporating a tuned correction factor and
collecting over multiple rounds significantly improves the performance of the
data estimators. Using our guidelines, practitioners can accurately estimate
data requirements of machine learning systems to gain savings in both
development time and data acquisition costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are metrics measuring what they should? An evaluation of image captioning task metrics. (arXiv:2207.01733v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01733">
<div class="article-summary-box-inner">
<span><p>Image Captioning is a current research task to describe the image content
using the objects and their relationships in the scene. To tackle this task,
two important research areas are used, artificial vision, and natural language
processing. In Image Captioning, as in any computational intelligence task, the
performance metrics are crucial for knowing how well (or bad) a method
performs. In recent years, it has been observed that classical metrics based on
n-grams are insufficient to capture the semantics and the critical meaning to
describe the content in an image. Looking to measure how well or not the set of
current and more recent metrics are doing, in this manuscript, we present an
evaluation of several kinds of Image Captioning metrics and a comparison
between them using the well-known MS COCO dataset. For this, we designed two
scenarios; 1) a set of artificially build captions with several quality, and 2)
a comparison of some state-of-the-art Image Captioning methods. We tried to
answer the questions: Are the current metrics helping to produce high quality
captions? How do actual metrics compare to each other? What are the metrics
really measuring?
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anomaly-aware multiple instance learning for rare anemia disorder classification. (arXiv:2207.01742v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01742">
<div class="article-summary-box-inner">
<span><p>Deep learning-based classification of rare anemia disorders is challenged by
the lack of training data and instance-level annotations. Multiple Instance
Learning (MIL) has shown to be an effective solution, yet it suffers from low
accuracy and limited explainability. Although the inclusion of attention
mechanisms has addressed these issues, their effectiveness highly depends on
the amount and diversity of cells in the training samples. Consequently, the
poor machine learning performance on rare anemia disorder classification from
blood samples remains unresolved. In this paper, we propose an interpretable
pooling method for MIL to address these limitations. By benefiting from
instance-level information of negative bags (i.e., homogeneous benign cells
from healthy individuals), our approach increases the contribution of anomalous
instances. We show that our strategy outperforms standard MIL classification
algorithms and provides a meaningful explanation behind its decisions.
Moreover, it can denote anomalous instances of rare blood diseases that are not
seen during the training phase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention Guided Network for Salient Object Detection in Optical Remote Sensing Images. (arXiv:2207.01755v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01755">
<div class="article-summary-box-inner">
<span><p>Due to the extreme complexity of scale and shape as well as the uncertainty
of the predicted location, salient object detection in optical remote sensing
images (RSI-SOD) is a very difficult task. The existing SOD methods can satisfy
the detection performance for natural scene images, but they are not well
adapted to RSI-SOD due to the above-mentioned image characteristics in remote
sensing images. In this paper, we propose a novel Attention Guided Network
(AGNet) for SOD in optical RSIs, including position enhancement stage and
detail refinement stage. Specifically, the position enhancement stage consists
of a semantic attention module and a contextual attention module to accurately
describe the approximate location of salient objects. The detail refinement
stage uses the proposed self-refinement module to progressively refine the
predicted results under the guidance of attention and reverse attention. In
addition, the hybrid loss is applied to supervise the training of the network,
which can improve the performance of the model from three perspectives of
pixel, region and statistics. Extensive experiments on two popular benchmarks
demonstrate that AGNet achieves competitive performance compared to other
state-of-the-art methods. The code will be available at
https://github.com/NuaaYH/AGNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Domain Adaptive Object Detector. (arXiv:2207.01756v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01756">
<div class="article-summary-box-inner">
<span><p>Universal domain adaptive object detection (UniDAOD)is more challenging than
domain adaptive object detection (DAOD) since the label space of the source
domain may not be the same as that of the target and the scale of objects in
the universal scenarios can vary dramatically (i.e, category shift and scale
shift). To this end, we propose US-DAF, namely Universal Scale-Aware Domain
Adaptive Faster RCNN with Multi-Label Learning, to reduce the negative transfer
effect during training while maximizing transferability as well as
discriminability in both domains under a variety of scales. Specifically, our
method is implemented by two modules: 1) We facilitate the feature alignment of
common classes and suppress the interference of private classes by designing a
Filter Mechanism module to overcome the negative transfer caused by category
shift. 2) We fill the blank of scale-aware adaptation in object detection by
introducing a new Multi-Label Scale-Aware Adapter to perform individual
alignment between the corresponding scale for two domains. Experiments show
that US-DAF achieves state-of-the-art results on three scenarios (i.e,
Open-Set, Partial-Set, and Closed-Set) and yields 7.1% and 5.9% relative
improvement on benchmark datasets Clipart1k and Watercolor in particular.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FDVTS's Solution for 2nd COV19D Competition on COVID-19 Detection and Severity Analysis. (arXiv:2207.01758v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01758">
<div class="article-summary-box-inner">
<span><p>This paper presents our solution for the 2nd COVID-19 Competition, occurring
in the framework of the AIMIA Workshop in the European Conference on Computer
Vision (ECCV 2022). In our approach, we employ an effective 3D Contrastive
Mixup Classification network for COVID-19 diagnosis on chest CT images, which
is composed of contrastive representation learning and mixup classification.
For the COVID-19 detection challenge, our approach reaches 0.9245 macro F1
score on 484 validation CT scans, which significantly outperforms the baseline
method by 16.5%. In the COVID-19 severity detection challenge, our approach
achieves 0.7186 macro F1 score on 61 validation samples, which also surpasses
the baseline by 8.86%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GP22: A Car Styling Dataset for Automotive Designers. (arXiv:2207.01760v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01760">
<div class="article-summary-box-inner">
<span><p>An automated design data archiving could reduce the time wasted by designers
from working creatively and effectively. Though many datasets on classifying,
detecting, and instance segmenting on car exterior exist, these large datasets
are not relevant for design practices as the primary purpose lies in autonomous
driving or vehicle verification. Therefore, we release GP22, composed of car
styling features defined by automotive designers. The dataset contains 1480 car
side profile images from 37 brands and ten car segments. It also contains
annotations of design features that follow the taxonomy of the car exterior
design features defined in the eye of the automotive designer. We trained the
baseline model using YOLO v5 as the design feature detection model with the
dataset. The presented model resulted in an mAP score of 0.995 and a recall of
0.984. Furthermore, exploration of the model performance on sketches and
rendering images of the car side profile implies the scalability of the dataset
for design purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rank-Based Filter Pruning for Real-Time UAV Tracking. (arXiv:2207.01768v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01768">
<div class="article-summary-box-inner">
<span><p>Unmanned aerial vehicle (UAV) tracking has wide potential applications in
such as agriculture, navigation, and public security. However, the limitations
of computing resources, battery capacity, and maximum load of UAV hinder the
deployment of deep learning-based tracking algorithms on UAV. Consequently,
discriminative correlation filters (DCF) trackers stand out in the UAV tracking
community because of their high efficiency. However, their precision is usually
much lower than trackers based on deep learning. Model compression is a
promising way to narrow the gap (i.e., effciency, precision) between DCF- and
deep learning- based trackers, which has not caught much attention in UAV
tracking. In this paper, we propose the P-SiamFC++ tracker, which is the first
to use rank-based filter pruning to compress the SiamFC++ model, achieving a
remarkable balance between efficiency and precision. Our method is general and
may encourage further studies on UAV tracking with model compression. Extensive
experiments on four UAV benchmarks, including UAV123@10fps, DTB70, UAVDT and
Vistrone2018, show that P-SiamFC++ tracker significantly outperforms
state-of-the-art UAV tracking methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SESS: Saliency Enhancing with Scaling and Sliding. (arXiv:2207.01769v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01769">
<div class="article-summary-box-inner">
<span><p>High-quality saliency maps are essential in several machine learning
application areas including explainable AI and weakly supervised object
detection and segmentation. Many techniques have been developed to generate
better saliency using neural networks. However, they are often limited to
specific saliency visualisation methods or saliency issues. We propose a novel
saliency enhancing approach called SESS (Saliency Enhancing with Scaling and
Sliding). It is a method and model agnostic extension to existing saliency map
generation methods. With SESS, existing saliency approaches become robust to
scale variance, multiple occurrences of target objects, presence of distractors
and generate less noisy and more discriminative saliency maps. SESS improves
saliency by fusing saliency maps extracted from multiple patches at different
scales from different areas, and combines these individual maps using a novel
fusion scheme that incorporates channel-wise weights and spatial weighted
average. To improve efficiency, we introduce a pre-filtering step that can
exclude uninformative saliency maps to improve efficiency while still enhancing
overall results. We evaluate SESS on object recognition and detection
benchmarks where it achieves significant improvement. The code is released
publicly to enable researchers to verify performance and further development.
Code is available at: https://github.com/neouyghur/SESS
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object-Level Targeted Selection via Deep Template Matching. (arXiv:2207.01778v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01778">
<div class="article-summary-box-inner">
<span><p>Retrieving images with objects that are semantically similar to objects of
interest (OOI) in a query image has many practical use cases. A few examples
include fixing failures like false negatives/positives of a learned model or
mitigating class imbalance in a dataset. The targeted selection task requires
finding the relevant data from a large-scale pool of unlabeled data. Manual
mining at this scale is infeasible. Further, the OOI are often small and occupy
less than 1% of image area, are occluded, and co-exist with many semantically
different objects in cluttered scenes. Existing semantic image retrieval
methods often focus on mining for larger sized geographical landmarks, and/or
require extra labeled data, such as images/image-pairs with similar objects,
for mining images with generic objects. We propose a fast and robust template
matching algorithm in the DNN feature space, that retrieves semantically
similar images at the object-level from a large unlabeled pool of data. We
project the region(s) around the OOI in the query image to the DNN feature
space for use as the template. This enables our method to focus on the
semantics of the OOI without requiring extra labeled data. In the context of
autonomous driving, we evaluate our system for targeted selection by using
failure cases of object detectors as OOI. We demonstrate its efficacy on a
large unlabeled dataset with 2.2M images and show high recall in mining for
images with small-sized OOI. We compare our method against a well-known
semantic image retrieval method, which also does not require extra labeled
data. Lastly, we show that our method is flexible and retrieves images with one
or more semantically different co-occurring OOI seamlessly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Part Assembly Generation with Instance Encoded Transformer. (arXiv:2207.01779v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01779">
<div class="article-summary-box-inner">
<span><p>It is desirable to enable robots capable of automatic assembly. Structural
understanding of object parts plays a crucial role in this task yet remains
relatively unexplored. In this paper, we focus on the setting of furniture
assembly from a complete set of part geometries, which is essentially a 6-DoF
part pose estimation problem. We propose a multi-layer transformer-based
framework that involves geometric and relational reasoning between parts to
update the part poses iteratively. We carefully design a unique instance
encoding to solve the ambiguity between geometrically-similar parts so that all
parts can be distinguished. In addition to assembling from scratch, we extend
our framework to a new task called in-process part assembly. Analogous to
furniture maintenance, it requires robots to continue with unfinished products
and assemble the remaining parts into appropriate positions. Our method
achieves far more than 10% improvements over the current state-of-the-art in
multiple metrics on the public PartNet dataset. Extensive experiments and
quantitative comparisons demonstrate the effectiveness of the proposed
framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A deep cascade of ensemble of dual domain networks with gradient-based T1 assistance and perceptual refinement for fast MRI reconstruction. (arXiv:2207.01791v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01791">
<div class="article-summary-box-inner">
<span><p>Deep learning networks have shown promising results in fast magnetic
resonance imaging (MRI) reconstruction. In our work, we develop deep networks
to further improve the quantitative and the perceptual quality of
reconstruction. To begin with, we propose reconsynergynet (RSN), a network that
combines the complementary benefits of independently operating on both the
image and the Fourier domain. For a single-coil acquisition, we introduce deep
cascade RSN (DC-RSN), a cascade of RSN blocks interleaved with data fidelity
(DF) units. Secondly, we improve the structure recovery of DC-RSN for T2
weighted Imaging (T2WI) through assistance of T1 weighted imaging (T1WI), a
sequence with short acquisition time. T1 assistance is provided to DC-RSN
through a gradient of log feature (GOLF) fusion. Furthermore, we propose
perceptual refinement network (PRN) to refine the reconstructions for better
visual information fidelity (VIF), a metric highly correlated to radiologists
opinion on the image quality. Lastly, for multi-coil acquisition, we propose
variable splitting RSN (VS-RSN), a deep cascade of blocks, each block
containing RSN, multi-coil DF unit, and a weighted average module. We
extensively validate our models DC-RSN and VS-RSN for single-coil and
multi-coil acquisitions and report the state-of-the-art performance. We obtain
a SSIM of 0.768, 0.923, 0.878 for knee single-coil-4x, multi-coil-4x, and
multi-coil-8x in fastMRI. We also conduct experiments to demonstrate the
efficacy of GOLF based T1 assistance and PRN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-agnostic Defense against Adversarial Patch Attacks. (arXiv:2207.01795v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01795">
<div class="article-summary-box-inner">
<span><p>Adversarial patch attacks mislead neural networks by injecting adversarial
pixels within a designated local region. Patch attacks can be highly effective
in a variety of tasks and physically realizable via attachment (e.g. a sticker)
to the real-world objects. Despite the diversity in attack patterns,
adversarial patches tend to be highly textured and different in appearance from
natural images. We exploit this property and present PatchZero, a task-agnostic
defense against white-box adversarial patches. Specifically, our defense
detects the adversarial pixels and "zeros out" the patch region by repainting
with mean pixel values. We formulate the patch detection problem as a semantic
segmentation task such that our model can generalize to patches of any size and
shape. We further design a two-stage adversarial training scheme to defend
against the stronger adaptive attacks. We thoroughly evaluate PatchZero on the
image classification (ImageNet, RESISC45), object detection (PASCAL VOC), and
video classification (UCF101) datasets. Our method achieves SOTA robust
accuracy without any degradation in the benign performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Parametric 3D Filters for Joint Video Denoising and Illumination Enhancement in Video Super Resolution. (arXiv:2207.01797v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01797">
<div class="article-summary-box-inner">
<span><p>Despite the quality improvement brought by the recent methods, video
super-resolution (SR) is still very challenging, especially for videos that are
low-light and noisy. The current best solution is to subsequently employ best
models of video SR, denoising, and illumination enhancement, but doing so often
lowers the image quality, due to the inconsistency between the models. This
paper presents a new parametric representation called the Deep Parametric 3D
Filters (DP3DF), which incorporates local spatiotemporal information to enable
simultaneous denoising, illumination enhancement, and SR efficiently in a
single encoder-and-decoder network. Also, a dynamic residual frame is jointly
learned with the DP3DF via a shared backbone to further boost the SR quality.
We performed extensive experiments, including a large-scale user study, to show
our method's effectiveness. Our method consistently surpasses the best
state-of-the-art methods on all the challenging real datasets with top PSNR and
user ratings, yet having a very fast run time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GSMFlow: Generation Shifts Mitigating Flow for Generalized Zero-Shot Learning. (arXiv:2207.01798v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01798">
<div class="article-summary-box-inner">
<span><p>Generalized Zero-Shot Learning (GZSL) aims to recognize images from both the
seen and unseen classes by transferring semantic knowledge from seen to unseen
classes. It is a promising solution to take the advantage of generative models
to hallucinate realistic unseen samples based on the knowledge learned from the
seen classes. However, due to the generation shifts, the synthesized samples by
most existing methods may drift from the real distribution of the unseen data.
To address this issue, we propose a novel flow-based generative framework that
consists of multiple conditional affine coupling layers for learning unseen
data generation. Specifically, we discover and address three potential problems
that trigger the generation shifts, i.e., semantic inconsistency, variance
collapse, and structure disorder. First, to enhance the reflection of the
semantic information in the generated samples, we explicitly embed the semantic
information into the transformation in each conditional affine coupling layer.
Second, to recover the intrinsic variance of the real unseen features, we
introduce a boundary sample mining strategy with entropy maximization to
discover more difficult visual variants of semantic prototypes and hereby
adjust the decision boundary of the classifiers. Third, a relative positioning
strategy is proposed to revise the attribute embeddings, guiding them to fully
preserve the inter-class geometric structure and further avoid structure
disorder in the semantic space. Extensive experimental results on four GZSL
benchmark datasets demonstrate that GSMFlow achieves the state-of-the-art
performance on GZSL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReMix: A General and Efficient Framework for Multiple Instance Learning based Whole Slide Image Classification. (arXiv:2207.01805v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01805">
<div class="article-summary-box-inner">
<span><p>Whole slide image (WSI) classification often relies on deep weakly supervised
multiple instance learning (MIL) methods to handle gigapixel resolution images
and slide-level labels. Yet the decent performance of deep learning comes from
harnessing massive datasets and diverse samples, urging the need for efficient
training pipelines for scaling to large datasets and data augmentation
techniques for diversifying samples. However, current MIL-based WSI
classification pipelines are memory-expensive and computation-inefficient since
they usually assemble tens of thousands of patches as bags for computation. On
the other hand, despite their popularity in other tasks, data augmentations are
unexplored for WSI MIL frameworks. To address them, we propose ReMix, a general
and efficient framework for MIL based WSI classification. It comprises two
steps: reduce and mix. First, it reduces the number of instances in WSI bags by
substituting instances with instance prototypes, i.e., patch cluster centroids.
Then, we propose a ``Mix-the-bag'' augmentation that contains four online,
stochastic and flexible latent space augmentations. It brings diverse and
reliable class-identity-preserving semantic changes in the latent space while
enforcing semantic-perturbation invariance. We evaluate ReMix on two public
datasets with two state-of-the-art MIL methods. In our experiments, consistent
improvements in precision, accuracy, and recall have been achieved but with
orders of magnitude reduced training time and memory consumption, demonstrating
ReMix's effectiveness and efficiency. Code is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aesthetic Attribute Assessment of Images Numerically on Mixed Multi-attribute Datasets. (arXiv:2207.01806v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01806">
<div class="article-summary-box-inner">
<span><p>With the continuous development of social software and multimedia technology,
images have become a kind of important carrier for spreading information and
socializing. How to evaluate an image comprehensively has become the focus of
recent researches. The traditional image aesthetic assessment methods often
adopt single numerical overall assessment scores, which has certain
subjectivity and can no longer meet the higher aesthetic requirements. In this
paper, we construct an new image attribute dataset called aesthetic mixed
dataset with attributes(AMD-A) and design external attribute features for
fusion. Besides, we propose a efficient method for image aesthetic attribute
assessment on mixed multi-attribute dataset and construct a multitasking
network architecture by using the EfficientNet-B0 as the backbone network. Our
model can achieve aesthetic classification, overall scoring and attribute
scoring. In each sub-network, we improve the feature extraction through ECA
channel attention module. As for the final overall scoring, we adopt the idea
of the teacher-student network and use the classification sub-network to guide
the aesthetic overall fine-grain regression. Experimental results, using the
MindSpore, show that our proposed method can effectively improve the
performance of the aesthetic overall and attribute assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deriving Surface Resistivity from Polarimetric SAR Data Using Dual-Input UNet. (arXiv:2207.01811v1 [physics.geo-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01811">
<div class="article-summary-box-inner">
<span><p>Traditional survey methods for finding surface resistivity are time-consuming
and labor intensive. Very few studies have focused on finding the
resistivity/conductivity using remote sensing data and deep learning
techniques. In this line of work, we assessed the correlation between surface
resistivity and Synthetic Aperture Radar (SAR) by applying various deep
learning methods and tested our hypothesis in the Coso Geothermal Area, USA.
For detecting the resistivity, L-band full polarimetric SAR data acquired by
UAVSAR were used, and MT (Magnetotellurics) inverted resistivity data of the
area were used as the ground truth. We conducted experiments to compare various
deep learning architectures and suggest the use of Dual Input UNet (DI-UNet)
architecture. DI-UNet uses a deep learning architecture to predict the
resistivity using full polarimetric SAR data by promising a quick survey
addition to the traditional method. Our proposed approach accomplished improved
outcomes for the mapping of MT resistivity from SAR data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Explainable and Fine-Grained 3D Grounding through Referring Textual Phrases. (arXiv:2207.01821v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01821">
<div class="article-summary-box-inner">
<span><p>Recent progress on 3D scene understanding has explored visual grounding
(3DVG) to localize a target object through a language description. However,
existing methods only consider the dependency between the entire sentence and
the target object, thus ignoring fine-grained relationships between contexts
and non-target ones. In this paper, we extend 3DVG to a more reliable and
explainable task, called 3D Phrase Aware Grounding (3DPAG). The 3DPAG task aims
to localize the target object in the 3D scenes by explicitly identifying all
phrase-related objects and then conducting reasoning according to contextual
phrases. To tackle this problem, we label about 400K phrase-level annotations
from 170K sentences in available 3DVG datasets, i.e., Nr3D, Sr3D and ScanRefer.
By tapping on these developed datasets, we propose a novel framework, i.e.,
PhraseRefer, which conducts phrase-aware and object-level representation
learning through phrase-object alignment optimization as well as
phrase-specific pre-training. In our setting, we extend previous 3DVG methods
to the phrase-aware scenario and provide metrics to measure the explainability
of the 3DPAG task. Extensive results confirm that 3DPAG effectively boosts the
3DVG, and PhraseRefer achieves state-of-the-arts across three datasets, i.e.,
63.0%, 54.4% and 55.5% overall accuracy on Sr3D, Nr3D and ScanRefer,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scene-Aware Prompt for Multi-modal Dialogue Understanding and Generation. (arXiv:2207.01823v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01823">
<div class="article-summary-box-inner">
<span><p>This paper introduces the schemes of Team LingJing's experiments in
NLPCC-2022-Shared-Task-4 Multi-modal Dialogue Understanding and Generation
(MDUG). The MDUG task can be divided into two phases: multi-modal context
understanding and response generation. To fully leverage the visual information
for both scene understanding and dialogue generation, we propose the
scene-aware prompt for the MDUG task. Specifically, we utilize the
multi-tasking strategy for jointly modelling the scene- and session-
multi-modal understanding. The visual captions are adopted to aware the scene
information, while the fixed-type templated prompt based on the scene- and
session-aware labels are used to further improve the dialogue generation
performance. Extensive experimental results show that the proposed method has
achieved state-of-the-art (SOTA) performance compared with other competitive
methods, where we rank the 1-st in all three subtasks in this MDUG competition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Local Implicit Fourier Representation for Image Warping. (arXiv:2207.01831v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01831">
<div class="article-summary-box-inner">
<span><p>Image warping aims to reshape images defined on rectangular grids into
arbitrary shapes. Recently, implicit neural functions have shown remarkable
performances in representing images in a continuous manner. However, a
standalone multi-layer perceptron suffers from learning high-frequency Fourier
coefficients. In this paper, we propose a local texture estimator for image
warping (LTEW) followed by an implicit neural representation to deform images
into continuous shapes. Local textures estimated from a deep super-resolution
(SR) backbone are multiplied by locally-varying Jacobian matrices of a
coordinate transformation to predict Fourier responses of a warped image. Our
LTEW-based neural function outperforms existing warping methods for
asymmetric-scale SR and homography transform. Furthermore, our algorithm well
generalizes arbitrary coordinate transformations, such as homography transform
with a large magnification factor and equirectangular projection (ERP)
perspective transform, which are not provided in training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ORF-Net: Deep Omni-supervised Rib Fracture Detection from Chest CT Scans. (arXiv:2207.01842v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01842">
<div class="article-summary-box-inner">
<span><p>Most of the existing object detection works are based on the bounding box
annotation: each object has a precise annotated box. However, for rib
fractures, the bounding box annotation is very labor-intensive and
time-consuming because radiologists need to investigate and annotate the rib
fractures on a slice-by-slice basis. Although a few studies have proposed
weakly-supervised methods or semi-supervised methods, they could not handle
different forms of supervision simultaneously. In this paper, we proposed a
novel omni-supervised object detection network, which can exploit multiple
different forms of annotated data to further improve the detection performance.
Specifically, the proposed network contains an omni-supervised detection head,
in which each form of annotation data corresponds to a unique classification
branch. Furthermore, we proposed a dynamic label assignment strategy for
different annotated forms of data to facilitate better learning for each
branch. Moreover, we also design a confidence-aware classification loss to
emphasize the samples with high confidence and further improve the model's
performance. Extensive experiments conducted on the testing dataset show our
proposed method outperforms other state-of-the-art approaches consistently,
demonstrating the efficacy of deep omni-supervised learning on improving rib
fracture detection performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Representation Learning via Adaptive Context Pooling. (arXiv:2207.01844v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01844">
<div class="article-summary-box-inner">
<span><p>Self-attention mechanisms model long-range context by using pairwise
attention between all input tokens. In doing so, they assume a fixed attention
granularity defined by the individual tokens (e.g., text characters or image
pixels), which may not be optimal for modeling complex dependencies at higher
levels. In this paper, we propose ContextPool to address this problem by
adapting the attention granularity for each token. Inspired by the success of
ConvNets that are combined with pooling to capture long-range dependencies, we
learn to pool neighboring features for each token before computing attention in
a given attention layer. The pooling weights and support size are adaptively
determined, allowing the pooled features to encode meaningful context with
varying scale. We show that ContextPool makes attention models more expressive,
achieving strong performance often with fewer layers and thus significantly
reduced cost. Experiments validate that our ContextPool module, when plugged
into transformer models, matches or surpasses state-of-the-art performance
using less compute on several language and image benchmarks, outperforms recent
works with learned context sizes or sparse attention patterns, and is also
applicable to ConvNets for efficient feature learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian approaches for Quantifying Clinicians' Variability in Medical Image Quantification. (arXiv:2207.01868v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01868">
<div class="article-summary-box-inner">
<span><p>Medical imaging, including MRI, CT, and Ultrasound, plays a vital role in
clinical decisions. Accurate segmentation is essential to measure the structure
of interest from the image. However, manual segmentation is highly
operator-dependent, which leads to high inter and intra-variability of
quantitative measurements. In this paper, we explore the feasibility that
Bayesian predictive distribution parameterized by deep neural networks can
capture the clinicians' inter-intra variability. By exploring and analyzing
recently emerged approximate inference schemes, we evaluate whether approximate
Bayesian deep learning with the posterior over segmentations can learn
inter-intra rater variability both in segmentation and clinical measurements.
The experiments are performed with two different imaging modalities: MRI and
ultrasound. We empirically demonstrated that Bayesian predictive distribution
parameterized by deep neural networks could approximate the clinicians'
inter-intra variability. We show a new perspective in analyzing medical images
quantitatively by providing clinical measurement uncertainty.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distance Matters in Human-Object Interaction Detection. (arXiv:2207.01869v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01869">
<div class="article-summary-box-inner">
<span><p>Human-Object Interaction (HOI) detection has received considerable attention
in the context of scene understanding. Despite the growing progress on
benchmarks, we realize that existing methods often perform unsatisfactorily on
distant interactions, where the leading causes are two-fold: 1) Distant
interactions are by nature more difficult to recognize than close ones. A
natural scene often involves multiple humans and objects with intricate spatial
relations, making the interaction recognition for distant human-object largely
affected by complex visual context. 2) Insufficient number of distant
interactions in benchmark datasets results in under-fitting on these instances.
To address these problems, in this paper, we propose a novel two-stage method
for better handling distant interactions in HOI detection. One essential
component in our method is a novel Far Near Distance Attention module. It
enables information propagation between humans and objects, whereby the spatial
distance is skillfully taken into consideration. Besides, we devise a novel
Distance-Aware loss function which leads the model to focus more on distant yet
rare interactions. We conduct extensive experiments on two challenging datasets
- HICO-DET and V-COCO. The results demonstrate that the proposed method can
surpass existing approaches by a large margin, resulting in new
state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latents2Segments: Disentangling the Latent Space of Generative Models for Semantic Segmentation of Face Images. (arXiv:2207.01871v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01871">
<div class="article-summary-box-inner">
<span><p>With the advent of an increasing number of Augmented and Virtual Reality
applications that aim to perform meaningful and controlled style edits on
images of human faces, the impetus for the task of parsing face images to
produce accurate and fine-grained semantic segmentation maps is more than ever
before. Few State of the Art (SOTA) methods which solve this problem, do so by
incorporating priors with respect to facial structure or other face attributes
such as expression and pose in their deep classifier architecture. Our
endeavour in this work is to do away with the priors and complex pre-processing
operations required by SOTA multi-class face segmentation models by reframing
this operation as a downstream task post infusion of disentanglement with
respect to facial semantic regions of interest (ROIs) in the latent space of a
Generative Autoencoder model. We present results for our model's performance on
the CelebAMask-HQ and HELEN datasets. The encoded latent space of our model
achieves significantly higher disentanglement with respect to semantic ROIs
than that of other SOTA works. Moreover, it achieves a 13\% faster inference
rate and comparable accuracy with respect to the publicly available SOTA for
the downstream task of semantic segmentation of face images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-based Uneven BEV Representation Learning with Polar Rasterization and Surface Estimation. (arXiv:2207.01878v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01878">
<div class="article-summary-box-inner">
<span><p>In this work, we propose PolarBEV for vision-based uneven BEV representation
learning. To adapt to the foreshortening effect of camera imaging, we rasterize
the BEV space both angularly and radially, and introduce polar embedding
decomposition to model the associations among polar grids. Polar grids are
rearranged to an array-like regular representation for efficient processing.
Besides, to determine the 2D-to-3D correspondence, we iteratively update the
BEV surface based on a hypothetical plane, and adopt height-based feature
transformation. PolarBEV keeps real-time inference speed on a single 2080Ti
GPU, and outperforms other methods for both BEV semantic segmentation and BEV
instance segmentation. Thorough ablations are presented to validate the design.
The code will be released at \url{https://github.com/SuperZ-Liu/PolarBEV}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MMGL: Multi-Scale Multi-View Global-Local Contrastive learning for Semi-supervised Cardiac Image Segmentation. (arXiv:2207.01883v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01883">
<div class="article-summary-box-inner">
<span><p>With large-scale well-labeled datasets, deep learning has shown significant
success in medical image segmentation. However, it is challenging to acquire
abundant annotations in clinical practice due to extensive expertise
requirements and costly labeling efforts. Recently, contrastive learning has
shown a strong capacity for visual representation learning on unlabeled data,
achieving impressive performance rivaling supervised learning in many domains.
In this work, we propose a novel multi-scale multi-view global-local
contrastive learning (MMGL) framework to thoroughly explore global and local
features from different scales and views for robust contrastive learning
performance, thereby improving segmentation performance with limited
annotations. Extensive experiments on the MM-WHS dataset demonstrate the
effectiveness of MMGL framework on semi-supervised cardiac image segmentation,
outperforming the state-of-the-art contrastive learning methods by a large
margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Vocabulary Multi-Label Classification via Multi-modal Knowledge Transfer. (arXiv:2207.01887v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01887">
<div class="article-summary-box-inner">
<span><p>Real-world recognition system often encounters a plenty of unseen labels in
practice. To identify such unseen labels, multi-label zero-shot learning
(ML-ZSL) focuses on transferring knowledge by a pre-trained textual label
embedding (e.g., GloVe). However, such methods only exploit singlemodal
knowledge from a language model, while ignoring the rich semantic information
inherent in image-text pairs. Instead, recently developed open-vocabulary (OV)
based methods succeed in exploiting such information of image-text pairs in
object detection, and achieve impressive performance. Inspired by the success
of OV-based methods, we propose a novel open-vocabulary framework, named
multimodal knowledge transfer (MKT), for multi-label classification.
Specifically, our method exploits multi-modal knowledge of image-text pairs
based on a vision and language pretraining (VLP) model. To facilitate
transferring the imagetext matching ability of VLP model, knowledge
distillation is used to guarantee the consistency of image and label
embeddings, along with prompt tuning to further update the label embeddings. To
further recognize multiple objects, a simple but effective two-stream module is
developed to capture both local and global features. Extensive experimental
results show that our method significantly outperforms state-of-theart methods
on public benchmark datasets. Code will be available at
https://github.com/seanhe97/MKT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACT-Net: Asymmetric Co-Teacher Network for Semi-supervised Memory-efficient Medical Image Segmentation. (arXiv:2207.01900v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01900">
<div class="article-summary-box-inner">
<span><p>While deep models have shown promising performance in medical image
segmentation, they heavily rely on a large amount of well-annotated data, which
is difficult to access, especially in clinical practice. On the other hand,
high-accuracy deep models usually come in large model sizes, limiting their
employment in real scenarios. In this work, we propose a novel asymmetric
co-teacher framework, ACT-Net, to alleviate the burden on both expensive
annotations and computational costs for semi-supervised knowledge distillation.
We advance teacher-student learning with a co-teacher network to facilitate
asymmetric knowledge distillation from large models to small ones by
alternating student and teacher roles, obtaining tiny but accurate models for
clinical employment. To verify the effectiveness of our ACT-Net, we employ the
ACDC dataset for cardiac substructure segmentation in our experiments.
Extensive experimental results demonstrate that ACT-Net outperforms other
knowledge distillation methods and achieves lossless segmentation performance
with 250x fewer parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial-Temporal Frequency Forgery Clue for Video Forgery Detection in VIS and NIR Scenario. (arXiv:2207.01906v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01906">
<div class="article-summary-box-inner">
<span><p>In recent years, with the rapid development of face editing and generation,
more and more fake videos are circulating on social media, which has caused
extreme public concerns. Existing face forgery detection methods based on
frequency domain find that the GAN forged images have obvious grid-like visual
artifacts in the frequency spectrum compared to the real images. But for
synthesized videos, these methods only confine to single frame and pay little
attention to the most discriminative part and temporal frequency clue among
different frames. To take full advantage of the rich information in video
sequences, this paper performs video forgery detection on both spatial and
temporal frequency domains and proposes a Discrete Cosine Transform-based
Forgery Clue Augmentation Network (FCAN-DCT) to achieve a more comprehensive
spatial-temporal feature representation. FCAN-DCT consists of a backbone
network and two branches: Compact Feature Extraction (CFE) module and Frequency
Temporal Attention (FTA) module. We conduct thorough experimental assessments
on two visible light (VIS) based datasets WildDeepfake and Celeb-DF (v2), and
our self-built video forgery dataset DeepfakeNIR, which is the first video
forgery dataset on near-infrared modality. The experimental results demonstrate
the effectiveness of our method on detecting forgery videos in both VIS and NIR
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleFlow For Content-Fixed Image to Image Translation. (arXiv:2207.01909v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01909">
<div class="article-summary-box-inner">
<span><p>Image-to-image (I2I) translation is a challenging topic in computer vision.
We divide this problem into three tasks: strongly constrained translation,
normally constrained translation, and weakly constrained translation. The
constraint here indicates the extent to which the content or semantic
information in the original image is preserved. Although previous approaches
have achieved good performance in weakly constrained tasks, they failed to
fully preserve the content in both strongly and normally constrained tasks,
including photo-realism synthesis, style transfer, and colorization, etc. To
achieve content-preserving transfer in strongly constrained and normally
constrained tasks, we propose StyleFlow, a new I2I translation model that
consists of normalizing flows and a novel Style-Aware Normalization (SAN)
module. With the invertible network structure, StyleFlow first projects input
images into deep feature space in the forward pass, while the backward pass
utilizes the SAN module to perform content-fixed feature transformation and
then projects back to image space. Our model supports both image-guided
translation and multi-modal synthesis. We evaluate our model in several I2I
translation benchmarks, and the results show that the proposed model has
advantages over previous methods in both strongly constrained and normally
constrained tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Symbolic Reasoning in Hyperbolic Space for Deep Discriminative Models. (arXiv:2207.01916v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01916">
<div class="article-summary-box-inner">
<span><p>Explanations for \emph{black-box} models help us understand model decisions
as well as provide information on model biases and inconsistencies. Most of the
current explainability techniques provide a single level of explanation, often
in terms of feature importance scores or feature attention maps in input space.
Our focus is on explaining deep discriminative models at \emph{multiple levels
of abstraction}, from fine-grained to fully abstract explanations. We achieve
this by using the natural properties of \emph{hyperbolic geometry} to more
efficiently model a hierarchy of symbolic features and generate
\emph{hierarchical symbolic rules} as part of our explanations. Specifically,
for any given deep discriminative model, we distill the underpinning knowledge
by discretisation of the continuous latent space using vector quantisation to
form symbols, followed by a \emph{hyperbolic reasoning block} to induce an
\emph{abstraction tree}. We traverse the tree to extract explanations in terms
of symbolic rules and its corresponding visual semantics. We demonstrate the
effectiveness of our method on the MNIST and AFHQ high-resolution animal faces
dataset. Our framework is available at
\url{https://github.com/koriavinash1/SymbolicInterpretability}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLANCE: Global to Local Architecture-Neutral Concept-based Explanations. (arXiv:2207.01917v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01917">
<div class="article-summary-box-inner">
<span><p>Most of the current explainability techniques focus on capturing the
importance of features in input space. However, given the complexity of models
and data-generating processes, the resulting explanations are far from being
`complete', in that they lack an indication of feature interactions and
visualization of their `effect'. In this work, we propose a novel
twin-surrogate explainability framework to explain the decisions made by any
CNN-based image classifier (irrespective of the architecture). For this, we
first disentangle latent features from the classifier, followed by aligning
these features to observed/human-defined `context' features. These aligned
features form semantically meaningful concepts that are used for extracting a
causal graph depicting the `perceived' data-generating process, describing the
inter- and intra-feature interactions between unobserved latent features and
observed `context' features. This causal graph serves as a global model from
which local explanations of different forms can be extracted. Specifically, we
provide a generator to visualize the `effect' of interactions among features in
latent space and draw feature importance therefrom as local explanations. Our
framework utilizes adversarial knowledge distillation to faithfully learn a
representation from the classifiers' latent space and use it for extracting
visual explanations. We use the styleGAN-v2 architecture with an additional
regularization term to enforce disentanglement and alignment. We demonstrate
and evaluate explanations obtained with our framework on Morpho-MNIST and on
the FFHQ human faces dataset. Our framework is available at
\url{https://github.com/koriavinash1/GLANCE-Explanations}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vector Quantisation for Robust Segmentation. (arXiv:2207.01919v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01919">
<div class="article-summary-box-inner">
<span><p>The reliability of segmentation models in the medical domain depends on the
model's robustness to perturbations in the input space. Robustness is a
particular challenge in medical imaging exhibiting various sources of image
noise, corruptions, and domain shifts. Obtaining robustness is often attempted
via simulating heterogeneous environments, either heuristically in the form of
data augmentation or by learning to generate specific perturbations in an
adversarial manner. We propose and justify that learning a discrete
representation in a low dimensional embedding space improves robustness of a
segmentation model. This is achieved with a dictionary learning method called
vector quantisation. We use a set of experiments designed to analyse robustness
in both the latent and output space under domain shift and noise perturbations
in the input space. We adapt the popular UNet architecture, inserting a
quantisation block in the bottleneck. We demonstrate improved segmentation
accuracy and better robustness on three segmentation tasks. Code is available
at
\url{https://github.com/AinkaranSanthi/Vector-Quantisation-for-Robust-Segmentation}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FishFormer: Annulus Slicing-based Transformer for Fisheye Rectification with Efficacy Domain Exploration. (arXiv:2207.01925v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01925">
<div class="article-summary-box-inner">
<span><p>Numerous significant progress on fisheye image rectification has been
achieved through CNN. Nevertheless, constrained by a fixed receptive field, the
global distribution and the local symmetry of the distortion have not been
fully exploited. To leverage these two characteristics, we introduced
Fishformer that processes the fisheye image as a sequence to enhance global and
local perception. We tuned the Transformer according to the structural
properties of fisheye images. First, the uneven distortion distribution in
patches generated by the existing square slicing method confuses the network,
resulting in difficult training. Therefore, we propose an annulus slicing
method to maintain the consistency of the distortion in each patch, thus
perceiving the distortion distribution well. Second, we analyze that different
distortion parameters have their own efficacy domains. Hence, the perception of
the local area is as important as the global, but Transformer has a weakness
for local texture perception. Therefore, we propose a novel layer attention
mechanism to enhance the local perception and texture transfer. Our network
simultaneously implements global perception and focused local perception
decided by the different parameters. Extensive experiments demonstrate that our
method provides superior performance compared with state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Drone Detection and Tracking in Real-Time by Fusion of Different Sensing Modalities. (arXiv:2207.01927v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01927">
<div class="article-summary-box-inner">
<span><p>Automatic detection of flying drones is a key issue where its presence,
specially if unauthorized, can create risky situations or compromise security.
Here, we design and evaluate a multi-sensor drone detection system. In
conjunction with common video cameras and microphone sensors, we explore the
use of thermal infrared cameras, pointed out as a feasible and promising
solution that is scarcely addressed in the related literature. Our solution
integrates a fish-eye camera as well to monitor a wider part of the sky and
steer the other cameras towards objects of interest. The sensing solutions are
complemented with an ADS-B receiver, a GPS receiver, and a radar module,
although the latter has been not included in our final deployment due to its
limited detection range. The thermal camera is shown to be a feasible solution
as good as the video camera, even if the camera employed here has a lower
resolution. Two other novelties of our work are the creation of a new public
dataset of multi-sensor annotated data that expand the number of classes in
comparison to existing ones, as well as the study of the detector performance
as a function of the sensor-to-target distance. Sensor fusion is also explored,
showing that the system can be made more robust in this way, mitigating false
detections of the individual sensors
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Coding for Machines with Omnipotent Feature Learning. (arXiv:2207.01932v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01932">
<div class="article-summary-box-inner">
<span><p>Image Coding for Machines (ICM) aims to compress images for AI tasks analysis
rather than meeting human perception. Learning a kind of feature that is both
general (for AI tasks) and compact (for compression) is pivotal for its
success. In this paper, we attempt to develop an ICM framework by learning
universal features while also considering compression. We name such features as
omnipotent features and the corresponding framework as Omni-ICM. Considering
self-supervised learning (SSL) improves feature generalization, we integrate it
with the compression task into the Omni-ICM framework to learn omnipotent
features. However, it is non-trivial to coordinate semantics modeling in SSL
and redundancy removing in compression, so we design a novel information
filtering (IF) module between them by co-optimization of instance
distinguishment and entropy minimization to adaptively drop information that is
weakly related to AI tasks (e.g., some texture redundancy). Different from
previous task-specific solutions, Omni-ICM could directly support AI tasks
analysis based on the learned omnipotent features without joint training or
extra transformation. Albeit simple and intuitive, Omni-ICM significantly
outperforms existing traditional and learning-based codecs on multiple
fundamental vision tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Safe Semi-supervised Graph Convolution Network. (arXiv:2207.01960v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01960">
<div class="article-summary-box-inner">
<span><p>In the semi-supervised learning field, Graph Convolution Network (GCN), as a
variant model of GNN, has achieved promising results for non-Euclidean data by
introducing convolution into GNN. However, GCN and its variant models fail to
safely use the information of risk unlabeled data, which will degrade the
performance of semi-supervised learning. Therefore, we propose a Safe GCN
framework (Safe-GCN) to improve the learning performance. In the Safe-GCN, we
design an iterative process to label the unlabeled data. In each iteration, a
GCN and its supervised version(S-GCN) are learned to find the unlabeled data
with high confidence. The high-confidence unlabeled data and their pseudo
labels are then added to the label set. Finally, both added unlabeled data and
labeled ones are used to train a S-GCN which can achieve the safe exploration
of the risk unlabeled data and enable safe use of large numbers of unlabeled
data. The performance of Safe-GCN is evaluated on three well-known citation
network datasets and the obtained results demonstrate the effectiveness of the
proposed framework over several graph-based semi-supervised learning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DualAfford: Learning Collaborative Visual Affordance for Dual-gripper Object Manipulation. (arXiv:2207.01971v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01971">
<div class="article-summary-box-inner">
<span><p>It is essential yet challenging for future home-assistant robots to
understand and manipulate diverse 3D objects in daily human environments.
Towards building scalable systems that can perform diverse manipulation tasks
over various 3D shapes, recent works have advocated and demonstrated promising
results learning visual actionable affordance, which labels every point over
the input 3D geometry with an action likelihood of accomplishing the downstream
task (e.g., pushing or picking-up). However, these works only studied
single-gripper manipulation tasks, yet many real-world tasks require two hands
to achieve collaboratively. In this work, we propose a novel learning
framework, DualAfford, to learn collaborative affordance for dual-gripper
manipulation tasks. The core design of the approach is to reduce the quadratic
problem for two grippers into two disentangled yet interconnected subtasks for
efficient learning. Using the large-scale PartNet-Mobility and ShapeNet
datasets, we set up four benchmark tasks for dual-gripper manipulation.
Experiments prove the effectiveness and superiority of our method over three
baselines. Additional results and videos can be found at
https://hyperplane-lab.github.io/DualAfford .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding and Improving Group Normalization. (arXiv:2207.01972v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01972">
<div class="article-summary-box-inner">
<span><p>Various normalization layers have been proposed to help the training of
neural networks. Group Normalization (GN) is one of the effective and
attractive studies that achieved significant performances in the visual
recognition task. Despite the great success achieved, GN still has several
issues that may negatively impact neural network training. In this paper, we
introduce an analysis framework and discuss the working principles of GN in
affecting the training process of the neural network. From experimental
results, we conclude the real cause of GN's inferior performance against Batch
normalization (BN): 1) \textbf{unstable training performance}, 2) \textbf{more
sensitive} to distortion, whether it comes from external noise or perturbations
introduced by the regularization. In addition, we found that GN can only help
the neural network training in some specific period, unlike BN, which helps the
network throughout the training. To solve these issues, we propose a new
normalization layer built on top of GN, by incorporating the advantages of BN.
Experimental results on the image classification task demonstrated that the
proposed normalization layer outperforms the official GN to improve recognition
accuracy regardless of the batch sizes and stabilize the network training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Self-supervised Learning for Video Understanding. (arXiv:2207.01975v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01975">
<div class="article-summary-box-inner">
<span><p>The ubiquity of camera-enabled mobile devices has lead to large amounts of
unlabelled video data being produced at the edge. Although various
self-supervised learning (SSL) methods have been proposed to harvest their
latent spatio-temporal representations for task-specific training, practical
challenges including privacy concerns and communication costs prevent SSL from
being deployed at large scales. To mitigate these issues, we propose the use of
Federated Learning (FL) to the task of video SSL. In this work, we evaluate the
performance of current state-of-the-art (SOTA) video-SSL techniques and
identify their shortcomings when integrated into the large-scale FL setting
simulated with kinetics-400 dataset. We follow by proposing a novel federated
SSL framework for video, dubbed FedVSSL, that integrates different aggregation
strategies and partial weight updating. Extensive experiments demonstrate the
effectiveness and significance of FedVSSL as it outperforms the centralized
SOTA for the downstream retrieval task by 6.66% on UCF-101 and 5.13% on
HMDB-51.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Vocabulary 3D Detection via Image-level Class and Debiased Cross-modal Contrastive Learning. (arXiv:2207.01987v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01987">
<div class="article-summary-box-inner">
<span><p>Current point-cloud detection methods have difficulty detecting the
open-vocabulary objects in the real world, due to their limited generalization
capability. Moreover, it is extremely laborious and expensive to collect and
fully annotate a point-cloud detection dataset with numerous classes of
objects, leading to the limited classes of existing point-cloud datasets and
hindering the model to learn general representations to achieve open-vocabulary
point-cloud detection. As far as we know, we are the first to study the problem
of open-vocabulary 3D point-cloud detection. Instead of seeking a point-cloud
dataset with full labels, we resort to ImageNet1K to broaden the vocabulary of
the point-cloud detector. We propose OV-3DETIC, an Open-Vocabulary 3D DETector
using Image-level Class supervision. Specifically, we take advantage of two
modalities, the image modality for recognition and the point-cloud modality for
localization, to generate pseudo labels for unseen classes. Then we propose a
novel debiased cross-modal contrastive learning method to transfer the
knowledge from image modality to point-cloud modality during training. Without
hurting the latency during inference, OV-3DETIC makes the point-cloud detector
capable of achieving open-vocabulary detection. Extensive experiments
demonstrate that the proposed OV-3DETIC achieves at least 10.77 % mAP
improvement (absolute value) and 9.56 % mAP improvement (absolute value) by a
wide range of baselines on the SUN-RGBD dataset and ScanNet dataset,
respectively. Besides, we conduct sufficient experiments to shed light on why
the proposed OV-3DETIC works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiview Detection with Cardboard Human Modeling. (arXiv:2207.02013v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02013">
<div class="article-summary-box-inner">
<span><p>Multiview detection uses multiple calibrated cameras with overlapping fields
of views to locate occluded pedestrians. In this field, existing methods
typically adopt a ``human modeling - aggregation'' strategy. To find robust
pedestrian representations, some intuitively use locations of detected 2D
bounding boxes, while others use entire frame features projected to the ground
plane. However, the former does not consider human appearance and leads to many
ambiguities, and the latter suffers from projection errors due to the lack of
accurate height of the human torso and head. In this paper, we propose a new
pedestrian representation scheme based on human point clouds modeling.
Specifically, using ray tracing for holistic human depth estimation, we model
pedestrians as upright, thin cardboard point clouds on the ground. Then, we
aggregate the point clouds of the pedestrian cardboard across multiple views
for a final decision. Compared with existing representations, the proposed
method explicitly leverages human appearance and reduces projection errors
significantly by relatively accurate height estimation. On two standard
evaluation benchmarks, the proposed method achieves very competitive results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepPS2: Revisiting Photometric Stereo Using Two Differently Illuminated Images. (arXiv:2207.02025v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02025">
<div class="article-summary-box-inner">
<span><p>Photometric stereo, a problem of recovering 3D surface normals using images
of an object captured under different lightings, has been of great interest and
importance in computer vision research. Despite the success of existing
traditional and deep learning-based methods, it is still challenging due to:
(i) the requirement of three or more differently illuminated images, (ii) the
inability to model unknown general reflectance, and (iii) the requirement of
accurate 3D ground truth surface normals and known lighting information for
training. In this work, we attempt to address an under-explored problem of
photometric stereo using just two differently illuminated images, referred to
as the PS2 problem. It is an intermediate case between a single image-based
reconstruction method like Shape from Shading (SfS) and the traditional
Photometric Stereo (PS), which requires three or more images. We propose an
inverse rendering-based deep learning framework, called DeepPS2, that jointly
performs surface normal, albedo, lighting estimation, and image relighting in a
completely self-supervised manner with no requirement of ground truth data. We
demonstrate how image relighting in conjunction with image reconstruction
enhances the lighting estimation in a self-supervised setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CNN-based Local Vision Transformer for COVID-19 Diagnosis. (arXiv:2207.02027v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02027">
<div class="article-summary-box-inner">
<span><p>Deep learning technology can be used as an assistive technology to help
doctors quickly and accurately identify COVID-19 infections. Recently, Vision
Transformer (ViT) has shown great potential towards image classification due to
its global receptive field. However, due to the lack of inductive biases
inherent to CNNs, the ViT-based structure leads to limited feature richness and
difficulty in model training. In this paper, we propose a new structure called
Transformer for COVID-19 (COVT) to improve the performance of ViT-based
architectures on small COVID-19 datasets. It uses CNN as a feature extractor to
effectively extract local structural information, and introduces average
pooling to ViT's Multilayer Perception(MLP) module for global information.
Experiments show the effectiveness of our method on the two COVID-19 datasets
and the ImageNet dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AvatarCap: Animatable Avatar Conditioned Monocular Human Volumetric Capture. (arXiv:2207.02031v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02031">
<div class="article-summary-box-inner">
<span><p>To address the ill-posed problem caused by partial observations in monocular
human volumetric capture, we present AvatarCap, a novel framework that
introduces animatable avatars into the capture pipeline for high-fidelity
reconstruction in both visible and invisible regions. Our method firstly
creates an animatable avatar for the subject from a small number (~20) of 3D
scans as a prior. Then given a monocular RGB video of this subject, our method
integrates information from both the image observation and the avatar prior,
and accordingly recon-structs high-fidelity 3D textured models with dynamic
details regardless of the visibility. To learn an effective avatar for
volumetric capture from only few samples, we propose GeoTexAvatar, which
leverages both geometry and texture supervisions to constrain the
pose-dependent dynamics in a decomposed implicit manner. An avatar-conditioned
volumetric capture method that involves a canonical normal fusion and a
reconstruction network is further proposed to integrate both image observations
and avatar dynamics for high-fidelity reconstruction in both observed and
invisible regions. Overall, our method enables monocular human volumetric
capture with detailed and pose-dependent dynamics, and the experiments show
that our method outperforms state of the art. Code is available at
https://github.com/lizhe00/AvatarCap.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PRoA: A Probabilistic Robustness Assessment against Functional Perturbations. (arXiv:2207.02036v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02036">
<div class="article-summary-box-inner">
<span><p>In safety-critical deep learning applications robustness measurement is a
vital pre-deployment phase. However, existing robustness verification methods
are not sufficiently practical for deploying machine learning systems in the
real world. On the one hand, these methods attempt to claim that no
perturbations can ``fool'' deep neural networks (DNNs), which may be too
stringent in practice. On the other hand, existing works rigorously consider
$L_p$ bounded additive perturbations on the pixel space, although
perturbations, such as colour shifting and geometric transformations, are more
practically and frequently occurring in the real world. Thus, from the
practical standpoint, we present a novel and general {\it probabilistic
robustness assessment method} (PRoA) based on the adaptive concentration, and
it can measure the robustness of deep learning models against functional
perturbations. PRoA can provide statistical guarantees on the probabilistic
robustness of a model, \textit{i.e.}, the probability of failure encountered by
the trained model after deployment. Our experiments demonstrate the
effectiveness and flexibility of PRoA in terms of evaluating the probabilistic
robustness against a broad range of functional perturbations, and PRoA can
scale well to various large-scale deep neural networks compared to existing
state-of-the-art baselines. For the purpose of reproducibility, we release our
tool on GitHub: \url{ https://github.com/TrustAI/PRoA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PKD: General Distillation Framework for Object Detectors via Pearson Correlation Coefficient. (arXiv:2207.02039v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02039">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation(KD) is a widely-used technique to train compact models
in object detection. However, there is still a lack of study on how to distill
between heterogeneous detectors. In this paper, we empirically find that better
FPN features from a heterogeneous teacher detector can help the student
although their detection heads and label assignments are different. However,
directly aligning the feature maps to distill detectors suffers from two
problems. First, the difference in feature magnitude between the teacher and
the student could enforce overly strict constraints on the student. Second, the
FPN stages and channels with large feature magnitude from the teacher model
could dominate the gradient of distillation loss, which will overwhelm the
effects of other features in KD and introduce much noise. To address the above
issues, we propose to imitate features with Pearson Correlation Coefficient to
focus on the relational information from the teacher and relax constraints on
the magnitude of the features. Our method consistently outperforms the existing
detection KD methods and works for both homogeneous and heterogeneous
student-teacher pairs. Furthermore, it converges faster. With a powerful
MaskRCNN-Swin detector as the teacher, ResNet-50 based RetinaNet and FCOS
achieve 41.5% and 43.9% mAP on COCO2017, which are 4.1\% and 4.8\% higher than
the baseline, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MVP: Robust Multi-View Practice for Driving Action Localization. (arXiv:2207.02042v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02042">
<div class="article-summary-box-inner">
<span><p>Distracted driving causes thousands of deaths per year, and how to apply
deep-learning methods to prevent these tragedies has become a crucial problem.
In Track3 of the 6th AI City Challenge, researchers provide a high-quality
video dataset with densely action annotations. Due to the small data scale and
unclear action boundary, the dataset presents a unique challenge to precisely
localize all the different actions and classify their categories. In this
paper, we make good use of the multi-view synchronization among videos, and
conduct robust Multi-View Practice (MVP) for driving action localization. To
avoid overfitting, we fine-tune SlowFast with Kinetics-700 pre-training as the
feature extractor. Then the features of different views are passed to
ActionFormer to generate candidate action proposals. For precisely localizing
all the actions, we design elaborate post-processing, including model voting,
threshold filtering and duplication removal. The results show that our MVP is
robust for driving action localization, which achieves 28.49% F1-score in the
Track3 test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer based Models for Unsupervised Anomaly Segmentation in Brain MR Images. (arXiv:2207.02059v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02059">
<div class="article-summary-box-inner">
<span><p>The quality of patient care associated with diagnostic radiology is
proportionate to a physician workload. Segmentation is a fundamental limiting
precursor to diagnostic and therapeutic procedures. Advances in Machine
Learning (ML) aim to increase diagnostic efficiency to replace single
application with generalized algorithms. In Unsupervised Anomaly Detection
(UAD), Convolutional Neural Network (CNN) based Autoencoders (AEs) and
Variational Autoencoders (VAEs) are considered as a de facto approach for
reconstruction based anomaly segmentation. Looking for anomalous regions in
medical images is one of the main applications that use anomaly segmentation.
The restricted receptive field in CNNs limit the CNN to model the global
context and hence if the anomalous regions cover parts of the image, the
CNN-based AEs are not capable to bring semantic understanding of the image. On
the other hand, Vision Transformers (ViTs) have emerged as a competitive
alternative to CNNs. It relies on the self-attention mechanism that is capable
to relate image patches to each other. To reconstruct a coherent and more
realistic image, in this work, we investigate Transformer capabilities in
building AEs for reconstruction based UAD task. We focus on anomaly
segmentation for Brain Magnetic Resonance Imaging (MRI) and present five
Transformer-based models while enabling segmentation performance comparable or
superior to State-of-The-Art (SOTA) models. The source code is available on
Github
https://github.com/ahmedgh970/Transformers_Unsupervised_Anomaly_Segmentation.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Amodal Completion: A Survey. (arXiv:2207.02062v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02062">
<div class="article-summary-box-inner">
<span><p>Existing computer vision systems can compete with humans in understanding the
visible parts of objects, but still fall far short of humans when it comes to
depicting the invisible parts of partially occluded objects. Image amodal
completion aims to equip computers with human-like amodal completion functions
to understand an intact object despite it being partially occluded. The main
purpose of this survey is to provide an intuitive understanding of the research
hotspots, key technologies and future trends in the field of image amodal
completion. Firstly, we present a comprehensive review of the latest literature
in this emerging field, exploring three key tasks in image amodal completion,
including amodal shape completion, amodal appearance completion, and order
perception. Then we examine popular datasets related to image amodal completion
along with their common data collection methods and evaluation metrics.
Finally, we discuss real-world applications and future research directions for
image amodal completion, facilitating the reader's understanding of the
challenges of existing technologies and upcoming research trends.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RepMix: Representation Mixing for Robust Attribution of Synthesized Images. (arXiv:2207.02063v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02063">
<div class="article-summary-box-inner">
<span><p>Rapid advances in Generative Adversarial Networks (GANs) raise new challenges
for image attribution; detecting whether an image is synthetic and, if so,
determining which GAN architecture created it. Uniquely, we present a solution
to this task capable of 1) matching images invariant to their semantic content;
2) robust to benign transformations (changes in quality, resolution, shape,
etc.) commonly encountered as images are re-shared online. In order to
formalize our research, a challenging benchmark, Attribution88, is collected
for robust and practical image attribution. We then propose RepMix, our GAN
fingerprinting technique based on representation mixing and a novel loss. We
validate its capability of tracing the provenance of GAN-generated images
invariant to the semantic content of the image and also robust to
perturbations. We show our approach improves significantly from existing GAN
fingerprinting works on both semantic generalization and robustness. Data and
code are available at https://github.com/TuBui/image_attribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Test-time Adaptation for Real Image Denoising via Meta-transfer Learning. (arXiv:2207.02066v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02066">
<div class="article-summary-box-inner">
<span><p>In recent years, a ton of research has been conducted on real image denoising
tasks. However, the efforts are more focused on improving real image denoising
through creating a better network architecture. We explore a different
direction where we propose to improve real image denoising performance through
a better learning strategy that can enable test-time adaptation on the
multi-task network. The learning strategy is two stages where the first stage
pre-train the network using meta-auxiliary learning to get better
meta-initialization. Meanwhile, we use meta-learning for fine-tuning
(meta-transfer learning) the network as the second stage of our training to
enable test-time adaptation on real noisy images. To exploit a better learning
strategy, we also propose a network architecture with self-supervised masked
reconstruction loss. Experiments on a real noisy dataset show the contribution
of the proposed method and show that the proposed method can outperform other
SOTA methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Densely Interconnected Network for Deep Learning Accelerated MRI. (arXiv:2207.02073v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02073">
<div class="article-summary-box-inner">
<span><p>Objective: To improve accelerated MRI reconstruction through a densely
connected cascading deep learning reconstruction framework.
</p>
<p>Materials and Methods: A cascading deep learning reconstruction framework
(baseline model) was modified by applying three architectural modifications:
Input-level dense connections between cascade inputs and outputs, an improved
deep learning sub-network, and long-range skip-connections between subsequent
deep learning networks. An ablation study was performed, where five model
configurations were trained on the NYU fastMRI neuro dataset with an end-to-end
scheme conjunct on four- and eight-fold acceleration. The trained models were
evaluated by comparing their respective structural similarity index measure
(SSIM), normalized mean square error (NMSE) and peak signal to noise ratio
(PSNR).
</p>
<p>Results: The proposed densely interconnected residual cascading network
(DIRCN), utilizing all three suggested modifications, achieved a SSIM
improvement of 8% and 11% for four- and eight-fold acceleration, respectively.
For eight-fold acceleration, the model achieved a 23% decrease in the NMSE when
compared to the baseline model. In an ablation study, the individual
architectural modifications all contributed to this improvement, by reducing
the SSIM and NMSE with approximately 3% and 5% for four-fold acceleration,
respectively.
</p>
<p>Conclusion: The proposed architectural modifications allow for simple
adjustments on an already existing cascading framework to further improve the
resulting reconstructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SiamMask: A Framework for Fast Online Object Tracking and Segmentation. (arXiv:2207.02088v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02088">
<div class="article-summary-box-inner">
<span><p>In this paper we introduce SiamMask, a framework to perform both visual
object tracking and video object segmentation, in real-time, with the same
simple method. We improve the offline training procedure of popular
fully-convolutional Siamese approaches by augmenting their losses with a binary
segmentation task. Once the offline training is completed, SiamMask only
requires a single bounding box for initialization and can simultaneously carry
out visual object tracking and segmentation at high frame-rates. Moreover, we
show that it is possible to extend the framework to handle multiple object
tracking and segmentation by simply re-using the multi-task model in a cascaded
fashion. Experimental results show that our approach has high processing
efficiency, at around 55 frames per second. It yields real-time
state-of-the-art results on visual-object tracking benchmarks, while at the
same time demonstrating competitive performance at a high speed for video
object segmentation benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CASHformer: Cognition Aware SHape Transformer for Longitudinal Analysis. (arXiv:2207.02091v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02091">
<div class="article-summary-box-inner">
<span><p>Modeling temporal changes in subcortical structures is crucial for a better
understanding of the progression of Alzheimer's disease (AD). Given their
flexibility to adapt to heterogeneous sequence lengths, mesh-based transformer
architectures have been proposed in the past for predicting hippocampus
deformations across time. However, one of the main limitations of transformers
is the large amount of trainable parameters, which makes the application on
small datasets very challenging. In addition, current methods do not include
relevant non-image information that can help to identify AD-related patterns in
the progression. To this end, we introduce CASHformer, a transformer-based
framework to model longitudinal shape trajectories in AD. CASHformer
incorporates the idea of pre-trained transformers as universal compute engines
that generalize across a wide range of tasks by freezing most layers during
fine-tuning. This reduces the number of parameters by over 90% with respect to
the original model and therefore enables the application of large models on
small datasets without overfitting. In addition, CASHformer models cognitive
decline to reveal AD atrophy patterns in the temporal sequence. Our results
show that CASHformer reduces the reconstruction error by 73% compared to
previously proposed methods. Moreover, the accuracy of detecting patients
progressing to AD increases by 3% with imputing missing longitudinal shape
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is a PET all you need? A multi-modal study for Alzheimer's disease using 3D CNNs. (arXiv:2207.02094v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02094">
<div class="article-summary-box-inner">
<span><p>Alzheimer's Disease (AD) is the most common form of dementia and often
difficult to diagnose due to the multifactorial etiology of dementia. Recent
works on neuroimaging-based computer-aided diagnosis with deep neural networks
(DNNs) showed that fusing structural magnetic resonance images (sMRI) and
fluorodeoxyglucose positron emission tomography (FDG-PET) leads to improved
accuracy in a study population of healthy controls and subjects with AD.
However, this result conflicts with the established clinical knowledge that
FDG-PET better captures AD-specific pathologies than sMRI. Therefore, we
propose a framework for the systematic evaluation of multi-modal DNNs and
critically re-evaluate single- and multi-modal DNNs based on FDG-PET and sMRI
for binary healthy vs. AD, and three-way healthy/mild cognitive impairment/AD
classification. Our experiments demonstrate that a single-modality network
using FDG-PET performs better than MRI (accuracy 0.91 vs 0.87) and does not
show improvement when combined. This conforms with the established clinical
knowledge on AD biomarkers, but raises questions about the true benefit of
multi-modal DNNs. We argue that future work on multi-modal fusion should
systematically assess the contribution of individual modalities following our
proposed evaluation framework. Finally, we encourage the community to go beyond
healthy vs. AD classification and focus on differential diagnosis of dementia,
where fusing multi-modal image information conforms with a clinical need.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Covariance Conditioning of the SVD Meta-layer by Orthogonality. (arXiv:2207.02119v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02119">
<div class="article-summary-box-inner">
<span><p>Inserting an SVD meta-layer into neural networks is prone to make the
covariance ill-conditioned, which could harm the model in the training
stability and generalization abilities. In this paper, we systematically study
how to improve the covariance conditioning by enforcing orthogonality to the
Pre-SVD layer. Existing orthogonal treatments on the weights are first
investigated. However, these techniques can improve the conditioning but would
hurt the performance. To avoid such a side effect, we propose the Nearest
Orthogonal Gradient (NOG) and Optimal Learning Rate (OLR). The effectiveness of
our methods is validated in two applications: decorrelated Batch Normalization
(BN) and Global Covariance Pooling (GCP). Extensive experiments on visual
recognition demonstrate that our methods can simultaneously improve the
covariance conditioning and generalization. Moreover, the combinations with
orthogonal weight can further boost the performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Semantic Segmentation in Transformers using Hierarchical Inter-Level Attention. (arXiv:2207.02126v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02126">
<div class="article-summary-box-inner">
<span><p>Existing transformer-based image backbones typically propagate feature
information in one direction from lower to higher-levels. This may not be ideal
since the localization ability to delineate accurate object boundaries, is most
prominent in the lower, high-resolution feature maps, while the semantics that
can disambiguate image signals belonging to one object vs. another, typically
emerges in a higher level of processing. We present Hierarchical Inter-Level
Attention (HILA), an attention-based method that captures Bottom-Up and
Top-Down Updates between features of different levels. HILA extends
hierarchical vision transformer architectures by adding local connections
between features of higher and lower levels to the backbone encoder. In each
iteration, we construct a hierarchy by having higher-level features compete for
assignments to update lower-level features belonging to them, iteratively
resolving object-part relationships. These improved lower-level features are
then used to re-update the higher-level features. HILA can be integrated into
the majority of hierarchical architectures without requiring any changes to the
base model. We add HILA into SegFormer and the Swin Transformer and show
notable improvements in accuracy in semantic segmentation with fewer parameters
and FLOPS. Project website and code:
https://www.cs.toronto.edu/~garyleung/hila/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Finger Vein Recognition: A Brief Survey of Recent Trend. (arXiv:2207.02148v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02148">
<div class="article-summary-box-inner">
<span><p>Finger vein image recognition technology plays an important role in biometric
recognition and has been successfully applied in many fields. Because veins are
buried beneath the skin tissue, finger vein image recognition has an
unparalleled advantage, which is not easily disturbed by external factors. This
review summarizes 46 papers about deep learning for finger vein image
recognition from 2017 to 2021. These papers are summarized according to the
tasks of deep neural networks. Besides, we present the challenges and potential
development directions of finger vein image recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class-Specific Semantic Reconstruction for Open Set Recognition. (arXiv:2207.02158v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02158">
<div class="article-summary-box-inner">
<span><p>Open set recognition enables deep neural networks (DNNs) to identify samples
of unknown classes, while maintaining high classification accuracy on samples
of known classes. Existing methods basing on auto-encoder (AE) and prototype
learning show great potential in handling this challenging task. In this study,
we propose a novel method, called Class-Specific Semantic Reconstruction
(CSSR), that integrates the power of AE and prototype learning. Specifically,
CSSR replaces prototype points with manifolds represented by class-specific
AEs. Unlike conventional prototype-based methods, CSSR models each known class
on an individual AE manifold, and measures class belongingness through AE's
reconstruction error. Class-specific AEs are plugged into the top of the DNN
backbone and reconstruct the semantic representations learned by the DNN
instead of the raw image. Through end-to-end learning, the DNN and the AEs
boost each other to learn both discriminative and representative information.
The results of experiments conducted on multiple datasets show that the
proposed method achieves outstanding performance in both close and open set
recognition and is sufficiently simple and flexible to incorporate into
existing frameworks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Robustness Analysis Against Language and Visual Perturbations. (arXiv:2207.02159v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02159">
<div class="article-summary-box-inner">
<span><p>Joint visual and language modeling on large-scale datasets has recently shown
a good progress in multi-modal tasks when compared to single modal learning.
However, robustness of these approaches against real-world perturbations has
not been studied. In this work, we perform the first extensive robustness study
of such models against various real-world perturbations focusing on video and
language. We focus on text-to-video retrieval and propose two large-scale
benchmark datasets, MSRVTT-P and YouCook2-P, which utilize 90 different visual
and 35 different textual perturbations. The study reveals some interesting
findings: 1) The studied models are more robust when text is perturbed versus
when video is perturbed 2) The transformer text encoder is more robust on
non-semantic changing text perturbations and visual perturbations compared to
word embedding approaches. 3) Using two-branch encoders in isolation is
typically more robust than when architectures use cross-attention. We hope this
study will serve as a benchmark and guide future research in robust multimodal
learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic inspection of cultural monuments using deep and tensor-based learning on hyperspectral imagery. (arXiv:2207.02163v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02163">
<div class="article-summary-box-inner">
<span><p>In Cultural Heritage, hyperspectral images are commonly used since they
provide extended information regarding the optical properties of materials.
Thus, the processing of such high-dimensional data becomes challenging from the
perspective of machine learning techniques to be applied. In this paper, we
propose a Rank-$R$ tensor-based learning model to identify and classify
material defects on Cultural Heritage monuments. In contrast to conventional
deep learning approaches, the proposed high order tensor-based learning
demonstrates greater accuracy and robustness against overfitting. Experimental
results on real-world data from UNESCO protected areas indicate the superiority
of the proposed scheme compared to conventional deep learning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DBN-Mix: Training Dual Branch Network Using Bilateral Mixup Augmentation for Long-Tailed Visual Recognition. (arXiv:2207.02173v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02173">
<div class="article-summary-box-inner">
<span><p>There is a growing interest in the challenging visual perception task of
learning from long-tailed class distributions. The extreme class imbalance in
the training dataset biases the model to prefer to recognize majority-class
data over minority-class data. Recently, the dual branch network (DBN)
framework has been proposed, where two branch networks; the conventional branch
and the re-balancing branch were employed to improve the accuracy of
long-tailed visual recognition. The re-balancing branch uses a reverse sampler
to generate class-balanced training samples to mitigate bias due to class
imbalance. Although this strategy has been quite successful in handling bias,
using a reversed sampler for training can degrade the representation learning
performance. To alleviate this issue, the conventional method used a carefully
designed cumulative learning strategy, in which the influence of the
re-balancing branch gradually increases throughout the entire training phase.
In this study, we aim to develop a simple yet effective method to improve the
performance of DBN without cumulative learning that is difficult to optimize.
We devise a simple data augmentation method termed bilateral mixup
augmentation, which combines one sample from the uniform sampler with another
sample from the reversed sampler to produce a training sample. Furthermore, we
present class-conditional temperature scaling that mitigates bias toward the
majority class for the proposed DBN architecture. Our experiments performed on
widely used long-tailed visual recognition datasets show that bilateral mixup
augmentation is quite effective in improving the representation learning
performance of DBNs, and that the proposed method achieves state-of-the-art
performance for some categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Activation Template Matching Loss for Explainable Face Recognition. (arXiv:2207.02179v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02179">
<div class="article-summary-box-inner">
<span><p>Can we construct an explainable face recognition network able to learn a
facial part-based feature like eyes, nose, mouth and so forth, without any
manual annotation or additionalsion datasets? In this paper, we propose a
generic Explainable Channel Loss (ECLoss) to construct an explainable face
recognition network. The explainable network trained with ECLoss can easily
learn the facial part-based representation on the target convolutional layer,
where an individual channel can detect a certain face part. Our experiments on
dozens of datasets show that ECLoss achieves superior explainability metrics,
and at the same time improves the performance of face verification without face
alignment. In addition, our visualization results also illustrate the
effectiveness of the proposed ECLoss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ST-CoNAL: Consistency-Based Acquisition Criterion Using Temporal Self-Ensemble for Active Learning. (arXiv:2207.02182v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02182">
<div class="article-summary-box-inner">
<span><p>Modern deep learning has achieved great success in various fields. However,
it requires the labeling of huge amounts of data, which is expensive and
labor-intensive. Active learning (AL), which identifies the most informative
samples to be labeled, is becoming increasingly important to maximize the
efficiency of the training process. The existing AL methods mostly use only a
single final fixed model for acquiring the samples to be labeled. This strategy
may not be good enough in that the structural uncertainty of a model for given
training data is not considered to acquire the samples. In this study, we
propose a novel acquisition criterion based on temporal self-ensemble generated
by conventional stochastic gradient descent (SGD) optimization. These
self-ensemble models are obtained by capturing the intermediate network weights
obtained through SGD iterations. Our acquisition function relies on a
consistency measure between the student and teacher models. The student models
are given a fixed number of temporal self-ensemble models, and the teacher
model is constructed by averaging the weights of the student models. Using the
proposed acquisition criterion, we present an AL algorithm, namely
student-teacher consistency-based AL (ST-CoNAL). Experiments conducted for
image classification tasks on CIFAR-10, CIFAR-100, Caltech-256, and Tiny
ImageNet datasets demonstrate that the proposed ST-CoNAL achieves significantly
better performance than the existing acquisition methods. Furthermore,
extensive experiments show the robustness and effectiveness of our methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLEAR: Improving Vision-Language Navigation with Cross-Lingual, Environment-Agnostic Representations. (arXiv:2207.02185v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02185">
<div class="article-summary-box-inner">
<span><p>Vision-and-Language Navigation (VLN) tasks require an agent to navigate
through the environment based on language instructions. In this paper, we aim
to solve two key challenges in this task: utilizing multilingual instructions
for improved instruction-path grounding and navigating through new environments
that are unseen during training. To address these challenges, we propose CLEAR:
Cross-Lingual and Environment-Agnostic Representations. First, our agent learns
a shared and visually-aligned cross-lingual language representation for the
three languages (English, Hindi and Telugu) in the Room-Across-Room dataset.
Our language representation learning is guided by text pairs that are aligned
by visual information. Second, our agent learns an environment-agnostic visual
representation by maximizing the similarity between semantically-aligned image
pairs (with constraints on object-matching) from different environments. Our
environment agnostic visual representation can mitigate the environment bias
induced by low-level visual information. Empirically, on the Room-Across-Room
dataset, we show that our multilingual agent gets large improvements in all
metrics over the strong baseline model when generalizing to unseen environments
with the cross-lingual language representation and the environment-agnostic
visual representation. Furthermore, we show that our learned language and
visual representations can be successfully transferred to the Room-to-Room and
Cooperative Vision-and-Dialogue Navigation task, and present detailed
qualitative and quantitative generalization and grounding analysis. Our code is
available at https://github.com/jialuli-luka/CLEAR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuralPassthrough: Learned Real-Time View Synthesis for VR. (arXiv:2207.02186v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02186">
<div class="article-summary-box-inner">
<span><p>Virtual reality (VR) headsets provide an immersive, stereoscopic visual
experience, but at the cost of blocking users from directly observing their
physical environment. Passthrough techniques are intended to address this
limitation by leveraging outward-facing cameras to reconstruct the images that
would otherwise be seen by the user without the headset. This is inherently a
real-time view synthesis challenge, since passthrough cameras cannot be
physically co-located with the eyes. Existing passthrough techniques suffer
from distracting reconstruction artifacts, largely due to the lack of accurate
depth information (especially for near-field and disoccluded objects), and also
exhibit limited image quality (e.g., being low resolution and monochromatic).
In this paper, we propose the first learned passthrough method and assess its
performance using a custom VR headset that contains a stereo pair of RGB
cameras. Through both simulations and experiments, we demonstrate that our
learned passthrough method delivers superior image quality compared to
state-of-the-art methods, while meeting strict VR requirements for real-time,
perspective-correct stereoscopic view synthesis over a wide field of view for
desktop-connected headsets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling. (arXiv:2207.02196v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02196">
<div class="article-summary-box-inner">
<span><p>Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. However, a fundamental limitation is that their
inference is very slow due to a need for many (e.g., 2000) iterations of
sequential computations. An intuitive acceleration method is to reduce the
sampling iterations which however causes severe performance degradation. We
investigate this problem by viewing the diffusion sampling process as a
Metropolis adjusted Langevin algorithm, which helps reveal the underlying cause
to be ill-conditioned curvature. Under this insight, we propose a
model-agnostic preconditioned diffusion sampling (PDS) method that leverages
matrix preconditioning to alleviate the aforementioned problem. Crucially, PDS
is proven theoretically to converge to the original target distribution of a
SGM, no need for retraining. Extensive experiments on three image datasets with
a variety of resolutions and diversity validate that PDS consistently
accelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In
particular, PDS can accelerate by up to 29x on more challenging high resolution
(1024x1024) image generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Spatial-Temporal Information Fusion for LiDAR-Based 3D Moving Object Segmentation. (arXiv:2207.02201v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02201">
<div class="article-summary-box-inner">
<span><p>Accurate moving object segmentation is an essential task for autonomous
driving. It can provide effective information for many downstream tasks, such
as collision avoidance, path planning, and static map construction. How to
effectively exploit the spatial-temporal information is a critical question for
3D LiDAR moving object segmentation (LiDAR-MOS). In this work, we propose a
novel deep neural network exploiting both spatial-temporal information and
different representation modalities of LiDAR scans to improve LiDAR-MOS
performance. Specifically, we first use a range image-based dual-branch
structure to separately deal with spatial and temporal information that can be
obtained from sequential LiDAR scans, and later combine them using
motion-guided attention modules. We also use a point refinement module via 3D
sparse convolution to fuse the information from both LiDAR range image and
point cloud representations and reduce the artifacts on the borders of the
objects. We verify the effectiveness of our proposed approach on the LiDAR-MOS
benchmark of SemanticKITTI. Our method outperforms the state-of-the-art methods
significantly in terms of LiDAR-MOS IoU. Benefiting from the devised
coarse-to-fine architecture, our method operates online at sensor frame rate.
The implementation of our method is available as open source at:
https://github.com/haomo-ai/MotionSeg3D.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoBEVT: Cooperative Bird's Eye View Semantic Segmentation with Sparse Transformers. (arXiv:2207.02202v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02202">
<div class="article-summary-box-inner">
<span><p>Bird's eye view (BEV) semantic segmentation plays a crucial role in spatial
sensing for autonomous driving. Although recent literature has made significant
progress on BEV map understanding, they are all based on single-agent
camera-based systems which are difficult to handle occlusions and detect
distant objects in complex traffic scenes. Vehicle-to-Vehicle (V2V)
communication technologies have enabled autonomous vehicles to share sensing
information, which can dramatically improve the perception performance and
range as compared to single-agent systems. In this paper, we propose CoBEVT,
the first generic multi-agent multi-camera perception framework that can
cooperatively generate BEV map predictions. To efficiently fuse camera features
from multi-view and multi-agent data in an underlying Transformer architecture,
we design a fused axial attention or FAX module, which can capture sparsely
local and global spatial interactions across views and agents. The extensive
experiments on the V2V perception dataset, OPV2V, demonstrate that CoBEVT
achieves state-of-the-art performance for cooperative BEV semantic
segmentation. Moreover, CoBEVT is shown to be generalizable to other tasks,
including 1) BEV segmentation with single-agent multi-camera and 2) 3D object
detection with multi-agent LiDAR systems, and achieves state-of-the-art
performance with real-time inference speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting and Recovering Sequential DeepFake Manipulation. (arXiv:2207.02204v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02204">
<div class="article-summary-box-inner">
<span><p>Since photorealistic faces can be readily generated by facial manipulation
technologies nowadays, potential malicious abuse of these technologies has
drawn great concerns. Numerous deepfake detection methods are thus proposed.
However, existing methods only focus on detecting one-step facial manipulation.
As the emergence of easy-accessible facial editing applications, people can
easily manipulate facial components using multi-step operations in a sequential
manner. This new threat requires us to detect a sequence of facial
manipulations, which is vital for both detecting deepfake media and recovering
original faces afterwards. Motivated by this observation, we emphasize the need
and propose a novel research problem called Detecting Sequential DeepFake
Manipulation (Seq-DeepFake). Unlike the existing deepfake detection task only
demanding a binary label prediction, detecting Seq-DeepFake manipulation
requires correctly predicting a sequential vector of facial manipulation
operations. To support a large-scale investigation, we construct the first
Seq-DeepFake dataset, where face images are manipulated sequentially with
corresponding annotations of sequential facial manipulation vectors. Based on
this new dataset, we cast detecting Seq-DeepFake manipulation as a specific
image-to-sequence (e.g. image captioning) task and propose a concise yet
effective Seq-DeepFake Transformer (SeqFakeFormer). Moreover, we build a
comprehensive benchmark and set up rigorous evaluation protocols and metrics
for this new research problem. Extensive experiments demonstrate the
effectiveness of SeqFakeFormer. Several valuable observations are also revealed
to facilitate future research in broader deepfake detection problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clustered Saliency Prediction. (arXiv:2207.02205v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02205">
<div class="article-summary-box-inner">
<span><p>We present a new method for image salience prediction, Clustered Saliency
Prediction. This method divides individuals into clusters based on their
personal features and their known saliency maps, and generates a separate image
salience model for each cluster. We test our approach on a public dataset of
personalized saliency maps, with varying importance weights for personal
feature factors and observe the effects on the clusters. For each cluster, we
use an image-to-image translation method, mainly Pix2Pix model, to convert
universal saliency maps to saliency maps of that cluster. We try three
state-of-the-art universal saliency prediction methods, DeepGaze II, ML-Net and
SalGAN, and see their impact on the results. We show that our Clustered
Saliency Prediction technique outperforms the state-of-the-art universal
saliency prediction models. Also we demonstrate the effectiveness of our
clustering method by comparing the results of Clustered Saliency Prediction
using clusters obtained by Subject Similarity Clustering algorithm with two
baseline methods. We propose an approach to assign new people to the most
appropriate cluster, based on their personal features and any known saliency
maps. In our experiments we see that this method of assigning new people to a
cluster on average chooses the cluster that gives higher saliency scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmenting Moving Objects via an Object-Centric Layered Representation. (arXiv:2207.02206v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02206">
<div class="article-summary-box-inner">
<span><p>The objective of this paper is a model that is able to discover, track and
segment multiple moving objects in a video. We make four contributions: First,
we introduce an object-centric segmentation model with a depth-ordered layer
representation. This is implemented using a variant of the transformer
architecture that ingests optical flow, where each query vector specifies an
object and its layer for the entire video. The model can effectively discover
multiple moving objects and handle mutual occlusions; Second, we introduce a
scalable pipeline for generating synthetic training data with multiple objects,
significantly reducing the requirements for labour-intensive annotations, and
supporting Sim2Real generalisation; Third, we show that the model is able to
learn object permanence and temporal shape consistency, and is able to predict
amodal segmentation masks; Fourth, we evaluate the model on standard video
segmentation benchmarks, DAVIS, MoCA, SegTrack, FBMS-59, and achieve
state-of-the-art unsupervised segmentation performance, even outperforming
several supervised approaches. With test-time adaptation, we observe further
performance boosts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DPC-Net: Deep Pose Correction for Visual Localization. (arXiv:1709.03128v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1709.03128">
<div class="article-summary-box-inner">
<span><p>We present a novel method to fuse the power of deep networks with the
computational efficiency of geometric and probabilistic localization
algorithms. In contrast to other methods that completely replace a classical
visual estimator with a deep network, we propose an approach that uses a
convolutional neural network to learn difficult-to-model corrections to the
estimator from ground-truth training data. To this end, we derive a novel loss
function for learning SE(3) corrections based on a matrix Lie groups approach,
with a natural formulation for balancing translation and rotation errors. We
use this loss to train a Deep Pose Correction network (DPC-Net) that predicts
corrections for a particular estimator, sensor and environment. Using the KITTI
odometry dataset, we demonstrate significant improvements to the accuracy of a
computationally-efficient sparse stereo visual odometry pipeline, that render
it as accurate as a modern computationally-intensive dense estimator. Further,
we show how DPC-Net can be used to mitigate the effect of poorly calibrated
lens distortion parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Matchable Image Transformations for Long-term Metric Visual Localization. (arXiv:1904.01080v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.01080">
<div class="article-summary-box-inner">
<span><p>Long-term metric self-localization is an essential capability of autonomous
mobile robots, but remains challenging for vision-based systems due to
appearance changes caused by lighting, weather, or seasonal variations. While
experience-based mapping has proven to be an effective technique for bridging
the `appearance gap,' the number of experiences required for reliable metric
localization over days or months can be very large, and methods for reducing
the necessary number of experiences are needed for this approach to scale.
Taking inspiration from color constancy theory, we learn a nonlinear
RGB-to-grayscale mapping that explicitly maximizes the number of inlier feature
matches for images captured under different lighting and weather conditions,
and use it as a pre-processing step in a conventional single-experience
localization pipeline to improve its robustness to appearance change. We train
this mapping by approximating the target non-differentiable localization
pipeline with a deep neural network, and find that incorporating a learned
low-dimensional context feature can further improve cross-appearance feature
matching. Using synthetic and real-world datasets, we demonstrate substantial
improvements in localization performance across day-night cycles, enabling
continuous metric localization over a 30-hour period using a single mapping
experience, and allowing experience-based localization to scale to long
deployments with dramatically reduced data requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting mechanical loosening of total hip replacement implant from plain radiograph using deep convolutional neural network. (arXiv:1912.00943v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.00943">
<div class="article-summary-box-inner">
<span><p>Plain radiography is widely used to detect mechanical loosening of total hip
replacement (THR) implants. Currently, radiographs are assessed manually by
medical professionals, which may be prone to poor inter and intra observer
reliability and low accuracy. Furthermore, manual detection of mechanical
loosening of THR implants requires experienced clinicians who might not always
be readily available, potentially resulting in delayed diagnosis. In this
study, we present a novel, fully automatic and interpretable approach to detect
mechanical loosening of THR implants from plain radiographs using deep
convolutional neural network (CNN). We trained a CNN on 40 patients
anteroposterior hip x rays using five fold cross validation and compared its
performance with a high volume board certified orthopaedic surgeon (AFC). To
increase the confidence in the machine outcome, we also implemented saliency
maps to visualize where the CNN looked at to make a diagnosis. CNN outperformed
the orthopaedic surgeon in diagnosing mechanical loosening of THR implants
achieving significantly higher sensitively (0.94) than the orthopaedic surgeon
(0.53) with the same specificity (0.96). The saliency maps showed that the CNN
looked at clinically relevant features to make a diagnosis. Such CNNs can be
used for automatic radiologic assessment of mechanical loosening of THR
implants to supplement the practitioners decision making process, increasing
their diagnostic accuracy, and freeing them to engage in more patient centric
care.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UltraSR: Spatial Encoding is a Missing Key for Implicit Image Function-based Arbitrary-Scale Super-Resolution. (arXiv:2103.12716v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12716">
<div class="article-summary-box-inner">
<span><p>The recent success of NeRF and other related implicit neural representation
methods has opened a new path for continuous image representation, where pixel
values no longer need to be looked up from stored discrete 2D arrays but can be
inferred from neural network models on a continuous spatial domain. Although
the recent work LIIF has demonstrated that such novel approaches can achieve
good performance on the arbitrary-scale super-resolution task, their upscaled
images frequently show structural distortion due to the inaccurate prediction
of high-frequency textures. In this work, we propose UltraSR, a simple yet
effective new network design based on implicit image functions in which we
deeply integrated spatial coordinates and periodic encoding with the implicit
neural representation. Through extensive experiments and ablation studies, we
show that spatial encoding is a missing key toward the next-stage
high-performing implicit image function. Our UltraSR sets new state-of-the-art
performance on the DIV2K benchmark under all super-resolution scales compared
to previous state-of-the-art methods. UltraSR also achieves superior
performance on other standard benchmark datasets in which it outperforms prior
works in almost all experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual 3D Convolutional Neural Networks for Real-time Processing of Videos. (arXiv:2106.00050v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00050">
<div class="article-summary-box-inner">
<span><p>We introduce Continual 3D Convolutional Neural Networks (Co3D CNNs), a new
computational formulation of spatio-temporal 3D CNNs, in which videos are
processed frame-by-frame rather than by clip. In online tasks demanding
frame-wise predictions, Co3D CNNs dispense with the computational redundancies
of regular 3D CNNs, namely the repeated convolutions over frames, which appear
in overlapping clips. We show that Continual 3D CNNs can reuse preexisting
3D-CNN weights to reduce the per-prediction floating point operations (FLOPs)
in proportion to the temporal receptive field while retaining similar memory
requirements and accuracy. This is validated with multiple models on
Kinetics-400 and Charades with remarkable results: CoX3D models attain
state-of-the-art complexity/accuracy trade-offs on Kinetics-400 with 12.1-15.3x
reductions of FLOPs and 2.3-3.8% improvements in accuracy compared to regular
X3D models while reducing peak memory consumption by up to 48%. Moreover, we
investigate the transient response of Co3D CNNs at start-up and perform
extensive benchmarks of on-hardware processing characteristics for publicly
available 3D CNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent Deep Semi-supervised Learning Approaches and Related Works. (arXiv:2106.11528v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11528">
<div class="article-summary-box-inner">
<span><p>The author of this work proposes an overview of the recent semi-supervised
learning approaches and related works. Despite the remarkable success of neural
networks in various applications, there exist few formidable constraints
including the need for a large amount of labeled data. Therefore,
semi-supervised learning, which is a learning scheme in which the scarce labels
and a larger amount of unlabeled data are utilized to train models (e.g., deep
neural networks) is getting more important. Based on the key assumptions of
semi-supervised learning, which are the manifold assumption, cluster
assumption, and continuity assumption, the work reviews the recent
semi-supervised learning approaches. In particular, the methods in regard to
using deep neural networks in a semi-supervised learning setting are primarily
discussed. In addition, the existing works are first classified based on the
underlying idea and explained, and then the holistic approaches that unify the
aforementioned ideas are detailed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Practicality of Deterministic Epistemic Uncertainty. (arXiv:2107.00649v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00649">
<div class="article-summary-box-inner">
<span><p>A set of novel approaches for estimating epistemic uncertainty in deep neural
networks with a single forward pass has recently emerged as a valid alternative
to Bayesian Neural Networks. On the premise of informative representations,
these deterministic uncertainty methods (DUMs) achieve strong performance on
detecting out-of-distribution (OOD) data while adding negligible computational
costs at inference time. However, it remains unclear whether DUMs are well
calibrated and can seamlessly scale to real-world applications - both
prerequisites for their practical deployment. To this end, we first provide a
taxonomy of DUMs, and evaluate their calibration under continuous
distributional shifts. Then, we extend them to semantic segmentation. We find
that, while DUMs scale to realistic vision tasks and perform well on OOD
detection, the practicality of current methods is undermined by poor
calibration under distributional shifts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimCC: a Simple Coordinate Classification Perspective for Human Pose Estimation. (arXiv:2107.03332v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03332">
<div class="article-summary-box-inner">
<span><p>The 2D heatmap-based approaches have dominated Human Pose Estimation (HPE)
for years due to high performance. However, the long-standing quantization
error problem in the 2D heatmap-based methods leads to several well-known
drawbacks: 1) The performance for the low-resolution inputs is limited; 2) To
improve the feature map resolution for higher localization precision, multiple
costly upsampling layers are required; 3) Extra post-processing is adopted to
reduce the quantization error. To address these issues, we aim to explore a
brand new scheme, called \textit{SimCC}, which reformulates HPE as two
classification tasks for horizontal and vertical coordinates. The proposed
SimCC uniformly divides each pixel into several bins, thus achieving
\emph{sub-pixel} localization precision and low quantization error. Benefiting
from that, SimCC can omit additional refinement post-processing and exclude
upsampling layers under certain settings, resulting in a more simple and
effective pipeline for HPE. Extensive experiments conducted over COCO,
CrowdPose, and MPII datasets show that SimCC outperforms heatmap-based
counterparts, especially in low-resolution settings by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benign Adversarial Attack: Tricking Models for Goodness. (arXiv:2107.11986v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11986">
<div class="article-summary-box-inner">
<span><p>In spite of the successful application in many fields, machine learning
models today suffer from notorious problems like vulnerability to adversarial
examples. Beyond falling into the cat-and-mouse game between adversarial attack
and defense, this paper provides alternative perspective to consider
adversarial example and explore whether we can exploit it in benign
applications. We first attribute adversarial example to the human-model
disparity on employing non-semantic features. While largely ignored in
classical machine learning mechanisms, non-semantic feature enjoys three
interesting characteristics as (1) exclusive to model, (2) critical to affect
inference, and (3) utilizable as features. Inspired by this, we present brave
new idea of benign adversarial attack to exploit adversarial examples for
goodness in three directions: (1) adversarial Turing test, (2) rejecting
malicious model application, and (3) adversarial data augmentation. Each
direction is positioned with motivation elaboration, justification analysis and
prototype applications to showcase its potential.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-grained Data Distribution Alignment for Post-Training Quantization. (arXiv:2109.04186v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04186">
<div class="article-summary-box-inner">
<span><p>While post-training quantization receives popularity mostly due to its
evasion in accessing the original complete training dataset, its poor
performance also stems from scarce images. To alleviate this limitation, in
this paper, we leverage the synthetic data introduced by zero-shot quantization
with calibration dataset and propose a fine-grained data distribution alignment
(FDDA) method to boost the performance of post-training quantization. The
method is based on two important properties of batch normalization statistics
(BNS) we observed in deep layers of the trained network, (i.e.), inter-class
separation and intra-class incohesion. To preserve this fine-grained
distribution information: 1) We calculate the per-class BNS of the calibration
dataset as the BNS centers of each class and propose a BNS-centralized loss to
force the synthetic data distributions of different classes to be close to
their own centers. 2) We add Gaussian noise into the centers to imitate the
incohesion and propose a BNS-distorted loss to force the synthetic data
distribution of the same class to be close to the distorted centers. By
utilizing these two fine-grained losses, our method manifests the
state-of-the-art performance on ImageNet, especially when both the first and
last layers are quantized to the low-bit. Code is at
\url{https://github.com/zysxmu/FDDA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pixel-wise Energy-biased Abstention Learning for Anomaly Segmentation on Complex Urban Driving Scenes. (arXiv:2111.12264v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12264">
<div class="article-summary-box-inner">
<span><p>State-of-the-art (SOTA) anomaly segmentation approaches on complex urban
driving scenes explore pixel-wise classification uncertainty learned from
outlier exposure, or external reconstruction models. However, previous
uncertainty approaches that directly associate high uncertainty to anomaly may
sometimes lead to incorrect anomaly predictions, and external reconstruction
models tend to be too inefficient for real-time self-driving embedded systems.
In this paper, we propose a new anomaly segmentation method, named pixel-wise
energy-biased abstention learning (PEBAL), that explores pixel-wise abstention
learning (AL) with a model that learns an adaptive pixel-level anomaly class,
and an energy-based model (EBM) that learns inlier pixel distribution. More
specifically, PEBAL is based on a non-trivial joint training of EBM and AL,
where EBM is trained to output high-energy for anomaly pixels (from outlier
exposure) and AL is trained such that these high-energy pixels receive adaptive
low penalty for being included to the anomaly class. We extensively evaluate
PEBAL against the SOTA and show that it achieves the best performance across
four benchmarks. Code is available at https://github.com/tianyu0207/PEBAL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Adaptation for Implicit Object Tracking and Shape Reconstruction in the Wild. (arXiv:2111.12728v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12728">
<div class="article-summary-box-inner">
<span><p>Tracking and reconstructing 3D objects from cluttered scenes are the key
components for computer vision, robotics and autonomous driving systems. While
recent progress in implicit function has shown encouraging results on
high-quality 3D shape reconstruction, it is still very challenging to
generalize to cluttered and partially observable LiDAR data. In this paper, we
propose to leverage the continuity in video data. We introduce a novel and
unified framework which utilizes a neural implicit function to simultaneously
track and reconstruct 3D objects in the wild. Our approach adapts the DeepSDF
model (i.e., an instantiation of the implicit function) in the video online,
iteratively improving the shape reconstruction while in return improving the
tracking, and vice versa. We experiment with both Waymo and KITTI datasets and
show significant improvements over state-of-the-art methods for both tracking
and shape reconstruction tasks. Our project page is at
https://jianglongye.com/implicit-tracking .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seeking Salient Facial Regions for Cross-Database Micro-Expression Recognition. (arXiv:2111.15361v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15361">
<div class="article-summary-box-inner">
<span><p>Cross-Database Micro-Expression Recognition (CDMER) aims to develop the
Micro-Expression Recognition (MER) methods with strong domain adaptability,
i.e., the ability to recognize the Micro-Expressions (MEs) of different
subjects captured by different imaging devices in different scenes. The
development of CDMER is faced with two key problems: 1) the severe feature
distribution gap between the source and target databases; 2) the feature
representation bottleneck of ME such local and subtle facial expressions. To
solve these problems, this paper proposes a novel Transfer Group Sparse
Regression method, namely TGSR, which aims to 1) optimize the measurement and
better alleviate the difference between the source and target databases, and 2)
highlight the valid facial regions to enhance extracted features, by the
operation of selecting the group features from the raw face feature, where each
region is associated with a group of raw face feature, i.e., the salient facial
region selection. Compared with previous transfer group sparse methods, our
proposed TGSR has the ability to select the salient facial regions, which is
effective in alleviating the aforementioned problems for better performance and
reducing the computational cost at the same time. We use two public ME
databases, i.e., CASME II and SMIC, to evaluate our proposed TGSR method.
Experimental results show that our proposed TGSR learns the discriminative and
explicable regions, and outperforms most state-of-the-art
subspace-learning-based domain-adaptive methods for CDMER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ScaleNet: A Shallow Architecture for Scale Estimation. (arXiv:2112.04846v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04846">
<div class="article-summary-box-inner">
<span><p>In this paper, we address the problem of estimating scale factors between
images. We formulate the scale estimation problem as a prediction of a
probability distribution over scale factors. We design a new architecture,
ScaleNet, that exploits dilated convolutions as well as self and
cross-correlation layers to predict the scale between images. We demonstrate
that rectifying images with estimated scales leads to significant performance
improvements for various tasks and methods. Specifically, we show how ScaleNet
can be combined with sparse local features and dense correspondence networks to
improve camera pose estimation, 3D reconstruction, or dense geometric matching
in different benchmarks and datasets. We provide an extensive evaluation on
several tasks and analyze the computational overhead of ScaleNet. The code,
evaluation protocols, and trained models are publicly available at
https://github.com/axelBarroso/ScaleNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Equivalence of Siamese Self-Supervised Learning via A Unified Gradient Framework. (arXiv:2112.05141v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05141">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning has shown its great potential to extract powerful
visual representations without human annotations. Various works are proposed to
deal with self-supervised learning from different perspectives: (1) contrastive
learning methods (e.g., MoCo, SimCLR) utilize both positive and negative
samples to guide the training direction; (2) asymmetric network methods (e.g.,
BYOL, SimSiam) get rid of negative samples via the introduction of a predictor
network and the stop-gradient operation; (3) feature decorrelation methods
(e.g., Barlow Twins, VICReg) instead aim to reduce the redundancy between
feature dimensions. These methods appear to be quite different in the designed
loss functions from various motivations. The final accuracy numbers also vary,
where different networks and tricks are utilized in different works. In this
work, we demonstrate that these methods can be unified into the same form.
Instead of comparing their loss functions, we derive a unified formula through
gradient analysis. Furthermore, we conduct fair and detailed experiments to
compare their performances. It turns out that there is little gap between these
methods, and the use of momentum encoder is the key factor to boost
performance. From this unified framework, we propose UniGrad, a simple but
effective gradient form for self-supervised learning. It does not require a
memory bank or a predictor network, but can still achieve state-of-the-art
performance and easily adopt other training strategies. Extensive experiments
on linear evaluation and many downstream tasks also show its effectiveness.
Code is released at https://github.com/fundamentalvision/UniGrad.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAC-GAN: Structure-Aware Image Composition. (arXiv:2112.06596v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06596">
<div class="article-summary-box-inner">
<span><p>We introduce an end-to-end learning framework for image-to-image composition,
aiming to seamlessly compose an object represented as a cropped patch from an
object image into a background scene image. As our approach emphasizes more on
semantic and structural coherence of the composed images, rather than their
pixel-level RGB accuracies, we tailor the input and output of our network with
structure-aware features and design our network losses accordingly, with ground
truth established in a self-supervised setting through the object cropping.
Specifically, our network takes the semantic layout features from the input
scene image, features encoded from the edges and silhouette in the input object
patch, as well as a latent code as inputs, and generates a 2D spatial affine
transform defining the translation and scaling of the object patch. The learned
parameters are further fed into a differentiable spatial transformer network to
transform the object patch into the target image, where our model is trained
adversarially using an affine transform discriminator and a layout
discriminator. We evaluate our network, coined SAC-GAN, for various image
composition scenarios in terms of quality, composability, and generalizability
of the composite images. Comparisons are made to state-of-the-art alternatives,
including Instance Insertion, ST-GAN, CompGAN and PlaceNet, confirming
superiority of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neuromorphic Camera Denoising using Graph Neural Network-driven Transformers. (arXiv:2112.09685v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09685">
<div class="article-summary-box-inner">
<span><p>Neuromorphic vision is a bio-inspired technology that has triggered a
paradigm shift in the computer-vision community and is serving as a key-enabler
for a multitude of applications. This technology has offered significant
advantages including reduced power consumption, reduced processing needs, and
communication speed-ups. However, neuromorphic cameras suffer from significant
amounts of measurement noise. This noise deteriorates the performance of
neuromorphic event-based perception and navigation algorithms. In this paper,
we propose a novel noise filtration algorithm to eliminate events which do not
represent real log-intensity variations in the observed scene. We employ a
Graph Neural Network (GNN)-driven transformer algorithm, called
GNN-Transformer, to classify every active event pixel in the raw stream into
real-log intensity variation or noise. Within the GNN, a message-passing
framework, called EventConv, is carried out to reflect the spatiotemporal
correlation among the events, while preserving their asynchronous nature. We
also introduce the Known-object Ground-Truth Labeling (KoGTL) approach for
generating approximate ground truth labels of event streams under various
illumination conditions. KoGTL is used to generate labeled datasets, from
experiments recorded in chalenging lighting conditions. These datasets are used
to train and extensively test our proposed algorithm. When tested on unseen
datasets, the proposed algorithm outperforms existing methods by 8.8% in terms
of filtration accuracy. Additional tests are also conducted on publicly
available datasets to demonstrate the generalization capabilities of the
proposed algorithm in the presence of illumination variations and different
motion dynamics. Compared to existing solutions, qualitative results verified
the superior capability of the proposed algorithm to eliminate noise while
preserving meaningful scene events.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Attention-Model Explainability through Faithfulness Violation Test. (arXiv:2201.12114v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12114">
<div class="article-summary-box-inner">
<span><p>Attention mechanisms are dominating the explainability of deep models. They
produce probability distributions over the input, which are widely deemed as
feature-importance indicators. However, in this paper, we find one critical
limitation in attention explanations: weakness in identifying the polarity of
feature impact. This would be somehow misleading -- features with higher
attention weights may not faithfully contribute to model predictions; instead,
they can impose suppression effects. With this finding, we reflect on the
explainability of current attention-based techniques, such as
Attentio$\odot$Gradient and LRP-based attention explanations. We first propose
an actionable diagnostic methodology (henceforth faithfulness violation test)
to measure the consistency between explanation weights and the impact polarity.
Through the extensive experiments, we then show that most tested explanation
methods are unexpectedly hindered by the faithfulness violation issue,
especially the raw attention. Empirical analyses on the factors affecting
violation issues further provide useful observations for adopting explanation
methods in attention models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Color Image Inpainting via Robust Pure Quaternion Matrix Completion: Error Bound and Weighted Loss. (arXiv:2202.02063v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02063">
<div class="article-summary-box-inner">
<span><p>In this paper, we study color image inpainting as a pure quaternion matrix
completion problem. In the literature, the theoretical guarantee for quaternion
matrix completion is not well-established. Our main aim is to propose a new
minimization problem with an objective combining nuclear norm and a quadratic
loss weighted among three channels. To fill the theoretical vacancy, we obtain
the error bound in both clean and corrupted regimes, which relies on some new
results of quaternion matrices. A general Gaussian noise is considered in
robust completion where all observations are corrupted. Motivated by the error
bound, we propose to handle unbalanced or correlated noise via a cross-channel
weight in the quadratic loss, with the main purpose of rebalancing noise level,
or removing noise correlation. Extensive experimental results on synthetic and
color image data are presented to confirm and demonstrate our theoretical
findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Offline Text-Independent Writer Identification based on word level data. (arXiv:2202.10207v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10207">
<div class="article-summary-box-inner">
<span><p>This paper proposes a novel scheme to identify the authorship of a document
based on handwritten input word images of an individual. Our approach is
text-independent and does not place any restrictions on the size of the input
word images under consideration. To begin with, we employ the SIFT algorithm to
extract multiple key points at various levels of abstraction (comprising
allograph, character, or combination of characters). These key points are then
passed through a trained CNN network to generate feature maps corresponding to
a convolution layer. However, owing to the scale corresponding to the SIFT key
points, the size of a generated feature map may differ. As an alleviation to
this issue, the histogram of gradients is applied on the feature map to produce
a fixed representation. Typically, in a CNN, the number of filters of each
convolution block increase depending on the depth of the network. Thus,
extracting histogram features for each of the convolution feature map increase
the dimension as well as the computational load. To address this aspect, we use
an entropy-based method to learn the weights of the feature maps of a
particular CNN layer during the training phase of our algorithm. The efficacy
of our proposed system has been demonstrated on two publicly available
databases namely CVL and IAM. We empirically show that the results obtained are
promising when compared with previous works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning based domain adaptation for mitochondria segmentation on EM volumes. (arXiv:2202.10773v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10773">
<div class="article-summary-box-inner">
<span><p>Accurate segmentation of electron microscopy (EM) volumes of the brain is
essential to characterize neuronal structures at a cell or organelle level.
While supervised deep learning methods have led to major breakthroughs in that
direction during the past years, they usually require large amounts of
annotated data to be trained, and perform poorly on other data acquired under
similar experimental and imaging conditions. This is a problem known as domain
adaptation, since models that learned from a sample distribution (or source
domain) struggle to maintain their performance on samples extracted from a
different distribution or target domain. In this work, we address the complex
case of deep learning based domain adaptation for mitochondria segmentation
across EM datasets from different tissues and species. We present three
unsupervised domain adaptation strategies to improve mitochondria segmentation
in the target domain based on (1) state-of-the-art style transfer between
images of both domains; (2) self-supervised learning to pre-train a model using
unlabeled source and target images, and then fine-tune it only with the source
labels; and (3) multi-task neural network architectures trained end-to-end with
both labeled and unlabeled images. Additionally, we propose a new training
stopping criterion based on morphological priors obtained exclusively in the
source domain. We carried out all possible cross-dataset experiments using
three publicly available EM datasets. We evaluated our proposed strategies on
the mitochondria semantic labels predicted on the target datasets. The methods
introduced here outperform the baseline methods and compare favorably to the
state of the art. In the absence of validation labels, monitoring our proposed
morphology-based metric is an intuitive and effective way to stop the training
process and select in average optimal models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Severity classification in cases of Collagen VI-related myopathy with Convolutional Neural Networks and handcrafted texture features. (arXiv:2202.13853v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13853">
<div class="article-summary-box-inner">
<span><p>Magnetic Resonance Imaging (MRI) is a non-invasive tool for the clinical
assessment of low-prevalence neuromuscular disorders. Automated diagnosis
methods might reduce the need for biopsies and provide valuable information on
disease follow-up. In this paper, three methods are proposed to classify target
muscles in Collagen VI-related myopathy cases, based on their degree of
involvement, notably a Convolutional Neural Network, a Fully Connected Network
to classify texture features, and a hybrid method combining the two feature
sets. The proposed methods were evaluated on axial T1-weighted Turbo Spin-Echo
MRI from 26 subjects, including Ullrich Congenital Muscular Dystrophy and
Bethlem Myopathy patients at different evolution stages. The hybrid model
achieved the best cross-validation results, with a global accuracy of 93.8%,
and F-scores of 0.99, 0.82, and 0.95, for healthy, mild and moderate/severe
cases, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Subsampling for Oversampled Data -- Application to Quantitative MRI. (arXiv:2203.09268v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09268">
<div class="article-summary-box-inner">
<span><p>We present PROSUB: PROgressive SUBsampling, a deep learning based, automated
methodology that subsamples an oversampled data set (e.g. multi-channeled 3D
images) with minimal loss of information. We build upon a recent dual-network
approach that won the MICCAI MUlti-DIffusion (MUDI) quantitative MRI
measurement sampling-reconstruction challenge, but suffers from deep learning
training instability, by subsampling with a hard decision boundary. PROSUB uses
the paradigm of recursive feature elimination (RFE) and progressively
subsamples measurements during deep learning training, improving optimization
stability. PROSUB also integrates a neural architecture search (NAS) paradigm,
allowing the network architecture hyperparameters to respond to the subsampling
process. We show PROSUB outperforms the winner of the MUDI MICCAI challenge,
producing large improvements &gt;18% MSE on the MUDI challenge sub-tasks and
qualitative improvements on downstream processes useful for clinical
applications. We also show the benefits of incorporating NAS and analyze the
effect of PROSUB's components. As our method generalizes to other problems
beyond MRI measurement selection-reconstruction, our code is
https://github.com/sbb-gh/PROSUB
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Audio Strikes Back: Boosting Augmentations Towards An Efficient Audio Classification Network. (arXiv:2204.11479v5 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11479">
<div class="article-summary-box-inner">
<span><p>While efficient architectures and a plethora of augmentations for end-to-end
image classification tasks have been suggested and heavily investigated,
state-of-the-art techniques for audio classifications still rely on numerous
representations of the audio signal together with large architectures,
fine-tuned from large datasets. By utilizing the inherited lightweight nature
of audio and novel audio augmentations, we were able to present an efficient
end-to-end network with strong generalization ability. Experiments on a variety
of sound classification sets demonstrate the effectiveness and robustness of
our approach, by achieving state-of-the-art results in various settings. Public
code is available at:
\href{https://github.com/Alibaba-MIIL/AudioClassfication}{this http url}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Generative Distillation. (arXiv:2205.01529v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01529">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation has been applied to various tasks successfully. The
current distillation algorithm usually improves students' performance by
imitating the output of the teacher. This paper shows that teachers can also
improve students' representation power by guiding students' feature recovery.
From this point of view, we propose Masked Generative Distillation (MGD), which
is simple: we mask random pixels of the student's feature and force it to
generate the teacher's full feature through a simple block. MGD is a truly
general feature-based distillation method, which can be utilized on various
tasks, including image classification, object detection, semantic segmentation
and instance segmentation. We experiment on different models with extensive
datasets and the results show that all the students achieve excellent
improvements. Notably, we boost ResNet-18 from 69.90% to 71.69% ImageNet top-1
accuracy, RetinaNet with ResNet-50 backbone from 37.4 to 41.0 Boundingbox mAP,
SOLO based on ResNet-50 from 33.1 to 36.2 Mask mAP and DeepLabV3 based on
ResNet-18 from 73.20 to 76.02 mIoU. Our codes are available at
https://github.com/yzd-v/MGD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Individual Topology Structure of Eye Movement Trajectories. (arXiv:2205.10667v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10667">
<div class="article-summary-box-inner">
<span><p>Traditionally, extracting patterns from eye movement data relies on
statistics of different macro-events such as fixations and saccades. This
requires an additional preprocessing step to separate the eye movement
subtypes, often with a number of parameters on which the classification results
depend. Besides that, definitions of such macro events are formulated in
different ways by different researchers.
</p>
<p>We propose an application of a new class of features to the quantitative
analysis of personal eye movement trajectories structure. This new class of
features based on algebraic topology allows extracting patterns from different
modalities of gaze such as time series of coordinates and amplitudes, heatmaps,
and point clouds in a unified way at all scales from micro to macro. We
experimentally demonstrate the competitiveness of the new class of features
with the traditional ones and their significant synergy while being used
together for the person authentication task on the recently published eye
movement trajectories dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Muti-expert Distribution Calibration for Long-tailed Video Classification. (arXiv:2205.10788v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10788">
<div class="article-summary-box-inner">
<span><p>Most existing state-of-the-art video classification methods assume that the
training data obey a uniform distribution. However, video data in the real
world typically exhibit an imbalanced long-tailed class distribution, resulting
in a model bias towards head class and relatively low performance on tail
class. While the current long-tailed classification methods usually focus on
image classification, adapting it to video data is not a trivial extension. We
propose an end-to-end multi-expert distribution calibration method to address
these challenges based on two-level distribution information. The method
jointly considers the distribution of samples in each class (intra-class
distribution) and the overall distribution of diverse data (inter-class
distribution) to solve the issue of imbalanced data under long-tailed
distribution. By modeling the two-level distribution information, the model can
jointly consider the head classes and the tail classes and significantly
transfer the knowledge from the head classes to improve the performance of the
tail classes. Extensive experiments verify that our method achieves
state-of-the-art performance on the long-tailed video classification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientFormer: Vision Transformers at MobileNet Speed. (arXiv:2206.01191v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01191">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViT) have shown rapid progress in computer vision tasks,
achieving promising results on various benchmarks. However, due to the massive
number of parameters and model design, e.g., attention mechanism, ViT-based
models are generally times slower than lightweight convolutional networks.
Therefore, the deployment of ViT for real-time applications is particularly
challenging, especially on resource-constrained hardware such as mobile
devices. Recent efforts try to reduce the computation complexity of ViT through
network architecture search or hybrid design with MobileNet block, yet the
inference speed is still unsatisfactory. This leads to an important question:
can transformers run as fast as MobileNet while obtaining high performance? To
answer this, we first revisit the network architecture and operators used in
ViT-based models and identify inefficient designs. Then we introduce a
dimension-consistent pure transformer (without MobileNet blocks) as a design
paradigm. Finally, we perform latency-driven slimming to get a series of final
models dubbed EfficientFormer. Extensive experiments show the superiority of
EfficientFormer in performance and speed on mobile devices. Our fastest model,
EfficientFormer-L1, achieves $79.2\%$ top-1 accuracy on ImageNet-1K with only
$1.6$ ms inference latency on iPhone 12 (compiled with CoreML), which { runs as
fast as MobileNetV2$\times 1.4$ ($1.6$ ms, $74.7\%$ top-1),} and our largest
model, EfficientFormer-L7, obtains $83.3\%$ accuracy with only $7.0$ ms
latency. Our work proves that properly designed transformers can reach
extremely low latency on mobile devices while maintaining high performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Siamese Image Modeling for Self-Supervised Vision Representation Learning. (arXiv:2206.01204v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01204">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) has delivered superior performance on a
variety of downstream vision tasks. Two main-stream SSL frameworks have been
proposed, i.e., Instance Discrimination (ID) and Masked Image Modeling (MIM).
ID pulls together the representations of different views from the same image,
while avoiding feature collapse. It does well on linear probing but is inferior
in detection performance. On the other hand, MIM reconstructs the original
content given a masked image. It excels at dense prediction but fails to
perform well on linear probing. Their distinctions are caused by neglecting the
representation requirements of either semantic alignment or spatial
sensitivity. Specifically, we observe that (1) semantic alignment demands
semantically similar views to be projected into nearby representation, which
can be achieved by contrasting different views with strong augmentations; (2)
spatial sensitivity requires to model the local structure within an image.
Predicting dense representations with masked image is therefore beneficial
because it models the conditional distribution of image content. Driven by
these analysis, we propose Siamese Image Modeling (SIM), which predicts the
dense representations of an augmented view, based on another masked view from
the same image but with different augmentations. Our method uses a Siamese
network with two branches. The online branch encodes the first view, and
predicts the second view's representation according to the relative positions
between these two views. The target branch produces the target by encoding the
second view. In this way, we are able to achieve comparable linear probing and
dense prediction performances with ID and MIM, respectively. We also
demonstrate that decent linear probing result can be obtained without a global
loss. Code shall be released at
https://github.com/fundamentalvision/Siamese-Image-Modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a General Purpose CNN for Long Range Dependencies in $N$D. (arXiv:2206.03398v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03398">
<div class="article-summary-box-inner">
<span><p>The use of Convolutional Neural Networks (CNNs) is widespread in Deep
Learning due to a range of desirable model properties which result in an
efficient and effective machine learning framework. However, performant CNN
architectures must be tailored to specific tasks in order to incorporate
considerations such as the input length, resolution, and dimentionality. In
this work, we overcome the need for problem-specific CNN architectures with our
Continuous Convolutional Neural Network (CCNN): a single CNN architecture
equipped with continuous convolutional kernels that can be used for tasks on
data of arbitrary resolution, dimensionality and length without structural
changes. Continuous convolutional kernels model long range dependencies at
every layer, and remove the need for downsampling layers and task-dependent
depths needed in current CNN architectures. We show the generality of our
approach by applying the same CCNN to a wide set of tasks on sequential
(1$\mathrm{D}$) and visual data (2$\mathrm{D}$). Our CCNN performs
competitively and often outperforms the current state-of-the-art across all
tasks considered.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs. (arXiv:2206.04674v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04674">
<div class="article-summary-box-inner">
<span><p>To build an artificial neural network like the biological intelligence
system, recent works have unified numerous tasks into a generalist model, which
can process various tasks with shared parameters and do not have any
task-specific modules. While generalist models achieve promising results on
various benchmarks, they have performance degradation on some tasks compared
with task-specialized models. In this work, we find that interference among
different tasks and modalities is the main factor to this phenomenon. To
mitigate such interference, we introduce the Conditional Mixture-of-Experts
(Conditional MoEs) to generalist models. Routing strategies under different
levels of conditions are proposed to take both the training/inference cost and
generalization ability into account. By incorporating the proposed Conditional
MoEs, the recently proposed generalist model Uni-Perceiver can effectively
mitigate the interference across tasks and modalities, and achieves
state-of-the-art results on a series of downstream tasks via prompt tuning on
1% of downstream data. Moreover, the introduction of Conditional MoEs still
holds the generalization ability of generalist models to conduct zero-shot
inference on new tasks, e.g., video-text retrieval and video caption. Code and
pre-trained generalist models shall be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making Sense of Dependence: Efficient Black-box Explanations Using Dependence Measure. (arXiv:2206.06219v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06219">
<div class="article-summary-box-inner">
<span><p>This paper presents a new efficient black-box attribution method based on
Hilbert-Schmidt Independence Criterion (HSIC), a dependence measure based on
Reproducing Kernel Hilbert Spaces (RKHS). HSIC measures the dependence between
regions of an input image and the output of a model based on kernel embeddings
of distributions. It thus provides explanations enriched by RKHS representation
capabilities. HSIC can be estimated very efficiently, significantly reducing
the computational cost compared to other black-box attribution methods. Our
experiments show that HSIC is up to 8 times faster than the previous best
black-box attribution methods while being as faithful. Indeed, we improve or
match the state-of-the-art of both black-box and white-box attribution methods
for several fidelity metrics on Imagenet with various recent model
architectures. Importantly, we show that these advances can be transposed to
efficiently and faithfully explain object detection models such as YOLOv4.
Finally, we extend the traditional attribution methods by proposing a new
kernel enabling an orthogonal decomposition of importance scores based on HSIC,
allowing us to evaluate not only the importance of each image patch but also
the importance of their pairwise interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Sampling: Exploring Out-of-Distribution data for Re-balancing Long-tailed datasets. (arXiv:2206.08802v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08802">
<div class="article-summary-box-inner">
<span><p>Deep neural networks usually perform poorly when the training dataset suffers
from extreme class imbalance. Recent studies found that directly training with
out-of-distribution data (i.e., open-set samples) in a semi-supervised manner
would harm the generalization performance. In this work, we theoretically show
that out-of-distribution data can still be leveraged to augment the minority
classes from a Bayesian perspective. Based on this motivation, we propose a
novel method called Open-sampling, which utilizes open-set noisy labels to
re-balance the class priors of the training dataset. For each open-set
instance, the label is sampled from our pre-defined distribution that is
complementary to the distribution of original class priors. We empirically show
that Open-sampling not only re-balances the class priors but also encourages
the neural network to learn separable representations. Extensive experiments
demonstrate that our proposed method significantly outperforms existing data
re-balancing methods and can boost the performance of existing state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations. (arXiv:2206.15462v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.15462">
<div class="article-summary-box-inner">
<span><p>We propose a margin-based loss for vision-language model pretraining that
encourages gradient-based explanations that are consistent with region-level
annotations. We refer to this objective as Attention Mask Consistency (AMC) and
demonstrate that it produces superior visual grounding performance compared to
models that rely instead on region-level annotations for explicitly training an
object detector such as Faster R-CNN. AMC works by encouraging gradient-based
explanation masks that focus their attention scores mostly within annotated
regions of interest for images that contain such annotations. Particularly, a
model trained with AMC on top of standard vision-language modeling objectives
obtains a state-of-the-art accuracy of 86.59% in the Flickr30k visual grounding
benchmark, an absolute improvement of 5.48% when compared to the best previous
model. Our approach also performs exceedingly well on established benchmarks
for referring expression comprehension and offers the added benefit by design
of gradient-based explanations that better align with human annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature-selected Graph Spatial Attention Network for Addictive Brain-Networks Identification. (arXiv:2207.00583v2 [q-bio.NC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00583">
<div class="article-summary-box-inner">
<span><p>Functional alterations in the relevant neural circuits occur from drug
addiction over a certain period. And these significant alterations are also
revealed by analyzing fMRI. However, because of fMRI's high dimensionality and
poor signal-to-noise ratio, it is challenging to encode efficient and robust
brain regional embeddings for both graph-level identification and region-level
biomarkers detection tasks between nicotine addiction (NA) and healthy control
(HC) groups. In this work, we represent the fMRI of the rat brain as a graph
with biological attributes and propose a novel feature-selected graph spatial
attention network(FGSAN) to extract the biomarkers of addiction and identify
from these brain networks. Specially, a graph spatial attention encoder is
employed to capture the features of spatiotemporal brain networks with spatial
information. The method simultaneously adopts a Bayesian feature selection
strategy to optimize the model and improve classification task by constraining
features. Experiments on an addiction-related neural imaging dataset show that
the proposed model can obtain superior performance and detect interpretable
biomarkers associated with addiction-relevant neural circuits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ABAW: Learning from Synthetic Data & Multi-Task Learning Challenges. (arXiv:2207.01138v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01138">
<div class="article-summary-box-inner">
<span><p>This paper describes the fourth Affective Behavior Analysis in-the-wild
(ABAW) Competition, held in conjunction with European Conference on Computer
Vision (ECCV), 2022. The 4th ABAW Competition is a continuation of the
Competitions held at IEEE CVPR 2022, ICCV 2021, IEEE FG 2020 and IEEE CVPR 2017
Conferences, and aims at automatically analyzing affect. In the previous runs
of this Competition, the Challenges targeted Valence-Arousal Estimation,
Expression Classification and Action Unit Detection. This year the Competition
encompasses two different Challenges: i) a Multi-Task-Learning one in which the
goal is to learn at the same time (i.e., in a multi-task learning setting) all
the three above mentioned tasks; and ii) a Learning from Synthetic Data one in
which the goal is to learn to recognise the basic expressions from artificially
generated data and generalise to real data. The Aff-Wild2 database is a large
scale in-the-wild database and the first one that contains annotations for
valence and arousal, expressions and action units. This database is the basis
for the above Challenges. In more detail: i) s-Aff-Wild2 -- a static version of
Aff-Wild2 database -- has been constructed and utilized for the purposes of the
Multi-Task-Learning Challenge; and ii) some specific frames-images from the
Aff-Wild2 database have been used in an expression manipulation manner for
creating the synthetic dataset, which is the basis for the Learning from
Synthetic Data Challenge. In this paper, at first we present the two
Challenges, along with the utilized corpora, then we outline the evaluation
metrics and finally present the baseline systems per Challenge, as well as
their derived results. More information regarding the Competition can be found
in the competition's website:
https://ibug.doc.ic.ac.uk/resources/eccv-2023-4th-abaw/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attributed Abnormality Graph Embedding for Clinically Accurate X-Ray Report Generation. (arXiv:2207.01208v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01208">
<div class="article-summary-box-inner">
<span><p>Automatic generation of medical reports from X-ray images can assist
radiologists to perform the time-consuming and yet important reporting task.
Yet, achieving clinically accurate generated reports remains challenging.
Modeling the underlying abnormalities using the knowledge graph approach has
been found promising in enhancing the clinical accuracy. In this paper, we
introduce a novel fined-grained knowledge graph structure called an attributed
abnormality graph (ATAG). The ATAG consists of interconnected abnormality nodes
and attribute nodes, allowing it to better capture the abnormality details. In
contrast to the existing methods where the abnormality graph was constructed
manually, we propose a methodology to automatically construct the fine-grained
graph structure based on annotations, medical reports in X-ray datasets, and
the RadLex radiology lexicon. We then learn the ATAG embedding using a deep
model with an encoder-decoder architecture for the report generation. In
particular, graph attention networks are explored to encode the relationships
among the abnormalities and their attributes. A gating mechanism is adopted and
integrated with various decoders for the generation. We carry out extensive
experiments based on the benchmark datasets, and show that the proposed
ATAG-based deep model outperforms the SOTA methods by a large margin and can
improve the clinical accuracy of the generated reports.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of ADHD based on Eye Movements during Natural Viewing. (arXiv:2207.01377v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01377">
<div class="article-summary-box-inner">
<span><p>Attention-deficit/hyperactivity disorder (ADHD) is a neurodevelopmental
disorder that is highly prevalent and requires clinical specialists to
diagnose. It is known that an individual's viewing behavior, reflected in their
eye movements, is directly related to attentional mechanisms and higher-order
cognitive processes. We therefore explore whether ADHD can be detected based on
recorded eye movements together with information about the video stimulus in a
free-viewing task. To this end, we develop an end-to-end deep learning-based
sequence model which we pre-train on a related task for which more data are
available. We find that the method is in fact able to detect ADHD and
outperforms relevant baselines. We investigate the relevance of the input
features in an ablation study. Interestingly, we find that the model's
performance is closely related to the content of the video, which provides
insights for future experimental designs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Positive-Negative Equal Contrastive Loss for Semantic Segmentation. (arXiv:2207.01417v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01417">
<div class="article-summary-box-inner">
<span><p>The contextual information is critical for various computer vision tasks,
previous works commonly design plug-and-play modules and structural losses to
effectively extract and aggregate the global context. These methods utilize
fine-label to optimize the model but ignore that fine-trained features are also
precious training resources, which can introduce preferable distribution to
hard pixels (i.e., misclassified pixels). Inspired by contrastive learning in
unsupervised paradigm, we apply the contrastive loss in a supervised manner and
re-design the loss function to cast off the stereotype of unsupervised learning
(e.g., imbalance of positives and negatives, confusion of anchors computing).
To this end, we propose Positive-Negative Equal contrastive loss (PNE loss),
which increases the latent impact of positive embedding on the anchor and
treats the positive as well as negative sample pairs equally. The PNE loss can
be directly plugged right into existing semantic segmentation frameworks and
leads to excellent performance with neglectable extra computational costs. We
utilize a number of classic segmentation methods (e.g., DeepLabV3, OCRNet,
UperNet) and backbone (e.g., ResNet, HRNet, Swin Transformer) to conduct
comprehensive experiments and achieve state-of-the-art performance on two
benchmark datasets (e.g., Cityscapes and COCO-Stuff). Our code will be publicly
available soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fidelity of Ensemble Aggregation for Saliency Map Explanations using Bayesian Optimization Techniques. (arXiv:2207.01565v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01565">
<div class="article-summary-box-inner">
<span><p>In recent years, an abundance of feature attribution methods for explaining
neural networks have been developed. Especially in the field of computer
vision, many methods for generating saliency maps providing pixel attributions
exist. However, their explanations often contradict each other and it is not
clear which explanation to trust. A natural solution to this problem is the
aggregation of multiple explanations. We present and compare different
pixel-based aggregation schemes with the goal of generating a new explanation,
whose fidelity to the model's decision is higher than each individual
explanation. Using methods from the field of Bayesian Optimization, we
incorporate the variance between the individual explanations into the
aggregation process. Additionally, we analyze the effect of multiple
normalization techniques on ensemble aggregation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaTeRF: Label and Text Driven Object Radiance Fields. (arXiv:2207.01583v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01583">
<div class="article-summary-box-inner">
<span><p>Obtaining 3D object representations is important for creating photo-realistic
simulators and collecting assets for AR/VR applications. Neural fields have
shown their effectiveness in learning a continuous volumetric representation of
a scene from 2D images, but acquiring object representations from these models
with weak supervision remains an open challenge. In this paper we introduce
LaTeRF, a method for extracting an object of interest from a scene given 2D
images of the entire scene and known camera poses, a natural language
description of the object, and a small number of point-labels of object and
non-object points in the input images. To faithfully extract the object from
the scene, LaTeRF extends the NeRF formulation with an additional `objectness'
probability at each 3D point. Additionally, we leverage the rich latent space
of a pre-trained CLIP model combined with our differentiable object renderer,
to inpaint the occluded parts of the object. We demonstrate high-fidelity
object extraction on both synthetic and real datasets and justify our design
choices through an extensive ablation study.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-07-06 23:07:48.938688507 UTC">2022-07-06 23:07:48 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>