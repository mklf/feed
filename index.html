<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-30T01:30:00Z">09-30</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Simplification for Comprehension-based Question-Answering. (arXiv:2109.13984v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13984">
<div class="article-summary-box-inner">
<span><p>Text simplification is the process of splitting and rephrasing a sentence to
a sequence of sentences making it easier to read and understand while
preserving the content and approximating the original meaning. Text
simplification has been exploited in NLP applications like machine translation,
summarization, semantic role labeling, and information extraction, opening a
broad avenue for its exploitation in comprehension-based question-answering
downstream tasks. In this work, we investigate the effect of text
simplification in the task of question-answering using a comprehension context.
We release Simple-SQuAD, a simplified version of the widely-used SQuAD dataset.
</p>
<p>Firstly, we outline each step in the dataset creation pipeline, including
style transfer, thresholding of sentences showing correct transfer, and offset
finding for each answer. Secondly, we verify the quality of the transferred
sentences through various methodologies involving both automated and human
evaluation. Thirdly, we benchmark the newly created corpus and perform an
ablation study for examining the effect of the simplification process in the
SQuAD-based question answering task. Our experiments show that simplification
leads to up to 2.04% and 1.74% increase in Exact Match and F1, respectively.
Finally, we conclude with an analysis of the transfer process, investigating
the types of edits made by the model, and the effect of sentence length on the
transfer model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shaking Syntactic Trees on the Sesame Street: Multilingual Probing with Controllable Perturbations. (arXiv:2109.14017v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14017">
<div class="article-summary-box-inner">
<span><p>Recent research has adopted a new experimental field centered around the
concept of text perturbations which has revealed that shuffled word order has
little to no impact on the downstream performance of Transformer-based language
models across many NLP tasks. These findings contradict the common
understanding of how the models encode hierarchical and structural information
and even question if the word order is modeled with position embeddings. To
this end, this paper proposes nine probing datasets organized by the type of
\emph{controllable} text perturbation for three Indo-European languages with a
varying degree of word order flexibility: English, Swedish and Russian. Based
on the probing analysis of the M-BERT and M-BART models, we report that the
syntactic sensitivity depends on the language and model pre-training
objectives. We also find that the sensitivity grows across layers together with
the increase of the perturbation granularity. Last but not least, we show that
the models barely use the positional information to induce syntactic trees from
their intermediate self-attention and contextualized representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Marked Attribute Bias in Natural Language Inference. (arXiv:2109.14039v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14039">
<div class="article-summary-box-inner">
<span><p>Reporting and providing test sets for harmful bias in NLP applications is
essential for building a robust understanding of the current problem. We
present a new observation of gender bias in a downstream NLP application:
marked attribute bias in natural language inference. Bias in downstream
applications can stem from training data, word embeddings, or be amplified by
the model in use. However, focusing on biased word embeddings is potentially
the most impactful first step due to their universal nature. Here we seek to
understand how the intrinsic properties of word embeddings contribute to this
observed marked attribute effect, and whether current post-processing methods
address the bias successfully. An investigation of the current debiasing
landscape reveals two open problems: none of the current debiased embeddings
mitigate the marked attribute error, and none of the intrinsic bias measures
are predictive of the marked attribute effect. By noticing that a new type of
intrinsic bias measure correlates meaningfully with the marked attribute
effect, we propose a new postprocessing debiasing scheme for static word
embeddings. The proposed method applied to existing embeddings achieves new
best results on the marked attribute bias test set. See
https://github.com/hillary-dawkins/MAB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Second Order WinoBias (SoWinoBias) Test Set for Latent Gender Bias Detection in Coreference Resolution. (arXiv:2109.14047v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14047">
<div class="article-summary-box-inner">
<span><p>We observe an instance of gender-induced bias in a downstream application,
despite the absence of explicit gender words in the test cases. We provide a
test set, SoWinoBias, for the purpose of measuring such latent gender bias in
coreference resolution systems. We evaluate the performance of current
debiasing methods on the SoWinoBias test set, especially in reference to the
method's design and altered embedding space properties. See
https://github.com/hillarydawkins/SoWinoBias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Summaries for Scientific Paper Review. (arXiv:2109.14059v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14059">
<div class="article-summary-box-inner">
<span><p>The review process is essential to ensure the quality of publications.
Recently, the increase of submissions for top venues in machine learning and
NLP has caused a problem of excessive burden on reviewers and has often caused
concerns regarding how this may not only overload reviewers, but also may
affect the quality of the reviews. An automatic system for assisting with the
reviewing process could be a solution for ameliorating the problem. In this
paper, we explore automatic review summary generation for scientific papers. We
posit that neural language models have the potential to be valuable candidates
for this task. In order to test this hypothesis, we release a new dataset of
scientific papers and their reviews, collected from papers published in the
NeurIPS conference from 2013 to 2020. We evaluate state of the art neural
summarization models, present initial results on the feasibility of automatic
review summary generation, and propose directions for the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAFT: A Real-World Few-Shot Text Classification Benchmark. (arXiv:2109.14076v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14076">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models have shown promise for few-shot learning,
completing text-based tasks given only a few task-specific examples. Will
models soon solve classification tasks that have so far been reserved for human
research assistants? Existing benchmarks are not designed to measure progress
in applied settings, and so don't directly answer this question. The RAFT
benchmark (Real-world Annotated Few-shot Tasks) focuses on naturally occurring
tasks and uses an evaluation setup that mirrors deployment. Baseline
evaluations on RAFT reveal areas current techniques struggle with: reasoning
over long texts and tasks with many classes. Human baselines show that some
classification tasks are difficult for non-expert humans, reflecting that
real-world value sometimes depends on domain expertise. Yet even non-expert
human baseline F1 scores exceed GPT-3 by an average of 0.11. The RAFT datasets
and leaderboard will track which model improvements translate into real-world
benefits at https://raft.elicit.org .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding. (arXiv:2109.14084v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14084">
<div class="article-summary-box-inner">
<span><p>We present VideoCLIP, a contrastive approach to pre-train a unified model for
zero-shot video and text understanding, without using any labels on downstream
tasks. VideoCLIP trains a transformer for video and text by contrasting
temporally overlapping positive video-text pairs with hard negatives from
nearest neighbor retrieval. Our experiments on a diverse series of downstream
tasks, including sequence-level text-video retrieval, VideoQA, token-level
action localization, and action segmentation reveal state-of-the-art
performance, surpassing prior work, and in some cases even outperforming
supervised approaches. Code is made available at
https://github.com/pytorch/fairseq/examples/MMPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Video-Language Segmentation. (arXiv:2109.14131v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14131">
<div class="article-summary-box-inner">
<span><p>We focus on the problem of segmenting a certain object referred by a natural
language sentence in video content, at the core of formulating a pinpoint
vision-language relation. While existing attempts mainly construct such
relation in an implicit way, i.e., grid-level multi-modal feature fusion, it
has been proven problematic to distinguish semantically similar objects under
this paradigm. In this work, we propose to interwind the visual and linguistic
modalities in an explicit way via the contrastive learning objective, which
directly aligns the referred object and the language description and separates
the unreferred content apart across frames. Moreover, to remedy for the
degradation problem, we present two complementary hard instance mining
strategies, i.e., Language-relevant Channel Filter and Relative Hard Instance
Construction. They encourage the network to exclude visual-distinguishable
feature and to focus on easy-confused objects during the contrastive training.
Extensive experiments on two benchmarks, i.e., A2D Sentences and J-HMDB
Sentences, quantitatively demonstrate the state-of-the-arts performance of our
method and qualitatively show the more accurate distinguishment between
semantically similar objects over baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Dialogue State Tracking by Joint Slot Modeling. (arXiv:2109.14144v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14144">
<div class="article-summary-box-inner">
<span><p>Dialogue state tracking models play an important role in a task-oriented
dialogue system. However, most of them model the slot types conditionally
independently given the input. We discover that it may cause the model to be
confused by slot types that share the same data type. To mitigate this issue,
we propose TripPy-MRF and TripPy-LSTM that models the slots jointly. Our
results show that they are able to alleviate the confusion mentioned above, and
they push the state-of-the-art on dataset MultiWoZ 2.1 from 58.7 to 61.3. Our
implementation is available at https://github.com/CTinRay/Trippy-Joint.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Arabic Diacritization by Learning to Diacritize and Translate. (arXiv:2109.14150v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14150">
<div class="article-summary-box-inner">
<span><p>We propose a novel multitask learning method for diacritization which trains
a model to both diacritize and translate. Our method addresses data sparsity by
exploiting large, readily available bitext corpora. Furthermore, translation
requires implicit linguistic and semantic knowledge, which is helpful for
resolving ambiguities in the diacritization task. We apply our method to the
Penn Arabic Treebank and report a new state-of-the-art word error rate of
4.79%. We also conduct manual and automatic analysis to better understand our
method and highlight some of the remaining challenges in diacritization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reflexivity in Issues of Scale and Representation in a Digital Humanities Project. (arXiv:2109.14184v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14184">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore issues that we have encountered in developing a
pipeline that combines natural language processing with data analysis and
visualization techniques. The characteristics of the corpus - being comprised
of diaries of a single person spanning several decades - present both
conceptual challenges in terms of issues of representation, and affordances as
a source for historical research. We consider these issues in a team context
with a particular focus on the generation and interpretation of visualizations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context based Roman-Urdu to Urdu Script Transliteration System. (arXiv:2109.14197v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14197">
<div class="article-summary-box-inner">
<span><p>Now a day computer is necessary for human being and it is very useful in many
fields like search engine, text processing, short messaging services, voice
chatting and text recognition. Since last many years there are many tools and
techniques that have been developed to support the writing of language script.
Most of the Asian languages like Arabic, Urdu, Persian, Chains and Korean are
written in Roman alphabets. Roman alphabets are the most commonly used for
transliteration of languages, which have non-Latin scripts. For writing Urdu
characters as an input, there are many layouts which are already exist. Mostly
Urdu speaker prefer to use Roman-Urdu for different applications, because
mostly user is not familiar with Urdu language keyboard. The objective of this
work is to improve the context base transliteration of Roman-Urdu to Urdu
script. In this paper, we propose an algorithm which effectively solve the
transliteration issues. The algorithm work like, convert the encoding roman
words into the words in the standard Urdu script and match it with the lexicon.
If match found, then display the word in the text editor. The highest frequency
words are displayed if more than one match found in the lexicon. Display the
first encoded and converted instance and set it to the default if there is not
a single instance of the match is found and then adjust the given ambiguous
word to their desire location according to their context. The outcome of this
algorithm proved the efficiency and significance as compare to other models and
algorithms which work for transliteration of Raman-Urdu to Urdu on context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Who says like a style of Vitamin: Towards Syntax-Aware DialogueSummarization using Multi-task Learning. (arXiv:2109.14199v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14199">
<div class="article-summary-box-inner">
<span><p>Abstractive dialogue summarization is a challenging task for several reasons.
First, most of the important pieces of information in a conversation are
scattered across utterances through multi-party interactions with different
textual styles. Second, dialogues are often informal structures, wherein
different individuals express personal perspectives, unlike text summarization,
tasks that usually target formal documents such as news articles. To address
these issues, we focused on the association between utterances from individual
speakers and unique syntactic structures. Speakers have unique textual styles
that can contain linguistic information, such as voiceprint. Therefore, we
constructed a syntax-aware model by leveraging linguistic information (i.e.,
POS tagging), which alleviates the above issues by inherently distinguishing
sentences uttered from individual speakers. We employed multi-task learning of
both syntax-aware information and dialogue summarization. To the best of our
knowledge, our approach is the first method to apply multi-task learning to the
dialogue summarization task. Experiments on a SAMSum corpus (a large-scale
dialogue summarization corpus) demonstrated that our method improved upon the
vanilla model. We further analyze the costs and benefits of our approach
relative to baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? -- A computational investigation. (arXiv:2109.14200v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14200">
<div class="article-summary-box-inner">
<span><p>Decades of research has studied how language learning infants learn to
discriminate speech sounds, segment words, and associate words with their
meanings. While gradual development of such capabilities is unquestionable, the
exact nature of these skills and the underlying mental representations yet
remains unclear. In parallel, computational studies have shown that basic
comprehension of speech can be achieved by statistical learning between speech
and concurrent referentially ambiguous visual input. These models can operate
without prior linguistic knowledge such as representations of linguistic units,
and without learning mechanisms specifically targeted at such units. This has
raised the question of to what extent knowledge of linguistic units, such as
phone(me)s, syllables, and words, could actually emerge as latent
representations supporting the translation between speech and representations
in other modalities, and without the units being proximal learning targets for
the learner. In this study, we formulate this idea as the so-called latent
language hypothesis (LLH), connecting linguistic representation learning to
general predictive processing within and across sensory modalities. We review
the extent that the audiovisual aspect of LLH is supported by the existing
computational studies. We then explore LLH further in extensive learning
simulations with different neural network models for audiovisual
cross-situational learning, and comparing learning from both synthetic and real
speech data. We investigate whether the latent representations learned by the
networks reflect phonetic, syllabic, or lexical structure of input speech by
utilizing an array of complementary evaluation metrics related to linguistic
selectivity and temporal characteristics of the representations. As a result,
we find that representations associated...
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLEU, METEOR, BERTScore: Evaluation of Metrics Performance in Assessing Critical Translation Errors in Sentiment-oriented Text. (arXiv:2109.14250v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14250">
<div class="article-summary-box-inner">
<span><p>Social media companies as well as authorities make extensive use of
artificial intelligence (AI) tools to monitor postings of hate speech,
celebrations of violence or profanity. Since AI software requires massive
volumes of data to train computers, Machine Translation (MT) of the online
content is commonly used to process posts written in several languages and
hence augment the data needed for training. However, MT mistakes are a regular
occurrence when translating sentiment-oriented user-generated content (UGC),
especially when a low-resource language is involved. The adequacy of the whole
process relies on the assumption that the evaluation metrics used give a
reliable indication of the quality of the translation. In this paper, we assess
the ability of automatic quality metrics to detect critical machine translation
errors which can cause serious misunderstanding of the affect message. We
compare the performance of three canonical metrics on meaningless translations
where the semantic content is seriously impaired as compared to meaningful
translations with a critical error which exclusively distorts the sentiment of
the source text. We conclude that there is a need for fine-tuning of automatic
metrics to make them more robust in detecting sentiment critical errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Character Tagger for Short Text Spelling Error Correction. (arXiv:2109.14259v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14259">
<div class="article-summary-box-inner">
<span><p>State-of-the-art approaches to spelling error correction problem include
Transformer-based Seq2Seq models, which require large training sets and suffer
from slow inference time; and sequence labeling models based on Transformer
encoders like BERT, which involve token-level label space and therefore a large
pre-defined vocabulary dictionary. In this paper we present a Hierarchical
Character Tagger model, or HCTagger, for short text spelling error correction.
We use a pre-trained language model at the character level as a text encoder,
and then predict character-level edits to transform the original text into its
error-free form with a much smaller label space. For decoding, we propose a
hierarchical multi-task approach to alleviate the issue of long-tail label
distribution without introducing extra model parameters. Experiments on two
public misspelling correction datasets demonstrate that HCTagger is an accurate
and much faster approach than many existing models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Call Larisa Ivanovna: Code-Switching Fools Multilingual NLU Models. (arXiv:2109.14350v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14350">
<div class="article-summary-box-inner">
<span><p>Practical needs of developing task-oriented dialogue assistants require the
ability to understand many languages. Novel benchmarks for multilingual natural
language understanding (NLU) include monolingual sentences in several
languages, annotated with intents and slots. In such setup models for
cross-lingual transfer show remarkable performance in joint intent recognition
and slot filling. However, existing benchmarks lack of code-switched
utterances, which are difficult to gather and label due to complexity in the
grammatical structure. The evaluation of NLU models seems biased and limited,
since code-switching is being left out of scope.
</p>
<p>Our work adopts recognized methods to generate plausible and
naturally-sounding code-switched utterances and uses them to create a synthetic
code-switched test set. Based on experiments, we report that the
state-of-the-art NLU models are unable to handle code-switching. At worst, the
performance, evaluated by semantic accuracy, drops as low as 15\% from 80\%
across languages. Further we show, that pre-training on synthetic code-mixed
data helps to maintain performance on the proposed test set at a comparable
level with monolingual data. Finally, we analyze different language pairs and
show that the closer the languages are, the better the NLU model handles their
alternation. This is in line with the common understanding of how multilingual
models conduct transferring between languages
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Fact Linking. (arXiv:2109.14364v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14364">
<div class="article-summary-box-inner">
<span><p>Knowledge-intensive NLP tasks can benefit from linking natural language text
with facts from a Knowledge Graph (KG). Although facts themselves are
language-agnostic, the fact labels (i.e., language-specific representation of
the fact) in the KG are often present only in a few languages. This makes it
challenging to link KG facts to sentences in languages other than the limited
set of languages. To address this problem, we introduce the task of
Multilingual Fact Linking (MFL) where the goal is to link fact expressed in a
sentence to corresponding fact in the KG, even when the fact label in the KG is
not available in the language of the sentence. To facilitate research in this
area, we present a new evaluation dataset, IndicLink. This dataset contains
11,293 linked WikiData facts and 6,429 sentences spanning English and six
Indian languages. We propose a Retrieval+Generation model, ReFCoG, that can
scale to millions of KG facts by combining Dual Encoder based retrieval with a
Seq2Seq based generation model which is constrained to output only valid KG
facts. ReFCoG outperforms standard Retrieval+Re-ranking models by 10.7 pts in
Precision@1. In spite of this gain, the model achieves an overall score of
52.1, showing ample scope for improvement in the task.ReFCoG code and IndicLink
data are available at https://github.com/SaiKeshav/mfl
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EdinSaar@WMT21: North-Germanic Low-Resource Multilingual NMT. (arXiv:2109.14368v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14368">
<div class="article-summary-box-inner">
<span><p>We describe the EdinSaar submission to the shared task of Multilingual
Low-Resource Translation for North Germanic Languages at the Sixth Conference
on Machine Translation (WMT2021). We submit multilingual translation models for
translations to/from Icelandic (is), Norwegian-Bokmal (nb), and Swedish (sv).
We employ various experimental approaches, including multilingual pre-training,
back-translation, fine-tuning, and ensembling. In most translation directions,
our models outperform other submitted systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EDGAR-CORPUS: Billions of Tokens Make The World Go Round. (arXiv:2109.14394v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14394">
<div class="article-summary-box-inner">
<span><p>We release EDGAR-CORPUS, a novel corpus comprising annual reports from all
the publicly traded companies in the US spanning a period of more than 25
years. To the best of our knowledge, EDGAR-CORPUSis the largest financial NLP
corpus available to date. All the reports are downloaded, split into their
corresponding items (sections), and provided in a clean, easy-to-use JSON
format. We use EDGAR-CORPUS to train and release EDGAR-W2V, which are WORD2VEC
embeddings for the financial domain. We employ these embeddings in a battery of
financial NLP tasks and showcase their superiority over generic GloVe
embeddings and other existing financial word embeddings. We also open-source
EDGAR-CRAWLER, a toolkit that facilitates downloading and extracting future
annual reports.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StoryDB: Broad Multi-language Narrative Dataset. (arXiv:2109.14396v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14396">
<div class="article-summary-box-inner">
<span><p>This paper presents StoryDB - a broad multi-language dataset of narratives.
StoryDB is a corpus of texts that includes stories in 42 different languages.
Every language includes 500+ stories. Some of the languages include more than
20 000 stories. Every story is indexed across languages and labeled with tags
such as a genre or a topic. The corpus shows rich topical and language
variation and can serve as a resource for the study of the role of narrative in
natural language processing across various languages including low resource
ones. We also demonstrate how the dataset could be used to benchmark three
modern multilanguage models, namely, mDistillBERT, mBERT, and XLM-RoBERTa.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiQUE: Biquaternionic Embeddings of Knowledge Graphs. (arXiv:2109.14401v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14401">
<div class="article-summary-box-inner">
<span><p>Knowledge graph embeddings (KGEs) compactly encode multi-relational knowledge
graphs (KGs). Existing KGE models rely on geometric operations to model
relational patterns. Euclidean (circular) rotation is useful for modeling
patterns such as symmetry, but cannot represent hierarchical semantics. In
contrast, hyperbolic models are effective at modeling hierarchical relations,
but do not perform as well on patterns on which circular rotation excels. It is
crucial for KGE models to unify multiple geometric transformations so as to
fully cover the multifarious relations in KGs. To do so, we propose BiQUE, a
novel model that employs biquaternions to integrate multiple geometric
transformations, viz., scaling, translation, Euclidean rotation, and hyperbolic
rotation. BiQUE makes the best trade-offs among geometric operators during
training, picking the best one (or their best combination) for each relation.
Experiments on five datasets show BiQUE's effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastCorrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition. (arXiv:2109.14420v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14420">
<div class="article-summary-box-inner">
<span><p>Error correction is widely used in automatic speech recognition (ASR) to
post-process the generated sentence, and can further reduce the word error rate
(WER). Although multiple candidates are generated by an ASR system through beam
search, current error correction approaches can only correct one sentence at a
time, failing to leverage the voting effect from multiple candidates to better
detect and correct error tokens. In this work, we propose FastCorrect 2, an
error correction model that takes multiple ASR candidates as input for better
correction accuracy. FastCorrect 2 adopts non-autoregressive generation for
fast inference, which consists of an encoder that processes multiple source
sentences and a decoder that generates the target sentence in parallel from the
adjusted source sentence, where the adjustment is based on the predicted
duration of each source token. However, there are some issues when handling
multiple source sentences. First, it is non-trivial to leverage the voting
effect from multiple source sentences since they usually vary in length. Thus,
we propose a novel alignment algorithm to maximize the degree of token
alignment among multiple sentences in terms of token and pronunciation
similarity. Second, the decoder can only take one adjusted source sentence as
input, while there are multiple source sentences. Thus, we develop a candidate
predictor to detect the most suitable candidate for the decoder. Experiments on
our inhouse dataset and AISHELL-1 show that FastCorrect 2 can further reduce
the WER over the previous correction model with single candidate by 3.2% and
2.6%, demonstrating the effectiveness of leveraging multiple candidates in ASR
error correction. FastCorrect 2 achieves better performance than the cascaded
re-scoring and correction pipeline and can serve as a unified post-processing
module for ASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overview of the Arabic Sentiment Analysis 2021 Competition at KAUST. (arXiv:2109.14456v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14456">
<div class="article-summary-box-inner">
<span><p>This paper provides an overview of the Arabic Sentiment Analysis Challenge
organized by King Abdullah University of Science and Technology (KAUST). The
task in this challenge is to develop machine learning models to classify a
given tweet into one of the three categories Positive, Negative, or Neutral.
From our recently released ASAD dataset, we provide the competitors with 55K
tweets for training, and 20K tweets for validation, based on which the
performance of participating teams are ranked on a leaderboard,
https://www.kaggle.com/c/arabic-sentiment-analysis-2021-kaust. The competition
received in total 1247 submissions from 74 teams (99 team members). The final
winners are determined by another private set of 20K tweets that have the same
distribution as the training and validation set. In this paper, we present the
main findings in the competition and summarize the methods and tools used by
the top ranked teams. The full dataset of 100K labeled tweets is also released
for public usage, at
https://www.kaggle.com/c/arabic-sentiment-analysis-2021-kaust/data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast and Scalable Dialogue State Tracking with Explicit Modular Decomposition. (arXiv:2004.10663v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.10663">
<div class="article-summary-box-inner">
<span><p>We present a fast and scalable architecture called Explicit Modular
Decomposition (EMD), in which we incorporate both classification-based and
extraction-based methods and design four modules (for classification and
sequence labelling) to jointly extract dialogue states. Experimental results
based on the MultiWoz 2.0 dataset validates the superiority of our proposed
model in terms of both complexity and scalability when compared to the
state-of-the-art methods, especially in the scenario of multi-domain dialogues
entangled with many turns of utterances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextualize Knowledge Bases with Transformer for End-to-end Task-Oriented Dialogue Systems. (arXiv:2010.05740v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.05740">
<div class="article-summary-box-inner">
<span><p>Incorporating knowledge bases (KB) into end-to-end task-oriented dialogue
systems is challenging, since it requires to properly represent the entity of
KB, which is associated with its KB context and dialogue context. The existing
works represent the entity with only perceiving a part of its KB context, which
can lead to the less effective representation due to the information loss, and
adversely favor KB reasoning and response generation. To tackle this issue, we
explore to fully contextualize the entity representation by dynamically
perceiving all the relevant entities} and dialogue history. To achieve this, we
propose a COntext-aware Memory Enhanced Transformer framework (COMET), which
treats the KB as a sequence and leverages a novel Memory Mask to enforce the
entity to only focus on its relevant entities and dialogue history, while
avoiding the distraction from the irrelevant entities. Through extensive
experiments, we show that our COMET framework can achieve superior performance
over the state of the arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Influence Patterns for Explaining Information Flow in BERT. (arXiv:2011.00740v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.00740">
<div class="article-summary-box-inner">
<span><p>While attention is all you need may be proving true, we do not know why:
attention-based transformer models such as BERT are superior but how
information flows from input tokens to output predictions are unclear. We
introduce influence patterns, abstractions of sets of paths through a
transformer model. Patterns quantify and localize the flow of information to
paths passing through a sequence of model nodes. Experimentally, we find that
significant portion of information flow in BERT goes through skip connections
instead of attention heads. We further show that consistency of patterns across
instances is an indicator of BERT's performance. Finally, We demonstrate that
patterns account for far more model performance than previous attention-based
and layer-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decomposing and Recomposing Event Structure. (arXiv:2103.10387v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10387">
<div class="article-summary-box-inner">
<span><p>We present an event structure classification empirically derived from
inferential properties annotated on sentence- and document-level Universal
Decompositional Semantics (UDS) graphs. We induce this classification jointly
with semantic role, entity, and event-event relation classifications using a
document-level generative model structured by these graphs. To support this
induction, we augment existing annotations found in the UDS1.0 dataset, which
covers the entirety of the English Web Treebank, with an array of inferential
properties capturing fine-grained aspects of the temporal and aspectual
structure of events. The resulting dataset (available at decomp.io) is the
largest annotation of event structure and (partial) event coreference to date.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding. (arXiv:2105.09996v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09996">
<div class="article-summary-box-inner">
<span><p>We present a simplified, task-agnostic multi-modal pre-training approach that
can accept either video or text input, or both for a variety of end tasks.
Existing pre-training are task-specific by adopting either a single cross-modal
encoder that requires both modalities, limiting their use for retrieval-style
end tasks or more complex multitask learning with two unimodal encoders,
limiting early cross-modal fusion. We instead introduce new pretraining masking
schemes that better mix across modalities (e.g. by forcing masks for text to
predict the closest video embeddings) while also maintaining separability (e.g.
unimodal predictions are sometimes required, without using all the input).
Experimental results show strong performance across a wider range of tasks than
any previous methods, often outperforming task-specific pre-training. Code is
made available at https://github.com/pytorch/fairseq/examples/MMPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Various Tokenizers for Arabic Text Classification. (arXiv:2106.07540v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07540">
<div class="article-summary-box-inner">
<span><p>The first step in any NLP pipeline is to split the text into individual
tokens. The most obvious and straightforward approach is to use words as
tokens. However, given a large text corpus, representing all the words is not
efficient in terms of vocabulary size. In the literature, many tokenization
algorithms have emerged to tackle this problem by creating subwords which in
turn limits the vocabulary size in a given text corpus. Most tokenization
techniques are language-agnostic i.e they don't incorporate the linguistic
features of a given language. Not to mention the difficulty of evaluating such
techniques in practice. In this paper, we introduce three new tokenization
algorithms for Arabic and compare them to three other baselines using
unsupervised evaluations. In addition to that, we compare all the six
algorithms by evaluating them on three supervised classification tasks which
are sentiment analysis, news classification and poetry classification using six
publicly available datasets. Our experiments show that none of the tokenization
technique is the best choice overall and that the performance of a given
tokenization algorithm depends on the size of the dataset, type of the task,
and the amount of morphology that exists in the dataset. However, some
tokenization techniques are better overall as compared to others on various
text classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark. (arXiv:2107.07498v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07498">
<div class="article-summary-box-inner">
<span><p>Pretrained Language Models (PLMs) have achieved tremendous success in natural
language understanding tasks. While different learning schemes -- fine-tuning,
zero-shot, and few-shot learning -- have been widely explored and compared for
languages such as English, there is comparatively little work in Chinese to
fairly and comprehensively evaluate and compare these methods and thus hinders
cumulative progress. In this paper, we introduce the Chinese Few-shot Learning
Evaluation Benchmark (FewCLUE), the first comprehensive few-shot evaluation
benchmark in Chinese. It includes nine tasks, ranging from single-sentence and
sentence-pair classification tasks to machine reading comprehension tasks. We
systematically evaluate five state-of-the-art (SOTA) few-shot learning methods
(including PET, ADAPET, LM-BFF, P-tuning and EFL), and compare their
performance with fine-tuning and zero-shot learning schemes on the newly
constructed FewCLUE benchmark. Experimental results reveal that: 1) The effect
of different few-shot learning methods is sensitive to the pre-trained model to
which the methods are applied; 2) PET and P-tuning achieve the best overall
performance with RoBERTa and ERNIE respectively. Our benchmark is used in the
few-shot learning contest of NLPCC 2021. In addition, we provide a
user-friendly toolkit, as well as an online leaderboard to help facilitate
further progress on Chinese few-shot learning. We provide a baseline
performance on different learning methods, a reference for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVID-19 Vaccine and Social Media: Exploring Emotions and Discussions on Twitter. (arXiv:2108.04816v2 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04816">
<div class="article-summary-box-inner">
<span><p>The understanding of the public response to COVID-19 vaccines is the key
success factor to control the COVID-19 pandemic. To understand the public
response, there is a need to explore public opinion. Traditional surveys are
expensive and time-consuming, address limited health topics, and obtain
small-scale data. Twitter can provide a great opportunity to understand public
opinion regarding COVID-19 vaccines. The current study proposes an approach
using computational and human coding methods to collect and analyze a large
number of tweets to provide a wider perspective on the COVID-19 vaccine. This
study identifies the sentiment of tweets using a machine learning rule-based
approach, discovers major topics, explores temporal trend and compares topics
of negative and non-negative tweets using statistical tests, and discloses top
topics of tweets having negative and non-negative sentiment. Our findings show
that the negative sentiment regarding the COVID-19 vaccine had a decreasing
trend between November 2020 and February 2021. We found Twitter users have
discussed a wide range of topics from vaccination sites to the 2020 U.S.
election between November 2020 and February 2021. The findings show that there
was a significant difference between tweets having negative and non-negative
sentiment regarding the weight of most topics. Our results also indicate that
the negative and non-negative tweets had different topic priorities and
focuses. This research illustrates that Twitter data can be used to explore
public opinion regarding the COVID-19 vaccine.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Does Adversarial Fine-Tuning Benefit BERT?. (arXiv:2108.13602v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13602">
<div class="article-summary-box-inner">
<span><p>Adversarial training (AT) is one of the most reliable methods for defending
against adversarial attacks in machine learning. Variants of this method have
been used as regularization mechanisms to achieve SOTA results on NLP
benchmarks, and they have been found to be useful for transfer learning and
continual learning. We search for the reasons for the effectiveness of AT by
contrasting vanilla and adversarially fine-tuned BERT models. We identify
partial preservation of BERT's syntactic abilities during fine-tuning as the
key to the success of AT. We observe that adversarially fine-tuned models
remain more faithful to BERT's language modeling behavior and are more
sensitive to the word order. As concrete examples of syntactic abilities, an
adversarially fine-tuned model could have an advantage of up to 38% on anaphora
agreement and up to 11% on dependency parsing. Our analysis demonstrates that
vanilla fine-tuning oversimplifies the sentence representation by focusing
heavily on a small subset of words. AT, however, moderates the effect of these
influential words and encourages representational diversity. This allows for a
more hierarchical representation of a sentence and leads to the mitigation of
BERT's loss of syntactic abilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset. (arXiv:2108.13897v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13897">
<div class="article-summary-box-inner">
<span><p>The MS MARCO ranking dataset has been widely used for training deep learning
models for IR tasks, achieving considerable effectiveness on diverse zero-shot
scenarios. However, this type of resource is scarce in other languages than
English. In this work we present mMARCO, a multilingual version of the MS MARCO
passage ranking dataset comprising 8 languages that was created using machine
translation. We evaluated mMARCO by fine-tuning mono and multilingual
re-ranking models on it. Experimental results demonstrate that multilingual
models fine-tuned on our translated dataset achieve superior effectiveness than
models fine-tuned on the original English version alone. Also, our distilled
multilingual re-ranker is competitive with non-distilled models while having
5.4 times fewer parameters. The translated datasets as well as fine-tuned
models are available at https://github.com/unicamp-dl/mMARCO.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patterns of Lexical Ambiguity in Contextualised Language Models. (arXiv:2109.13032v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13032">
<div class="article-summary-box-inner">
<span><p>One of the central aspects of contextualised language models is that they
should be able to distinguish the meaning of lexically ambiguous words by their
contexts. In this paper we investigate the extent to which the contextualised
embeddings of word forms that display multiplicity of sense reflect traditional
distinctions of polysemy and homonymy. To this end, we introduce an extended,
human-annotated dataset of graded word sense similarity and co-predication
acceptability, and evaluate how well the similarity of embeddings predicts
similarity in meaning. Both types of human judgements indicate that the
similarity of polysemic interpretations falls in a continuum between identity
of meaning and homonymy. However, we also observe significant differences
within the similarity ratings of polysemes, forming consistent patterns for
different types of polysemic sense alternation. Our dataset thus appears to
capture a substantial part of the complexity of lexical ambiguity, and can
provide a realistic test bed for contextualised embeddings. Among the tested
models, BERT Large shows the strongest correlation with the collected word
sense similarity ratings, but struggles to consistently replicate the observed
similarity patterns. When clustering ambiguous word forms based on their
embeddings, the model displays high confidence in discerning homonyms and some
types of polysemic alternations, but consistently fails for others.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-tuning Vision Transformers for the Prediction of State Variables in Ising Models. (arXiv:2109.13925v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13925">
<div class="article-summary-box-inner">
<span><p>Transformers are state-of-the-art deep learning models that are composed of
stacked attention and point-wise, fully connected layers designed for handling
sequential data. Transformers are not only ubiquitous throughout Natural
Language Processing (NLP), but, recently, they have inspired a new wave of
Computer Vision (CV) applications research. In this work, a Vision Transformer
(ViT) is applied to predict the state variables of 2-dimensional Ising model
simulations. Our experiments show that ViT outperform state-of-the-art
Convolutional Neural Networks (CNN) when using a small number of microstate
images from the Ising model corresponding to various boundary conditions and
temperatures. This work opens the possibility of applying ViT to other
simulations, and raises interesting research directions on how attention maps
can learn about the underlying physics governing different phenomena.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">All-Around Real Label Supervision: Cyclic Prototype Consistency Learning for Semi-supervised Medical Image Segmentation. (arXiv:2109.13930v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13930">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning has substantially advanced medical image
segmentation since it alleviates the heavy burden of acquiring the costly
expert-examined annotations. Especially, the consistency-based approaches have
attracted more attention for their superior performance, wherein the real
labels are only utilized to supervise their paired images via supervised loss
while the unlabeled images are exploited by enforcing the perturbation-based
\textit{"unsupervised"} consistency without explicit guidance from those real
labels. However, intuitively, the expert-examined real labels contain more
reliable supervision signals. Observing this, we ask an unexplored but
interesting question: can we exploit the unlabeled data via explicit real label
supervision for semi-supervised training? To this end, we discard the previous
perturbation-based consistency but absorb the essence of non-parametric
prototype learning. Based on the prototypical network, we then propose a novel
cyclic prototype consistency learning (CPCL) framework, which is constructed by
a labeled-to-unlabeled (L2U) prototypical forward process and an
unlabeled-to-labeled (U2L) backward process. Such two processes synergistically
enhance the segmentation network by encouraging more discriminative and compact
features. In this way, our framework turns previous \textit{"unsupervised"}
consistency into new \textit{"supervised"} consistency, obtaining the
\textit{"all-around real label supervision"} property of our method. Extensive
experiments on brain tumor segmentation from MRI and kidney segmentation from
CT images show that our CPCL can effectively exploit the unlabeled data and
outperform other state-of-the-art semi-supervised medical image segmentation
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A framework for quantitative analysis of Computed Tomography images of viral pneumonitis: radiomic features in COVID and non-COVID patients. (arXiv:2109.13931v1 [physics.med-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13931">
<div class="article-summary-box-inner">
<span><p>Purpose: to optimize a pipeline of clinical data gathering and CT images
processing implemented during the COVID-19 pandemic crisis and to develop
artificial intelligence model for different of viral pneumonia. Methods: 1028
chest CT image of patients with positive swab were segmented automatically for
lung extraction. A Gaussian model developed in Python language was applied to
calculate quantitative metrics (QM) describing well-aerated and ill portions of
the lungs from the histogram distribution of lung CT numbers in both lungs of
each image and in four geometrical subdivision. Furthermore, radiomic features
(RF) of first and second order were extracted from bilateral lungs using
PyRadiomic tools. QM and RF were used to develop 4 different Multi-Layer
Perceptron (MLP) classifier to discriminate images of patients with COVID
(n=646) and non-COVID (n=382) viral pneumonia. Results: The Gaussian model
applied to lung CT histogram correctly described healthy parenchyma 94% of the
patients. The resulting accuracy of the models for COVID diagnosis were in the
range 0.76-0.87, as the integral of the receiver operating curve. The best
diagnostic performances were associated to the model based on RF of first and
second order, with 21 relevant features after LASSO regression and an accuracy
of 0.81$\pm$0.02 after 4-fold cross validation Conclusions: Despite these
results were obtained with CT images from a single center, a platform for
extracting useful quantitative metrics from CT images was developed and
optimized. Four artificial intelligence-based models for classifying patients
with COVID and non-COVID viral pneumonia were developed and compared showing
overall good diagnostic performances
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One-shot Key Information Extraction from Document with Deep Partial Graph Matching. (arXiv:2109.13967v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13967">
<div class="article-summary-box-inner">
<span><p>Automating the Key Information Extraction (KIE) from documents improves
efficiency, productivity, and security in many industrial scenarios such as
rapid indexing and archiving. Many existing supervised learning methods for the
KIE task need to feed a large number of labeled samples and learn separate
models for different types of documents. However, collecting and labeling a
large dataset is time-consuming and is not a user-friendly requirement for many
cloud platforms. To overcome these challenges, we propose a deep end-to-end
trainable network for one-shot KIE using partial graph matching. Contrary to
previous methods that the learning of similarity and solving are optimized
separately, our method enables the learning of the two processes in an
end-to-end framework. Existing one-shot KIE methods are either template or
simple attention-based learning approach that struggle to handle texts that are
shifted beyond their desired positions caused by printers, as illustrated in
Fig.1. To solve this problem, we add one-to-(at most)-one constraint such that
we will find the globally optimized solution even if some texts are drifted.
Further, we design a multimodal context ensemble block to boost the performance
through fusing features of spatial, textual, and aspect representations. To
promote research of KIE, we collected and annotated a one-shot document KIE
dataset named DKIE with diverse types of images. The DKIE dataset consists of
2.5K document images captured by mobile phones in natural scenes, and it is the
largest available one-shot KIE dataset up to now. The results of experiments on
DKIE show that our method achieved state-of-the-art performance compared with
recent one-shot and supervised learning approaches. The dataset and proposed
one-shot KIE model will be released soo
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Competence-Aware Path Planning via Introspective Perception. (arXiv:2109.13974v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13974">
<div class="article-summary-box-inner">
<span><p>Robots deployed in the real world over extended periods of time need to
reason about unexpected failures, learn to predict them, and to proactively
take actions to avoid future failures. Existing approaches for competence-aware
planning are either model-based, requiring explicit enumeration of known
failure modes, or purely statistical, using state- and location-specific
failure statistics to infer competence. We instead propose a structured
model-free approach to competence-aware planning by reasoning about plan
execution failures due to errors in perception, without requiring a-priori
enumeration of failure modes or requiring location-specific failure statistics.
We introduce competence-aware path planning via introspective perception
(CPIP), a Bayesian framework to iteratively learn and exploit task-level
competence in novel deployment environments. CPIP factorizes the
competence-aware planning problem into two components. First, perception errors
are learned in a model-free and location-agnostic setting via introspective
perception prior to deployment in novel environments. Second, during actual
deployments, the prediction of task-level failures is learned in a
context-aware setting. Experiments in a simulation show that the proposed CPIP
approach outperforms the frequentist baseline in multiple mobile robot tasks,
and is further validated via real robot experiments in an environment with
perceptually challenging obstacles and terrain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Y-GAN: Learning Dual Data Representations for Efficient Anomaly Detection. (arXiv:2109.14020v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14020">
<div class="article-summary-box-inner">
<span><p>We propose a novel reconstruction-based model for anomaly detection, called
Y-GAN. The model consists of a Y-shaped auto-encoder and represents images in
two separate latent spaces. The first captures meaningful image semantics, key
for representing (normal) training data, whereas the second encodes low-level
residual image characteristics. To ensure the dual representations encode
mutually exclusive information, a disentanglement procedure is designed around
a latent (proxy) classifier. Additionally, a novel consistency loss is proposed
to prevent information leakage between the latent spaces. The model is trained
in a one-class learning setting using normal training data only. Due to the
separation of semantically-relevant and residual information, Y-GAN is able to
derive informative data representations that allow for efficient anomaly
detection across a diverse set of anomaly detection tasks. The model is
evaluated in comprehensive experiments with several recent anomaly detection
models using four popular datasets, i.e., MNIST, FMNIST and CIFAR10, and
PlantVillage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Unrolled Recovery in Sparse Biological Imaging. (arXiv:2109.14025v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14025">
<div class="article-summary-box-inner">
<span><p>Deep algorithm unrolling has emerged as a powerful model-based approach to
develop deep architectures that combine the interpretability of iterative
algorithms with the performance gains of supervised deep learning, especially
in cases of sparse optimization. This framework is well-suited to applications
in biological imaging, where physics-based models exist to describe the
measurement process and the information to be recovered is often highly
structured. Here, we review the method of deep unrolling, and show how it
improves source localization in several biological imaging settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoPhaseNN: Unsupervised Physics-aware Deep Learning of 3D Nanoscale Coherent Imaging. (arXiv:2109.14053v1 [physics.app-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14053">
<div class="article-summary-box-inner">
<span><p>The problem of phase retrieval, or the algorithmic recovery of lost phase
information from measured intensity alone, underlies various imaging methods
from astronomy to nanoscale imaging. Traditional methods of phase retrieval are
iterative in nature, and are therefore computationally expensive and time
consuming. More recently, deep learning (DL) models have been developed to
either provide learned priors to iterative phase retrieval or in some cases
completely replace phase retrieval with networks that learn to recover the lost
phase information from measured intensity alone. However, such models require
vast amounts of labeled data, which can only be obtained through simulation or
performing computationally prohibitive phase retrieval on hundreds of or even
thousands of experimental datasets. Using a 3D nanoscale X-ray imaging modality
(Bragg Coherent Diffraction Imaging or BCDI) as a representative technique, we
demonstrate AutoPhaseNN, a DL-based approach which learns to solve the phase
problem without labeled data. By incorporating the physics of the imaging
technique into the DL model during training, AutoPhaseNN learns to invert 3D
BCDI data from reciprocal space to real space in a single shot without ever
being shown real space images. Once trained, AutoPhaseNN is about one hundred
times faster than traditional iterative phase retrieval methods while providing
comparable image quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding. (arXiv:2109.14084v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14084">
<div class="article-summary-box-inner">
<span><p>We present VideoCLIP, a contrastive approach to pre-train a unified model for
zero-shot video and text understanding, without using any labels on downstream
tasks. VideoCLIP trains a transformer for video and text by contrasting
temporally overlapping positive video-text pairs with hard negatives from
nearest neighbor retrieval. Our experiments on a diverse series of downstream
tasks, including sequence-level text-video retrieval, VideoQA, token-level
action localization, and action segmentation reveal state-of-the-art
performance, surpassing prior work, and in some cases even outperforming
supervised approaches. Code is made available at
https://github.com/pytorch/fairseq/examples/MMPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visually Grounded Concept Composition. (arXiv:2109.14115v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14115">
<div class="article-summary-box-inner">
<span><p>We investigate ways to compose complex concepts in texts from primitive ones
while grounding them in images. We propose Concept and Relation Graph (CRG),
which builds on top of constituency analysis and consists of recursively
combined concepts with predicate functions. Meanwhile, we propose a concept
composition neural network called Composer to leverage the CRG for visually
grounded concept learning. Specifically, we learn the grounding of both
primitive and all composed concepts by aligning them to images and show that
learning to compose leads to more robust grounding results, measured in
text-to-image matching accuracy. Notably, our model can model grounded concepts
forming at both the finer-grained sentence level and the coarser-grained
intermediate level (or word-level). Composer leads to pronounced improvement in
matching accuracy when the evaluation data has significant compound divergence
from the training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparison of atlas-based and neural-network-based semantic segmentation for DENSE MRI images. (arXiv:2109.14116v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14116">
<div class="article-summary-box-inner">
<span><p>Two segmentation methods, one atlas-based and one neural-network-based, were
compared to see how well they can each automatically segment the brain stem and
cerebellum in Displacement Encoding with Stimulated Echoes Magnetic Resonance
Imaging (DENSE-MRI) data. The segmentation is a pre-requisite for estimating
the average displacements in these regions, which have recently been proposed
as biomarkers in the diagnosis of Chiari Malformation type I (CMI). In
numerical experiments, the segmentations of both methods were similar to manual
segmentations provided by trained experts. It was found that, overall, the
neural-network-based method alone produced more accurate segmentations than the
atlas-based method did alone, but that a combination of the two methods -- in
which the atlas-based method is used for the segmentation of the brain stem and
the neural-network is used for the segmentation of the cerebellum -- may be the
most successful.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta Learning on a Sequence of Imbalanced Domains with Difficulty Awareness. (arXiv:2109.14120v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14120">
<div class="article-summary-box-inner">
<span><p>Recognizing new objects by learning from a few labeled examples in an
evolving environment is crucial to obtain excellent generalization ability for
real-world machine learning systems. A typical setting across current meta
learning algorithms assumes a stationary task distribution during meta
training. In this paper, we explore a more practical and challenging setting
where task distribution changes over time with domain shift. Particularly, we
consider realistic scenarios where task distribution is highly imbalanced with
domain labels unavailable in nature. We propose a kernel-based method for
domain change detection and a difficulty-aware memory management mechanism that
jointly considers the imbalanced domain size and domain importance to learn
across domains continuously. Furthermore, we introduce an efficient adaptive
task sampling method during meta training, which significantly reduces task
gradient variance with theoretical guarantees. Finally, we propose a
challenging benchmark with imbalanced domain sequences and varied domain
difficulty. We have performed extensive evaluations on the proposed benchmark,
demonstrating the effectiveness of our method. We made our code publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grouptron: Dynamic Multi-Scale Graph Convolutional Networks for Group-Aware Dense Crowd Trajectory Forecasting. (arXiv:2109.14128v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14128">
<div class="article-summary-box-inner">
<span><p>Accurate, long-term forecasting of human pedestrian trajectories in highly
dynamic and interactive scenes is a long-standing challenge. Recent advances in
using data-driven approaches have achieved significant improvements in terms of
prediction accuracy. However, the lack of group-aware analysis has limited the
performance of forecasting models. This is especially apparent in highly
populated scenes, where pedestrians are moving in groups and the interactions
between groups are extremely complex and dynamic. In this paper, we present
Grouptron, a multi-scale dynamic forecasting framework that leverages
pedestrian group detection and utilizes individual-level, group-level, and
scene-level information for better understanding and representation of the
scenes. Our approach employs spatio-temporal clustering algorithms to identify
pedestrian groups, creates spatio-temporal graphs at the individual, group, and
scene levels. It then uses graph neural networks to encode dynamics at
different scales and incorporates encoding across different scales for
trajectory prediction. We carried out extensive comparisons and ablation
experiments to demonstrate the effectiveness of our approach. Our method
achieves 9.3% decrease in final displacement error (FDE) compared with
state-of-the-art methods on ETH/UCY benchmark datasets, and 16.1% decrease in
FDE in more crowded scenes where extensive human group interactions are more
frequently present.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Video-Language Segmentation. (arXiv:2109.14131v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14131">
<div class="article-summary-box-inner">
<span><p>We focus on the problem of segmenting a certain object referred by a natural
language sentence in video content, at the core of formulating a pinpoint
vision-language relation. While existing attempts mainly construct such
relation in an implicit way, i.e., grid-level multi-modal feature fusion, it
has been proven problematic to distinguish semantically similar objects under
this paradigm. In this work, we propose to interwind the visual and linguistic
modalities in an explicit way via the contrastive learning objective, which
directly aligns the referred object and the language description and separates
the unreferred content apart across frames. Moreover, to remedy for the
degradation problem, we present two complementary hard instance mining
strategies, i.e., Language-relevant Channel Filter and Relative Hard Instance
Construction. They encourage the network to exclude visual-distinguishable
feature and to focus on easy-confused objects during the contrastive training.
Extensive experiments on two benchmarks, i.e., A2D Sentences and J-HMDB
Sentences, quantitatively demonstrate the state-of-the-arts performance of our
method and qualitatively show the more accurate distinguishment between
semantically similar objects over baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Grained Zero-Shot Learning with DNA as Side Information. (arXiv:2109.14133v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14133">
<div class="article-summary-box-inner">
<span><p>Fine-grained zero-shot learning task requires some form of side-information
to transfer discriminative information from seen to unseen classes. As manually
annotated visual attributes are extremely costly and often impractical to
obtain for a large number of classes, in this study we use DNA as side
information for the first time for fine-grained zero-shot classification of
species. Mitochondrial DNA plays an important role as a genetic marker in
evolutionary biology and has been used to achieve near-perfect accuracy in the
species classification of living organisms. We implement a simple hierarchical
Bayesian model that uses DNA information to establish the hierarchy in the
image space and employs local priors to define surrogate classes for unseen
ones. On the benchmark CUB dataset, we show that DNA can be equally promising
yet in general a more accessible alternative than word vectors as a side
information. This is especially important as obtaining robust word
representations for fine-grained species names is not a practicable goal when
information about these species in free-form text is limited. On a newly
compiled fine-grained insect dataset that uses DNA information from over a
thousand species, we show that the Bayesian approach outperforms
state-of-the-art by a wide margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Xception with Dual Attention Mechanism and Feature Fusion for Face Forgery Detection. (arXiv:2109.14136v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14136">
<div class="article-summary-box-inner">
<span><p>With the rapid development of deep learning technology, more and more face
forgeries by deepfake are widely spread on social media, causing serious social
concern. Face forgery detection has become a research hotspot in recent years,
and many related methods have been proposed until now. For those images with
low quality and/or diverse sources, however, the detection performances of
existing methods are still far from satisfactory. In this paper, we propose an
improved Xception with dual attention mechanism and feature fusion for face
forgery detection. Different from the middle flow in original Xception model,
we try to catch different high-semantic features of the face images using
different levels of convolution, and introduce the convolutional block
attention module and feature fusion to refine and reorganize those
high-semantic features. In the exit flow, we employ the self-attention
mechanism and depthwise separable convolution to learn the global information
and local information of the fused features separately to improve the
classification the ability of the proposed model. Experimental results
evaluated on three Deepfake datasets demonstrate that the proposed method
outperforms Xception as well as other related methods both in effectiveness and
generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometry-Entangled Visual Semantic Transformer for Image Captioning. (arXiv:2109.14137v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14137">
<div class="article-summary-box-inner">
<span><p>Recent advancements of image captioning have featured Visual-Semantic Fusion
or Geometry-Aid attention refinement. However, those fusion-based models, they
are still criticized for the lack of geometry information for inter and intra
attention refinement. On the other side, models based on Geometry-Aid attention
still suffer from the modality gap between visual and semantic information. In
this paper, we introduce a novel Geometry-Entangled Visual Semantic Transformer
(GEVST) network to realize the complementary advantages of Visual-Semantic
Fusion and Geometry-Aid attention refinement. Concretely, a Dense-Cap model
proposes some dense captions with corresponding geometry information at first.
Then, to empower GEVST with the ability to bridge the modality gap among visual
and semantic information, we build four parallel transformer encoders VV(Pure
Visual), VS(Semantic fused to Visual), SV(Visual fused to Semantic), SS(Pure
Semantic) for final caption generation. Both visual and semantic geometry
features are used in the Fusion module and also the Self-Attention module for
better attention measurement. To validate our model, we conduct extensive
experiments on the MS-COCO dataset, the experimental results show that our
GEVST model can obtain promising performance gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-frame Joint Enhancement for Early Interlaced Videos. (arXiv:2109.14151v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14151">
<div class="article-summary-box-inner">
<span><p>Early interlaced videos usually contain multiple and interlacing and complex
compression artifacts, which significantly reduce the visual quality. Although
the high-definition reconstruction technology for early videos has made great
progress in recent years, related research on deinterlacing is still lacking.
Traditional methods mainly focus on simple interlacing mechanism, and cannot
deal with the complex artifacts in real-world early videos. Recent interlaced
video reconstruction deep deinterlacing models only focus on single frame,
while neglecting important temporal information. Therefore, this paper proposes
a multiframe deinterlacing network joint enhancement network for early
interlaced videos that consists of three modules, i.e., spatial vertical
interpolation module, temporal alignment and fusion module, and final
refinement module. The proposed method can effectively remove the complex
artifacts in early videos by using temporal redundancy of multi-fields.
Experimental results demonstrate that the proposed method can recover high
quality results for both synthetic dataset and real-world early interlaced
videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Dynamic Contrast and Probability Distillation for Unsupervised Person Re-Id. (arXiv:2109.14157v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14157">
<div class="article-summary-box-inner">
<span><p>Unsupervised person re-identification (Re-Id) has attracted increasing
attention due to its practical application in the read-world video surveillance
system. The traditional unsupervised Re-Id are mostly based on the method
alternating between clustering and fine-tuning with the classification or
metric learning objectives on the grouped clusters. However, since person Re-Id
is an open-set problem, the clustering based methods often leave out lots of
outlier instances or group the instances into the wrong clusters, thus they can
not make full use of the training samples as a whole. To solve these problems,
we present the hybrid dynamic cluster contrast and probability distillation
algorithm. It formulates the unsupervised Re-Id problem into an unified
local-to-global dynamic contrastive learning and self-supervised probability
distillation framework. Specifically, the proposed method can make the utmost
of the self-supervised signals of all the clustered and un-clustered instances,
from both the instances' self-contrastive level and the probability
distillation respective, in the memory-based non-parametric manner. Besides,
the proposed hybrid local-to-global contrastive learning can take full
advantage of the informative and valuable training examples for effective and
robust training. Extensive experiment results show that the proposed method
achieves superior performances to state-of-the-art methods, under both the
purely unsupervised and unsupervised domain adaptation experiment settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Communications With AI Tasks. (arXiv:2109.14170v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14170">
<div class="article-summary-box-inner">
<span><p>A radical paradigm shift of wireless networks from ``connected things'' to
``connected intelligence'' undergoes, which coincides with the Shanno and
Weaver's envisions: Communications will transform from the technical level to
the semantic level. This article proposes a semantic communication method with
artificial intelligence tasks (SC-AIT). First, the architecture of SC-AIT is
elaborated. Then, based on the proposed architecture, we implement SC-AIT for a
image classifications task. A prototype of SC-AIT is also established for
surface defect detection, is conducted. Experimental results show that SC-AIT
has much lower bandwidth requirements, and can achieve more than $40\%$
classification accuracy gains compared with the communications at the technical
level. Future trends and key challenges for semantic communications are also
identified.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Segmentation of Radiation-Induced Pulmonary Fibrosis from Lung CT Scans with Multi-Scale Guided Dense Attention. (arXiv:2109.14172v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14172">
<div class="article-summary-box-inner">
<span><p>Computed Tomography (CT) plays an important role in monitoring
radiation-induced Pulmonary Fibrosis (PF), where accurate segmentation of the
PF lesions is highly desired for diagnosis and treatment follow-up. However,
the task is challenged by ambiguous boundary, irregular shape, various position
and size of the lesions, as well as the difficulty in acquiring a large set of
annotated volumetric images for training. To overcome these problems, we
propose a novel convolutional neural network called PF-Net and incorporate it
into a semi-supervised learning framework based on Iterative Confidence-based
Refinement And Weighting of pseudo Labels (I-CRAWL). Our PF-Net combines 2D and
3D convolutions to deal with CT volumes with large inter-slice spacing, and
uses multi-scale guided dense attention to segment complex PF lesions. For
semi-supervised learning, our I-CRAWL employs pixel-level uncertainty-based
confidence-aware refinement to improve the accuracy of pseudo labels of
unannotated images, and uses image-level uncertainty for confidence-based image
weighting to suppress low-quality pseudo labels in an iterative training
process. Extensive experiments with CT scans of Rhesus Macaques with
radiation-induced PF showed that: 1) PF-Net achieved higher segmentation
accuracy than existing 2D, 3D and 2.5D neural networks, and 2) I-CRAWL
outperformed state-of-the-art semi-supervised learning methods for the PF
lesion segmentation task. Our method has a potential to improve the diagnosis
of PF and clinical assessment of side effects of radiotherapy for lung cancers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TSAMT: Time-Series-Analysis-based Motion Transfer among Multiple Cameras. (arXiv:2109.14174v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14174">
<div class="article-summary-box-inner">
<span><p>Along with advances in optical sensors is the common practice of building an
imaging system with heterogeneous cameras. While high-resolution (HR) videos
acquisition and analysis are benefited from hybrid sensors, the intrinsic
characteristics of multiple cameras lead to an interesting motion transfer
problem. Unfortunately, most of the existing methods provide no theoretical
analysis and require intensive training data. In this paper, we propose an
algorithm using time series analysis for motion transfer among multiple
cameras. Specifically, we firstly identify seasonality in motion data and then
build an addictive time series model to extract patterns that could be
transferred across cameras. Our approach has a complete and clear mathematical
formulation, thus being efficient and interpretable. Through quantitative
evaluations on real-world data, we demonstrate the effectiveness of our method.
Furthermore, our motion transfer algorithm could combine with and facilitate
downstream tasks, e.g., enhancing pose estimation on LR videos with inherent
patterns extracted from HR ones. Code is available at
https://github.com/IndigoPurple/TSAMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REFLACX, a dataset of reports and eye-tracking data for localization of abnormalities in chest x-rays. (arXiv:2109.14187v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14187">
<div class="article-summary-box-inner">
<span><p>Deep learning has shown recent success in classifying anomalies in chest
x-rays, but datasets are still small compared to natural image datasets.
Supervision of abnormality localization has been shown to improve trained
models, partially compensating for dataset sizes. However, explicitly labeling
these anomalies requires an expert and is very time-consuming. We propose a
method for collecting implicit localization data using an eye tracker to
capture gaze locations and a microphone to capture a dictation of a report,
imitating the setup of a reading room, and potentially scalable for large
datasets. The resulting REFLACX (Reports and Eye-Tracking Data for Localization
of Abnormalities in Chest X-rays) dataset was labeled by five radiologists and
contains 3,032 synchronized sets of eye-tracking data and timestamped report
transcriptions. We also provide bounding boxes around lungs and heart and
validation labels consisting of ellipses localizing abnormalities and
image-level labels. Furthermore, a small subset of the data contains readings
from all radiologists, allowing for the calculation of inter-rater scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WEDGE: Web-Image Assisted Domain Generalization for Semantic Segmentation. (arXiv:2109.14196v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14196">
<div class="article-summary-box-inner">
<span><p>Domain generalization for semantic segmentation is highly demanded in real
applications, where a trained model is expected to work well in previously
unseen domains. One challenge lies in the lack of data which could cover the
diverse distributions of the possible unseen domains for training. In this
paper, we propose a WEb-image assisted Domain GEneralization (WEDGE) scheme,
which is the first to exploit the diversity of web-crawled images for
generalizable semantic segmentation. To explore and exploit the real-world data
distributions, we collect a web-crawled dataset which presents large diversity
in terms of weather conditions, sites, lighting, camera styles, etc. We also
present a method which injects the style representation of the web-crawled data
into the source domain on-the-fly during training, which enables the network to
experience images of diverse styles with reliable labels for effective
training. Moreover, we use the web-crawled dataset with predicted pseudo labels
for training to further enhance the capability of the network. Extensive
experiments demonstrate that our method clearly outperforms existing domain
generalization techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? -- A computational investigation. (arXiv:2109.14200v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14200">
<div class="article-summary-box-inner">
<span><p>Decades of research has studied how language learning infants learn to
discriminate speech sounds, segment words, and associate words with their
meanings. While gradual development of such capabilities is unquestionable, the
exact nature of these skills and the underlying mental representations yet
remains unclear. In parallel, computational studies have shown that basic
comprehension of speech can be achieved by statistical learning between speech
and concurrent referentially ambiguous visual input. These models can operate
without prior linguistic knowledge such as representations of linguistic units,
and without learning mechanisms specifically targeted at such units. This has
raised the question of to what extent knowledge of linguistic units, such as
phone(me)s, syllables, and words, could actually emerge as latent
representations supporting the translation between speech and representations
in other modalities, and without the units being proximal learning targets for
the learner. In this study, we formulate this idea as the so-called latent
language hypothesis (LLH), connecting linguistic representation learning to
general predictive processing within and across sensory modalities. We review
the extent that the audiovisual aspect of LLH is supported by the existing
computational studies. We then explore LLH further in extensive learning
simulations with different neural network models for audiovisual
cross-situational learning, and comparing learning from both synthetic and real
speech data. We investigate whether the latent representations learned by the
networks reflect phonetic, syllabic, or lexical structure of input speech by
utilizing an array of complementary evaluation metrics related to linguistic
selectivity and temporal characteristics of the representations. As a result,
we find that representations associated...
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identity-Expression Ambiguity in 3D Morphable Face Models. (arXiv:2109.14203v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14203">
<div class="article-summary-box-inner">
<span><p>3D Morphable Models are a class of generative models commonly used to model
faces. They are typically applied to ill-posed problems such as 3D
reconstruction from 2D data. Several ambiguities in this problem's image
formation process have been studied explicitly. We demonstrate that
non-orthogonality of the variation in identity and expression can cause
identity-expression ambiguity in 3D Morphable Models, and that in practice
expression and identity are far from orthogonal and can explain each other
surprisingly well. Whilst previously reported ambiguities only arise in an
inverse rendering setting, identity-expression ambiguity emerges in the 3D
shape generation process itself. We demonstrate this effect with 3D shapes
directly as well as through an inverse rendering task, and use two popular
models built from high quality 3D scans as well as a model built from a large
collection of 2D images and videos. We explore this issue's implications for
inverse rendering and observe that it cannot be resolved by a purely
statistical prior on identity and expression deformations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Brightness Agnostic Adversarial Examples Against Face Recognition Systems. (arXiv:2109.14205v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14205">
<div class="article-summary-box-inner">
<span><p>This paper introduces a novel adversarial example generation method against
face recognition systems (FRSs). An adversarial example (AX) is an image with
deliberately crafted noise to cause incorrect predictions by a target system.
The AXs generated from our method remain robust under real-world brightness
changes. Our method performs non-linear brightness transformations while
leveraging the concept of curriculum learning during the attack generation
procedure. We demonstrate that our method outperforms conventional techniques
from comprehensive experimental investigations in the digital and physical
world. Furthermore, this method enables practical risk assessment of FRSs
against brightness agnostic AXs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation in Semantic Segmentation Based on Pixel Alignment and Self-Training. (arXiv:2109.14219v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14219">
<div class="article-summary-box-inner">
<span><p>This paper proposes an unsupervised cross-modality domain adaptation approach
based on pixel alignment and self-training. Pixel alignment transfers ceT1
scans to hrT2 modality, helping to reduce domain shift in the training
segmentation model. Self-training adapts the decision boundary of the
segmentation network to fit the distribution of hrT2 scans. Experiment results
show that PAST has outperformed the non-UDA baseline significantly, and it
received rank-2 on CrossMoDA validation phase Leaderboard with a mean Dice
score of 0.8395.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multipath CNN with alpha matte inference for knee tissue segmentation from MRI. (arXiv:2109.14249v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14249">
<div class="article-summary-box-inner">
<span><p>Precise segmentation of knee tissues from magnetic resonance imaging (MRI) is
critical in quantitative imaging and diagnosis. Convolutional neural networks
(CNNs), which are state of the art, have limitations owing to the lack of
image-specific adaptation, such as low tissue contrasts and structural
inhomogeneities, thereby leading to incomplete segmentation results. This paper
presents a deep learning based automatic segmentation framework for knee tissue
segmentation. A novel multipath CNN-based method is proposed, which consists of
an encoder decoder-based segmentation network in combination with a low rank
tensor-reconstructed segmentation network. Low rank reconstruction in MRI
tensor sub-blocks is introduced to exploit the structural and morphological
variations in knee tissues. To further improve the segmentation from CNNs,
trimap generation, which effectively utilizes superimposed regions, is proposed
for defining high, medium and low confidence regions from the multipath CNNs.
The secondary path with low rank reconstructed input mitigates the conditions
in which the primary segmentation network can potentially fail and overlook the
boundary regions. The outcome of the segmentation is solved as an alpha matting
problem by blending the trimap with the source input. Experiments on
Osteoarthritis Initiative (OAI) datasets and a self prepared scan validate the
effectiveness of the proposed method. We specifically demonstrate the
application of the proposed method in a cartilage segmentation based thickness
map for diagnosis purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Road Network Guided Fine-Grained Urban Traffic Flow Inference. (arXiv:2109.14251v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14251">
<div class="article-summary-box-inner">
<span><p>Accurate inference of fine-grained traffic flow from coarse-grained one is an
emerging yet crucial problem, which can help greatly reduce the number of
traffic monitoring sensors for cost savings. In this work, we notice that
traffic flow has a high correlation with road network, which was either
completely ignored or simply treated as an external factor in previous works.
To facilitate this problem, we propose a novel Road-Aware Traffic Flow
Magnifier (RATFM) that explicitly exploits the prior knowledge of road networks
to fully learn the road-aware spatial distribution of fine-grained traffic
flow. Specifically, a multi-directional 1D convolutional layer is first
introduced to extract the semantic feature of the road network. Subsequently,
we incorporate the road network feature and coarse-grained flow feature to
regularize the short-range spatial distribution modeling of road-relative
traffic flow. Furthermore, we take the road network feature as a query to
capture the long-range spatial distribution of traffic flow with a transformer
architecture. Benefiting from the road-aware inference mechanism, our method
can generate high-quality fine-grained traffic flow maps. Extensive experiments
on three real-world datasets show that the proposed RATFM outperforms
state-of-the-art models under various scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Designing Counterfactual Generators using Deep Model Inversion. (arXiv:2109.14274v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14274">
<div class="article-summary-box-inner">
<span><p>Explanation techniques that synthesize small, interpretable changes to a
given image while producing desired changes in the model prediction have become
popular for introspecting black-box models. Commonly referred to as
counterfactuals, the synthesized explanations are required to contain
discernible changes (for easy interpretability) while also being realistic
(consistency to the data manifold). In this paper, we focus on the case where
we have access only to the trained deep classifier and not the actual training
data. While the problem of inverting deep models to synthesize images from the
training distribution has been explored, our goal is to develop a deep
inversion approach to generate counterfactual explanations for a given query
image. Despite their effectiveness in conditional image synthesis, we show that
existing deep inversion methods are insufficient for producing meaningful
counterfactuals. We propose DISC (Deep Inversion for Synthesizing
Counterfactuals) that improves upon deep inversion by utilizing (a) stronger
image priors, (b) incorporating a novel manifold consistency objective and (c)
adopting a progressive optimization strategy. We find that, in addition to
producing visually meaningful explanations, the counterfactuals from DISC are
effective at learning classifier decision boundaries and are robust to unknown
test-time corruptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Localizing Objects with Self-Supervised Transformers and no Labels. (arXiv:2109.14279v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14279">
<div class="article-summary-box-inner">
<span><p>Localizing objects in image collections without supervision can help to avoid
expensive annotation campaigns. We propose a simple approach to this problem,
that leverages the activation features of a vision transformer pre-trained in a
self-supervised manner. Our method, LOST, does not require any external object
proposal nor any exploration of the image collection; it operates on a single
image. Yet, we outperform state-of-the-art object discovery methods by up to 8
CorLoc points on PASCAL VOC 2012. We also show that training a class-agnostic
detector on the discovered objects boosts results by another 7 points.
Moreover, we show promising results on the unsupervised object discovery task.
The code to reproduce our results can be found at
https://github.com/valeoai/LOST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning for 3D Medical Image Analysis using 3D SimCLR and Monte Carlo Dropout. (arXiv:2109.14288v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14288">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning methods can be used to learn meaningful
representations from unlabeled data that can be transferred to supervised
downstream tasks to reduce the need for labeled data. In this paper, we propose
a 3D self-supervised method that is based on the contrastive (SimCLR) method.
Additionally, we show that employing Bayesian neural networks (with Monte-Carlo
Dropout) during the inference phase can further enhance the results on the
downstream tasks. We showcase our models on two medical imaging segmentation
tasks: i) Brain Tumor Segmentation from 3D MRI, ii) Pancreas Tumor Segmentation
from 3D CT. Our experimental results demonstrate the benefits of our proposed
methods in both downstream data-efficiency and performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Three-Stream 3D/1D CNN for Fine-Grained Action Classification and Segmentation in Table Tennis. (arXiv:2109.14306v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14306">
<div class="article-summary-box-inner">
<span><p>This paper proposes a fusion method of modalities extracted from video
through a three-stream network with spatio-temporal and temporal convolutions
for fine-grained action classification in sport. It is applied to TTStroke-21
dataset which consists of untrimmed videos of table tennis games. The goal is
to detect and classify table tennis strokes in the videos, the first step of a
bigger scheme aiming at giving feedback to the players for improving their
performance. The three modalities are raw RGB data, the computed optical flow
and the estimated pose of the player. The network consists of three branches
with attention blocks. Features are fused at the latest stage of the network
using bilinear layers. Compared to previous approaches, the use of three
modalities allows faster convergence and better performances on both tasks:
classification of strokes with known temporal boundaries and joint segmentation
and classification. The pose is also further investigated in order to offer
richer feedback to the athletes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Beginner to Master: A Survey for Deep Learning-based Single-Image Super-Resolution. (arXiv:2109.14335v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14335">
<div class="article-summary-box-inner">
<span><p>Single-image super-resolution (SISR) is an important task in image
processing, which aims to enhance the resolution of imaging systems. Recently,
SISR has made a huge leap and has achieved promising results with the help of
deep learning (DL). In this survey, we give an overview of DL-based SISR
methods and group them according to their targets, such as reconstruction
efficiency, reconstruction accuracy, and perceptual accuracy. Specifically, we
first introduce the problem definition, research background, and the
significance of SISR. Secondly, we introduce some related works, including
benchmark datasets, upsampling methods, optimization objectives, and image
quality assessment methods. Thirdly, we provide a detailed investigation of
SISR and give some domain-specific applications of it. Fourthly, we present the
reconstruction results of some classic SISR methods to intuitively know their
performance. Finally, we discuss some issues that still exist in SISR and
summarize some new trends and future directions. This is an exhaustive survey
of SISR, which can help researchers better understand SISR and inspire more
exciting research in this field. An investigation project for SISR is provided
in https://github.com/CV-JunchengLi/SISR-Survey.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Infrared Small-Dim Target Detection with Transformer under Complex Backgrounds. (arXiv:2109.14379v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14379">
<div class="article-summary-box-inner">
<span><p>The infrared small-dim target detection is one of the key techniques in the
infrared search and tracking system. Since the local regions which similar to
infrared small-dim targets spread over the whole background, exploring the
interaction information amongst image features in large-range dependencies to
mine the difference between the target and background is crucial for robust
detection. However, existing deep learning-based methods are limited by the
locality of convolutional neural networks, which impairs the ability to capture
large-range dependencies. To this end, we propose a new infrared small-dim
target detection method with the transformer. We adopt the self-attention
mechanism of the transformer to learn the interaction information of image
features in a larger range. Additionally, we design a feature enhancement
module to learn more features of small-dim targets. After that, we adopt a
decoder with the U-Net-like skip connection operation to get the detection
result. Extensive experiments on two public datasets show the obvious
superiority of the proposed method over state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UFO-ViT: High Performance Linear Vision Transformer without Softmax. (arXiv:2109.14382v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14382">
<div class="article-summary-box-inner">
<span><p>Vision transformers have become one of the most important models for computer
vision tasks. While they outperform earlier convolutional networks, the
complexity quadratic to $N$ is one of the major drawbacks when using
traditional self-attention algorithms. Here we propose the UFO-ViT(Unit Force
Operated Vision Trnasformer), novel method to reduce the computations of
self-attention by eliminating some non-linearity. Modifying few of lines from
self-attention, UFO-ViT achieves linear complexity without the degradation of
performance. The proposed models outperform most transformer-based models on
image classification and dense prediction tasks through most capacity regime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Knitworks: Patched Neural Implicit Representation Networks. (arXiv:2109.14406v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14406">
<div class="article-summary-box-inner">
<span><p>Coordinate-based Multilayer Perceptron (MLP) networks, despite being capable
of learning neural implicit representations, are not performant for internal
image synthesis applications. Convolutional Neural Networks (CNNs) are
typically used instead for a variety of internal generative tasks, at the cost
of a larger model. We propose Neural Knitwork, an architecture for neural
implicit representation learning of natural images that achieves image
synthesis by optimizing the distribution of image patches in an adversarial
manner and by enforcing consistency between the patch predictions. To the best
of our knowledge, this is the first implementation of a coordinate-based MLP
tailored for synthesis tasks such as image inpainting, super-resolution, and
denoising. We demonstrate the utility of the proposed technique by training on
these three tasks. The results show that modeling natural images using patches,
rather than pixels, produces results of higher fidelity. The resulting model
requires 80% fewer parameters than alternative CNN-based solutions while
achieving comparable performance and training time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-loss ensemble deep learning for chest X-ray classification. (arXiv:2109.14433v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14433">
<div class="article-summary-box-inner">
<span><p>Class imbalance is common in medical image classification tasks, where the
number of abnormal samples is fewer than the number of normal samples. The
difficulty of imbalanced classification is compounded by other issues such as
the size and distribution of the dataset. Reliable training of deep neural
networks continues to be a major challenge in such class-imbalanced conditions.
The loss function used to train the deep neural networks highly impact the
performance of both balanced and imbalanced tasks. Currently, the cross-entropy
loss remains the de-facto loss function for balanced and imbalanced
classification tasks. This loss, however, asserts equal learning to all
classes, leading to the classification of most samples as the majority normal
class. To provide a critical analysis of different loss functions and identify
those suitable for class-imbalanced classification, we benchmark various
state-of-the-art loss functions and propose novel loss functions to train a DL
model and analyze its performance in a multiclass classification setting that
classifies pediatric chest X-rays as showing normal lungs, bacterial pneumonia,
or viral pneumonia manifestations. We also construct prediction-level and
model-level ensembles of the models that are trained with various loss
functions to improve classification performance. We performed localization
studies to interpret model behavior to ensure that the individual models and
their ensembles precisely learned the regions of interest showing disease
manifestations to classify the chest X-rays to their respective categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Loss for All: Deep Hashing with a Single Cosine Similarity based Learning Objective. (arXiv:2109.14449v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14449">
<div class="article-summary-box-inner">
<span><p>A deep hashing model typically has two main learning objectives: to make the
learned binary hash codes discriminative and to minimize a quantization error.
With further constraints such as bit balance and code orthogonality, it is not
uncommon for existing models to employ a large number (&gt;4) of losses. This
leads to difficulties in model training and subsequently impedes their
effectiveness. In this work, we propose a novel deep hashing model with only a
single learning objective. Specifically, we show that maximizing the cosine
similarity between the continuous codes and their corresponding binary
orthogonal codes can ensure both hash code discriminativeness and quantization
error minimization. Further, with this learning objective, code balancing can
be achieved by simply using a Batch Normalization (BN) layer and multi-label
classification is also straightforward with label smoothing. The result is an
one-loss deep hashing model that removes all the hassles of tuning the weights
of various losses. Importantly, extensive experiments show that our model is
highly effective, outperforming the state-of-the-art multi-loss hashing models
on three large-scale instance retrieval benchmarks, often by significant
margins. Code is available at https://github.com/kamwoh/orthohash
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Programmable Spectral Filter Arrays for Hyperspectral Imaging. (arXiv:2109.14450v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14450">
<div class="article-summary-box-inner">
<span><p>Modulating the spectral dimension of light has numerous applications in
computational imaging. While there are many techniques for achieving this,
there are few, if any, for implementing a spatially-varying and programmable
spectral filter. This paper provides an optical design for implementing such a
capability. Our key insight is that spatially-varying spectral modulation can
be implemented using a liquid crystal spatial light modulator since it provides
an array of liquid crystal cells, each of which can be purposed to act as a
programmable spectral filter array. Relying on this insight, we provide an
optical schematic and an associated lab prototype for realizing the capability,
as well as address the associated challenges at implementation using optical
and computational innovations. We show a number of unique operating points with
our prototype including single- and multi-image hyperspectral imaging, as well
as its application in material identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CCTrans: Simplifying and Improving Crowd Counting with Transformer. (arXiv:2109.14483v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14483">
<div class="article-summary-box-inner">
<span><p>Most recent methods used for crowd counting are based on the convolutional
neural network (CNN), which has a strong ability to extract local features. But
CNN inherently fails in modeling the global context due to the limited
receptive fields. However, the transformer can model the global context easily.
In this paper, we propose a simple approach called CCTrans to simplify the
design pipeline. Specifically, we utilize a pyramid vision transformer backbone
to capture the global crowd information, a pyramid feature aggregation (PFA)
model to combine low-level and high-level features, an efficient regression
head with multi-scale dilated convolution (MDC) to predict density maps.
Besides, we tailor the loss functions for our pipeline. Without bells and
whistles, extensive experiments demonstrate that our method achieves new
state-of-the-art results on several benchmarks both in weakly and
fully-supervised crowd counting. Moreover, we currently rank No.1 on the
leaderboard of NWPU-Crowd. Our code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Probabilistic Image Colorization. (arXiv:2109.14518v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14518">
<div class="article-summary-box-inner">
<span><p>We propose Generative Probabilistic Image Colorization, a diffusion-based
generative process that trains a sequence of probabilistic models to reverse
each step of noise corruption. Given a line-drawing image as input, our method
suggests multiple candidate colorized images. Therefore, our method accounts
for the ill-posed nature of the colorization problem. We conducted
comprehensive experiments investigating the colorization of line-drawing
images, report the influence of a score-based MCMC approach that corrects the
marginal distribution of estimated samples, and further compare different
combinations of models and the similarity of their generated images. Despite
using only a relatively small training dataset, we experimentally develop a
method to generate multiple diverse colorization candidates which avoids mode
collapse and does not require any additional constraints, losses, or
re-training with alternative training conditions. Our proposed approach
performed well not only on color-conditional image generation tasks using
biased initial values, but also on some practical image completion and
inpainting tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detailed Region-Adaptive Normalization for Heavy Makeup Transfer. (arXiv:2109.14525v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14525">
<div class="article-summary-box-inner">
<span><p>In recent years, facial makeup transfer has attracted growing attention due
to its efficiency and flexibility in transferring makeup styles between
different faces. Although recent works have achieved realistic results, most of
them fail to handle heavy makeup styles with multiple colors and subtle
details. Hence we propose a novel GAN model to handle heavy makeup transfer,
while maintaining the robustness to different poses and expressions. Firstly, a
Makeup Multi-Extraction Network is introduced to learn region-wise makeup
features from multiple layers. Then, a key transferring module called Detailed
Region-Adaptive Normalization is proposed to fuse different levels of makeup
styles in an adaptive way, making great improvement to the quality of heavy
makeup transfer. With the outputs from the two components, Makeup Transfer
Network is used to perform makeup transfer. To evaluate the efficacy of our
proposed method, we collected a new makeup dataset containing a wide range of
heavy styles. Experiments show that our method achieves state-of-the-art
results both on light and heavy makeup styles, and is robust to different poses
and expressions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-Guided Quadrupedal Locomotion in the Wild with Multi-Modal Delay Randomization. (arXiv:2109.14549v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14549">
<div class="article-summary-box-inner">
<span><p>Developing robust vision-guided controllers for quadrupedal robots in complex
environments, with various obstacles, dynamical surroundings and uneven
terrains, is very challenging. While Reinforcement Learning (RL) provides a
promising paradigm for agile locomotion skills with vision inputs in
simulation, it is still very challenging to deploy the RL policy in the real
world. Our key insight is that aside from the discrepancy in the domain gap, in
visual appearance between the simulation and the real world, the latency from
the control pipeline is also a major cause of difficulty. In this paper, we
propose Multi-Modal Delay Randomization (MMDR) to address this issue when
training RL agents. Specifically, we simulate the latency of real hardware by
using past observations, sampled with randomized periods, for both
proprioception and vision. We train the RL policy for end-to-end control in a
physical simulator without any predefined controller or reference motion, and
directly deploy it on the real A1 quadruped robot running in the wild. We
evaluate our method in different outdoor environments with complex terrains and
obstacles. We demonstrate the robot can smoothly maneuver at a high speed,
avoid the obstacles, and show significant improvement over the baselines. Our
project page with videos is at https://mehooz.github.io/mmdr-wild/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Temporal Ensembling for Learning with Noisy Labels. (arXiv:2109.14563v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14563">
<div class="article-summary-box-inner">
<span><p>Successful training of deep neural networks with noisy labels is an essential
capability as most real-world datasets contain some amount of mislabeled data.
Left unmitigated, label noise can sharply degrade typical supervised learning
approaches. In this paper, we present robust temporal ensembling (RTE), which
combines robust loss with semi-supervised regularization methods to achieve
noise-robust learning. We demonstrate that RTE achieves state-of-the-art
performance across the CIFAR-10, CIFAR-100, ImageNet, WebVision, and Food-101N
datasets, while forgoing the recent trend of label filtering and/or fixing.
Finally, we show that RTE also retains competitive corruption robustness to
unforeseen input noise using CIFAR-10-C, obtaining a mean corruption error
(mCE) of 13.50% even in the presence of an 80% noise ratio, versus 26.9% mCE
with standard methods on clean data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Flexible Blind JPEG Artifacts Removal. (arXiv:2109.14573v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14573">
<div class="article-summary-box-inner">
<span><p>Training a single deep blind model to handle different quality factors for
JPEG image artifacts removal has been attracting considerable attention due to
its convenience for practical usage. However, existing deep blind methods
usually directly reconstruct the image without predicting the quality factor,
thus lacking the flexibility to control the output as the non-blind methods. To
remedy this problem, in this paper, we propose a flexible blind convolutional
neural network, namely FBCNN, that can predict the adjustable quality factor to
control the trade-off between artifacts removal and details preservation.
Specifically, FBCNN decouples the quality factor from the JPEG image via a
decoupler module and then embeds the predicted quality factor into the
subsequent reconstructor module through a quality factor attention block for
flexible control. Besides, we find existing methods are prone to fail on
non-aligned double JPEG images even with only a one-pixel shift, and we thus
propose a double JPEG degradation model to augment the training data. Extensive
experiments on single JPEG images, more general double JPEG images, and
real-world JPEG images demonstrate that our proposed FBCNN achieves favorable
performance against state-of-the-art methods in terms of both quantitative
metrics and visual quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decomposable-Net: Scalable Low-Rank Compression for Neural Networks. (arXiv:1910.13141v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1910.13141">
<div class="article-summary-box-inner">
<span><p>Compressing DNNs is important for the real-world applications operating on
resource-constrained devices. However, we typically observe drastic performance
deterioration when changing model size after training is completed. Therefore,
retraining is required to resume the performance of the compressed models
suitable for different devices. In this paper, we propose Decomposable-Net (the
network decomposable in any size), which allows flexible changes to model size
without retraining. We decompose weight matrices in the DNNs via singular value
decomposition and adjust ranks according to the target model size. Unlike the
existing low-rank compression methods that specialize the model to a fixed
size, we propose a novel backpropagation scheme that jointly minimizes losses
for both of full- and low-rank networks. This enables not only to maintain the
performance of a full-rank network {\it without retraining} but also to improve
low-rank networks in multiple sizes. Additionally, we introduce a simple
criterion for rank selection that effectively suppresses approximation error.
In experiments on the ImageNet classification task, Decomposable-Net yields
superior accuracy in a wide range of model sizes. In particular,
Decomposable-Net achieves the top-1 accuracy of $73.2\%$ with $0.27\times$MACs
with ResNet-50, compared to Tucker decomposition ($67.4\% / 0.30\times$),
Trained Rank Pruning ($70.6\% / 0.28\times$), and universally slimmable
networks ($71.4\% / 0.26\times$).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Degenerative Adversarial NeuroImage Nets for Brain Scan Simulations: Application in Ageing and Dementia. (arXiv:1912.01526v5 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.01526">
<div class="article-summary-box-inner">
<span><p>Accurate and realistic simulation of high-dimensional medical images has
become an important research area relevant to many AI-enabled healthcare
applications. However, current state-of-the-art approaches lack the ability to
produce satisfactory high-resolution and accurate subject-specific images. In
this work, we present a deep learning framework, namely 4D-Degenerative
Adversarial NeuroImage Net (4D-DANI-Net), to generate high-resolution,
longitudinal MRI scans that mimic subject-specific neurodegeneration in ageing
and dementia. 4D-DANI-Net is a modular framework based on adversarial training
and a set of novel spatiotemporal, biologically-informed constraints. To ensure
efficient training and overcome memory limitations affecting such
high-dimensional problems, we rely on three key technological advances: i) a
new 3D training consistency mechanism called Profile Weight Functions (PWFs),
ii) a 3D super-resolution module and iii) a transfer learning strategy to
fine-tune the system for a given individual. To evaluate our approach, we
trained the framework on 9852 T1-weighted MRI scans from 876 participants in
the Alzheimer's Disease Neuroimaging Initiative dataset and held out a separate
test set of 1283 MRI scans from 170 participants for quantitative and
qualitative assessment of the personalised time series of synthetic images. We
performed three evaluations: i) image quality assessment; ii) quantifying the
accuracy of regional brain volumes over and above benchmark models; and iii)
quantifying visual perception of the synthetic images by medical experts.
Overall, both quantitative and qualitative results show that 4D-DANI-Net
produces realistic, low-artefact, personalised time series of synthetic T1 MRI
that outperforms benchmark models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LGVTON: A Landmark Guided Approach to Virtual Try-On. (arXiv:2004.00562v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.00562">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a Landmark Guided Virtual Try-On (LGVTON) method
for clothes, which aims to solve the problem of clothing trials on e-commerce
websites. Given the images of two people: a person and a model, it generates a
rendition of the person wearing the clothes of the model. This is useful
considering the fact that on most e-commerce websites images of only clothes
are not usually available. We follow a three-stage approach to achieve our
objective. In the first stage, LGVTON warps the clothes of the model using a
Thin-Plate Spline (TPS) based transformation to fit the person. Unlike previous
TPS-based methods, we use the landmarks (of human and clothes) to compute the
TPS transformation. This enables the warping to work independently of the
complex patterns, such as stripes, florals, and textures, present on the
clothes. However, this computed warp may not always be very precise. We,
therefore, further refine it in the subsequent stages with the help of a mask
generator (Stage 2) and an image synthesizer (Stage 3) modules. The mask
generator improves the fit of the warped clothes, and the image synthesizer
ensures a realistic output. To tackle the problem of lack of paired training
data, we resort to a self-supervised training strategy. Here paired data refers
to the image pair of model and person wearing the same cloth. We compare LGVTON
with four existing methods on two popular fashion datasets namely MPV and
DeepFashion using two performance measures, FID (Fr\'echet Inception Distance)
and SSIM (Structural Similarity Index). The proposed method in most cases
outperforms the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Background-Aware Correlation Filters: Adaptive Context Modeling by Hand-Crafted and Deep RGB Features for Visual Tracking. (arXiv:2004.02932v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.02932">
<div class="article-summary-box-inner">
<span><p>In recent years, the background-aware correlation filters have achie-ved a
lot of research interest in the visual target tracking. However, these methods
cannot suitably model the target appearance due to the exploitation of
hand-crafted features. On the other hand, the recent deep learning-based visual
tracking methods have provided a competitive performance along with extensive
computations. In this paper, an adaptive background-aware correlation
filter-based tracker is proposed that effectively models the target appearance
by using either the histogram of oriented gradients (HOG) or convolutional
neural network (CNN) feature maps. The proposed method exploits the fast 2D
non-maximum suppression (NMS) algorithm and the semantic information comparison
to detect challenging situations. When the HOG-based response map is not
reliable, or the context region has a low semantic similarity with prior
regions, the proposed method constructs the CNN context model to improve the
target region estimation. Furthermore, the rejection option allows the proposed
method to update the CNN context model only on valid regions. Comprehensive
experimental results demonstrate that the proposed adaptive method clearly
outperforms the accuracy and robustness of visual target tracking compared to
the state-of-the-art methods on the OTB-50, OTB-100, TC-128, UAV-123, and
VOT-2015 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RobFR: Benchmarking Adversarial Robustness on Face Recognition. (arXiv:2007.04118v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.04118">
<div class="article-summary-box-inner">
<span><p>Face recognition (FR) has recently made substantial progress and achieved
high accuracy on standard benchmarks. However, it has raised security concerns
in enormous FR applications because deep CNNs are unusually vulnerable to
adversarial examples, and it is still lack of a comprehensive robustness
evaluation before a FR model is deployed in safety-critical scenarios. To
facilitate a better understanding of the adversarial vulnerability on FR, we
develop an adversarial robustness evaluation library on FR named
\textbf{RobFR}, which serves as a reference for evaluating the robustness of
downstream tasks. Specifically, RobFR involves 15 popular naturally trained FR
models, 9 models with representative defense mechanisms and 2 commercial FR API
services, to perform the robustness evaluation by using various adversarial
attacks as an important surrogate. The evaluations are conducted under diverse
adversarial settings in terms of dodging and impersonation, $\ell_2$ and
$\ell_\infty$, as well as white-box and black-box attacks. We further propose a
landmark-guided cutout (LGC) attack method to improve the transferability of
adversarial examples for black-box attacks by considering the special
characteristics of FR. Based on large-scale evaluations, the commercial FR API
services fail to exhibit acceptable performance on robustness evaluation, and
we also draw several important conclusions for understanding the adversarial
robustness of FR models and providing insights for the design of robust FR
models. RobFR is open-source and maintains all extendable modules, i.e.,
\emph{Datasets}, \emph{FR Models}, \emph{Attacks\&amp;Defenses}, and
\emph{Evaluations} at
\url{https://github.com/ShawnXYang/Face-Robustness-Benchmark}, which will be
continuously updated to promote future research on robust FR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">flexgrid2vec: Learning Efficient Visual Representations Vectors. (arXiv:2007.15444v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.15444">
<div class="article-summary-box-inner">
<span><p>We propose flexgrid2vec, a novel approach for image representation learning.
Existing visual representation methods suffer from several issues, including
the need for highly intensive computation, the risk of losing in-depth
structural information and the specificity of the method to certain shapes or
objects. flexgrid2vec converts an image to a low-dimensional feature vector. We
represent each image with a graph of flexible, unique node locations and edge
distances. flexgrid2vec is a multi-channel GCN that learns features of the most
representative image patches. We have investigated both spectral and
non-spectral implementations of the GCN node-embedding. Specifically, we have
implemented flexgrid2vec based on different node-aggregation methods, such as
vector summation, concatenation and normalisation with eigenvector centrality.
We compare the performance of flexgrid2vec with a set of state-of-the-art
visual representation learning models on binary and multi-class image
classification tasks. Although we utilise imbalanced, low-size and
low-resolution datasets, flexgrid2vec shows stable and outstanding results
against well-known base classifiers. flexgrid2vec achieves 96.23% on CIFAR-10,
83.05% on CIFAR-100, 94.50% on STL-10, 98.8% on ASIRRA and 89.69% on the COCO
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Automatic System to Monitor the Physical Distance and Face Mask Wearing of Construction Workers in COVID-19 Pandemic. (arXiv:2101.01373v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.01373">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic has caused many shutdowns in different industries
around the world. Sectors such as infrastructure construction and maintenance
projects have not been suspended due to their significant effect on people's
routine life. In such projects, workers work close together that makes a high
risk of infection. The World Health Organization recommends wearing a face mask
and practicing physical distancing to mitigate the virus's spread. This paper
developed a computer vision system to automatically detect the violation of
face mask wearing and physical distancing among construction workers to assure
their safety on infrastructure projects during the pandemic. For the face mask
detection, the paper collected and annotated 1,000 images, including different
types of face mask wearing, and added them to a pre-existing face mask dataset
to develop a dataset of 1,853 images. Then trained and tested multiple
Tensorflow state-of-the-art object detection models on the face mask dataset
and chose the Faster R-CNN Inception ResNet V2 network that yielded the
accuracy of 99.8%. For physical distance detection, the paper employed the
Faster R-CNN Inception V2 to detect people. A transformation matrix was used to
eliminate the camera angle's effect on the object distances on the image. The
Euclidian distance used the pixels of the transformed image to compute the
actual distance between people. A threshold of six feet was considered to
capture physical distance violation. The paper also used transfer learning for
training the model. The final model was applied on four videos of road
maintenance projects in Houston, TX, that effectively detected the face mask
and physical distance. We recommend that construction owners use the proposed
system to enhance construction workers' safety in the pandemic situation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention-Guided Black-box Adversarial Attacks with Large-Scale Multiobjective Evolutionary Optimization. (arXiv:2101.07512v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07512">
<div class="article-summary-box-inner">
<span><p>Fooling deep neural networks (DNNs) with the black-box optimization has
become a popular adversarial attack fashion, as the structural prior knowledge
of DNNs is always unknown. Nevertheless, recent black-box adversarial attacks
may struggle to balance their attack ability and visual quality of the
generated adversarial examples (AEs) in tackling high-resolution images. In
this paper, we propose an attention-guided black-box adversarial attack based
on the large-scale multiobjective evolutionary optimization, termed as LMOA. By
considering the spatial semantic information of images, we firstly take
advantage of the attention map to determine the perturbed pixels. Instead of
attacking the entire image, reducing the perturbed pixels with the attention
mechanism can help to avoid the notorious curse of dimensionality and thereby
improves the performance of attacking. Secondly, a large-scale multiobjective
evolutionary algorithm is employed to traverse the reduced pixels in the
salient region. Benefiting from its characteristics, the generated AEs have the
potential to fool target DNNs while being imperceptible by the human vision.
Extensive experimental results have verified the effectiveness of the proposed
LMOA on the ImageNet dataset. More importantly, it is more competitive to
generate high-resolution AEs with better visual quality compared with the
existing black-box adversarial attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNETR: Transformers for 3D Medical Image Segmentation. (arXiv:2103.10504v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10504">
<div class="article-summary-box-inner">
<span><p>Fully Convolutional Neural Networks (FCNNs) with contracting and expanding
paths have shown prominence for the majority of medical image segmentation
applications since the past decade. In FCNNs, the encoder plays an integral
role by learning both global and local features and contextual representations
which can be utilized for semantic output prediction by the decoder. Despite
their success, the locality of convolutional layers in FCNNs, limits the
capability of learning long-range spatial dependencies. Inspired by the recent
success of transformers for Natural Language Processing (NLP) in long-range
sequence learning, we reformulate the task of volumetric (3D) medical image
segmentation as a sequence-to-sequence prediction problem. We introduce a novel
architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer
as the encoder to learn sequence representations of the input volume and
effectively capture the global multi-scale information, while also following
the successful "U-shaped" network design for the encoder and decoder. The
transformer encoder is directly connected to a decoder via skip connections at
different resolutions to compute the final semantic segmentation output. We
have validated the performance of our method on the Multi Atlas Labeling Beyond
The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical
Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation
tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV
leaderboard. Code: https://monai.io/research/unetr
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time Multi-Adaptive-Resolution-Surfel 6D LiDAR Odometry using Continuous-time Trajectory Optimization. (arXiv:2105.02010v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.02010">
<div class="article-summary-box-inner">
<span><p>Simultaneous Localization and Mapping (SLAM) is an essential capability for
autonomous robots, but due to high data rates of 3D LiDARs real-time SLAM is
challenging. We propose a real-time method for 6D LiDAR odometry. Our approach
combines a continuous-time B-Spline trajectory representation with a Gaussian
Mixture Model (GMM) formulation to jointly align local multi-resolution surfel
maps. Sparse voxel grids and permutohedral lattices ensure fast access to map
surfels, and an adaptive resolution selection scheme effectively speeds up
registration. A thorough experimental evaluation shows the performance of our
approach on multiple datasets and during real-robot experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding. (arXiv:2105.09996v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09996">
<div class="article-summary-box-inner">
<span><p>We present a simplified, task-agnostic multi-modal pre-training approach that
can accept either video or text input, or both for a variety of end tasks.
Existing pre-training are task-specific by adopting either a single cross-modal
encoder that requires both modalities, limiting their use for retrieval-style
end tasks or more complex multitask learning with two unimodal encoders,
limiting early cross-modal fusion. We instead introduce new pretraining masking
schemes that better mix across modalities (e.g. by forcing masks for text to
predict the closest video embeddings) while also maintaining separability (e.g.
unimodal predictions are sometimes required, without using all the input).
Experimental results show strong performance across a wider range of tasks than
any previous methods, often outperforming task-specific pre-training. Code is
made available at https://github.com/pytorch/fairseq/examples/MMPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Seamless and High-Performance Out-of-Distribution Detection Approach Simply Replacing the SoftMax Loss. (arXiv:2105.14399v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14399">
<div class="article-summary-box-inner">
<span><p>Current out-of-distribution detection approaches usually present special
requirements (e.g., collecting outlier data and hyperparameter validation) and
produce side effects (classification accuracy drop and slow/inefficient
inferences). Recently, entropic out-of-distribution detection has been proposed
as a seamless approach (i.e., a solution that avoids all the previously
mentioned drawbacks). The entropic out-of-distribution detection solution
comprises the IsoMax loss for training and the entropic score for
out-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in
replacement because swapping the SoftMax loss with the IsoMax loss requires no
changes in the model's architecture or training procedures/hyperparameters. In
this paper, we propose to perform what we call an isometrization of the
distances used in the IsoMax loss. Additionally, we propose to replace the
entropic score with the minimum distance score. Our experiments showed that
these simple modifications increase out-of-distribution detection performance
while keeping the solution seamless. Besides being competitive with or
outperforming all major current approaches, our solution avoids all their
current limitations in addition to being much easier to use, as just a simple
loss replacement for training the neural network is required. Code available at
https://github.com/dlmacedo/entropic-out-of-distribution-detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Contextual Knowledge to Visual Features for Fine Art Classification. (arXiv:2105.15028v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.15028">
<div class="article-summary-box-inner">
<span><p>Automatic art analysis has seen an ever-increasing interest from the pattern
recognition and computer vision community. However, most of the current work is
mainly based solely on digitized artwork images, sometimes supplemented with
some metadata and textual comments. A knowledge graph that integrates a rich
body of information about artworks, artists, painting schools, etc., in a
unified structured framework can provide a valuable resource for more powerful
information retrieval and knowledge discovery tools in the artistic domain. To
this end, this paper presents ArtGraph: an artistic knowledge graph based on
WikiArt and DBpedia. The graph, implemented in Neo4j, already provides
knowledge discovery capabilities without having to train a learning system. In
addition, the embeddings extracted from the graph are used to inject
"contextual" knowledge into a deep learning model to improve the accuracy of
artwork attribute prediction tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Positional Contrastive Learning for Volumetric Medical Image Segmentation. (arXiv:2106.09157v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09157">
<div class="article-summary-box-inner">
<span><p>The success of deep learning heavily depends on the availability of large
labeled training sets. However, it is hard to get large labeled datasets in
medical image domain because of the strict privacy concern and costly labeling
efforts. Contrastive learning, an unsupervised learning technique, has been
proved powerful in learning image-level representations from unlabeled data.
The learned encoder can then be transferred or fine-tuned to improve the
performance of downstream tasks with limited labels. A critical step in
contrastive learning is the generation of contrastive data pairs, which is
relatively simple for natural image classification but quite challenging for
medical image segmentation due to the existence of the same tissue or organ
across the dataset. As a result, when applied to medical image segmentation,
most state-of-the-art contrastive learning frameworks inevitably introduce a
lot of false-negative pairs and result in degraded segmentation quality. To
address this issue, we propose a novel positional contrastive learning (PCL)
framework to generate contrastive data pairs by leveraging the position
information in volumetric medical images. Experimental results on CT and MRI
datasets demonstrate that the proposed PCL method can substantially improve the
segmentation performance compared to existing methods in both semi-supervised
setting and transfer learning setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ToAlign: Task-oriented Alignment for Unsupervised Domain Adaptation. (arXiv:2106.10812v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10812">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptive classification intends to improve
theclassification performance on unlabeled target domain. To alleviate the
adverse effect of domain shift, many approaches align the source and target
domains in the feature space. However, a feature is usually taken as a whole
for alignment without explicitly making domain alignment proactively serve the
classification task, leading to sub-optimal solution. What sub-feature should
be aligned for better adaptation is under-explored. In this paper, we propose
an effective Task-oriented Alignment (ToAlign) for unsupervised domain
adaptation (UDA). We study what features should be aligned across domains and
propose to make the domain alignment proactively serve classification by
performing feature decomposition and alignment under the guidance of the prior
knowledge induced from the classification taskitself. Particularly, we
explicitly decompose a feature in the source domain intoa
task-related/discriminative feature that should be aligned, and a
task-irrelevant feature that should be avoided/ignored, based on the
classification meta-knowledge. Extensive experimental results on various
benchmarks (e.g., Office-Home, Visda-2017, and DomainNet) under different
domain adaptation settings demonstrate theeffectiveness of ToAlign which helps
achieve the state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the importance of cross-task features for class-incremental learning. (arXiv:2106.11930v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11930">
<div class="article-summary-box-inner">
<span><p>In class-incremental learning, an agent with limited resources needs to learn
a sequence of classification tasks, forming an ever growing classification
problem, with the constraint of not being able to access data from previous
tasks. The main difference with task-incremental learning, where a task-ID is
available at inference time, is that the learner also needs to perform
cross-task discrimination, i.e. distinguish between classes that have not been
seen together. Approaches to tackle this problem are numerous and mostly make
use of an external memory (buffer) of non-negligible size. In this paper, we
ablate the learning of cross-task features and study its influence on the
performance of basic replay strategies used for class-IL. We also define a new
forgetting measure for class-incremental learning, and see that forgetting is
not the principal cause of low performance. Our experimental results show that
future algorithms for class-incremental learning should not only prevent
forgetting, but also aim to improve the quality of the cross-task features, and
the knowledge transfer between tasks. This is especially important when tasks
contain limited amount of data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GTNet:Guided Transformer Network for Detecting Human-Object Interactions. (arXiv:2108.00596v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00596">
<div class="article-summary-box-inner">
<span><p>The human-object interaction (HOI) detection task refers to localizing
humans, localizing objects, and predicting the interactions between each
human-object pair. HOI is considered one of the fundamental steps in truly
understanding complex visual scenes. For detecting HOI, it is important to
utilize relative spatial configurations and object semantics to find salient
spatial regions of images that highlight the interactions between human object
pairs. This issue is addressed by the novel self-attention based guided
transformer network, GTNet. GTNet encodes this spatial contextual information
in human and object visual features via self-attention while achieving state of
the art results on both the V-COCO and HICO-DET datasets. Code will be made
available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Cut by Watching Movies. (arXiv:2108.04294v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04294">
<div class="article-summary-box-inner">
<span><p>Video content creation keeps growing at an incredible pace; yet, creating
engaging stories remains challenging and requires non-trivial video editing
expertise. Many video editing components are astonishingly hard to automate
primarily due to the lack of raw video materials. This paper focuses on a new
task for computational video editing, namely the task of raking cut
plausibility. Our key idea is to leverage content that has already been edited
to learn fine-grained audiovisual patterns that trigger cuts. To do this, we
first collected a data source of more than 10K videos, from which we extract
more than 255K cuts. We devise a model that learns to discriminate between real
and artificial cuts via contrastive learning. We set up a new task and a set of
baselines to benchmark video cut generation. We observe that our proposed model
outperforms the baselines by large margins. To demonstrate our model in
real-world applications, we conduct human studies in a collection of unedited
videos. The results show that our model does a better job at cutting than
random and alternative baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Information Theory-inspired Strategy for Automatic Network Pruning. (arXiv:2108.08532v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08532">
<div class="article-summary-box-inner">
<span><p>Despite superior performance on many computer vision tasks, deep convolution
neural networks are well known to be compressed on devices that have resource
constraints. Most existing network pruning methods require laborious human
efforts and prohibitive computation resources, especially when the constraints
are changed. This practically limits the application of model compression when
the model needs to be deployed on a wide range of devices. Besides, existing
methods are still challenged by the missing theoretical guidance. In this paper
we propose an information theory-inspired strategy for automatic model
compression. The principle behind our method is the information bottleneck
theory, i.e., the hidden representation should compress information with each
other. We thus introduce the normalized Hilbert-Schmidt Independence Criterion
(nHSIC) on network activations as a stable and generalized indicator of layer
importance. When a certain resource constraint is given, we integrate the HSIC
indicator with the constraint to transform the architecture search problem into
a linear programming problem with quadratic constraints. Such a problem is
easily solved by a convex optimization method with a few seconds. We also
provide a rigorous proof to reveal that optimizing the normalized HSIC
simultaneously minimizes the mutual information between different layers.
Without any search process, our method achieves better compression tradeoffs
comparing to the state-of-the-art compression algorithms. For instance, with
ResNet-50, we achieve a 45.3%-FLOPs reduction, with a 75.75 top-1 accuracy on
ImageNet. Codes are avaliable at
https://github.com/MAC-AutoML/ITPruner/tree/master.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Objective for Novel Class Discovery. (arXiv:2108.08536v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08536">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the problem of Novel Class Discovery (NCD). NCD aims
at inferring novel object categories in an unlabeled set by leveraging from
prior knowledge of a labeled set containing different, but related classes.
Existing approaches tackle this problem by considering multiple objective
functions, usually involving specialized loss terms for the labeled and the
unlabeled samples respectively, and often requiring auxiliary regularization
terms. In this paper, we depart from this traditional scheme and introduce a
UNified Objective function (UNO) for discovering novel classes, with the
explicit purpose of favoring synergy between supervised and unsupervised
learning. Using a multi-view self-labeling strategy, we generate pseudo-labels
that can be treated homogeneously with ground truth labels. This leads to a
single classification objective operating on both known and unknown classes.
Despite its simplicity, UNO outperforms the state of the art by a significant
margin on several benchmarks (~+10% on CIFAR-100 and +8% on ImageNet). The
project page is available at: https://ncd-uno.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">External Knowledge enabled Text Visual Question Answering. (arXiv:2108.09717v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09717">
<div class="article-summary-box-inner">
<span><p>The open-ended question answering task of Text-VQA requires reading and
reasoning about local, often previously unseen, scene-text content of an image
to generate answers. In this work, we propose the generalized use of external
knowledge to augment our understanding of the said scene-text. We design a
framework to extract, validate, and reason with knowledge using a standard
multimodal transformer for vision language understanding tasks. Through
empirical evidence and qualitative results, we demonstrate how external
knowledge can highlight instance-only cues and thus help deal with training
data bias, improve answer entity type correctness, and detect multiword named
entities. We generate results comparable to the state-of-the-art on two
publicly available datasets, under the constraints of similar upstream OCR
systems and training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The 2nd Anti-UAV Workshop & Challenge: Methods and Results. (arXiv:2108.09909v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09909">
<div class="article-summary-box-inner">
<span><p>The 2nd Anti-UAV Workshop \&amp; Challenge aims to encourage research in
developing novel and accurate methods for multi-scale object tracking. The
Anti-UAV dataset used for the Anti-UAV Challenge has been publicly released.
There are two subsets in the dataset, $i.e.$, the test-dev subset and
test-challenge subset. Both subsets consist of 140 thermal infrared video
sequences, spanning multiple occurrences of multi-scale UAVs. Around 24
participating teams from the globe competed in the 2nd Anti-UAV Challenge. In
this paper, we provide a brief summary of the 2nd Anti-UAV Workshop \&amp;
Challenge including brief introductions to the top three methods.The submission
leaderboard will be reopened for researchers that are interested in the
Anti-UAV challenge. The benchmark dataset and other information can be found
at: https://anti-uav.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stain-Robust Mitotic Figure Detection for the Mitosis Domain Generalization Challenge. (arXiv:2109.00853v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00853">
<div class="article-summary-box-inner">
<span><p>The detection of mitotic figures from different scanners/sites remains an
important topic of research, owing to its potential in assisting clinicians
with tumour grading. The MItosis DOmain Generalization (MIDOG) challenge aims
to test the robustness of detection models on unseen data from multiple
scanners for this task. We present a short summary of the approach employed by
the TIA Centre team to address this challenge. Our approach is based on a
hybrid detection model, where mitotic candidates are segmented on stain
normalised images, before being refined by a deep learning classifier.
Cross-validation on the training images achieved the F1-score of 0.786 and
0.765 on the preliminary test set, demonstrating the generalizability of our
model to unseen data from new scanners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptive Cascade R-CNN for MItosis DOmain Generalization (MIDOG) Challenge. (arXiv:2109.00965v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00965">
<div class="article-summary-box-inner">
<span><p>We present a summary of the domain adaptive cascade R-CNN method for mitosis
detection of digital histopathology images. By comprehensive data augmentation
and adapting existing popular detection architecture, our proposed method has
achieved an F1 score of 0.7500 on the preliminary test set in MItosis DOmain
Generalization (MIDOG) Challenge at MICCAI 2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Mitosis Detection Using a Cascade Mask-RCNN Approach With Domain-Specific Residual Cycle-GAN Data Augmentation. (arXiv:2109.01878v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01878">
<div class="article-summary-box-inner">
<span><p>For the MIDOG mitosis detection challenge, we created a cascade algorithm
consisting of a Mask-RCNN detector, followed by a classification ensemble
consisting of ResNet50 and DenseNet201 to refine detected mitotic candidates.
The MIDOG training data consists of 200 frames originating from four scanners,
three of which are annotated for mitotic instances with centroid annotations.
Our main algorithmic choices are as follows: first, to enhance the
generalizability of our detector and classification networks, we use a
state-of-the-art residual Cycle-GAN to transform each scanner domain to every
other scanner domain. During training, we then randomly load, for each image,
one of the four domains. In this way, our networks can learn from the fourth
non-annotated scanner domain even if we don't have annotations for it. Second,
for training the detector network, rather than using centroid-based fixed-size
bounding boxes, we create mitosis-specific bounding boxes. We do this by
manually annotating a small selection of mitoses, training a Mask-RCNN on this
small dataset, and applying it to the rest of the data to obtain full
annotations. We trained the follow-up classification ensemble using only the
challenge-provided positive and hard-negative examples. On the preliminary test
set, the algorithm scores an F1 score of 0.7578, putting us as the second-place
team on the leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differential Diagnosis of Frontotemporal Dementia and Alzheimer's Disease using Generative Adversarial Network. (arXiv:2109.05627v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05627">
<div class="article-summary-box-inner">
<span><p>Frontotemporal dementia and Alzheimer's disease are two common forms of
dementia and are easily misdiagnosed as each other due to their similar pattern
of clinical symptoms. Differentiating between the two dementia types is crucial
for determining disease-specific intervention and treatment. Recent development
of Deep-learning-based approaches in the field of medical image computing are
delivering some of the best performance for many binary classification tasks,
although its application in differential diagnosis, such as neuroimage-based
differentiation for multiple types of dementia, has not been explored. In this
study, a novel framework was proposed by using the Generative Adversarial
Network technique to distinguish FTD, AD and normal control subjects, using
volumetric features extracted at coarse-to-fine structural scales from Magnetic
Resonance Imaging scans. Experiments of 10-folds cross-validation on 1,954
images achieved high accuracy. With the proposed framework, we have
demonstrated that the combination of multi-scale structural features and
synthetic data augmentation based on generative adversarial network can improve
the performance of challenging tasks such as differentiating Dementia
sub-types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Contrastive Learning for Label-efficient Medical Image Segmentation. (arXiv:2109.07407v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07407">
<div class="article-summary-box-inner">
<span><p>The success of deep learning methods in medical image segmentation tasks
heavily depends on a large amount of labeled data to supervise the training. On
the other hand, the annotation of biomedical images requires domain knowledge
and can be laborious. Recently, contrastive learning has demonstrated great
potential in learning latent representation of images even without any label.
Existing works have explored its application to biomedical image segmentation
where only a small portion of data is labeled, through a pre-training phase
based on self-supervised contrastive learning without using any labels followed
by a supervised fine-tuning phase on the labeled portion of data only. In this
paper, we establish that by including the limited label in formation in the
pre-training phase, it is possible to boost the performance of contrastive
learning. We propose a supervised local contrastive loss that leverages limited
pixel-wise annotation to force pixels with the same label to gather around in
the embedding space. Such loss needs pixel-wise computation which can be
expensive for large images, and we further propose two strategies, downsampling
and block division, to address the issue. We evaluate our methods on two public
biomedical image datasets of different modalities. With different amounts of
labeled data, our methods consistently outperform the state-of-the-art
contrast-based methods and other semi-supervised learning techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mass Segmentation in Automated 3-D Breast Ultrasound Using Dual-Path U-net. (arXiv:2109.08330v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08330">
<div class="article-summary-box-inner">
<span><p>Automated 3-D breast ultrasound (ABUS) is a newfound system for breast
screening that has been proposed as a supplementary modality to mammography for
breast cancer detection. While ABUS has better performance in dense breasts,
reading ABUS images is exhausting and time-consuming. So, a computer-aided
detection system is necessary for interpretation of these images. Mass
segmentation plays a vital role in the computer-aided detection systems and it
affects the overall performance. Mass segmentation is a challenging task
because of the large variety in size, shape, and texture of masses. Moreover,
an imbalanced dataset makes segmentation harder. A novel mass segmentation
approach based on deep learning is introduced in this paper. The deep network
that is used in this study for image segmentation is inspired by U-net, which
has been used broadly for dense segmentation in recent years. The system's
performance was determined using a dataset of 50 masses including 38 malign and
12 benign lesions. The proposed segmentation method attained a mean Dice of
0.82 which outperformed a two-stage supervised edge-based method with a mean
Dice of 0.74 and an adaptive region growing method with a mean Dice of 0.65.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging. (arXiv:2109.09658v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09658">
<div class="article-summary-box-inner">
<span><p>The recent advancements in artificial intelligence (AI) combined with the
extensive amount of data generated by today's clinical systems, has led to the
development of imaging AI solutions across the whole value chain of medical
imaging, including image reconstruction, medical image segmentation,
image-based diagnosis and treatment planning. Notwithstanding the successes and
future potential of AI in medical imaging, many stakeholders are concerned of
the potential risks and ethical implications of imaging AI solutions, which are
perceived as complex, opaque, and difficult to comprehend, utilise, and trust
in critical clinical applications. Despite these concerns and risks, there are
currently no concrete guidelines and best practices for guiding future AI
developments in medical imaging towards increased trust, safety and adoption.
To bridge this gap, this paper introduces a careful selection of guiding
principles drawn from the accumulated experiences, consensus, and best
practices from five large European projects on AI in Health Imaging. These
guiding principles are named FUTURE-AI and its building blocks consist of (i)
Fairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness
and (vi) Explainability. In a step-by-step approach, these guidelines are
further translated into a framework of concrete recommendations for specifying,
developing, evaluating, and deploying technically, clinically and ethically
trustworthy AI solutions into clinical practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compressive Visual Representations. (arXiv:2109.12909v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12909">
<div class="article-summary-box-inner">
<span><p>Learning effective visual representations that generalize well without human
supervision is a fundamental problem in order to apply Machine Learning to a
wide variety of tasks. Recently, two families of self-supervised methods,
contrastive learning and latent bootstrapping, exemplified by SimCLR and BYOL
respectively, have made significant progress. In this work, we hypothesize that
adding explicit information compression to these algorithms yields better and
more robust representations. We verify this by developing SimCLR and BYOL
formulations compatible with the Conditional Entropy Bottleneck (CEB)
objective, allowing us to both measure and control the amount of compression in
the learned representation, and observe their impact on downstream tasks.
Furthermore, we explore the relationship between Lipschitz continuity and
compression, showing a tractable lower bound on the Lipschitz constant of the
encoders we learn. As Lipschitz continuity is closely related to robustness,
this provides a new explanation for why compressed models are more robust. Our
experiments confirm that adding compression to SimCLR and BYOL significantly
improves linear evaluation accuracies and model robustness across a wide range
of domain shifts. In particular, the compressed version of BYOL achieves 76.0%
Top-1 linear evaluation accuracy on ImageNet with ResNet-50, and 78.8% with
ResNet-50 2x.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HarrisZ$^+$: Harris Corner Selection for Next-Gen Image Matching Pipelines. (arXiv:2109.12925v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12925">
<div class="article-summary-box-inner">
<span><p>Due to its role in many computer vision tasks, image matching has been
subjected to an active investigation by researchers, which has lead to better
and more discriminant feature descriptors and to more robust matching
strategies, also thanks to the advent of the deep learning and the increased
computational power of the modern hardware. Despite of these achievements, the
keypoint extraction process at the base of the image matching pipeline has not
seen equivalent progresses. This paper presents Harrisz$^{+}$, an upgrade to
the HarrisZ corner detector, optimized to synergically take advance of the
recent improvements of the other steps of the image matching pipeline.
Harrisz$^{+}$ does not only consists of a tuning of the setup parameters, but
introduces further refinements to the selection criteria delineated by HarrisZ,
so providing more, yet discriminative, keypoints, which are better distributed
on the image and with higher localization accuracy. The image matching pipeline
including Harrisz$^{+}$, together with the other modern components, obtained in
different recent matching benchmarks state-of-the-art results among the classic
image matching pipelines, closely following results of the more recent fully
deep end-to-end trainable approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Contrastive Learning Approach to Auroral Identification and Classification. (arXiv:2109.13899v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13899">
<div class="article-summary-box-inner">
<span><p>Unsupervised learning algorithms are beginning to achieve accuracies
comparable to their supervised counterparts on benchmark computer vision tasks,
but their utility for practical applications has not yet been demonstrated. In
this work, we present a novel application of unsupervised learning to the task
of auroral image classification. Specifically, we modify and adapt the Simple
framework for Contrastive Learning of Representations (SimCLR) algorithm to
learn representations of auroral images in a recently released auroral image
dataset constructed using image data from Time History of Events and Macroscale
Interactions during Substorms (THEMIS) all-sky imagers. We demonstrate that (a)
simple linear classifiers fit to the learned representations of the images
achieve state-of-the-art classification performance, improving the
classification accuracy by almost 10 percentage points over the current
benchmark; and (b) the learned representations naturally cluster into more
clusters than exist manually assigned categories, suggesting that existing
categorizations are overly coarse and may obscure important connections between
auroral types, near-earth solar wind conditions, and geomagnetic disturbances
at the earth's surface. Moreover, our model is much lighter than the previous
benchmark on this dataset, requiring in the area of fewer than 25\% of the
number of parameters. Our approach exceeds an established threshold for
operational purposes, demonstrating readiness for deployment and utilization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PDC-Net+: Enhanced Probabilistic Dense Correspondence Network. (arXiv:2109.13912v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13912">
<div class="article-summary-box-inner">
<span><p>Establishing robust and accurate correspondences between a pair of images is
a long-standing computer vision problem with numerous applications. While
classically dominated by sparse methods, emerging dense approaches offer a
compelling alternative paradigm that avoids the keypoint detection step.
However, dense flow estimation is often inaccurate in the case of large
displacements, occlusions, or homogeneous regions. In order to apply dense
methods to real-world applications, such as pose estimation, image
manipulation, or 3D reconstruction, it is therefore crucial to estimate the
confidence of the predicted matches.
</p>
<p>We propose the Enhanced Probabilistic Dense Correspondence Network, PDC-Net+,
capable of estimating accurate dense correspondences along with a reliable
confidence map. We develop a flexible probabilistic approach that jointly
learns the flow prediction and its uncertainty. In particular, we parametrize
the predictive distribution as a constrained mixture model, ensuring better
modelling of both accurate flow predictions and outliers. Moreover, we develop
an architecture and an enhanced training strategy tailored for robust and
generalizable uncertainty prediction in the context of self-supervised
training. Our approach obtains state-of-the-art results on multiple challenging
geometric matching and optical flow datasets. We further validate the
usefulness of our probabilistic confidence estimation for the tasks of pose
estimation, 3D reconstruction, image-based localization, and image retrieval.
Code and models are available at https://github.com/PruneTruong/DenseMatching.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-30 23:02:29.031142587 UTC">2021-09-30 23:02:29 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>