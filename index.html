<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-04-13T01:30:00Z">04-13</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Position-wise optimizer: A nature-inspired optimization algorithm. (arXiv:2204.05312v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05312">
<div class="article-summary-box-inner">
<span><p>The human nervous system utilizes synaptic plasticity to solve optimization
problems. Previous studies have tried to add the plasticity factor to the
training process of artificial neural networks, but most of those models
require complex external control over the network or complex novel rules. In
this manuscript, a novel nature-inspired optimization algorithm is introduced
that imitates biological neural plasticity. Furthermore, the model is tested on
three datasets and the results are compared with gradient descent optimization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-Scale Streaming End-to-End Speech Translation with Neural Transducers. (arXiv:2204.05352v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05352">
<div class="article-summary-box-inner">
<span><p>Neural transducers have been widely used in automatic speech recognition
(ASR). In this paper, we introduce it to streaming end-to-end speech
translation (ST), which aims to convert audio signals to texts in other
languages directly. Compared with cascaded ST that performs ASR followed by
text-based machine translation (MT), the proposed Transformer transducer
(TT)-based ST model drastically reduces inference latency, exploits speech
information, and avoids error propagation from ASR to MT. To improve the
modeling capacity, we propose attention pooling for the joint network in TT. In
addition, we extend TT-based ST to multilingual ST, which generates texts of
multiple languages at the same time. Experimental results on a large-scale 50
thousand (K) hours pseudo-labeled training set show that TT-based ST not only
significantly reduces inference time but also outperforms non-streaming
cascaded ST for English-German translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Generative Language Model for Few-shot Aspect-Based Sentiment Analysis. (arXiv:2204.05356v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05356">
<div class="article-summary-box-inner">
<span><p>Sentiment analysis is an important task in natural language processing. In
recent works, pre-trained language models are often used to achieve
state-of-the-art results, especially when training data is scarce. It is common
to fine-tune on the downstream task, usually by adding task-specific layers on
top of the model. In this paper, we focus on aspect-based sentiment analysis,
which involves extracting aspect term, category, and predicting their
corresponding polarities. In particular, we are interested in few-shot
settings. We propose to reformulate the extraction and prediction tasks into
the sequence generation task, using a generative language model with
unidirectional attention (GPT2 is used unless stated otherwise). This way, the
model learns to accomplish the tasks via language generation without the need
of training task-specific layers. Our evaluation results on the single-task
polarity prediction show that our approach outperforms the previous
state-of-the-art (based on BERT) on average performance by a large margins in
few-shot and full-shot settings. More importantly, our generative approach
significantly reduces the model variance caused by low-resource data. We
further demonstrate that the proposed generative language model can handle
joint and multi-task settings, unlike previous work. We observe that the
proposed sequence generation method achieves further improved performances on
polarity prediction when the model is trained via joint and multi-task
settings. Further evaluation on similar sentiment analysis datasets, SST-2,
SST- and OOS intent detection validates the superiority and noise robustness of
generative language model in few-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Speech-Text Pre-training for Speech Translation and Recognition. (arXiv:2204.05409v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05409">
<div class="article-summary-box-inner">
<span><p>We describe a method to jointly pre-train speech and text in an
encoder-decoder modeling framework for speech translation and recognition. The
proposed method incorporates four self-supervised and supervised subtasks for
cross modality learning. A self-supervised speech subtask leverages unlabelled
speech data, and a (self-)supervised text to text subtask makes use of abundant
text training data. Two auxiliary supervised speech tasks are included to unify
speech and text modeling space. Our contribution lies in integrating linguistic
information from the text corpus into the speech pre-training. Detailed
analysis reveals learning interference among subtasks. Two pre-training
configurations for speech translation and recognition, respectively, are
presented to alleviate subtask interference. Our experiments show the proposed
method can effectively fuse speech and text information into one model. It
achieves between 1.7 and 2.3 BLEU improvement above the state of the art on the
MuST-C speech translation dataset and comparable WERs to wav2vec 2.0 on the
Librispeech speech recognition task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beam Decoding with Controlled Patience. (arXiv:2204.05424v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05424">
<div class="article-summary-box-inner">
<span><p>Text generation with beam search has proven successful in a wide range of
applications. The commonly-used implementation of beam decoding follows a first
come, first served heuristic: it keeps a set of already completed sequences
over time steps and stops when the size of this set reaches the beam size. We
introduce a patience factor, a simple modification to this decoding algorithm,
that generalizes the stopping criterion and provides flexibility to the depth
of search. Extensive empirical results demonstrate that the patience factor
improves decoding performance of strong pretrained models on news text
summarization and machine translation over diverse language pairs, with a
negligible inference slowdown. Our approach only modifies one line of code and
can be thus readily incorporated in any implementation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProtoTEx: Explaining Model Decisions with Prototype Tensors. (arXiv:2204.05426v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05426">
<div class="article-summary-box-inner">
<span><p>We present ProtoTEx, a novel white-box NLP classification architecture based
on prototype networks. ProtoTEx faithfully explains model decisions based on
prototype tensors that encode latent clusters of training examples. At
inference time, classification decisions are based on the distances between the
input text and the prototype tensors, explained via the training examples most
similar to the most influential prototypes. We also describe a novel
interleaved training algorithm that effectively handles classes characterized
by the absence of indicative features. On a propaganda detection task, ProtoTEx
accuracy matches BART-large and exceeds BERT-large with the added benefit of
providing faithful explanations. A user study also shows that prototype-based
explanations help non-experts to better recognize propaganda in online news.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multilingual Perspective Towards the Evaluation of Attribution Methods in Natural Language Inference. (arXiv:2204.05428v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05428">
<div class="article-summary-box-inner">
<span><p>Most evaluations of attribution methods focus on the English language. In
this work, we present a multilingual approach for evaluating attribution
methods for the Natural Language Inference (NLI) task in terms of plausibility
and faithfulness properties. First, we introduce a novel cross-lingual strategy
to measure faithfulness based on word alignments, which eliminates the
potential downsides of erasure-based evaluations. We then perform a
comprehensive evaluation of attribution methods, considering different output
mechanisms and aggregation methods. Finally, we augment the XNLI dataset with
highlight-based explanations, providing a multilingual NLI dataset with
highlights, which may support future exNLP studies. Our results show that
attribution methods performing best for plausibility and faithfulness are
different.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Easy Adaptation to Mitigate Gender Bias in Multilingual Text Classification. (arXiv:2204.05459v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05459">
<div class="article-summary-box-inner">
<span><p>Existing approaches to mitigate demographic biases evaluate on monolingual
data, however, multilingual data has not been examined. In this work, we treat
the gender as domains (e.g., male vs. female) and present a standard domain
adaptation model to reduce the gender bias and improve performance of text
classifiers under multilingual settings. We evaluate our approach on two text
classification tasks, hate speech detection and rating prediction, and
demonstrate the effectiveness of our approach with three fair-aware baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CorrectSpeech: A Fully Automated System for Speech Correction and Accent Reduction. (arXiv:2204.05460v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05460">
<div class="article-summary-box-inner">
<span><p>This study extends our previous work on text-based speech editing to
developing a fully automated system for speech correction and accent reduction.
Consider the application scenario that a recorded speech audio contains certain
errors, e.g., inappropriate words, mispronunciations, that need to be
corrected. The proposed system, named CorrectSpeech, performs the correction in
three steps: recognizing the recorded speech and converting it into
time-stamped symbol sequence, aligning recognized symbol sequence with target
text to determine locations and types of required edit operations, and
generating the corrected speech. Experiments show that the quality and
naturalness of corrected speech depend on the performance of speech recognition
and alignment modules, as well as the granularity level of editing operations.
The proposed system is evaluated on two corpora: a manually perturbed version
of VCTK and L2-ARCTIC. The results demonstrate that our system is able to
correct mispronunciation and reduce accent in speech recordings. Audio samples
are available online for demonstration
https://daxintan-cuhk.github.io/CorrectSpeech/ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Redwood: Using Collision Detection to Grow a Large-Scale Intent Classification Dataset. (arXiv:2204.05483v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05483">
<div class="article-summary-box-inner">
<span><p>Dialog systems must be capable of incorporating new skills via updates over
time in order to reflect new use cases or deployment scenarios. Similarly,
developers of such ML-driven systems need to be able to add new training data
to an already-existing dataset to support these new skills. In intent
classification systems, problems can arise if training data for a new skill's
intent overlaps semantically with an already-existing intent. We call such
cases collisions. This paper introduces the task of intent collision detection
between multiple datasets for the purposes of growing a system's skillset. We
introduce several methods for detecting collisions, and evaluate our methods on
real datasets that exhibit collisions. To highlight the need for intent
collision detection, we show that model performance suffers if new data is
added in such a way that does not arbitrate colliding intents. Finally, we use
collision detection to construct and benchmark a new dataset, Redwood, which is
composed of 451 ntent categories from 13 original intent classification
datasets, making it the largest publicly available intent classification
benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overlapping Word Removal is All You Need: Revisiting Data Imbalance in Hope Speech Detection. (arXiv:2204.05488v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05488">
<div class="article-summary-box-inner">
<span><p>Hope Speech Detection, a task of recognizing positive expressions, has made
significant strides recently. However, much of the current works focus on model
development without considering the issue of inherent imbalance in the data.
Our work revisits this issue in hope-speech detection by introducing focal
loss, data augmentation, and pre-processing strategies. Accordingly, we find
that introducing focal loss as part of Multilingual-BERT's (M-BERT) training
process mitigates the effect of class imbalance and improves overall F1-Macro
by 0.11. At the same time, contextual and back-translation-based word
augmentation with M-BERT improves results by 0.10 over baseline despite
imbalance. Finally, we show that overlapping word removal based on
pre-processing, though simple, improves F1-Macro by 0.28. In due process, we
present detailed studies depicting various behaviors of each of these
strategies and summarize key findings from our empirical results for those
interested in getting the most out of M-BERT for hope speech detection under
real-world conditions of data imbalance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GERE: Generative Evidence Retrieval for Fact Verification. (arXiv:2204.05511v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05511">
<div class="article-summary-box-inner">
<span><p>Fact verification (FV) is a challenging task which aims to verify a claim
using multiple evidential sentences from trustworthy corpora, e.g., Wikipedia.
Most existing approaches follow a three-step pipeline framework, including
document retrieval, sentence retrieval and claim verification. High-quality
evidences provided by the first two steps are the foundation of the effective
reasoning in the last step. Despite being important, high-quality evidences are
rarely studied by existing works for FV, which often adopt the off-the-shelf
models to retrieve relevant documents and sentences in an
"index-retrieve-then-rank" fashion. This classical approach has clear drawbacks
as follows: i) a large document index as well as a complicated search process
is required, leading to considerable memory and computational overhead; ii)
independent scoring paradigms fail to capture the interactions among documents
and sentences in ranking; iii) a fixed number of sentences are selected to form
the final evidence set. In this work, we propose \textit{GERE}, the first
system that retrieves evidences in a generative fashion, i.e., generating the
document titles as well as evidence sentence identifiers. This enables us to
mitigate the aforementioned technical issues since: i) the memory and
computational cost is greatly reduced because the document index is eliminated
and the heavy ranking process is replaced by a light generative process; ii)
the dependency between documents and that between sentences could be captured
via sequential generation process; iii) the generative formulation allows us to
dynamically select a precise set of relevant evidences for each claim. The
experimental results on the FEVER dataset show that GERE achieves significant
improvements over the state-of-the-art baselines, with both time-efficiency and
memory-efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study of Faithfulness Metrics for Model Interpretability Methods. (arXiv:2204.05514v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05514">
<div class="article-summary-box-inner">
<span><p>Interpretation methods to reveal the internal reasoning processes behind
machine learning models have attracted increasing attention in recent years. To
quantify the extent to which the identified interpretations truly reflect the
intrinsic decision-making mechanisms, various faithfulness evaluation metrics
have been proposed. However, we find that different faithfulness metrics show
conflicting preferences when comparing different interpretations. Motivated by
this observation, we aim to conduct a comprehensive and comparative study of
the widely adopted faithfulness metrics. In particular, we introduce two
assessment dimensions, namely diagnosticity and time complexity. Diagnosticity
refers to the degree to which the faithfulness metric favours relatively
faithful interpretations over randomly generated ones, and time complexity is
measured by the average number of model forward passes. According to the
experimental results, we find that sufficiency and comprehensiveness metrics
have higher diagnosticity and lower time complexity than the other faithfulness
metric
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLMLF:A Contrastive Learning and Multi-Layer Fusion Method for Multimodal Sentiment Detection. (arXiv:2204.05515v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05515">
<div class="article-summary-box-inner">
<span><p>Compared with unimodal data, multimodal data can provide more features to
help the model analyze the sentiment of data. Previous research works rarely
consider token-level feature fusion, and few works explore learning the common
features related to sentiment in multimodal data to help the model fuse
multimodal features. In this paper, we propose a Contrastive Learning and
Multi-Layer Fusion (CLMLF) method for multimodal sentiment detection.
Specifically, we first encode text and image to obtain hidden representations,
and then use a multi-layer fusion module to align and fuse the token-level
features of text and image. In addition to the sentiment analysis task, we also
designed two contrastive learning tasks, label based contrastive learning and
data based contrastive learning tasks, which will help the model learn common
features related to sentiment in multimodal data. Extensive experiments
conducted on three publicly available multimodal datasets demonstrate the
effectiveness of our approach for multimodal sentiment detection compared with
existing methods. The codes are available for use at
https://github.com/Link-Li/CLMLF
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trigger-GNN: A Trigger-Based Graph Neural Network for Nested Named Entity Recognition. (arXiv:2204.05518v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05518">
<div class="article-summary-box-inner">
<span><p>Nested named entity recognition (NER) aims to identify the entity boundaries
and recognize categories of the named entities in a complex hierarchical
sentence. Some works have been done using character-level, word-level, or
lexicon-level based models. However, such researches ignore the role of the
complementary annotations. In this paper, we propose a trigger-based graph
neural network (Trigger-GNN) to leverage the nested NER. It obtains the
complementary annotation embeddings through entity trigger encoding and
semantic matching, and tackle nested entity utilizing an efficient graph
message passing architecture, aggregation-update mode. We posit that using
entity triggers as external annotations can add in complementary supervision
signals on the whole sentences. It helps the model to learn and generalize more
efficiently and cost-effectively. Experiments show that the Trigger-GNN
consistently outperforms the baselines on four public NER datasets, and it can
effectively alleviate the nested NER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Annotation of Therapeutic Working Alliance in Psychotherapy. (arXiv:2204.05522v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05522">
<div class="article-summary-box-inner">
<span><p>The therapeutic working alliance is an important predictor of the outcome of
the psychotherapy treatment. In practice, the working alliance is estimated
from a set of scoring questionnaires in an inventory that both the patient and
the therapists fill out. In this work, we propose an analytical framework of
directly inferring the therapeutic working alliance from the natural language
within the psychotherapy sessions in a turn-level resolution with deep
embeddings such as the Doc2Vec and SentenceBERT models. The transcript of each
psychotherapy session can be transcribed and generated in real-time from the
session speech recordings, and these embedded dialogues are compared with the
distributed representations of the statements in the working alliance
inventory. We demonstrate, in a real-world dataset with over 950 sessions of
psychotherapy treatments in anxiety, depression, schizophrenia and suicidal
patients, the effectiveness of this method in mapping out trajectories of
patient-therapist alignment and the interpretability that can offer insights in
clinical psychiatry. We believe such a framework can be provide timely feedback
to the therapist regarding the quality of the conversation in interview
sessions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How does fake news use a thumbnail? CLIP-based Multimodal Detection on the Unrepresentative News Image. (arXiv:2204.05533v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05533">
<div class="article-summary-box-inner">
<span><p>This study investigates how fake news uses a thumbnail for a news article
with a focus on whether a news article's thumbnail represents the news content
correctly. A news article shared with an irrelevant thumbnail can mislead
readers into having a wrong impression of the issue, especially in social media
environments where users are less likely to click the link and consume the
entire content. We propose to capture the degree of semantic incongruity in the
multimodal relation by using the pretrained CLIP representation. From a
source-level analysis, we found that fake news employs a more incongruous image
to the main content than general news. Going further, we attempted to detect
news articles with image-text incongruity. Evaluation experiments suggest that
CLIP-based methods can successfully detect news articles in which the thumbnail
is semantically irrelevant to news text. This study contributes to the research
by providing a novel view on tackling online fake news and misinformation. Code
and datasets are available at
https://github.com/ssu-humane/fake-news-thumbnail.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not always about you: Prioritizing community needs when developing endangered language technology. (arXiv:2204.05541v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05541">
<div class="article-summary-box-inner">
<span><p>Languages are classified as low-resource when they lack the quantity of data
necessary for training statistical and machine learning tools and models.
Causes of resource scarcity vary but can include poor access to technology for
developing these resources, a relatively small population of speakers, or a
lack of urgency for collecting such resources in bilingual populations where
the second language is high-resource. As a result, the languages described as
low-resource in the literature are as different as Finnish on the one hand,
with millions of speakers using it in every imaginable domain, and Seneca, with
only a small-handful of fluent speakers using the language primarily in a
restricted domain. While issues stemming from the lack of resources necessary
to train models unite this disparate group of languages, many other issues cut
across the divide between widely-spoken low resource languages and endangered
languages. In this position paper, we discuss the unique technological,
cultural, practical, and ethical challenges that researchers and indigenous
speech community members face when working together to develop language
technology to support endangered language documentation and revitalization. We
report the perspectives of language teachers, Master Speakers and elders from
indigenous communities, as well as the point of view of academics. We describe
an ongoing fruitful collaboration and make recommendations for future
partnerships between academic researchers and language community stakeholders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delving Deep into Regularity: A Simple but Effective Method for Chinese Named Entity Recognition. (arXiv:2204.05544v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05544">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed the improving performance of Chinese Named Entity
Recognition (NER) from proposing new frameworks or incorporating word lexicons.
However, the inner composition of entity mentions in character-level Chinese
NER has been rarely studied. Actually, most mentions of regular types have
strong name regularity. For example, entities end with indicator words such as
"company" or "bank" usually belong to organization. In this paper, we propose a
simple but effective method for investigating the regularity of entity spans in
Chinese NER, dubbed as Regularity-Inspired reCOgnition Network (RICON).
Specifically, the proposed model consists of two branches: a regularity-aware
module and a regularityagnostic module. The regularity-aware module captures
the internal regularity of each span for better entity type prediction, while
the regularity-agnostic module is employed to locate the boundary of entities
and relieve the excessive attention to span regularity. An orthogonality space
is further constructed to encourage two modules to extract different aspects of
regularity features. To verify the effectiveness of our method, we conduct
extensive experiments on three benchmark datasets and a practical medical
dataset. The experimental results show that our RICON significantly outperforms
previous state-of-the-art methods, including various lexicon-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving Price Per Unit Problem Around the World: Formulating Fact Extraction as Question Answering. (arXiv:2204.05555v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05555">
<div class="article-summary-box-inner">
<span><p>Price Per Unit (PPU) is an essential information for consumers shopping on
e-commerce websites when comparing products. Finding total quantity in a
product is required for computing PPU, which is not always provided by the
sellers. To predict total quantity, all relevant quantities given in a product
attributes such as title, description and image need to be inferred correctly.
We formulate this problem as a question-answering (QA) task rather than named
entity recognition (NER) task for fact extraction. In our QA approach, we first
predict the unit of measure (UoM) type (e.g., volume, weight or count), that
formulates the desired question (e.g., "What is the total volume?") and then
use this question to find all the relevant answers. Our model architecture
consists of two subnetworks for the two subtasks: a classifier to predict UoM
type (or the question) and an extractor to extract the relevant quantities. We
use a deep character-level CNN architecture for both subtasks, which enables
(1) easy expansion to new stores with similar alphabets, (2) multi-span
answering due to its span-image architecture and (3) easy deployment by keeping
model-inference latency low. Our QA approach outperforms rule-based methods by
34.4% in precision and also BERT-based fact extraction approach in all stores
globally, with largest precision lift of 10.6% in the US store.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stylized Knowledge-Grounded Dialogue Generation via Disentangled Template Rewriting. (arXiv:2204.05610v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05610">
<div class="article-summary-box-inner">
<span><p>Current Knowledge-Grounded Dialogue Generation (KDG) models specialize in
producing rational and factual responses. However, to establish long-term
relationships with users, the KDG model needs the capability to generate
responses in a desired style or attribute. Thus, we study a new problem:
Stylized Knowledge-Grounded Dialogue Generation (SKDG). It presents two
challenges: (1) How to train a SKDG model where no &lt;context, knowledge,
stylized response&gt; triples are available. (2) How to cohere with context and
preserve the knowledge when generating a stylized response. In this paper, we
propose a novel disentangled template rewriting (DTR) method which generates
responses via combing disentangled style templates (from monolingual stylized
corpus) and content templates (from KDG corpus). The entire framework is
end-to-end differentiable and learned without supervision. Extensive
experiments on two benchmarks indicate that DTR achieves a significant
improvement on all evaluation metrics compared with previous state-of-the-art
stylized dialogue generation methods. Besides, DTR achieves comparable
performance with the state-of-the-art KDG methods in standard KDG evaluation
setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASR in German: A Detailed Error Analysis. (arXiv:2204.05617v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05617">
<div class="article-summary-box-inner">
<span><p>The amount of freely available systems for automatic speech recognition (ASR)
based on neural networks is growing steadily, with equally increasingly
reliable predictions. However, the evaluation of trained models is typically
exclusively based on statistical metrics such as WER or CER, which do not
provide any insight into the nature or impact of the errors produced when
predicting transcripts from speech input. This work presents a selection of ASR
model architectures that are pretrained on the German language and evaluates
them on a benchmark of diverse test datasets. It identifies cross-architectural
prediction errors, classifies those into categories and traces the sources of
errors per category back into training data as well as other sources. Finally,
it discusses solutions in order to create qualitatively better training
datasets and more robust ASR systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">X-DETR: A Versatile Architecture for Instance-wise Vision-Language Tasks. (arXiv:2204.05626v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05626">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the challenging instance-wise vision-language tasks,
where the free-form language is required to align with the objects instead of
the whole image. To address these tasks, we propose X-DETR, whose architecture
has three major components: an object detector, a language encoder, and
vision-language alignment. The vision and language streams are independent
until the end and they are aligned using an efficient dot-product operation.
The whole network is trained end-to-end, such that the detector is optimized
for the vision-language tasks instead of an off-the-shelf component. To
overcome the limited size of paired object-language annotations, we leverage
other weak types of supervision to expand the knowledge coverage. This simple
yet effective architecture of X-DETR shows good accuracy and fast speeds for
multiple instance-wise vision-language tasks, e.g., 16.4 AP on LVIS detection
of 1.2K categories at ~20 frames per second without using any LVIS annotation
during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Idiomify -- Building a Collocation-supplemented Reverse Dictionary of English Idioms with Word2Vec for non-native learners. (arXiv:2204.05634v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05634">
<div class="article-summary-box-inner">
<span><p>The aim of idiomify is to build a collocation-supplemented reverse dictionary
of idioms for the non-native learners of English. We aim to do so because the
reverse dictionary could help the non-natives explore idioms on demand, and the
collocations could also guide them on using idioms more adequately. The
cornerstone of the project is a reliable way of mining idioms from corpora,
which is however a challenge because idioms extensively vary in forms. We
tackle this by automatically deriving matching rules from their base forms. We
use Point-wise Mutual Inclusion (PMI), Term Frequency - Inverse Document
Frequency (TF-IDF) to model collocations, since both of them are popular metric
for pairwise significance. We also try Term Frequency (TF) as the baseline
model. As for implementing the reverse-dictionary, three approaches could be
taken: inverted index, graphs and distributional semantics. We choose to take
the last approach and implement the reverse dictionary with Word2Vec, because
it is the most flexible approach of all and Word2Vec is a simple yet strong
baseline. Evaluating the methods has revealed rooms for improvement. We learn
that we can better identify idioms with the help of slop, wildcard and
reordering techniques. We also learn that we can get the best of both PMI and
TF-IDF if we use machine learning to find the sweet spot. Lastly, We learn that
Idiomify could be further improved with a mixture of inverted index and
distributional semantics approach. The limits aside, the proposed methods are
feasible, and their benefits to the non-natives are apparent, which therefore
can be used to aid the non-natives in acquiring English idioms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Creativity in translation: machine translation as a constraint for literary texts. (arXiv:2204.05655v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05655">
<div class="article-summary-box-inner">
<span><p>This article presents the results of a study involving the translation of a
short story by Kurt Vonnegut from English to Catalan and Dutch using three
modalities: machine-translation (MT), post-editing (PE) and translation without
aid (HT). Our aim is to explore creativity, understood to involve novelty and
acceptability, from a quantitative perspective. The results show that HT has
the highest creativity score, followed by PE, and lastly, MT, and this is
unanimous from all reviewers. A neural MT system trained on literary data does
not currently have the necessary capabilities for a creative translation; it
renders literal solutions to translation problems. More importantly, using MT
to post-edit raw output constrains the creativity of translators, resulting in
a poorer translation often not fit for publication, according to experts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks. (arXiv:2204.05660v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05660">
<div class="article-summary-box-inner">
<span><p>Given the ubiquitous nature of numbers in text, reasoning with numbers to
perform simple calculations is an important skill of AI systems. While many
datasets and models have been developed to this end, state-of-the-art AI
systems are brittle; failing to perform the underlying mathematical reasoning
when they appear in a slightly different scenario. Drawing inspiration from
GLUE that was proposed in the context of natural language understanding, we
propose NumGLUE, a multi-task benchmark that evaluates the performance of AI
systems on eight different tasks, that at their core require simple arithmetic
understanding. We show that this benchmark is far from being solved with neural
models including state-of-the-art large-scale language models performing
significantly worse than humans (lower by 46.4%). Further, NumGLUE promotes
sharing knowledge across tasks, especially those with limited training data as
evidenced by the superior performance (average gain of 3.4% on each task) when
a model is jointly trained on all the tasks as opposed to task-specific
modeling. Finally, we hope that NumGLUE will encourage systems that perform
robust and general arithmetic reasoning within language, a first step towards
being able to perform more complex mathematical reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What do Toothbrushes do in the Kitchen? How Transformers Think our World is Structured. (arXiv:2204.05673v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05673">
<div class="article-summary-box-inner">
<span><p>Transformer-based models are now predominant in NLP. They outperform
approaches based on static models in many respects. This success has in turn
prompted research that reveals a number of biases in the language models
generated by transformers. In this paper we utilize this research on biases to
investigate to what extent transformer-based language models allow for
extracting knowledge about object relations (X occurs in Y; X consists of Z;
action A involves using X). To this end, we compare contextualized models with
their static counterparts. We make this comparison dependent on the application
of a number of similarity measures and classifiers. Our results are threefold:
Firstly, we show that the models combined with the different similarity
measures differ greatly in terms of the amount of knowledge they allow for
extracting. Secondly, our results suggest that similarity measures perform much
worse than classifier-based approaches. Thirdly, we show that, surprisingly,
static models perform almost as well as contextualized models -- in some cases
even better.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Generative Approach for Financial Causality Extraction. (arXiv:2204.05674v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05674">
<div class="article-summary-box-inner">
<span><p>Causality represents the foremost relation between events in financial
documents such as financial news articles, financial reports. Each financial
causality contains a cause span and an effect span. Previous works proposed
sequence labeling approaches to solve this task. But sequence labeling models
find it difficult to extract multiple causalities and overlapping causalities
from the text segments. In this paper, we explore a generative approach for
causality extraction using the encoder-decoder framework and pointer networks.
We use a causality dataset from the financial domain, \textit{FinCausal}, for
our experiments and our proposed framework achieves very competitive
performance on this dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Losses for One-Class Textual Anomaly Detection. (arXiv:2204.05695v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05695">
<div class="article-summary-box-inner">
<span><p>Current deep learning methods for anomaly detection in text rely on
supervisory signals in inliers that may be unobtainable or bespoke
architectures that are difficult to tune. We study a simpler alternative:
fine-tuning Transformers on the inlier data with self-supervised objectives and
using the losses as an anomaly score. Overall, the self-supervision approach
outperforms other methods under various anomaly detection scenarios, improving
the AUROC score on semantic anomalies by 11.6% and on syntactic anomalies by
22.8% on average. Additionally, the optimal objective and resultant learnt
representation depend on the type of downstream anomaly. The separability of
anomalies and inliers signals that a representation is more effective for
detecting semantic anomalies, whilst the presence of narrow feature directions
signals a representation that is effective for detecting syntactic anomalies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Not Fire the Linguist: Grammatical Profiles Help Language Models Detect Semantic Change. (arXiv:2204.05717v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05717">
<div class="article-summary-box-inner">
<span><p>Morphological and syntactic changes in word usage (as captured, e.g., by
grammatical profiles) have been shown to be good predictors of a word's meaning
change. In this work, we explore whether large pre-trained contextualised
language models, a common tool for lexical semantic change detection, are
sensitive to such morphosyntactic changes. To this end, we first compare the
performance of grammatical profiles against that of a multilingual neural
language model (XLM-R) on 10 datasets, covering 7 languages, and then combine
the two approaches in ensembles to assess their complementarity. Our results
show that ensembling grammatical profiles with XLM-R improves semantic change
detection performance for most datasets and languages. This indicates that
language models do not fully cover the fine-grained morphological and syntactic
signals that are explicitly represented in grammatical profiles.
</p>
<p>An interesting exception are the test sets where the time spans under
analysis are much longer than the time gap between them (for example,
century-long spans with a one-year gap between them). Morphosyntactic change is
slow so grammatical profiles do not detect in such cases. In contrast, language
models, thanks to their access to lexical information, are able to detect fast
topical changes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decomposed Meta-Learning for Few-Shot Named Entity Recognition. (arXiv:2204.05751v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05751">
<div class="article-summary-box-inner">
<span><p>Few-shot named entity recognition (NER) systems aim at recognizing
novel-class named entities based on only a few labeled examples. In this paper,
we present a decomposed meta-learning approach which addresses the problem of
few-shot NER by sequentially tackling few-shot span detection and few-shot
entity typing using meta-learning. In particular, we take the few-shot span
detection as a sequence labeling problem and train the span detector by
introducing the model-agnostic meta-learning (MAML) algorithm to find a good
model parameter initialization that could fast adapt to new entity classes. For
few-shot entity typing, we propose MAML-ProtoNet, i.e., MAML-enhanced
prototypical networks to find a good embedding space that can better
distinguish text span representations from different entity classes. Extensive
experiments on various benchmarks show that our approach achieves superior
performance over prior methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Express in Knowledge-Grounded Conversation. (arXiv:2204.05805v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05805">
<div class="article-summary-box-inner">
<span><p>Grounding dialogue generation by extra knowledge has shown great potentials
towards building a system capable of replying with knowledgeable and engaging
responses. Existing studies focus on how to synthesize a response with proper
knowledge, yet neglect that the same knowledge could be expressed differently
by speakers even under the same context. In this work, we mainly consider two
aspects of knowledge expression, namely the structure of the response and style
of the content in each part. We therefore introduce two sequential latent
variables to represent the structure and the content style respectively. We
propose a segmentation-based generation model and optimize the model by a
variational approach to discover the underlying pattern of knowledge expression
in a response. Evaluation results on two benchmarks indicate that our model can
learn the structure style defined by a few examples and generate responses in
desired content style.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MuCoT: Multilingual Contrastive Training for Question-Answering in Low-resource Languages. (arXiv:2204.05814v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05814">
<div class="article-summary-box-inner">
<span><p>Accuracy of English-language Question Answering (QA) systems has improved
significantly in recent years with the advent of Transformer-based models
(e.g., BERT). These models are pre-trained in a self-supervised fashion with a
large English text corpus and further fine-tuned with a massive English QA
dataset (e.g., SQuAD). However, QA datasets on such a scale are not available
for most of the other languages. Multi-lingual BERT-based models (mBERT) are
often used to transfer knowledge from high-resource languages to low-resource
languages. Since these models are pre-trained with huge text corpora containing
multiple languages, they typically learn language-agnostic embeddings for
tokens from different languages. However, directly training an mBERT-based QA
system for low-resource languages is challenging due to the paucity of training
data. In this work, we augment the QA samples of the target language using
translation and transliteration into other languages and use the augmented data
to fine-tune an mBERT-based QA model, which is already pre-trained in English.
Experiments on the Google ChAII dataset show that fine-tuning the mBERT model
with translations from the same language family boosts the question-answering
performance, whereas the performance degrades in the case of cross-language
families. We further show that introducing a contrastive loss between the
translated question-context feature pairs during the fine-tuning process,
prevents such degradation with cross-lingual family translations and leads to
marginal improvement. The code for this work is available at
https://github.com/gokulkarthik/mucot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Few-shot Entity Recognition in Document Images: A Label-aware Sequence-to-Sequence Framework. (arXiv:2204.05819v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05819">
<div class="article-summary-box-inner">
<span><p>Entity recognition is a fundamental task in understanding document images.
Traditional sequence labeling frameworks treat the entity types as class IDs
and rely on extensive data and high-quality annotations to learn semantics
which are typically expensive in practice. In this paper, we aim to build an
entity recognition model requiring only a few shots of annotated document
images. To overcome the data limitation, we propose to leverage the label
surface names to better inform the model of the target entity type semantics
and also embed the labels into the spatial embedding space to capture the
spatial correspondence between regions and labels. Specifically, we go beyond
sequence labeling and develop a novel label-aware seq2seq framework, LASER. The
proposed model follows a new labeling scheme that generates the label surface
names word-by-word explicitly after generating the entities. During training,
LASER refines the label semantics by updating the label surface name
representations and also strengthens the label-region correlation. In this way,
LASER recognizes the entities from document images through both semantic and
layout correspondence. Extensive experiments on two benchmark datasets
demonstrate the superiority of LASER under the few-shot setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?. (arXiv:2204.05832v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05832">
<div class="article-summary-box-inner">
<span><p>Large pretrained Transformer language models have been shown to exhibit
zero-shot generalization, i.e. they can perform a wide variety of tasks that
they were not explicitly trained on. However, the architectures and pretraining
objectives used across state-of-the-art models differ significantly, and there
has been limited systematic comparison of these factors. In this work, we
present a large-scale evaluation of modeling choices and their impact on
zero-shot generalization. In particular, we focus on text-to-text models and
experiment with three model architectures (causal/non-causal decoder-only and
encoder-decoder), trained with two different pretraining objectives
(autoregressive and masked language modeling), and evaluated with and without
multitask prompted finetuning. We train models with over 5 billion parameters
for more than 170 billion tokens, thereby increasing the likelihood that our
conclusions will transfer to even larger scales. Our experiments show that
causal decoder-only models trained on an autoregressive language modeling
objective exhibit the strongest zero-shot generalization after purely
unsupervised pretraining. However, models with non-causal visibility on their
input trained with a masked language modeling objective followed by multitask
finetuning perform the best among our experiments. We therefore consider the
adaptation of pretrained models across architectures and objectives. We find
that pretrained non-causal decoder models can be adapted into performant
generative causal decoder models, using autoregressive language modeling as a
downstream task. Furthermore, we find that pretrained causal decoder models can
be efficiently adapted into non-causal decoder models, ultimately achieving
competitive performance after multitask finetuning. Code and checkpoints are
available at https://github.com/bigscience-workshop/architecture-objective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Project Dialogism Novel Corpus: A Dataset for Quotation Attribution in Literary Texts. (arXiv:2204.05836v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05836">
<div class="article-summary-box-inner">
<span><p>We present the Project Dialogism Novel Corpus, or PDNC, an annotated dataset
of quotations for English literary texts. PDNC contains annotations for 35,978
quotations across 22 full-length novels, and is by an order of magnitude the
largest corpus of its kind. Each quotation is annotated for the speaker,
addressees, type of quotation, referring expression, and character mentions
within the quotation text. The annotated attributes allow for a comprehensive
evaluation of models of quotation attribution and coreference for literary
texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. (arXiv:2204.05862v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05862">
<div class="article-summary-box-inner">
<span><p>We apply preference modeling and reinforcement learning from human feedback
(RLHF) to finetune language models to act as helpful and harmless assistants.
We find this alignment training improves performance on almost all NLP
evaluations, and is fully compatible with training for specialized skills such
as python coding and summarization. We explore an iterated online mode of
training, where preference models and RL policies are updated on a weekly
cadence with fresh human feedback data, efficiently improving our datasets and
models. Finally, we investigate the robustness of RLHF training, and identify a
roughly linear relation between the RL reward and the square root of the KL
divergence between the policy and its initialization. Alongside our main
results, we perform peripheral analyses on calibration, competing objectives,
and the use of OOD detection, compare our models with human writers, and
provide samples from our models using prompts appearing in recent related work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Full Length Wikipedia Biographies: The Impact of Gender Bias on the Retrieval-Based Generation of Women Biographies. (arXiv:2204.05879v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05879">
<div class="article-summary-box-inner">
<span><p>Generating factual, long-form text such as Wikipedia articles raises three
key challenges: how to gather relevant evidence, how to structure information
into well-formed text, and how to ensure that the generated text is factually
correct. We address these by developing a model for English text that uses a
retrieval mechanism to identify relevant supporting information on the web and
a cache-based pre-trained encoder-decoder to generate long-form biographies
section by section, including citation information. To assess the impact of
available web evidence on the output text, we compare the performance of our
approach when generating biographies about women (for which less information is
available on the web) vs. biographies generally. To this end, we curate a
dataset of 1,500 biographies about women. We analyze our generated text to
understand how differences in available web evidence data affect generation. We
evaluate the factuality, fluency, and quality of the generated texts using
automatic metrics and human evaluation. We hope that these techniques can be
used as a starting point for human writers, to aid in reducing the complexity
inherent in the creation of long-form, factual text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XQA-DST: Multi-Domain and Multi-Lingual Dialogue State Tracking. (arXiv:2204.05895v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05895">
<div class="article-summary-box-inner">
<span><p>In a task-oriented dialogue system, Dialogue State Tracking (DST) keeps track
of all important information by filling slots with values given through the
conversation. Existing methods generally rely on a predefined set of values and
struggle to generalise to previously unseen slots in new domains. In this
paper, we propose a multi-domain and multi-lingual dialogue state tracker in a
neural reading comprehension approach. Our approach fills the slot values using
span prediction, where the values are extracted from the dialogue itself. With
a novel training strategy and an independent domain classifier, empirical
results demonstrate that our model is a domain-scalable and open-vocabulary
model that achieves 53.2% Joint Goal Accuracy (JGA) on MultiWOZ 2.1. We show
its competitive transferability by zero-shot domain-adaptation experiments on
MultiWOZ 2.1 with an average JGA of 31.6% for five domains. In addition, it
achieves cross-lingual transfer with state-of-the-art zero-shot results, 64.9%
JGA from English to German and 68.6% JGA from English to Italian on WOZ 2.0.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Dialogue Policy Transformer for Continual Reinforcement Learning. (arXiv:2204.05928v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05928">
<div class="article-summary-box-inner">
<span><p>Continual learning is one of the key components of human learning and a
necessary requirement of artificial intelligence. As dialogue can potentially
span infinitely many topics and tasks, a task-oriented dialogue system must
have the capability to continually learn, dynamically adapting to new
challenges while preserving the knowledge it already acquired. Despite the
importance, continual reinforcement learning of the dialogue policy has
remained largely unaddressed. The lack of a framework with training protocols,
baseline models and suitable metrics, has so far hindered research in this
direction. In this work we fill precisely this gap, enabling research in
dialogue policy optimisation to go from static to dynamic learning. We provide
a continual learning algorithm, baseline architectures and metrics for
assessing continual learning models. Moreover, we propose the dynamic dialogue
policy transformer (DDPT), a novel dynamic architecture that can integrate new
knowledge seamlessly, is capable of handling large state spaces and obtains
significant zero-shot performance when being exposed to unseen domains, without
any growth in network parameter size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mining Logical Event Schemas From Pre-Trained Language Models. (arXiv:2204.05939v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05939">
<div class="article-summary-box-inner">
<span><p>We present NESL (the Neuro-Episodic Schema Learner), an event schema learning
system that combines large language models, FrameNet parsing, a powerful
logical representation of language, and a set of simple behavioral schemas
meant to bootstrap the learning process. In lieu of a pre-made corpus of
stories, our dataset is a continuous feed of "situation samples" from a
pre-trained language model, which are then parsed into FrameNet frames, mapped
into simple behavioral schemas, and combined and generalized into complex,
hierarchical schemas for a variety of everyday scenarios. We show that careful
sampling from the language model can help emphasize stereotypical properties of
situations and de-emphasize irrelevant details, and that the resulting schemas
specify situations more comprehensively than those learned by other systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explore More Guidance: A Task-aware Instruction Network for Sign Language Translation Enhanced with Data Augmentation. (arXiv:2204.05953v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05953">
<div class="article-summary-box-inner">
<span><p>Sign language recognition and translation first uses a recognition module to
generate glosses from sign language videos and then employs a translation
module to translate glosses into spoken sentences. Most existing works focus on
the recognition step, while paying less attention to sign language translation.
In this work, we propose a task-aware instruction network, namely TIN-SLT, for
sign language translation, by introducing the instruction module and the
learning-based feature fuse strategy into a Transformer network. In this way,
the pre-trained model's language ability can be well explored and utilized to
further boost the translation performance. Moreover, by exploring the
representation space of sign language glosses and target spoken language, we
propose a multi-level data augmentation scheme to adjust the data distribution
of the training set. We conduct extensive experiments on two challenging
benchmark datasets, PHOENIX-2014-T and ASLG-PC12, on which our method
outperforms former best solutions by 1.65 and 1.42 in terms of BLEU-4. Our code
is published at https://github.com/yongcaoplus/TIN-SLT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantified Reproducibility Assessment of NLP Results. (arXiv:2204.05961v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05961">
<div class="article-summary-box-inner">
<span><p>This paper describes and tests a method for carrying out quantified
reproducibility assessment (QRA) that is based on concepts and definitions from
metrology. QRA produces a single score estimating the degree of reproducibility
of a given system and evaluation measure, on the basis of the scores from, and
differences between, different reproductions. We test QRA on 18 system and
evaluation measure combinations (involving diverse NLP tasks and types of
evaluation), for each of which we have the original results and one to seven
reproduction results. The proposed QRA method produces
degree-of-reproducibility scores that are comparable across multiple
reproductions not only of the same, but of different original studies. We find
that the proposed method facilitates insights into causes of variation between
reproductions, and allows conclusions to be drawn about what changes to system
and/or evaluation design might lead to improved reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering material information using hierarchical Reformer model on financial regulatory filings. (arXiv:2204.05979v1 [q-fin.ST])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05979">
<div class="article-summary-box-inner">
<span><p>Most applications of machine learning for finance are related to forecasting
tasks for investment decisions. Instead, we aim to promote a better
understanding of financial markets with machine learning techniques. Leveraging
the tremendous progress in deep learning models for natural language
processing, we construct a hierarchical Reformer ([15]) model capable of
processing a large document level dataset, SEDAR, from canadian financial
regulatory filings. Using this model, we show that it is possible to predict
trade volume changes using regulatory filings. We adapt the pretraining task of
HiBERT ([36]) to obtain good sentence level representations using a large
unlabelled document dataset. Finetuning the model to successfully predict trade
volume changes indicates that the model captures a view from financial markets
and processing regulatory filings is beneficial. Analyzing the attention
patterns of our model reveals that it is able to detect some indications of
material information without explicit training, which is highly relevant for
investors and also for the market surveillance mandate of financial regulators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection, Disambiguation, Re-ranking: Autoregressive Entity Linking as a Multi-Task Problem. (arXiv:2204.05990v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05990">
<div class="article-summary-box-inner">
<span><p>We propose an autoregressive entity linking model, that is trained with two
auxiliary tasks, and learns to re-rank generated samples at inference time. Our
proposed novelties address two weaknesses in the literature. First, a recent
method proposes to learn mention detection and then entity candidate selection,
but relies on predefined sets of candidates. We use encoder-decoder
autoregressive entity linking in order to bypass this need, and propose to
train mention detection as an auxiliary task instead. Second, previous work
suggests that re-ranking could help correct prediction errors. We add a new,
auxiliary task, match prediction, to learn re-ranking. Without the use of a
knowledge base or candidate sets, our model sets a new state of the art in two
benchmark datasets of entity linking: COMETA in the biomedical domain, and
AIDA-CoNLL in the news domain. We show through ablation studies that each of
the two auxiliary tasks increases performance, and that re-ranking is an
important factor to the increase. Finally, our low-resource experimental
results suggest that performance on the main task benefits from the knowledge
learned by the auxiliary tasks, and not just from the additional training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension. (arXiv:2204.05991v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05991">
<div class="article-summary-box-inner">
<span><p>Training a referring expression comprehension (ReC) model for a new visual
domain requires collecting referring expressions, and potentially corresponding
bounding boxes, for images in the domain. While large-scale pre-trained models
are useful for image classification across domains, it remains unclear if they
can be applied in a zero-shot manner to more complex tasks like ReC. We present
ReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a
state-of-the-art large-scale model, for ReC. Motivated by the close connection
between ReC and CLIP's contrastive pre-training objective, the first component
of ReCLIP is a region-scoring method that isolates object proposals via
cropping and blurring, and passes them to CLIP. However, through controlled
experiments on a synthetic dataset, we find that CLIP is largely incapable of
performing spatial reasoning off-the-shelf. Thus, the second component of
ReCLIP is a spatial relation resolver that handles several types of spatial
relations. We reduce the gap between zero-shot baselines from prior work and
supervised models by as much as 29% on RefCOCOg, and on RefGTA (video game
imagery), ReCLIP's relative improvement over supervised ReC models trained on
real images is 8%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Entity Disambiguation with BERT. (arXiv:1909.00426v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.00426">
<div class="article-summary-box-inner">
<span><p>We propose a global entity disambiguation (ED) model based on BERT. To
capture global contextual information for ED, our model treats not only words
but also entities as input tokens, and solves the task by sequentially
resolving mentions to their referent entities and using resolved entities as
inputs at each step. We train the model using a large entity-annotated corpus
obtained from Wikipedia. We achieve new state-of-the-art results on five
standard ED datasets: AIDA-CoNLL, MSNBC, AQUAINT, ACE2004, and WNED-WIKI. The
source code and model checkpoint are available at
https://github.com/studio-ousia/luke.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Represent Programs with Heterogeneous Graphs. (arXiv:2012.04188v3 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04188">
<div class="article-summary-box-inner">
<span><p>Program source code contains complex structure information, which can be
represented in structured data forms like trees or graphs. To acquire the
structural information in source code, most existing researches use abstract
syntax trees (AST). A group of works add additional edges to ASTs to convert
source code into graphs and use graph neural networks to learn representations
for program graphs. Although these works provide additional control or data
flow information to ASTs for downstream tasks, they neglect an important aspect
of structure information in AST itself: the different types of nodes and edges.
In ASTs, different nodes contain different kinds of information like variables
or control flow, and the relation between a node and all its children can also
be different.
</p>
<p>To address the information of node and edge types, we bring the idea of
heterogeneous graphs to learning on source code and present a new formula of
building heterogeneous program graphs from ASTs with additional type
information for nodes and edges. We use the ASDL grammar of programming
language to define the node and edge types of program graphs. Then we use
heterogeneous graph neural networks to learn on these graphs. We evaluate our
approach on two tasks: code comment generation and method naming. Both tasks
require reasoning on the semantics of complete code snippets. Experiment
results show that our approach outperforms baseline models, including
homogeneous graph-based models, showing that leveraging the type information of
nodes and edges in program graphs can help in learning program semantics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection. (arXiv:2012.10289v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.10289">
<div class="article-summary-box-inner">
<span><p>Hate speech is a challenging issue plaguing the online social media. While
better models for hate speech detection are continuously being developed, there
is little research on the bias and interpretability aspects of hate speech. In
this paper, we introduce HateXplain, the first benchmark hate speech dataset
covering multiple aspects of the issue. Each post in our dataset is annotated
from three different perspectives: the basic, commonly used 3-class
classification (i.e., hate, offensive or normal), the target community (i.e.,
the community that has been the victim of hate speech/offensive speech in the
post), and the rationales, i.e., the portions of the post on which their
labelling decision (as hate, offensive or normal) is based. We utilize existing
state-of-the-art models and observe that even models that perform very well in
classification do not score high on explainability metrics like model
plausibility and faithfulness. We also observe that models, which utilize the
human rationales for training, perform better in reducing unintended bias
towards target communities. We have made our code and dataset public at
https://github.com/punyajoy/HateXplain
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Your fairness may vary: Group fairness of pretrained language models in toxic text classification. (arXiv:2108.01250v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01250">
<div class="article-summary-box-inner">
<span><p>The popularity of pretrained language models in natural language processing
systems calls for a careful evaluation of such models in down-stream tasks,
which have a higher potential for societal impact. The evaluation of such
systems usually focuses on \textit{accuracy measures}. Our findings in this
paper call for attention to be paid to \textit{fairness measures} as well.
Through the analysis of more than a dozen pretrained language models of varying
sizes on two toxic text classification tasks (English), we demonstrate that
focusing on accuracy measures alone can lead to models with wide variation in
fairness characteristics. Specifically, we observe that fairness can vary even
more than accuracy with increasing training data size and different random
initializations. At the same time, we find that little of the fairness
variation is explained by model size, despite claims in the literature. To
improve model fairness without retraining, we show that two post-processing
methods developed for structured, tabular data can be successfully applied to a
range of pretrained language models. \textit{Warning: This paper contains
samples of offensive text.}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP. (arXiv:2109.03777v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03777">
<div class="article-summary-box-inner">
<span><p>Graph neural networks have triggered a resurgence of graph-based text
classification methods, defining today's state of the art. We show that a wide
multi-layer perceptron (MLP) using a Bag-of-Words (BoW) outperforms the recent
graph-based models TextGCN and HeteGCN in an inductive text classification
setting and is comparable with HyperGAT. Moreover, we fine-tune a
sequence-based BERT and a lightweight DistilBERT model, which both outperform
all state-of-the-art models. These results question the importance of synthetic
graphs used in modern text classifiers. In terms of efficiency, DistilBERT is
still twice as large as our BoW-based wide MLP, while graph-based models like
TextGCN require setting up an $\mathcal{O}(N^2)$ graph, where $N$ is the
vocabulary plus corpus size. Finally, since Transformers need to compute
$\mathcal{O}(L^2)$ attention weights with sequence length $L$, the MLP models
show higher training and inference speeds on datasets with long sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STraTA: Self-Training with Task Augmentation for Better Few-shot Learning. (arXiv:2109.06270v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06270">
<div class="article-summary-box-inner">
<span><p>Despite their recent successes in tackling many NLP tasks, large-scale
pre-trained language models do not perform as well in few-shot settings where
only a handful of training examples are available. To address this shortcoming,
we propose STraTA, which stands for Self-Training with Task Augmentation, an
approach that builds on two key ideas for effective leverage of unlabeled data.
First, STraTA uses task augmentation, a novel technique that synthesizes a
large amount of data for auxiliary-task fine-tuning from target-task unlabeled
texts. Second, STraTA performs self-training by further fine-tuning the strong
base model created by task augmentation on a broad distribution of
pseudo-labeled data. Our experiments demonstrate that STraTA can substantially
improve sample efficiency across 12 few-shot benchmarks. Remarkably, on the
SST-2 sentiment dataset, STraTA, with only 8 training examples per class,
achieves comparable results to standard fine-tuning with 67K training examples.
Our analyses reveal that task augmentation and self-training are both
complementary and independently effective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-learning via Language Model In-context Tuning. (arXiv:2110.07814v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07814">
<div class="article-summary-box-inner">
<span><p>The goal of meta-learning is to learn to adapt to a new task with only a few
labeled examples. To tackle this problem in NLP, we propose $\textit{in-context
tuning}$, which recasts adaptation and prediction as a simple sequence
prediction problem: to form the input sequence, we concatenate the task
instruction, the labeled examples, and the target input to predict; to
meta-train the model to learn from in-context examples, we fine-tune a
pre-trained language model (LM) to predict the target label from the input
sequences on a collection of tasks.
</p>
<p>We benchmark our method on two collections of text classification tasks: LAMA
and BinaryClfs. Compared to first-order MAML which adapts the model with
gradient descent, our method better leverages the inductive bias of LMs to
perform pattern matching, and outperforms MAML by an absolute $6\%$ AUC ROC
score on BinaryClfs, with increasing advantage w.r.t. model size. Compared to
non-fine-tuned in-context learning (i.e. prompting a raw LM), in-context tuning
directly learns to learn from in-context examples. On BinaryClfs, in-context
tuning improves the average AUC-ROC score by an absolute $10\%$, and reduces
the variance with respect to example ordering by 6x and example choices by 2x.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Catch Me If You Can: Blackbox Adversarial Attacks on Automatic Speech Recognition using Frequency Masking. (arXiv:2112.01821v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01821">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) models are prevalent, particularly in
applications for voice navigation and voice control of domestic appliances. The
computational core of ASRs are deep neural networks (DNNs) that have been shown
to be susceptible to adversarial perturbations; easily misused by attackers to
generate malicious outputs. To help test the security and robustnesss of ASRS,
we propose techniques that generate blackbox (agnostic to the DNN), untargeted
adversarial attacks that are portable across ASRs. This is in contrast to
existing work that focuses on whitebox targeted attacks that are time consuming
and lack portability.
</p>
<p>Our techniques generate adversarial attacks that have no human audible
difference by manipulating the audio signal using a psychoacoustic model that
maintains the audio perturbations below the thresholds of human perception. We
evaluate portability and effectiveness of our techniques using three popular
ASRs and two input audio datasets using the metrics - Word Error Rate (WER) of
output transcription, Similarity to original audio, attack Success Rate on
different ASRs and Detection score by a defense system. We found our
adversarial attacks were portable across ASRs, not easily detected by a
state-of-the-art defense system, and had significant difference in output
transcriptions while sounding similar to original audio.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GenIE: Generative Information Extraction. (arXiv:2112.08340v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08340">
<div class="article-summary-box-inner">
<span><p>Structured and grounded representation of text is typically formalized by
closed information extraction, the problem of extracting an exhaustive set of
(subject, relation, object) triplets that are consistent with a predefined set
of entities and relations from a knowledge base schema. Most existing works are
pipelines prone to error accumulation, and all approaches are only applicable
to unrealistically small numbers of entities and relations. We introduce GenIE
(generative information extraction), the first end-to-end autoregressive
formulation of closed information extraction. GenIE naturally exploits the
language knowledge from the pre-trained transformer by autoregressively
generating relations and entities in textual form. Thanks to a new bi-level
constrained generation strategy, only triplets consistent with the predefined
knowledge base schema are produced. Our experiments show that GenIE is
state-of-the-art on closed information extraction, generalizes from fewer
training data points than baselines, and scales to a previously unmanageable
number of entities and relations. With this work, closed information extraction
becomes practical in realistic scenarios, providing new opportunities for
downstream tasks. Finally, this work paves the way towards a unified end-to-end
approach to the core tasks of information extraction. Code, data and models
available at https://github.com/epfl-dlab/GenIE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A survey in Adversarial Defences and Robustness in NLP. (arXiv:2203.06414v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06414">
<div class="article-summary-box-inner">
<span><p>In recent years, it has been seen that deep neural networks are lacking
robustness and are likely to break in case of adversarial perturbations in
input data. Strong adversarial attacks are proposed by various authors for
computer vision and Natural Language Processing (NLP). As a counter-effort,
several defense mechanisms are also proposed to save these networks from
failing. In contrast with image data, generating adversarial attacks and
defending these models is not easy in NLP because of the discrete nature of the
text data. However, numerous methods for adversarial defense are proposed of
late, for different NLP tasks such as text classification, named entity
recognition, natural language inferencing, etc. These methods are not just used
for defending neural networks from adversarial attacks, but also used as a
regularization mechanism during training, saving the model from overfitting.
The proposed survey is an attempt to review different methods proposed for
adversarial defenses in NLP in the recent past by proposing a novel taxonomy.
This survey also highlights the fragility of the advanced deep neural networks
in NLP and the challenges in defending them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyperlink-induced Pre-training for Passage Retrieval in Open-domain Question Answering. (arXiv:2203.06942v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06942">
<div class="article-summary-box-inner">
<span><p>To alleviate the data scarcity problem in training question answering
systems, recent works propose additional intermediate pre-training for dense
passage retrieval (DPR). However, there still remains a large discrepancy
between the provided upstream signals and the downstream question-passage
relevance, which leads to less improvement. To bridge this gap, we propose the
HyperLink-induced Pre-training (HLP), a method to pre-train the dense retriever
with the text relevance induced by hyperlink-based topology within Web
documents. We demonstrate that the hyperlink-based structures of dual-link and
co-mention can provide effective relevance signals for large-scale pre-training
that better facilitate downstream passage retrieval. We investigate the
effectiveness of our approach across a wide range of open-domain QA datasets
under zero-shot, few-shot, multi-hop, and out-of-domain scenarios. The
experiments show our HLP outperforms the BM25 by up to 7 points as well as
other pre-training methods by more than 10 points in terms of top-20 retrieval
accuracy under the zero-shot scenario. Furthermore, HLP significantly
outperforms other pre-training methods under the other scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Bayesian approach to translators' reliability assessment. (arXiv:2203.07135v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07135">
<div class="article-summary-box-inner">
<span><p>Translation Quality Assessment (TQA) is a process conducted by human
translators and is widely used, both for estimating the performance of
(increasingly used) Machine Translation, and for finding an agreement between
translation providers and their customers. While translation scholars are aware
of the importance of having a reliable way to conduct the TQA process, it seems
that there is limited literature that tackles the issue of reliability with a
quantitative approach. In this work, we consider the TQA as a complex process
from the point of view of physics of complex systems and approach the
reliability issue from the Bayesian paradigm. Using a dataset of translation
quality evaluations (in the form of error annotations), produced entirely by
the Professional Translation Service Provider Translated SRL, we compare two
Bayesian models that parameterise the following features involved in the TQA
process: the translation difficulty, the characteristics of the translators
involved in producing the translation, and of those assessing its quality - the
reviewers. We validate the models in an unsupervised setting and show that it
is possible to get meaningful insights into translators even with just one
review per translation; subsequently, we extract information like translators'
skills and reviewers' strictness, as well as their consistency in their
respective roles. Using this, we show that the reliability of reviewers cannot
be taken for granted even in the case of expert translators: a translator's
expertise can induce a cognitive bias when reviewing a translation produced by
another translator. The most expert translators, however, are characterised by
the highest level of consistency, both in translating and in assessing the
translation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gender and Racial Stereotype Detection in Legal Opinion Word Embeddings. (arXiv:2203.13369v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13369">
<div class="article-summary-box-inner">
<span><p>Studies have shown that some Natural Language Processing (NLP) systems encode
and replicate harmful biases with potential adverse ethical effects in our
society. In this article, we propose an approach for identifying gender and
racial stereotypes in word embeddings trained on judicial opinions from U.S.
case law. Embeddings containing stereotype information may cause harm when used
by downstream systems for classification, information extraction, question
answering, or other machine learning systems used to build legal research
tools. We first explain how previously proposed methods for identifying these
biases are not well suited for use with word embeddings trained on legal
opinion text. We then propose a domain adapted method for identifying gender
and racial biases in the legal domain. Our analyses using these methods suggest
that racial and gender biases are encoded into word embeddings trained on legal
opinions. These biases are not mitigated by exclusion of historical data, and
appear across multiple large topical areas of the law. Implications for
downstream systems that use legal opinion word embeddings and suggestions for
potential mitigation strategies based on our observations are also discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Audio-text Representation for Automated Audio Captioning with Contrastive Learning. (arXiv:2203.15526v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15526">
<div class="article-summary-box-inner">
<span><p>Automated Audio captioning (AAC) is a cross-modal task that generates natural
language to describe the content of input audio. Most prior works usually
extract single-modality acoustic features and are therefore sub-optimal for the
cross-modal decoding task. In this work, we propose a novel AAC system called
CLIP-AAC to learn interactive cross-modality representation with both acoustic
and textual information. Specifically, the proposed CLIP-AAC introduces an
audio-head and a text-head in the pre-trained encoder to extract audio-text
information. Furthermore, we also apply contrastive learning to narrow the
domain difference by learning the correspondence between the audio signal and
its paired captions. Experimental results show that the proposed CLIP-AAC
approach surpasses the best baseline by a significant margin on the Clotho
dataset in terms of NLP evaluation metrics. The ablation study indicates that
both the pre-trained model and contrastive learning contribute to the
performance gain of the AAC model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clozer: Adaptable Data Augmentation for Cloze-style Reading Comprehension. (arXiv:2203.16027v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16027">
<div class="article-summary-box-inner">
<span><p>Task-adaptive pre-training (TAPT) alleviates the lack of labelled data and
provides performance lift by adapting unlabelled data to downstream task.
Unfortunately, existing adaptations mainly involve deterministic rules that
cannot generalize well. Here, we propose Clozer, a sequence-tagging based cloze
answer extraction method used in TAPT that is extendable for adaptation on any
cloze-style machine reading comprehension (MRC) downstream tasks. We experiment
on multiple-choice cloze-style MRC tasks, and show that Clozer performs
significantly better compared to the oracle and state-of-the-art in escalating
TAPT effectiveness in lifting model performance, and prove that Clozer is able
to recognize the gold answers independently of any heuristics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Commonsense-aware Moment-Text Alignment for Fast Video Temporal Grounding. (arXiv:2204.01450v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01450">
<div class="article-summary-box-inner">
<span><p>Grounding temporal video segments described in natural language queries
effectively and efficiently is a crucial capability needed in
vision-and-language fields. In this paper, we deal with the fast video temporal
grounding (FVTG) task, aiming at localizing the target segment with high speed
and favorable accuracy. Most existing approaches adopt elaborately designed
cross-modal interaction modules to improve the grounding performance, which
suffer from the test-time bottleneck. Although several common space-based
methods enjoy the high-speed merit during inference, they can hardly capture
the comprehensive and explicit relations between visual and textual modalities.
In this paper, to tackle the dilemma of speed-accuracy tradeoff, we propose a
commonsense-aware cross-modal alignment (CCA) framework, which incorporates
commonsense-guided visual and text representations into a complementary common
space for fast video temporal grounding. Specifically, the commonsense concepts
are explored and exploited by extracting the structural semantic information
from a language corpus. Then, a commonsense-aware interaction module is
designed to obtain bridged visual and text features by utilizing the learned
commonsense concepts. Finally, to maintain the original semantic information of
textual queries, a cross-modal complementary common space is optimized to
obtain matching scores for performing FVTG. Extensive results on two
challenging benchmarks show that our CCA method performs favorably against
state-of-the-arts while running at high speed. Our code is available at
https://github.com/ZiyueWu59/CCA.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic, Instance and Semantic Relations: A Relational Context Encoder to Enhance Panoptic Segmentation. (arXiv:2204.05370v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05370">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel framework to integrate both semantic and instance
contexts for panoptic segmentation. In existing works, it is common to use a
shared backbone to extract features for both things (countable classes such as
vehicles) and stuff (uncountable classes such as roads). This, however, fails
to capture the rich relations among them, which can be utilized to enhance
visual understanding and segmentation performance. To address this shortcoming,
we propose a novel Panoptic, Instance, and Semantic Relations (PISR) module to
exploit such contexts. First, we generate panoptic encodings to summarize key
features of the semantic classes and predicted instances. A Panoptic Relational
Attention (PRA) module is then applied to the encodings and the global feature
map from the backbone. It produces a feature map that captures 1) the relations
across semantic classes and instances and 2) the relations between these
panoptic categories and spatial features. PISR also automatically learns to
focus on the more important instances, making it robust to the number of
instances used in the relational attention module. Moreover, PISR is a general
module that can be applied to any existing panoptic segmentation architecture.
Through extensive evaluations on panoptic segmentation benchmarks like
Cityscapes, COCO, and ADE20K, we show that PISR attains considerable
improvements over existing approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">medXGAN: Visual Explanations for Medical Classifiers through a Generative Latent Space. (arXiv:2204.05376v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05376">
<div class="article-summary-box-inner">
<span><p>Despite the surge of deep learning in the past decade, some users are
skeptical to deploy these models in practice due to their black-box nature.
Specifically, in the medical space where there are severe potential
repercussions, we need to develop methods to gain confidence in the models'
decisions. To this end, we propose a novel medical imaging generative
adversarial framework, medXGAN (medical eXplanation GAN), to visually explain
what a medical classifier focuses on in its binary predictions. By encoding
domain knowledge of medical images, we are able to disentangle anatomical
structure and pathology, leading to fine-grained visualization through latent
interpolation. Furthermore, we optimize the latent space such that
interpolation explains how the features contribute to the classifier's output.
Our method outperforms baselines such as Gradient-Weighted Class Activation
Mapping (Grad-CAM) and Integrated Gradients in localization and explanatory
ability. Additionally, a combination of the medXGAN with Integrated Gradients
can yield explanations more robust to noise. The code is available at:
https://github.com/avdravid/medXGAN_explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Vision Transformers for Joint SAR-optical Representation Learning. (arXiv:2204.05381v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05381">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) has attracted much interest in remote sensing
and earth observation due to its ability to learn task-agnostic representations
without human annotation. While most of the existing SSL works in remote
sensing utilize ConvNet backbones and focus on a single modality, we explore
the potential of vision transformers (ViTs) for joint SAR-optical
representation learning. Based on DINO, a state-of-the-art SSL algorithm that
distills knowledge from two augmented views of an input image, we combine SAR
and optical imagery by concatenating all channels to a unified input.
Subsequently, we randomly mask out channels of one modality as a data
augmentation strategy. While training, the model gets fed optical-only,
SAR-only, and SAR-optical image pairs learning both inner- and intra-modality
representations. Experimental results employing the BigEarthNet-MM dataset
demonstrate the benefits of both, the ViT backbones and the proposed multimodal
SSL algorithm DINO-MM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Few-Shot Part Segmentation using Coarse Supervision. (arXiv:2204.05393v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05393">
<div class="article-summary-box-inner">
<span><p>A significant bottleneck in training deep networks for part segmentation is
the cost of obtaining detailed annotations. We propose a framework to exploit
coarse labels such as figure-ground masks and keypoint locations that are
readily available for some categories to improve part segmentation models. A
key challenge is that these annotations were collected for different tasks and
with different labeling styles and cannot be readily mapped to the part labels.
To this end, we propose to jointly learn the dependencies between labeling
styles and the part segmentation model, allowing us to utilize supervision from
diverse labels. To evaluate our approach we develop a benchmark on the
Caltech-UCSD birds and OID Aircraft dataset. Our approach outperforms baselines
based on multi-task learning, semi-supervised learning, and competitive methods
relying on loss functions manually designed to exploit sparse-supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizing Adversarial Explanations with Grad-CAM. (arXiv:2204.05427v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05427">
<div class="article-summary-box-inner">
<span><p>Gradient-weighted Class Activation Mapping (Grad- CAM), is an example-based
explanation method that provides a gradient activation heat map as an
explanation for Convolution Neural Network (CNN) models. The drawback of this
method is that it cannot be used to generalize CNN behaviour. In this paper, we
present a novel method that extends Grad-CAM from example-based explanations to
a method for explaining global model behaviour. This is achieved by introducing
two new metrics, (i) Mean Observed Dissimilarity (MOD) and (ii) Variation in
Dissimilarity (VID), for model generalization. These metrics are computed by
comparing a Normalized Inverted Structural Similarity Index (NISSIM) metric of
the Grad-CAM generated heatmap for samples from the original test set and
samples from the adversarial test set. For our experiment, we study adversarial
attacks on deep models such as VGG16, ResNet50, and ResNet101, and wide models
such as InceptionNetv3 and XceptionNet using Fast Gradient Sign Method (FGSM).
We then compute the metrics MOD and VID for the automatic face recognition
(AFR) use case with the VGGFace2 dataset. We observe a consistent shift in the
region highlighted in the Grad-CAM heatmap, reflecting its participation to the
decision making, across all models under adversarial attacks. The proposed
method can be used to understand adversarial attacks and explain the behaviour
of black box CNN models for image analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Approach to Adversarial Robustness in Few-shot Image Classification. (arXiv:2204.05432v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05432">
<div class="article-summary-box-inner">
<span><p>Few-shot image classification, where the goal is to generalize to tasks with
limited labeled data, has seen great progress over the years. However, the
classifiers are vulnerable to adversarial examples, posing a question regarding
their generalization capabilities. Recent works have tried to combine
meta-learning approaches with adversarial training to improve the robustness of
few-shot classifiers. We show that a simple transfer-learning based approach
can be used to train adversarially robust few-shot classifiers. We also present
a method for novel classification task based on calibrating the centroid of the
few-shot category towards the base classes. We show that standard adversarial
training on base categories along with calibrated centroid-based classifier in
the novel categories, outperforms or is on-par with state-of-the-art advanced
methods on standard benchmarks for few-shot learning. Our method is simple,
easy to scale, and with little effort can lead to robust few-shot classifiers.
Code is available here: \url{https://github.com/UCDvision/Simple_few_shot.git}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Glass Segmentation with RGB-Thermal Image Pairs. (arXiv:2204.05453v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05453">
<div class="article-summary-box-inner">
<span><p>This paper proposes a new glass segmentation method utilizing paired RGB and
thermal images. Due to the large difference between the transmission property
of visible light and that of the thermal energy through the glass where most
glass is transparent to the visible light but opaque to thermal energy, glass
regions of a scene are made more distinguishable with a pair of RGB and thermal
images than solely with an RGB image. To exploit such a unique property, we
propose a neural network architecture that effectively combines an RGB-thermal
image pair with a new multi-modal fusion module based on attention, and
integrate CNN and transformer to extract local features and long-range
dependencies, respectively. As well, we have collected a new dataset containing
5551 RGB-thermal image pairs with ground-truth segmentation annotations. The
qualitative and quantitative evaluations demonstrate the effectiveness of the
proposed approach on fusing RGB and thermal data for glass segmentation. Our
code and data are available at
https://github.com/Dong-Huo/RGB-T-Glass-Segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Multimodal Transformers Robust to Missing Modality?. (arXiv:2204.05454v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05454">
<div class="article-summary-box-inner">
<span><p>Multimodal data collected from the real world are often imperfect due to
missing modalities. Therefore multimodal models that are robust against
modal-incomplete data are highly preferred. Recently, Transformer models have
shown great success in processing multimodal data. However, existing work has
been limited to either architecture designs or pre-training strategies; whether
Transformer models are naturally robust against missing-modal data has rarely
been investigated. In this paper, we present the first-of-its-kind work to
comprehensively investigate the behavior of Transformers in the presence of
modal-incomplete data. Unsurprising, we find Transformer models are sensitive
to missing modalities while different modal fusion strategies will
significantly affect the robustness. What surprised us is that the optimal
fusion strategy is dataset dependent even for the same Transformer model; there
does not exist a universal strategy that works in general cases. Based on these
findings, we propose a principle method to improve the robustness of
Transformer models by automatically searching for an optimal fusion strategy
regarding input data. Experimental validations on three benchmarks support the
superior performance of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Out-Of-Distribution Detection In Unsupervised Continual Learning. (arXiv:2204.05462v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05462">
<div class="article-summary-box-inner">
<span><p>Unsupervised continual learning aims to learn new tasks incrementally without
requiring human annotations. However, most existing methods, especially those
targeted on image classification, only work in a simplified scenario by
assuming all new data belong to new tasks, which is not realistic if the class
labels are not provided. Therefore, to perform unsupervised continual learning
in real life applications, an out-of-distribution detector is required at
beginning to identify whether each new data corresponds to a new task or
already learned tasks, which still remains under-explored yet. In this work, we
formulate the problem for Out-of-distribution Detection in Unsupervised
Continual Learning (OOD-UCL) with the corresponding evaluation protocol. In
addition, we propose a novel OOD detection method by correcting the output bias
at first and then enhancing the output confidence for in-distribution data
based on task discriminativeness, which can be applied directly without
modifying the learning procedures and objectives of continual learning. Our
method is evaluated on CIFAR-100 dataset by following the proposed evaluation
protocol and we show improved performance compared with existing OOD detection
methods under the unsupervised continual learning scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HiTPR: Hierarchical Transformer for Place Recognition in Point Cloud. (arXiv:2204.05481v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05481">
<div class="article-summary-box-inner">
<span><p>Place recognition or loop closure detection is one of the core components in
a full SLAM system. In this paper, aiming at strengthening the relevancy of
local neighboring points and the contextual dependency among global points
simultaneously, we investigate the exploitation of transformer-based network
for feature extraction, and propose a Hierarchical Transformer for Place
Recognition (HiTPR). The HiTPR consists of four major parts: point cell
generation, short-range transformer (SRT), long-range transformer (LRT) and
global descriptor aggregation. Specifically, the point cloud is initially
divided into a sequence of small cells by downsampling and nearest neighbors
searching. In the SRT, we extract the local feature for each point cell. While
in the LRT, we build the global dependency among all of the point cells in the
whole point cloud. Experiments on several standard benchmarks demonstrate the
superiority of the HiTPR in terms of average recall rate, achieving 93.71% at
top 1% and 86.63% at top 1 on the Oxford RobotCar dataset for example.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Graph Matching for Modification Similarity Applied to Electronic Document Comparison. (arXiv:2204.05486v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05486">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel neural graph matching approach applied to
document comparison. Document comparison is a common task in the legal and
financial industries. In some cases, the most important differences may be the
addition or omission of words, sentences, clauses, or paragraphs. However, it
is a challenging task without recording or tracing whole edited process. Under
many temporal uncertainties, we explore the potentiality of our approach to
proximate the accurate comparison to make sure which element blocks have a
relation of edition with others. In beginning, we apply a document layout
analysis that combining traditional and modern technics to segment layout in
blocks of various types appropriately. Then we transform this issue to a
problem of layout graph matching with textual awareness. About graph matching,
it is a long-studied problem with a broad range of applications. However,
different from previous works focusing on visual images or structural layout,
we also bring textual features into our model for adapting this domain.
Specifically, based on the electronic document, we introduce an encoder to deal
with the visual presentation decoding from PDF. Additionally, because the
modifications can cause the inconsistency of document layout analysis between
modified documents and the blocks can be merged and split, Sinkhorn divergence
is adopted in our graph neural approach, which tries to overcome both these
issues with many-to-many block matching. We demonstrate this on two categories
of layouts, as follows., legal agreement and scientific articles, collected
from our real-case datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Learning with Noisy Labels. (arXiv:2204.05494v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05494">
<div class="article-summary-box-inner">
<span><p>Few-shot learning (FSL) methods typically assume clean support sets with
accurately labeled samples when training on novel classes. This assumption can
often be unrealistic: support sets, no matter how small, can still include
mislabeled samples. Robustness to label noise is therefore essential for FSL
methods to be practical, but this problem surprisingly remains largely
unexplored. To address mislabeled samples in FSL settings, we make several
technical contributions. (1) We offer simple, yet effective, feature
aggregation methods, improving the prototypes used by ProtoNet, a popular FSL
technique. (2) We describe a novel Transformer model for Noisy Few-Shot
Learning (TraNFS). TraNFS leverages a transformer's attention mechanism to
weigh mislabeled versus correct samples. (3) Finally, we extensively test these
methods on noisy versions of MiniImageNet and TieredImageNet. Our results show
that TraNFS is on-par with leading FSL methods on clean support sets, yet
outperforms them, by far, in the presence of label noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Position-aware Location Regression Network for Temporal Video Grounding. (arXiv:2204.05499v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05499">
<div class="article-summary-box-inner">
<span><p>The key to successful grounding for video surveillance is to understand a
semantic phrase corresponding to important actors and objects. Conventional
methods ignore comprehensive contexts for the phrase or require heavy
computation for multiple phrases. To understand comprehensive contexts with
only one semantic phrase, we propose Position-aware Location Regression Network
(PLRN) which exploits position-aware features of a query and a video.
Specifically, PLRN first encodes both the video and query using positional
information of words and video segments. Then, a semantic phrase feature is
extracted from an encoded query with attention. The semantic phrase feature and
encoded video are merged and made into a context-aware feature by reflecting
local and global contexts. Finally, PLRN predicts start, end, center, and width
values of a grounding boundary. Our experiments show that PLRN achieves
competitive performance over existing methods with less computation time and
memory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoupleFace: Relation Matters for Face Recognition Distillation. (arXiv:2204.05502v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05502">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation is an effective method to improve the performance of a
lightweight neural network (i.e., student model) by transferring the knowledge
of a well-performed neural network (i.e., teacher model), which has been widely
applied in many computer vision tasks, including face recognition.
Nevertheless, the current face recognition distillation methods usually utilize
the Feature Consistency Distillation (FCD) (e.g., L2 distance) on the learned
embeddings extracted by the teacher and student models for each sample, which
is not able to fully transfer the knowledge from the teacher to the student for
face recognition. In this work, we observe that mutual relation knowledge
between samples is also important to improve the discriminative ability of the
learned representation of the student model, and propose an effective face
recognition distillation method called CoupleFace by additionally introducing
the Mutual Relation Distillation (MRD) into existing distillation framework.
Specifically, in MRD, we first propose to mine the informative mutual
relations, and then introduce the Relation-Aware Distillation (RAD) loss to
transfer the mutual relation knowledge of the teacher model to the student
model. Extensive experimental results on multiple benchmark datasets
demonstrate the effectiveness of our proposed CoupleFace for face recognition.
Moreover, based on our proposed CoupleFace, we have won the first place in the
ICCV21 Masked Face Recognition Challenge (MS1M track).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FSOINet: Feature-Space Optimization-Inspired Network for Image Compressive Sensing. (arXiv:2204.05503v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05503">
<div class="article-summary-box-inner">
<span><p>In recent years, deep learning-based image compressive sensing (ICS) methods
have achieved brilliant success. Many optimization-inspired networks have been
proposed to bring the insights of optimization algorithms into the network
structure design and have achieved excellent reconstruction quality with low
computational complexity. But they keep the information flow in pixel space as
traditional algorithms by updating and transferring the image in pixel space,
which does not fully use the information in the image features. In this paper,
we propose the idea of achieving information flow phase by phase in feature
space and design a Feature-Space Optimization-Inspired Network (dubbed FSOINet)
to implement it by mapping both steps of proximal gradient descent algorithm
from pixel space to feature space. Moreover, the sampling matrix is learned
end-to-end with other network parameters. Experiments show that the proposed
FSOINet outperforms the existing state-of-the-art methods by a large margin
both quantitatively and qualitatively. The source code is available on
https://github.com/cwjjun/FSOINet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fully End-to-end Autonomous Driving with Semantic Depth Cloud Mapping and Multi-Agent. (arXiv:2204.05513v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05513">
<div class="article-summary-box-inner">
<span><p>Focusing on the task of point-to-point navigation for an autonomous driving
vehicle, we propose a novel deep learning model trained with end-to-end and
multi-task learning manners to perform both perception and control tasks
simultaneously. The model is used to drive the ego vehicle safely by following
a sequence of routes defined by the global planner. The perception part of the
model is used to encode high-dimensional observation data provided by an RGBD
camera while performing semantic segmentation, semantic depth cloud (SDC)
mapping, and traffic light state and stop sign prediction. Then, the control
part decodes the encoded features along with additional information provided by
GPS and speedometer to predict waypoints that come with a latent feature space.
Furthermore, two agents are employed to process these outputs and make a
control policy that determines the level of steering, throttle, and brake as
the final action. The model is evaluated on CARLA simulator with various
scenarios made of normal-adversarial situations and different weathers to mimic
real-world conditions. In addition, we do a comparative study with some recent
models to justify the performance in multiple aspects of driving. Moreover, we
also conduct an ablation study on SDC mapping and multi-agent to understand
their roles and behavior. As a result, our model achieves the highest driving
score even with fewer parameters and computation load. To support future
studies, we share our codes at
https://github.com/oskarnatan/end-to-end-driving.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation. (arXiv:2204.05525v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05525">
<div class="article-summary-box-inner">
<span><p>Although vision transformers (ViTs) have achieved great success in computer
vision, the heavy computational cost hampers their applications to dense
prediction tasks such as semantic segmentation on mobile devices. In this
paper, we present a mobile-friendly architecture named \textbf{To}ken
\textbf{P}yramid Vision Trans\textbf{former} (\textbf{TopFormer}). The proposed
\textbf{TopFormer} takes Tokens from various scales as input to produce
scale-aware semantic features, which are then injected into the corresponding
tokens to augment the representation. Experimental results demonstrate that our
method significantly outperforms CNN- and ViT-based networks across several
semantic segmentation datasets and achieves a good trade-off between accuracy
and latency. On the ADE20K dataset, TopFormer achieves 5\% higher accuracy in
mIoU than MobileNetV3 with lower latency on an ARM-based mobile device.
Furthermore, the tiny version of TopFormer achieves real-time inference on an
ARM-based mobile device with competitive results. The code and models are
available at: https://github.com/hustvl/TopFormer
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unidirectional Video Denoising by Mimicking Backward Recurrent Modules with Look-ahead Forward Ones. (arXiv:2204.05532v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05532">
<div class="article-summary-box-inner">
<span><p>While significant progress has been made in deep video denoising, it remains
very challenging for exploiting historical and future frames. Bidirectional
recurrent networks (BiRNN) have exhibited appealing performance in several
video restoration tasks. However, BiRNN is intrinsically offline because it
uses backward recurrent modules to propagate from the last to current frames,
which causes high latency and large memory consumption. To address the offline
issue of BiRNN, we present a novel recurrent network consisting of forward and
look-ahead recurrent modules for unidirectional video denoising. Particularly,
look-ahead module is an elaborate forward module for leveraging information
from near-future frames. When denoising the current frame, the hidden features
by forward and look-ahead recurrent modules are combined, thereby making it
feasible to exploit both historical and near-future frames. Due to the scene
motion between non-neighboring frames, border pixels missing may occur when
warping look-ahead feature from near-future frame to current frame, which can
be largely alleviated by incorporating forward warping and border enlargement.
Experiments show that our method achieves state-of-the-art performance with
constant latency and memory consumption. The source code and pre-trained models
will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-set Text Recognition via Character-Context Decoupling. (arXiv:2204.05535v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05535">
<div class="article-summary-box-inner">
<span><p>The open-set text recognition task is an emerging challenge that requires an
extra capability to cognize novel characters during evaluation. We argue that a
major cause of the limited performance for current methods is the confounding
effect of contextual information over the visual information of individual
characters. Under open-set scenarios, the intractable bias in contextual
information can be passed down to visual information, consequently impairing
the classification performance. In this paper, a Character-Context Decoupling
framework is proposed to alleviate this problem by separating contextual
information and character-visual information. Contextual information can be
decomposed into temporal information and linguistic information. Here, temporal
information that models character order and word length is isolated with a
detached temporal attention module. Linguistic information that models n-gram
and other linguistic statistics is separated with a decoupled context anchor
mechanism. A variety of quantitative and qualitative experiments show that our
method achieves promising performance on open-set, zero-shot, and close-set
text recognition datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NightLab: A Dual-level Architecture with Hardness Detection for Segmentation at Night. (arXiv:2204.05538v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05538">
<div class="article-summary-box-inner">
<span><p>The semantic segmentation of nighttime scenes is a challenging problem that
is key to impactful applications like self-driving cars. Yet, it has received
little attention compared to its daytime counterpart. In this paper, we propose
NightLab, a novel nighttime segmentation framework that leverages multiple deep
learning models imbued with night-aware features to yield State-of-The-Art
(SoTA) performance on multiple night segmentation benchmarks. Notably, NightLab
contains models at two levels of granularity, i.e. image and regional, and each
level is composed of light adaptation and segmentation modules. Given a
nighttime image, the image level model provides an initial segmentation
estimate while, in parallel, a hardness detection module identifies regions and
their surrounding context that need further analysis. A regional level model
focuses on these difficult regions to provide a significantly improved
segmentation. All the models in NightLab are trained end-to-end using a set of
proposed night-aware losses without handcrafted heuristics. Extensive
experiments on the NightCity and BDD100K datasets show NightLab achieves SoTA
performance compared to concurrent methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Content and Style Aware Generation of Text-line Images for Handwriting Recognition. (arXiv:2204.05539v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05539">
<div class="article-summary-box-inner">
<span><p>Handwritten Text Recognition has achieved an impressive performance in public
benchmarks. However, due to the high inter- and intra-class variability between
handwriting styles, such recognizers need to be trained using huge volumes of
manually labeled training data. To alleviate this labor-consuming problem,
synthetic data produced with TrueType fonts has been often used in the training
loop to gain volume and augment the handwriting style variability. However,
there is a significant style bias between synthetic and real data which hinders
the improvement of recognition performance. To deal with such limitations, we
propose a generative method for handwritten text-line images, which is
conditioned on both visual appearance and textual content. Our method is able
to produce long text-line samples with diverse handwriting styles. Once
properly trained, our method can also be adapted to new target data by only
accessing unlabeled text-line images to mimic handwritten styles and produce
images with any textual content. Extensive experiments have been done on making
use of the generated samples to boost Handwritten Text Recognition performance.
Both qualitative and quantitative results demonstrate that the proposed
approach outperforms the current state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Reliable Image Outpainting: Learning Structure-Aware Multimodal Fusion with Depth Guidance. (arXiv:2204.05543v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05543">
<div class="article-summary-box-inner">
<span><p>Image outpainting technology generates visually reasonable content regardless
of authenticity, making it unreliable to serve for practical applications even
though introducing additional modalities eg. the sketch. Since sparse depth
maps are widely captured in robotics and autonomous systems, together with RGB
images, we combine the sparse depth in the image outpainting task to provide
more reliable performance. Concretely, we propose a Depth-Guided Outpainting
Network (DGONet) to model the feature representations of different modalities
differentially and learn the structure-aware cross-modal fusion. To this end,
two components are designed to implement: 1) The Multimodal Learning Module
produces unique depth and RGB feature representations from the perspectives of
different modal characteristics. 2) The Depth Guidance Fusion Module leverages
the complete depth modality to guide the establishment of RGB contents by
progressive multimodal feature fusion. Furthermore, we specially design an
additional constraint strategy consisting of Cross-modal Loss and Edge Loss to
enhance ambiguous contours and expedite reliable content generation. Extensive
experiments on KITTI demonstrate our superiority over the state-of-the-art
methods with more reliable content generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Undoing the Damage of Label Shift for Cross-domain Semantic Segmentation. (arXiv:2204.05546v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05546">
<div class="article-summary-box-inner">
<span><p>Existing works typically treat cross-domain semantic segmentation (CDSS) as a
data distribution mismatch problem and focus on aligning the marginal
distribution or conditional distribution. However, the label shift issue is
unfortunately overlooked, which actually commonly exists in the CDSS task, and
often causes a classifier bias in the learnt model. In this paper, we give an
in-depth analysis and show that the damage of label shift can be overcome by
aligning the data conditional distribution and correcting the posterior
probability. To this end, we propose a novel approach to undo the damage of the
label shift problem in CDSS. In implementation, we adopt class-level feature
alignment for conditional distribution alignment, as well as two simple yet
effective methods to rectify the classifier bias from source to target by
remolding the classifier predictions. We conduct extensive experiments on the
benchmark datasets of urban scenes, including GTA5 to Cityscapes and SYNTHIA to
Cityscapes, where our proposed approach outperforms previous methods by a large
margin. For instance, our model equipped with a self-training strategy reaches
59.3% mIoU on GTA5 to Cityscapes, pushing to a new state-of-the-art. The code
will be available at https://github.com/manmanjun/Undoing UDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DistPro: Searching A Fast Knowledge Distillation Process via Meta Optimization. (arXiv:2204.05547v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05547">
<div class="article-summary-box-inner">
<span><p>Recent Knowledge distillation (KD) studies show that different manually
designed schemes impact the learned results significantly. Yet, in KD,
automatically searching an optimal distillation scheme has not yet been well
explored. In this paper, we propose DistPro, a novel framework which searches
for an optimal KD process via differentiable meta-learning. Specifically, given
a pair of student and teacher networks, DistPro first sets up a rich set of KD
connection from the transmitting layers of the teacher to the receiving layers
of the student, and in the meanwhile, various transforms are also proposed for
comparing feature maps along its pathway for the distillation. Then, each
combination of a connection and a transform choice (pathway) is associated with
a stochastic weighting process which indicates its importance at every step
during the distillation. In the searching stage, the process can be effectively
learned through our proposed bi-level meta-optimization strategy. In the
distillation stage, DistPro adopts the learned processes for knowledge
distillation, which significantly improves the student accuracy especially when
faster training is required. Lastly, we find the learned processes can be
generalized between similar tasks and networks. In our experiments, DistPro
produces state-of-the-art (SoTA) accuracy under varying number of learning
epochs on popular datasets, i.e. CIFAR100 and ImageNet, which demonstrate the
effectiveness of our framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compact Model Training by Low-Rank Projection with Energy Transfer. (arXiv:2204.05566v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05566">
<div class="article-summary-box-inner">
<span><p>Low-rankness plays an important role in traditional machine learning, but is
not so popular in deep learning. Most previous low-rank network compression
methods compress the networks by approximating pre-trained models and
re-training. However, optimal solution in the Euclidean space may be quite
different from the one in the low-rank manifold. A well pre-trained model is
not a good initialization for the model with low-rank constraint. Thus, the
performance of low-rank compressed network degrades significantly. Compared to
other network compression methods such as pruning, low-rank methods attracts
less attention in recent years. In this paper, we devise a new training method,
low-rank projection with energy transfer (LRPET), that trains low-rank
compressed networks from scratch and achieves competitive performance. First,
we propose to alternately perform stochastic gradient descent training and
projection onto the low-rank manifold. This asymptotically approaches the
optimal solution in the low-rank manifold. Compared to re-training on compact
model, this enables fully utilization of model capacity since solution space is
relaxed back to Euclidean space after projection. Second, the matrix energy
(the sum of squares of singular values) reduction caused by projection is
compensated by energy transfer. We uniformly transfer the energy of the pruned
singular values to the remaining ones. We theoretically show that energy
transfer eases the trend of gradient vanishing caused by projection.
Comprehensive experiment on CIFAR-10 and ImageNet have justified that our
method is superior to other low-rank compression methods and also outperforms
recent state-of-the-art pruning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAIR-V2X: A Large-Scale Dataset for Vehicle-Infrastructure Cooperative 3D Object Detection. (arXiv:2204.05575v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05575">
<div class="article-summary-box-inner">
<span><p>Autonomous driving faces great safety challenges for a lack of global
perspective and the limitation of long-range perception capabilities. It has
been widely agreed that vehicle-infrastructure cooperation is required to
achieve Level 5 autonomy. However, there is still NO dataset from real
scenarios available for computer vision researchers to work on
vehicle-infrastructure cooperation-related problems. To accelerate computer
vision research and innovation for Vehicle-Infrastructure Cooperative
Autonomous Driving (VICAD), we release DAIR-V2X Dataset, which is the first
large-scale, multi-modality, multi-view dataset from real scenarios for VICAD.
DAIR-V2X comprises 71254 LiDAR frames and 71254 Camera frames, and all frames
are captured from real scenes with 3D annotations. The Vehicle-Infrastructure
Cooperative 3D Object Detection problem (VIC3D) is introduced, formulating the
problem of collaboratively locating and identifying 3D objects using sensory
inputs from both vehicle and infrastructure. In addition to solving traditional
3D object detection problems, the solution of VIC3D needs to consider the
temporal asynchrony problem between vehicle and infrastructure sensors and the
data transmission cost between them. Furthermore, we propose Time Compensation
Late Fusion (TCLF), a late fusion framework for the VIC3D task as a benchmark
based on DAIR-V2X. Find data, code, and more up-to-date information at
https://thudair.baai.ac.cn/index and https://github.com/AIR-THU/DAIR-V2X.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SwinNet: Swin Transformer drives edge-aware RGB-D and RGB-T salient object detection. (arXiv:2204.05585v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05585">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) are good at extracting contexture
features within certain receptive fields, while transformers can model the
global long-range dependency features. By absorbing the advantage of
transformer and the merit of CNN, Swin Transformer shows strong feature
representation ability. Based on it, we propose a cross-modality fusion model
SwinNet for RGB-D and RGB-T salient object detection. It is driven by Swin
Transformer to extract the hierarchical features, boosted by attention
mechanism to bridge the gap between two modalities, and guided by edge
information to sharp the contour of salient object. To be specific, two-stream
Swin Transformer encoder first extracts multi-modality features, and then
spatial alignment and channel re-calibration module is presented to optimize
intra-level cross-modality features. To clarify the fuzzy boundary, edge-guided
decoder achieves inter-level cross-modality fusion under the guidance of edge
features. The proposed model outperforms the state-of-the-art models on RGB-D
and RGB-T datasets, showing that it provides more insight into the
cross-modality complementarity task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic detection of glaucoma via fundus imaging and artificial intelligence: A review. (arXiv:2204.05591v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05591">
<div class="article-summary-box-inner">
<span><p>Glaucoma is a leading cause of irreversible vision impairment globally and
cases are continuously rising worldwide. Early detection is crucial, allowing
timely intervention which can prevent further visual field loss. To detect
glaucoma, examination of the optic nerve head via fundus imaging can be
performed, at the centre of which is the assessment of the optic cup and disc
boundaries. Fundus imaging is non-invasive and low-cost; however, the image
examination relies on subjective, time-consuming, and costly expert
assessments. A timely question to ask is can artificial intelligence mimic
glaucoma assessments made by experts. Namely, can artificial intelligence
automatically find the boundaries of the optic cup and disc (providing a
so-called segmented fundus image) and then use the segmented image to identify
glaucoma with high accuracy. We conducted a comprehensive review on artificial
intelligence-enabled glaucoma detection frameworks that produce and use
segmented fundus images. We found 28 papers and identified two main approaches:
1) logical rule-based frameworks, based on a set of simplistic decision rules;
and 2) machine learning/statistical modelling based frameworks. We summarise
the state-of-art of the two approaches and highlight the key hurdles to
overcome for artificial intelligence-enabled glaucoma detection frameworks to
be translated into clinical practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Equity of Nuclear Norm Maximization in Unsupervised Domain Adaptation. (arXiv:2204.05596v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05596">
<div class="article-summary-box-inner">
<span><p>Nuclear norm maximization has shown the power to enhance the transferability
of unsupervised domain adaptation model (UDA) in an empirical scheme. In this
paper, we identify a new property termed equity, which indicates the balance
degree of predicted classes, to demystify the efficacy of nuclear norm
maximization for UDA theoretically. With this in mind, we offer a new
discriminability-and-equity maximization paradigm built on squares loss, such
that predictions are equalized explicitly. To verify its feasibility and
flexibility, two new losses termed Class Weighted Squares Maximization (CWSM)
and Normalized Squares Maximization (NSM), are proposed to maximize both
predictive discriminability and equity, from the class level and the sample
level, respectively. Importantly, we theoretically relate these two novel
losses (i.e., CWSM and NSM) to the equity maximization under mild conditions,
and empirically suggest the importance of the predictive equity in UDA.
Moreover, it is very efficient to realize the equity constraints in both
losses. Experiments of cross-domain image classification on three popular
benchmark datasets show that both CWSM and NSM contribute to outperforming the
corresponding counterparts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperDet3D: Learning a Scene-conditioned 3D Object Detector. (arXiv:2204.05599v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05599">
<div class="article-summary-box-inner">
<span><p>A bathtub in a library, a sink in an office, a bed in a laundry room -- the
counter-intuition suggests that scene provides important prior knowledge for 3D
object detection, which instructs to eliminate the ambiguous detection of
similar objects. In this paper, we propose HyperDet3D to explore
scene-conditioned prior knowledge for 3D object detection. Existing methods
strive for better representation of local elements and their relations without
scene-conditioned knowledge, which may cause ambiguity merely based on the
understanding of individual points and object candidates. Instead, HyperDet3D
simultaneously learns scene-agnostic embeddings and scene-specific knowledge
through scene-conditioned hypernetworks. More specifically, our HyperDet3D not
only explores the sharable abstracts from various 3D scenes, but also adapts
the detector to the given scene at test time. We propose a discriminative
Multi-head Scene-specific Attention (MSA) module to dynamically control the
layer parameters of the detector conditioned on the fusion of scene-conditioned
knowledge. Our HyperDet3D achieves state-of-the-art results on the 3D object
detection benchmark of the ScanNet and SUN RGB-D datasets. Moreover, through
cross-dataset evaluation, we show the acquired scene-conditioned prior
knowledge still takes effect when facing 3D scenes with domain gap.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Open-Set Object Detection and Discovery. (arXiv:2204.05604v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05604">
<div class="article-summary-box-inner">
<span><p>With the human pursuit of knowledge, open-set object detection (OSOD) has
been designed to identify unknown objects in a dynamic world. However, an issue
with the current setting is that all the predicted unknown objects share the
same category as "unknown", which require incremental learning via a
human-in-the-loop approach to label novel classes. In order to address this
problem, we present a new task, namely Open-Set Object Detection and Discovery
(OSODD). This new task aims to extend the ability of open-set object detectors
to further discover the categories of unknown objects based on their visual
appearance without human effort. We propose a two-stage method that first uses
an open-set object detector to predict both known and unknown objects. Then, we
study the representation of predicted objects in an unsupervised manner and
discover new categories from the set of unknown objects. With this method, a
detector is able to detect objects belonging to known classes and define novel
categories for objects of unknown classes with minimal supervision. We show the
performance of our model on the MS-COCO dataset under a thorough evaluation
protocol. We hope that our work will promote further research towards a more
robust real-world detection system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regression or Classification? Reflection on BP prediction from PPG data using Deep Neural Networks in the scope of practical applications. (arXiv:2204.05605v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05605">
<div class="article-summary-box-inner">
<span><p>Photoplethysmographic (PPG) signals offer diagnostic potential beyond heart
rate analysis or blood oxygen level monitoring. In the recent past, research
focused extensively on non-invasive PPG-based approaches to blood pressure (BP)
estimation. These approaches can be subdivided into regression and
classification methods. The latter assign PPG signals to predefined BP
intervals that represent clinically relevant ranges. The former predict
systolic (SBP) and diastolic (DBP) BP as continuous variables and are of
particular interest to the research community. However, the reported accuracies
of BP regression methods vary widely among publications with some authors even
questioning the feasibility of PPG-based BP regression altogether. In our work,
we compare BP regression and classification approaches. We argue that BP
classification might provide diagnostic value that is equivalent to regression
in many clinically relevant scenarios while being similar or even superior in
terms of performance. We compare several established neural architectures using
publicly available PPG data for SBP regression and classification with and
without personalization using subject-specific data. We found that
classification and regression models perform similar before personalization.
However, after personalization, the accuracy of classification based methods
outperformed regression approaches. We conclude that BP classification might be
preferable over BP regression in certain scenarios where a coarser segmentation
of the BP range is sufficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Predictive Learning from Videos. (arXiv:2204.05624v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05624">
<div class="article-summary-box-inner">
<span><p>Predictive learning ideally builds the world model of physical processes in
one or more given environments. Typical setups assume that we can collect data
from all environments at all times. In practice, however, different prediction
tasks may arrive sequentially so that the environments may change persistently
throughout the training procedure. Can we develop predictive learning
algorithms that can deal with more realistic, non-stationary physical
environments? In this paper, we study a new continual learning problem in the
context of video prediction, and observe that most existing methods suffer from
severe catastrophic forgetting in this setup. To tackle this problem, we
propose the continual predictive learning (CPL) approach, which learns a
mixture world model via predictive experience replay and performs test-time
adaptation with non-parametric task inference. We construct two new benchmarks
based on RoboNet and KTH, in which different tasks correspond to different
physical robotic environments or human actions. Our approach is shown to
effectively mitigate forgetting and remarkably outperform the na\"ive
combinations of previous art in video prediction and continual learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">X-DETR: A Versatile Architecture for Instance-wise Vision-Language Tasks. (arXiv:2204.05626v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05626">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the challenging instance-wise vision-language tasks,
where the free-form language is required to align with the objects instead of
the whole image. To address these tasks, we propose X-DETR, whose architecture
has three major components: an object detector, a language encoder, and
vision-language alignment. The vision and language streams are independent
until the end and they are aligned using an efficient dot-product operation.
The whole network is trained end-to-end, such that the detector is optimized
for the vision-language tasks instead of an off-the-shelf component. To
overcome the limited size of paired object-language annotations, we leverage
other weak types of supervision to expand the knowledge coverage. This simple
yet effective architecture of X-DETR shows good accuracy and fast speeds for
multiple instance-wise vision-language tasks, e.g., 16.4 AP on LVIS detection
of 1.2K categories at ~20 frames per second without using any LVIS annotation
during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Register a Live onto a Liver ? Partial Matching in the Space of Varifolds. (arXiv:2204.05665v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05665">
<div class="article-summary-box-inner">
<span><p>Partial shapes correspondences is a problem that often occurs in computer
vision (occlusion, evolution in time...). In medical imaging, data may come
from different modalities and be acquired under different conditions which
leads to variations in shapes and topologies. In this paper we use an
asymmetric data dissimilarity term applicable to various geometric shapes like
sets of curves or surfaces, assessing the embedding of a shape into another one
without relying on correspondences. It is designed as a data attachment for the
Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework, allowing to
compute a meaningful deformation of one shape onto a subset of the other. We
refine it in order to control the resulting non-rigid deformations and provide
consistent deformations of the shapes along with their ambient space. We show
that partial matching can be used for robust multi-modal liver registration
between a Computed Tomography (CT) volume and a Cone Beam Computed Tomography
(CBCT) volume. The 3D imaging of the patient CBCT at point of care that we call
live is truncated while the CT pre-intervention provides a full visualization
of the liver. The proposed method allows the truncated surfaces from CBCT to be
aligned non-rigidly, yet realistically, with surfaces from CT with an average
distance of 2.6mm(+/- 2.2). The generated deformations extend consistently to
the liver volume, and are evaluated on points of interest for the physicians,
with an average distance of 5.8mm (+/- 2.7) for vessels bifurcations and 5.13mm
(+/- 2.5) for tumors landmarks. Such multi-modality volumes registrations would
help the physicians in the perspective of navigating their tools in the
patient's anatomy to locate structures that are hardly visible in the CBCT used
during their procedures. Our code is available at
https://github.com/plantonsanti/PartialMatchingVarifolds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Three-Stream Joint Network for Zero-Shot Sketch-Based Image Retrieval. (arXiv:2204.05666v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05666">
<div class="article-summary-box-inner">
<span><p>The Zero-Shot Sketch-based Image Retrieval (ZS-SBIR) is a challenging task
because of the large domain gap between sketches and natural images as well as
the semantic inconsistency between seen and unseen categories. Previous
literature bridges seen and unseen categories by semantic embedding, which
requires prior knowledge of the exact class names and additional extraction
efforts. And most works reduce domain gap by mapping sketches and natural
images into a common high-level space using constructed sketch-image pairs,
which ignore the unpaired information between images and sketches. To address
these issues, in this paper, we propose a novel Three-Stream Joint Training
Network (3JOIN) for the ZS-SBIR task. To narrow the domain differences between
sketches and images, we extract edge maps for natural images and treat them as
a bridge between images and sketches, which have similar content to images and
similar style to sketches. For exploiting a sufficient combination of sketches,
natural images, and edge maps, a novel three-stream joint training network is
proposed. In addition, we use a teacher network to extract the implicit
semantics of the samples without the aid of other semantics and transfer the
learned knowledge to unseen classes. Extensive experiments conducted on two
real-world datasets demonstrate the superiority of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3DeformRS: Certifying Spatial Deformations on Point Clouds. (arXiv:2204.05687v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05687">
<div class="article-summary-box-inner">
<span><p>3D computer vision models are commonly used in security-critical applications
such as autonomous driving and surgical robotics. Emerging concerns over the
robustness of these models against real-world deformations must be addressed
practically and reliably. In this work, we propose 3DeformRS, a method to
certify the robustness of point cloud Deep Neural Networks (DNNs) against
real-world deformations. We developed 3DeformRS by building upon recent work
that generalized Randomized Smoothing (RS) from pixel-intensity perturbations
to vector-field deformations. In particular, we specialized RS to certify DNNs
against parameterized deformations (e.g. rotation, twisting), while enjoying
practical computational costs. We leverage the virtues of 3DeformRS to conduct
a comprehensive empirical study on the certified robustness of four
representative point cloud DNNs on two datasets and against seven different
deformations. Compared to previous approaches for certifying point cloud DNNs,
3DeformRS is fast, scales well with point cloud size, and provides
comparable-to-better certificates. For instance, when certifying a plain
PointNet against a 3{\deg} z-rotation on 1024-point clouds, 3DeformRS grants a
certificate 3x larger and 20x faster than previous work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Super-Resolution for Selfie Biometrics: Introduction and Application to Face and Iris. (arXiv:2204.05688v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05688">
<div class="article-summary-box-inner">
<span><p>The lack of resolution has a negative impact on the performance of
image-based biometrics. Many applications which are becoming ubiquitous in
mobile devices do not operate in a controlled environment, and their
performance significantly drops due to the lack of pixel resolution. While many
generic super-resolution techniques have been studied to restore low-resolution
images for biometrics, the results obtained are not always as desired. Those
generic methods are usually aimed to enhance the visual appearance of the
scene. However, producing an overall visual enhancement of biometric images
does not necessarily correlate with a better recognition performance. Such
techniques are designed to restore generic images and therefore do not exploit
the specific structure found in biometric images (e.g. iris or faces), which
causes the solution to be sub-optimal. For this reason, super-resolution
techniques have to be adapted for the particularities of images from a specific
biometric modality. In recent years, there has been an increased interest in
the application of super-resolution to different biometric modalities, such as
face iris, gait or fingerprint. This chapter presents an overview of recent
advances in super-resolution reconstruction of face and iris images, which are
the two prevalent modalities in selfie biometrics. We also provide experimental
results using several state-of-the-art reconstruction algorithms, demonstrating
the benefits of using super-resolution to improve the quality of face and iris
images prior to classification. In the reported experiments, we study the
application of super-resolution to face and iris images captured in the visible
range, using experimental setups that represent well the selfie biometrics
scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Anomaly and Change Detection with Multivariate Gaussianization. (arXiv:2204.05699v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05699">
<div class="article-summary-box-inner">
<span><p>Anomaly detection is a field of intense research. Identifying low probability
events in data/images is a challenging problem given the high-dimensionality of
the data, especially when no (or little) information about the anomaly is
available a priori. While plenty of methods are available, the vast majority of
them do not scale well to large datasets and require the choice of some (very
often critical) hyperparameters. Therefore, unsupervised and computationally
efficient detection methods become strictly necessary. We propose an
unsupervised method for detecting anomalies and changes in remote sensing
images by means of a multivariate Gaussianization methodology that allows to
estimate multivariate densities accurately, a long-standing problem in
statistics and machine learning. The methodology transforms arbitrarily complex
multivariate data into a multivariate Gaussian distribution. Since the
transformation is differentiable, by applying the change of variables formula
one can estimate the probability at any point of the original domain. The
assumption is straightforward: pixels with low estimated probability are
considered anomalies. Our method can describe any multivariate distribution,
makes an efficient use of memory and computational resources, and is
parameter-free. We show the efficiency of the method in experiments involving
both anomaly detection and change detection in different remote sensing image
sets. Results show that our approach outperforms other linear and nonlinear
methods in terms of detection power in both anomaly and change detection
scenarios, showing robustness and scalability to dimensionality and sample
sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Back to the Roots: Reconstructing Large and Complex Cranial Defects using an Image-based Statistical Shape Model. (arXiv:2204.05703v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05703">
<div class="article-summary-box-inner">
<span><p>Designing implants for large and complex cranial defects is a challenging
task, even for professional designers. Current efforts on automating the design
process focused mainly on convolutional neural networks (CNN), which have
produced state-of-the-art results on reconstructing synthetic defects. However,
existing CNN-based methods have been difficult to translate to clinical
practice in cranioplasty, as their performance on complex and irregular cranial
defects remains unsatisfactory. In this paper, a statistical shape model (SSM)
built directly on the segmentation masks of the skulls is presented. We
evaluate the SSM on several cranial implant design tasks, and the results show
that, while the SSM performs suboptimally on synthetic defects compared to
CNN-based approaches, it is capable of reconstructing large and complex defects
with only minor manual corrections. The quality of the resulting implants is
examined and assured by experienced neurosurgeons. In contrast, CNN-based
approaches, even with massive data augmentation, fail or produce
less-than-satisfactory implants for these cases. Codes are publicly available
at https://github.com/Jianningli/ssm
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GARF: Gaussian Activated Radiance Fields for High Fidelity Reconstruction and Pose Estimation. (arXiv:2204.05735v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05735">
<div class="article-summary-box-inner">
<span><p>Despite Neural Radiance Fields (NeRF) showing compelling results in
photorealistic novel views synthesis of real-world scenes, most existing
approaches require accurate prior camera poses. Although approaches for jointly
recovering the radiance field and camera pose exist (BARF), they rely on a
cumbersome coarse-to-fine auxiliary positional embedding to ensure good
performance. We present Gaussian Activated neural Radiance Fields (GARF), a new
positional embedding-free neural radiance field architecture - employing
Gaussian activations - that outperforms the current state-of-the-art in terms
of high fidelity reconstruction and pose estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LifeLonger: A Benchmark for Continual Disease Classification. (arXiv:2204.05737v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05737">
<div class="article-summary-box-inner">
<span><p>Deep learning models have shown a great effectiveness in recognition of
findings in medical images. However, they cannot handle the ever-changing
clinical environment, bringing newly annotated medical data from different
sources. To exploit the incoming streams of data, these models would benefit
largely from sequentially learning from new samples, without forgetting the
previously obtained knowledge. In this paper we introduce LifeLonger, a
benchmark for continual disease classification on the MedMNIST collection, by
applying existing state-of-the-art continual learning methods. In particular,
we consider three continual learning scenarios, namely, task and class
incremental learning and the newly defined cross-domain incremental learning.
Task and class incremental learning of diseases address the issue of
classifying new samples without re-training the models from scratch, while
cross-domain incremental learning addresses the issue of dealing with datasets
originating from different institutions while retaining the previously obtained
knowledge. We perform a thorough analysis of the performance and examine how
the well-known challenges of continual learning, such as the catastrophic
forgetting exhibit themselves in this setting. The encouraging results
demonstrate that continual learning has a major potential to advance disease
classification and to produce a more robust and efficient learning framework
for clinical settings. The code repository, data partitions and baseline
results for the complete benchmark will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Examining the Proximity of Adversarial Examples to Class Manifolds in Deep Networks. (arXiv:2204.05764v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05764">
<div class="article-summary-box-inner">
<span><p>Deep neural networks achieve remarkable performance in multiple fields.
However, after proper training they suffer from an inherent vulnerability
against adversarial examples (AEs). In this work we shed light on inner
representations of the AEs by analysing their activations on the hidden layers.
We test various types of AEs, each crafted using a specific norm constraint,
which affects their visual appearance and eventually their behavior in the
trained networks. Our results in image classification tasks (MNIST and
CIFAR-10) reveal qualitative differences between the individual types of AEs,
when comparing their proximity to the class-specific manifolds on the inner
representations. We propose two methods that can be used to compare the
distances to class-specific manifolds, regardless of the changing dimensions
throughout the network. Using these methods, we consistently confirm that some
of the adversarials do not necessarily leave the proximity of the manifold of
the correct class, not even in the last hidden layer of the neural network.
Next, using UMAP visualisation technique, we project the class activations to
2D space. The results indicate that the activations of the individual AEs are
entangled with the activations of the test set. This, however, does not hold
for a group of crafted inputs called the rubbish class. We also confirm the
entanglement of adversarials with the test set numerically using the soft
nearest neighbour loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GORDA: Graph-based ORientation Distribution Analysis of SLI scatterometry Patterns of Nerve Fibres. (arXiv:2204.05776v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05776">
<div class="article-summary-box-inner">
<span><p>Scattered Light Imaging (SLI) is a novel approach for microscopically
revealing the fibre architecture of unstained brain sections. The measurements
are obtained by illuminating brain sections from different angles and measuring
the transmitted (scattered) light under normal incidence. The evaluation of
scattering profiles commonly relies on a peak picking technique and feature
extraction from the peaks, which allows quantitative determination of parallel
and crossing in-plane nerve fibre directions for each image pixel. However, the
estimation of the 3D orientation of the fibres cannot be assessed with the
traditional methodology. We propose an unsupervised learning approach using
spherical convolutions for estimating the 3D orientation of neural fibres,
resulting in a more detailed interpretation of the fibre orientation
distributions in the brain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Anomaly Detection in 3D Brain MRI using Deep Learning with impured training data. (arXiv:2204.05778v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05778">
<div class="article-summary-box-inner">
<span><p>The detection of lesions in magnetic resonance imaging (MRI)-scans of human
brains remains challenging, time-consuming and error-prone. Recently,
unsupervised anomaly detection (UAD) methods have shown promising results for
this task. These methods rely on training data sets that solely contain healthy
samples. Compared to supervised approaches, this significantly reduces the need
for an extensive amount of labeled training data. However, data labelling
remains error-prone. We study how unhealthy samples within the training data
affect anomaly detection performance for brain MRI-scans. For our evaluations,
we consider three publicly available data sets and use autoencoders (AE) as a
well-established baseline method for UAD. We systematically evaluate the effect
of impured training data by injecting different quantities of unhealthy samples
to our training set of healthy samples from T1-weighted MRI-scans. We evaluate
a method to identify falsely labeled samples directly during training based on
the reconstruction error of the AE. Our results show that training with impured
data decreases the UAD performance notably even with few falsely labeled
samples. By performing outlier removal directly during training based on the
reconstruction-loss, we demonstrate that falsely labeled data can be detected
and removed to mitigate the effect of falsely labeled data. Overall, we
highlight the importance of clean data sets for UAD in brain MRI and
demonstrate an approach for detecting falsely labeled data directly during
training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Breast Cancer Classification via Hypercomplex Neural Networks. (arXiv:2204.05798v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05798">
<div class="article-summary-box-inner">
<span><p>Traditionally, deep learning-based methods for breast cancer classification
perform a single-view analysis. However, radiologists simultaneously analyze
all four views that compose a mammography exam, owing to the correlations
contained in mammography views, which present crucial information for
identifying tumors. In light of this, some studies have started to propose
multi-view methods. Nevertheless, in such existing architectures, mammogram
views are processed as independent images by separate convolutional branches,
thus losing correlations among them. To overcome such limitations, in this
paper we propose a novel approach for multi-view breast cancer classification
based on parameterized hypercomplex neural networks. Thanks to hypercomplex
algebra properties, our networks are able to model, and thus leverage, existing
correlations between the different views that comprise a mammogram exam, thus
mimicking the reading process performed by clinicians. As a consequence, the
proposed method is able to handle the information of a patient altogether
without breaking the multi-view nature of the exam. Starting from the proposed
hypercomplex approach, we define architectures designed to process two-view
exams, namely PHResNets, and four-view exams, i.e., PHYSEnet and PHYSBOnet,
with the ability to grasp inter-view correlations in a wide range of clinical
use cases. Through an extensive experimental evaluation conducted with two
publicly available datasets, CBIS-DDSM and INbreast, we demonstrate that our
parameterized hypercomplex models clearly outperform real-valued counterparts
and also state-of-the-art methods, proving that breast cancer classification
benefits from the proposed multi-view architecture. Full code and pretrained
models for complete reproducibility of our experiments are freely available at:
https://github.com/ispamm/PHBreast.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EVOPS Benchmark: Evaluation of Plane Segmentation from RGBD and LiDAR Data. (arXiv:2204.05799v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05799">
<div class="article-summary-box-inner">
<span><p>This paper provides the EVOPS dataset for plane segmentation from 3D data,
both from RGBD images and LiDAR point clouds (PC). We have designed two
annotation methodologies (RGBD and LiDAR) running on well-known and widely-used
datasets and we have provided a complete set of benchmarking tools including
point, planes and segmentation metrics. The data includes a total number of 10k
RGBD and 7K LiDAR frames over different selected scenes which consist of high
quality segmented planes. The experiments report quality of SOTA methods for
RGBD plane segmentation on our annotated data. All labeled data and benchmark
tools used have been made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Few-shot Entity Recognition in Document Images: A Label-aware Sequence-to-Sequence Framework. (arXiv:2204.05819v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05819">
<div class="article-summary-box-inner">
<span><p>Entity recognition is a fundamental task in understanding document images.
Traditional sequence labeling frameworks treat the entity types as class IDs
and rely on extensive data and high-quality annotations to learn semantics
which are typically expensive in practice. In this paper, we aim to build an
entity recognition model requiring only a few shots of annotated document
images. To overcome the data limitation, we propose to leverage the label
surface names to better inform the model of the target entity type semantics
and also embed the labels into the spatial embedding space to capture the
spatial correspondence between regions and labels. Specifically, we go beyond
sequence labeling and develop a novel label-aware seq2seq framework, LASER. The
proposed model follows a new labeling scheme that generates the label surface
names word-by-word explicitly after generating the entities. During training,
LASER refines the label semantics by updating the label surface name
representations and also strengthens the label-region correlation. In this way,
LASER recognizes the entities from document images through both semantic and
layout correspondence. Extensive experiments on two benchmark datasets
demonstrate the superiority of LASER under the few-shot setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Cross-Attention-Driven Spatial-Spectral Graph Convolutional Network for Hyperspectral Image Classification. (arXiv:2204.05823v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05823">
<div class="article-summary-box-inner">
<span><p>Recently, graph convolutional networks (GCNs) have been developed to explore
spatial relationship between pixels, achieving better classification
performance of hyperspectral images (HSIs). However, these methods fail to
sufficiently leverage the relationship between spectral bands in HSI data. As
such, we propose an adaptive cross-attention-driven spatial-spectral graph
convolutional network (ACSS-GCN), which is composed of a spatial GCN (Sa-GCN)
subnetwork, a spectral GCN (Se-GCN) subnetwork, and a graph cross-attention
fusion module (GCAFM). Specifically, Sa-GCN and Se-GCN are proposed to extract
the spatial and spectral features by modeling correlations between spatial
pixels and between spectral bands, respectively. Then, by integrating attention
mechanism into information aggregation of graph, the GCAFM, including three
parts, i.e., spatial graph attention block, spectral graph attention block, and
fusion block, is designed to fuse the spatial and spectral features and
suppress noise interference in Sa-GCN and Se-GCN. Moreover, the idea of the
adaptive graph is introduced to explore an optimal graph through back
propagation during the training process. Experiments on two HSI data sets show
that the proposed method achieves better performance than other classification
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Negative Replay for Continual Learning. (arXiv:2204.05842v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05842">
<div class="article-summary-box-inner">
<span><p>Learning continually is a key aspect of intelligence and a necessary ability
to solve many real-life problems. One of the most effective strategies to
control catastrophic forgetting, the Achilles' heel of continual learning, is
storing part of the old data and replaying them interleaved with new
experiences (also known as the replay approach). Generative replay, which is
using generative models to provide replay patterns on demand, is particularly
intriguing, however, it was shown to be effective mainly under simplified
assumptions, such as simple scenarios and low-dimensional data. In this paper,
we show that, while the generated data are usually not able to improve the
classification accuracy for the old classes, they can be effective as negative
examples (or antagonists) to better learn the new classes, especially when the
learning experiences are small and contain examples of just one or few classes.
The proposed approach is validated on complex class-incremental and
data-incremental continual learning scenarios (CORe50 and ImageNet-1000)
composed of high-dimensional data and a large number of training experiences: a
setup where existing generative replay approaches usually fail.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistic Compositional Embeddings for Multimodal Image Retrieval. (arXiv:2204.05845v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05845">
<div class="article-summary-box-inner">
<span><p>Existing works in image retrieval often consider retrieving images with one
or two query inputs, which do not generalize to multiple queries. In this work,
we investigate a more challenging scenario for composing multiple multimodal
queries in image retrieval. Given an arbitrary number of query images and (or)
texts, our goal is to retrieve target images containing the semantic concepts
specified in multiple multimodal queries. To learn an informative embedding
that can flexibly encode the semantics of various queries, we propose a novel
multimodal probabilistic composer (MPC). Specifically, we model input images
and texts as probabilistic embeddings, which can be further composed by a
probabilistic composition rule to facilitate image retrieval with multiple
multimodal queries. We propose a new benchmark based on the MS-COCO dataset and
evaluate our model on various setups that compose multiple images and (or) text
queries for multimodal image retrieval. Without bells and whistles, we show
that our probabilistic model formulation significantly outperforms existing
related methods on multimodal image retrieval while generalizing well to query
with different amounts of inputs given in arbitrary visual and (or) textual
modalities. Code is available here: https://github.com/andreineculai/MPC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DCMS: Motion Forecasting with Dual Consistency and Multi-Pseudo-Target Supervision. (arXiv:2204.05859v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05859">
<div class="article-summary-box-inner">
<span><p>We present a novel framework for motion forecasting with Dual Consistency
Constraints and Multi-Pseudo-Target supervision. The motion forecasting task
predicts future trajectories of vehicles by incorporating spatial and temporal
information from the past. A key design of DCMS is the proposed Dual
Consistency Constraints that regularize the predicted trajectories under
spatial and temporal perturbation during the training stage. In addition, we
design a novel self-ensembling scheme to obtain accurate pseudo targets to
model the multi-modality in motion forecasting through supervision with
multiple targets explicitly, namely Multi-Pseudo-Target supervision. Our
experimental results on the Argoverse motion forecasting benchmark show that
DCMS significantly outperforms the state-of-the-art methods, achieving 1st
place on the leaderboard. We also demonstrate that our proposed strategies can
be incorporated into other motion forecasting approaches as general training
schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic keypoint-based pose estimation from single RGB frames. (arXiv:2204.05864v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05864">
<div class="article-summary-box-inner">
<span><p>This paper presents an approach to estimating the continuous 6-DoF pose of an
object from a single RGB image. The approach combines semantic keypoints
predicted by a convolutional network (convnet) with a deformable shape model.
Unlike prior investigators, we are agnostic to whether the object is textured
or textureless, as the convnet learns the optimal representation from the
available training-image data. Furthermore, the approach can be applied to
instance- and class-based pose recovery. Additionally, we accompany our main
pipeline with a technique for semi-automatic data generation from unlabeled
videos. This procedure allows us to train the learnable components of our
method with minimal manual intervention in the labeling process. Empirically,
we show that our approach can accurately recover the 6-DoF object pose for both
instance- and class-based scenarios even against a cluttered background. We
apply our approach both to several, existing, large-scale datasets - including
PASCAL3D+, LineMOD-Occluded, YCB-Video, and TUD-Light - and, using our labeling
pipeline, to a new dataset with novel object classes that we introduce here.
Extensive empirical evaluations show that our approach is able to provide pose
estimation results comparable to the state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Event Camera-based Odometry for Planetary Robots. (arXiv:2204.05880v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05880">
<div class="article-summary-box-inner">
<span><p>Due to their resilience to motion blur and high robustness in low-light and
high dynamic range conditions, event cameras are poised to become enabling
sensors for vision-based exploration on future Mars helicopter missions.
However, existing event-based visual-inertial odometry (VIO) algorithms either
suffer from high tracking errors or are brittle, since they cannot cope with
significant depth uncertainties caused by an unforeseen loss of tracking or
other effects. In this work, we introduce EKLT-VIO, which addresses both
limitations by combining a state-of-the-art event-based frontend with a
filter-based backend. This makes it both accurate and robust to uncertainties,
outperforming event- and frame-based VIO algorithms on challenging benchmarks
by 32%. In addition, we demonstrate accurate performance in hover-like
conditions (outperforming existing event-based methods) as well as high
robustness in newly collected Mars-like and high-dynamic-range sequences, where
existing frame-based methods fail. In doing so, we show that event-based VIO is
the way forward for vision-based exploration on Mars.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisCUIT: Visual Auditor for Bias in CNN Image Classifier. (arXiv:2204.05899v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05899">
<div class="article-summary-box-inner">
<span><p>CNN image classifiers are widely used, thanks to their efficiency and
accuracy. However, they can suffer from biases that impede their practical
applications. Most existing bias investigation techniques are either
inapplicable to general image classification tasks or require significant user
efforts in perusing all data subgroups to manually specify which data
attributes to inspect. We present VisCUIT, an interactive visualization system
that reveals how and why a CNN classifier is biased. VisCUIT visually
summarizes the subgroups on which the classifier underperforms and helps users
discover and characterize the cause of the underperformances by revealing image
concepts responsible for activating neurons that contribute to
misclassifications. VisCUIT runs in modern browsers and is open-source,
allowing people to easily access and extend the tool to other model
architectures and datasets. VisCUIT is available at the following public demo
link: https://poloclub.github.io/VisCUIT. A video demo is available at
https://youtu.be/eNDbSyM4R_4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label Distribution Learning for Generalizable Multi-source Person Re-identification. (arXiv:2204.05903v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05903">
<div class="article-summary-box-inner">
<span><p>Person re-identification (Re-ID) is a critical technique in the video
surveillance system, which has achieved significant success in the supervised
setting. However, it is difficult to directly apply the supervised model to
arbitrary unseen domains due to the domain gap between the available source
domains and unseen target domains. In this paper, we propose a novel label
distribution learning (LDL) method to address the generalizable multi-source
person Re-ID task (i.e., there are multiple available source domains, and the
testing domain is unseen during training), which aims to explore the relation
of different classes and mitigate the domain-shift across different domains so
as to improve the discrimination of the model and learn the domain-invariant
feature, simultaneously. Specifically, during the training process, we produce
the label distribution via the online manner to mine the relation information
of different classes, thus it is beneficial for extracting the discriminative
feature. Besides, for the label distribution of each class, we further revise
it to give more and equal attention to the other domains that the class does
not belong to, which can effectively reduce the domain gap across different
domains and obtain the domain-invariant feature. Furthermore, we also give the
theoretical analysis to demonstrate that the proposed method can effectively
deal with the domain-shift issue. Extensive experiments on multiple benchmark
datasets validate the effectiveness of the proposed method and show that the
proposed method can outperform the state-of-the-art methods. Besides, further
analysis also reveals the superiority of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Forgery Detection via Guided Adversarial Interpolation. (arXiv:2204.05905v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05905">
<div class="article-summary-box-inner">
<span><p>Realistic visual media synthesis is becoming a critical societal issue with
the surge of face manipulation models; new forgery approaches emerge at an
unprecedented pace. Unfortunately, existing forgery detection methods suffer
significant performance drops when applied to novel forgery approaches. In this
work, we address the few-shot forgery detection problem by designing a
comprehensive benchmark based on coverage analysis among various forgery
approaches, and proposing Guided Adversarial Interpolation (GAI). Our key
insight is that there exist transferable distribution characteristics among
different forgery approaches with the majority and minority classes.
Specifically, we enhance the discriminative ability against novel forgery
approaches via adversarially interpolating the artifacts of the minority
samples to the majority samples under the guidance of a teacher network. Unlike
the standard re-balancing method which usually results in over-fitting to
minority classes, our method simultaneously takes account of the diversity of
majority information as well as the significance of minority information.
Extensive experiments demonstrate that our GAI achieves state-of-the-art
performances on the established few-shot forgery detection benchmark. Notably,
our method is also validated to be robust to choices of majority and minority
forgery approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Arch-Graph: Acyclic Architecture Relation Predictor for Task-Transferable Neural Architecture Search. (arXiv:2204.05941v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05941">
<div class="article-summary-box-inner">
<span><p>Neural Architecture Search (NAS) aims to find efficient models for multiple
tasks. Beyond seeking solutions for a single task, there are surging interests
in transferring network design knowledge across multiple tasks. In this line of
research, effectively modeling task correlations is vital yet highly neglected.
Therefore, we propose \textbf{Arch-Graph}, a transferable NAS method that
predicts task-specific optimal architectures with respect to given task
embeddings. It leverages correlations across multiple tasks by using their
embeddings as a part of the predictor's input for fast adaptation. We also
formulate NAS as an architecture relation graph prediction problem, with the
relational graph constructed by treating candidate architectures as nodes and
their pairwise relations as edges. To enforce some basic properties such as
acyclicity in the relational graph, we add additional constraints to the
optimization process, converting NAS into the problem of finding a Maximal
Weighted Acyclic Subgraph (MWAS). Our algorithm then strives to eliminate
cycles and only establish edges in the graph if the rank results can be
trusted. Through MWAS, Arch-Graph can effectively rank candidate models for
each task with only a small budget to finetune the predictor. With extensive
experiments on TransNAS-Bench-101, we show Arch-Graph's transferability and
high sample efficiency across numerous tasks, beating many NAS methods designed
for both single-task and multi-task search. It is able to find top 0.16\% and
0.29\% architectures on average on two search spaces under the budget of only
50 models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RL-CoSeg : A Novel Image Co-Segmentation Algorithm with Deep Reinforcement Learning. (arXiv:2204.05951v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05951">
<div class="article-summary-box-inner">
<span><p>This paper proposes an automatic image co-segmentation algorithm based on
deep reinforcement learning (RL). Existing co-segmentation tasks mainly rely on
deep learning methods, and the obtained foreground edges are often rough. In
order to obtain more precise foreground edges, we use deep RL to solve this
problem and achieve the finer segmentation. To our best knowledge, this is the
first work to apply RL methods to co-segmentation. We define the problem as a
Markov Decision Process (MDP) and optimize it by RL with asynchronous advantage
actor-critic (A3C). The RL image co-segmentation network uses the correlation
between images to segment common and salient objects from a set of related
images. In order to achieve automatic segmentation, our RL-CoSeg method
eliminates user's hints. For the image co-segmentation problem, we propose a
collaborative RL algorithm based on the A3C model. We propose a Siamese RL
co-segmentation network structure to obtain the co-attention of images for
co-segmentation. We improve the self-attention for automatic RL algorithm to
obtain long-distance dependence and enlarge the receptive field. The image
feature information obtained by self-attention can be used to supplement the
deleted user's hints and help to obtain more accurate actions. Experimental
results have shown that our method can improve the performance effectively on
both coarse and fine initial segmentations, and it achieves the
state-of-the-art performance on Internet dataset, iCoseg dataset and MLMR-COS
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Localization Distillation for Object Detection. (arXiv:2204.05957v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05957">
<div class="article-summary-box-inner">
<span><p>Previous knowledge distillation (KD) methods for object detection mostly
focus on feature imitation instead of mimicking the classification logits due
to its inefficiency in distilling the localization information. In this paper,
we investigate whether logit mimicking always lags behind feature imitation.
Towards this goal, we first present a novel localization distillation (LD)
method which can efficiently transfer the localization knowledge from the
teacher to the student. Second, we introduce the concept of valuable
localization region that can aid to selectively distill the classification and
localization knowledge for a certain region. Combining these two new
components, for the first time, we show that logit mimicking can outperform
feature imitation and the absence of localization distillation is a critical
reason for why logit mimicking underperforms for years. The thorough studies
exhibit the great potential of logit mimicking that can significantly alleviate
the localization ambiguity, learn robust feature representation, and ease the
training difficulty in the early stage. We also provide the theoretical
connection between the proposed LD and the classification KD, that they share
the equivalent optimization effect. Our distillation scheme is simple as well
as effective and can be easily applied to both dense horizontal object
detectors and rotated object detectors. Extensive experiments on the MS COCO,
PASCAL VOC, and DOTA benchmarks demonstrate that our method can achieve
considerable AP improvement without any sacrifice on the inference speed. Our
source code and pretrained models are publicly available at
https://github.com/HikariTJU/LD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Captioning: a comparative review of where we are and which could be the route. (arXiv:2204.05976v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05976">
<div class="article-summary-box-inner">
<span><p>Video captioning is the process of describing the content of a sequence of
images capturing its semantic relationships and meanings. Dealing with this
task with a single image is arduous, not to mention how difficult it is for a
video (or images sequence). The amount and relevance of the applications of
video captioning are vast, mainly to deal with a significant amount of video
recordings in video surveillance, or assisting people visually impaired, to
mention a few. To analyze where the efforts of our community to solve the video
captioning task are, as well as what route could be better to follow, this
manuscript presents an extensive review of more than 105 papers for the period
of 2016 to 2021. As a result, the most-used datasets and metrics are
identified. Also, the main approaches used and the best ones. We compute a set
of rankings based on several performance metrics to obtain, according to its
performance, the best method with the best result on the video captioning task.
Finally, some insights are concluded about which could be the next steps or
opportunity areas to improve dealing with this complex task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optical flow GNSS for navigation in the Indian subcontinent (NavIC). (arXiv:2204.05980v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05980">
<div class="article-summary-box-inner">
<span><p>This paper reveals about global navigation satellite system GNSS in the
indian subcontinent known as the navigation in the indian subcontinent(NavIC)
We have tried to model a new technique in GNSS known as the optical flow
tracking global navigation system (OF GNSS). This method using differential
equations is very accurate for very small distances on the surface of the earth
in the 1500km range of the Indian subcontinent satellite coverage. When we talk
of accuracy of the GPS system it should be very accurate on the surface of the
earth when used to show changes in coordinate of the moving body with respect
to the ground by the satellite which is situated on the earths orbit. Optical
flow is a method which uses movements with respect to x and y axis for
infinitesimal changes in its coordinates and then uses this algorithm to use it
in global positioning system to find accurate position of the body with respect
to the satellite coordinates with respect to ground positioning. The modern
method of differential frames is also very accurate as it involves
infinitesimal frames which are modelled together from the satellite to find
changes in the coordinates on the earths surface, so we have designed a new
algorithm in this paper on the Optical flow GNSS system which is an alternative
and can improve the study done in the design of these algorithms in this field
of applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparison Analysis of Traditional Machine Learning and Deep Learning Techniques for Data and Image Classification. (arXiv:2204.05983v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05983">
<div class="article-summary-box-inner">
<span><p>The purpose of the study is to analyse and compare the most common machine
learning and deep learning techniques used for computer vision 2D object
classification tasks. Firstly, we will present the theoretical background of
the Bag of Visual words model and Deep Convolutional Neural Networks (DCNN).
Secondly, we will implement a Bag of Visual Words model, the VGG16 CNN
Architecture. Thirdly, we will present our custom and novice DCNN in which we
test the aforementioned implementations on a modified version of the Belgium
Traffic Sign dataset. Our results showcase the effects of hyperparameters on
traditional machine learning and the advantage in terms of accuracy of DCNNs
compared to classical machine learning methods. As our tests indicate, our
proposed solution can achieve similar - and in some cases better - results than
existing DCNNs architectures. Finally, the technical merit of this article lies
in the presented computationally simpler DCNN architecture, which we believe
can pave the way towards using more efficient architectures for basic tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning Security against Data Poisoning: Are We There Yet?. (arXiv:2204.05986v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05986">
<div class="article-summary-box-inner">
<span><p>The recent success of machine learning has been fueled by the increasing
availability of computing power and large amounts of data in many different
applications. However, the trustworthiness of the resulting models can be
compromised when such data is maliciously manipulated to mislead the learning
process. In this article, we first review poisoning attacks that compromise the
training data used to learn machine-learning models, including attacks that aim
to reduce the overall performance, manipulate the predictions on specific test
samples, and even implant backdoors in the model. We then discuss how to
mitigate these attacks before, during, and after model training. We conclude
our article by formulating some relevant open challenges which are hindering
the development of testing methods and benchmarks suitable for assessing and
improving the trustworthiness of machine-learning models against data poisoning
attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension. (arXiv:2204.05991v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05991">
<div class="article-summary-box-inner">
<span><p>Training a referring expression comprehension (ReC) model for a new visual
domain requires collecting referring expressions, and potentially corresponding
bounding boxes, for images in the domain. While large-scale pre-trained models
are useful for image classification across domains, it remains unclear if they
can be applied in a zero-shot manner to more complex tasks like ReC. We present
ReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a
state-of-the-art large-scale model, for ReC. Motivated by the close connection
between ReC and CLIP's contrastive pre-training objective, the first component
of ReCLIP is a region-scoring method that isolates object proposals via
cropping and blurring, and passes them to CLIP. However, through controlled
experiments on a synthetic dataset, we find that CLIP is largely incapable of
performing spatial reasoning off-the-shelf. Thus, the second component of
ReCLIP is a spatial relation resolver that handles several types of spatial
relations. We reduce the gap between zero-shot baselines from prior work and
supervised models by as much as 29% on RefCOCOg, and on RefGTA (video game
imagery), ReCLIP's relative improvement over supervised ReC models trained on
real images is 8%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Malceiver: Perceiver with Hierarchical and Multi-modal Features for Android Malware Detection. (arXiv:2204.05994v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05994">
<div class="article-summary-box-inner">
<span><p>We propose the Malceiver, a hierarchical Perceiver model for Android malware
detection that makes use of multi-modal features. The primary inputs are the
opcode sequence and the requested permissions of a given Android APK file. To
reach a malware classification decision the model combines hierarchical
features extracted from the opcode sequence together with the requested
permissions. The model's architecture is based on the Perceiver/PerceiverIO
which allows for very long opcode sequences to be processed efficiently. Our
proposed model can be easily extended to use multi-modal features. We show
experimentally that this model outperforms a conventional CNN architecture for
opcode sequence based malware detection. We then show that using additional
modalities improves performance. Our proposed architecture opens new avenues
for the use of Transformer-style networks in malware research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">They are Not Completely Useless: Towards Recycling Transferable Unlabeled Data for Class-Mismatched Semi-Supervised Learning. (arXiv:2011.13529v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.13529">
<div class="article-summary-box-inner">
<span><p>Semi-Supervised Learning (SSL) with mismatched classes deals with the problem
that the classes-of-interests in the limited labeled data is only a subset of
the classes in massive unlabeled data. As a result, the classes only possessed
by the unlabeled data may mislead the classifier training and thus hindering
the realistic landing of various SSL methods. To solve this problem, existing
methods usually divide unlabeled data to in-distribution (ID) data and
out-of-distribution (OOD) data, and directly discard or weaken the OOD data to
avoid their adverse impact. In other words, they treat OOD data as completely
useless and thus the potential valuable information for classification
contained by them is totally ignored. To remedy this defect, this paper
proposes a "Transferable OOD data Recycling" (TOOR) method which properly
utilizes ID data as well as the "recyclable" OOD data to enrich the information
for conducting class-mismatched SSL. Specifically, TOOR firstly attributes all
unlabeled data to ID data or OOD data, among which the ID data are directly
used for training. Then we treat the OOD data that have a close relationship
with ID data and labeled data as recyclable, and employ adversarial domain
adaptation to project them to the space of ID data and labeled data. In other
words, the recyclability of an OOD datum is evaluated by its transferability,
and the recyclable OOD data are transferred so that they are compatible with
the distribution of known classes-of-interests. Consequently, our TOOR method
extracts more information from unlabeled data than existing approaches, so it
can achieve the improved performance which is demonstrated by the experiments
on typical benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CMS-LSTM: Context Embedding and Multi-Scale Spatiotemporal Expression LSTM for Predictive Learning. (arXiv:2102.03586v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03586">
<div class="article-summary-box-inner">
<span><p>Spatiotemporal predictive learning (ST-PL) is a hotspot with numerous
applications, such as object movement and meteorological prediction. It aims at
predicting the subsequent frames via observed sequences. However, inherent
uncertainty among consecutive frames exacerbates the difficulty in long-term
prediction. To tackle the increasing ambiguity during forecasting, we design
CMS-LSTM to focus on context correlations and multi-scale spatiotemporal flow
with details on fine-grained locals, containing two elaborate designed blocks:
Context Embedding (CE) and Spatiotemporal Expression (SE) blocks. CE is
designed for abundant context interactions, while SE focuses on multi-scale
spatiotemporal expression in hidden states. The newly introduced blocks also
facilitate other spatiotemporal models (e.g., PredRNN, SA-ConvLSTM) to produce
representative implicit features for ST-PL and improve prediction quality.
Qualitative and quantitative experiments demonstrate the effectiveness and
flexibility of our proposed method. With fewer params, CMS-LSTM outperforms
state-of-the-art methods in numbers of metrics on two representative benchmarks
and scenarios. Code is available at https://github.com/czh-98/CMS-LSTM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Federated Peer Learning for Skin Lesion Classification. (arXiv:2103.03703v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03703">
<div class="article-summary-box-inner">
<span><p>Globally, Skin carcinoma is among the most lethal diseases. Millions of
people are diagnosed with this cancer every year. Sill, early detection can
decrease the medication cost and mortality rate substantially. The recent
improvement in automated cancer classification using deep learning methods has
reached a human-level performance requiring a large amount of annotated data
assembled in one location, yet, finding such conditions usually is not
feasible. Recently, federated learning (FL) has been proposed to train
decentralized models in a privacy-preserved fashion depending on labeled data
at the client-side, which is usually not available and costly. To address this,
we propose \verb!FedPerl!, a semi-supervised federated learning method. Our
method is inspired by peer learning from educational psychology and ensemble
averaging from committee machines. FedPerl builds communities based on clients'
similarities. Then it encourages communities members to learn from each other
to generate more accurate pseudo labels for the unlabeled data. We also
proposed the peer anonymization (PA) technique to anonymize clients. As a core
component of our method, PA is orthogonal to other methods without additional
complexity and reduces the communication cost while enhancing performance.
Finally, we propose a dynamic peer-learning policy that controls the learning
stream to avoid any degradation in the performance, especially for individual
clients. Our experimental setup consists of 71,000 skin lesion images collected
from 5 publicly available datasets. We test our method in four different
scenarios in SSFL. With few annotated data, FedPerl is on par with a
state-of-the-art method in skin lesion classification in the standard setup
while outperforming SSFLs and the baselines by 1.8% and 15.8%, respectively.
Also, it generalizes better to unseen clients while being less sensitive to
noisy ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Staircase Sign Method for Boosting Adversarial Attacks. (arXiv:2104.09722v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09722">
<div class="article-summary-box-inner">
<span><p>Crafting adversarial examples for the transfer-based attack is challenging
and remains a research hot spot. Currently, such attack methods are based on
the hypothesis that the substitute model and the victim model learn similar
decision boundaries, and they conventionally apply Sign Method (SM) to
manipulate the gradient as the resultant perturbation. Although SM is
efficient, it only extracts the sign of gradient units but ignores their value
difference, which inevitably leads to a deviation. Therefore, we propose a
novel Staircase Sign Method (S$^2$M) to alleviate this issue, thus boosting
attacks. Technically, our method heuristically divides the gradient sign into
several segments according to the values of the gradient units, and then
assigns each segment with a staircase weight for better crafting adversarial
perturbation. As a result, our adversarial examples perform better in both
white-box and black-box manner without being more visible. Since S$^2$M just
manipulates the resultant gradient, our method can be generally integrated into
the family of FGSM algorithms, and the computational overhead is negligible.
Extensive experiments on the ImageNet dataset demonstrate the effectiveness of
our proposed methods, which significantly improve the transferability (i.e., on
average, \textbf{5.1\%} for normally trained models and \textbf{12.8\%} for
adversarially trained defenses). Our code is available at
\url{https://github.com/qilong-zhang/Staircase-sign-method}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tackling the Challenges in Scene Graph Generation with Local-to-Global Interactions. (arXiv:2106.08543v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08543">
<div class="article-summary-box-inner">
<span><p>In this work, we seek new insights into the underlying challenges of the
Scene Graph Generation (SGG) task. Quantitative and qualitative analysis of the
Visual Genome dataset implies -- 1) Ambiguity: even if inter-object
relationship contains the same object (or predicate), they may not be visually
or semantically similar, 2) Asymmetry: despite the nature of the relationship
that embodied the direction, it was not well addressed in previous studies, and
3) Higher-order contexts: leveraging the identities of certain graph elements
can help to generate accurate scene graphs. Motivated by the analysis, we
design a novel SGG framework, Local-to-Global Interaction Networks (LOGIN).
Locally, interactions extract the essence between three instances of subject,
object, and background, while baking direction awareness into the network by
explicitly constraining the input order of subject and object. Globally,
interactions encode the contexts between every graph component (i.e., nodes and
edges). Finally, Attract &amp; Repel loss is utilized to fine-tune the distribution
of predicate embeddings. By design, our framework enables predicting the scene
graph in a bottom-up manner, leveraging the possible complementariness. To
quantify how much LOGIN is aware of relational direction, a new diagnostic task
called Bidirectional Relationship Classification (BRC) is also proposed.
Experimental results demonstrate that LOGIN can successfully distinguish
relational direction than existing methods (in BRC task), while showing
state-of-the-art results on the Visual Genome benchmark (in SGG task).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Optimal Source Task Performance Imply Optimal Pre-training for a Target Task?. (arXiv:2106.11174v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11174">
<div class="article-summary-box-inner">
<span><p>Fine-tuning of pre-trained deep nets is commonly used to improve accuracies
and training times for neural nets. It is generally assumed that pre-training a
net for optimal source task performance best prepares it for fine-tuning to
learn an arbitrary target task. This is generally not true. Stopping source
task training, prior to optimal performance, can create a pre-trained net
better suited for fine-tuning to learn a new task. We perform several
experiments demonstrating this effect, as well as the influence of the amount
of training and of learning rate. Additionally, our results indicate that this
reflects a general loss of learning ability that even extends to relearning the
source task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Unsupervised Domain Generalization. (arXiv:2107.06219v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06219">
<div class="article-summary-box-inner">
<span><p>Domain generalization (DG) aims to help models trained on a set of source
domains generalize better on unseen target domains. The performances of current
DG methods largely rely on sufficient labeled data, which are usually costly or
unavailable, however. Since unlabeled data are far more accessible, we seek to
explore how unsupervised learning can help deep models generalize across
domains. Specifically, we study a novel generalization problem called
unsupervised domain generalization (UDG), which aims to learn generalizable
models with unlabeled data and analyze the effects of pre-training on DG. In
UDG, models are pretrained with unlabeled data from various source domains
before being trained on labeled source data and eventually tested on unseen
target domains. Then we propose a method named Domain-Aware Representation
LearnING (DARLING) to cope with the significant and misleading heterogeneity
within unlabeled pretraining data and severe distribution shifts between source
and target data. Surprisingly we observe that DARLING can not only
counterbalance the scarcity of labeled data but also further strengthen the
generalization ability of models when the labeled data are insufficient. As a
pretraining approach, DARLING shows superior or comparable performance compared
with ImageNet pretraining protocol even when the available data are unlabeled
and of a vastly smaller amount compared to ImageNet, which may shed light on
improving generalization with large-scale unlabeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic cerebral hemisphere segmentation in rat MRI with lesions via attention-based convolutional neural networks. (arXiv:2108.01941v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01941">
<div class="article-summary-box-inner">
<span><p>We present MedicDeepLabv3+, a convolutional neural network that is the first
completely automatic method to segment cerebral hemispheres in magnetic
resonance (MR) volumes of rats with lesions. MedicDeepLabv3+ improves the
state-of-the-art DeepLabv3+ with an advanced decoder, incorporating spatial
attention layers and additional skip connections that, as we show in our
experiments, lead to more precise segmentations. MedicDeepLabv3+ requires no MR
image preprocessing, such as bias-field correction or registration to a
template, produces segmentations in less than a second, and its GPU memory
requirements can be adjusted based on the available resources. We optimized
MedicDeepLabv3+ and six other state-of-the-art convolutional neural networks
(DeepLabv3+, UNet, HighRes3DNet, V-Net, VoxResNet, Demon) on a heterogeneous
training set comprised by MR volumes from 11 cohorts acquired at different
lesion stages. Then, we evaluated the trained models and two approaches
specifically designed for rodent MRI skull stripping (RATS and RBET) on a large
dataset of 655 MR rat brain volumes. In our experiments, MedicDeepLabv3+
outperformed the other methods, yielding an average Dice coefficient of 0.952
and 0.944 in the brain and contralateral hemisphere regions. Additionally, we
show that despite limiting the GPU memory and the training data, our
MedicDeepLabv3+ also provided satisfactory segmentations. In conclusion, our
method, publicly available at https://github.com/jmlipman/MedicDeepLabv3Plus,
yielded excellent results in multiple scenarios, demonstrating its capability
to reduce human workload in rat neuroimaging studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Less to More: Spectral Splitting and Aggregation Network for Hyperspectral Face Super-Resolution. (arXiv:2108.13584v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13584">
<div class="article-summary-box-inner">
<span><p>High-resolution (HR) hyperspectral face image plays an important role in face
related computer vision tasks under uncontrolled conditions, such as low-light
environment and spoofing attacks. However, the dense spectral bands of
hyperspectral face images come at the cost of limited amount of photons reached
a narrow spectral window on average, which greatly reduces the spatial
resolution of hyperspectral face images. In this paper, we investigate how to
adapt the deep learning techniques to hyperspectral face image super-resolution
(HFSR), especially when the training samples are very limited. Benefiting from
the amount of spectral bands, in which each band can be seen as an image, we
present a spectral splitting and aggregation network (SSANet) for HFSR with
limited training samples. In the shallow layers, we split the hyperspectral
image into different spectral groups. Then, we gradually aggregate the neighbor
bands at deeper layers to exploit spectral correlations. By this spectral
splitting and aggregation strategy (SSAS), we can divide the original
hyperspectral image into multiple samples (\emph{from less to more}) to support
the efficient training of the network and effectively exploit the spectral
correlations among spectrum. To cope with the challenge of small training
sample size (S3) problem, we propose to expand the training samples by a
self-representation model and symmetry-induced augmentation. Experiments show
that SSANet can well model the joint correlations of spatial and spectral
information. By expanding the training samples, SSANet can effectively
alleviate the S3 problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Autoencoder Training Performance for Hyperspectral Unmixing with Network Reinitialisation. (arXiv:2109.13748v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13748">
<div class="article-summary-box-inner">
<span><p>Neural networks, in particular autoencoders, are one of the most promising
solutions for unmixing hyperspectral data, i.e. reconstructing the spectra of
observed substances (endmembers) and their relative mixing fractions
(abundances), which is needed for effective hyperspectral analysis and
classification. However, as we show in this paper, the training of autoencoders
for unmixing is highly dependent on weights initialisation; some sets of
weights lead to degenerate or low-performance solutions, introducing negative
bias in the expected performance. In this work, we experimentally investigate
autoencoders stability as well as network reinitialisation methods based on
coefficients of neurons' dead activations. We demonstrate that the proposed
techniques have a positive effect on autoencoder training in terms of
reconstruction, abundances and endmembers errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Colored Point Cloud to Image Alignment. (arXiv:2110.03249v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03249">
<div class="article-summary-box-inner">
<span><p>Reconstruction of geometric structures from images using supervised learning
suffers from limited available amount of accurate data. One type of such data
is accurate real-world RGB-D images. A major challenge in acquiring such ground
truth data is the accurate alignment between RGB images and the point cloud
measured by a depth scanner. To overcome this difficulty, we consider a
differential optimization method that aligns a colored point cloud with a given
color image through iterative geometric and color matching. In the proposed
framework, the optimization minimizes the photometric difference between the
colors of the point cloud and the corresponding colors of the image pixels.
Unlike other methods that try to reduce this photometric error, we analyze the
computation of the gradient on the image plane and propose a different direct
scheme. We assume that the colors produced by the geometric scanner camera and
the color camera sensor are different and therefore characterized by different
chromatic acquisition properties. Under these multimodal conditions, we find
the transformation between the camera image and the point cloud colors. We
alternately optimize for aligning the position of the point cloud and matching
the different color spaces. The alignments produced by the proposed method are
demonstrated on both synthetic data with quantitative evaluation and real
scenes with qualitative results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Field Extraction from Forms with Unlabeled Data. (arXiv:2110.04282v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04282">
<div class="article-summary-box-inner">
<span><p>We propose a novel framework to conduct field extraction from forms with
unlabeled data. To bootstrap the training process, we develop a rule-based
method for mining noisy pseudo-labels from unlabeled forms. Using the
supervisory signal from the pseudo-labels, we extract a discriminative token
representation from a transformer-based model by modeling the interaction
between text in the form. To prevent the model from overfitting to label noise,
we introduce a refinement module based on a progressive pseudo-label ensemble.
Experimental results demonstrate the effectiveness of our framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CamLiFlow: Bidirectional Camera-LiDAR Fusion for Joint Optical Flow and Scene Flow Estimation. (arXiv:2111.10502v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10502">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the problem of jointly estimating the optical flow
and scene flow from synchronized 2D and 3D data. Previous methods either employ
a complex pipeline that splits the joint task into independent stages, or fuse
2D and 3D information in an "early-fusion" or "late-fusion" manner. Such
one-size-fits-all approaches suffer from a dilemma of failing to fully utilize
the characteristic of each modality or to maximize the inter-modality
complementarity. To address the problem, we propose a novel end-to-end
framework, called CamLiFlow. It consists of 2D and 3D branches with multiple
bidirectional connections between them in specific layers. Different from
previous work, we apply a point-based 3D branch to better extract the geometric
features and design a symmetric learnable operator to fuse dense image features
and sparse point features. Experiments show that CamLiFlow achieves better
performance with fewer parameters. Our method ranks 1st on the KITTI Scene Flow
benchmark, outperforming the previous art with 1/7 parameters. Code is
available at https://github.com/MCG-NJU/CamLiFlow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TDAM: Top-Down Attention Module for Contextually Guided Feature Selection in CNNs. (arXiv:2111.13470v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13470">
<div class="article-summary-box-inner">
<span><p>Attention modules for Convolutional Neural Networks (CNNs) are an effective
method to enhance performance on multiple computer-vision tasks. While existing
methods appropriately model channel-, spatial- and self-attention, they
primarily operate in a feedforward bottom-up manner. Consequently, the
attention mechanism strongly depends on the local information of a single input
feature map and does not incorporate relatively semantically-richer contextual
information available at higher layers that can specify "what and where to
look" in lower-level feature maps through top-down information flow.
</p>
<p>Accordingly, in this work, we propose a lightweight top-down attention module
(TDAM) that iteratively generates a "visual searchlight" to perform channel and
spatial modulation of its inputs and outputs more contextually-relevant feature
maps at each computation step. Our experiments indicate that TDAM enhances the
performance of CNNs across multiple object-recognition benchmarks and
outperforms prominent attention modules while being more parameter and memory
efficient. Further, TDAM-based models learn to "shift attention" by localizing
individual objects or features at each computation step without any explicit
supervision resulting in a 5% improvement for ResNet50 on weakly-supervised
object localization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Gating for Single-Photon 3D Imaging. (arXiv:2111.15047v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15047">
<div class="article-summary-box-inner">
<span><p>Single-photon avalanche diodes (SPADs) are growing in popularity for depth
sensing tasks. However, SPADs still struggle in the presence of high ambient
light due to the effects of pile-up. Conventional techniques leverage fixed or
asynchronous gating to minimize pile-up effects, but these gating schemes are
all non-adaptive, as they are unable to incorporate factors such as scene
priors and previous photon detections into their gating strategy. We propose an
adaptive gating scheme built upon Thompson sampling. Adaptive gating
periodically updates the gate position based on prior photon observations in
order to minimize depth errors. Our experiments show that our gating strategy
results in significantly reduced depth reconstruction error and acquisition
time, even when operating outdoors under strong sunlight conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global-Local Context Network for Person Search. (arXiv:2112.02500v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02500">
<div class="article-summary-box-inner">
<span><p>Person search aims to jointly localize and identify a query person from
natural, uncropped images, which has been actively studied over the past few
years. In this paper, we delve into the rich context information globally and
locally surrounding the target person, which we refer to as scene and group
context, respectively. Unlike previous works that treat the two types of
context individually, we exploit them in a unified global-local context network
(GLCNet) with the intuitive aim of feature enhancement. Specifically, re-ID
embeddings and context features are simultaneously learned in a multi-stage
fashion, ultimately leading to enhanced, discriminative features for person
search. We conduct the experiments on two person search benchmarks (i.e.,
CUHK-SYSU and PRW) as well as extend our approach to a more challenging setting
(i.e., character search on MovieNet). Extensive experimental results
demonstrate the consistent improvement of the proposed GLCNet over the
state-of-the-art methods on all three datasets. Our source codes, pre-trained
models, and the new setting w.r.t. character search are publicly available at:
https://github.com/ZhengPeng7/GLCNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PolyphonicFormer: Unified Query Learning for Depth-aware Video Panoptic Segmentation. (arXiv:2112.02582v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02582">
<div class="article-summary-box-inner">
<span><p>The Depth-aware Video Panoptic Segmentation (DVPS) is a new challenging
vision problem that aims to predict panoptic segmentation and depth in a video
simultaneously. The previous work solves this task by extending the existing
panoptic segmentation method with extra dense depth prediction and instance
tracking head. However, the relationship between the depth and panoptic
segmentation is not well explored -- simply combining existing methods leads to
competetion and needs careful weight balancing. In this paper, we present
PolyphonicFormer, a vision transformer to unify these sub-tasks under the DVPS
task and lead to more robust results. Our principal insight is that the depth
can be harmonized with the panoptic segmentation with our proposed new paradigm
of predicting instance level depth maps with object queries. Then the
relationship between the two tasks via query-based learning is explored. In
particular, we propose to learn the correlations among these queries and
corresponding features via query learning including grouping, updating, and
reasoning. From the experiments, we prove the benefits of our design from both
depth estimation and panoptic segmentation aspects. Since each thing query also
encodes the instance-wise appearance information, it is natural to perform
tracking directly with appearance learning. Our method achieves the
state-of-the-art results on two DVPS datasets (Semantic KITTI, Cityscapes), and
ranks 1st on the ICCV-2021 BMTT Challenge video + depth track. Code will be
available at https://github.com/HarborYuan/PolyphonicFormer .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recurrent Glimpse-based Decoder for Detection with Transformer. (arXiv:2112.04632v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04632">
<div class="article-summary-box-inner">
<span><p>Although detection with Transformer (DETR) is increasingly popular, its
global attention modeling requires an extremely long training period to
optimize and achieve promising detection performance. Alternative to existing
studies that mainly develop advanced feature or embedding designs to tackle the
training issue, we point out that the Region-of-Interest (RoI) based detection
refinement can easily help mitigate the difficulty of training for DETR
methods. Based on this, we introduce a novel REcurrent Glimpse-based decOder
(REGO) in this paper. In particular, the REGO employs a multi-stage recurrent
processing structure to help the attention of DETR gradually focus on
foreground objects more accurately. In each processing stage, visual features
are extracted as glimpse features from RoIs with enlarged bounding box areas of
detection results from the previous stage. Then, a glimpse-based decoder is
introduced to provide refined detection results based on both the glimpse
features and the attention modeling outputs of the previous stage. In practice,
REGO can be easily embedded in representative DETR variants while maintaining
their fully end-to-end training and inference pipelines. In particular, REGO
helps Deformable DETR achieve 44.8 AP on the MSCOCO dataset with only 36
training epochs, compared with the first DETR and the Deformable DETR that
require 500 and 50 epochs to achieve comparable performance, respectively.
Experiments also show that REGO consistently boosts the performance of
different DETR detectors by up to 7% relative gain at the same setting of 50
training epochs. Code is available via
https://github.com/zhechen/Deformable-DETR-REGO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-tail Recognition via Compositional Knowledge Transfer. (arXiv:2112.06741v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06741">
<div class="article-summary-box-inner">
<span><p>In this work, we introduce a novel strategy for long-tail recognition that
addresses the tail classes' few-shot problem via training-free knowledge
transfer. Our objective is to transfer knowledge acquired from information-rich
common classes to semantically similar, and yet data-hungry, rare classes in
order to obtain stronger tail class representations. We leverage the fact that
class prototypes and learned cosine classifiers provide two different,
complementary representations of class cluster centres in feature space, and
use an attention mechanism to select and recompose learned classifier features
from common classes to obtain higher quality rare class representations. Our
knowledge transfer process is training free, reducing overfitting risks, and
can afford continual extension of classifiers to new classes. Experiments show
that our approach can achieve significant performance boosts on rare classes
while maintaining robust common class performance, outperforming directly
comparable state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Distillation Interaction Weighting Network for Lightweight Image Super-Resolution. (arXiv:2112.08655v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08655">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks based single-image super-resolution (SISR) has
made great progress in recent years. However, it is difficult to apply these
methods to real-world scenarios due to the computational and memory cost.
Meanwhile, how to take full advantage of the intermediate features under the
constraints of limited parameters and calculations is also a huge challenge. To
alleviate these issues, we propose a lightweight yet efficient Feature
Distillation Interaction Weighted Network (FDIWN). Specifically, FDIWN utilizes
a series of specially designed Feature Shuffle Weighted Groups (FSWG) as the
backbone, and several novel mutual Wide-residual Distillation Interaction
Blocks (WDIB) form an FSWG. In addition, Wide Identical Residual Weighting
(WIRW) units and Wide Convolutional Residual Weighting (WCRW) units are
introduced into WDIB for better feature distillation. Moreover, a Wide-Residual
Distillation Connection (WRDC) framework and a Self-Calibration Fusion (SCF)
unit are proposed to interact features with different scales more flexibly and
efficiently.Extensive experiments show that our FDIWN is superior to other
models to strike a good balance between model performance and efficiency. The
code is available at https://github.com/IVIPLab/FDIWN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaFocus V2: End-to-End Training of Spatial Dynamic Networks for Video Recognition. (arXiv:2112.14238v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14238">
<div class="article-summary-box-inner">
<span><p>Recent works have shown that the computational efficiency of video
recognition can be significantly improved by reducing the spatial redundancy.
As a representative work, the adaptive focus method (AdaFocus) has achieved a
favorable trade-off between accuracy and inference speed by dynamically
identifying and attending to the informative regions in each video frame.
However, AdaFocus requires a complicated three-stage training pipeline
(involving reinforcement learning), leading to slow convergence and is
unfriendly to practitioners. This work reformulates the training of AdaFocus as
a simple one-stage algorithm by introducing a differentiable
interpolation-based patch selection operation, enabling efficient end-to-end
optimization. We further present an improved training scheme to address the
issues introduced by the one-stage formulation, including the lack of
supervision, input diversity and training stability. Moreover, a
conditional-exit technique is proposed to perform temporal adaptive computation
on top of AdaFocus without additional training. Extensive experiments on six
benchmark datasets (i.e., ActivityNet, FCVID, Mini-Kinetics,
Something-Something V1&amp;V2, and Jester) demonstrate that our model significantly
outperforms the original AdaFocus and other competitive baselines, while being
considerably more simple and efficient to train. Code is available at
https://github.com/LeapLabTHU/AdaFocusV2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Mask Uncertainty in Hyperspectral Image Reconstruction. (arXiv:2112.15362v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15362">
<div class="article-summary-box-inner">
<span><p>Recently, hyperspectral imaging (HSI) has attracted increasing research
attention, especially for the ones based on a coded aperture snapshot spectral
imaging (CASSI) system. Existing deep HSI reconstruction models are generally
trained on paired data to retrieve original signals upon 2D compressed
measurements given by a particular optical hardware mask in CASSI, during which
the mask largely impacts the reconstruction performance and could work as a
"model hyperparameter" governing on data augmentations. This mask-specific
training style will lead to a hardware miscalibration issue, which sets up
barriers to deploying deep HSI models among different hardware and noisy
environments. To address this challenge, we introduce mask uncertainty for HSI
with a complete variational Bayesian learning treatment and explicitly model it
through a mask decomposition inspired by real hardware. Specifically, we
propose a novel Graph-based Self-Tuning (GST) network to reason uncertainties
adapting to varying spatial structures of masks among different hardware.
Moreover, we develop a bilevel optimization framework to balance HSI
reconstruction and uncertainty estimation, accounting for the hyperparameter
property of masks. Extensive experimental results and model discussions
validate the effectiveness (over 33/30 dB) of the proposed GST method under two
miscalibration scenarios and demonstrate a highly competitive performance
compared with the state-of-the-art well-calibrated methods. Our code and
pre-trained model are available at
https://github.com/Jiamian-Wang/mask_uncertainty_spectral_SCI
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Faces with Faced Masks. (arXiv:2201.06427v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06427">
<div class="article-summary-box-inner">
<span><p>Modern face recognition systems (FRS) still fall short when the subjects are
wearing facial masks, a common theme in the age of respiratory pandemics. An
intuitive partial remedy is to add a mask detector to flag any masked faces so
that the FRS can act accordingly for those low-confidence masked faces. In this
work, we set out to investigate the potential vulnerability of such FRS
equipped with a mask detector, on large-scale masked faces, which might trigger
a serious risk, e.g., letting a suspect evade the FRS where both facial
identity and mask are undetected. As existing face recognizers and mask
detectors have high performance in their respective tasks, it is significantly
challenging to simultaneously fool them and preserve the transferability of the
attack. We formulate the new task as the generation of realistic &amp;
adversarial-faced mask and make three main contributions: First, we study the
naive Delanunay-based masking method (DM) to simulate the process of wearing a
faced mask that is cropped from a template image, which reveals the main
challenges of this new task. Second, we further equip the DM with the
adversarial noise attack and propose the adversarial noise Delaunay-based
masking method (AdvNoise-DM) that can fool the face recognition and mask
detection effectively but make the face less natural. Third, we propose the
adversarial filtering Delaunay-based masking method denoted as MF2M by
employing the adversarial filtering for AdvNoise-DM and obtain more natural
faces. With the above efforts, the final version not only leads to significant
performance deterioration of the state-of-the-art (SOTA) deep learning-based
FRS, but also remains undetected by the SOTA facial mask detector, thus
successfully fooling both systems at the same time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Addressing the Intra-class Mode Collapse Problem using Adaptive Input Image Normalization in GAN-based X-ray Images. (arXiv:2201.10324v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10324">
<div class="article-summary-box-inner">
<span><p>Biomedical image datasets can be imbalanced due to the rarity of targeted
diseases. Generative Adversarial Networks play a key role in addressing this
imbalance by enabling the generation of synthetic images to augment datasets.
It is important to generate synthetic images that incorporate a diverse range
of features to accurately represent the distribution of features present in the
training imagery. Furthermore, the absence of diverse features in synthetic
images can degrade the performance of machine learning classifiers. The mode
collapse problem can impact a Generative Adversarial Network's capacity to
generate diversified images. Mode collapse comes in two varieties: intra-class
and inter-class. In this paper, the intra-class mode collapse problem is
investigated, and its subsequent impact on the diversity of synthetic X-ray
images is evaluated. This work contributes an empirical demonstration of the
benefits of integrating the adaptive input-image normalization for the Deep
Convolutional GAN to alleviate the intra-class mode collapse problem. Results
demonstrate that the DCGAN with adaptive input-image normalization outperforms
DCGAN with un-normalized X-ray images as evident by the superior diversity
scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning To Recognize Procedural Activities with Distant Supervision. (arXiv:2201.10990v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10990">
<div class="article-summary-box-inner">
<span><p>In this paper we consider the problem of classifying fine-grained, multi-step
activities (e.g., cooking different recipes, making disparate home
improvements, creating various forms of arts and crafts) from long videos
spanning up to several minutes. Accurately categorizing these activities
requires not only recognizing the individual steps that compose the task but
also capturing their temporal dependencies. This problem is dramatically
different from traditional action classification, where models are typically
optimized on videos that span only a few seconds and that are manually trimmed
to contain simple atomic actions. While step annotations could enable the
training of models to recognize the individual steps of procedural activities,
existing large-scale datasets in this area do not include such segment labels
due to the prohibitive cost of manually annotating temporal boundaries in long
videos. To address this issue, we propose to automatically identify steps in
instructional videos by leveraging the distant supervision of a textual
knowledge base (wikiHow) that includes detailed descriptions of the steps
needed for the execution of a wide variety of complex activities. Our method
uses a language model to match noisy, automatically-transcribed speech from the
video to step descriptions in the knowledge base. We demonstrate that video
models trained to recognize these automatically-labeled steps (without manual
supervision) yield a representation that achieves superior generalization
performance on four downstream tasks: recognition of procedural activities,
step classification, step forecasting and egocentric video classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rate Coding or Direct Coding: Which One is Better for Accurate, Robust, and Energy-efficient Spiking Neural Networks?. (arXiv:2202.03133v2 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03133">
<div class="article-summary-box-inner">
<span><p>Recent Spiking Neural Networks (SNNs) works focus on an image classification
task, therefore various coding techniques have been proposed to convert an
image into temporal binary spikes. Among them, rate coding and direct coding
are regarded as prospective candidates for building a practical SNN system as
they show state-of-the-art performance on large-scale datasets. Despite their
usage, there is little attention to comparing these two coding schemes in a
fair manner. In this paper, we conduct a comprehensive analysis of the two
codings from three perspectives: accuracy, adversarial robustness, and
energy-efficiency. First, we compare the performance of two coding techniques
with various architectures and datasets. Then, we measure the robustness of the
coding techniques on two adversarial attack methods. Finally, we compare the
energy-efficiency of two coding schemes on a digital hardware platform. Our
results show that direct coding can achieve better accuracy especially for a
small number of timesteps. In contrast, rate coding shows better robustness to
adversarial attacks owing to the non-differentiable spike generation process.
Rate coding also yields higher energy-efficiency than direct coding which
requires multi-bit precision for the first layer. Our study explores the
characteristics of two codings, which is an important design consideration for
building SNNs. The code is made available at
https://github.com/Intelligent-Computing-Lab-Yale/Rate-vs-Direct.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation and Analysis of Different Aggregation and Hyperparameter Selection Methods for Federated Brain Tumor Segmentation. (arXiv:2202.08261v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08261">
<div class="article-summary-box-inner">
<span><p>Availability of large, diverse, and multi-national datasets is crucial for
the development of effective and clinically applicable AI systems in the
medical imaging domain. However, forming a global model by bringing these
datasets together at a central location, comes along with various data privacy
and ownership problems. To alleviate these problems, several recent studies
focus on the federated learning paradigm, a distributed learning approach for
decentralized data. Federated learning leverages all the available data without
any need for sharing collaborators' data with each other or collecting them on
a central server. Studies show that federated learning can provide competitive
performance with conventional central training, while having a good
generalization capability. In this work, we have investigated several federated
learning approaches on the brain tumor segmentation problem. We explore
different strategies for faster convergence and better performance which can
also work on strong Non-IID cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Anomaly Detection from Time-of-Flight Depth Images. (arXiv:2203.01052v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01052">
<div class="article-summary-box-inner">
<span><p>Video anomaly detection (VAD) addresses the problem of automatically finding
anomalous events in video data. The primary data modalities on which current
VAD systems work on are monochrome or RGB images. Using depth data in this
context instead is still hardly explored in spite of depth images being a
popular choice in many other computer vision research areas and the increasing
availability of inexpensive depth camera hardware. We evaluate the application
of existing autoencoder-based methods on depth video and propose how the
advantages of using depth data can be leveraged by integration into the loss
function. Training is done unsupervised using normal sequences without need for
any additional annotations. We show that depth allows easy extraction of
auxiliary information for scene analysis in the form of a foreground mask and
demonstrate its beneficial effect on the anomaly detection performance through
evaluation on a large public dataset, for which we are also the first ones to
present results on.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiT: Self-supervised Pre-training for Document Image Transformer. (arXiv:2203.02378v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02378">
<div class="article-summary-box-inner">
<span><p>Image Transformer has recently achieved significant progress for natural
image understanding, either using supervised (ViT, DeiT, etc.) or
self-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we
propose DiT, a self-supervised pre-trained Document Image Transformer model
using large-scale unlabeled text images for Document AI tasks, which is
essential since no supervised counterparts ever exist due to the lack of human
labeled document images. We leverage DiT as the backbone network in a variety
of vision-based Document AI tasks, including document image classification,
document layout analysis, table detection as well as text detection for OCR.
Experiment results have illustrated that the self-supervised pre-trained DiT
model achieves new state-of-the-art results on these downstream tasks, e.g.
document image classification (91.11 $\rightarrow$ 92.69), document layout
analysis (91.0 $\rightarrow$ 94.9), table detection (94.23 $\rightarrow$ 96.55)
and text detection for OCR (93.07 $\rightarrow$ 94.29). The code and
pre-trained models are publicly available at \url{https://aka.ms/msdit}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers. (arXiv:2203.04838v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04838">
<div class="article-summary-box-inner">
<span><p>Pixel-wise semantic segmentation of RGB images can be advanced by exploiting
informative features from supplementary modalities. In this work, we propose
CMX, a vision-transformer-based cross-modal fusion framework for RGB-X semantic
segmentation. To generalize to different sensing modalities encompassing
various supplements and uncertainties, we consider that comprehensive
cross-modal interactions should be provided. CMX is built with two streams to
extract features from RGB images and the complementary modality (X-modality).
In each feature extraction stage, we design a Cross-Modal Feature Rectification
Module (CM-FRM) to calibrate the feature of the current modality by combining
the feature from the other modality, in spatial- and channel-wise dimensions.
With rectified feature pairs, we deploy a Feature Fusion Module (FFM) to mix
them for the final semantic prediction. FFM is constructed with a
cross-attention mechanism, which enables exchange of long-range contexts,
enhancing both modalities' features at a global level. Extensive experiments
show that CMX generalizes to diverse multi-modal combinations, achieving
state-of-the-art performances on five RGB-Depth benchmarks, as well as
RGB-Thermal and RGB-Polarization datasets. Besides, to investigate the
generalizability to dense-sparse data fusion, we establish an RGB-Event
semantic segmentation benchmark based on the EventScape dataset, on which CMX
sets the new state-of-the-art. Code is available at
https://github.com/huaaaliu/RGBX_Semantic_Segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometric Synthesis: A Free lunch for Large-scale Palmprint Recognition Model Pretraining. (arXiv:2203.05703v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05703">
<div class="article-summary-box-inner">
<span><p>Palmprints are private and stable information for biometric recognition. In
the deep learning era, the development of palmprint recognition is limited by
the lack of sufficient training data. In this paper, by observing that palmar
creases are the key information to deep-learning-based palmprint recognition,
we propose to synthesize training data by manipulating palmar creases.
Concretely, we introduce an intuitive geometric model which represents palmar
creases with parameterized B\'ezier curves. By randomly sampling B\'ezier
parameters, we can synthesize massive training samples of diverse identities,
which enables us to pretrain large-scale palmprint recognition models.
Experimental results demonstrate that such synthetically pretrained models have
a very strong generalization ability: they can be efficiently transferred to
real datasets, leading to significant performance improvements on palmprint
recognition. For example, under the open-set protocol, our method improves the
strong ArcFace baseline by more than 10\% in terms of TAR@1e-6. And under the
closed-set protocol, our method reduces the equal error rate (EER) by an order
of magnitude.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuralReshaper: Single-image Human-body Retouching with Deep Neural Networks. (arXiv:2203.10496v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10496">
<div class="article-summary-box-inner">
<span><p>In this paper, we present NeuralReshaper, a novel method for semantic
reshaping of human bodies in single images using deep generative networks. To
achieve globally coherent reshaping effects, our approach follows a
fit-then-reshape pipeline, which first fits a parametric 3D human model to a
source human image and then reshapes the fitted 3D model with respect to
user-specified semantic attributes. Previous methods rely on image warping to
transfer 3D reshaping effects to the entire image domain and thus often cause
distortions in both foreground and background. In contrast, we resort to
generative adversarial nets conditioned on the source image and a 2D warping
field induced by the reshaped 3D model, to achieve more realistic reshaping
results. Specifically, we separately encode the foreground and background
information in the source image using a two-headed UNet-like generator, and
guide the information flow from the foreground branch to the background branch
via feature space warping. Furthermore, to deal with the lack-of-data problem
that no paired data exist (i.e., the same human bodies in varying shapes), we
introduce a novel self-supervised strategy to train our network. Unlike
previous methods that often require manual efforts to correct undesirable
artifacts caused by incorrect body-to-image fitting, our method is fully
automatic. Extensive experiments on both indoor and outdoor datasets
demonstrate the superiority of our method over previous approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark. (arXiv:2203.11089v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11089">
<div class="article-summary-box-inner">
<span><p>Methods for 3D lane detection have been recently proposed to address the
issue of inaccurate lane layouts in many autonomous driving scenarios
(uphill/downhill, bump, etc.). Previous work struggled in complex cases due to
their simple designs of the spatial transformation between front view and
bird's eye view (BEV) and the lack of a realistic dataset. Towards these
issues, we present PersFormer: an end-to-end monocular 3D lane detector with a
novel Transformer-based spatial feature transformation module. Our model
generates BEV features by attending to related front-view local regions with
camera parameters as a reference. PersFormer adopts a unified 2D/3D anchor
design and an auxiliary task to detect 2D/3D lanes simultaneously, enhancing
the feature consistency and sharing the benefits of multi-task learning.
Moreover, we release one of the first large-scale real-world 3D lane datasets,
which is called OpenLane, with high-quality annotation and scenario diversity.
OpenLane contains 200,000 frames, over 880,000 instance-level lanes, 14 lane
categories, along with scene tags and the closed-in-path object annotations to
encourage the development of lane detection and more industrial-related
autonomous driving methods. We show that PersFormer significantly outperforms
competitive baselines in the 3D lane detection task on our new OpenLane dataset
as well as Apollo 3D Lane Synthetic dataset, and is also on par with
state-of-the-art algorithms in the 2D task on OpenLane. The project page is
available at https://github.com/OpenPerceptionX/PersFormer_3DLane and OpenLane
dataset is provided at https://github.com/OpenPerceptionX/OpenLane.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Long Short-term Memory Based Recurrent Neural Network for Interventional MRI Reconstruction. (arXiv:2203.14769v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14769">
<div class="article-summary-box-inner">
<span><p>Interventional magnetic resonance imaging (i-MRI) for surgical guidance could
help visualize the interventional process such as deep brain stimulation (DBS),
improving the surgery performance and patient outcome. Different from
retrospective reconstruction in conventional dynamic imaging, i-MRI for DBS has
to acquire and reconstruct the interventional images sequentially online. Here
we proposed a convolutional long short-term memory (Conv-LSTM) based recurrent
neural network (RNN), or ConvLR, to reconstruct interventional images with
golden-angle radial sampling. By using an initializer and Conv-LSTM blocks, the
priors from the pre-operative reference image and intra-operative frames were
exploited for reconstructing the current frame. Data consistency for radial
sampling was implemented by a soft-projection method. To improve the
reconstruction accuracy, an adversarial learning strategy was adopted. A set of
interventional images based on the pre-operative and post-operative MR images
were simulated for algorithm validation. Results showed with only 10 radial
spokes, ConvLR provided the best performance compared with state-of-the-art
methods, giving an acceleration up to 40 folds. The proposed algorithm has the
potential to achieve real-time i-MRI for DBS and can be used for general
purpose MR-guided intervention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word separation in continuous sign language using isolated signs and post-processing. (arXiv:2204.00923v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00923">
<div class="article-summary-box-inner">
<span><p>Continuous Sign Language Recognition (CSLR) is a long challenging task in
Computer Vision due to the difficulties in detecting the explicit boundaries
between the words in a sign sentence. To deal with this challenge, we propose a
two-stage model. In the first stage, the predictor model, which includes a
combination of CNN, SVD, and LSTM, is trained with the isolated signs. In the
second stage, we apply a post-processing algorithm to the Softmax outputs
obtained from the first part of the model in order to separate the isolated
signs in the continuous signs. Due to the lack of a large dataset, including
both the sign sequences and the corresponding isolated signs, two public
datasets in Isolated Sign Language Recognition (ISLR), RKS-PERSIANSIGN and
ASLVID, are used for evaluation. Results of the continuous sign videos confirm
the efficiency of the proposed model to deal with isolated sign boundaries
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Commonsense-aware Moment-Text Alignment for Fast Video Temporal Grounding. (arXiv:2204.01450v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01450">
<div class="article-summary-box-inner">
<span><p>Grounding temporal video segments described in natural language queries
effectively and efficiently is a crucial capability needed in
vision-and-language fields. In this paper, we deal with the fast video temporal
grounding (FVTG) task, aiming at localizing the target segment with high speed
and favorable accuracy. Most existing approaches adopt elaborately designed
cross-modal interaction modules to improve the grounding performance, which
suffer from the test-time bottleneck. Although several common space-based
methods enjoy the high-speed merit during inference, they can hardly capture
the comprehensive and explicit relations between visual and textual modalities.
In this paper, to tackle the dilemma of speed-accuracy tradeoff, we propose a
commonsense-aware cross-modal alignment (CCA) framework, which incorporates
commonsense-guided visual and text representations into a complementary common
space for fast video temporal grounding. Specifically, the commonsense concepts
are explored and exploited by extracting the structural semantic information
from a language corpus. Then, a commonsense-aware interaction module is
designed to obtain bridged visual and text features by utilizing the learned
commonsense concepts. Finally, to maintain the original semantic information of
textual queries, a cross-modal complementary common space is optimized to
obtain matching scores for performing FVTG. Extensive results on two
challenging benchmarks show that our CCA method performs favorably against
state-of-the-arts while running at high speed. Our code is available at
https://github.com/ZiyueWu59/CCA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature robustness and sex differences in medical imaging: a case study in MRI-based Alzheimer's disease detection. (arXiv:2204.01737v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01737">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks have enabled significant improvements in
medical image-based disease classification. It has, however, become
increasingly clear that these models are susceptible to performance degradation
due to spurious correlations and dataset shifts, which may lead to
underperformance on underrepresented patient groups, among other problems. In
this paper, we compare two classification schemes on the ADNI MRI dataset: a
very simple logistic regression model that uses manually selected volumetric
features as inputs, and a convolutional neural network trained on 3D MRI data.
We assess the robustness of the trained models in the face of varying dataset
splits, training set sex composition, and stage of disease. In contrast to
earlier work on diagnosing lung diseases based on chest x-ray data, we do not
find a strong dependence of model performance for male and female test subjects
on the sex composition of the training dataset. Moreover, in our analysis, the
low-dimensional model with manually selected features outperforms the 3D CNN,
thus emphasizing the need for automatic robust feature extraction methods and
the value of manual feature specification (based on prior knowledge) for
robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Optimal K-space Acquisition and Reconstruction using Physics-Informed Neural Networks. (arXiv:2204.02480v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02480">
<div class="article-summary-box-inner">
<span><p>The inherent slow imaging speed of Magnetic Resonance Image (MRI) has spurred
the development of various acceleration methods, typically through
heuristically undersampling the MRI measurement domain known as k-space.
Recently, deep neural networks have been applied to reconstruct undersampled
k-space data and have shown improved reconstruction performance. While most of
these methods focus on designing novel reconstruction networks or new training
strategies for a given undersampling pattern, e.g., Cartesian undersampling or
Non-Cartesian sampling, to date, there is limited research aiming to learn and
optimize k-space sampling strategies using deep neural networks. This work
proposes a novel optimization framework to learn k-space sampling trajectories
by considering it as an Ordinary Differential Equation (ODE) problem that can
be solved using neural ODE. In particular, the sampling of k-space data is
framed as a dynamic system, in which neural ODE is formulated to approximate
the system with additional constraints on MRI physics. In addition, we have
also demonstrated that trajectory optimization and image reconstruction can be
learned collaboratively for improved imaging efficiency and reconstruction
performance. Experiments were conducted on different in-vivo datasets (e.g.,
brain and knee images) acquired with different sequences. Initial results have
shown that our proposed method can generate better image quality in accelerated
MRI than conventional undersampling schemes in Cartesian and Non-Cartesian
acquisitions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MixFormer: Mixing Features across Windows and Dimensions. (arXiv:2204.02557v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02557">
<div class="article-summary-box-inner">
<span><p>While local-window self-attention performs notably in vision tasks, it
suffers from limited receptive field and weak modeling capability issues. This
is mainly because it performs self-attention within non-overlapped windows and
shares weights on the channel dimension. We propose MixFormer to find a
solution. First, we combine local-window self-attention with depth-wise
convolution in a parallel design, modeling cross-window connections to enlarge
the receptive fields. Second, we propose bi-directional interactions across
branches to provide complementary clues in the channel and spatial dimensions.
These two designs are integrated to achieve efficient feature mixing among
windows and dimensions. Our MixFormer provides competitive results on image
classification with EfficientNet and shows better results than RegNet and Swin
Transformer. Performance in downstream tasks outperforms its alternatives by
significant margins with less computational costs in 5 dense prediction tasks
on MS COCO, ADE20k, and LVIS. Code is available at
\url{https://github.com/PaddlePaddle/PaddleClas}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HunYuan_tvr for Text-Video Retrivial. (arXiv:2204.03382v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03382">
<div class="article-summary-box-inner">
<span><p>Text-Video Retrieval plays an important role in multi-modal understanding and
has attracted increasing attention in recent years. Most existing methods focus
on constructing contrastive pairs between whole videos and complete caption
sentences, while ignoring fine-grained cross-modal relationships, e.g., short
clips and phrases or single frame and word. In this paper, we propose a novel
method, named HunYuan\_tvr, to explore hierarchical cross-modal interactions by
simultaneously exploring video-sentence, clip-phrase, and frame-word
relationships. Considering intrinsic semantic relations between frames,
HunYuan\_tvr first performs self-attention to explore frame-wise correlations
and adaptively clusters correlated frames into clip-level representations.
Then, the clip-wise correlation is explored to aggregate clip representations
into a compact one to describe the video globally. In this way, we can
construct hierarchical video representations for frame-clip-video
granularities, and also explore word-wise correlations to form
word-phrase-sentence embeddings for the text modality. Finally, hierarchical
contrastive learning is designed to explore cross-modal
relationships,~\emph{i.e.,} frame-word, clip-phrase, and video-sentence, which
enables HunYuan\_tvr to achieve a comprehensive multi-modal understanding.
Further boosted by adaptive label denosing and marginal sample enhancement,
HunYuan\_tvr obtains new state-of-the-art results on various benchmarks, e.g.,
Rank@1 of 55.0%, 57.8%, 29.7%, 52.1%, and 57.3% on MSR-VTT, MSVD, LSMDC,
DiDemo, and ActivityNet respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Multi-Head Convolutional Attention with Various Kernel Sizes for Medical Image Super-Resolution. (arXiv:2204.04218v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04218">
<div class="article-summary-box-inner">
<span><p>Super-resolving medical images can help physicians in providing more accurate
diagnostics. In many situations, computed tomography (CT) or magnetic resonance
imaging (MRI) techniques output several scans (modes) during a single
investigation, which can jointly be used (in a multimodal fashion) to further
boost the quality of super-resolution results. To this end, we propose a novel
multimodal multi-head convolutional attention module to super-resolve CT and
MRI scans. Our attention module uses the convolution operation to perform joint
spatial-channel attention on multiple concatenated input tensors, where the
kernel (receptive field) size controls the reduction rate of the spatial
attention and the number of convolutional filters controls the reduction rate
of the channel attention, respectively. We introduce multiple attention heads,
each head having a distinct receptive field size corresponding to a particular
reduction rate for the spatial attention. We integrate our multimodal
multi-head convolutional attention (MMHCA) into two deep neural architectures
for super-resolution and conduct experiments on three data sets. Our empirical
results show the superiority of our attention module over the state-of-the-art
attention mechanisms used in super-resolution. Moreover, we conduct an ablation
study to assess the impact of the components involved in our attention module,
e.g. the number of inputs or the number of heads.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NAN: Noise-Aware NeRFs for Burst-Denoising. (arXiv:2204.04668v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04668">
<div class="article-summary-box-inner">
<span><p>Burst denoising is now more relevant than ever, as computational photography
helps overcome sensitivity issues inherent in mobile phones and small cameras.
A major challenge in burst-denoising is in coping with pixel misalignment,
which was so far handled with rather simplistic assumptions of simple motion,
or the ability to align in pre-processing. Such assumptions are not realistic
in the presence of large motion and high levels of noise. We show that Neural
Radiance Fields (NeRFs), originally suggested for physics-based novel-view
rendering, can serve as a powerful framework for burst denoising. NeRFs have an
inherent capability of handling noise as they integrate information from
multiple images, but they are limited in doing so, mainly since they build on
pixel-wise operations which are suitable to ideal imaging conditions. Our
approach, termed NAN, leverages inter-view and spatial information in NeRFs to
better deal with noise. It achieves state-of-the-art results in burst denoising
and is especially successful in coping with large movement and occlusions,
under very high levels of noise. With the rapid advances in accelerating NeRFs,
it could provide a powerful platform for denoising in challenging environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coreset of Hyperspectral Images on Small Quantum Computer. (arXiv:2204.04691v2 [quant-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04691">
<div class="article-summary-box-inner">
<span><p>Machine Learning (ML) techniques are employed to analyze and process big
Remote Sensing (RS) data, and one well-known ML technique is a Support Vector
Machine (SVM). An SVM is a quadratic programming (QP) problem, and a D-Wave
quantum annealer (D-Wave QA) promises to solve this QP problem more efficiently
than a conventional computer. However, the D-Wave QA cannot solve directly the
SVM due to its very few input qubits. Hence, we use a coreset ("core of a
dataset") of given EO data for training an SVM on this small D-Wave QA. The
coreset is a small, representative weighted subset of an original dataset, and
any training models generate competitive classes by using the coreset in
contrast to by using its original dataset. We measured the closeness between an
original dataset and its coreset by employing a Kullback-Leibler (KL)
divergence measure. Moreover, we trained the SVM on the coreset data by using
both a D-Wave QA and a conventional method. We conclude that the coreset
characterizes the original dataset with very small KL divergence measure. In
addition, we present our KL divergence results for demonstrating the closeness
between our original data and its coreset. As practical RS data, we use
Hyperspectral Image (HSI) of Indian Pine, USA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pyramid Grafting Network for One-Stage High Resolution Saliency Detection. (arXiv:2204.05041v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05041">
<div class="article-summary-box-inner">
<span><p>Recent salient object detection (SOD) methods based on deep neural network
have achieved remarkable performance. However, most of existing SOD models
designed for low-resolution input perform poorly on high-resolution images due
to the contradiction between the sampling depth and the receptive field size.
Aiming at resolving this contradiction, we propose a novel one-stage framework
called Pyramid Grafting Network (PGNet), using transformer and CNN backbone to
extract features from different resolution images independently and then graft
the features from transformer branch to CNN branch. An attention-based
Cross-Model Grafting Module (CMGM) is proposed to enable CNN branch to combine
broken detailed information more holistically, guided by different source
feature during decoding process. Moreover, we design an Attention Guided Loss
(AGL) to explicitly supervise the attention matrix generated by CMGM to help
the network better interact with the attention from different models. We
contribute a new Ultra-High-Resolution Saliency Detection dataset UHRSD,
containing 5,920 images at 4K-8K resolutions. To our knowledge, it is the
largest dataset in both quantity and resolution for high-resolution SOD task,
which can be used for training and testing in future research. Sufficient
experiments on UHRSD and widely-used SOD datasets demonstrate that our method
achieves superior performance compared to the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-04-13 23:08:14.766644758 UTC">2022-04-13 23:08:14 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>