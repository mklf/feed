<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-05-17T01:30:00Z">05-17</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Deconstructing NLG Evaluation: Evaluation Practices, Assumptions, and Their Implications. (arXiv:2205.06828v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06828">
<div class="article-summary-box-inner">
<span><p>There are many ways to express similar things in text, which makes evaluating
natural language generation (NLG) systems difficult. Compounding this
difficulty is the need to assess varying quality criteria depending on the
deployment setting. While the landscape of NLG evaluation has been well-mapped,
practitioners' goals, assumptions, and constraints -- which inform decisions
about what, when, and how to evaluate -- are often partially or implicitly
stated, or not stated at all. Combining a formative semi-structured interview
study of NLG practitioners (N=18) with a survey study of a broader sample of
practitioners (N=61), we surface goals, community practices, assumptions, and
constraints that shape NLG evaluations, examining their implications and how
they embody ethical considerations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IRB-NLP at SemEval-2022 Task 1: Exploring the Relationship Between Words and Their Semantic Representations. (arXiv:2205.06840v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06840">
<div class="article-summary-box-inner">
<span><p>What is the relation between a word and its description, or a word and its
embedding? Both descriptions and embeddings are semantic representations of
words. But, what information from the original word remains in these
representations? Or more importantly, which information about a word do these
two representations share? Definition Modeling and Reverse Dictionary are two
opposite learning tasks that address these questions. The goal of the
Definition Modeling task is to investigate the power of information laying
inside a word embedding to express the meaning of the word in a humanly
understandable way -- as a dictionary definition. Conversely, the Reverse
Dictionary task explores the ability to predict word embeddings directly from
its definition. In this paper, by tackling these two tasks, we are exploring
the relationship between words and their semantic representations. We present
our findings based on the descriptive, exploratory, and predictive data
analysis conducted on the CODWOE dataset. We give a detailed overview of the
systems that we designed for Definition Modeling and Reverse Dictionary tasks,
and that achieved top scores on SemEval-2022 CODWOE challenge in several
subtasks. We hope that our experimental results concerning the predictive
models and the data analyses we provide will prove useful in future
explorations of word representations and their relationships.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Approach for Automatic Construction of an Algorithmic Knowledge Graph from Textual Resources. (arXiv:2205.06854v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06854">
<div class="article-summary-box-inner">
<span><p>There is enormous growth in various fields of research. This development is
accompanied by new problems. To solve these problems efficiently and in an
optimized manner, algorithms are created and described by researchers in the
scientific literature. Scientific algorithms are vital for understanding and
reusing existing work in numerous domains. However, algorithms are generally
challenging to find. Also, the comparison among similar algorithms is difficult
because of the disconnected documentation. Information about algorithms is
mostly present in websites, code comments, and so on. There is an absence of
structured metadata to portray algorithms. As a result, sometimes redundant or
similar algorithms are published, and the researchers build them from scratch
instead of reusing or expanding upon the already existing algorithm. In this
paper, we introduce an approach for automatically developing a knowledge graph
(KG) for algorithmic problems from unstructured data. Because it captures
information more clearly and extensively, an algorithm KG will give additional
context and explainability to the algorithm metadata.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment Analysis of Covid-related Reddits. (arXiv:2205.06863v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06863">
<div class="article-summary-box-inner">
<span><p>This paper focuses on Sentiment Analysis of Covid-19 related messages from
the r/Canada and r/Unitedkingdom subreddits of Reddit. We apply manual
annotation and three Machine Learning algorithms to analyze sentiments conveyed
in those messages. We use VADER and TextBlob to label messages for Machine
Learning experiments. Our results show that removal of shortest and longest
messages improves VADER and TextBlob agreement on positive sentiments and
F-score of sentiment classification by all the three algorithms
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Near-Negative Distinction: Giving a Second Life to Human Evaluation Datasets. (arXiv:2205.06871v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06871">
<div class="article-summary-box-inner">
<span><p>Precisely assessing the progress in natural language generation (NLG) tasks
is challenging, and human evaluation to establish preference in a model's
output over another is often necessary. However, human evaluation is usually
costly, difficult to reproduce, and non-reusable. In this paper, we propose a
new and simple automatic evaluation method for NLG called Near-Negative
Distinction (NND) that repurposes prior human annotations into NND tests. In an
NND test, an NLG model must place higher likelihood on a high-quality output
candidate than on a near-negative candidate with a known error. Model
performance is established by the number of NND tests a model passes, as well
as the distribution over task-specific errors the model fails on. Through
experiments on three NLG tasks (question generation, question answering, and
summarization), we show that NND achieves higher correlation with human
judgments than standard NLG evaluation metrics. We then illustrate NND
evaluation in four practical scenarios, for example performing fine-grain model
analysis, or studying model training dynamics. Our findings suggest NND can
give a second life to human annotations and provide low-cost NLG evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PathologyBERT -- Pre-trained Vs. A New Transformer Language Model for Pathology Domain. (arXiv:2205.06885v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06885">
<div class="article-summary-box-inner">
<span><p>Pathology text mining is a challenging task given the reporting variability
and constant new findings in cancer sub-type definitions. However, successful
text mining of a large pathology database can play a critical role to advance
'big data' cancer research like similarity-based treatment selection, case
identification, prognostication, surveillance, clinical trial screening, risk
stratification, and many others. While there is a growing interest in
developing language models for more specific clinical domains, no
pathology-specific language space exist to support the rapid data-mining
development in pathology space. In literature, a few approaches fine-tuned
general transformer models on specialized corpora while maintaining the
original tokenizer, but in fields requiring specialized terminology, these
models often fail to perform adequately. We propose PathologyBERT - a
pre-trained masked language model which was trained on 347,173 histopathology
specimen reports and publicly released in the Huggingface repository. Our
comprehensive experiments demonstrate that pre-training of transformer model on
pathology corpora yields performance improvements on Natural Language
Understanding (NLU) and Breast Cancer Diagnose Classification when compared to
nonspecific language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bootstrapping Text Anonymization Models with Distant Supervision. (arXiv:2205.06895v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06895">
<div class="article-summary-box-inner">
<span><p>We propose a novel method to bootstrap text anonymization models based on
distant supervision. Instead of requiring manually labeled training data, the
approach relies on a knowledge graph expressing the background information
assumed to be publicly available about various individuals. This knowledge
graph is employed to automatically annotate text documents including personal
data about a subset of those individuals. More precisely, the method determines
which text spans ought to be masked in order to guarantee $k$-anonymity,
assuming an adversary with access to both the text documents and the background
information expressed in the knowledge graph. The resulting collection of
labeled documents is then used as training data to fine-tune a pre-trained
language model for text anonymization. We illustrate this approach using a
knowledge graph extracted from Wikidata and short biographical texts from
Wikipedia. Evaluation results with a RoBERTa-based model and a manually
annotated collection of 553 summaries showcase the potential of the approach,
but also unveil a number of issues that may arise if the knowledge graph is
noisy or incomplete. The results also illustrate that, contrary to most
sequence labeling problems, the text anonymization task may admit several
alternative solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Developing a Production System for Purpose of Call Detection in Business Phone Conversations. (arXiv:2205.06904v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06904">
<div class="article-summary-box-inner">
<span><p>For agents at a contact centre receiving calls, the most important piece of
information is the reason for a given call. An agent cannot provide support on
a call if they do not know why a customer is calling. In this paper we describe
our implementation of a commercial system to detect Purpose of Call statements
in English business call transcripts in real time. We present a detailed
analysis of types of Purpose of Call statements and language patterns related
to them, discuss an approach to collect rich training data by bootstrapping
from a set of rules to a neural model, and describe a hybrid model which
consists of a transformer-based classifier and a set of rules by leveraging
insights from the analysis of call transcripts. The model achieved 88.6 F1 on
average in various types of business calls when tested on real life data and
has low inference time. We reflect on the challenges and design decisions when
developing and deploying the system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Property Induction Framework for Neural Language Models. (arXiv:2205.06910v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06910">
<div class="article-summary-box-inner">
<span><p>To what extent can experience from language contribute to our conceptual
knowledge? Computational explorations of this question have shed light on the
ability of powerful neural language models (LMs) -- informed solely through
text input -- to encode and elicit information about concepts and properties.
To extend this line of research, we present a framework that uses
neural-network language models (LMs) to perform property induction -- a task in
which humans generalize novel property knowledge (has sesamoid bones) from one
or more concepts (robins) to others (sparrows, canaries). Patterns of property
induction observed in humans have shed considerable light on the nature and
organization of human conceptual knowledge. Inspired by this insight, we use
our framework to explore the property inductions of LMs, and find that they
show an inductive preference to generalize novel properties on the basis of
category membership, suggesting the presence of a taxonomic bias in their
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Literal and Implied Subquestions to Fact-check Complex Claims. (arXiv:2205.06938v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06938">
<div class="article-summary-box-inner">
<span><p>Verifying complex political claims is a challenging task, especially when
politicians use various tactics to subtly misrepresent the facts. Automatic
fact-checking systems fall short here, and their predictions like "half-true"
are not very useful in isolation, since we have no idea which parts of the
claim are true and which are not. In this work, we focus on decomposing a
complex claim into a comprehensive set of yes-no subquestions whose answers
influence the veracity of the claim. We present ClaimDecomp, a dataset of
decompositions for over 1000 claims. Given a claim and its verification
paragraph written by fact-checkers, our trained annotators write subquestions
covering both explicit propositions of the original claim and its implicit
facets, such as asking about additional political context that changes our view
of the claim's veracity. We study whether state-of-the-art models can generate
such subquestions, showing that these models generate reasonable questions to
ask, but predicting the comprehensive set of subquestions from the original
claim without evidence remains challenging. We further show that these
subquestions can help identify relevant evidence to fact-check the full claim
and derive the veracity through their answers, suggesting that they can be
useful pieces of a fact-checking pipeline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auto-Select Reading Passages in English Assessment Tests?. (arXiv:2205.06961v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06961">
<div class="article-summary-box-inner">
<span><p>We show a method to auto-select reading passages in English assessment tests
and share some key insights that can be helpful in related fields. In
specifics, we prove that finding a similar passage (to a passage that already
appeared in the test) can give a suitable passage for test development. In the
process, we create a simple database-tagger-filter algorithm and perform a
human evaluation. However, 1. the textual features, that we analyzed, lack
coverage, and 2. we fail to find meaningful correlations between each feature
and suitability score. Lastly, we describe the future developments to improve
automated reading passage selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Consistency Training for Semi-Supervised Sequence-to-Sequence ASR via Speech Chain Reconstruction and Self-Transcribing. (arXiv:2205.06963v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06963">
<div class="article-summary-box-inner">
<span><p>Consistency regularization has recently been applied to semi-supervised
sequence-to-sequence (S2S) automatic speech recognition (ASR). This principle
encourages an ASR model to output similar predictions for the same input speech
with different perturbations. The existing paradigm of semi-supervised S2S ASR
utilizes SpecAugment as data augmentation and requires a static teacher model
to produce pseudo transcripts for untranscribed speech. However, this paradigm
fails to take full advantage of consistency regularization. First, the masking
operations of SpecAugment may damage the linguistic contents of the speech,
thus influencing the quality of pseudo labels. Second, S2S ASR requires both
input speech and prefix tokens to make the next prediction. The static prefix
tokens made by the offline teacher model cannot match dynamic pseudo labels
during consistency training. In this work, we propose an improved consistency
training paradigm of semi-supervised S2S ASR. We utilize speech chain
reconstruction as the weak augmentation to generate high-quality pseudo labels.
Moreover, we demonstrate that dynamic pseudo transcripts produced by the
student ASR model benefit the consistency training. Experiments on LJSpeech and
LibriSpeech corpora show that compared to supervised baselines, our improved
paradigm achieves a 12.2% CER improvement in the single-speaker setting and
38.6% in the multi-speaker setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACCoRD: A Multi-Document Approach to Generating Diverse Descriptions of Scientific Concepts. (arXiv:2205.06982v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06982">
<div class="article-summary-box-inner">
<span><p>Systems that can automatically define unfamiliar terms hold the promise of
improving the accessibility of scientific texts, especially for readers who may
lack prerequisite background knowledge. However, current systems assume a
single "best" description per concept, which fails to account for the many
potentially useful ways a concept can be described. We present ACCoRD, an
end-to-end system tackling the novel task of generating sets of descriptions of
scientific concepts. Our system takes advantage of the myriad ways a concept is
mentioned across the scientific literature to produce distinct, diverse
descriptions of target scientific concepts in terms of different reference
concepts. To support research on the task, we release an expert-annotated
resource, the ACCoRD corpus, which includes 1,275 labeled contexts and 1,787
hand-authored concept descriptions. We conduct a user study demonstrating that
(1) users prefer descriptions produced by our end-to-end system, and (2) users
prefer multiple descriptions to a single "best" description.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL. (arXiv:2205.06983v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06983">
<div class="article-summary-box-inner">
<span><p>Relational structures such as schema linking and schema encoding have been
validated as a key component to qualitatively translating natural language into
SQL queries. However, introducing these structural relations comes with prices:
they often result in a specialized model structure, which largely prohibits the
use of large pretrained models in text-to-SQL. To address this problem, we
propose RASAT: a Transformer seq2seq architecture augmented with relation-aware
self-attention that could leverage a variety of relational structures while at
the meantime being able to effectively inherit the pretrained parameters from
the T5 model. Our model is able to incorporate almost all types of existing
relations in the literature, and in addition, we propose to introduce
co-reference relations for the multi-turn scenario. Experimental results on
three widely used text-to-SQL datasets, covering both single-turn and
multi-turn scenarios, have shown that RASAT could achieve competitive results
in all three benchmarks, achieving state-of-the-art performance in execution
accuracy (80.5\% EX on Spider, 53.1\% IEX on SParC, and 37.5\% IEX on CoSQL).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Review-Based Tip Generation for Music Songs. (arXiv:2205.06985v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06985">
<div class="article-summary-box-inner">
<span><p>Reviews of songs play an important role in online music service platforms.
Prior research shows that users can make quicker and more informed decisions
when presented with meaningful song reviews. However, reviews of music songs
are generally long in length and most of them are non-informative for users. It
is difficult for users to efficiently grasp meaningful messages for making
decisions. To solve this problem, one practical strategy is to provide tips,
i.e., short, concise, empathetic, and self-contained descriptions about songs.
Tips are produced from song reviews and should express non-trivial insight
about the songs. To the best of our knowledge, no prior studies have explored
the tip generation task in music domain. In this paper, we create a dataset
named MTips for the task and propose a framework named GenTMS for automatically
generating tips from song reviews. The dataset involves 8,003 Chinese
tips/non-tips from 128 songs which are distributed in five different song
genres. Experimental results show that GenTMS achieves top-10 precision at
85.56%, outperforming the baseline models by at least 3.34%. Besides, to
simulate the practical usage of our proposed framework, we also experiment with
previously-unseen songs, during which GenTMS also achieves the best performance
with top-10 precision at 78.89% on average. The results demonstrate the
effectiveness of the proposed framework in tip generation of the music domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Neural Machine Translation of Indigenous Languages with Multilingual Transfer Learning. (arXiv:2205.06993v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06993">
<div class="article-summary-box-inner">
<span><p>Machine translation (MT) involving Indigenous languages, including those
possibly endangered, is challenging due to lack of sufficient parallel data. We
describe an approach exploiting bilingual and multilingual pretrained MT models
in a transfer learning setting to translate from Spanish to ten South American
Indigenous languages. Our models set new SOTA on five out of the ten language
pairs we consider, even doubling performance on one of these five pairs. Unlike
previous SOTA that perform data augmentation to enlarge the train sets, we
retain the low-resource setting to test the effectiveness of our models under
such a constraint. In spite of the rarity of linguistic information available
about the Indigenous languages, we offer a number of quantitative and
qualitative analyses (e.g., as to morphology, tokenization, and orthography) to
contextualize our results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integration of Text and Graph-based Features for Detecting Mental Health Disorders from Voice. (arXiv:2205.07006v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07006">
<div class="article-summary-box-inner">
<span><p>With the availability of voice-enabled devices such as smart phones, mental
health disorders could be detected and treated earlier, particularly
post-pandemic. The current methods involve extracting features directly from
audio signals. In this paper, two methods are used to enrich voice analysis for
depression detection: graph transformation of voice signals, and natural
language processing of the transcript based on representational learning, fused
together to produce final class labels. The results of experiments with the
DAIC-WOZ dataset suggest that integration of text-based voice classification
and learning from low level and graph-based voice signal features can improve
the detection of mental disorders like depression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Naturalistic Causal Probing for Morpho-Syntax. (arXiv:2205.07043v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07043">
<div class="article-summary-box-inner">
<span><p>Probing has become a go-to methodology for interpreting and analyzing deep
neural models in natural language processing. Yet recently, there has been much
debate around the limitations and weaknesses of probes. In this work, we
suggest a naturalistic strategy for input-level intervention on real world data
in Spanish, which is a language with gender marking. Using our approach, we
isolate morpho-syntactic features from counfounders in sentences, e.g. topic,
which will then allow us to causally probe pre-trained models. We apply this
methodology to analyze causal effects of gender and number on contextualized
representations extracted from pre-trained models -- BERT, RoBERTa and GPT-2.
Our experiments suggest that naturalistic intervention can give us stable
estimates of causal effects, which varies across different words in a sentence.
We further show the utility of our estimator in investigating gender bias in
adjectives, and answering counterfactual questions in masked prediction. Our
probing experiments highlights the importance of conducting causal probing in
determining if a particular property is encoded in representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What do Models Learn From Training on More Than Text? Measuring Visual Commonsense Knowledge. (arXiv:2205.07065v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07065">
<div class="article-summary-box-inner">
<span><p>There are limitations in learning language from text alone. Therefore, recent
focus has been on developing multimodal models. However, few benchmarks exist
that can measure what language models learn about language from multimodal
training. We hypothesize that training on a visual modality should improve on
the visual commonsense knowledge in language models. Therefore, we introduce
two evaluation tasks for measuring visual commonsense knowledge in language
models and use them to evaluate different multimodal models and unimodal
baselines. Primarily, we find that the visual commonsense knowledge is not
significantly different between the multimodal models and unimodal baseline
models trained on visual text data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pretraining Approaches for Spoken Language Recognition: TalTech Submission to the OLR 2021 Challenge. (arXiv:2205.07083v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07083">
<div class="article-summary-box-inner">
<span><p>This paper investigates different pretraining approaches to spoken language
identification. The paper is based on our submission to the Oriental Language
Recognition 2021 Challenge. We participated in two tracks of the challenge:
constrained and unconstrained language recognition. For the constrained track,
we first trained a Conformer-based encoder-decoder model for multilingual
automatic speech recognition (ASR), using the provided training data that had
transcripts available. The shared encoder of the multilingual ASR model was
then finetuned for the language identification task. For the unconstrained
task, we relied on both externally available pretrained models as well as
external data: the multilingual XLSR-53 wav2vec2.0 model was finetuned on the
VoxLingua107 corpus for the language recognition task, and finally finetuned on
the provided target language training data, augmented with CommonVoice data.
Our primary metric $C_{\rm avg}$ values on the Test set are 0.0079 for the
constrained task and 0.0119 for the unconstrained task which resulted in the
second place in both rankings. In post-evaluation experiments, we study the
amount of target language data needed for training an accurate backend model,
the importance of multilingual pretraining data, and compare different models
as finetuning starting points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collar-aware Training for Streaming Speaker Change Detection in Broadcast Speech. (arXiv:2205.07086v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07086">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel training method for speaker change
detection models. Speaker change detection is often viewed as a binary sequence
labelling problem. The main challenges with this approach are the vagueness of
annotated change points caused by the silences between speaker turns and
imbalanced data due to the majority of frames not including a speaker change.
Conventional training methods tackle these by artificially increasing the
proportion of positive labels in the training data. Instead, the proposed
method uses an objective function which encourages the model to predict a
single positive label within a specified collar. This is done by marginalizing
over all possible subsequences that have exactly one positive label within the
collar. Experiments on English and Estonian datasets show large improvements
over the conventional training method. Additionally, the model outputs have
peaks concentrated to a single frame, removing the need for post-processing to
find the exact predicted change point which is particularly useful for
streaming applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiformer: A Head-Configurable Transformer-Based Model for Direct Speech Translation. (arXiv:2205.07100v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07100">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have been achieving state-of-the-art results in
several fields of Natural Language Processing. However, its direct application
to speech tasks is not trivial. The nature of this sequences carries problems
such as long sequence lengths and redundancy between adjacent tokens.
Therefore, we believe that regular self-attention mechanism might not be well
suited for it.
</p>
<p>Different approaches have been proposed to overcome these problems, such as
the use of efficient attention mechanisms. However, the use of these methods
usually comes with a cost, which is a performance reduction caused by
information loss. In this study, we present the Multiformer, a
Transformer-based model which allows the use of different attention mechanisms
on each head. By doing this, the model is able to bias the self-attention
towards the extraction of more diverse token interactions, and the information
loss is reduced. Finally, we perform an analysis of the head contributions, and
we observe that those architectures where all heads relevance is uniformly
distributed obtain better results. Our results show that mixing attention
patterns along the different heads and layers outperforms our baseline by up to
0.7 BLEU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The VoicePrivacy 2020 Challenge Evaluation Plan. (arXiv:2205.07123v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07123">
<div class="article-summary-box-inner">
<span><p>The VoicePrivacy Challenge aims to promote the development of privacy
preservation tools for speech technology by gathering a new community to define
the tasks of interest and the evaluation methodology, and benchmarking
solutions through a series of challenges. In this document, we formulate the
voice anonymization task selected for the VoicePrivacy 2020 Challenge and
describe the datasets used for system development and evaluation. We also
present the attack models and the associated objective and subjective
evaluation metrics. We introduce two anonymization baselines and report
objective evaluation results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Generalizability of Fine-Tuned Models for Fake News Detection. (arXiv:2205.07154v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07154">
<div class="article-summary-box-inner">
<span><p>The Covid-19 pandemic has caused a dramatic and parallel rise in dangerous
misinformation, denoted an `infodemic' by the CDC and WHO. Misinformation tied
to the Covid-19 infodemic changes continuously; this can lead to performance
degradation of fine-tuned models due to concept drift. Degredation can be
mitigated if models generalize well-enough to capture some cyclical aspects of
drifted data. In this paper, we explore generalizability of pre-trained and
fine-tuned fake news detectors across 9 fake news datasets. We show that
existing models often overfit on their training dataset and have poor
performance on unseen data. However, on some subsets of unseen data that
overlap with training data, models have higher accuracy. Based on this
observation, we also present KMeans-Proxy, a fast and effective method based on
K-Means clustering for quickly identifying these overlapping subsets of unseen
data. KMeans-Proxy improves generalizability on unseen fake news datasets by
0.1-0.2 f1-points across datasets. We present both our generalizability
experiments as well as KMeans-Proxy to further research in tackling the fake
news problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Cognitive to Computational Modeling: Text-based Risky Decision-Making Guided by Fuzzy Trace Theory. (arXiv:2205.07164v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07164">
<div class="article-summary-box-inner">
<span><p>Understanding, modelling and predicting human risky decision-making is
challenging due to intrinsic individual differences and irrationality. Fuzzy
trace theory (FTT) is a powerful paradigm that explains human decision-making
by incorporating gists, i.e., fuzzy representations of information which
capture only its quintessential meaning. Inspired by Broniatowski and Reyna's
FTT cognitive model, we propose a computational framework which combines the
effects of the underlying semantics and sentiments on text-based
decision-making. In particular, we introduce Category-2-Vector to learn
categorical gists and categorical sentiments, and demonstrate how our
computational model can be optimised to predict risky decision-making in groups
and individuals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hero-Gang Neural Model For Named Entity Recognition. (arXiv:2205.07177v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07177">
<div class="article-summary-box-inner">
<span><p>Named entity recognition (NER) is a fundamental and important task in NLP,
aiming at identifying named entities (NEs) from free text. Recently, since the
multi-head attention mechanism applied in the Transformer model can effectively
capture longer contextual information, Transformer-based models have become the
mainstream methods and have achieved significant performance in this task.
Unfortunately, although these models can capture effective global context
information, they are still limited in the local feature and position
information extraction, which is critical in NER. In this paper, to address
this limitation, we propose a novel Hero-Gang Neural structure (HGN), including
the Hero and Gang module, to leverage both global and local information to
promote NER. Specifically, the Hero module is composed of a Transformer-based
encoder to maintain the advantage of the self-attention mechanism, and the Gang
module utilizes a multi-window recurrent module to extract local features and
position information under the guidance of the Hero module. Afterward, the
proposed multi-window attention effectively combines global information and
multiple local features for predicting entity labels. Experimental results on
several benchmark datasets demonstrate the effectiveness of our proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-tuning Pre-trained Language Models for Few-shot Intent Detection: Supervised Pre-training and Isotropization. (arXiv:2205.07208v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07208">
<div class="article-summary-box-inner">
<span><p>It is challenging to train a good intent classifier for a task-oriented
dialogue system with only a few annotations. Recent studies have shown that
fine-tuning pre-trained language models with a small amount of labeled
utterances from public benchmarks in a supervised manner is extremely helpful.
However, we find that supervised pre-training yields an anisotropic feature
space, which may suppress the expressive power of the semantic representations.
Inspired by recent research in isotropization, we propose to improve supervised
pre-training by regularizing the feature space towards isotropy. We propose two
regularizers based on contrastive learning and correlation matrix respectively,
and demonstrate their effectiveness through extensive experiments. Our main
finding is that it is promising to regularize supervised pre-training with
isotropization to further improve the performance of few-shot intent detection.
The source code can be found at https://github.com/fanolabs/isoIntentBert-main.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech Synthesis. (arXiv:2205.07211v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07211">
<div class="article-summary-box-inner">
<span><p>Style transfer for out-of-domain (OOD) speech synthesis aims to generate
speech samples with unseen style (e.g., speaker identity, emotion, and prosody)
derived from an acoustic reference, while facing the following challenges: 1)
The highly dynamic style features in expressive voice are difficult to model
and transfer; and 2) the TTS models should be robust enough to handle diverse
OOD conditions that differ from the source data. This paper proposes
GenerSpeech, a text-to-speech model towards high-fidelity zero-shot style
transfer of OOD custom voice. GenerSpeech decomposes the speech variation into
the style-agnostic and style-specific parts by introducing two components: 1) a
multi-level style adaptor to efficiently model a large range of style
conditions, including global speaker and emotion characteristics, and the local
(utterance, phoneme, and word-level) fine-grained prosodic representations; and
2) a generalizable content adaptor with Mix-Style Layer Normalization to
eliminate style information in the linguistic content representation and thus
improve model generalization. Our evaluations on zero-shot style transfer
demonstrate that GenerSpeech surpasses the state-of-the-art models in terms of
audio quality and style similarity. The extension studies to adaptive style
transfer further show that GenerSpeech performs robustly in the few-shot data
setting. Audio samples are available at \url{https://GenerSpeech.github.io/}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Prompt Learning-based Few-Shot Sentiment Analysis. (arXiv:2205.07220v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07220">
<div class="article-summary-box-inner">
<span><p>In the field of natural language processing, sentiment analysis via deep
learning has a excellent performance by using large labeled datasets.
Meanwhile, labeled data are insufficient in many sentiment analysis, and
obtaining these data is time-consuming and laborious. Prompt learning devotes
to resolving the data deficiency by reformulating downstream tasks with the
help of prompt. In this way, the appropriate prompt is very important for the
performance of the model. This paper proposes an adaptive prompting(AP)
construction strategy using seq2seq-attention structure to acquire the semantic
information of the input sequence. Then dynamically construct adaptive prompt
which can not only improve the quality of the prompt, but also can effectively
generalize to other fields by pre-trained prompt which is constructed by
existing public labeled data. The experimental results on FewCLUE datasets
demonstrate that the proposed method AP can effectively construct appropriate
adaptive prompt regardless of the quality of hand-crafted prompt and outperform
the state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Toxic Degeneration with Empathetic Data: Exploring the Relationship Between Toxicity and Empathy. (arXiv:2205.07233v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07233">
<div class="article-summary-box-inner">
<span><p>Large pre-trained neural language models have supported the effectiveness of
many NLP tasks, yet are still prone to generating toxic language hindering the
safety of their use. Using empathetic data, we improve over recent work on
controllable text generation that aims to reduce the toxicity of generated
text. We find we are able to dramatically reduce the size of fine-tuning data
to 7.5-30k samples while at the same time making significant improvements over
state-of-the-art toxicity mitigation of up to 3.4% absolute reduction (26%
relative) from the original work on 2.3m samples, by strategically sampling
data based on empathy scores. We observe that the degree of improvement is
subject to specific communication components of empathy. In particular, the
cognitive components of empathy significantly beat the original dataset in
almost all experiments, while emotional empathy was tied to less improvement
and even underperforming random samples of the original data. This is a
particularly implicative insight for NLP work concerning empathy as until
recently the research and resources built for it have exclusively considered
empathy as an emotional concept.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering Latent Concepts Learned in BERT. (arXiv:2205.07237v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07237">
<div class="article-summary-box-inner">
<span><p>A large number of studies that analyze deep neural network models and their
ability to encode various linguistic and non-linguistic concepts provide an
interpretation of the inner mechanics of these models. The scope of the
analyses is limited to pre-defined concepts that reinforce the traditional
linguistic knowledge and do not reflect on how novel concepts are learned by
the model. We address this limitation by discovering and analyzing latent
concepts learned in neural network models in an unsupervised fashion and
provide interpretations from the model's perspective. In this work, we study:
i) what latent concepts exist in the pre-trained BERT model, ii) how the
discovered latent concepts align or diverge from classical linguistic hierarchy
and iii) how the latent concepts evolve across layers. Our findings show: i) a
model learns novel concepts (e.g. animal categories and demographic groups),
which do not strictly adhere to any pre-defined categorization (e.g. POS,
semantic tags), ii) several latent concepts are based on multiple properties
which may include semantics, syntax, and morphology, iii) the lower layers in
the model dominate in learning shallow lexical concepts while the higher layers
learn semantic relations and iv) the discovered latent concepts highlight
potential biases learned in the model. We also release a novel BERT ConceptNet
dataset (BCN) consisting of 174 concept labels and 1M annotated instances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not to Overfit or Underfit? A Study of Domain Generalization in Question Answering. (arXiv:2205.07257v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07257">
<div class="article-summary-box-inner">
<span><p>Machine learning models are prone to overfitting their source (training)
distributions, which is commonly believed to be why they falter in novel target
domains. Here we examine the contrasting view that multi-source domain
generalization (DG) is in fact a problem of mitigating source domain
underfitting: models not adequately learning the signal in their multi-domain
training data. Experiments on a reading comprehension DG benchmark show that as
a model gradually learns its source domains better -- using known methods such
as knowledge distillation from a larger model -- its zero-shot out-of-domain
accuracy improves at an even faster rate. Improved source domain learning also
demonstrates superior generalization over three popular domain-invariant
learning methods that aim to counter overfitting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic Modelling on Consumer Financial Protection Bureau Data: An Approach Using BERT Based Embeddings. (arXiv:2205.07259v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07259">
<div class="article-summary-box-inner">
<span><p>Customers' reviews and comments are important for businesses to understand
users' sentiment about the products and services. However, this data needs to
be analyzed to assess the sentiment associated with topics/aspects to provide
efficient customer assistance. LDA and LSA fail to capture the semantic
relationship and are not specific to any domain. In this study, we evaluate
BERTopic, a novel method that generates topics using sentence embeddings on
Consumer Financial Protection Bureau (CFPB) data. Our work shows that BERTopic
is flexible and yet provides meaningful and diverse topics compared to LDA and
LSA. Furthermore, domain-specific pre-trained embeddings (FinBERT) yield even
better topics. We evaluated the topics on coherence score (c_v) and UMass.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Textual Explanations and Critiques in Recommendation Systems. (arXiv:2205.07268v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07268">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence and machine learning algorithms have become
ubiquitous. Although they offer a wide range of benefits, their adoption in
decision-critical fields is limited by their lack of interpretability,
particularly with textual data. Moreover, with more data available than ever
before, it has become increasingly important to explain automated predictions.
</p>
<p>Generally, users find it difficult to understand the underlying computational
processes and interact with the models, especially when the models fail to
generate the outcomes or explanations, or both, correctly. This problem
highlights the growing need for users to better understand the models' inner
workings and gain control over their actions. This dissertation focuses on two
fundamental challenges of addressing this need. The first involves explanation
generation: inferring high-quality explanations from text documents in a
scalable and data-driven manner. The second challenge consists in making
explanations actionable, and we refer to it as critiquing. This dissertation
examines two important applications in natural language processing and
recommendation tasks.
</p>
<p>Overall, we demonstrate that interpretability does not come at the cost of
reduced performance in two consequential applications. Our framework is
applicable to other fields as well. This dissertation presents an effective
means of closing the gap between promise and practice in artificial
intelligence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classifiers are Better Experts for Controllable Text Generation. (arXiv:2205.07276v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07276">
<div class="article-summary-box-inner">
<span><p>This paper proposes a simple method for controllable text generation based on
weighting logits produced, namely CAIF sampling. Using an arbitrary third-party
text classifier, we adjust a small part of a language model's logits and guide
text generation towards or away from classifier prediction. We show that the
proposed method significantly outperforms recent PPLM, GeDi, and DExperts on
PPL and sentiment accuracy based on the external classifier of generated texts.
A the same time, it is also easier to implement and tune, and has significantly
fewer restrictions and requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptation in Multilingual and Multi-Domain Monolingual Settings for Complex Word Identification. (arXiv:2205.07283v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07283">
<div class="article-summary-box-inner">
<span><p>Complex word identification (CWI) is a cornerstone process towards proper
text simplification. CWI is highly dependent on context, whereas its difficulty
is augmented by the scarcity of available datasets which vary greatly in terms
of domains and languages. As such, it becomes increasingly more difficult to
develop a robust model that generalizes across a wide array of input examples.
In this paper, we propose a novel training technique for the CWI task based on
domain adaptation to improve the target character and context representations.
This technique addresses the problem of working with multiple domains, inasmuch
as it creates a way of smoothing the differences between the explored datasets.
Moreover, we also propose a similar auxiliary task, namely text simplification,
that can be used to complement lexical complexity prediction. Our model obtains
a boost of up to 2.42% in terms of Pearson Correlation Coefficients in contrast
to vanilla training techniques, when considering the CompLex from the Lexical
Complexity Prediction 2021 dataset. At the same time, we obtain an increase of
3% in Pearson scores, while considering a cross-lingual setup relying on the
Complex Word Identification 2018 dataset. In addition, our model yields
state-of-the-art results in terms of Mean Absolute Error.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta Self-Refinement for Robust Learning with Weak Supervision. (arXiv:2205.07290v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07290">
<div class="article-summary-box-inner">
<span><p>Training deep neural networks (DNNs) with weak supervision has been a hot
topic as it can significantly reduce the annotation cost. However, labels from
weak supervision can be rather noisy and the high capacity of DNNs makes them
easy to overfit the noisy labels. Recent methods leverage self-training
techniques to train noise-robust models, where a teacher trained on noisy
labels is used to teach a student. However, the teacher from such models might
fit a substantial amount of noise and produce wrong pseudo-labels with high
confidence, leading to error propagation. In this work, we propose Meta
Self-Refinement (MSR), a noise-resistant learning framework, to effectively
combat noisy labels from weak supervision sources. Instead of purely relying on
a fixed teacher trained on noisy labels, we keep updating the teacher to refine
its pseudo-labels. At each training step, it performs a meta gradient descent
on the current mini-batch to maximize the student performance on a clean
validation set. Extensive experimentation on eight NLP benchmarks demonstrates
that MSR is robust against noise in all settings and outperforms the
state-of-the-art up to 11.4% in accuracy and 9.26% in F1 score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TiBERT: Tibetan Pre-trained Language Model. (arXiv:2205.07303v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07303">
<div class="article-summary-box-inner">
<span><p>The pre-trained language model is trained on large-scale unlabeled text and
can achieve state-of-the-art results in many different downstream tasks.
However, the current pre-trained language model is mainly concentrated in the
Chinese and English fields. For low resource language such as Tibetan, there is
lack of a monolingual pre-trained model. To promote the development of Tibetan
natural language processing tasks, this paper collects the large-scale training
data from Tibetan websites and constructs a vocabulary that can cover 99.95$\%$
of the words in the corpus by using Sentencepiece. Then, we train the Tibetan
monolingual pre-trained language model named TiBERT on the data and vocabulary.
Finally, we apply TiBERT to the downstream tasks of text classification and
question generation, and compare it with classic models and multilingual
pre-trained models, the experimental results show that TiBERT can achieve the
best performance. Our model is published in <a href="http://tibert.cmli-nlp.com/">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transkimmer: Transformer Learns to Layer-wise Skim. (arXiv:2205.07324v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07324">
<div class="article-summary-box-inner">
<span><p>Transformer architecture has become the de-facto model for many machine
learning tasks from natural language processing and computer vision. As such,
improving its computational efficiency becomes paramount. One of the major
computational inefficiency of Transformer-based models is that they spend the
identical amount of computation throughout all layers. Prior works have
proposed to augment the Transformer model with the capability of skimming
tokens to improve its computational efficiency. However, they suffer from not
having effectual and end-to-end optimization of the discrete skimming
predictor. To address the above limitations, we propose the Transkimmer
architecture, which learns to identify hidden state tokens that are not
required by each layer. The skimmed tokens are then forwarded directly to the
final output, thus reducing the computation of the successive layers. The key
idea in Transkimmer is to add a parameterized predictor before each layer that
learns to make the skimming decision. We also propose to adopt
reparameterization trick and add skim loss for the end-to-end training of
Transkimmer. Transkimmer achieves 10.97x average speedup on GLUE benchmark
compared with vanilla BERT-base baseline with less than 1% accuracy
degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-term Control for Dialogue Generation: Methods and Evaluation. (arXiv:2205.07352v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07352">
<div class="article-summary-box-inner">
<span><p>Current approaches for controlling dialogue response generation are primarily
focused on high-level attributes like style, sentiment, or topic. In this work,
we focus on constrained long-term dialogue generation, which involves more
fine-grained control and requires a given set of control words to appear in
generated responses. This setting requires a model to not only consider the
generation of these control words in the immediate context, but also produce
utterances that will encourage the generation of the words at some time in the
(possibly distant) future. We define the problem of constrained long-term
control for dialogue generation, identify gaps in current methods for
evaluation, and propose new metrics that better measure long-term control. We
also propose a retrieval-augmented method that improves performance of
long-term controlled generation via logit modification techniques. We show
through experiments on three task-oriented dialogue datasets that our metrics
better assess dialogue control relative to current alternatives and that our
method outperforms state-of-the-art constrained generation baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeqZero: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models. (arXiv:2205.07381v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07381">
<div class="article-summary-box-inner">
<span><p>Recent research showed promising results on combining pretrained language
models (LMs) with canonical utterance for few-shot semantic parsing. The
canonical utterance is often lengthy and complex due to the compositional
structure of formal languages. Learning to generate such canonical utterance
requires significant amount of data to reach high performance. Fine-tuning with
only few-shot samples, the LMs can easily forget pretrained knowledge, overfit
spurious biases, and suffer from compositionally out-of-distribution
generalization errors. To tackle these issues, we propose a novel few-shot
semantic parsing method -- SeqZero. SeqZero decomposes the problem into a
sequence of sub-problems, which correspond to the sub-clauses of the formal
language. Based on the decomposition, the LMs only need to generate short
answers using prompts for predicting sub-clauses. Thus, SeqZero avoids
generating a long canonical utterance at once. Moreover, SeqZero employs not
only a few-shot model but also a zero-shot model to alleviate the overfitting.
In particular, SeqZero brings out the merits from both models via ensemble
equipped with our proposed constrained rescaling. SeqZero achieves SOTA
performance of BART-based models on GeoQuery and EcommerceQuery, which are two
few-shot datasets with compositional data split.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Downstream Transformer Generation of Question-Answer Pairs with Preprocessing and Postprocessing Pipelines. (arXiv:2205.07387v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07387">
<div class="article-summary-box-inner">
<span><p>We present a system called TP3 to perform a downstream task of transformers
on generating question-answer pairs (QAPs) from a given article. TP3 first
finetunes pretrained transformers on QAP datasets, then uses a preprocessing
pipeline to select appropriate answers, feeds the relevant sentences and the
answer to the finetuned transformer to generate candidate QAPs, and finally
uses a postprocessing pipeline to filter inadequate QAPs. In particular, using
pretrained T5 models as transformers and the SQuAD dataset as the finetruning
dataset, we show that TP3 generates satisfactory number of QAPs with high
qualities on the Gaokao-EN dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What GPT Knows About Who is Who. (arXiv:2205.07407v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07407">
<div class="article-summary-box-inner">
<span><p>Coreference resolution -- which is a crucial task for understanding discourse
and language at large -- has yet to witness widespread benefits from large
language models (LLMs). Moreover, coreference resolution systems largely rely
on supervised labels, which are highly expensive and difficult to annotate,
thus making it ripe for prompt engineering. In this paper, we introduce a
QA-based prompt-engineering method and discern \textit{generative}, pre-trained
LLMs' abilities and limitations toward the task of coreference resolution. Our
experiments show that GPT-2 and GPT-Neo can return valid answers, but that
their capabilities to identify coreferent mentions are limited and
prompt-sensitive, leading to inconsistent results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Miutsu: NTU's TaskBot for the Alexa Prize. (arXiv:2205.07446v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07446">
<div class="article-summary-box-inner">
<span><p>This paper introduces Miutsu, National Taiwan University's Alexa Prize
TaskBot, which is designed to assist users in completing tasks requiring
multiple steps and decisions in two different domains -- home improvement and
cooking. We overview our system design and architectural goals, and detail the
proposed core elements, including question answering, task retrieval, social
chatting, and various conversational modules. A dialogue flow is proposed to
provide a robust and engaging conversation when handling complex tasks. We
discuss the faced challenges during the competition and potential future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning about Procedures with Natural Language Processing: A Tutorial. (arXiv:2205.07455v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07455">
<div class="article-summary-box-inner">
<span><p>This tutorial provides a comprehensive and in-depth view of the research on
procedures, primarily in Natural Language Processing. A procedure is a sequence
of steps intended to achieve some goal. Understanding procedures in natural
language has a long history, with recent breakthroughs made possible by
advances in technology. First, we discuss established approaches to collect
procedures, by human annotation or extraction from web resources. Then, we
examine different angles from which procedures can be reasoned about, as well
as ways to represent them. Finally, we enumerate scenarios where procedural
knowledge can be applied to the real world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Directed Acyclic Transformer for Non-Autoregressive Machine Translation. (arXiv:2205.07459v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07459">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive Transformers (NATs) significantly reduce the decoding
latency by generating all tokens in parallel. However, such independent
predictions prevent NATs from capturing the dependencies between the tokens for
generating multiple possible translations. In this paper, we propose Directed
Acyclic Transfomer (DA-Transformer), which represents the hidden states in a
Directed Acyclic Graph (DAG), where each path of the DAG corresponds to a
specific translation. The whole DAG simultaneously captures multiple
translations and facilitates fast predictions in a non-autoregressive fashion.
Experiments on the raw training data of WMT benchmark show that DA-Transformer
substantially outperforms previous NATs by about 3 BLEU on average, which is
the first NAT model that achieves competitive results with autoregressive
Transformers without relying on knowledge distillation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Distributed Representation of News (DRNews) for Stock Market Predictions. (arXiv:2005.11706v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.11706">
<div class="article-summary-box-inner">
<span><p>In this study, a novel Distributed Representation of News (DRNews) model is
developed and applied in deep learning-based stock market predictions. With the
merit of integrating contextual information and cross-documental knowledge, the
DRNews model creates news vectors that describe both the semantic information
and potential linkages among news events through an attributed news network.
Two stock market prediction tasks, namely the short-term stock movement
prediction and stock crises early warning, are implemented in the framework of
the attention-based Long Short Term-Memory (LSTM) network. It is suggested that
DRNews substantially enhances the results of both tasks comparing with five
baselines of news embedding models. Further, the attention mechanism suggests
that short-term stock trend and stock market crises both receive influences
from daily news with the former demonstrates more critical responses on the
information related to the stock market {\em per se}, whilst the latter draws
more concerns on the banking sector and economic policies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Gender Bias in Speech Translation. (arXiv:2010.14465v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.14465">
<div class="article-summary-box-inner">
<span><p>The scientific community is increasingly aware of the necessity to embrace
pluralism and consistently represent major and minor social groups. Currently,
there are no standard evaluation techniques for different types of biases.
Accordingly, there is an urgent need to provide evaluation sets and protocols
to measure existing biases in our automatic systems. Evaluating the biases
should be an essential step towards mitigating them in the systems.
</p>
<p>This paper introduces WinoST, a new freely available challenge set for
evaluating gender bias in speech translation. WinoST is the speech version of
WinoMT which is a MT challenge set and both follow an evaluation protocol to
measure gender accuracy. Using a state-of-the-art end-to-end speech translation
system, we report the gender bias evaluation on four language pairs and we show
that gender accuracy in speech translation is more than 23% lower than in MT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing the Transformer Decoder with Transition-based Syntax. (arXiv:2101.12640v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.12640">
<div class="article-summary-box-inner">
<span><p>Notwithstanding recent advances, syntactic generalization remains a challenge
for text decoders. While some studies showed gains from incorporating
source-side symbolic syntactic and semantic structure into text generation
Transformers, very little work addressed the decoding of such structure. We
propose a general approach for tree decoding using a transition-based approach.
Examining the challenging test case of incorporating Universal Dependencies
syntax into machine translation, we present substantial improvements on test
sets that focus on syntactic generalization, while presenting improved or
comparable performance on standard MT benchmarks. Further qualitative analysis
addresses cases where syntactic generalization in the vanilla Transformer
decoder is inadequate and demonstrates the advantages afforded by integrating
syntactic information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning From Human Correction For Data-Centric Deep Learning. (arXiv:2102.00225v8 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00225">
<div class="article-summary-box-inner">
<span><p>In industry NLP application, our manually labeled data has a certain number
of noisy data. We present a simple method to find the noisy data and relabel
them manually, meanwhile we collect the correction information. Then we present
novel method to incorporate the human correction information into deep learning
model. Human know how to correct noisy data. So the correction information can
be inject into deep learning model. We do the experiment on our own text
classification dataset, which is manually labeled, because we relabel the noisy
data in our dataset for our industry application. The experiment result shows
that our method improve the classification accuracy from 91.7% to 92.5%. The
91.7% accuracy is trained on the corrected dataset, which improve the baseline
from 83.3% to 91.7%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chess as a Testbed for Language Model State Tracking. (arXiv:2102.13249v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.13249">
<div class="article-summary-box-inner">
<span><p>Transformer language models have made tremendous strides in natural language
understanding tasks. However, the complexity of natural language makes it
challenging to ascertain how accurately these models are tracking the world
state underlying the text. Motivated by this issue, we consider the task of
language modeling for the game of chess. Unlike natural language, chess
notations describe a simple, constrained, and deterministic domain. Moreover,
we observe that the appropriate choice of chess notation allows for directly
probing the world state, without requiring any additional probing-related
machinery. We find that: (a) With enough training data, transformer language
models can learn to track pieces and predict legal moves with high accuracy
when trained solely on move sequences. (b) For small training sets providing
access to board state information during training can yield significant
improvements. (c) The success of transformer language models is dependent on
access to the entire game history i.e. "full attention". Approximating this
full attention results in a significant performance drop. We propose this
testbed as a benchmark for future work on the development and analysis of
transformer language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inferring the Reader: Guiding Automated Story Generation with Commonsense Reasoning. (arXiv:2105.01311v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01311">
<div class="article-summary-box-inner">
<span><p>Transformer-based language model approaches to automated story generation
currently provide state-of-the-art results. However, they still suffer from
plot incoherence when generating narratives over time, and critically lack
basic commonsense reasoning. Furthermore, existing methods generally focus only
on single-character stories, or fail to track characters at all. To improve the
coherence of generated narratives and to expand the scope of character-centric
narrative generation, we introduce Commonsense-inference Augmented neural
StoryTelling (CAST), a framework for introducing commonsense reasoning into the
generation process with the option to model the interaction between multiple
characters. We find that our CAST method produces significantly more coherent,
on-topic, enjoyable, and fluent stories than existing models in both the
single-character and two-character settings in three storytelling domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Diversity and Limits of Human Explanations. (arXiv:2106.11988v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11988">
<div class="article-summary-box-inner">
<span><p>A growing effort in NLP aims to build datasets of human explanations.
However, the term explanation encompasses a broad range of notions, each with
different properties and ramifications. Our goal is to provide an overview of
diverse types of explanations and human limitations, and discuss implications
for collecting and using explanations in NLP. Inspired by prior work in
psychology and cognitive sciences, we group existing human explanations in NLP
into three categories: proximal mechanism, evidence, and procedure. These three
types differ in nature and have implications for the resultant explanations.
For instance, procedure is not considered explanations in psychology and
connects with a rich body of work on learning from instructions. The diversity
of explanations is further evidenced by proxy questions that are needed for
annotators to interpret and answer open-ended why questions. Finally,
explanations may require different, often deeper, understandings than
predictions, which casts doubt on whether humans can provide useful
explanations in some tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion Recognition under Consideration of the Emotion Component Process Model. (arXiv:2107.12895v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12895">
<div class="article-summary-box-inner">
<span><p>Emotion classification in text is typically performed with neural network
models which learn to associate linguistic units with emotions. While this
often leads to good predictive performance, it does only help to a limited
degree to understand how emotions are communicated in various domains. The
emotion component process model (CPM) by Scherer (2005) is an interesting
approach to explain emotion communication. It states that emotions are a
coordinated process of various subcomponents, in reaction to an event, namely
the subjective feeling, the cognitive appraisal, the expression, a
physiological bodily reaction, and a motivational action tendency. We
hypothesize that these components are associated with linguistic realizations:
an emotion can be expressed by describing a physiological bodily reaction ("he
was trembling"), or the expression ("she smiled"), etc. We annotate existing
literature and Twitter emotion corpora with emotion component classes and find
that emotions on Twitter are predominantly expressed by event descriptions or
subjective reports of the feeling, while in literature, authors prefer to
describe what characters do, and leave the interpretation to the reader. We
further include the CPM in a multitask learning model and find that this
supports the emotion categorization. The annotated corpora are available at
https://www.ims.uni-stuttgart.de/data/emotion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion Stimulus Detection in German News Headlines. (arXiv:2107.12920v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12920">
<div class="article-summary-box-inner">
<span><p>Emotion stimulus extraction is a fine-grained subtask of emotion analysis
that focuses on identifying the description of the cause behind an emotion
expression from a text passage (e.g., in the sentence "I am happy that I passed
my exam" the phrase "passed my exam" corresponds to the stimulus.). Previous
work mainly focused on Mandarin and English, with no resources or models for
German. We fill this research gap by developing a corpus of 2006 German news
headlines annotated with emotions and 811 instances with annotations of
stimulus phrases. Given that such corpus creation efforts are time-consuming
and expensive, we additionally work on an approach for projecting the existing
English GoodNewsEveryone (GNE) corpus to a machine-translated German version.
We compare the performance of a conditional random field (CRF) model (trained
monolingually on German and cross-lingually via projection) with a multilingual
XLM-RoBERTa (XLM-R) model. Our results show that training with the German
corpus achieves higher F1 scores than projection. Experiments with XLM-R
outperform their respective CRF counterparts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. (arXiv:2108.10904v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10904">
<div class="article-summary-box-inner">
<span><p>With recent progress in joint modeling of visual and textual representations,
Vision-Language Pretraining (VLP) has achieved impressive performance on many
multimodal downstream tasks. However, the requirement for expensive annotations
including clean image captions and regional labels limits the scalability of
existing approaches, and complicates the pretraining procedure with the
introduction of multiple dataset-specific objectives. In this work, we relax
these constraints and present a minimalist pretraining framework, named Simple
Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training
complexity by exploiting large-scale weak supervision, and is trained
end-to-end with a single prefix language modeling objective. Without utilizing
extra data or task-specific customization, the resulting model significantly
outperforms previous pretraining methods and achieves new state-of-the-art
results on a wide range of discriminative and generative vision-language
benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE
(+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score).
Furthermore, we demonstrate that SimVLM acquires strong generalization and
transfer ability, enabling zero-shot behavior including open-ended visual
question answering and cross-modality transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Selective Differential Privacy for Language Modeling. (arXiv:2108.12944v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12944">
<div class="article-summary-box-inner">
<span><p>With the increasing applications of language models, it has become crucial to
protect these models from leaking private information. Previous work has
attempted to tackle this challenge by training RNN-based language models with
differential privacy guarantees. However, applying classical differential
privacy to language models leads to poor model performance as the underlying
privacy notion is over-pessimistic and provides undifferentiated protection for
all tokens in the data. Given that the private information in natural language
is sparse (for example, the bulk of an email might not carry personally
identifiable information), we propose a new privacy notion, selective
differential privacy, to provide rigorous privacy guarantees on the sensitive
portion of the data to improve model utility. To realize such a new notion, we
develop a corresponding privacy mechanism, Selective-DPSGD, for RNN-based
language models. Besides language modeling, we also apply the method to a more
concrete application--dialog systems. Experiments on both language modeling and
dialog system building show that the proposed privacy-preserving mechanism
achieves better utilities while remaining safe under various privacy attacks
compared to the baselines. The data and code are released at
https://github.com/wyshi/lm_privacy to facilitate future research .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges in Generalization in Open Domain Question Answering. (arXiv:2109.01156v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01156">
<div class="article-summary-box-inner">
<span><p>Recent work on Open Domain Question Answering has shown that there is a large
discrepancy in model performance between novel test questions and those that
largely overlap with training questions. However, it is unclear which aspects
of novel questions make them challenging. Drawing upon studies on systematic
generalization, we introduce and annotate questions according to three
categories that measure different levels and kinds of generalization: training
set overlap, compositional generalization (comp-gen), and novel-entity
generalization (novel-entity). When evaluating six popular parametric and
non-parametric models, we find that for the established Natural Questions and
TriviaQA datasets, even the strongest model performance for
comp-gen/novel-entity is 13.1/5.4% and 9.6/1.5% lower compared to that for the
full test set -- indicating the challenge posed by these types of questions.
Furthermore, we show that whilst non-parametric models can handle questions
containing novel entities relatively well, they struggle with those requiring
compositional generalization. Lastly, we find that key question difficulty
factors are: cascading errors from the retrieval component, frequency of
question pattern, and frequency of the entity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balancing out Bias: Achieving Fairness Through Balanced Training. (arXiv:2109.08253v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08253">
<div class="article-summary-box-inner">
<span><p>Group bias in natural language processing tasks manifests as disparities in
system error rates across texts authorized by different demographic groups,
typically disadvantaging minority groups. Dataset balancing has been shown to
be effective at mitigating bias, however existing approaches do not directly
account for correlations between author demographics and linguistic variables,
limiting their effectiveness. To achieve Equal Opportunity fairness, such as
equal job opportunity without regard to demographics, this paper introduces a
simple, but highly effective, objective for countering bias using balanced
training. We extend the method in the form of a gated model, which incorporates
protected attributes as input, and show that it is effective at reducing bias
in predictions through demographic input perturbation, outperforming all other
bias mitigation techniques when combined with balanced training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying the Suicidal Tendency on Social Media: A Survey. (arXiv:2110.03663v2 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03663">
<div class="article-summary-box-inner">
<span><p>Amid lockdown period more people express their feelings over social media
platforms due to closed third-place and academic researchers have witnessed
strong associations between the mental healthcare and social media posts. The
stress for a brief period may lead to clinical depressions and the long-lasting
traits of prevailing depressions can be life threatening with suicidal ideation
as the possible outcome. The increasing concern towards the rise in number of
suicide cases is because it is one of the leading cause of premature but
preventable death. Recent studies have shown that mining social media data has
helped in quantifying the suicidal tendency of users at risk. This potential
manuscript elucidates the taxonomy of mental healthcare and highlights some
recent attempts in examining the potential of quantifying suicidal tendency on
social media data. This manuscript presents the classification of heterogeneous
features from social media data and handling feature vector representation.
Aiming to identify the new research directions and advances in the development
of Machine Learning (ML) and Deep Learning (DL) based models, a quantitative
synthesis and a qualitative review was carried out with corpus of over 77
potential research articles related to stress, depression and suicide risk from
2013 to 2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Case Study on the Independence of Speech Emotion Recognition in Bangla and English Languages using Language-Independent Prosodic Features. (arXiv:2111.10776v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10776">
<div class="article-summary-box-inner">
<span><p>A language agnostic approach to recognizing emotions from speech remains an
incomplete and challenging task. In this paper, we performed a step-by-step
comparative analysis of Speech Emotion Recognition (SER) using Bangla and
English languages to assess whether distinguishing emotions from speech is
independent of language. Six emotions were categorized for this study, such as
- happy, angry, neutral, sad, disgust, and fear. We employed three Emotional
Speech Sets (ESS), of which the first two were developed by native Bengali
speakers in Bangla and English languages separately. The third was a subset of
the Toronto Emotional Speech Set (TESS), which was developed by native English
speakers from Canada. We carefully selected language-independent prosodic
features, adopted a Support Vector Machine (SVM) model, and conducted three
experiments to carry out our proposition. In the first experiment, we measured
the performance of the three speech sets individually, followed by the second
experiment, where different ESS pairs were integrated to analyze the impact on
SER. Finally, we measured the recognition rate by training and testing the
model with different speech sets in the third experiment. Although this study
reveals that SER in Bangla and English languages is mostly
language-independent, some disparities were observed while recognizing
emotional states like disgust and fear in these two languages. Moreover, our
investigations revealed that non-native speakers convey emotions through
speech, much like expressing themselves in their native tongue.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Injecting Numerical Reasoning Skills into Knowledge Base Question Answering Models. (arXiv:2112.06109v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06109">
<div class="article-summary-box-inner">
<span><p>Embedding-based methods are popular for Knowledge Base Question Answering
(KBQA), but few current models have numerical reasoning skills and thus
struggle to answer ordinal constrained questions. This paper proposes a new
embedding-based KBQA framework which particularly takes numerical reasoning
into account. We present NumericalTransformer on top of NSM, a state-of-the-art
embedding-based KBQA model, to create NT-NSM. To enable better training, we
propose two pre-training tasks with explicit numerical-oriented loss functions
on two generated training datasets and a template-based data augmentation
method for enriching ordinal constrained QA dataset. Extensive experiments on
KBQA benchmarks demonstrate that with the help of our training algorithm,
NT-NSM is empowered with numerical reasoning skills and substantially
outperforms the baselines in answering ordinal constrained questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Block-Skim: Efficient Question Answering for Transformer. (arXiv:2112.08560v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08560">
<div class="article-summary-box-inner">
<span><p>Transformer models have achieved promising results on natural language
processing (NLP) tasks including extractive question answering (QA). Common
Transformer encoders used in NLP tasks process the hidden states of all input
tokens in the context paragraph throughout all layers. However, different from
other tasks such as sequence classification, answering the raised question does
not necessarily need all the tokens in the context paragraph. Following this
motivation, we propose Block-skim, which learns to skim unnecessary context in
higher hidden layers to improve and accelerate the Transformer performance. The
key idea of Block-Skim is to identify the context that must be further
processed and those that could be safely discarded early on during inference.
Critically, we find that such information could be sufficiently derived from
the self-attention weights inside the Transformer model. We further prune the
hidden states corresponding to the unnecessary positions early in lower layers,
achieving significant inference-time speedup. To our surprise, we observe that
models pruned in this way outperform their full-size counterparts. Block-Skim
improves QA models' accuracy on different datasets and achieves 3 times speedup
on BERT-base model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guiding Neural Story Generation with Reader Models. (arXiv:2112.08596v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08596">
<div class="article-summary-box-inner">
<span><p>Automated storytelling has long captured the attention of researchers for the
ubiquity of narratives in everyday life. However, it is challenging to maintain
coherence and stay on-topic toward a specific ending when generating narratives
with neural language models. In this paper, we introduce Story generation with
Reader Models (StoRM), a framework in which a reader model is used to reason
about the story should progress. A reader model infers what a human reader
believes about the concepts, entities, and relations about the fictional story
world. We show how an explicit reader model represented as a knowledge graph
affords story coherence and provides controllability in the form of achieving a
given story world state goal. Experiments show that our model produces
significantly more coherent and on-topic stories, outperforming baselines in
dimensions including plot plausibility and staying on topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Call for Customized Conversation: Customized Conversation Grounding Persona and Knowledge. (arXiv:2112.08619v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08619">
<div class="article-summary-box-inner">
<span><p>Humans usually have conversations by making use of prior knowledge about a
topic and background information of the people whom they are talking to.
However, existing conversational agents and datasets do not consider such
comprehensive information, and thus they have a limitation in generating the
utterances where the knowledge and persona are fused properly. To address this
issue, we introduce a call For Customized conversation (FoCus) dataset where
the customized answers are built with the user's persona and Wikipedia
knowledge. To evaluate the abilities to make informative and customized
utterances of pre-trained language models, we utilize BART and GPT-2 as well as
transformer-based models. We assess their generation abilities with automatic
scores and conduct human evaluations for qualitative results. We examine
whether the model reflects adequate persona and knowledge with our proposed two
sub-tasks, persona grounding (PG) and knowledge grounding (KG). Moreover, we
show that the utterances of our data are constructed with the proper knowledge
and persona through grounding quality assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks. (arXiv:2112.08688v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08688">
<div class="article-summary-box-inner">
<span><p>Retrieval-augmented generation models have shown state-of-the-art performance
across many knowledge-intensive NLP tasks such as open question answering and
fact verification. These models are trained to generate the final output given
the retrieved passages, which can be irrelevant to the original query, leading
to learning spurious cues or answer memorization. This work introduces a method
to incorporate the evidentiality of passages -- whether a passage contains
correct evidence to support the output -- into training the generator. We
introduce a multi-task learning framework to jointly generate the final output
and predict the evidentiality of each passage, leveraging a new task-agnostic
method to obtain silver evidentiality labels for supervision. Our experiments
on five datasets across three knowledge-intensive tasks show that our new
evidentiality-guided generator significantly outperforms its direct counterpart
with the same-size model and advances the state of the art on FaVIQ-Ambig. We
attribute these improvements to both the auxiliary multi-task learning and
silver evidentiality mining techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Does Data Corruption Affect Natural Language Understanding Models? A Study on GLUE datasets. (arXiv:2201.04467v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04467">
<div class="article-summary-box-inner">
<span><p>A central question in natural language understanding (NLU) research is
whether high performance demonstrates the models' strong reasoning
capabilities. We present an extensive series of controlled experiments where
pre-trained language models are exposed to data that have undergone specific
corruption transformations. These involve removing instances of specific word
classes and often lead to non-sensical sentences. Our results show that
performance remains high on most GLUE tasks when the models are fine-tuned or
tested on corrupted data, suggesting that they leverage other cues for
prediction even in non-sensical contexts. Our proposed data transformations can
be used to assess the extent to which a specific dataset constitutes a proper
testbed for evaluating models' language understanding capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pretrained Language Models for Text Generation: A Survey. (arXiv:2201.05273v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05273">
<div class="article-summary-box-inner">
<span><p>Text Generation aims to produce plausible and readable text in a human
language from input data. The resurgence of deep learning has greatly advanced
this field, in particular, with the help of neural generation models based on
pre-trained language models (PLMs). Text generation based on PLMs is viewed as
a promising approach in both academia and industry. In this paper, we provide a
survey on the utilization of PLMs in text generation. We begin with introducing
three key aspects of applying PLMs to text generation: 1) how to encode the
input into representations preserving input semantics which can be fused into
PLMs; 2) how to design an effective PLM to serve as the generation model; and
3) how to effectively optimize PLMs given the reference text and to ensure that
the generated texts satisfy special text properties. Then, we show the major
challenges arisen in these aspects, as well as possible solutions for them. We
also include a summary of various useful resources and typical text generation
applications based on PLMs. Finally, we highlight the future research
directions which will further improve these PLMs for text generation. This
comprehensive survey is intended to help researchers interested in text
generation problems to learn the core concepts, the main techniques and the
latest developments in this area based on PLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks. (arXiv:2201.05729v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05729">
<div class="article-summary-box-inner">
<span><p>Contrastive language-image pretraining (CLIP) links vision and language
modalities into a unified embedding space, yielding the tremendous potential
for vision-language (VL) tasks. While early concurrent works have begun to
study this potential on a subset of tasks, important questions remain: 1) What
is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in
low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches
without impacting inference or pretraining complexity? In this work, we seek to
answer these questions through two key contributions. First, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data availability constraints and conditions of domain shift. Second, we
propose an approach, named CLIP Targeted Distillation (CLIP-TD), to
intelligently distill knowledge from CLIP into existing architectures using a
dynamically weighted objective applied to adaptively selected tokens per
instance. Experiments demonstrate that our proposed CLIP-TD leads to
exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to
71.3%) conditions of VCR, while simultaneously improving performance under
standard fully-supervised conditions (up to 2%), achieving state-of-art
performance on VCR compared to other single models that are pretrained with
image-text data only. On SNLI-VE, CLIP-TD produces significant gains in
low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On
VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in
fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works
utilizing CLIP for finetuning, as well as baseline naive distillation
approaches. Code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Learning for Aspect and Polarity Classification in Persian Reviews Using Multi-Task Deep Learning. (arXiv:2201.06313v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06313">
<div class="article-summary-box-inner">
<span><p>The purpose of this paper focuses on two sub-tasks related to aspect-based
sentiment analysis, namely, aspect category detection (ACD) and aspect category
polarity (ACP) in the Persian language. Its ability to identify all aspects
discussed in the text is what makes aspect-based sentiment analysis so
important and useful. While aspect-based sentiment analysis analyses all
aspects of the text, it will be most useful when it is able to identify their
polarity along with their identification. Most of the previous methods only
focus on solving one of these sub-tasks separately or use two separate models.
Thus, the process is pipelined, that is, the aspects are identified before the
polarities are identified. In practice, these methods lead to model errors that
are unsuitable for practical applications. In other words, ACD mistakes are
sent to ACP. In this paper, we propose a multi-task learning model based on
deep neural networks, which can concurrently detect aspect category and detect
aspect category polarity. We evaluated the proposed method using a Persian
language dataset in the movie domain on different deep learning-based models.
Final experiments show that the CNN model has better results than other models.
The reason is CNN's capability to extract local features. Since sentiment is
expressed using specific words and phrases, CNN has been able to be more
efficient in identifying these in this dataset.iments show that the CNN model
has better results than other models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JARVix at SemEval-2022 Task 2: It Takes One to Know One? Idiomaticity Detection using Zero and One Shot Learning. (arXiv:2202.02394v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02394">
<div class="article-summary-box-inner">
<span><p>Large Language Models have been successful in a wide variety of Natural
Language Processing tasks by capturing the compositionality of the text
representations. In spite of their great success, these vector representations
fail to capture meaning of idiomatic multi-word expressions (MWEs). In this
paper, we focus on the detection of idiomatic expressions by using binary
classification. We use a dataset consisting of the literal and idiomatic usage
of MWEs in English and Portuguese. Thereafter, we perform the classification in
two different settings: zero shot and one shot, to determine if a given
sentence contains an idiom or not. N shot classification for this task is
defined by N number of common idioms between the training and testing sets. In
this paper, we train multiple Large Language Models in both the settings and
achieve an F1 score (macro) of 0.73 for the zero shot setting and an F1 score
(macro) of 0.85 for the one shot setting. An implementation of our work can be
found at
https://github.com/ashwinpathak20/Idiomaticity_Detection_Using_Few_Shot_Learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Complementarity of Images and Text for the Expression of Emotions in Social Media. (arXiv:2202.07427v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07427">
<div class="article-summary-box-inner">
<span><p>Authors of posts in social media communicate their emotions and what causes
them with text and images. While there is work on emotion and stimulus
detection for each modality separately, it is yet unknown if the modalities
contain complementary emotion information in social media. We aim at filling
this research gap and contribute a novel, annotated corpus of English
multimodal Reddit posts. On this resource, we develop models to automatically
detect the relation between image and text, an emotion stimulus category and
the emotion class. We evaluate if these tasks require both modalities and find
for the image-text relations, that text alone is sufficient for most categories
(complementary, illustrative, opposing): the information in the text allows to
predict if an image is required for emotion understanding. The emotions of
anger and sadness are best predicted with a multimodal model, while text alone
is sufficient for disgust, joy, and surprise. Stimuli depicted by objects,
animals, food, or a person are best predicted by image-only models, while
multimodal models are most effective on art, events, memes, places, or
screenshots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Items from Psychometric Tests as Training Data for Personality Profiling Models of Twitter Users. (arXiv:2202.10415v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10415">
<div class="article-summary-box-inner">
<span><p>Machine-learned models for author profiling in social media often rely on
data acquired via self-reporting-based psychometric tests (questionnaires)
filled out by social media users. This is an expensive but accurate data
collection strategy. Another, less costly alternative, which leads to
potentially more noisy and biased data, is to rely on labels inferred from
publicly available information in the profiles of the users, for instance
self-reported diagnoses or test results. In this paper, we explore a third
strategy, namely to directly use a corpus of items from validated psychometric
tests as training data. Items from psychometric tests often consist of
sentences from an I-perspective (e.g., "I make friends easily."). Such corpora
of test items constitute 'small data', but their availability for many concepts
is a rich resource. We investigate this approach for personality profiling, and
evaluate BERT classifiers fine-tuned on such psychometric test items for the
big five personality traits (openness, conscientiousness, extraversion,
agreeableness, neuroticism) and analyze various augmentation strategies
regarding their potential to address the challenges coming with such a small
corpus. Our evaluation on a publicly available Twitter corpus shows a
comparable performance to in-domain training for 4/5 personality traits with
T5-based data augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"splink" is happy and "phrouth" is scary: Emotion Intensity Analysis for Nonsense Words. (arXiv:2202.12132v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12132">
<div class="article-summary-box-inner">
<span><p>People associate affective meanings to words - "death" is scary and sad while
"party" is connotated with surprise and joy. This raises the question if the
association is purely a product of the learned affective imports inherent to
semantic meanings, or is also an effect of other features of words, e.g.,
morphological and phonological patterns. We approach this question with an
annotation-based analysis leveraging nonsense words. Specifically, we conduct a
best-worst scaling crowdsourcing study in which participants assign intensity
scores for joy, sadness, anger, disgust, fear, and surprise to 272 non-sense
words and, for comparison of the results to previous work, to 68 real words.
Based on this resource, we develop character-level and phonology-based
intensity regressors. We evaluate them on both nonsense words and real words
(making use of the NRC emotion intensity lexicon of 7493 words), across six
emotion categories. The analysis of our data reveals that some phonetic
patterns show clear differences between emotion intensities. For instance, s as
a first phoneme contributes to joy, sh to surprise, p as last phoneme more to
disgust than to anger and fear. In the modelling experiments, a regressor
trained on real words from the NRC emotion intensity lexicon shows a higher
performance (r = 0.17) than regressors that aim at learning the emotion
connotation purely from nonsense words. We conclude that humans do associate
affective meaning to words based on surface patterns, but also based on
similarities to existing words ("juy" to "joy", or "flike" to "like").
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Equal Opportunity Fairness through Adversarial Learning. (arXiv:2203.06317v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06317">
<div class="article-summary-box-inner">
<span><p>Adversarial training is a common approach for bias mitigation in natural
language processing. Although most work on debiasing is motivated by equal
opportunity, it is not explicitly captured in standard adversarial training. In
this paper, we propose an augmented discriminator for adversarial training,
which takes the target class as input to create richer features and more
explicitly model equal opportunity. Experimental results over two datasets show
that our method substantially improves over standard adversarial debiasing
methods, in terms of the performance--fairness trade-off.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Self-Augmentation for Named Entity Recognition with Meta Reweighting. (arXiv:2204.11406v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11406">
<div class="article-summary-box-inner">
<span><p>Self-augmentation has received increasing research interest recently to
improve named entity recognition (NER) performance in low-resource scenarios.
Token substitution and mixup are two feasible heterogeneous self-augmentation
techniques for NER that can achieve effective performance with certain
specialized efforts. Noticeably, self-augmentation may introduce potentially
noisy augmented data. Prior research has mainly resorted to heuristic
rule-based constraints to reduce the noise for specific self-augmentation
methods individually. In this paper, we revisit these two typical
self-augmentation methods for NER, and propose a unified meta-reweighting
strategy for them to achieve a natural integration. Our method is easily
extensible, imposing little effort on a specific self-augmentation method.
Experiments on different Chinese and English NER benchmarks show that our token
substitution and mixup method, as well as their integration, can achieve
effective performance improvement. Based on the meta-reweighting mechanism, we
can enhance the advantages of the self-augmentation techniques without much
extra effort.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Testing the Ability of Language Models to Interpret Figurative Language. (arXiv:2204.12632v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12632">
<div class="article-summary-box-inner">
<span><p>Figurative and metaphorical language are commonplace in discourse, and
figurative expressions play an important role in communication and cognition.
However, figurative language has been a relatively under-studied area in NLP,
and it remains an open question to what extent modern language models can
interpret nonliteral phrases. To address this question, we introduce Fig-QA, a
Winograd-style nonliteral language understanding task consisting of correctly
interpreting paired figurative phrases with divergent meanings. We evaluate the
performance of several state-of-the-art language models on this task, and find
that although language models achieve performance significantly over chance,
they still fall short of human performance, particularly in zero- or few-shot
settings. This suggests that further work is needed to improve the nonliteral
reasoning capabilities of language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TimeBERT: Enhancing Pre-Trained Language Representations with Temporal Information. (arXiv:2204.13032v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13032">
<div class="article-summary-box-inner">
<span><p>Time is an important aspect of text documents, which has been widely
exploited in natural language processing and has strong influence, for example,
in temporal information retrieval, where the temporal information of queries or
documents needs to be identified for relevance estimation. Event-related tasks
like event ordering, which aims to order events by their occurrence time, needs
to determine the temporal information of events, too. In this work, we
investigate methods for incorporating temporal information during pre-training
to further improve the performance on time-related tasks. Compared with BERT
which utilizes synchronic document collections (BooksCorpus and English
Wikipedia) as the training corpora, we use long-span temporal news collection
for building word representations. We introduce TimeBERT, a novel language
representation model trained on a temporal collection of news articles via two
new pre-training tasks, which harness two distinct temporal signals to
construct time-aware language representation. The experimental results show
that TimeBERT consistently outperforms BERT and other existing pre-trained
models, with substantial gains on different downstream NLP tasks or
applications for which time is of importance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COSPLAY: Concept Set Guided Personalized Dialogue Generation Across Both Party Personas. (arXiv:2205.00872v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00872">
<div class="article-summary-box-inner">
<span><p>Maintaining a consistent persona is essential for building a human-like
conversational model. However, the lack of attention to the partner makes the
model more egocentric: they tend to show their persona by all means such as
twisting the topic stiffly, pulling the conversation to their own interests
regardless, and rambling their persona with little curiosity to the partner. In
this work, we propose COSPLAY(COncept Set guided PersonaLized dialogue
generation Across both partY personas) that considers both parties as a "team":
expressing self-persona while keeping curiosity toward the partner, leading
responses around mutual personas, and finding the common ground. Specifically,
we first represent self-persona, partner persona and mutual dialogue all in the
concept sets. Then, we propose the Concept Set framework with a suite of
knowledge-enhanced operations to process them such as set algebras, set
expansion, and set distance. Based on these operations as medium, we train the
model by utilizing 1) concepts of both party personas, 2) concept relationship
between them, and 3) their relationship to the future dialogue. Extensive
experiments on a large public dataset, Persona-Chat, demonstrate that our model
outperforms state-of-the-art baselines for generating less egocentric, more
human-like, and higher quality responses in both automatic and human
evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemAttack: Natural Textual Attacks via Different Semantic Spaces. (arXiv:2205.01287v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01287">
<div class="article-summary-box-inner">
<span><p>Recent studies show that pre-trained language models (LMs) are vulnerable to
textual adversarial attacks. However, existing attack methods either suffer
from low attack success rates or fail to search efficiently in the
exponentially large perturbation space. We propose an efficient and effective
framework SemAttack to generate natural adversarial text by constructing
different semantic perturbation functions. In particular, SemAttack optimizes
the generated perturbations constrained on generic semantic spaces, including
typo space, knowledge space (e.g., WordNet), contextualized semantic space
(e.g., the embedding space of BERT clusterings), or the combination of these
spaces. Thus, the generated adversarial texts are more semantically close to
the original inputs. Extensive experiments reveal that state-of-the-art (SOTA)
large-scale LMs (e.g., DeBERTa-v2) and defense strategies (e.g., FreeLB) are
still vulnerable to SemAttack. We further demonstrate that SemAttack is general
and able to generate natural adversarial texts for different languages (e.g.,
English and Chinese) with high attack success rates. Human evaluations also
confirm that our generated adversarial texts are natural and barely affect
human performance. Our code is publicly available at
https://github.com/AI-secure/SemAttack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Transfer Prompts for Text Generation. (arXiv:2205.01543v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01543">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (PLMs) have made remarkable progress in text
generation tasks via fine-tuning. While, it is challenging to fine-tune PLMs in
a data-scarce situation. Therefore, it is non-trivial to develop a general and
lightweight model that can adapt to various text generation tasks based on
PLMs. To fulfill this purpose, the recent prompt-based learning offers a
potential solution. In this paper, we improve this technique and propose a
novel prompt-based method (PTG) for text generation in a transferable setting.
First, PTG learns a set of source prompts for various source generation tasks
and then transfers these prompts as target prompts to perform target generation
tasks. To consider both task- and instance-level information, we design an
adaptive attention mechanism to derive the target prompts. For each data
instance, PTG learns a specific target prompt by attending to highly relevant
source prompts. In extensive experiments, PTG yields competitive or better
results than fine-tuning methods. We release our source prompts as an open
resource, where users can add or reuse them to improve new text generation
tasks for future research. Code and data can be available at
https://github.com/RUCAIBox/Transfer-Prompts-for-Text-Generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CogIntAc: Modeling the Relationships between Intention, Emotion and Action in Interactive Process from Cognitive Perspective. (arXiv:2205.03540v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03540">
<div class="article-summary-box-inner">
<span><p>Intention, emotion and action are important psychological factors in human
activities, which play an important role in the interaction between
individuals. How to model the interaction process between individuals by
analyzing the relationship of their intentions, emotions, and actions at the
cognitive level is challenging. In this paper, we propose a novel cognitive
framework of individual interaction. The core of the framework is that
individuals achieve interaction through external action driven by their inner
intention. Based on this idea, the interactions between individuals can be
constructed by establishing relationships between the intention, emotion and
action. Furthermore, we conduct analysis on the interaction between individuals
and give a reasonable explanation for the predicting results. To verify the
effectiveness of the framework, we reconstruct a dataset and propose three
tasks as well as the corresponding baseline models, including action abduction,
emotion prediction and action generation. The novel framework shows an
interesting perspective on mimicking the mental state of human beings in
cognitive science.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building Machine Translation Systems for the Next Thousand Languages. (arXiv:2205.03983v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03983">
<div class="article-summary-box-inner">
<span><p>In this paper we share findings from our effort to build practical machine
translation (MT) systems capable of translating across over one thousand
languages. We describe results in three research domains: (i) Building clean,
web-mined datasets for 1500+ languages by leveraging semi-supervised
pre-training for language identification and developing data-driven filtering
techniques; (ii) Developing practical MT models for under-served languages by
leveraging massively multilingual models trained with supervised parallel data
for over 100 high-resource languages and monolingual datasets for an additional
1000+ languages; and (iii) Studying the limitations of evaluation metrics for
these languages and conducting qualitative analysis of the outputs from our MT
models, highlighting several frequent error modes of these types of models. We
hope that our work provides useful insights to practitioners working towards
building MT systems for currently understudied languages, and highlights
research directions that can complement the weaknesses of massively
multilingual models in data-sparse settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Climate Awareness in NLP Research. (arXiv:2205.05071v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05071">
<div class="article-summary-box-inner">
<span><p>The climate impact of AI, and NLP research in particular, has become a
serious issue given the enormous amount of energy that is increasingly being
used for training and running computational models. Consequently, increasing
focus is placed on efficient NLP. However, this important initiative lacks
simple guidelines that would allow for systematic climate reporting of NLP
research. We argue that this deficiency is one of the reasons why very few
publications in NLP report key figures that would allow a more thorough
examination of environmental impact. As a remedy, we propose a climate
performance model card with the primary purpose of being practically usable
with only limited information about experiments and the underlying computer
hardware. We describe why this step is essential to increase awareness about
the environmental impact of NLP research and, thereby, paving the way for more
thorough discussions.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A Framework for Event-based Computer Vision on a Mobile Device. (arXiv:2205.06836v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06836">
<div class="article-summary-box-inner">
<span><p>We present the first publicly available Android framework to stream data from
an event camera directly to a mobile phone. Today's mobile devices handle a
wider range of workloads than ever before and they incorporate a growing gamut
of sensors that make devices smarter, more user friendly and secure.
Conventional cameras in particular play a central role in such tasks, but they
cannot record continuously, as the amount of redundant information recorded is
costly to process. Bio-inspired event cameras on the other hand only record
changes in a visual scene and have shown promising low-power applications that
specifically suit mobile tasks such as face detection, gesture recognition or
gaze tracking. Our prototype device is the first step towards embedding such an
event camera into a battery-powered handheld device. The mobile framework
allows us to stream events in real-time and opens up the possibilities for
always-on and on-demand sensing on mobile phones. To liaise the asynchronous
event camera output with synchronous von Neumann hardware, we look at how
buffering events and processing them in batches can benefit mobile
applications. We evaluate our framework in terms of latency and throughput and
show examples of computer vision tasks that involve both event-by-event and
pre-trained neural network methods for gesture recognition, aperture robust
optical flow and grey-level image reconstruction from events. The code is
available at https://github.com/neuromorphic-paris/frog
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Images to Probabilistic Anatomical Shapes: A Deep Variational Bottleneck Approach. (arXiv:2205.06862v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06862">
<div class="article-summary-box-inner">
<span><p>Statistical shape modeling (SSM) directly from 3D medical images is an
underutilized tool for detecting pathology, diagnosing disease, and conducting
population-level morphology analysis. Deep learning frameworks have increased
the feasibility of adopting SSM in medical practice by reducing the
expert-driven manual and computational overhead in traditional SSM workflows.
However, translating such frameworks to clinical practice requires calibrated
uncertainty measures as neural networks can produce over-confident predictions
that cannot be trusted in sensitive clinical decision-making. Existing
techniques for predicting shape with aleatoric (data-dependent) uncertainty
utilize a principal component analysis (PCA) based shape representation
computed in isolation from the model training. This constraint restricts the
learning task to solely estimating pre-defined shape descriptors from 3D images
and imposes a linear relationship between this shape representation and the
output (i.e., shape) space. In this paper, we propose a principled framework
based on the variational information bottleneck theory to relax these
assumptions while predicting probabilistic shapes of anatomy directly from
images without supervised encoding of shape descriptors. Here, the latent
representation is learned in the context of the learning task, resulting in a
more scalable, flexible model that better captures data non-linearity.
Additionally, this model is self-regularized and generalizes better given
limited training data. Our experiments demonstrate that the proposed method
provides improved accuracy and better calibrated aleatoric uncertainty
estimates than state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Augmented Face Images to Improve Facial Recognition Tasks. (arXiv:2205.06873v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06873">
<div class="article-summary-box-inner">
<span><p>We present a framework that uses GAN-augmented images to complement certain
specific attributes, usually underrepresented, for machine learning model
training. This allows us to improve inference quality over those attributes for
the facial recognition tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AVCAffe: A Large Scale Audio-Visual Dataset of Cognitive Load and Affect for Remote Work. (arXiv:2205.06887v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06887">
<div class="article-summary-box-inner">
<span><p>We introduce AVCAffe, the first Audio-Visual dataset consisting of Cognitive
load and Affect attributes. We record AVCAffe by simulating remote work
scenarios over a video-conferencing platform, where subjects collaborate to
complete a number of cognitively engaging tasks. AVCAffe is the largest
originally collected (not collected from the Internet) affective dataset in
English language. We recruit 106 participants from 18 different countries of
origin, spanning an age range of 18 to 57 years old, with a balanced
male-female ratio. AVCAffe comprises a total of 108 hours of video, equivalent
to more than 58,000 clips along with task-based self-reported ground truth
labels for arousal, valence, and cognitive load attributes such as mental
demand, temporal demand, effort, and a few others. We believe AVCAffe would be
a challenging benchmark for the deep learning research community given the
inherent difficulty of classifying affect and cognitive load in particular.
Moreover, our dataset fills an existing timely gap by facilitating the creation
of learning systems for better self-management of remote work meetings, and
further study of hypotheses regarding the impact of remote work on cognitive
load and affective states.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Representation Learning for 3D MRI Super Resolution with Degradation Adaptation. (arXiv:2205.06891v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06891">
<div class="article-summary-box-inner">
<span><p>High-resolution (HR) MRI is critical in assisting the doctor's diagnosis and
image-guided treatment, but is hard to obtain in a clinical setting due to long
acquisition time. Therefore, the research community investigated deep
learning-based super-resolution (SR) technology to reconstruct HR MRI images
with shortened acquisition time. However, training such neural networks usually
requires paired HR and low-resolution (LR) in-vivo images, which are difficult
to acquire due to patient movement during and between the image acquisition.
Rigid movements of hard tissues can be corrected with image-registration,
whereas the alignment of deformed soft tissues is challenging, making it
impractical to train the neural network with such authentic HR and LR image
pairs. Therefore, most of the previous studies proposed SR reconstruction by
employing authentic HR images and synthetic LR images downsampled from the HR
images, yet the difference in degradation representations between synthetic and
authentic LR images suppresses the performance of SR reconstruction from
authentic LR images. To mitigate the aforementioned problems, we propose a
novel Unsupervised DEgradation Adaptation Network (UDEAN). Our model consists
of two components: the degradation learning network and the SR reconstruction
network. The degradation learning network downsamples the HR images by
addressing the degradation representation of the misaligned or unpaired LR
images, and the SR reconstruction network learns the mapping from the
downsampled HR images to their original HR images. As a result, the SR
reconstruction network can generate SR images from the LR images and achieve
comparable quality to the HR images. Experimental results show that our method
outperforms the state-of-the-art models and can potentially be applied in
real-world clinical settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ImageSig: A signature transform for ultra-lightweight image recognition. (arXiv:2205.06929v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06929">
<div class="article-summary-box-inner">
<span><p>This paper introduces a new lightweight method for image recognition.
ImageSig is based on computing signatures and does not require a convolutional
structure or an attention-based encoder. It is striking to the authors that it
achieves: a) an accuracy for 64 X 64 RGB images that exceeds many of the
state-of-the-art methods and simultaneously b) requires orders of magnitude
less FLOPS, power and memory footprint. The pretrained model can be as small as
44.2 KB in size. ImageSig shows unprecedented performance on hardware such as
Raspberry Pi and Jetson-nano. ImageSig treats images as streams with multiple
channels. These streams are parameterized by spatial directions. We contribute
to the functionality of signature and rough path theory to stream-like data and
vision tasks on static images beyond temporal streams. With very few parameters
and small size models, the key advantage is that one could have many of these
"detectors" assembled on the same chip; moreover, the feature acquisition can
be performed once and shared between different models of different tasks -
further accelerating the process. This contributes to energy efficiency and the
advancements of embedded AI at the edge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Saliency-Guided Street View Image Inpainting Framework for Efficient Last-Meters Wayfinding. (arXiv:2205.06934v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06934">
<div class="article-summary-box-inner">
<span><p>Global Positioning Systems (GPS) have played a crucial role in various
navigation applications. Nevertheless, localizing the perfect destination
within the last few meters remains an important but unresolved problem. Limited
by the GPS positioning accuracy, navigation systems always show users a
vicinity of a destination, but not its exact location. Street view images (SVI)
in maps as an immersive media technology have served as an aid to provide the
physical environment for human last-meters wayfinding. However, due to the
large diversity of geographic context and acquisition conditions, the captured
SVI always contains various distracting objects (e.g., pedestrians and
vehicles), which will distract human visual attention from efficiently finding
the destination in the last few meters. To address this problem, we highlight
the importance of reducing visual distraction in image-based wayfinding by
proposing a saliency-guided image inpainting framework. It aims at redirecting
human visual attention from distracting objects to destination-related objects
for more efficient and accurate wayfinding in the last meters. Specifically, a
context-aware distracting object detection method driven by deep salient object
detection has been designed to extract distracting objects from three semantic
levels in SVI. Then we employ a large-mask inpainting method with fast Fourier
convolutions to remove the detected distracting objects. Experimental results
with both qualitative and quantitative analysis show that our saliency-guided
inpainting method can not only achieve great perceptual quality in street view
images but also redirect the human's visual attention to focus more on static
location-related objects than distracting ones. The human-based evaluation also
justified the effectiveness of our method in improving the efficiency of
locating the target destination.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense residual Transformer for image denoising. (arXiv:2205.06944v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06944">
<div class="article-summary-box-inner">
<span><p>Image denoising is an important low-level computer vision task, which aims to
reconstruct a noise-free and high-quality image from a noisy image. With the
development of deep learning, convolutional neural network (CNN) has been
gradually applied and achieved great success in image denoising, image
compression, image enhancement, etc. Recently, Transformer has been a hot
technique, which is widely used to tackle computer vision tasks. However, few
Transformer-based methods have been proposed for low-level vision tasks. In
this paper, we proposed an image denoising network structure based on
Transformer, which is named DenSformer. DenSformer consists of three modules,
including a preprocessing module, a local-global feature extraction module, and
a reconstruction module. Specifically, the local-global feature extraction
module consists of several Sformer groups, each of which has several
ETransformer layers and a convolution layer, together with a residual
connection. These Sformer groups are densely skip-connected to fuse the feature
of different layers, and they jointly capture the local and global information
from the given noisy images. We conduct our model on comprehensive experiments.
Experimental results prove that our DenSformer achieves improvement compared to
some state-of-the-art methods, both for the synthetic noise data and real noise
data, in the objective and subjective evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BronchusNet: Region and Structure Prior Embedded Representation Learning for Bronchus Segmentation and Classification. (arXiv:2205.06947v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06947">
<div class="article-summary-box-inner">
<span><p>CT-based bronchial tree analysis plays an important role in the
computer-aided diagnosis for respiratory diseases, as it could provide
structured information for clinicians. The basis of airway analysis is
bronchial tree reconstruction, which consists of bronchus segmentation and
classification. However, there remains a challenge for accurate bronchial
analysis due to the individual variations and the severe class imbalance. In
this paper, we propose a region and structure prior embedded framework named
BronchusNet to achieve accurate segmentation and classification of bronchial
regions in CT images. For bronchus segmentation, we propose an adaptive hard
region-aware UNet that incorporates multi-level prior guidance of hard
pixel-wise samples in the general Unet segmentation network to achieve better
hierarchical feature learning. For the classification of bronchial branches, we
propose a hybrid point-voxel graph learning module to fully exploit bronchial
structure priors and to support simultaneous feature interactions across
different branches. To facilitate the study of bronchial analysis, we
contribute~\textbf{BRSC}: an open-access benchmark of \textbf{BR}onchus imaging
analysis with high-quality pixel-wise \textbf{S}egmentation masks and the
\textbf{C}lass of bronchial segments. Experimental results on BRSC show that
our proposed method not only achieves the state-of-the-art performance for
binary segmentation of bronchial region but also exceeds the best existing
method on bronchial branches classification by 6.9\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RiCS: A 2D Self-Occlusion Map for Harmonizing Volumetric Objects. (arXiv:2205.06975v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06975">
<div class="article-summary-box-inner">
<span><p>There have been remarkable successes in computer vision with deep learning.
While such breakthroughs show robust performance, there have still been many
challenges in learning in-depth knowledge, like occlusion or predicting
physical interactions. Although some recent works show the potential of 3D data
in serving such context, it is unclear how we efficiently provide 3D input to
the 2D models due to the misalignment in dimensionality between 2D and 3D. To
leverage the successes of 2D models in predicting self-occlusions, we design
Ray-marching in Camera Space (RiCS), a new method to represent the
self-occlusions of foreground objects in 3D into a 2D self-occlusion map. We
test the effectiveness of our representation on the human image harmonization
task by predicting shading that is coherent with a given background image. Our
experiments demonstrate that our representation map not only allows us to
enhance the image quality but also to model temporally coherent complex shadow
effects compared with the simulation-to-real and harmonization methods, both
quantitatively and qualitatively. We further show that we can significantly
improve the performance of human parts segmentation networks trained on
existing synthetic datasets by enhancing the harmonization quality with our
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Gesture Recognition for the Assistance of Visually Impaired People using Multi-Head Neural Networks. (arXiv:2205.06980v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06980">
<div class="article-summary-box-inner">
<span><p>This paper proposes an interactive system for mobile devices controlled by
hand gestures aimed at helping people with visual impairments. This system
allows the user to interact with the device by making simple static and dynamic
hand gestures. Each gesture triggers a different action in the system, such as
object recognition, scene description or image scaling (e.g., pointing a finger
at an object will show a description of it). The system is based on a
multi-head neural network architecture, which initially detects and classifies
the gestures, and subsequently, depending on the gesture detected, performs a
second stage that carries out the corresponding action. This multi-head
architecture optimizes the resources required to perform different tasks
simultaneously, and takes advantage of the information obtained from an initial
backbone to perform different processes in a second stage. To train and
evaluate the system, a dataset with about 40k images was manually compiled and
labeled including different types of hand gestures, backgrounds (indoors and
outdoors), lighting conditions, etc. This dataset contains synthetic gestures
(whose objective is to pre-train the system in order to improve the results)
and real images captured using different mobile phones. The results obtained
and the comparison made with the state of the art show competitive results as
regards the different actions performed by the system, such as the accuracy of
classification and localization of gestures, or the generation of descriptions
for objects and scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Voxel-wise Adversarial Semi-supervised Learning for Medical Image Segmentation. (arXiv:2205.06987v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06987">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning for medical image segmentation is an important area
of research for alleviating the huge cost associated with the construction of
reliable large-scale annotations in the medical domain. Recent semi-supervised
approaches have demonstrated promising results by employing consistency
regularization, pseudo-labeling techniques, and adversarial learning. These
methods primarily attempt to learn the distribution of labeled and unlabeled
data by enforcing consistency in the predictions or embedding context. However,
previous approaches have focused only on local discrepancy minimization or
context relations across single classes. In this paper, we introduce a novel
adversarial learning-based semi-supervised segmentation method that effectively
embeds both local and global features from multiple hidden layers and learns
context relations between multiple classes. Our voxel-wise adversarial learning
method utilizes a voxel-wise feature discriminator, which considers multilayer
voxel-wise features (involving both local and global features) as an input by
embedding class-specific voxel-wise feature distribution. Furthermore, we
improve our previous representation learning method by overcoming information
loss and learning stability problems, which enables rich representations of
labeled data. Our method outperforms current best-performing state-of-the-art
semi-supervised learning approaches on the image segmentation of the left
atrium (single class) and multiorgan datasets (multiclass). Moreover, our
visual interpretation of the feature space demonstrates that our proposed
method enables a well-distributed and separated feature space from both labeled
and unlabeled data, which improves the overall prediction results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic-PHNet: Towards Real-Time and High-Precision LiDAR Panoptic Segmentation via Clustering Pseudo Heatmap. (arXiv:2205.07002v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07002">
<div class="article-summary-box-inner">
<span><p>As a rising task, panoptic segmentation is faced with challenges in both
semantic segmentation and instance segmentation. However, in terms of speed and
accuracy, existing LiDAR methods in the field are still limited. In this paper,
we propose a fast and high-performance LiDAR-based framework, referred to as
Panoptic-PHNet, with three attractive aspects: 1) We introduce a clustering
pseudo heatmap as a new paradigm, which, followed by a center grouping module,
yields instance centers for efficient clustering without object-level learning
tasks. 2) A knn-transformer module is proposed to model the interaction among
foreground points for accurate offset regression. 3) For backbone design, we
fuse the fine-grained voxel features and the 2D Bird's Eye View (BEV) features
with different receptive fields to utilize both detailed and global
information. Extensive experiments on both SemanticKITTI dataset and nuScenes
dataset show that our Panoptic-PHNet surpasses state-of-the-art methods by
remarkable margins with a real-time speed. We achieve the 1st place on the
public leaderboard of SemanticKITTI and leading performance on the recently
released leaderboard of nuScenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SaiNet: Stereo aware inpainting behind objects with generative networks. (arXiv:2205.07014v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07014">
<div class="article-summary-box-inner">
<span><p>In this work, we present an end-to-end network for stereo-consistent image
inpainting with the objective of inpainting large missing regions behind
objects. The proposed model consists of an edge-guided UNet-like network using
Partial Convolutions. We enforce multi-view stereo consistency by introducing a
disparity loss. More importantly, we develop a training scheme where the model
is learned from realistic stereo masks representing object occlusions, instead
of the more common random masks. The technique is trained in a supervised way.
Our evaluation shows competitive results compared to previous state-of-the-art
techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Importance Weighted Structure Learning for Scene Graph Generation. (arXiv:2205.07017v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07017">
<div class="article-summary-box-inner">
<span><p>Scene graph generation is a structured prediction task aiming to explicitly
model objects and their relationships via constructing a visually-grounded
scene graph for an input image. Currently, the message passing neural network
based mean field variational Bayesian methodology is the ubiquitous solution
for such a task, in which the variational inference objective is often assumed
to be the classical evidence lower bound. However, the variational
approximation inferred from such loose objective generally underestimates the
underlying posterior, which often leads to inferior generation performance. In
this paper, we propose a novel importance weighted structure learning method
aiming to approximate the underlying log-partition function with a tighter
importance weighted lower bound, which is computed from multiple samples drawn
from a reparameterizable Gumbel-Softmax sampler. A generic entropic mirror
descent algorithm is applied to solve the resulting constrained variational
inference task. The proposed method achieves the state-of-the-art performance
on various popular scene graph generation benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Generalization Ability of Super-Resolution Networks. (arXiv:2205.07019v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07019">
<div class="article-summary-box-inner">
<span><p>Performance and generalization ability are two important aspects to evaluate
deep learning models. However, research on the generalization ability of
Super-Resolution (SR) networks is currently absent. We make the first attempt
to propose a Generalization Assessment Index for SR networks, namely SRGA. SRGA
exploits the statistical characteristics of internal features of deep networks,
not output images to measure the generalization ability. Specially, it is a
non-parametric and non-learning metric. To better validate our method, we
collect a patch-based image evaluation set (PIES) that includes both synthetic
and real-world images, covering a wide range of degradations. With SRGA and
PIES dataset, we benchmark existing SR models on the generalization ability.
This work could lay the foundation for future research on model generalization
in low-level vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Assisted Active Learning for Skin Lesion Segmentation. (arXiv:2205.07021v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07021">
<div class="article-summary-box-inner">
<span><p>Label scarcity has been a long-standing issue for biomedical image
segmentation, due to high annotation costs and professional requirements.
Recently, active learning (AL) strategies strive to reduce annotation costs by
querying a small portion of data for annotation, receiving much traction in the
field of medical imaging. However, most of the existing AL methods have to
initialize models with some randomly selected samples followed by active
selection based on various criteria, such as uncertainty and diversity. Such
random-start initialization methods inevitably introduce under-value redundant
samples and unnecessary annotation costs. For the purpose of addressing the
issue, we propose a novel self-supervised assisted active learning framework in
the cold-start setting, in which the segmentation model is first warmed up with
self-supervised learning (SSL), and then SSL features are used for sample
selection via latent feature clustering without accessing labels. We assess our
proposed methodology on skin lesions segmentation task. Extensive experiments
demonstrate that our approach is capable of achieving promising performance
with substantial improvements over existing baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object-Aware Self-supervised Multi-Label Learning. (arXiv:2205.07028v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07028">
<div class="article-summary-box-inner">
<span><p>Multi-label Learning on Image data has been widely exploited with deep
learning models. However, supervised training on deep CNN models often cannot
discover sufficient discriminative features for classification. As a result,
numerous self-supervision methods are proposed to learn more robust image
representations. However, most self-supervised approaches focus on
single-instance single-label data and fall short on more complex images with
multiple objects. Therefore, we propose an Object-Aware Self-Supervision (OASS)
method to obtain more fine-grained representations for multi-label learning,
dynamically generating auxiliary tasks based on object locations. Secondly, the
robust representation learned by OASS can be leveraged to efficiently generate
Class-Specific Instances (CSI) in a proposal-free fashion to better guide
multi-label supervision signal transfer to instances. Extensive experiments on
the VOC2012 dataset for multi-label classification demonstrate the
effectiveness of the proposed method against the state-of-the-art counterparts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Realistic Defocus Blur for Multiplane Computer-Generated Holography. (arXiv:2205.07030v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07030">
<div class="article-summary-box-inner">
<span><p>This paper introduces a new multiplane CGH computation method to reconstruct
artefact-free high-quality holograms with natural-looking defocus blur. Our
method introduces a new targeting scheme and a new loss function. While the
targeting scheme accounts for defocused parts of the scene at each depth plane,
the new loss function analyzes focused and defocused parts separately in
reconstructed images. Our method support phase-only CGH calculations using
various iterative (e.g., Gerchberg-Saxton, Gradient Descent) and non-iterative
(e.g., Double Phase) CGH techniques. We achieve our best image quality using a
modified gradient descent-based optimization recipe where we introduce a
constraint inspired by the double phase method. We validate our method
experimentally using our proof-of-concept holographic display, comparing
various algorithms, including multi-depth scenes with sparse and dense
contents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer Scale Gate for Semantic Segmentation. (arXiv:2205.07056v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07056">
<div class="article-summary-box-inner">
<span><p>Effectively encoding multi-scale contextual information is crucial for
accurate semantic segmentation. Existing transformer-based segmentation models
combine features across scales without any selection, where features on
sub-optimal scales may degrade segmentation outcomes. Leveraging from the
inherent properties of Vision Transformers, we propose a simple yet effective
module, Transformer Scale Gate (TSG), to optimally combine multi-scale
features.TSG exploits cues in self and cross attentions in Vision Transformers
for the scale selection. TSG is a highly flexible plug-and-play module, and can
easily be incorporated with any encoder-decoder-based hierarchical vision
Transformer architecture. Extensive experiments on the Pascal Context and
ADE20K datasets demonstrate that our feature selection strategy achieves
consistent gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis. (arXiv:2205.07058v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07058">
<div class="article-summary-box-inner">
<span><p>We present a large-scale synthetic dataset for novel view synthesis
consisting of ~300k images rendered from nearly 2000 complex scenes using
high-quality ray tracing at high resolution (1600 x 1600 pixels). The dataset
is orders of magnitude larger than existing synthetic datasets for novel view
synthesis, thus providing a large unified benchmark for both training and
evaluation. Using 4 distinct sources of high-quality 3D meshes, the scenes of
our dataset exhibit challenging variations in camera views, lighting, shape,
materials, and textures. Because our dataset is too large for existing methods
to process, we propose Sparse Voxel Light Field (SVLF), an efficient
voxel-based light field approach for novel view synthesis that achieves
comparable performance to NeRF on synthetic data, while being an order of
magnitude faster to train and two orders of magnitude faster to render. SVLF
achieves this speed by relying on a sparse voxel octree, careful voxel sampling
(requiring only a handful of queries per ray), and reduced network structure;
as well as ground truth depth maps at training time. Our dataset is generated
by NViSII, a Python-based ray tracing renderer, which is designed to be simple
for non-experts to use and share, flexible and powerful through its use of
scripting, and able to create high-quality and physically-based rendered
images. Experiments with a subset of our dataset allow us to compare standard
methods like NeRF and mip-NeRF for single-scene modeling, and pixelNeRF for
category-level modeling, pointing toward the need for future improvements in
this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unifying Multi-sampling-ratio CS-MRI Framework With Two-grid-cycle Correction and Geometric Prior Distillation. (arXiv:2205.07062v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07062">
<div class="article-summary-box-inner">
<span><p>CS is an efficient method to accelerate the acquisition of MR images from
under-sampled k-space data. Although existing deep learning CS-MRI methods have
achieved considerably impressive performance, explainability and
generalizability continue to be challenging for such methods since most of them
are not flexible enough to handle multi-sampling-ratio reconstruction
assignments, often the transition from mathematical analysis to network design
not always natural enough. In this work, to tackle explainability and
generalizability, we propose a unifying deep unfolding multi-sampling-ratio
CS-MRI framework, by merging advantages of model-based and deep learning-based
methods. The combined approach offers more generalizability than previous works
whereas deep learning gains explainability through a geometric prior module.
Inspired by multigrid algorithm, we first embed the CS-MRI-based optimization
algorithm into correction-distillation scheme that consists of three
ingredients: pre-relaxation module, correction module and geometric prior
distillation module. Furthermore, we employ a condition module to learn
adaptively step-length and noise level from compressive sampling ratio in every
stage, which enables the proposed framework to jointly train multi-ratio tasks
through a single model. The proposed model can not only compensate the lost
contextual information of reconstructed image which is refined from low
frequency error in geometric characteristic k-space, but also integrate the
theoretical guarantee of model-based methods and the superior reconstruction
performances of deep learning-based methods. All physical-model parameters are
learnable, and numerical experiments show that our framework outperforms
state-of-the-art methods in terms of qualitative and quantitative evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Architecture for the detection of GAN-generated Flood Images with Localization Capabilities. (arXiv:2205.07073v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07073">
<div class="article-summary-box-inner">
<span><p>In this paper, we address a new image forensics task, namely the detection of
fake flood images generated by ClimateGAN architecture. We do so by proposing a
hybrid deep learning architecture including both a detection and a localization
branch, the latter being devoted to the identification of the image regions
manipulated by ClimateGAN. Even if our goal is the detection of fake flood
images, in fact, we found that adding a localization branch helps the network
to focus on the most relevant image regions with significant improvements in
terms of generalization capabilities and robustness against image processing
operations. The good performance of the proposed architecture is validated on
two datasets of pristine flood images downloaded from the internet and three
datasets of fake flood images generated by ClimateGAN starting from a large set
of diverse street images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Corrosion Detection for Industrial Objects: From Multi-Sensor System to 5D Feature Space. (arXiv:2205.07075v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07075">
<div class="article-summary-box-inner">
<span><p>Corrosion is a form of damage that often appears on the surface of metal-made
objects used in industrial applications. Those damages can be critical
depending on the purpose of the used object. Optical-based testing systems
provide a form of non-contact data acquisition, where the acquired data can
then be used to analyse the surface of an object. In the field of industrial
image processing, this is called surface inspection. We provide a testing setup
consisting of a rotary table which rotates the object by 360 degrees, as well
as industrial RGB cameras and laser triangulation sensors for the acquisition
of 2D and 3D data as our multi-sensor system. These sensors acquire data while
the object to be tested takes a full rotation. Further on, data augmentation is
applied to prepare new data or enhance already acquired data. In order to
evaluate the impact of a laser triangulation sensor for corrosion detection,
one challenge is to at first fuse the data of both domains. After the data
fusion process, 5 different channels can be utilized to create a 5D feature
space. Besides the red, green and blue channels of the image (1-3), additional
range data from the laser triangulation sensor is incorporated (4). As a fifth
channel, said sensor provides additional intensity data (5). With a
multi-channel image classification, a 5D feature space will lead to slightly
superior results opposed to a 3D feature space, composed of only the RGB
channels of the image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spiking Approximations of the MaxPooling Operation in Deep SNNs. (arXiv:2205.07076v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07076">
<div class="article-summary-box-inner">
<span><p>Spiking Neural Networks (SNNs) are an emerging domain of biologically
inspired neural networks that have shown promise for low-power AI. A number of
methods exist for building deep SNNs, with Artificial Neural Network
(ANN)-to-SNN conversion being highly successful. MaxPooling layers in
Convolutional Neural Networks (CNNs) are an integral component to downsample
the intermediate feature maps and introduce translational invariance, but the
absence of their hardware-friendly spiking equivalents limits such CNNs'
conversion to deep SNNs. In this paper, we present two hardware-friendly
methods to implement Max-Pooling in deep SNNs, thus facilitating easy
conversion of CNNs with MaxPooling layers to SNNs. In a first, we also execute
SNNs with spiking-MaxPooling layers on Intel's Loihi neuromorphic hardware
(with MNIST, FMNIST, &amp; CIFAR10 dataset); thus, showing the feasibility of our
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monitoring of Pigmented Skin Lesions Using 3D Whole Body Imaging. (arXiv:2205.07085v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07085">
<div class="article-summary-box-inner">
<span><p>Modern data-driven machine learning research that enables revolutionary
advances in image analysis has now become a critical tool to redefine how skin
lesions are documented, mapped, and tracked. We propose a 3D whole body imaging
prototype to enable rapid evaluation and mapping of skin lesions. A modular
camera rig arranged in a cylindrical configuration is designed to automatically
capture synchronised images from multiple angles for entire body scanning. We
develop algorithms for 3D body image reconstruction, data processing and skin
lesion detection based on deep convolutional neural networks. We also propose a
customised, intuitive and flexible interface that allows the user to interact
and collaborate with the machine to understand the data. The hybrid of the
human and computer is represented by the analysis of 2D lesion detection, 3D
mapping and data management. The experimental results using synthetic and real
images demonstrate the effectiveness of the proposed solution by providing
multiple views of the target skin lesion, enabling further 3D geometry
analysis. Skin lesions are identified as outliers which deserve more attention
from a skin cancer physician. Our detector identifies lesions at a comparable
performance level as a physician. The proposed 3D whole body imaging system can
be used by dermatological clinics, allowing for fast documentation of lesions,
quick and accurate analysis of the entire body to detect suspicious lesions.
Because of its fast examination, the method might be used for screening or
epidemiological investigations. 3D data analysis has the potential to change
the paradigm of total-body photography with many applications in skin diseases,
including inflammatory and pigmentary disorders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal curb detection and filtering. (arXiv:2205.07096v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07096">
<div class="article-summary-box-inner">
<span><p>Reliable knowledge of road boundaries is critical for autonomous vehicle
navigation. We propose a robust curb detection and filtering technique based on
the fusion of camera semantics and dense lidar point clouds. The lidar point
clouds are collected by fusing multiple lidars for robust feature detection.
The camera semantics are based on a modified EfficientNet architecture which is
trained with labeled data collected from onboard fisheye cameras. The point
clouds are associated with the closest curb segment with $L_2$-norm analysis
after projecting into the image space with the fisheye model projection. Next,
the selected points are clustered using unsupervised density-based spatial
clustering to detect different curb regions. As new curb points are detected in
consecutive frames they are associated with the existing curb clusters using
temporal reachability constraints. If no reachability constraints are found a
new curb cluster is formed from these new points. This ensures we can detect
multiple curbs present in road segments consisting of multiple lanes if they
are in the sensors' field of view. Finally, Delaunay filtering is applied for
outlier removal and its performance is compared to traditional RANSAC-based
filtering. An objective evaluation of the proposed solution is done using a
high-definition map containing ground truth curb points obtained from a
commercial map supplier. The proposed system has proven capable of detecting
curbs of any orientation in complex urban road scenarios comprising straight
roads, curved roads, and intersections with traffic isles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable SAR Renderer and SAR Target Reconstruction. (arXiv:2205.07099v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07099">
<div class="article-summary-box-inner">
<span><p>Forward modeling of wave scattering and radar imaging mechanisms is the key
to information extraction from synthetic aperture radar (SAR) images. Like
inverse graphics in optical domain, an inherently-integrated forward-inverse
approach would be promising for SAR advanced information retrieval and target
reconstruction. This paper presents such an attempt to the inverse graphics for
SAR imagery. A differentiable SAR renderer (DSR) is developed which
reformulates the mapping and projection algorithm of SAR imaging mechanism in
the differentiable form of probability maps. First-order gradients of the
proposed DSR are then analytically derived which can be back-propagated from
rendered image/silhouette to the target geometry and scattering attributes. A
3D inverse target reconstruction algorithm from SAR images is devised. Several
simulation and reconstruction experiments are conducted, including targets with
and without background, using both synthesized data or real measured inverse
SAR (ISAR) data by ground radar. Results demonstrate the efficacy of the
proposed DSR and its inverse approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Deep Learning Methods for Identification of Defective Casting Products. (arXiv:2205.07118v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07118">
<div class="article-summary-box-inner">
<span><p>Quality inspection has become crucial in any large-scale manufacturing
industry recently. In order to reduce human error, it has become imperative to
use efficient and low computational AI algorithms to identify such defective
products. In this paper, we have compared and contrasted various pre-trained
and custom-built architectures using model size, performance and CPU latency in
the detection of defective casting products. Our results show that custom
architectures are efficient than pre-trained mobile architectures. Moreover,
custom models perform 6 to 9 times faster than lightweight models such as
MobileNetV2 and NasNet. The number of training parameters and the model size of
the custom architectures is significantly lower (~386 times &amp; ~119 times
respectively) than the best performing models such as MobileNetV2 and NasNet.
Augmentation experimentations have also been carried out on the custom
architectures to make the models more robust and generalizable. Our work sheds
light on the efficiency of these custom-built architectures for deployment on
Edge and IoT devices and that transfer learning models may not always be ideal.
Instead, they should be specific to the kind of dataset and the classification
problem at hand.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Facial Key Point Detection: An Efficient Approach Using Deep Neural Networks. (arXiv:2205.07121v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07121">
<div class="article-summary-box-inner">
<span><p>Facial landmark detection is a widely researched field of deep learning as
this has a wide range of applications in many fields. These key points are
distinguishing characteristic points on the face, such as the eyes center, the
eye's inner and outer corners, the mouth center, and the nose tip from which
human emotions and intent can be explained. The focus of our work has been
evaluating transfer learning models such as MobileNetV2 and NasNetMobile,
including custom CNN architectures. The objective of the research has been to
develop efficient deep learning models in terms of model size, parameters, and
inference time and to study the effect of augmentation imputation and
fine-tuning on these models. It was found that while augmentation techniques
produced lower RMSE scores than imputation techniques, they did not affect the
inference time. MobileNetV2 architecture produced the lowest RMSE and inference
time. Moreover, our results indicate that manually optimized CNN architectures
performed similarly to Auto Keras tuned architecture. However, manually
optimized architectures yielded better inference time and training curves.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of Astronomical Bodies by Efficient Layer Fine-Tuning of Deep Neural Networks. (arXiv:2205.07124v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07124">
<div class="article-summary-box-inner">
<span><p>The SDSS-IV dataset contains information about various astronomical bodies
such as Galaxies, Stars, and Quasars captured by observatories. Inspired by our
work on deep multimodal learning, which utilized transfer learning to classify
the SDSS-IV dataset, we further extended our research in the fine tuning of
these architectures to study the effect in the classification scenario.
Architectures such as Resnet-50, DenseNet-121 VGG-16, Xception, EfficientNetB2,
MobileNetV2 and NasnetMobile have been built using layer wise fine tuning at
different levels. Our findings suggest that freezing all layers with Imagenet
weights and adding a final trainable layer may not be the optimal solution.
Further, baseline models and models that have higher number of trainable layers
performed similarly in certain architectures. Model need to be fine tuned at
different levels and a specific training ratio is required for a model to be
termed ideal. Different architectures had different responses to the change in
the number of trainable layers w.r.t accuracies. While models such as
DenseNet-121, Xception, EfficientNetB2 achieved peak accuracies that were
relatively consistent with near perfect training curves, models such as
Resnet-50,VGG-16, MobileNetV2 and NasnetMobile had lower, delayed peak
accuracies with poorly fitting training curves. It was also found that though
mobile neural networks have lesser parameters and model size, they may not
always be ideal for deployment on a low computational device as they had
consistently lower validation accuracies. Customized evaluation metrics such as
Tuning Parameter Ratio and Tuning Layer Ratio are used for model evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ETAD: A Unified Framework for Efficient Temporal Action Detection. (arXiv:2205.07134v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07134">
<div class="article-summary-box-inner">
<span><p>Untrimmed video understanding such as temporal action detection (TAD) often
suffers from the pain of huge demand for computing resources. Because of long
video durations and limited GPU memory, most action detectors can only operate
on pre-extracted features rather than the original videos, and they still
require a lot of computation to achieve high detection performance. To
alleviate the heavy computation problem in TAD, in this work, we first propose
an efficient action detector with detector proposal sampling, based on the
observation that performance saturates at a small number of proposals. This
detector is designed with several important techniques, such as LSTM-boosted
temporal aggregation and cascaded proposal refinement to achieve high detection
quality as well as low computational cost. To enable joint optimization of this
action detector and the feature encoder, we also propose encoder gradient
sampling, which selectively back-propagates through video snippets and
tremendously reduces GPU memory consumption. With the two sampling strategies
and the effective detector, we build a unified framework for efficient
end-to-end temporal action detection (ETAD), making real-world untrimmed video
understanding tractable. ETAD achieves state-of-the-art performance on both
THUMOS-14 and ActivityNet-1.3. Interestingly, on ActivityNet-1.3, it reaches
37.78% average mAP, while only requiring 6 mins of training time and 1.23 GB
memory based on pre-extracted features. With end-to-end training, it reduces
the GPU memory footprint by more than 70% with even higher performance (38.21%
average mAP), as compared with traditional end-to-end methods. The code is
available at https://github.com/sming256/ETAD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breaking with Fixed Set Pathology Recognition through Report-Guided Contrastive Training. (arXiv:2205.07139v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07139">
<div class="article-summary-box-inner">
<span><p>When reading images, radiologists generate text reports describing the
findings therein. Current state-of-the-art computer-aided diagnosis tools
utilize a fixed set of predefined categories automatically extracted from these
medical reports for training. This form of supervision limits the potential
usage of models as they are unable to pick up on anomalies outside of their
predefined set, thus, making it a necessity to retrain the classifier with
additional data when faced with novel classes. In contrast, we investigate
direct text supervision to break away from this closed set assumption. By doing
so, we avoid noisy label extraction via text classifiers and incorporate more
contextual information.
</p>
<p>We employ a contrastive global-local dual-encoder architecture to learn
concepts directly from unstructured medical reports while maintaining its
ability to perform free form classification.
</p>
<p>We investigate relevant properties of open set recognition for radiological
data and propose a method to employ currently weakly annotated data into
training.
</p>
<p>We evaluate our approach on the large-scale chest X-Ray datasets MIMIC-CXR,
CheXpert, and ChestX-Ray14 for disease classification. We show that despite
using unstructured medical report supervision, we perform on par with direct
label supervision through a sophisticated inference setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Uncertainty Calibration for Open-Set Recognition. (arXiv:2205.07160v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07160">
<div class="article-summary-box-inner">
<span><p>Despite achieving enormous success in predictive accuracy for visual
classification problems, deep neural networks (DNNs) suffer from providing
overconfident probabilities on out-of-distribution (OOD) data. Yet, accurate
uncertainty estimation is crucial for safe and reliable robot autonomy. In this
paper, we evaluate popular calibration techniques for open-set conditions in a
way that is distinctly different from the conventional evaluation of
calibration methods on OOD data. Our results show that closed-set DNN
calibration approaches are much less effective for open-set recognition, which
highlights the need to develop new DNN calibration methods to address this
problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLaMa: Joint Spatial and Frequency Loss for General Image Inpainting. (arXiv:2205.07162v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07162">
<div class="article-summary-box-inner">
<span><p>The purpose of image inpainting is to recover scratches and damaged areas
using context information from remaining parts. In recent years, thanks to the
resurgence of convolutional neural networks (CNNs), image inpainting task has
made great breakthroughs. However, most of the work consider insufficient types
of mask, and their performance will drop dramatically when encountering unseen
masks. To combat these challenges, we propose a simple yet general method to
solve this problem based on the LaMa image inpainting framework, dubbed GLaMa.
Our proposed GLaMa can better capture different types of missing information by
using more types of masks. By incorporating more degraded images in the
training phase, we can expect to enhance the robustness of the model with
respect to various masks. In order to yield more reasonable results, we further
introduce a frequency-based loss in addition to the traditional spatial
reconstruction loss and adversarial loss. In particular, we introduce an
effective reconstruction loss both in the spatial and frequency domain to
reduce the chessboard effect and ripples in the reconstructed image. Extensive
experiments demonstrate that our method can boost the performance over the
original LaMa method for each type of mask on FFHQ, ImageNet, Places2 and
WikiArt dataset. The proposed GLaMa was ranked first in terms of PSNR, LPIPS
and SSIM in the NTIRE 2022 Image Inpainting Challenge Track 1 Unsupervised.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proxyless Neural Architecture Adaptation for Supervised Learning and Self-Supervised Learning. (arXiv:2205.07168v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07168">
<div class="article-summary-box-inner">
<span><p>Recently, Neural Architecture Search (NAS) methods have been introduced and
show impressive performance on many benchmarks. Among those NAS studies, Neural
Architecture Transformer (NAT) aims to adapt the given neural architecture to
improve performance while maintaining computational costs. However, NAT lacks
reproducibility and it requires an additional architecture adaptation process
before network weight training. In this paper, we propose proxyless neural
architecture adaptation that is reproducible and efficient. Our method can be
applied to both supervised learning and self-supervised learning. The proposed
method shows stable performance on various architectures. Extensive
reproducibility experiments on two datasets, i.e., CIFAR-10 and Tiny Imagenet,
present that the proposed method definitely outperforms NAT and is applicable
to other models and datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection. (arXiv:2205.07179v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07179">
<div class="article-summary-box-inner">
<span><p>Growing interests in RGB-D salient object detection (RGB-D SOD) have been
witnessed in recent years, owing partly to the popularity of depth sensors and
the rapid progress of deep learning techniques. Unfortunately, existing RGB-D
SOD methods typically demand large quantity of training images being thoroughly
annotated at pixel-level. The laborious and time-consuming manual annotation
has become a real bottleneck in various practical scenarios. On the other hand,
current unsupervised RGB-D SOD methods still heavily rely on handcrafted
feature representations. This inspires us to propose in this paper a deep
unsupervised RGB-D saliency detection approach, which requires no manual
pixel-level annotation during training. It is realized by two key ingredients
in our training pipeline. First, a depth-disentangled saliency update (DSU)
framework is designed to automatically produce pseudo-labels with iterative
follow-up refinements, which provides more trustworthy supervision signals for
training the saliency network. Second, an attentive training strategy is
introduced to tackle the issue of noisy pseudo-labels, by properly re-weighting
to highlight the more reliable pseudo-labels. Extensive experiments demonstrate
the superior efficiency and effectiveness of our approach in tackling the
challenging unsupervised RGB-D SOD scenarios. Moreover, our approach can also
be adapted to work in fully-supervised situation. Empirical studies show the
incorporation of our approach gives rise to notably performance improvement in
existing supervised RGB-D SOD models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Lip-Based Audio-Visual Speaker Embeddings with AV-HuBERT. (arXiv:2205.07180v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07180">
<div class="article-summary-box-inner">
<span><p>This paper investigates self-supervised pre-training for audio-visual speaker
representation learning where a visual stream showing the speaker's mouth area
is used alongside speech as inputs. Our study focuses on the Audio-Visual
Hidden Unit BERT (AV-HuBERT) approach, a recently developed general-purpose
audio-visual speech pre-training framework. We conducted extensive experiments
probing the effectiveness of pre-training and visual modality. Experimental
results suggest that AV-HuBERT generalizes decently to speaker related
downstream tasks, improving label efficiency by roughly ten fold for both
audio-only and audio-visual speaker verification. We also show that
incorporating visual information, even just the lip area, greatly improves the
performance and noise robustness, reducing EER by 38% in the clean condition
and 75% in noisy conditions. Our code and models will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nonconvex ${{L_ {{1/2}}}} $-Regularized Nonlocal Self-similarity Denoiser for Compressive Sensing based CT Reconstruction. (arXiv:2205.07185v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07185">
<div class="article-summary-box-inner">
<span><p>Compressive sensing (CS) based computed tomography (CT) image reconstruction
aims at reducing the radiation risk through sparse-view projection data. It is
usually challenging to achieve satisfying image quality from incomplete
projections. Recently, the nonconvex ${{L_ {{1/2}}}} $-norm has achieved
promising performance in sparse recovery, while the applications on imaging are
unsatisfactory due to its nonconvexity. In this paper, we develop a ${{L_
{{1/2}}}} $-regularized nonlocal self-similarity (NSS) denoiser for CT
reconstruction problem, which integrates low-rank approximation with group
sparse coding (GSC) framework. Concretely, we first split the CT reconstruction
problem into two subproblems, and then improve the CT image quality furtherly
using our ${{L_ {{1/2}}}} $-regularized NSS denoiser. Instead of optimizing the
nonconvex problem under the perspective of GSC, we particularly reconstruct CT
image via low-rank minimization based on two simple yet essential schemes,
which build the equivalent relationship between GSC based denoiser and low-rank
minimization. Furtherly, the weighted singular value thresholding (WSVT)
operator is utilized to optimize the resulting nonconvex ${{L_ {{1/2}}}} $
minimization problem. Following this, our proposed denoiser is integrated with
the CT reconstruction problem by alternating direction method of multipliers
(ADMM) framework. Extensive experimental results on typical clinical CT images
have demonstrated that our approach can further achieve better performance than
popular approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-centric Consistency Learning for Deepfake Detection. (arXiv:2205.07201v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07201">
<div class="article-summary-box-inner">
<span><p>Most of previous deepfake detection researches bent their efforts to describe
and discriminate artifacts in human perceptible ways, which leave a bias in the
learned networks of ignoring some critical invariance features intra-class and
underperforming the robustness of internet interference. Essentially, the
target of deepfake detection problem is to represent natural faces and fake
faces at the representation space discriminatively, and it reminds us whether
we could optimize the feature extraction procedure at the representation space
through constraining intra-class consistence and inter-class inconsistence to
bring the intra-class representations close and push the inter-class
representations apart? Therefore, inspired by contrastive representation
learning, we tackle the deepfake detection problem through learning the
invariant representations of both classes and propose a novel real-centric
consistency learning method. We constraint the representation from both the
sample level and the feature level. At the sample level, we take the procedure
of deepfake synthesis into consideration and propose a novel forgery
semantical-based pairing strategy to mine latent generation-related features.
At the feature level, based on the centers of natural faces at the
representation space, we design a hard positive mining and synthesizing method
to simulate the potential marginal features. Besides, a hard negative fusion
method is designed to improve the discrimination of negative marginal features
with the help of supervised contrastive margin loss we developed. The
effectiveness and robustness of the proposed method has been demonstrated
through extensive experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fused Deep Neural Network based Transfer Learning in Occluded Face Classification and Person re-Identification. (arXiv:2205.07203v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07203">
<div class="article-summary-box-inner">
<span><p>Recent period of pandemic has brought person identification even with
occluded face image a great importance with increased number of mask usage.
This paper aims to recognize the occlusion of one of four types in face images.
Various transfer learning methods were tested, and the results show that
MobileNet V2 with Gated Recurrent Unit(GRU) performs better than any other
Transfer Learning methods, with a perfect accuracy of 99% in classification of
images as with or without occlusion and if with occlusion, then the type of
occlusion. In parallel, identifying the Region of interest from the device
captured image is done. This extracted Region of interest is utilised in face
identification. Such a face identification process is done using the ResNet
model with its Caffe implementation. To reduce the execution time, after the
face occlusion type was recognized the person was searched to confirm their
face image in the registered database. The face label of the person obtained
from both simultaneous processes was verified for their matching score. If the
matching score was above 90, the recognized label of the person was logged into
a file with their name, type of mask, date, and time of recognition.
MobileNetV2 is a lightweight framework which can also be used in embedded or
IoT devices to perform real time detection and identification in suspicious
areas of investigations using CCTV footages. When MobileNetV2 was combined with
GRU, a reliable accuracy was obtained. The data provided in the paper belong to
two categories, being either collected from Google Images for occlusion
classification, face recognition, and facial landmarks, or collected in
fieldwork. The motive behind this research is to identify and log person
details which could serve surveillance activities in society-based
e-governance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Frame Interpolation with Transformer. (arXiv:2205.07230v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07230">
<div class="article-summary-box-inner">
<span><p>Video frame interpolation (VFI), which aims to synthesize intermediate frames
of a video, has made remarkable progress with development of deep convolutional
networks over past years. Existing methods built upon convolutional networks
generally face challenges of handling large motion due to the locality of
convolution operations. To overcome this limitation, we introduce a novel
framework, which takes advantage of Transformer to model long-range pixel
correlation among video frames. Further, our network is equipped with a novel
cross-scale window-based attention mechanism, where cross-scale windows
interact with each other. This design effectively enlarges the receptive field
and aggregates multi-scale information. Extensive quantitative and qualitative
experiments demonstrate that our method achieves new state-of-the-art results
on various benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combating COVID-19 using Generative Adversarial Networks and Artificial Intelligence for Medical Images: A Scoping Review. (arXiv:2205.07236v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07236">
<div class="article-summary-box-inner">
<span><p>This review presents a comprehensive study on the role of GANs in addressing
the challenges related to COVID-19 data scarcity and diagnosis. It is the first
review that summarizes the different GANs methods and the lungs images datasets
for COVID-19. It attempts to answer the questions related to applications of
GANs, popular GAN architectures, frequently used image modalities, and the
availability of source code. This review included 57 full-text studies that
reported the use of GANs for different applications in COVID-19 lungs images
data. Most of the studies (n=42) used GANs for data augmentation to enhance the
performance of AI techniques for COVID-19 diagnosis. Other popular applications
of GANs were segmentation of lungs and super-resolution of the lungs images.
The cycleGAN and the conditional GAN were the most commonly used architectures
used in nine studies each. 29 studies used chest X-Ray images while 21 studies
used CT images for the training of GANs. For majority of the studies (n=47),
the experiments were done and results were reported using publicly available
data. A secondary evaluation of the results by radiologists/clinicians was
reported by only two studies. Conclusion: Studies have shown that GANs have
great potential to address the data scarcity challenge for lungs images of
COVID-19. Data synthesized with GANs have been helpful to improve the training
of the Convolutional Neural Network (CNN) models trained for the diagnosis of
COVID-19. Besides, GANs have also contributed to enhancing the CNNs performance
through the super-resolution of the images and segmentation. This review also
identified key limitations of the potential transformation of GANs based
methods in clinical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FreeMatch: Self-adaptive Thresholding for Semi-supervised Learning. (arXiv:2205.07246v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07246">
<div class="article-summary-box-inner">
<span><p>Pseudo labeling and consistency regularization approaches with
confidence-based thresholding have made great progress in semi-supervised
learning (SSL). In this paper, we theoretically and empirically analyze the
relationship between the unlabeled data distribution and the desirable
confidence threshold. Our analysis shows that previous methods might fail to
define favorable threshold since they either require a pre-defined / fixed
threshold or an ad-hoc threshold adjusting scheme that does not reflect the
learning effect well, resulting in inferior performance and slow convergence,
especially for complicated unlabeled data distributions. We hence propose
\emph{FreeMatch} to define and adjust the confidence threshold in a
self-adaptive manner according to the model's learning status. To handle
complicated unlabeled data distributions more effectively, we further propose a
self-adaptive class fairness regularization method that encourages the model to
produce diverse predictions during training. Extensive experimental results
indicate the superiority of FreeMatch especially when the labeled data are
extremely rare. FreeMatch achieves \textbf{5.78}\%, \textbf{13.59}\%, and
\textbf{1.28}\% error rate reduction over the latest state-of-the-art method
FlexMatch on CIFAR-10 with 1 label per class, STL-10 with 4 labels per class,
and ImageNet with 100k labels respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guidelines for the Regularization of Gammas in Batch Normalization for Deep Residual Networks. (arXiv:2205.07260v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07260">
<div class="article-summary-box-inner">
<span><p>L2 regularization for weights in neural networks is widely used as a standard
training trick. However, L2 regularization for gamma, a trainable parameter of
batch normalization, remains an undiscussed mystery and is applied in different
ways depending on the library and practitioner. In this paper, we study whether
L2 regularization for gamma is valid. To explore this issue, we consider two
approaches: 1) variance control to make the residual network behave like
identity mapping and 2) stable optimization through the improvement of
effective learning rate. Through two analyses, we specify the desirable and
undesirable gamma to apply L2 regularization and propose four guidelines for
managing them. In several experiments, we observed the increase and decrease in
performance caused by applying L2 regularization to gamma of four categories,
which is consistent with our four guidelines. Our proposed guidelines were
validated through various tasks and architectures, including variants of
residual networks and transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regulating Facial Processing Technologies: Tensions Between Legal and Technical Considerations in the Application of Illinois BIPA. (arXiv:2205.07299v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07299">
<div class="article-summary-box-inner">
<span><p>Harms resulting from the development and deployment of facial processing
technologies (FPT) have been met with increasing controversy. Several states
and cities in the U.S. have banned the use of facial recognition by law
enforcement and governments, but FPT are still being developed and used in a
wide variety of contexts where they primarily are regulated by state biometric
information privacy laws. Among these laws, the 2008 Illinois Biometric
Information Privacy Act (BIPA) has generated a significant amount of
litigation. Yet, with most BIPA lawsuits reaching settlements before there have
been meaningful clarifications of relevant technical intricacies and legal
definitions, there remains a great degree of uncertainty as to how exactly this
law applies to FPT. What we have found through applications of BIPA in FPT
litigation so far, however, points to potential disconnects between technical
and legal communities. This paper analyzes what we know based on BIPA court
proceedings and highlights these points of tension: areas where the technical
operationalization of BIPA may create unintended and undesirable incentives for
FPT development, as well as areas where BIPA litigation can bring to light the
limitations of solely technical methods in achieving legal privacy values.
These factors are relevant for (i) reasoning about biometric information
privacy laws as a governing mechanism for FPT, (ii) assessing the potential
harms of FPT, and (iii) providing incentives for the mitigation of these harms.
By illuminating these considerations, we hope to empower courts and lawmakers
to take a more nuanced approach to regulating FPT and developers to better
understand privacy values in the current U.S. legal landscape.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Vector Graphics Generation for Music Cover Images. (arXiv:2205.07301v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07301">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GAN) have motivated a rapid growth of the
domain of computer image synthesis. As almost all the existing image synthesis
algorithms consider an image as a pixel matrix, the high-resolution image
synthesis is complicated.A good alternative can be vector images. However, they
belong to the highly sophisticated parametric space, which is a restriction for
solving the task of synthesizing vector graphics by GANs. In this paper, we
consider a specific application domain that softens this restriction
dramatically allowing the usage of vector image synthesis.
</p>
<p>Music cover images should meet the requirements of Internet streaming
services and printing standards, which imply high resolution of graphic
materials without any additional requirements on the content of such images.
Existing music cover image generation services do not analyze tracks
themselves; however, some services mostly consider only genre tags. To generate
music covers as vector images that reflect the music and consist of simple
geometric objects, we suggest a GAN-based algorithm called CoverGAN. The
assessment of resulting images is based on their correspondence to the music
compared with AttnGAN and DALL-E text-to-image generation according to title or
lyrics. Moreover, the significance of the patterns found by CoverGAN has been
evaluated in terms of the correspondence of the generated cover images to the
musical tracks. Listeners evaluate the music covers generated by the proposed
algorithm as quite satisfactory and corresponding to the tracks. Music cover
images generation code and demo are available at
https://github.com/IzhanVarsky/CoverGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty estimation for Cross-dataset performance in Trajectory prediction. (arXiv:2205.07310v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07310">
<div class="article-summary-box-inner">
<span><p>While a lot of work has been done on developing trajectory prediction
methods, and various datasets have been proposed for benchmarking this task,
little study has been done so far on the generalizability and the
transferability of these methods across dataset. In this paper, we study the
performance of a state-of-the-art trajectory prediction method across four
different datasets (Argoverse, NuScenes, Interaction, Shifts). We first check
how a similar method can be applied and trained on all these datasets with
similar hyperparameters. Then we highlight which datasets work best on others,
and study how uncertainty estimation allows for a better transferable
performance; proposing a novel way to estimate uncertainty and to directly use
it in prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trucks Don't Mean Trump: Diagnosing Human Error in Image Analysis. (arXiv:2205.07333v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07333">
<div class="article-summary-box-inner">
<span><p>Algorithms provide powerful tools for detecting and dissecting human bias and
error. Here, we develop machine learning methods to to analyze how humans err
in a particular high-stakes task: image interpretation. We leverage a unique
dataset of 16,135,392 human predictions of whether a neighborhood voted for
Donald Trump or Joe Biden in the 2020 US election, based on a Google Street
View image. We show that by training a machine learning estimator of the Bayes
optimal decision for each image, we can provide an actionable decomposition of
human error into bias, variance, and noise terms, and further identify specific
features (like pickup trucks) which lead humans astray. Our methods can be
applied to ensure that human-in-the-loop decision-making is accurate and fair
and are also applicable to black-box algorithmic systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Novel Multicolumn Kernel Extreme Learning Machine for Food Detection via Optimal Features from CNN. (arXiv:2205.07348v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07348">
<div class="article-summary-box-inner">
<span><p>Automatic food detection is an emerging topic of interest due to its wide
array of applications ranging from detecting food images on social media
platforms to filtering non-food photos from the users in dietary assessment
apps. Recently, during the COVID-19 pandemic, it has facilitated enforcing an
eating ban by automatically detecting eating activities from cameras in public
places. Therefore, to tackle the challenge of recognizing food images with high
accuracy, we proposed the idea of a hybrid framework for extracting and
selecting optimal features from an efficient neural network. There on, a
nonlinear classifier is employed to discriminate between linearly inseparable
feature vectors with great precision. In line with this idea, our method
extracts features from MobileNetV3, selects an optimal subset of attributes by
using Shapley Additive exPlanations (SHAP) values, and exploits kernel extreme
learning machine (KELM) due to its nonlinear decision boundary and good
generalization ability. However, KELM suffers from the 'curse of dimensionality
problem' for large datasets due to the complex computation of kernel matrix
with large numbers of hidden nodes. We solved this problem by proposing a novel
multicolumn kernel extreme learning machine (MCKELM) which exploited the k-d
tree algorithm to divide data into N subsets and trains separate KELM on each
subset of data. Then, the method incorporates KELM classifiers into parallel
structures and selects the top k nearest subsets during testing by using the
k-d tree search for classifying input instead of the whole network. For
evaluating a proposed framework large food/non-food dataset is prepared using
nine publically available datasets. Experimental results showed the superiority
of our method on an integrated set of measures while solving the problem of
'curse of dimensionality in KELM for large datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-Resolution CMB Lensing Reconstruction with Deep Learning. (arXiv:2205.07368v1 [astro-ph.CO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07368">
<div class="article-summary-box-inner">
<span><p>Next-generation cosmic microwave background (CMB) surveys are expected to
provide valuable information about the primordial universe by creating maps of
the mass along the line of sight. Traditional tools for creating these lensing
convergence maps include the quadratic estimator and the maximum likelihood
based iterative estimator. Here, we apply a generative adversarial network
(GAN) to reconstruct the lensing convergence field. We compare our results with
a previous deep learning approach -- Residual-UNet -- and discuss the pros and
cons of each. In the process, we use training sets generated by a variety of
power spectra, rather than the one used in testing the methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SuperWarp: Supervised Learning and Warping on U-Net for Invariant Subvoxel-Precise Registration. (arXiv:2205.07399v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07399">
<div class="article-summary-box-inner">
<span><p>In recent years, learning-based image registration methods have gradually
moved away from direct supervision with target warps to instead use
self-supervision, with excellent results in several registration benchmarks.
These approaches utilize a loss function that penalizes the intensity
differences between the fixed and moving images, along with a suitable
regularizer on the deformation. In this paper, we argue that the relative
failure of supervised registration approaches can in part be blamed on the use
of regular U-Nets, which are jointly tasked with feature extraction, feature
matching, and estimation of deformation. We introduce one simple but crucial
modification to the U-Net that disentangles feature extraction and matching
from deformation prediction, allowing the U-Net to warp the features, across
levels, as the deformation field is evolved. With this modification, direct
supervision using target warps begins to outperform self-supervision approaches
that require segmentations, presenting new directions for registration when
images do not have segmentations. We hope that our findings in this preliminary
workshop paper will re-ignite research interest in supervised image
registration techniques. Our code is publicly available from
https://github.com/balbasty/superwarp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PillarNet: High-Performance Pillar-based 3D Object Detection. (arXiv:2205.07403v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07403">
<div class="article-summary-box-inner">
<span><p>Real-time and high-performance 3D object detection is of critical importance
for autonomous driving. Recent top-performing 3D object detectors mainly rely
on point-based or 3D voxel-based convolutions, which are both computationally
inefficient for onboard deployment. In contrast, pillar-based methods use
merely 2D convolutions, which consume less computation resources, but they lag
far behind their voxel-based counterparts in detection accuracy. In this paper,
by examining the primary performance gap between pillar- and voxel-based
detectors, we develop a real-time and high-performance pillar-based detector,
dubbed PillarNet. The proposed PillarNet consists of a powerful encoder network
for effective pillar feature learning, a neck network for spatial-semantic
feature fusion and the commonly used detect head. Using only 2D convolutions,
PillarNet is flexible to an optional pillar size and compatible with classical
2D CNN backbones, such as VGGNet and ResNet. Additionally, PillarNet benefits
from an orientation-decoupled IoU regression loss along with the IoU-aware
prediction branch. Extensive experimental results on the large-scale nuScenes
Dataset and Waymo Open Dataset demonstrate that the proposed PillarNet performs
well over the state-of-the-art 3D detectors in terms of effectiveness and
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Outlier Removal Strategy Based on Reliability of Correspondence Graph for Fast Point Cloud Registration. (arXiv:2205.07404v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07404">
<div class="article-summary-box-inner">
<span><p>Registration is a basic yet crucial task in point cloud processing. In
correspondence-based point cloud registration, matching correspondences by
point feature techniques may lead to an extremely high outlier ratio. Current
methods still suffer from low efficiency, accuracy, and recall rate. We use a
simple and intuitive method to describe the 6-DOF (degree of freedom)
curtailment process in point cloud registration and propose an outlier removal
strategy based on the reliability of the correspondence graph. The method
constructs the corresponding graph according to the given correspondences and
designs the concept of the reliability degree of the graph node for optimal
candidate selection and the reliability degree of the graph edge to obtain the
global maximum consensus set. The presented method could achieve fast and
accurate outliers removal along with gradual aligning parameters estimation.
Extensive experiments on simulations and challenging real-world datasets
demonstrate that the proposed method can still perform effective point cloud
registration even the correspondence outlier ratio is over 99%, and the
efficiency is better than the state-of-the-art. Code is available at
https://github.com/WPC-WHU/GROR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers in 3D Point Clouds: A Survey. (arXiv:2205.07417v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07417">
<div class="article-summary-box-inner">
<span><p>In recent years, Transformer models have been proven to have the remarkable
ability of long-range dependencies modeling. They have achieved satisfactory
results both in Natural Language Processing (NLP) and image processing. This
significant achievement sparks great interest among researchers in 3D point
cloud processing to apply them to various 3D tasks. Due to the inherent
permutation invariance and strong global feature learning ability, 3D
Transformers are well suited for point cloud processing and analysis. They have
achieved competitive or even better performance compared to the
state-of-the-art non-Transformer algorithms. This survey aims to provide a
comprehensive overview of 3D Transformers designed for various tasks (e.g.
point cloud classification, segmentation, object detection, and so on). We
start by introducing the fundamental components of the general Transformer and
providing a brief description of its application in 2D and 3D fields. Then, we
present three different taxonomies (i.e., Transformer implementation-based
taxonomy, data representation-based taxonomy, and task-based taxonomy) for
method classification, which allows us to analyze involved methods from
multiple perspectives. Furthermore, we also conduct an investigation of 3D
self-attention mechanism variants designed for performance improvement. To
demonstrate the superiority of 3D Transformers, we compare the performance of
Transformer-based algorithms in terms of point cloud classification,
segmentation, and object detection. Finally, we point out three potential
future research directions, expecting to provide some benefit references for
the development of 3D Transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Binarizing by Classification: Is soft function really necessary?. (arXiv:2205.07433v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07433">
<div class="article-summary-box-inner">
<span><p>Binary neural network leverages the $Sign$ function to binarize real values,
and its non-derivative property inevitably brings huge gradient errors during
backpropagation. Although many hand-designed soft functions have been proposed
to approximate gradients, their mechanism is not clear and there are still huge
performance gaps between binary models and their full-precision counterparts.
To address this, we propose to tackle network binarization as a binary
classification problem and use a multi-layer perceptron (MLP) as the
classifier. The MLP-based classifier can fit any continuous function
theoretically and is adaptively learned to binarize networks and backpropagate
gradients without any specific soft function. With this view, we further prove
experimentally that even a simple linear function can outperform previous
complex soft functions. Extensive experiments demonstrate that the proposed
method yields surprising performance both in image classification and human
pose estimation tasks. Specifically, we achieve 65.7% top-1 accuracy of
ResNet-34 on ImageNet dataset, with an absolute improvement of 2.8%. When
evaluating on the challenging Microsoft COCO keypoint dataset, the proposed
method enables binary networks to achieve a mAP of 60.6 for the first time, on
par with some full-precision methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReDFeat: Recoupling Detection and Description for Multimodal Feature Learning. (arXiv:2205.07439v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07439">
<div class="article-summary-box-inner">
<span><p>Deep-learning-based local feature extraction algorithms that combine
detection and description have made significant progress in visible image
matching. However, the end-to-end training of such frameworks is notoriously
unstable due to the lack of strong supervision of detection and the
inappropriate coupling between detection and description. The problem is
magnified in cross-modal scenarios, in which most methods heavily rely on the
pre-training. In this paper, we recouple independent constraints of detection
and description of multimodal feature learning with a mutual weighting
strategy, in which the detected probabilities of robust features are forced to
peak and repeat, while features with high detection scores are emphasized
during optimization. Different from previous works, those weights are detached
from back propagation so that the detected probability of indistinct features
would not be directly suppressed and the training would be more stable.
Moreover, we propose the Super Detector, a detector that possesses a large
receptive field and is equipped with learnable non-maximum suppression layers,
to fulfill the harsh terms of detection. Finally, we build a benchmark that
contains cross visible, infrared, near-infrared and synthetic aperture radar
image pairs for evaluating the performance of features in feature matching and
image registration tasks. Extensive experiments demonstrate that features
trained with the recoulped detection and description, named ReDFeat, surpass
previous state-of-the-arts in the benchmark, while the model can be readily
trained from scratch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffusion Models for Adversarial Purification. (arXiv:2205.07460v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07460">
<div class="article-summary-box-inner">
<span><p>Adversarial purification refers to a class of defense methods that remove
adversarial perturbations using a generative model. These methods do not make
assumptions on the form of attack and the classification model, and thus can
defend pre-existing classifiers against unseen threats. However, their
performance currently falls behind adversarial training methods. In this work,
we propose DiffPure that uses diffusion models for adversarial purification:
Given an adversarial example, we first diffuse it with a small amount of noise
following a forward diffusion process, and then recover the clean image through
a reverse generative process. To evaluate our method against strong adaptive
attacks in an efficient and scalable way, we propose to use the adjoint method
to compute full gradients of the reverse generative process. Extensive
experiments on three image datasets including CIFAR-10, ImageNet and CelebA-HQ
with three classifier architectures including ResNet, WideResNet and ViT
demonstrate that our method achieves the state-of-the-art results,
outperforming current adversarial training and adversarial purification
methods, often by a large margin. Project page: https://diffpure.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Representation via Dynamic Feature Aggregation. (arXiv:2205.07466v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07466">
<div class="article-summary-box-inner">
<span><p>Deep convolutional neural network (CNN) based models are vulnerable to the
adversarial attacks. One of the possible reasons is that the embedding space of
CNN based model is sparse, resulting in a large space for the generation of
adversarial samples. In this study, we propose a method, denoted as Dynamic
Feature Aggregation, to compress the embedding space with a novel
regularization. Particularly, the convex combination between two samples are
regarded as the pivot for aggregation. In the embedding space, the selected
samples are guided to be similar to the representation of the pivot. On the
other side, to mitigate the trivial solution of such regularization, the last
fully-connected layer of the model is replaced by an orthogonal classifier, in
which the embedding codes for different classes are processed orthogonally and
separately. With the regularization and orthogonal classifier, a more compact
embedding space can be obtained, which accordingly improves the model
robustness against adversarial attacks. An averaging accuracy of 56.91% is
achieved by our method on CIFAR-10 against various attack methods, which
significantly surpasses a solid baseline (Mixup) by a margin of 37.31%. More
surprisingly, empirical results show that, the proposed method can also achieve
the state-of-the-art performance for out-of-distribution (OOD) detection, due
to the learned compact feature space. An F1 score of 0.937 is achieved by the
proposed method, when adopting CIFAR-10 as in-distribution (ID) dataset and
LSUN as OOD dataset. Code is available at
https://github.com/HaozheLiu-ST/DynamicFeatureAggregation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Convolutional Dictionary Network for CT Metal Artifact Reduction. (arXiv:2205.07471v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07471">
<div class="article-summary-box-inner">
<span><p>Inspired by the great success of deep neural networks, learning-based methods
have gained promising performances for metal artifact reduction (MAR) in
computed tomography (CT) images. However, most of the existing approaches put
less emphasis on modelling and embedding the intrinsic prior knowledge
underlying this specific MAR task into their network designs. Against this
issue, we propose an adaptive convolutional dictionary network (ACDNet), which
leverages both model-based and learning-based methods. Specifically, we explore
the prior structures of metal artifacts, e.g., non-local repetitive streaking
patterns, and encode them as an explicit weighted convolutional dictionary
model. Then, a simple-yet-effective algorithm is carefully designed to solve
the model. By unfolding every iterative substep of the proposed algorithm into
a network module, we explicitly embed the prior structure into a deep network,
\emph{i.e.,} a clear interpretability for the MAR task. Furthermore, our ACDNet
can automatically learn the prior for artifact-free CT images via training data
and adaptively adjust the representation kernels for each input CT image based
on its content. Hence, our method inherits the clear interpretability of
model-based methods and maintains the powerful representation ability of
learning-based methods. Comprehensive experiments executed on synthetic and
clinical datasets show the superiority of our ACDNet in terms of effectiveness
and model generalization. {\color{blue}{{\textit{Code is available at
{\url{https://github.com/hongwang01/ACDNet}}.}}}}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frequency selective extrapolation with residual filtering for image error concealment. (arXiv:2205.07476v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07476">
<div class="article-summary-box-inner">
<span><p>The purpose of signal extrapolation is to estimate unknown signal parts from
known samples. This task is especially important for error concealment in image
and video communication. For obtaining a high quality reconstruction,
assumptions have to be made about the underlying signal in order to solve this
underdetermined problem. Among existent reconstruction algorithms, frequency
selective extrapolation (FSE) achieves high performance by assuming that image
signals can be sparsely represented in the frequency domain. However, FSE does
not take into account the low-pass behaviour of natural images. In this paper,
we propose a modified FSE that takes this prior knowledge into account for the
modelling, yielding significant PSNR gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Manifold Characteristics That Predict Downstream Task Performance. (arXiv:2205.07477v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07477">
<div class="article-summary-box-inner">
<span><p>Pretraining methods are typically compared by evaluating the accuracy of
linear classifiers, transfer learning performance, or visually inspecting the
representation manifold's (RM) lower-dimensional projections. We show that the
differences between methods can be understood more clearly by investigating the
RM directly, which allows for a more detailed comparison. To this end, we
propose a framework and new metric to measure and compare different RMs. We
also investigate and report on the RM characteristics for various pretraining
methods. These characteristics are measured by applying sequentially larger
local alterations to the input data, using white noise injections and Projected
Gradient Descent (PGD) adversarial attacks, and then tracking each datapoint.
We calculate the total distance moved for each datapoint and the relative
change in distance between successive alterations. We show that self-supervised
methods learn an RM where alterations lead to large but constant size changes,
indicating a smoother RM than fully supervised methods. We then combine these
measurements into one metric, the Representation Manifold Quality Metric
(RMQM), where larger values indicate larger and less variable step sizes, and
show that RMQM correlates positively with performance on downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topologically Persistent Features-based Object Recognition in Cluttered Indoor Environments. (arXiv:2205.07479v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07479">
<div class="article-summary-box-inner">
<span><p>Recognition of occluded objects in unseen indoor environments is a
challenging problem for mobile robots. This work proposes a new slicing-based
topological descriptor that captures the 3D shape of object point clouds to
address this challenge. It yields similarities between the descriptors of the
occluded and the corresponding unoccluded objects, enabling object unity-based
recognition using a library of trained models. The descriptor is obtained by
partitioning an object's point cloud into multiple 2D slices and constructing
filtrations (nested sequences of simplicial complexes) on the slices to mimic
further slicing of the slices, thereby capturing detailed shapes through
persistent homology-generated features. We use nine different sequences of
cluttered scenes from a benchmark dataset for performance evaluation. Our
method outperforms two state-of-the-art deep learning-based point cloud
classification methods, namely, DGCNN and SimpleView.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Out-of-Distribution Detection for Real-World Settings. (arXiv:1911.11132v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.11132">
<div class="article-summary-box-inner">
<span><p>Detecting out-of-distribution examples is important for safety-critical
machine learning applications such as detecting novel biological phenomena and
self-driving cars. However, existing research mainly focuses on simple
small-scale settings. To set the stage for more realistic out-of-distribution
detection, we depart from small-scale settings and explore large-scale
multiclass and multi-label settings with high-resolution images and thousands
of classes. To make future work in real-world settings possible, we create new
benchmarks for three large-scale settings. To test ImageNet multiclass anomaly
detectors, we introduce the Species dataset containing over 700,000 images and
over a thousand anomalous species. We leverage ImageNet-21K to evaluate PASCAL
VOC and COCO multilabel anomaly detectors. Third, we introduce a new benchmark
for anomaly segmentation by introducing a segmentation benchmark with road
anomalies. We conduct extensive experiments in these more realistic settings
for out-of-distribution detection and find that a surprisingly simple detector
based on the maximum logit outperforms prior methods in all the large-scale
multi-class, multi-label, and segmentation tasks, establishing a simple new
baseline for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-light Image Enhancement Using the Cell Vibration Model. (arXiv:2006.02271v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.02271">
<div class="article-summary-box-inner">
<span><p>Low light very likely leads to the degradation of an image's quality and even
causes visual task failures. Existing image enhancement technologies are prone
to overenhancement, color distortion or time consumption, and their
adaptability is fairly limited. Therefore, we propose a new single low-light
image lightness enhancement method. First, an energy model is presented based
on the analysis of membrane vibrations induced by photon stimulations. Then,
based on the unique mathematical properties of the energy model and combined
with the gamma correction model, a new global lightness enhancement model is
proposed. Furthermore, a special relationship between image lightness and gamma
intensity is found. Finally, a local fusion strategy, including segmentation,
filtering and fusion, is proposed to optimize the local details of the global
lightness enhancement images. Experimental results show that the proposed
algorithm is superior to nine state-of-the-art methods in avoiding color
distortion, restoring the textures of dark areas, reproducing natural colors
and reducing time cost. The image source and code will be released at
https://github.com/leixiaozhou/CDEFmethod.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PFGDF: Pruning Filter via Gaussian Distribution Feature for Deep Neural Networks Acceleration. (arXiv:2006.12963v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.12963">
<div class="article-summary-box-inner">
<span><p>The existence of a lot of redundant information in convolutional neural
networks leads to the slow deployment of its equipment on the edge. To solve
this issue, we proposed a novel deep learning model compression acceleration
method based on data distribution characteristics, namely Pruning Filter via
Gaussian Distribution Feature(PFGDF) which was to found the smaller interval of
the convolution layer of a certain layer to describe the original on the
grounds of distribution characteristics . Compared with revious advanced
methods, PFGDF compressed the model by filters with insignificance in
distribution regardless of the contribution and sensitivity information of the
convolution filter. The pruning process of the model was automated, and always
ensured that the compressed model could restore the performance of original
model. Notably, on CIFAR-10, PFGDF compressed the convolution filter on VGG-16
by 66:62%, the parameter reducing more than 90%, and FLOPs achieved 70:27%. On
ResNet-32, PFGDF reduced the convolution filter by 21:92%. The parameter was
reduced to 54:64%, and the FLOPs exceeded 42%
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weight-dependent Gates for Network Pruning. (arXiv:2007.02066v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.02066">
<div class="article-summary-box-inner">
<span><p>In this paper, a simple yet effective network pruning framework is proposed
to simultaneously address the problems of pruning indicator, pruning ratio, and
efficiency constraint. This paper argues that the pruning decision should
depend on the convolutional weights, and thus proposes novel weight-dependent
gates (W-Gates) to learn the information from filter weights and obtain binary
gates to prune or keep the filters automatically. To prune the network under
efficiency constraints, a switchable Efficiency Module is constructed to
predict the hardware latency or FLOPs of candidate pruned networks. Combined
with the proposed Efficiency Module, W-Gates can perform filter pruning in an
efficiency-aware manner and achieve a compact network with a better
accuracy-efficiency trade-off. We have demonstrated the effectiveness of the
proposed method on ResNet34, ResNet50, and MobileNet V2, respectively achieving
up to 1.33/1.28/1.1 higher Top-1 accuracy with lower hardware latency on
ImageNet. Compared with state-of-the-art methods, W-Gates also achieves
superior performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">cMinMax: A Fast Algorithm to Find the Corners of an N-dimensional Convex Polytope. (arXiv:2011.14035v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14035">
<div class="article-summary-box-inner">
<span><p>During the last years, the emerging field of Augmented &amp; Virtual Reality
(AR-VR) has seen tremendousgrowth. At the same time there is a trend to develop
low cost high-quality AR systems where computing poweris in demand. Feature
points are extensively used in these real-time frame-rate and 3D applications,
thereforeefficient high-speed feature detectors are necessary. Corners are such
special features and often are used as thefirst step in the marker alignment in
Augmented Reality (AR). Corners are also used in image registration
andrecognition, tracking, SLAM, robot path finding and 2D or 3D object
detection and retrieval. Therefore thereis a large number of corner detection
algorithms but most of them are too computationally intensive for use
inreal-time applications of any complexity. Many times the border of the image
is a convex polygon. For thisspecial, but quite common case, we have developed
a specific algorithm, cMinMax. The proposed algorithmis faster, approximately
by a factor of 5 compared to the widely used Harris Corner Detection algorithm.
Inaddition is highly parallelizable. The algorithm is suitable for the fast
registration of markers in augmentedreality systems and in applications where a
computationally efficient real time feature detector is necessary.The algorithm
can also be extended to N-dimensional polyhedrons.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-Based Guidance for Tracking Dynamic Objects. (arXiv:2104.09301v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09301">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel vision-based framework for tracking dynamic
objects using guidance laws based on a rendezvous cone approach. These guidance
laws enable an unmanned aircraft system equipped with a monocular camera to
continuously follow a moving object within the sensor's field of view. We
identify and classify feature point estimators for managing the occurrence of
occlusions during the tracking process in an exclusive manner. Furthermore, we
develop an open-source simulation environment and perform a series of
simulations to show the efficacy of our methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Spatial Reasoning on Multi-View Line Drawings. (arXiv:2104.13433v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13433">
<div class="article-summary-box-inner">
<span><p>Spatial reasoning on multi-view line drawings by state-of-the-art supervised
deep networks is recently shown with puzzling low performances on the SPARE3D
dataset. Based on the fact that self-supervised learning is helpful when a
large number of data are available, we propose two self-supervised learning
approaches to improve the baseline performance for view consistency reasoning
and camera pose reasoning tasks on the SPARE3D dataset. For the first task, we
use a self-supervised binary classification network to contrast the line
drawing differences between various views of any two similar 3D objects,
enabling the trained networks to effectively learn detail-sensitive yet
view-invariant line drawing representations of 3D objects. For the second type
of task, we propose a self-supervised multi-class classification framework to
train a model to select the correct corresponding view from which a line
drawing is rendered. Our method is even helpful for the downstream tasks with
unseen camera poses. Experiments show that our method could significantly
increase the baseline performance in SPARE3D, while some popular
self-supervised learning methods cannot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZeroWaste Dataset: Towards Deformable Object Segmentation in Cluttered Scenes. (arXiv:2106.02740v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02740">
<div class="article-summary-box-inner">
<span><p>Less than 35% of recyclable waste is being actually recycled in the US, which
leads to increased soil and sea pollution and is one of the major concerns of
environmental researchers as well as the common public. At the heart of the
problem are the inefficiencies of the waste sorting process (separating paper,
plastic, metal, glass, etc.) due to the extremely complex and cluttered nature
of the waste stream. Recyclable waste detection poses a unique computer vision
challenge as it requires detection of highly deformable and often translucent
objects in cluttered scenes without the kind of context information usually
present in human-centric datasets. This challenging computer vision task
currently lacks suitable datasets or methods in the available literature. In
this paper, we take a step towards computer-aided waste detection and present
the first in-the-wild industrial-grade waste detection and segmentation
dataset, ZeroWaste. We believe that ZeroWaste will catalyze research in object
detection and semantic segmentation in extreme clutter as well as applications
in the recycling domain. Our project page can be found at
<a href="http://ai.bu.edu/zerowaste/.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Applications of knowledge graphs for food science and industry. (arXiv:2107.05869v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05869">
<div class="article-summary-box-inner">
<span><p>The deployment of various networks (e.g., Internet of Things [IoT] and mobile
networks), databases (e.g., nutrition tables and food compositional databases),
and social media (e.g., Instagram and Twitter) generates huge amounts of food
data, which present researchers with an unprecedented opportunity to study
various problems and applications in food science and industry via data-driven
computational methods. However, these multi-source heterogeneous food data
appear as information silos, leading to difficulty in fully exploiting these
food data. The knowledge graph provides a unified and standardized conceptual
terminology in a structured form, and thus can effectively organize these food
data to benefit various applications. In this review, we provide a brief
introduction to knowledge graphs and the evolution of food knowledge
organization mainly from food ontology to food knowledge graphs. We then
summarize seven representative applications of food knowledge graphs, such as
new recipe development, diet-disease correlation discovery, and personalized
dietary recommendation. We also discuss future directions in this field, such
as multimodal food knowledge graph construction and food knowledge graphs for
human health.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lifting the Convex Conjugate in Lagrangian Relaxations: A Tractable Approach for Continuous Markov Random Fields. (arXiv:2107.06028v2 [math.OC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06028">
<div class="article-summary-box-inner">
<span><p>Dual decomposition approaches in nonconvex optimization may suffer from a
duality gap. This poses a challenge when applying them directly to nonconvex
problems such as MAP-inference in a Markov random field (MRF) with continuous
state spaces. To eliminate such gaps, this paper considers a reformulation of
the original nonconvex task in the space of measures. This infinite-dimensional
reformulation is then approximated by a semi-infinite one, which is obtained
via a piecewise polynomial discretization in the dual. We provide a geometric
intuition behind the primal problem induced by the dual discretization and draw
connections to optimization over moment spaces. In contrast to existing
discretizations which suffer from a grid bias, we show that a piecewise
polynomial discretization better preserves the continuous nature of our
problem. Invoking results from optimal transport theory and convex algebraic
geometry we reduce the semi-infinite program to a finite one and provide a
practical implementation based on semidefinite programming. We show,
experimentally and in theory, that the approach successfully reduces the
duality gap. To showcase the scalability of our approach, we apply it to the
stereo matching problem between two images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clipped Hyperbolic Classifiers Are Super-Hyperbolic Classifiers. (arXiv:2107.11472v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11472">
<div class="article-summary-box-inner">
<span><p>Hyperbolic space can naturally embed hierarchies, unlike Euclidean space.
Hyperbolic Neural Networks (HNNs) exploit such representational power by
lifting Euclidean features into hyperbolic space for classification,
outperforming Euclidean neural networks (ENNs) on datasets with known semantic
hierarchies. However, HNNs underperform ENNs on standard benchmarks without
clear hierarchies, greatly restricting HNNs' applicability in practice.
</p>
<p>Our key insight is that HNNs' poorer general classification performance
results from vanishing gradients during backpropagation, caused by their hybrid
architecture connecting Euclidean features to a hyperbolic classifier. We
propose an effective solution by simply clipping the Euclidean feature
magnitude while training HNNs.
</p>
<p>Our experiments demonstrate that clipped HNNs become super-hyperbolic
classifiers: They are not only consistently better than HNNs which already
outperform ENNs on hierarchical data, but also on-par with ENNs on MNIST,
CIFAR10, CIFAR100 and ImageNet benchmarks, with better adversarial robustness
and out-of-distribution detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Operator-Splitting Method for the Gaussian Curvature Regularization Model with Applications to Surface Smoothing and Imaging. (arXiv:2108.01914v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01914">
<div class="article-summary-box-inner">
<span><p>Gaussian curvature is an important geometric property of surfaces, which has
been used broadly in mathematical modeling. Due to the full nonlinearity of the
Gaussian curvature, efficient numerical methods for models based on it are
uncommon in literature. In this article, we propose an operator-splitting
method for a general Gaussian curvature model. In our method, we decouple the
full nonlinearity of Gaussian curvature from differential operators by
introducing two matrix- and vector-valued functions. The optimization problem
is then converted into the search for the steady state solution of a time
dependent PDE system. The above PDE system is well-suited to time
discretization by operator splitting, the sub-problems encountered at each
fractional step having either a closed form solution or being solvable by
efficient algorithms. The proposed method is not sensitive to the choice of
parameters, its efficiency and performances being demonstrated via systematic
experiments on surface smoothing and image denoising.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AASeg: Attention Aware Network for Real Time Semantic Segmentation. (arXiv:2108.04349v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04349">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a new network named Attention Aware Network (AASeg)
for real time semantic image segmentation. Our network incorporates spatial and
channel information using Spatial Attention (SA) and Channel Attention (CA)
modules respectively. It also uses dense local multi-scale context information
using Multi Scale Context (MSC) module. The feature maps are concatenated
individually to produce the final segmentation map. We demonstrate the
effectiveness of our method using a comprehensive analysis, quantitative
experimental results and ablation study using Cityscapes, ADE20K and Camvid
datasets. Our network performs better than most previous architectures with a
74.4\% Mean IOU on Cityscapes test dataset while running at 202.7 FPS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. (arXiv:2108.10904v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10904">
<div class="article-summary-box-inner">
<span><p>With recent progress in joint modeling of visual and textual representations,
Vision-Language Pretraining (VLP) has achieved impressive performance on many
multimodal downstream tasks. However, the requirement for expensive annotations
including clean image captions and regional labels limits the scalability of
existing approaches, and complicates the pretraining procedure with the
introduction of multiple dataset-specific objectives. In this work, we relax
these constraints and present a minimalist pretraining framework, named Simple
Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training
complexity by exploiting large-scale weak supervision, and is trained
end-to-end with a single prefix language modeling objective. Without utilizing
extra data or task-specific customization, the resulting model significantly
outperforms previous pretraining methods and achieves new state-of-the-art
results on a wide range of discriminative and generative vision-language
benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE
(+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score).
Furthermore, we demonstrate that SimVLM acquires strong generalization and
transfer ability, enabling zero-shot behavior including open-ended visual
question answering and cross-modality transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Coated Adversarial Camouflages for Object Detectors. (arXiv:2109.00124v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00124">
<div class="article-summary-box-inner">
<span><p>An adversary can fool deep neural network object detectors by generating
adversarial noises. Most of the existing works focus on learning local visible
noises in an adversarial "patch" fashion. However, the 2D patch attached to a
3D object tends to suffer from an inevitable reduction in attack performance as
the viewpoint changes. To remedy this issue, this work proposes the Coated
Adversarial Camouflage (CAC) to attack the detectors in arbitrary viewpoints.
Unlike the patch trained in the 2D space, our camouflage generated by a
conceptually different training framework consists of 3D rendering and dense
proposals attack. Specifically, we make the camouflage perform 3D spatial
transformations according to the pose changes of the object. Based on the
multi-view rendering results, the top-n proposals of the region proposal
network are fixed, and all the classifications in the fixed dense proposals are
attacked simultaneously to output errors. In addition, we build a virtual 3D
scene to fairly and reproducibly evaluate different attacks. Extensive
experiments demonstrate the superiority of CAC over the existing attacks, and
it shows impressive performance both in the virtual scene and the real world.
This poses a potential threat to the security-critical computer vision systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Cross-Scale Visual Representations for Real-Time Image Geo-Localization. (arXiv:2109.04087v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04087">
<div class="article-summary-box-inner">
<span><p>Robot localization remains a challenging task in GPS denied environments.
State estimation approaches based on local sensors, e.g. cameras or IMUs, are
drifting-prone for long-range missions as error accumulates. In this study, we
aim to address this problem by localizing image observations in a 2D
multi-modal geospatial map. We introduce the cross-scale dataset and a
methodology to produce additional data from cross-modality sources. We propose
a framework that learns cross-scale visual representations without supervision.
Experiments are conducted on data from two different domains, underwater and
aerial. In contrast to existing studies in cross-view image geo-localization,
our approach a) performs better on smaller-scale multi-modal maps; b) is more
computationally efficient for real-time applications; c) can serve directly in
concert with state estimation pipelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Sparse Masks for Diffusion-based Image Inpainting. (arXiv:2110.02636v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02636">
<div class="article-summary-box-inner">
<span><p>Diffusion-based inpainting is a powerful tool for the reconstruction of
images from sparse data. Its quality strongly depends on the choice of known
data. Optimising their spatial location -- the inpainting mask -- is
challenging. A commonly used tool for this task are stochastic optimisation
strategies. However, they are slow as they compute multiple inpainting results.
We provide a remedy in terms of a learned mask generation model. By emulating
the complete inpainting pipeline with two networks for mask generation and
neural surrogate inpainting, we obtain a model for highly efficient adaptive
mask generation. Experiments indicate that our model can achieve competitive
quality with an acceleration by as much as four orders of magnitude. Our
findings serve as a basis for making diffusion-based inpainting more attractive
for applications such as image compression, where fast encoding is highly
desirable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Flows as a General Purpose Solution for Inverse Problems. (arXiv:2110.13285v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13285">
<div class="article-summary-box-inner">
<span><p>Due to the success of generative flows to model data distributions, they have
been explored in inverse problems. Given a pre-trained generative flow,
previous work proposed to minimize the 2-norm of the latent variables as a
regularization term. The intuition behind it was to ensure high likelihood
latent variables that produce the closest restoration. However, high-likelihood
latent variables may generate unrealistic samples as we show in our
experiments. We therefore propose a solver to directly produce high-likelihood
reconstructions. We hypothesize that our approach could make generative flows a
general purpose solver for inverse problems. Furthermore, we propose 1 x 1
coupling functions to introduce permutations in a generative flow. It has the
advantage that its inverse does not require to be calculated in the generation
process. Finally, we evaluate our method for denoising, deblurring, inpainting,
and colorization. We observe a compelling improvement of our method over prior
works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic 3D Scene Reconstruction From a Single RGB Image. (arXiv:2111.02444v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02444">
<div class="article-summary-box-inner">
<span><p>Understanding 3D scenes from a single image is fundamental to a wide variety
of tasks, such as for robotics, motion planning, or augmented reality. Existing
works in 3D perception from a single RGB image tend to focus on geometric
reconstruction only, or geometric reconstruction with semantic segmentation or
instance segmentation. Inspired by 2D panoptic segmentation, we propose to
unify the tasks of geometric reconstruction, 3D semantic segmentation, and 3D
instance segmentation into the task of panoptic 3D scene reconstruction - from
a single RGB image, predicting the complete geometric reconstruction of the
scene in the camera frustum of the image, along with semantic and instance
segmentations. We thus propose a new approach for holistic 3D scene
understanding from a single RGB image which learns to lift and propagate 2D
features from an input image to a 3D volumetric scene representation. We
demonstrate that this holistic view of joint scene reconstruction, semantic,
and instance segmentation is beneficial over treating the tasks independently,
thus outperforming alternative approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DSPoint: Dual-scale Point Cloud Recognition with High-frequency Fusion. (arXiv:2111.10332v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10332">
<div class="article-summary-box-inner">
<span><p>Point cloud processing is a challenging task due to its sparsity and
irregularity. Prior works introduce delicate designs on either local feature
aggregator or global geometric architecture, but few combine both advantages.
We propose Dual-Scale Point Cloud Recognition with High-frequency Fusion
(DSPoint) to extract local-global features by concurrently operating on voxels
and points. We reverse the conventional design of applying convolution on
voxels and attention to points. Specifically, we disentangle point features
through channel dimension for dual-scale processing: one by point-wise
convolution for fine-grained geometry parsing, the other by voxel-wise global
attention for long-range structural exploration. We design a co-attention
fusion module for feature alignment to blend local-global modalities, which
conducts inter-scale cross-modality interaction by communicating high-frequency
coordinates information. Experiments and ablations on widely-adopted
ModelNet40, ShapeNet, and S3DIS demonstrate the state-of-the-art performance of
our DSPoint.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TridentAdapt: Learning Domain-invariance via Source-Target Confrontation and Self-induced Cross-domain Augmentation. (arXiv:2111.15300v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15300">
<div class="article-summary-box-inner">
<span><p>Due to the difficulty of obtaining ground-truth labels, learning from
virtual-world datasets is of great interest for real-world applications like
semantic segmentation. From domain adaptation perspective, the key challenge is
to learn domain-agnostic representation of the inputs in order to benefit from
virtual data. In this paper, we propose a novel trident-like architecture that
enforces a shared feature encoder to satisfy confrontational source and target
constraints simultaneously, thus learning a domain-invariant feature space.
Moreover, we also introduce a novel training pipeline enabling self-induced
cross-domain data augmentation during the forward pass. This contributes to a
further reduction of the domain gap. Combined with a self-training process, we
obtain state-of-the-art results on benchmark datasets (e.g. GTA5 or Synthia to
Cityscapes adaptation). Code and pre-trained models are available at
https://github.com/HMRC-AEL/TridentAdapt
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoSSL: Co-Learning of Representation and Classifier for Imbalanced Semi-Supervised Learning. (arXiv:2112.04564v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04564">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel co-learning framework (CoSSL) with
decoupled representation learning and classifier learning for imbalanced SSL.
To handle the data imbalance, we devise Tail-class Feature Enhancement (TFE)
for classifier learning. Furthermore, the current evaluation protocol for
imbalanced SSL focuses only on balanced test sets, which has limited
practicality in real-world scenarios. Therefore, we further conduct a
comprehensive evaluation under various shifted test distributions. In
experiments, we show that our approach outperforms other methods over a large
range of shifted distributions, achieving state-of-the-art performance on
benchmark datasets ranging from CIFAR-10, CIFAR-100, ImageNet, to Food-101. Our
code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Transformers with Primal Object Queries for Multi-Label Image Classification. (arXiv:2112.05485v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05485">
<div class="article-summary-box-inner">
<span><p>Multi-label image classification is about predicting a set of class labels
that can be considered as orderless sequential data. Transformers process the
sequential data as a whole, therefore they are inherently good at set
prediction. The first vision-based transformer model, which was proposed for
the object detection task introduced the concept of object queries. Object
queries are learnable positional encodings that are used by attention modules
in decoder layers to decode the object classes or bounding boxes using the
region of interests in an image. However, inputting the same set of object
queries to different decoder layers hinders the training: it results in lower
performance and delays convergence. In this paper, we propose the usage of
primal object queries that are only provided at the start of the transformer
decoder stack. In addition, we improve the mixup technique proposed for
multi-label classification. The proposed transformer model with primal object
queries improves the state-of-the-art class wise F1 metric by 2.1% and 1.8%;
and speeds up the convergence by 79.0% and 38.6% on MS-COCO and NUS-WIDE
datasets respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HerosNet: Hyperspectral Explicable Reconstruction and Optimal Sampling Deep Network for Snapshot Compressive Imaging. (arXiv:2112.06238v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06238">
<div class="article-summary-box-inner">
<span><p>Hyperspectral imaging is an essential imaging modality for a wide range of
applications, especially in remote sensing, agriculture, and medicine. Inspired
by existing hyperspectral cameras that are either slow, expensive, or bulky,
reconstructing hyperspectral images (HSIs) from a low-budget snapshot
measurement has drawn wide attention. By mapping a truncated numerical
optimization algorithm into a network with a fixed number of phases, recent
deep unfolding networks (DUNs) for spectral snapshot compressive sensing (SCI)
have achieved remarkable success. However, DUNs are far from reaching the scope
of industrial applications limited by the lack of cross-phase feature
interaction and adaptive parameter adjustment. In this paper, we propose a
novel Hyperspectral Explicable Reconstruction and Optimal Sampling deep Network
for SCI, dubbed HerosNet, which includes several phases under the
ISTA-unfolding framework. Each phase can flexibly simulate the sensing matrix
and contextually adjust the step size in the gradient descent step, and
hierarchically fuse and interact the hidden states of previous phases to
effectively recover current HSI frames in the proximal mapping step.
Simultaneously, a hardware-friendly optimal binary mask is learned end-to-end
to further improve the reconstruction performance. Finally, our HerosNet is
validated to outperform the state-of-the-art methods on both simulation and
real datasets by large margins. The source code is available at
https://github.com/jianzhangcs/HerosNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FIgLib & SmokeyNet: Dataset and Deep Learning Model for Real-Time Wildland Fire Smoke Detection. (arXiv:2112.08598v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08598">
<div class="article-summary-box-inner">
<span><p>The size and frequency of wildland fires in the western United States have
dramatically increased in recent years. On high-fire-risk days, a small fire
ignition can rapidly grow and become out of control. Early detection of fire
ignitions from initial smoke can assist the response to such fires before they
become difficult to manage. Past deep learning approaches for wildfire smoke
detection have suffered from small or unreliable datasets that make it
difficult to extrapolate performance to real-world scenarios. In this work, we
present the Fire Ignition Library (FIgLib), a publicly available dataset of
nearly 25,000 labeled wildfire smoke images as seen from fixed-view cameras
deployed in Southern California. We also introduce SmokeyNet, a novel deep
learning architecture using spatiotemporal information from camera imagery for
real-time wildfire smoke detection. When trained on the FIgLib dataset,
SmokeyNet outperforms comparable baselines and rivals human performance. We
hope that the availability of the FIgLib dataset and the SmokeyNet architecture
will inspire further research into deep learning methods for wildfire smoke
detection, leading to automated notification systems that reduce the time to
wildfire response.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks. (arXiv:2201.05729v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05729">
<div class="article-summary-box-inner">
<span><p>Contrastive language-image pretraining (CLIP) links vision and language
modalities into a unified embedding space, yielding the tremendous potential
for vision-language (VL) tasks. While early concurrent works have begun to
study this potential on a subset of tasks, important questions remain: 1) What
is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in
low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches
without impacting inference or pretraining complexity? In this work, we seek to
answer these questions through two key contributions. First, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data availability constraints and conditions of domain shift. Second, we
propose an approach, named CLIP Targeted Distillation (CLIP-TD), to
intelligently distill knowledge from CLIP into existing architectures using a
dynamically weighted objective applied to adaptively selected tokens per
instance. Experiments demonstrate that our proposed CLIP-TD leads to
exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to
71.3%) conditions of VCR, while simultaneously improving performance under
standard fully-supervised conditions (up to 2%), achieving state-of-art
performance on VCR compared to other single models that are pretrained with
image-text data only. On SNLI-VE, CLIP-TD produces significant gains in
low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On
VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in
fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works
utilizing CLIP for finetuning, as well as baseline naive distillation
approaches. Code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmentation of the Carotid Lumen and Vessel Wall using Deep Learning and Location Priors. (arXiv:2201.06259v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06259">
<div class="article-summary-box-inner">
<span><p>In this report we want to present our method and results for the Carotid
Artery Vessel Wall Segmentation Challenge. We propose an image-based pipeline
utilizing the U-Net architecture and location priors to solve the segmentation
problem at hand.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTHA: Video Captioning Evaluation Via Transfer-Learned Human Assessment. (arXiv:2201.10243v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10243">
<div class="article-summary-box-inner">
<span><p>Evaluating video captioning systems is a challenging task as there are
multiple factors to consider; for instance: the fluency of the caption,
multiple actions happening in a single scene, and the human bias of what is
considered important. Most metrics try to measure how similar the system
generated captions are to a single or a set of human-annotated captions. This
paper presents a new method based on a deep learning model to evaluate these
systems. The model is based on BERT, which is a language model that has been
shown to work well in multiple NLP tasks. The aim is for the model to learn to
perform an evaluation similar to that of a human. To do so, we use a dataset
that contains human evaluations of system generated captions. The dataset
consists of the human judgments of the captions produce by the system
participating in various years of the TRECVid video to text task. These
annotations will be made publicly available. BERTHA obtain favourable results,
outperforming the commonly used metrics in some setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalized visual encoding model construction with small data. (arXiv:2202.02245v2 [q-bio.QM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02245">
<div class="article-summary-box-inner">
<span><p>Encoding models that predict brain response patterns to stimuli are one way
to capture this relationship between variability in bottom-up neural systems
and individual's behavior or pathological state. However, they generally need a
large amount of training data to achieve optimal accuracy. Here, we propose and
test an alternative personalized ensemble encoding model approach to utilize
existing encoding models, to create encoding models for novel individuals with
relatively little stimuli-response data. We show that these personalized
ensemble encoding models trained with small amounts of data for a specific
individual, i.e. ~300 image-response pairs, achieve accuracy not different from
models trained on ~20,000 image-response pairs for the same individual.
Importantly, the personalized ensemble encoding models preserve patterns of
inter-individual variability in the image-response relationship. Additionally,
we show the proposed approach is robust against domain shift by validating on a
prospectively collected set of image-response data in novel individuals with a
different scanner and experimental setup. Finally, we use our personalized
ensemble encoding model within the recently developed NeuroGen framework to
generate optimal stimuli designed to maximize specific regions' activations for
a specific individual. We show that the inter-individual differences in face
areas responses to images of animal vs human faces observed previously is
replicated using NeuroGen with the ensemble encoding model. Our approach shows
the potential to use previously collected, deeply sampled data to efficiently
create accurate, personalized encoding models and, subsequently, personalized
optimal synthetic images for new individuals scanned under different
experimental conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIPasso: Semantically-Aware Object Sketching. (arXiv:2202.05822v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05822">
<div class="article-summary-box-inner">
<span><p>Abstraction is at the heart of sketching due to the simple and minimal nature
of line drawings. Abstraction entails identifying the essential visual
properties of an object or scene, which requires semantic understanding and
prior knowledge of high-level concepts. Abstract depictions are therefore
challenging for artists, and even more so for machines. We present CLIPasso, an
object sketching method that can achieve different levels of abstraction,
guided by geometric and semantic simplifications. While sketch generation
methods often rely on explicit sketch datasets for training, we utilize the
remarkable ability of CLIP (Contrastive-Language-Image-Pretraining) to distill
semantic concepts from sketches and images alike. We define a sketch as a set
of B\'ezier curves and use a differentiable rasterizer to optimize the
parameters of the curves directly with respect to a CLIP-based perceptual loss.
The abstraction degree is controlled by varying the number of strokes. The
generated sketches demonstrate multiple levels of abstraction while maintaining
recognizability, underlying structure, and essential visual components of the
subject drawn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Complementarity of Images and Text for the Expression of Emotions in Social Media. (arXiv:2202.07427v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07427">
<div class="article-summary-box-inner">
<span><p>Authors of posts in social media communicate their emotions and what causes
them with text and images. While there is work on emotion and stimulus
detection for each modality separately, it is yet unknown if the modalities
contain complementary emotion information in social media. We aim at filling
this research gap and contribute a novel, annotated corpus of English
multimodal Reddit posts. On this resource, we develop models to automatically
detect the relation between image and text, an emotion stimulus category and
the emotion class. We evaluate if these tasks require both modalities and find
for the image-text relations, that text alone is sufficient for most categories
(complementary, illustrative, opposing): the information in the text allows to
predict if an image is required for emotion understanding. The emotions of
anger and sadness are best predicted with a multimodal model, while text alone
is sufficient for disgust, joy, and surprise. Stimuli depicted by objects,
animals, food, or a person are best predicted by image-only models, while
multimodal models are most effective on art, events, memes, places, or
screenshots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variable Rate Compression for Raw 3D Point Clouds. (arXiv:2202.13862v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13862">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel variable rate deep compression architecture
that operates on raw 3D point cloud data. The majority of learning-based point
cloud compression methods work on a downsampled representation of the data.
Moreover, many existing techniques require training multiple networks for
different compression rates to generate consolidated point clouds of varying
quality. In contrast, our network is capable of explicitly processing point
clouds and generating a compressed description at a comprehensive range of
bitrates. Furthermore, our approach ensures that there is no loss of
information as a result of the voxelization process and the density of the
point cloud does not affect the encoder/decoder performance. An extensive
experimental evaluation shows that our model obtains state-of-the-art results,
it is computationally efficient, and it can work directly with point cloud data
thus avoiding an expensive voxelized representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PASS: Part-Aware Self-Supervised Pre-Training for Person Re-Identification. (arXiv:2203.03931v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03931">
<div class="article-summary-box-inner">
<span><p>In person re-identification (ReID), very recent researches have validated
pre-training the models on unlabelled person images is much better than on
ImageNet. However, these researches directly apply the existing self-supervised
learning (SSL) methods designed for image classification to ReID without any
adaption in the framework. These SSL methods match the outputs of local views
(e.g., red T-shirt, blue shorts) to those of the global views at the same time,
losing lots of details. In this paper, we propose a ReID-specific pre-training
method, Part-Aware Self-Supervised pre-training (PASS), which can generate
part-level features to offer fine-grained information and is more suitable for
ReID. PASS divides the images into several local areas, and the local views
randomly cropped from each area are assigned with a specific learnable [PART]
token. On the other hand, the [PART]s of all local areas are also appended to
the global views. PASS learns to match the output of the local views and global
views on the same [PART]. That is, the learned [PART] of the local views from a
local area is only matched with the corresponding [PART] learned from the
global views. As a result, each [PART] can focus on a specific local area of
the image and extracts fine-grained information of this area. Experiments show
PASS sets the new state-of-the-art performances on Market1501 and MSMT17 on
various ReID tasks, e.g., vanilla ViT-S/16 pre-trained by PASS achieves
92.2\%/90.2\%/88.5\% mAP accuracy on Market1501 for supervised/UDA/USL ReID.
Our codes are available at https://github.com/CASIA-IVA-Lab/PASS-reID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Temporal Consistency for Source-Free Video Domain Adaptation. (arXiv:2203.04559v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04559">
<div class="article-summary-box-inner">
<span><p>Video-based Unsupervised Domain Adaptation (VUDA) methods improve the
robustness of video models, enabling them to be applied to action recognition
tasks across different environments. However, these methods require constant
access to source data during the adaptation process. Yet in many real-world
applications, subjects and scenes in the source video domain should be
irrelevant to those in the target video domain. With the increasing emphasis on
data privacy, such methods that require source data access would raise serious
privacy issues. Therefore, to cope with such concern, a more practical domain
adaptation scenario is formulated as the Source-Free Video-based Domain
Adaptation (SFVDA). Though there are a few methods for Source-Free Domain
Adaptation (SFDA) on image data, these methods yield degenerating performance
in SFVDA due to the multi-modality nature of videos, with the existence of
additional temporal features. In this paper, we propose a novel Attentive
Temporal Consistent Network (ATCoN) to address SFVDA by learning temporal
consistency, guaranteed by two novel consistency objectives, namely feature
consistency and source prediction consistency, performed across local temporal
features. ATCoN further constructs effective overall temporal features by
attending to local temporal features based on prediction confidence. Empirical
results demonstrate the state-of-the-art performance of ATCoN across various
cross-domain action recognition benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ARTEMIS: Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity. (arXiv:2203.08101v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08101">
<div class="article-summary-box-inner">
<span><p>An intuitive way to search for images is to use queries composed of an
example image and a complementary text. While the first provides rich and
implicit context for the search, the latter explicitly calls for new traits, or
specifies how some elements of the example image should be changed to retrieve
the desired target image. Current approaches typically combine the features of
each of the two elements of the query into a single representation, which can
then be compared to the ones of the potential target images. Our work aims at
shedding new light on the task by looking at it through the prism of two
familiar and related frameworks: text-to-image and image-to-image retrieval.
Taking inspiration from them, we exploit the specific relation of each query
element with the targeted image and derive light-weight attention mechanisms
which enable to mediate between the two complementary modalities. We validate
our approach on several retrieval benchmarks, querying with images and their
associated free-form text modifiers. Our method obtains state-of-the-art
results without resorting to side information, multi-level features, heavy
pre-training nor large architectures as in previous works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Segmentation with Active Semi-Supervised Learning. (arXiv:2203.10730v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10730">
<div class="article-summary-box-inner">
<span><p>Using deep learning, we now have the ability to create exceptionally good
semantic segmentation systems; however, collecting the prerequisite pixel-wise
annotations for training images remains expensive and time-consuming.
Therefore, it would be ideal to minimize the number of human annotations needed
when creating a new dataset. Here, we address this problem by proposing a novel
algorithm that combines active learning and semi-supervised learning. Active
learning is an approach for identifying the best unlabeled samples to annotate.
While there has been work on active learning for segmentation, most methods
require annotating all pixel objects in each image, rather than only the most
informative regions. We argue that this is inefficient. Instead, our active
learning approach aims to minimize the number of annotations per-image. Our
method is enriched with semi-supervised learning, where we use pseudo labels
generated with a teacher-student framework to identify image regions that help
disambiguate confused classes. We also integrate mechanisms that enable better
performance on imbalanced label distributions, which have not been studied
previously for active learning in semantic segmentation. In experiments on the
CamVid and CityScapes datasets, our method obtains over 95% of the network's
performance on the full-training set using less than 17% of the training data,
whereas the previous state of the art required 40% of the training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Multi-Object Tracking Using Graph Neural Networks with Cross-Edge Modality Attention. (arXiv:2203.10926v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10926">
<div class="article-summary-box-inner">
<span><p>Online 3D multi-object tracking (MOT) has witnessed significant research
interest in recent years, largely driven by demand from the autonomous systems
community. However, 3D offline MOT is relatively less explored. Labeling 3D
trajectory scene data at a large scale while not relying on high-cost human
experts is still an open research question. In this work, we propose Batch3DMOT
which follows the tracking-by-detection paradigm and represents real-world
scenes as directed, acyclic, and category-disjoint tracking graphs that are
attributed using various modalities such as camera, LiDAR, and radar. We
present a multi-modal graph neural network that uses a cross-edge attention
mechanism mitigating modality intermittence, which translates into sparsity in
the graph domain. Additionally, we present attention-weighted convolutions over
frame-wise k-NN neighborhoods as suitable means to allow information exchange
across disconnected graph components. We evaluate our approach using various
sensor modalities and model configurations on the challenging nuScenes and
KITTI datasets. Extensive experiments demonstrate that our proposed approach
yields an overall improvement of 3.3% in the AMOTA score on nuScenes thereby
setting the new state-of-the-art for 3D tracking and further enhancing false
positive filtering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Probability Sampling Network for Stochastic Human Trajectory Prediction. (arXiv:2203.13471v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13471">
<div class="article-summary-box-inner">
<span><p>Capturing multimodal natures is essential for stochastic pedestrian
trajectory prediction, to infer a finite set of future trajectories. The
inferred trajectories are based on observation paths and the latent vectors of
potential decisions of pedestrians in the inference step. However, stochastic
approaches provide varying results for the same data and parameter settings,
due to the random sampling of the latent vector. In this paper, we analyze the
problem by reconstructing and comparing probabilistic distributions from
prediction samples and socially-acceptable paths, respectively. Through this
analysis, we observe that the inferences of all stochastic models are biased
toward the random sampling, and fail to generate a set of realistic paths from
finite samples. The problem cannot be resolved unless an infinite number of
samples is available, which is infeasible in practice. We introduce that the
Quasi-Monte Carlo (QMC) method, ensuring uniform coverage on the sampling
space, as an alternative to the conventional random sampling. With the same
finite number of samples, the QMC improves all the multimodal prediction
results. We take an additional step ahead by incorporating a learnable sampling
network into the existing networks for trajectory prediction. For this purpose,
we propose the Non-Probability Sampling Network (NPSN), a very small network
(~5K parameters) that generates purposive sample sequences using the past paths
of pedestrians and their social interactions. Extensive experiments confirm
that NPSN can significantly improve both the prediction accuracy (up to 60%)
and reliability of the public pedestrian trajectory prediction benchmark. Code
is publicly available at https://github.com/inhwanbae/NPSN .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalized Prediction of Future Lesion Activity and Treatment Effect in Multiple Sclerosis from Baseline MRI. (arXiv:2204.01702v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01702">
<div class="article-summary-box-inner">
<span><p>Precision medicine for chronic diseases such as multiple sclerosis (MS)
involves choosing a treatment which best balances efficacy and side
effects/preferences for individual patients. Making this choice as early as
possible is important, as delays in finding an effective therapy can lead to
irreversible disability accrual. To this end, we present the first deep neural
network model for individualized treatment decisions from baseline magnetic
resonance imaging (MRI) (with clinical information if available) for MS
patients. Our model (a) predicts future new and enlarging T2 weighted (NE-T2)
lesion counts on follow-up MRI on multiple treatments and (b) estimates the
conditional average treatment effect (CATE), as defined by the predicted future
suppression of NE-T2 lesions, between different treatment options relative to
placebo. Our model is validated on a proprietary federated dataset of 1817
multi-sequence MRIs acquired from MS patients during four multi-centre
randomized clinical trials. Our framework achieves high average precision in
the binarized regression of future NE-T2 lesions on five different treatments,
identifies heterogeneous treatment effects, and provides a personalized
treatment recommendation that accounts for treatment-associated risk (e.g. side
effects, patient preference, administration difficulties).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lightweight HDR Camera ISP for Robust Perception in Dynamic Illumination Conditions via Fourier Adversarial Networks. (arXiv:2204.01795v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01795">
<div class="article-summary-box-inner">
<span><p>The limited dynamic range of commercial compact camera sensors results in an
inaccurate representation of scenes with varying illumination conditions,
adversely affecting image quality and subsequently limiting the performance of
underlying image processing algorithms. Current state-of-the-art (SoTA)
convolutional neural networks (CNN) are developed as post-processing techniques
to independently recover under-/over-exposed images. However, when applied to
images containing real-world degradations such as glare, high-beam, color
bleeding with varying noise intensity, these algorithms amplify the
degradations, further degrading image quality. We propose a lightweight
two-stage image enhancement algorithm sequentially balancing illumination and
noise removal using frequency priors for structural guidance to overcome these
limitations. Furthermore, to ensure realistic image quality, we leverage the
relationship between frequency and spatial domain properties of an image and
propose a Fourier spectrum-based adversarial framework (AFNet) for consistent
image enhancement under varying illumination conditions. While current
formulations of image enhancement are envisioned as post-processing techniques,
we examine if such an algorithm could be extended to integrate the
functionality of the Image Signal Processing (ISP) pipeline within the camera
sensor benefiting from RAW sensor data and lightweight CNN architecture. Based
on quantitative and qualitative evaluations, we also examine the practicality
and effects of image enhancement techniques on the performance of common
perception tasks such as object detection and semantic segmentation in varying
illumination conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of Remote Sensing Pretraining. (arXiv:2204.02825v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02825">
<div class="article-summary-box-inner">
<span><p>Deep learning has largely reshaped remote sensing (RS) research for aerial
image understanding and made a great success. Nevertheless, most of the
existing deep models are initialized with the ImageNet pretrained weights.
Since natural images inevitably present a large domain gap relative to aerial
images, probably limiting the finetuning performance on downstream aerial scene
tasks. This issue motivates us to conduct an empirical study of remote sensing
pretraining (RSP) on aerial images. To this end, we train different networks
from scratch with the help of the largest RS scene recognition dataset up to
now -- MillionAID, to obtain a series of RS pretrained backbones, including
both convolutional neural networks (CNN) and vision transformers such as Swin
and ViTAE, which have shown promising performance on computer vision tasks.
Then, we investigate the impact of RSP on representative downstream tasks
including scene recognition, semantic segmentation, object detection, and
change detection using these CNN and vision transformer backbones. Empirical
study shows that RSP can help deliver distinctive performances in scene
recognition tasks and in perceiving RS related semantics such as "Bridge" and
"Airplane". We also find that, although RSP mitigates the data discrepancies of
traditional ImageNet pretraining on RS images, it may still suffer from task
discrepancies, where downstream tasks require different representations from
scene recognition tasks. These findings call for further research efforts on
both large-scale pretraining datasets and effective pretraining methods. The
codes and pretrained models will be released at
https://github.com/ViTAE-Transformer/ViTAE-Transformer-Remote-Sensing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Vision Transformer Methods for Deep Reinforcement Learning from Pixels. (arXiv:2204.04905v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04905">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViT) have recently demonstrated the significant
potential of transformer architectures for computer vision. To what extent can
image-based deep reinforcement learning also benefit from ViT architectures, as
compared to standard convolutional neural network (CNN) architectures? To
answer this question, we evaluate ViT training methods for image-based
reinforcement learning (RL) control tasks and compare these results to a
leading convolutional-network architecture method, RAD. For training the ViT
encoder, we consider several recently-proposed self-supervised losses that are
treated as auxiliary tasks, as well as a baseline with no additional loss
terms. We find that the CNN architectures trained using RAD still generally
provide superior performance. For the ViT methods, all three types of auxiliary
tasks that we consider provide a benefit over plain ViT training. Furthermore,
ViT reconstruction-based tasks are found to significantly outperform ViT
contrastive-learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ONCE-3DLanes: Building Monocular 3D Lane Detection. (arXiv:2205.00301v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00301">
<div class="article-summary-box-inner">
<span><p>We present ONCE-3DLanes, a real-world autonomous driving dataset with lane
layout annotation in 3D space. Conventional 2D lane detection from a monocular
image yields poor performance of following planning and control tasks in
autonomous driving due to the case of uneven road. Predicting the 3D lane
layout is thus necessary and enables effective and safe driving. However,
existing 3D lane detection datasets are either unpublished or synthesized from
a simulated environment, severely hampering the development of this field. In
this paper, we take steps towards addressing these issues. By exploiting the
explicit relationship between point clouds and image pixels, a dataset
annotation pipeline is designed to automatically generate high-quality 3D lane
locations from 2D lane annotations in 211K road scenes. In addition, we present
an extrinsic-free, anchor-free method, called SALAD, regressing the 3D
coordinates of lanes in image view without converting the feature map into the
bird's-eye view (BEV). To facilitate future research on 3D lane detection, we
benchmark the dataset and provide a novel evaluation metric, performing
extensive experiments of both existing approaches and our proposed method. The
aim of our work is to revive the interest of 3D lane detection in a real-world
scenario. We believe our work can lead to the expected and unexpected
innovations in both academia and industry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compressive Ptychography using Deep Image and Generative Priors. (arXiv:2205.02397v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02397">
<div class="article-summary-box-inner">
<span><p>Ptychography is a well-established coherent diffraction imaging technique
that enables non-invasive imaging of samples at a nanometer scale. It has been
extensively used in various areas such as the defense industry or materials
science. One major limitation of ptychography is the long data acquisition time
due to mechanical scanning of the sample; therefore, approaches to reduce the
scan points are highly desired. However, reconstructions with less number of
scan points lead to imaging artifacts and significant distortions, hindering a
quantitative evaluation of the results. To address this bottleneck, we propose
a generative model combining deep image priors with deep generative priors. The
self-training approach optimizes the deep generative neural network to create a
solution for a given dataset. We complement our approach with a prior acquired
from a previously trained discriminator network to avoid a possible divergence
from the desired output caused by the noise in the measurements. We also
suggest using the total variation as a complementary before combat artifacts
due to measurement noise. We analyze our approach with numerical experiments
through different probe overlap percentages and varying noise levels. We also
demonstrate improved reconstruction accuracy compared to the state-of-the-art
method and discuss the advantages and disadvantages of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building Brains: Subvolume Recombination for Data Augmentation in Large Vessel Occlusion Detection. (arXiv:2205.02848v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02848">
<div class="article-summary-box-inner">
<span><p>Ischemic strokes are often caused by large vessel occlusions (LVOs), which
can be visualized and diagnosed with Computed Tomography Angiography scans. As
time is brain, a fast, accurate and automated diagnosis of these scans is
desirable. Human readers compare the left and right hemispheres in their
assessment of strokes. A large training data set is required for a standard
deep learning-based model to learn this strategy from data. As labeled medical
data in this field is rare, other approaches need to be developed. To both
include the prior knowledge of side comparison and increase the amount of
training data, we propose an augmentation method that generates artificial
training samples by recombining vessel tree segmentations of the hemispheres or
hemisphere subregions from different patients. The subregions cover vessels
commonly affected by LVOs, namely the internal carotid artery (ICA) and middle
cerebral artery (MCA). In line with the augmentation scheme, we use a
3D-DenseNet fed with task-specific input, fostering a side-by-side comparison
between the hemispheres. Furthermore, we propose an extension of that
architecture to process the individual hemisphere subregions. All
configurations predict the presence of an LVO, its side, and the affected
subregion. We show the effect of recombination as an augmentation strategy in a
5-fold cross validated ablation study. We enhanced the AUC for patient-wise
classification regarding the presence of an LVO of all investigated
architectures. For one variant, the proposed method improved the AUC from 0.73
without augmentation to 0.89. The best configuration detects LVOs with an AUC
of 0.91, LVOs in the ICA with an AUC of 0.96, and in the MCA with 0.91 while
accurately predicting the affected side.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HierAttn: Effectively Learn Representations from Stage Attention and Branch Attention for Skin Lesions Diagnosis. (arXiv:2205.04326v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04326">
<div class="article-summary-box-inner">
<span><p>Accurate and unbiased examinations of skin lesions are critical for early
diagnosis and treatment of skin conditions and disorders. Visual features of
skin lesions vary significantly because the skin images are collected from
patients with different skin colours by using dissimilar type of imaging
equipment. Recent studies have reported ensembled convolutional neural networks
(CNNs) to classify the images for early diagnosis of skin disorders. However,
the practical use of CNNs is limited because the majority of networks are
heavyweight and inadequate to use the contextual information. Although
lightweight networks (e.g., MobileNetV3 and EfficientNet) were developed to
save the computational cost for implementing deep neural networks on mobile
devices, not sufficient representation depth restricts their performance. To
address the limitations, we introduce a new light and effective neural network,
namely HierAttn network. The HierAttn applies a novel strategy to balance the
learning local and global features by using a multi-stage attention mechanism
in a hierarchical architecture. The efficacy of HierAttn was evaluated by using
the dermoscopy images dataset ISIC2019 and smartphone photos dataset
PAD-UFES-20. The experimental results show that HierAttn achieves the best
top-1 accuracy and AUC among the state-of-the-art light-weight networks. The
new light HierAttn network has the potential in promoting the use of deep
learning in clinics and allowing patients for early diagnosis of skin disorders
with personal devices. The code is available at
https://github.com/anthonyweidai/HierAttn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Activating More Pixels in Image Super-Resolution Transformer. (arXiv:2205.04437v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04437">
<div class="article-summary-box-inner">
<span><p>Transformer-based methods have shown impressive performance in low-level
vision tasks, such as image super-resolution. However, we find that these
networks can only utilize a limited spatial range of input information through
attribution analysis. This implies that the potential of Transformer is still
not fully exploited in existing networks. In order to activate more input
pixels for reconstruction, we propose a novel Hybrid Attention Transformer
(HAT). It combines channel attention and self-attention schemes, thus making
use of their complementary advantages. Moreover, to better aggregate the
cross-window information, we introduce an overlapping cross-attention module to
enhance the interaction between neighboring window features. In the training
stage, we additionally propose a same-task pre-training strategy to bring
further improvement. Extensive experiments show the effectiveness of the
proposed modules, and the overall method significantly outperforms the
state-of-the-art methods by more than 1dB. Codes and models will be available
at https://github.com/chxy95/HAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reduce Information Loss in Transformers for Pluralistic Image Inpainting. (arXiv:2205.05076v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05076">
<div class="article-summary-box-inner">
<span><p>Transformers have achieved great success in pluralistic image inpainting
recently. However, we find existing transformer based solutions regard each
pixel as a token, thus suffer from information loss issue from two aspects: 1)
They downsample the input image into much lower resolutions for efficiency
consideration, incurring information loss and extra misalignment for the
boundaries of masked regions. 2) They quantize $256^3$ RGB pixels to a small
number (such as 512) of quantized pixels. The indices of quantized pixels are
used as tokens for the inputs and prediction targets of transformer. Although
an extra CNN network is used to upsample and refine the low-resolution results,
it is difficult to retrieve the lost information back.To keep input information
as much as possible, we propose a new transformer based framework "PUT".
Specifically, to avoid input downsampling while maintaining the computation
efficiency, we design a patch-based auto-encoder P-VQVAE, where the encoder
converts the masked image into non-overlapped patch tokens and the decoder
recovers the masked regions from inpainted tokens while keeping the unmasked
regions unchanged. To eliminate the information loss caused by quantization, an
Un-Quantized Transformer (UQ-Transformer) is applied, which directly takes the
features from P-VQVAE encoder as input without quantization and regards the
quantized tokens only as prediction targets. Extensive experiments show that
PUT greatly outperforms state-of-the-art methods on image fidelity, especially
for large masked regions and complex large-scale datasets. Code is available at
https://github.com/liuqk3/PUT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Continual Deepfake Detection Benchmark: Dataset, Methods, and Essentials. (arXiv:2205.05467v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05467">
<div class="article-summary-box-inner">
<span><p>There have been emerging a number of benchmarks and techniques for the
detection of deepfakes. However, very few works study the detection of
incrementally appearing deepfakes in the real-world scenarios. To simulate the
wild scenes, this paper suggests a continual deepfake detection benchmark
(CDDB) over a new collection of deepfakes from both known and unknown
generative models. The suggested CDDB designs multiple evaluations on the
detection over easy, hard, and long sequence of deepfake tasks, with a set of
appropriate measures. In addition, we exploit multiple approaches to adapt
multiclass incremental learning methods, commonly used in the continual visual
recognition, to the continual deepfake detection problem. We evaluate several
methods, including the adapted ones, on the proposed CDDB. Within the proposed
benchmark, we explore some commonly known essentials of standard continual
learning. Our study provides new insights on these essentials in the context of
continual deepfake detection. The suggested CDDB is clearly more challenging
than the existing benchmarks, which thus offers a suitable evaluation avenue to
the future research. Our benchmark dataset and the source code will be made
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Teaching Independent Parts Separately"(TIPSy-GAN) : Improving Accuracy and Stability in Unsupervised Adversarial 2D to 3D Human Pose Estimation. (arXiv:2205.05980v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05980">
<div class="article-summary-box-inner">
<span><p>We present TIPSy-GAN, a new approach to improve the accuracy and stability in
unsupervised adversarial 2D to 3D human pose estimation. In our work we
demonstrate that the human kinematic skeleton should not be assumed as one
spatially codependent structure. In fact, we believe when a full 2D pose is
provided during training, there is an inherent bias learned where the 3D
coordinate of a keypoint is spatially codependent on the 2D locations of all
other keypoints. To investigate our theory we follow previous adversarial
approaches but train two generators on spatially independent parts of the
kinematic skeleton, the torso and the legs. We find that improving the 2D
reprojection self-consistency cycle is key to lowering the evaluation error and
therefore introduce new consistency constraints during training. A TIPSy is
produced model via knowledge distillation from these generators which can
predict the 3D coordinates for the entire 2D pose with improved results.
Furthermore, we address the question left unanswered in prior work detailing
how long to train for a truly unsupervised scenario. We show that two
independent generators training adversarially has improved stability than that
of a solo generator which will collapse due to the adversarial network becoming
unstable. TIPSy decreases the average error by 18% when compared to that of a
baseline solo generator. TIPSy improves upon other unsupervised approaches
while also performing strongly against supervised and weakly-supervised
approaches during evaluation on both the Human3.6M and MPI-INF-3DHP dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELODI: Ensemble Logit Difference Inhibition for Positive-Congruent Training. (arXiv:2205.06265v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06265">
<div class="article-summary-box-inner">
<span><p>Negative flips are errors introduced in a classification system when a legacy
model is replaced with a new one. Existing methods to reduce the negative flip
rate (NFR) either do so at the expense of overall accuracy using model
distillation, or use ensembles, which multiply inference cost prohibitively. We
present a method to train a classification system that achieves paragon
performance in both error rate and NFR, at the inference cost of a single
model. Our method introduces a generalized distillation objective, Logit
Difference Inhibition (LDI), that penalizes changes in the logits between the
new and old model, without forcing them to coincide as in ordinary
distillation. LDI affords the model flexibility to reduce error rate along with
NFR. The method uses a homogeneous ensemble as the reference model for LDI,
hence the name Ensemble LDI, or ELODI. The reference model can then be
substituted with a single model at inference time. The method leverages the
observation that negative flips are typically not close to the decision
boundary, but often exhibit large deviations in the distance among their
logits, which are reduced by ELODI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monocular Human Digitization via Implicit Re-projection Networks. (arXiv:2205.06468v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06468">
<div class="article-summary-box-inner">
<span><p>We present an approach to generating 3D human models from images. The key to
our framework is that we predict double-sided orthographic depth maps and color
images from a single perspective projected image. Our framework consists of
three networks. The first network predicts normal maps to recover geometric
details such as wrinkles in the clothes and facial regions. The second network
predicts shade-removed images for the front and back views by utilizing the
predicted normal maps. The last multi-headed network takes both normal maps and
shade-free images and predicts depth maps while selectively fusing photometric
and geometric information through multi-headed attention gates. Experimental
results demonstrate that our method shows visually plausible results and
competitive performance in terms of various evaluation metrics over
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-05-17 23:08:31.710334787 UTC">2022-05-17 23:08:31 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>