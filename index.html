<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-23T01:30:00Z">09-23</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Grammatical Profiling for Semantic Change Detection. (arXiv:2109.10397v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10397">
<div class="article-summary-box-inner">
<span><p>Semantics, morphology and syntax are strongly interdependent. However, the
majority of computational methods for semantic change detection use
distributional word representations which encode mostly semantics. We
investigate an alternative method, grammatical profiling, based entirely on
changes in the morphosyntactic behaviour of words. We demonstrate that it can
be used for semantic change detection and even outperforms some distributional
semantic methods. We present an in-depth qualitative and quantitative analysis
of the predictions made by our grammatical profiling system, showing that they
are plausible and interpretable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RETRONLU: Retrieval Augmented Task-Oriented Semantic Parsing. (arXiv:2109.10410v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10410">
<div class="article-summary-box-inner">
<span><p>While large pre-trained language models accumulate a lot of knowledge in
their parameters, it has been demonstrated that augmenting it with
non-parametric retrieval-based memory has a number of benefits from accuracy
improvements to data efficiency for knowledge-focused tasks, such as question
answering. In this paper, we are applying retrieval-based modeling ideas to the
problem of multi-domain task-oriented semantic parsing for conversational
assistants. Our approach, RetroNLU, extends a sequence-to-sequence model
architecture with a retrieval component, used to fetch existing similar
examples and provide them as an additional input to the model. In particular,
we analyze two settings, where we augment an input with (a) retrieved nearest
neighbor utterances (utterance-nn), and (b) ground-truth semantic parses of
nearest neighbor utterances (semparse-nn). Our technique outperforms the
baseline method by 1.5% absolute macro-F1, especially at the low resource
setting, matching the baseline model accuracy with only 40% of the data.
Furthermore, we analyze the nearest neighbor retrieval component's quality,
model sensitivity and break down the performance for semantic parses of
different utterance complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Would it Take to get Biomedical QA Systems into Practice?. (arXiv:2109.10415v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10415">
<div class="article-summary-box-inner">
<span><p>Medical question answering (QA) systems have the potential to answer
clinicians uncertainties about treatment and diagnosis on demand, informed by
the latest evidence. However, despite the significant progress in general QA
made by the NLP community, medical QA systems are still not widely used in
clinical environments. One likely reason for this is that clinicians may not
readily trust QA system outputs, in part because transparency, trustworthiness,
and provenance have not been key considerations in the design of such models.
In this paper we discuss a set of criteria that, if met, we argue would likely
increase the utility of biomedical QA systems, which may in turn lead to
adoption of such systems in practice. We assess existing models, tasks, and
datasets with respect to these criteria, highlighting shortcomings of
previously proposed approaches and pointing toward what might be more usable QA
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Debiasing Techniques for Intersectional Biases. (arXiv:2109.10441v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10441">
<div class="article-summary-box-inner">
<span><p>Bias is pervasive in NLP models, motivating the development of automatic
debiasing techniques. Evaluation of NLP debiasing methods has largely been
limited to binary attributes in isolation, e.g., debiasing with respect to
binary gender or race, however many corpora involve multiple such attributes,
possibly with higher cardinality. In this paper we argue that a truly fair
model must consider `gerrymandering' groups which comprise not only single
attributes, but also intersectional groups. We evaluate a form of
bias-constrained model which is new to NLP, as well an extension of the
iterative nullspace projection technique which can handle multiple protected
attributes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fairness-aware Class Imbalanced Learning. (arXiv:2109.10444v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10444">
<div class="article-summary-box-inner">
<span><p>Class imbalance is a common challenge in many NLP tasks, and has clear
connections to bias, in that bias in training data often leads to higher
accuracy for majority groups at the expense of minority groups. However there
has traditionally been a disconnect between research on class-imbalanced
learning and mitigating bias, and only recently have the two been looked at
through a common lens. In this work we evaluate long-tail learning methods for
tweet sentiment and occupation classification, and extend a margin-loss based
approach with methods to enforce fairness. We empirically show through
controlled experiments that the proposed approaches help mitigate both class
imbalance and demographic biases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Fine-Grained Knowledge Graphs of Scientific Claims: Dataset and Transformer-Based Results. (arXiv:2109.10453v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10453">
<div class="article-summary-box-inner">
<span><p>Recent transformer-based approaches demonstrate promising results on
relational scientific information extraction. Existing datasets focus on
high-level description of how research is carried out. Instead we focus on the
subtleties of how experimental associations are presented by building SciClaim,
a dataset of scientific claims drawn from Social and Behavior Science (SBS),
PubMed, and CORD-19 papers. Our novel graph annotation schema incorporates not
only coarse-grained entity spans as nodes and relations as edges between them,
but also fine-grained attributes that modify entities and their relations, for
a total of 12,738 labels in the corpus. By including more label types and more
than twice the label density of previous datasets, SciClaim captures causal,
comparative, predictive, statistical, and proportional associations over
experimental variables along with their qualifications, subtypes, and evidence.
We extend work in transformer-based joint entity and relation extraction to
effectively infer our schema, showing the promise of fine-grained knowledge
graphs in scientific claims and beyond.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scalable and Efficient MoE Training for Multitask Multilingual Models. (arXiv:2109.10465v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10465">
<div class="article-summary-box-inner">
<span><p>The Mixture of Experts (MoE) models are an emerging class of sparsely
activated deep learning models that have sublinear compute costs with respect
to their parameters. In contrast with dense models, the sparse architecture of
MoE offers opportunities for drastically growing model size with significant
accuracy gain while consuming much lower compute budget. However, supporting
large scale MoE training also has its own set of system and modeling
challenges. To overcome the challenges and embrace the opportunities of MoE, we
first develop a system capable of scaling MoE models efficiently to trillions
of parameters. It combines multi-dimensional parallelism and heterogeneous
memory technologies harmoniously with MoE to empower 8x larger models on the
same hardware compared with existing work. Besides boosting system efficiency,
we also present new training methods to improve MoE sample efficiency and
leverage expert pruning strategy to improve inference time efficiency. By
combining the efficient system and training methods, we are able to
significantly scale up large multitask multilingual models for language
generation which results in a great improvement in model accuracy. A model
trained with 10 billion parameters on 50 languages can achieve state-of-the-art
performance in Machine Translation (MT) and multilingual natural language
generation tasks. The system support of efficient MoE training has been
implemented and open-sourced with the DeepSpeed library.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salience-Aware Event Chain Modeling for Narrative Understanding. (arXiv:2109.10475v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10475">
<div class="article-summary-box-inner">
<span><p>Storytelling, whether via fables, news reports, documentaries, or memoirs,
can be thought of as the communication of interesting and related events that,
taken together, form a concrete process. It is desirable to extract the event
chains that represent such processes. However, this extraction remains a
challenging problem. We posit that this is due to the nature of the texts from
which chains are discovered. Natural language text interleaves a narrative of
concrete, salient events with background information, contextualization,
opinion, and other elements that are important for a variety of necessary
discourse and pragmatics acts but are not part of the principal chain of events
being communicated. We introduce methods for extracting this principal chain
from natural language text, by filtering away non-salient events and supportive
sentences. We demonstrate the effectiveness of our methods at isolating
critical event chains by comparing their effect on downstream tasks. We show
that by pre-training large language models on our extracted chains, we obtain
improvements in two tasks that benefit from a clear understanding of event
chains: narrative prediction and event-based temporal question answering. The
demonstrated improvements and ablative studies confirm that our extraction
method isolates critical event chains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialogueBERT: A Self-Supervised Learning based Dialogue Pre-training Encoder. (arXiv:2109.10480v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10480">
<div class="article-summary-box-inner">
<span><p>With the rapid development of artificial intelligence, conversational bots
have became prevalent in mainstream E-commerce platforms, which can provide
convenient customer service timely. To satisfy the user, the conversational
bots need to understand the user's intention, detect the user's emotion, and
extract the key entities from the conversational utterances. However,
understanding dialogues is regarded as a very challenging task. Different from
common language understanding, utterances in dialogues appear alternately from
different roles and are usually organized as hierarchical structures. To
facilitate the understanding of dialogues, in this paper, we propose a novel
contextual dialogue encoder (i.e. DialogueBERT) based on the popular
pre-trained language model BERT. Five self-supervised learning pre-training
tasks are devised for learning the particularity of dialouge utterances. Four
different input embeddings are integrated to catch the relationship between
utterances, including turn embedding, role embedding, token embedding and
position embedding. DialogueBERT was pre-trained with 70 million dialogues in
real scenario, and then fine-tuned in three different downstream dialogue
understanding tasks. Experimental results show that DialogueBERT achieves
exciting results with 88.63% accuracy for intent recognition, 94.25% accuracy
for emotion recognition and 97.04% F1 score for named entity recognition, which
outperforms several strong baselines by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The NiuTrans Machine Translation Systems for WMT21. (arXiv:2109.10485v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10485">
<div class="article-summary-box-inner">
<span><p>This paper describes NiuTrans neural machine translation systems of the WMT
2021 news translation tasks. We made submissions to 9 language directions,
including English$\leftrightarrow$$\{$Chinese, Japanese, Russian, Icelandic$\}$
and English$\rightarrow$Hausa tasks. Our primary systems are built on several
effective variants of Transformer, e.g., Transformer-DLCL, ODE-Transformer. We
also utilize back-translation, knowledge distillation, post-ensemble, and
iterative fine-tuning techniques to enhance the model performance further.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Approach to Jointly Rank Passages and Select Relevant Sentences in the OBQA Context. (arXiv:2109.10497v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10497">
<div class="article-summary-box-inner">
<span><p>In the open question answering (OBQA) task, how to select the relevant
information from a large corpus is a crucial problem for reasoning and
inference. Some datasets (e.g, HotpotQA) mainly focus on testing the model's
reasoning ability at the sentence level. To overcome this challenge, many
existing frameworks use a deep learning model to select relevant passages and
then answer each question by matching a sentence in the corresponding passage.
However, such frameworks require long inference time and fail to take advantage
of the relationship between passages and sentences. In this work, we present a
simple yet effective framework to address these problems by jointly ranking
passages and selecting sentences. We propose consistency and similarity
constraints to promote the correlation and interaction between passage ranking
and sentence selection. In our experiments, we demonstrate that our framework
can achieve competitive results and outperform the baseline by 28\% in terms of
exact matching of relevant sentences on the HotpotQA dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperExpan: Taxonomy Expansion with Hyperbolic Representation Learning. (arXiv:2109.10500v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10500">
<div class="article-summary-box-inner">
<span><p>Taxonomies are valuable resources for many applications, but the limited
coverage due to the expensive manual curation process hinders their general
applicability. Prior works attempt to automatically expand existing taxonomies
to improve their coverage by learning concept embeddings in Euclidean space,
while taxonomies, inherently hierarchical, more naturally align with the
geometric properties of a hyperbolic space. In this paper, we present
HyperExpan, a taxonomy expansion algorithm that seeks to preserve the structure
of a taxonomy in a more expressive hyperbolic embedding space and learn to
represent concepts and their relations with a Hyperbolic Graph Neural Network
(HGNN). Specifically, HyperExpan leverages position embeddings to exploit the
structure of the existing taxonomies, and characterizes the concept profile
information to support the inference on unseen concepts during training.
Experiments show that our proposed HyperExpan outperforms baseline models with
representation learning in a Euclidean feature space and achieves
state-of-the-art performance on the taxonomy expansion benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tecnologica cosa: Modeling Storyteller Personalities in Boccaccio's Decameron. (arXiv:2109.10506v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10506">
<div class="article-summary-box-inner">
<span><p>We explore Boccaccio's Decameron to see how digital humanities tools can be
used for tasks that have limited data in a language no longer in contemporary
use: medieval Italian. We focus our analysis on the question: Do the different
storytellers in the text exhibit distinct personalities? To answer this
question, we curate and release a dataset based on the authoritative edition of
the text. We use supervised classification methods to predict storytellers
based on the stories they tell, confirming the difficulty of the task, and
demonstrate that topic modeling can extract thematic storyteller "profiles."
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Contextualized Document Representation. (arXiv:2109.10509v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10509">
<div class="article-summary-box-inner">
<span><p>Several NLP tasks need the effective representation of text documents. Arora
et. al., 2017 demonstrate that simple weighted averaging of word vectors
frequently outperforms neural models. SCDV (Mekala et. al., 2017) further
extends this from sentences to documents by employing soft and sparse
clustering over pre-computed word vectors. However, both techniques ignore the
polysemy and contextual character of words. In this paper, we address this
issue by proposing SCDV+BERT(ctxd), a simple and effective unsupervised
representation that combines contextualized BERT (Devlin et al., 2019) based
word embedding for word sense disambiguation with SCDV soft clustering
approach. We show that our embeddings outperform original SCDV, pre-train BERT,
and several other baselines on many classification datasets. We also
demonstrate our embeddings effectiveness on other tasks, such as concept
matching and sentence similarity. In addition, we show that SCDV+BERT(ctxd)
outperforms fine-tune BERT and different embedding approaches in scenarios with
limited data and only few shots examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FCM: A Fine-grained Comparison Model forMulti-turn Dialogue Reasoning. (arXiv:2109.10510v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10510">
<div class="article-summary-box-inner">
<span><p>Despite the success of neural dialogue systems in achieving high performance
on the leader-board, they cannot meet users' requirements in practice, due to
their poor reasoning skills. The underlying reason is that most neural dialogue
models only capture the syntactic and semantic information, but fail to model
the logical consistency between the dialogue history and the generated
response. Recently, a new multi-turn dialogue reasoning task has been proposed,
to facilitate dialogue reasoning research. However, this task is challenging,
because there are only slight differences between the illogical response and
the dialogue history. How to effectively solve this challenge is still worth
exploring. This paper proposes a Fine-grained Comparison Model (FCM) to tackle
this problem. Inspired by human's behavior in reading comprehension, a
comparison mechanism is proposed to focus on the fine-grained differences in
the representation of each response candidate. Specifically, each candidate
representation is compared with the whole history to obtain a history
consistency representation. Furthermore, the consistency signals between each
candidate and the speaker's own history are considered to drive a model to
prefer a candidate that is logically consistent with the speaker's history
logic. Finally, the above consistency representations are employed to output a
ranking list of the candidate responses for multi-turn dialogue reasoning.
Experimental results on two public dialogue datasets show that our method
obtains higher ranking scores than the baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards The Automatic Coding of Medical Transcripts to Improve Patient-Centered Communication. (arXiv:2109.10514v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10514">
<div class="article-summary-box-inner">
<span><p>This paper aims to provide an approach for automatic coding of
physician-patient communication transcripts to improve patient-centered
communication (PCC). PCC is a central part of high-quality health care. To
improve PCC, dialogues between physicians and patients have been recorded and
tagged with predefined codes. Trained human coders have manually coded the
transcripts. Since it entails huge labor costs and poses possible human errors,
automatic coding methods should be considered for efficiency and effectiveness.
We adopted three machine learning algorithms (Na\"ive Bayes, Random Forest, and
Support Vector Machine) to categorize lines in transcripts into corresponding
codes. The result showed that there is evidence to distinguish the codes, and
this is considered to be sufficient for training of human annotators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Role of Language Relatedness in Multilingual Fine-tuning of Language Models: A Case Study in Indo-Aryan Languages. (arXiv:2109.10534v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10534">
<div class="article-summary-box-inner">
<span><p>We explore the impact of leveraging the relatedness of languages that belong
to the same family in NLP models using multilingual fine-tuning. We hypothesize
and validate that multilingual fine-tuning of pre-trained language models can
yield better performance on downstream NLP applications, compared to models
fine-tuned on individual languages. A first of its kind detailed study is
presented to track performance change as languages are added to a base language
in a graded and greedy (in the sense of best boost of performance) manner;
which reveals that careful selection of subset of related languages can
significantly improve performance than utilizing all related languages. The
Indo-Aryan (IA) language family is chosen for the study, the exact languages
being Bengali, Gujarati, Hindi, Marathi, Oriya, Punjabi and Urdu. The script
barrier is crossed by simple rule-based transliteration of the text of all
languages to Devanagari. Experiments are performed on mBERT, IndicBERT, MuRIL
and two RoBERTa-based LMs, the last two being pre-trained by us. Low resource
languages, such as Oriya and Punjabi, are found to be the largest beneficiaries
of multilingual fine-tuning. Textual Entailment, Entity Classification, Section
Title Prediction, tasks of IndicGLUE and POS tagging form our test bed.
Compared to monolingual fine tuning we get relative performance improvement of
up to 150% in the downstream tasks. The surprise take-away is that for any
language there is a particular combination of other languages which yields the
best performance, and any additional language is in fact detrimental.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing. (arXiv:2109.10540v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10540">
<div class="article-summary-box-inner">
<span><p>Recent years pretrained language models (PLMs) hit a success on several
downstream tasks, showing their power on modeling language. To better
understand and leverage what PLMs have learned, several techniques have emerged
to explore syntactic structures entailed by PLMs. However, few efforts have
been made to explore grounding capabilities of PLMs, which are also essential.
In this paper, we highlight the ability of PLMs to discover which token should
be grounded to which concept, if combined with our proposed
erasing-then-awakening approach. Empirical studies on four datasets demonstrate
that our approach can awaken latent grounding which is understandable to human
experts, even if it is not exposed to such labels during training. More
importantly, our approach shows great potential to benefit downstream semantic
parsing models. Taking text-to-SQL as a case study, we successfully couple our
approach with two off-the-shelf parsers, obtaining an absolute improvement of
up to 9.8%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diarisation using Location tracking with agglomerative clustering. (arXiv:2109.10598v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10598">
<div class="article-summary-box-inner">
<span><p>Previous works have shown that spatial location information can be
complementary to speaker embeddings for a speaker diarisation task. However,
the models used often assume that speakers are fairly stationary throughout a
meeting. This paper proposes to relax this assumption, by explicitly modelling
the movements of speakers within an Agglomerative Hierarchical Clustering (AHC)
diarisation framework. Kalman filters, which track the locations of speakers,
are used to compute log-likelihood ratios that contribute to the cluster
affinity computations for the AHC merging and stopping decisions. Experiments
show that the proposed approach is able to yield improvements on a Microsoft
rich meeting transcription task, compared to methods that do not use location
information or that make stationarity assumptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset. (arXiv:2109.10604v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10604">
<div class="article-summary-box-inner">
<span><p>While diverse question answering (QA) datasets have been proposed and
contributed significantly to the development of deep learning models for QA
tasks, the existing datasets fall short in two aspects. First, we lack QA
datasets covering complex questions that involve answers as well as the
reasoning processes to get the answers. As a result, the state-of-the-art QA
research on numerical reasoning still focuses on simple calculations and does
not provide the mathematical expressions or evidences justifying the answers.
Second, the QA community has contributed much effort to improving the
interpretability of QA models. However, these models fail to explicitly show
the reasoning process, such as the evidence order for reasoning and the
interactions between different pieces of evidence. To address the above
shortcomings, we introduce NOAHQA, a conversational and bilingual QA dataset
with questions requiring numerical reasoning with compound mathematical
expressions. With NOAHQA, we develop an interpretable reasoning graph as well
as the appropriate evaluation metric to measure the answer quality. We evaluate
the state-of-the-art QA models trained using existing QA datasets on NOAHQA and
show that the best among them can only achieve 55.5 exact match scores, while
the human performance is 89.7. We also present a new QA model for generating a
reasoning graph where the reasoning graph metric still has a large gap compared
with that of humans, e.g., 28 scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVR: A test-bed for Visually Grounded Compositional Generalization with real images. (arXiv:2109.10613v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10613">
<div class="article-summary-box-inner">
<span><p>While interest in models that generalize at test time to new compositions has
risen in recent years, benchmarks in the visually-grounded domain have thus far
been restricted to synthetic images. In this work, we propose COVR, a new
test-bed for visually-grounded compositional generalization with real images.
To create COVR, we use real images annotated with scene graphs, and propose an
almost fully automatic procedure for generating question-answer pairs along
with a set of context images. COVR focuses on questions that require complex
reasoning, including higher-order operations such as quantification and
aggregation. Due to the automatic generation process, COVR facilitates the
creation of compositional splits, where models at test time need to generalize
to new concepts and compositions in a zero- or few-shot setting. We construct
compositional splits using COVR and demonstrate a myriad of cases where
state-of-the-art pre-trained language-and-vision models struggle to
compositionally generalize.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enriching and Controlling Global Semantics for Text Summarization. (arXiv:2109.10616v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10616">
<div class="article-summary-box-inner">
<span><p>Recently, Transformer-based models have been proven effective in the
abstractive summarization task by creating fluent and informative summaries.
Nevertheless, these models still suffer from the short-range dependency
problem, causing them to produce summaries that miss the key points of
document. In this paper, we attempt to address this issue by introducing a
neural topic model empowered with normalizing flow to capture the global
semantics of the document, which are then integrated into the summarization
model. In addition, to avoid the overwhelming effect of global semantics on
contextualized representation, we introduce a mechanism to control the amount
of global semantics supplied to the text generation module. Our method
outperforms state-of-the-art summarization models on five common text
summarization datasets, namely CNN/DailyMail, XSum, Reddit TIFU, arXiv, and
PubMed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning for Fair Representations. (arXiv:2109.10645v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10645">
<div class="article-summary-box-inner">
<span><p>Trained classification models can unintentionally lead to biased
representations and predictions, which can reinforce societal preconceptions
and stereotypes. Existing debiasing methods for classification models, such as
adversarial training, are often expensive to train and difficult to optimise.
In this paper, we propose a method for mitigating bias in classifier training
by incorporating contrastive learning, in which instances sharing the same
class label are encouraged to have similar representations, while instances
sharing a protected attribute are forced further apart. In such a way our
method learns representations which capture the task label in focused regions,
while ensuring the protected attribute has diverse spread, and thus has limited
impact on prediction and thereby results in fairer models. Extensive
experimental results across four tasks in NLP and computer vision show (a) that
our proposed method can achieve fairer representations and realises bias
reductions compared with competitive baselines; and (b) that it can do so
without sacrificing main task performance; (c) that it sets a new
state-of-the-art performance in one task despite reducing the bias. Finally,
our method is conceptually simple and agnostic to network architectures, and
incurs minimal additional compute cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MiRANews: Dataset and Benchmarks for Multi-Resource-Assisted News Summarization. (arXiv:2109.10650v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10650">
<div class="article-summary-box-inner">
<span><p>One of the most challenging aspects of current single-document news
summarization is that the summary often contains 'extrinsic hallucinations',
i.e., facts that are not present in the source document, which are often
derived via world knowledge. This causes summarization systems to act more like
open-ended language models tending to hallucinate facts that are erroneous. In
this paper, we mitigate this problem with the help of multiple supplementary
resource documents assisting the task. We present a new dataset MiRANews and
benchmark existing summarization models. In contrast to multi-document
summarization, which addresses multiple events from several source documents,
we still aim at generating a summary for a single document. We show via data
analysis that it's not only the models which are to blame: more than 27% of
facts mentioned in the gold summaries of MiRANews are better grounded on
assisting documents than in the main source articles. An error analysis of
generated summaries from pretrained models fine-tuned on MiRANews reveals that
this has an even bigger effects on models: assisted summarization reduces 55%
of hallucinations when compared to single-document summarization models trained
on the main article only. Our code and data are available at
https://github.com/XinnuoXu/MiRANews.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers. (arXiv:2109.10686v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10686">
<div class="article-summary-box-inner">
<span><p>There remain many open questions pertaining to the scaling behaviour of
Transformer architectures. These scaling decisions and findings can be
critical, as training runs often come with an associated computational cost
which have both financial and/or environmental impact. The goal of this paper
is to present scaling insights from pretraining and finetuning Transformers.
While Kaplan et al. presents a comprehensive study of the scaling behaviour of
Transformer language models, the scope is only on the upstream (pretraining)
loss. Therefore, it is still unclear if these set of findings transfer to
downstream task within the context of the pretrain-finetune paradigm. The key
findings of this paper are as follows: (1) we show that aside from only the
model size, model shape matters for downstream fine-tuning, (2) scaling
protocols operate differently at different compute regions, (3) widely adopted
T5-base and T5-large sizes are Pareto-inefficient. To this end, we present
improved scaling protocols whereby our redesigned models achieve similar
downstream fine-tuning quality while having 50\% fewer parameters and training
40\% faster compared to the widely adopted T5-base model. We publicly release
over 100 pretrained checkpoints of different T5 configurations to facilitate
future research and analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simulated Annealing for Emotional Dialogue Systems. (arXiv:2109.10715v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10715">
<div class="article-summary-box-inner">
<span><p>Explicitly modeling emotions in dialogue generation has important
applications, such as building empathetic personal companions. In this study,
we consider the task of expressing a specific emotion for dialogue generation.
Previous approaches take the emotion as an input signal, which may be ignored
during inference. We instead propose a search-based emotional dialogue system
by simulated annealing (SA). Specifically, we first define a scoring function
that combines contextual coherence and emotional correctness. Then, SA
iteratively edits a general response and searches for a sentence with a higher
score, enforcing the presence of the desired emotion. We evaluate our system on
the NLPCC2017 dataset. Our proposed method shows 12% improvements in emotion
accuracy compared with the previous state-of-the-art method, without hurting
the generation quality (measured by BLEU).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-Latency Incremental Text-to-Speech Synthesis with Distilled Context Prediction Network. (arXiv:2109.10724v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10724">
<div class="article-summary-box-inner">
<span><p>Incremental text-to-speech (TTS) synthesis generates utterances in small
linguistic units for the sake of real-time and low-latency applications. We
previously proposed an incremental TTS method that leverages a large
pre-trained language model to take unobserved future context into account
without waiting for the subsequent segment. Although this method achieves
comparable speech quality to that of a method that waits for the future
context, it entails a huge amount of processing for sampling from the language
model at each time step. In this paper, we propose an incremental TTS method
that directly predicts the unobserved future context with a lightweight model,
instead of sampling words from the large-scale language model. We perform
knowledge distillation from a GPT2-based context prediction network into a
simple recurrent model by minimizing a teacher-student loss defined between the
context embedding vectors of those models. Experimental results show that the
proposed method requires about ten times less inference time to achieve
comparable synthetic speech quality to that of our previous method, and it can
perform incremental synthesis much faster than the average speaking speed of
human English speakers, demonstrating the availability of our method to
real-time applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Small-Bench NLP: Benchmark for small single GPU trained models in Natural Language Processing. (arXiv:2109.10847v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10847">
<div class="article-summary-box-inner">
<span><p>Recent progress in the Natural Language Processing domain has given us
several State-of-the-Art (SOTA) pretrained models which can be finetuned for
specific tasks. These large models with billions of parameters trained on
numerous GPUs/TPUs over weeks are leading in the benchmark leaderboards. In
this paper, we discuss the need for a benchmark for cost and time effective
smaller models trained on a single GPU. This will enable researchers with
resource constraints experiment with novel and innovative ideas on
tokenization, pretraining tasks, architecture, fine tuning methods etc. We set
up Small-Bench NLP, a benchmark for small efficient neural language models
trained on a single GPU. Small-Bench NLP benchmark comprises of eight NLP tasks
on the publicly available GLUE datasets and a leaderboard to track the progress
of the community. Our ELECTRA-DeBERTa (15M parameters) small model architecture
achieves an average score of 81.53 which is comparable to that of BERT-Base's
82.20 (110M parameters). Our models, code and leaderboard are available at
https://github.com/smallbenchnlp
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pix2seq: A Language Modeling Framework for Object Detection. (arXiv:2109.10852v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10852">
<div class="article-summary-box-inner">
<span><p>This paper presents Pix2Seq, a simple and generic framework for object
detection. Unlike existing approaches that explicitly integrate prior knowledge
about the task, we simply cast object detection as a language modeling task
conditioned on the observed pixel inputs. Object descriptions (e.g., bounding
boxes and class labels) are expressed as sequences of discrete tokens, and we
train a neural net to perceive the image and generate the desired sequence. Our
approach is based mainly on the intuition that if a neural net knows about
where and what the objects are, we just need to teach it how to read them out.
Beyond the use of task-specific data augmentations, our approach makes minimal
assumptions about the task, yet it achieves competitive results on the
challenging COCO dataset, compared to highly specialized and well optimized
detection algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BFClass: A Backdoor-free Text Classification Framework. (arXiv:2109.10855v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10855">
<div class="article-summary-box-inner">
<span><p>Backdoor attack introduces artificial vulnerabilities into the model by
poisoning a subset of the training data via injecting triggers and modifying
labels. Various trigger design strategies have been explored to attack text
classifiers, however, defending such attacks remains an open problem. In this
work, we propose BFClass, a novel efficient backdoor-free training framework
for text classification. The backbone of BFClass is a pre-trained discriminator
that predicts whether each token in the corrupted input was replaced by a
masked language model. To identify triggers, we utilize this discriminator to
locate the most suspicious token from each training sample and then distill a
concise set by considering their association strengths with particular labels.
To recognize the poisoned subset, we examine the training samples with these
identified triggers as the most suspicious token, and check if removing the
trigger will change the poisoned model's prediction. Extensive experiments
demonstrate that BFClass can identify all the triggers, remove 95% poisoned
training samples with very limited false alarms, and achieve almost the same
performance as the models trained on the benign training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse2Fine: Fine-grained Text Classification on Coarsely-grained Annotated Data. (arXiv:2109.10856v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10856">
<div class="article-summary-box-inner">
<span><p>Existing text classification methods mainly focus on a fixed label set,
whereas many real-world applications require extending to new fine-grained
classes as the number of samples per label increases. To accommodate such
requirements, we introduce a new problem called coarse-to-fine grained
classification, which aims to perform fine-grained classification on coarsely
annotated data. Instead of asking for new fine-grained human annotations, we
opt to leverage label surface names as the only human guidance and weave in
rich pre-trained generative language models into the iterative weak supervision
strategy. Specifically, we first propose a label-conditioned finetuning
formulation to attune these generators for our task. Furthermore, we devise a
regularization objective based on the coarse-fine label constraints derived
from our problem setting, giving us even further improvements over the prior
formulation. Our framework uses the fine-tuned generative models to sample
pseudo-training data for training the classifier, and bootstraps on real
unlabeled data for model refinement. Extensive experiments and case studies on
two real-world datasets demonstrate superior performance over SOTA zero-shot
classification baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pushing the Right Buttons: Adversarial Evaluation of Quality Estimation. (arXiv:2109.10859v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10859">
<div class="article-summary-box-inner">
<span><p>Current Machine Translation (MT) systems achieve very good results on a
growing variety of language pairs and datasets. However, they are known to
produce fluent translation outputs that can contain important meaning errors,
thus undermining their reliability in practice. Quality Estimation (QE) is the
task of automatically assessing the performance of MT systems at test time.
Thus, in order to be useful, QE systems should be able to detect such errors.
However, this ability is yet to be tested in the current evaluation practices,
where QE systems are assessed only in terms of their correlation with human
judgements. In this work, we bridge this gap by proposing a general methodology
for adversarial testing of QE for MT. First, we show that despite a high
correlation with human judgements achieved by the recent SOTA, certain types of
meaning errors are still problematic for QE to detect. Second, we show that on
average, the ability of a given model to discriminate between
meaning-preserving and meaning-altering perturbations is predictive of its
overall performance, thus potentially allowing for comparing QE systems without
relying on manual quality annotation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recursively Summarizing Books with Human Feedback. (arXiv:2109.10862v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10862">
<div class="article-summary-box-inner">
<span><p>A major challenge for scaling machine learning is training models to perform
tasks that are very difficult or time-consuming for humans to evaluate. We
present progress on this problem on the task of abstractive summarization of
entire fiction novels. Our method combines learning from human feedback with
recursive task decomposition: we use models trained on smaller parts of the
task to assist humans in giving feedback on the broader task. We collect a
large volume of demonstrations and comparisons from human labelers, and
fine-tune GPT-3 using behavioral cloning and reward modeling to do
summarization recursively. At inference time, the model first summarizes small
sections of the book and then recursively summarizes these summaries to produce
a summary of the entire book. Our human labelers are able to supervise and
evaluate the models quickly, despite not having read the entire books
themselves. Our resulting model generates sensible summaries of entire books,
even matching the quality of human-written summaries in a few cases ($\sim5\%$
of books). We achieve state-of-the-art results on the recent BookSum dataset
for book-length summarization. A zero-shot question-answering model using these
summaries achieves state-of-the-art results on the challenging NarrativeQA
benchmark for answering questions about books and movie scripts. We release
datasets of samples from our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OTEANN: Estimating the Transparency of Orthographies with an Artificial Neural Network. (arXiv:1912.13321v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.13321">
<div class="article-summary-box-inner">
<span><p>To transcribe spoken language to written medium, most alphabets enable an
unambiguous sound-to-letter rule. However, some writing systems have distanced
themselves from this simple concept and little work exists in Natural Language
Processing (NLP) on measuring such distance. In this study, we use an
Artificial Neural Network (ANN) model to evaluate the transparency between
written words and their pronunciation, hence its name Orthographic Transparency
Estimation with an ANN (OTEANN). Based on datasets derived from Wikimedia
dictionaries, we trained and tested this model to score the percentage of
correct predictions in phoneme-to-grapheme and grapheme-to-phoneme translation
tasks. The scores obtained on 17 orthographies were in line with the
estimations of other studies. Interestingly, the model also provided insight
into typical mistakes made by learners who only consider the phonemic rule in
reading and writing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Wait, I'm Still Talking!" Predicting the Dialogue Interaction Behavior Using Imagine-Then-Arbitrate Model. (arXiv:2002.09616v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.09616">
<div class="article-summary-box-inner">
<span><p>Producing natural and accurate responses like human beings is the ultimate
goal of intelligent dialogue agents. So far, most of the past works concentrate
on selecting or generating one pertinent and fluent response according to
current query and its context. These models work on a one-to-one environment,
making one response to one utterance each round. However, in real human-human
conversations, human often sequentially sends several short messages for
readability instead of a long message in one turn. Thus messages will not end
with an explicit ending signal, which is crucial for agents to decide when to
reply. So the first step for an intelligent dialogue agent is not replying but
deciding if it should reply at the moment. To address this issue, in this
paper, we propose a novel Imagine-then-Arbitrate (ITA) neural dialogue model to
help the agent decide whether to wait or to make a response directly. Our
method has two imaginator modules and an arbitrator module. The two imaginators
will learn the agent's and user's speaking style respectively, generate
possible utterances as the input of the arbitrator, combining with dialogue
history. And the arbitrator decides whether to wait or to make a response to
the user directly. To verify the performance and effectiveness of our method,
we prepared two dialogue datasets and compared our approach with several
popular models. Experimental results show that our model performs well on
addressing ending prediction issue and outperforms baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fighting the COVID-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society. (arXiv:2005.00033v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.00033">
<div class="article-summary-box-inner">
<span><p>With the emergence of the COVID-19 pandemic, the political and the medical
aspects of disinformation merged as the problem got elevated to a whole new
level to become the first global infodemic. Fighting this infodemic has been
declared one of the most important focus areas of the World Health
Organization, with dangers ranging from promoting fake cures, rumors, and
conspiracy theories to spreading xenophobia and panic. Addressing the issue
requires solving a number of challenging problems such as identifying messages
containing claims, determining their check-worthiness and factuality, and their
potential to do harm as well as the nature of that harm, to mention just a few.
To address this gap, we release a large dataset of 16K manually annotated
tweets for fine-grained disinformation analysis that (i) focuses on COVID-19,
(ii) combines the perspectives and the interests of journalists, fact-checkers,
social media platforms, policy makers, and society, and (iii) covers Arabic,
Bulgarian, Dutch, and English. Finally, we show strong evaluation results using
pretrained Transformers, thus confirming the practical utility of the dataset
in monolingual vs. multilingual, and single task vs. multitask settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predict-then-Decide: A Predictive Approach for Wait or Answer Task in Dialogue Systems. (arXiv:2005.13119v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.13119">
<div class="article-summary-box-inner">
<span><p>Different people have different habits of describing their intents in
conversations. Some people tend to deliberate their intents in several
successive utterances, i.e., they use several consistent messages for
readability instead of a long sentence to express their question. This creates
a predicament faced by the application of dialogue systems, especially in
real-world industry scenarios, in which the dialogue system is unsure whether
it should answer the query of user immediately or wait for further
supplementary input. Motivated by such an interesting predicament, we define a
novel Wait-or-Answer task for dialogue systems. We shed light on a new research
topic about how the dialogue system can be more intelligent to behave in this
Wait-or-Answer quandary. Further, we propose a predictive approach named
Predict-then-Decide (PTD) to tackle this Wait-or-Answer task. More
specifically, we take advantage of a decision model to help the dialogue system
decide whether to wait or answer. The decision of decision model is made with
the assistance of two ancillary prediction models: a user prediction and an
agent prediction. The user prediction model tries to predict what the user
would supplement and uses its prediction to persuade the decision model that
the user has some information to add, so the dialogue system should wait. The
agent prediction model tries to predict the answer of the dialogue system and
convince the decision model that it is a superior choice to answer the query of
user immediately since the input of user has come to an end. We conduct our
experiments on two real-life scenarios and three public datasets. Experimental
results on five datasets show our proposed PTD approach significantly
outperforms the existing models in solving this Wait-or-Answer problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GIPFA: Generating IPA Pronunciation from Audio. (arXiv:2006.07573v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.07573">
<div class="article-summary-box-inner">
<span><p>Transcribing spoken audio samples into the International Phonetic Alphabet
(IPA) has long been reserved for experts. In this study, we examine the use of
an Artificial Neural Network (ANN) model to automatically extract the IPA
phonemic pronunciation of a word based on its audio pronunciation, hence its
name Generating IPA Pronunciation From Audio (GIPFA). Based on the French
Wikimedia dictionary, we trained our model which then correctly predicted 75%
of the IPA pronunciations tested. Interestingly, by studying inference errors,
the model made it possible to highlight possible errors in the dataset as well
as to identify the closest phonemes in French.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures. (arXiv:2007.08970v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.08970">
<div class="article-summary-box-inner">
<span><p>While mainstream machine learning methods are known to have limited ability
to compositionally generalize, new architectures and techniques continue to be
proposed to address this limitation. We investigate state-of-the-art techniques
and architectures in order to assess their effectiveness in improving
compositional generalization in semantic parsing tasks based on the SCAN and
CFQ datasets. We show that masked language model (MLM) pre-training rivals
SCAN-inspired architectures on primitive holdout splits. On a more complex
compositional task, we show that pre-training leads to significant improvements
in performance vs. comparable non-pre-trained models, whereas architectures
proposed to encourage compositional generalization on SCAN or in the area of
algorithm learning fail to lead to significant improvements. We establish a new
state of the art on the CFQ compositional generalization benchmark using MLM
pre-training together with an intermediate representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Sound Change: Deep and Iterative Learning, Convolutional Neural Networks, and Language Change. (arXiv:2011.05463v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.05463">
<div class="article-summary-box-inner">
<span><p>This paper proposes a framework for modeling sound change that combines deep
learning and iterative learning. Acquisition and transmission of speech is
modeled by training generations of Generative Adversarial Networks (GANs) on
unannotated raw speech data. The paper argues that several properties of sound
change emerge from the proposed architecture. GANs (Goodfellow et al. 2014
<a href="/abs/1406.2661">arXiv:1406.2661</a>, Donahue et al. 2019 <a href="/abs/1705.07904">arXiv:1705.07904</a>) are uniquely appropriate
for modeling language change because the networks are trained on raw
unsupervised acoustic data, contain no language-specific features and, as
argued in Begu\v{s} (2020 <a href="/abs/2006.03965">arXiv:2006.03965</a>), encode phonetic and phonological
representations in their latent space and generate linguistically informative
innovative data. The first generation of networks is trained on the relevant
sequences in human speech from TIMIT. The subsequent generations are not
trained on TIMIT, but on generated outputs from the previous generation and
thus start learning from each other in an iterative learning task. The initial
allophonic distribution is progressively being lost with each generation,
likely due to pressures from the global distribution of aspiration in the
training data. The networks show signs of a gradual shift in phonetic targets
characteristic of a gradual phonetic sound change. At endpoints, the outputs
superficially resemble a phonological change -- rule loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Transferability of Adversarial Attacksagainst Neural Text Classifier. (arXiv:2011.08558v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.08558">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are vulnerable to adversarial attacks, where a small
perturbation to an input alters the model prediction. In many cases, malicious
inputs intentionally crafted for one model can fool another model. In this
paper, we present the first study to systematically investigate the
transferability of adversarial examples for text classification models and
explore how various factors, including network architecture, tokenization
scheme, word embedding, and model capacity, affect the transferability of
adversarial examples. Based on these studies, we propose a genetic algorithm to
find an ensemble of models that can be used to induce adversarial examples to
fool almost all existing models. Such adversarial examples reflect the defects
of the learning process and the data bias in the training set. Finally, we
derive word replacement rules that can be used for model diagnostics from these
adversarial examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Automatic Evaluation of Dialog Systems: A Model-Free Off-Policy Evaluation Approach. (arXiv:2102.10242v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10242">
<div class="article-summary-box-inner">
<span><p>Reliable automatic evaluation of dialogue systems under an interactive
environment has long been overdue. An ideal environment for evaluating dialog
systems, also known as the Turing test, needs to involve human interaction,
which is usually not affordable for large-scale experiments. Though researchers
have attempted to use metrics (e.g., perplexity, BLEU) in language generation
tasks or some model-based reinforcement learning methods (e.g., self-play
evaluation) for automatic evaluation, these methods only show a very weak
correlation with the actual human evaluation in practice. To bridge such a gap,
we propose a new framework named ENIGMA for estimating human evaluation scores
based on recent advances of off-policy evaluation in reinforcement learning.
ENIGMA only requires a handful of pre-collected experience data, and therefore
does not involve human interaction with the target policy during the
evaluation, making automatic evaluations feasible. More importantly, ENIGMA is
model-free and agnostic to the behavior policies for collecting the experience
data (see details in Section 2), which significantly alleviates the technical
difficulties of modeling complex dialogue environments and human behaviors. Our
experiments show that ENIGMA significantly outperforms existing methods in
terms of correlation with human evaluation scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Classifiers: Promises, Shortcomings, and Advances. (arXiv:2102.12452v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12452">
<div class="article-summary-box-inner">
<span><p>Probing classifiers have emerged as one of the prominent methodologies for
interpreting and analyzing deep neural network models of natural language
processing. The basic idea is simple -- a classifier is trained to predict some
linguistic property from a model's representations -- and has been used to
examine a wide variety of models and properties. However, recent studies have
demonstrated various methodological limitations of this approach. This article
critically reviews the probing classifiers framework, highlighting their
promises, shortcomings, and advances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media. (arXiv:2104.05893v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05893">
<div class="article-summary-box-inner">
<span><p>Online misinformation is a prevalent societal issue, with adversaries relying
on tools ranging from cheap fakes to sophisticated deep fakes. We are motivated
by the threat scenario where an image is used out of context to support a
certain narrative. While some prior datasets for detecting image-text
inconsistency generate samples via text manipulation, we propose a dataset
where both image and text are unmanipulated but mismatched. We introduce
several strategies for automatically retrieving convincing images for a given
caption, capturing cases with inconsistent entities or semantic context. Our
large-scale automatically generated NewsCLIPpings Dataset: (1) demonstrates
that machine-driven image repurposing is now a realistic threat, and (2)
provides samples that represent challenging instances of mismatch between text
and image in news that are able to mislead humans. We benchmark several
state-of-the-art multimodal models on our dataset and analyze their performance
across different pretraining domains and visual backbones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Attention Free Transformer. (arXiv:2105.14103v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14103">
<div class="article-summary-box-inner">
<span><p>We introduce Attention Free Transformer (AFT), an efficient variant of
Transformers that eliminates the need for dot product self attention. In an AFT
layer, the key and value are first combined with a set of learned position
biases, the result of which is multiplied with the query in an element-wise
fashion. This new operation has a memory complexity linear w.r.t. both the
context size and the dimension of features, making it compatible to both large
input and model sizes. We also introduce AFT-local and AFT-conv, two model
variants that take advantage of the idea of locality and spatial weight sharing
while maintaining global connectivity. We conduct extensive experiments on two
autoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image
recognition task (ImageNet-1K classification). We show that AFT demonstrates
competitive performance on all the benchmarks, while providing excellent
efficiency at the same time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-task Learning with Cross Attention for Keyword Spotting. (arXiv:2107.07634v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07634">
<div class="article-summary-box-inner">
<span><p>Keyword spotting (KWS) is an important technique for speech applications,
which enables users to activate devices by speaking a keyword phrase. Although
a phoneme classifier can be used for KWS, exploiting a large amount of
transcribed data for automatic speech recognition (ASR), there is a mismatch
between the training criterion (phoneme recognition) and the target task (KWS).
Recently, multi-task learning has been applied to KWS to exploit both ASR and
KWS training data. In this approach, an output of an acoustic model is split
into two branches for the two tasks, one for phoneme transcription trained with
the ASR data and one for keyword classification trained with the KWS data. In
this paper, we introduce a cross attention decoder in the multi-task learning
framework. Unlike the conventional multi-task learning approach with the simple
split of the output layer, the cross attention decoder summarizes information
from a phonetic encoder by performing cross attention between the encoder
outputs and a trainable query sequence to predict a confidence score for the
KWS task. Experimental results on KWS tasks show that the proposed approach
achieves a 12% relative reduction in the false reject ratios compared to the
conventional multi-task learning with split branches and a bi-directional long
short-team memory decoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00590">
<div class="article-summary-box-inner">
<span><p>Web search is fundamentally multimodal and multihop. Often, even before
asking a question we choose to go directly to image search to find our answers.
Further, rarely do we find an answer from a single source but aggregate
information and reason through implications. Despite the frequency of this
everyday occurrence, at present, there is no unified question answering
benchmark that requires a single model to answer long-form natural language
questions from text and open-ended visual sources -- akin to a human's
experience. We propose to bridge this gap between the natural language and
computer vision communities with WebQA. We show that A. our multihop text
queries are difficult for a large-scale transformer model, and B. existing
multi-modal transformers and visual representations do not perform well on
open-domain visual queries. Our challenge for the community is to create a
unified multimodal reasoning model that seamlessly transitions and reasons
regardless of the source modality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling. (arXiv:2109.04699v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04699">
<div class="article-summary-box-inner">
<span><p>While large scale pre-training has achieved great achievements in bridging
the gap between vision and language, it still faces several challenges. First,
the cost for pre-training is expensive. Second, there is no efficient way to
handle the data noise which degrades model performance. Third, previous methods
only leverage limited image-text paired data, while ignoring richer
single-modal data, which may result in poor generalization to single-modal
downstream tasks. In this work, we propose an EfficientCLIP method via Ensemble
Confident Learning to obtain a less noisy data subset. Extra rich non-paired
single-modal text data is used for boosting the generalization of text branch.
We achieve the state-of-the-art performance on Chinese cross-modal retrieval
tasks with only 1/10 training resources compared to CLIP and WenLan, while
showing excellent generalization to single-modal tasks, including text
retrieval and text classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets. (arXiv:2109.05184v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05184">
<div class="article-summary-box-inner">
<span><p>Internet memes have become powerful means to transmit political,
psychological, and socio-cultural ideas. Although memes are typically humorous,
recent days have witnessed an escalation of harmful memes used for trolling,
cyberbullying, and abuse. Detecting such memes is challenging as they can be
highly satirical and cryptic. Moreover, while previous work has focused on
specific aspects of memes such as hate speech and propaganda, there has been
little work on harm in general. Here, we aim to bridge this gap. We focus on
two tasks: (i)detecting harmful memes, and (ii)identifying the social entities
they target. We further extend a recently released HarMeme dataset, which
covered COVID-19, with additional memes and a new topic: US politics. To solve
these tasks, we propose MOMENTA (MultimOdal framework for detecting harmful
MemEs aNd Their tArgets), a novel multimodal deep neural network that uses
global and local perspectives to detect harmful memes. MOMENTA systematically
analyzes the local and the global perspective of the input meme (in both
modalities) and relates it to the background context. MOMENTA is interpretable
and generalizable, and our experiments show that it outperforms several strong
rivaling approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keyword Extraction for Improved Document Retrieval in Conversational Search. (arXiv:2109.05979v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05979">
<div class="article-summary-box-inner">
<span><p>Recent research has shown that mixed-initiative conversational search, based
on the interaction between users and computers to clarify and improve a query,
provides enormous advantages. Nonetheless, incorporating additional information
provided by the user from the conversation poses some challenges. In fact,
further interactions could confuse the system as a user might use words
irrelevant to the information need but crucial for correct sentence
construction in the context of multi-turn conversations. To this aim, in this
paper, we have collected two conversational keyword extraction datasets and
propose an end-to-end document retrieval pipeline incorporating them.
Furthermore, we study the performance of two neural keyword extraction models,
namely, BERT and sequence to sequence, in terms of extraction accuracy and
human annotation. Finally, we study the effect of keyword extraction on the
end-to-end neural IR performance and show that our approach beats
state-of-the-art IR models. We make the two datasets publicly available to
foster research in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Commonsense-Focused Dialogues for Response Generation: An Empirical Study. (arXiv:2109.06427v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06427">
<div class="article-summary-box-inner">
<span><p>Smooth and effective communication requires the ability to perform latent or
explicit commonsense inference. Prior commonsense reasoning benchmarks (such as
SocialIQA and CommonsenseQA) mainly focus on the discriminative task of
choosing the right answer from a set of candidates, and do not involve
interactive language generation as in dialogue. Moreover, existing dialogue
datasets do not explicitly focus on exhibiting commonsense as a facet. In this
paper, we present an empirical study of commonsense in dialogue response
generation. We first auto-extract commonsensical dialogues from existing
dialogue datasets by leveraging ConceptNet, a commonsense knowledge graph.
Furthermore, building on social contexts/situations in SocialIQA, we collect a
new dialogue dataset with 25K dialogues aimed at exhibiting social commonsense
in an interactive setting. We evaluate response generation models trained using
these datasets and find that models trained on both extracted and our collected
data produce responses that consistently exhibit more commonsense than
baselines. Finally we propose an approach for automatic evaluation of
commonsense that relies on features derived from ConceptNet and pre-trained
language and dialog models, and show reasonable correlation with human
evaluation of responses' commonsense quality. We are releasing a subset of our
collected data, Commonsense-Dialogues, containing about 11K dialogs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Clinical Information Extraction with Transferred Contextual Embeddings. (arXiv:2109.07243v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07243">
<div class="article-summary-box-inner">
<span><p>The Bidirectional Encoder Representations from Transformers (BERT) model has
achieved the state-of-the-art performance for many natural language processing
(NLP) tasks. Yet, limited research has been contributed to studying its
effectiveness when the target domain is shifted from the pre-training corpora,
for example, for biomedical or clinical NLP applications. In this paper, we
applied it to a widely studied a hospital information extraction (IE) task and
analyzed its performance under the transfer learning setting. Our application
became the new state-of-the-art result by a clear margin, compared with a range
of existing IE models. Specifically, on this nursing handover data set, the
macro-average F1 score from our model was 0.438, whilst the previous best deep
learning models had 0.416. In conclusion, we showed that BERT based
pre-training models can be transferred to health-related documents under mild
conditions and with a proper fine-tuning process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DyLex: Incorporating Dynamic Lexicons into BERT for Sequence Labeling. (arXiv:2109.08818v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08818">
<div class="article-summary-box-inner">
<span><p>Incorporating lexical knowledge into deep learning models has been proved to
be very effective for sequence labeling tasks. However, previous works commonly
have difficulty dealing with large-scale dynamic lexicons which often cause
excessive matching noise and problems of frequent updates. In this paper, we
propose DyLex, a plug-in lexicon incorporation approach for BERT based sequence
labeling tasks. Instead of leveraging embeddings of words in the lexicon as in
conventional methods, we adopt word-agnostic tag embeddings to avoid
re-training the representation while updating the lexicon. Moreover, we employ
an effective supervised lexical knowledge denoising method to smooth out
matching noise. Finally, we introduce a col-wise attention based knowledge
fusion mechanism to guarantee the pluggability of the proposed framework.
Experiments on ten datasets of three tasks show that the proposed framework
achieves new SOTA, even with very large scale lexicons.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Joint Intent Detection and Slot Filling via Higher-order Attention. (arXiv:2109.08890v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08890">
<div class="article-summary-box-inner">
<span><p>Intent detection (ID) and Slot filling (SF) are two major tasks in spoken
language understanding (SLU). Recently, attention mechanism has been shown to
be effective in jointly optimizing these two tasks in an interactive manner.
However, latest attention-based works concentrated only on the first-order
attention design, while ignoring the exploration of higher-order attention
mechanisms. In this paper, we propose a BiLinear attention block, which
leverages bilinear pooling to simultaneously exploit both the contextual and
channel-wise bilinear attention distributions to capture the second-order
interactions between the input intent or slot features. Higher and even
infinity order interactions are built by stacking numerous blocks and assigning
Exponential Linear Unit (ELU) to blocks. Before the decoding stage, we
introduce the Dynamic Feature Fusion Layer to implicitly fuse intent and slot
information in a more effective way. Technically, instead of simply
concatenating intent and slot features, we first compute two correlation
matrices to weight on two features. Furthermore, we present Higher-order
Attention Network for the SLU tasks. Experiments on two benchmark datasets show
that our approach yields improvements compared with the state-of-the-art
approach. We also provide discussion to demonstrate the effectiveness of the
proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models. (arXiv:2109.10282v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10282">
<div class="article-summary-box-inner">
<span><p>Text recognition is a long-standing research problem for document
digitalization. Existing approaches for text recognition are usually built
based on CNN for image understanding and RNN for char-level text generation. In
addition, another language model is usually needed to improve the overall
accuracy as a post-processing step. In this paper, we propose an end-to-end
text recognition approach with pre-trained image Transformer and text
Transformer models, namely TrOCR, which leverages the Transformer architecture
for both image understanding and wordpiece-level text generation. The TrOCR
model is simple but effective, and can be pre-trained with large-scale
synthetic data and fine-tuned with human-labeled datasets. Experiments show
that the TrOCR model outperforms the current state-of-the-art models on both
printed and handwritten text recognition tasks. The code and models will be
publicly available at https://aka.ms/TrOCR.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">An Ultra-Fast Method for Simulation of Realistic Ultrasound Images. (arXiv:2109.10353v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10353">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have attracted a rapidly growing
interest in a variety of different processing tasks in the medical ultrasound
community. However, the performance of CNNs is highly reliant on both the
amount and fidelity of the training data. Therefore, scarce data is almost
always a concern, particularly in the medical field, where clinical data is not
easily accessible. The utilization of synthetic data is a popular approach to
address this challenge. However, but simulating a large number of images using
packages such as Field II is time-consuming, and the distribution of simulated
images is far from that of the real images. Herein, we introduce a novel
ultra-fast ultrasound image simulation method based on the Fourier transform
and evaluate its performance in a lesion segmentation task. We demonstrate that
data augmentation using the images generated by the proposed method
substantially outperforms Field II in terms of Dice similarity coefficient,
while the simulation is almost 36000 times faster (both on CPU).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust marginalization of baryonic effects for cosmological inference at the field level. (arXiv:2109.10360v1 [astro-ph.CO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10360">
<div class="article-summary-box-inner">
<span><p>We train neural networks to perform likelihood-free inference from
$(25\,h^{-1}{\rm Mpc})^2$ 2D maps containing the total mass surface density
from thousands of hydrodynamic simulations of the CAMELS project. We show that
the networks can extract information beyond one-point functions and power
spectra from all resolved scales ($\gtrsim 100\,h^{-1}{\rm kpc}$) while
performing a robust marginalization over baryonic physics at the field level:
the model can infer the value of $\Omega_{\rm m} (\pm 4\%)$ and $\sigma_8 (\pm
2.5\%)$ from simulations completely different to the ones used to train it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coast Sargassum Level Estimation from Smartphone Pictures. (arXiv:2109.10390v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10390">
<div class="article-summary-box-inner">
<span><p>Since 2011, significant and atypical arrival of two species of surface
dwelling algae, Sargassum natans and Sargassum Fluitans, have been detected in
the Mexican Caribbean. This massive accumulation of algae has had a great
environmental and economic impact. Therefore, for the government, ecologists,
and local businesses, it is important to keep track of the amount of sargassum
that arrives on the Caribbean coast. High-resolution satellite imagery is
expensive or may be time delayed. Therefore, we propose to estimate the amount
of sargassum based on ground-level smartphone photographs. From the computer
vision perspective, the problem is quite difficult since no information about
the 3D world is provided, in consequence, we have to model it as a
classification problem, where a set of five labels define the amount. For this
purpose, we have built a dataset with more than one thousand examples from
public forums such as Facebook or Instagram and we have tested several
state-of-the-art convolutional networks. As a result, the VGG network trained
under fine-tuning showed the best performance. Even though the reached accuracy
could be improved with more examples, the current prediction distribution is
narrow, so the predictions are adequate for keeping a record and taking quick
ecological actions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Real-Time Facial Analysis System. (arXiv:2109.10393v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10393">
<div class="article-summary-box-inner">
<span><p>Facial analysis is an active research area in computer vision, with many
practical applications. Most of the existing studies focus on addressing one
specific task and maximizing its performance. For a complete facial analysis
system, one needs to solve these tasks efficiently to ensure a smooth
experience. In this work, we present a system-level design of a real-time
facial analysis system. With a collection of deep neural networks for object
detection, classification, and regression, the system recognizes age, gender,
facial expression, and facial similarity for each person that appears in the
camera view. We investigate the parallelization and interplay of individual
tasks. Results on common off-the-shelf architecture show that the system's
accuracy is comparable to the state-of-the-art methods, and the recognition
speed satisfies real-time requirements. Moreover, we propose a multitask
network for jointly predicting the first three attributes, i.e., age, gender,
and facial expression. Source code and trained models are available at
https://github.com/mahehu/TUT-live-age-estimator.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The First Vision For Vitals (V4V) Challenge for Non-Contact Video-Based Physiological Estimation. (arXiv:2109.10471v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10471">
<div class="article-summary-box-inner">
<span><p>Telehealth has the potential to offset the high demand for help during public
health emergencies, such as the COVID-19 pandemic. Remote Photoplethysmography
(rPPG) - the problem of non-invasively estimating blood volume variations in
the microvascular tissue from video - would be well suited for these
situations. Over the past few years a number of research groups have made rapid
advances in remote PPG methods for estimating heart rate from digital video and
obtained impressive results. How these various methods compare in naturalistic
conditions, where spontaneous behavior, facial expressions, and illumination
changes are present, is relatively unknown. To enable comparisons among
alternative methods, the 1st Vision for Vitals Challenge (V4V) presented a
novel dataset containing high-resolution videos time-locked with varied
physiological signals from a diverse population. In this paper, we outline the
evaluation protocol, the data used, and the results. V4V is to be held in
conjunction with the 2021 International Conference on Computer Vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rotor Localization and Phase Mapping of Cardiac Excitation Waves using Deep Neural Networks. (arXiv:2109.10472v1 [physics.med-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10472">
<div class="article-summary-box-inner">
<span><p>The analysis of electrical impulse phenomena in cardiac muscle tissue is
important for the diagnosis of heart rhythm disorders and other cardiac
pathophysiology. Cardiac mapping techniques acquire numerous local temporal
measurements and combine them to visualize the spread of electrophysiological
wave phenomena across the heart surface. However, low spatial resolutions,
sparse measurement locations, noise and other artifacts make it challenging to
accurately visualize spatio-temporal activity. For instance, electro-anatomical
catheter mapping is severely limited by the sparsity of the measurements and
optical mapping is prone to noise and motion artifacts. In the past, several
approaches have been proposed to obtain more reliable maps from noisy or sparse
mapping data. Here, we demonstrate that deep learning can be used to compute
phase maps and detect phase singularities from both noisy and sparse electrical
mapping data with high precision and efficiency. The self-supervised deep
learning approach is fundamentally different from classical phase mapping
techniques. Rather than encoding a phase signal from time-series data, the
network instead learns to directly associate short spatio-temporal sequences of
electrical data with phase maps and the positions of phase singularities. Using
this method, we were able to accurately compute phase maps and locate rotor
cores even from extremely sparse and noisy data, generated from both optical
mapping experiments and computer simulations. Neural networks are a promising
alternative to conventional phase mapping and rotor core localization methods,
that could be used in optical mapping studies in basic cardiovascular research
as well as in the clinical setting for the analysis of atrial fibrillation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MVM3Det: A Novel Method for Multi-view Monocular 3D Detection. (arXiv:2109.10473v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10473">
<div class="article-summary-box-inner">
<span><p>Monocular 3D object detection encounters occlusion problems in many
application scenarios, such as traffic monitoring, pedestrian monitoring, etc.,
which leads to serious false negative. Multi-view object detection effectively
solves this problem by combining data from different perspectives. However, due
to label confusion and feature confusion, the orientation estimation of
multi-view 3D object detection is intractable, which is important for object
tracking and intention prediction. In this paper, we propose a novel multi-view
3D object detection method named MVM3Det which simultaneously estimates the 3D
position and orientation of the object according to the multi-view monocular
information. The method consists of two parts: 1) Position proposal network,
which integrates the features from different perspectives into consistent
global features through feature orthogonal transformation to estimate the
position. 2) Multi-branch orientation estimation network, which introduces
feature perspective pooling to overcome the two confusion problems during the
orientation estimation. In addition, we present a first dataset for multi-view
3D object detection named MVM3D. Comparing with State-Of-The-Art (SOTA) methods
on our dataset and public dataset WildTrack, our method achieves very
competitive results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rapid detection and recognition of whole brain activity in a freely behaving Caenorhabditis elegans. (arXiv:2109.10474v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10474">
<div class="article-summary-box-inner">
<span><p>Advanced volumetric imaging methods and genetically encoded activity
indicators have permitted a comprehensive characterization of whole brain
activity at single neuron resolution in \textit{Caenorhabditis elegans}. The
constant motion and deformation of the mollusc nervous system, however, impose
a great challenge for a consistent identification of densely packed neurons in
a behaving animal. Here, we propose a cascade solution for long-term and rapid
recognition of head ganglion neurons in a freely moving \textit{C. elegans}.
First, potential neuronal regions from a stack of fluorescence images are
detected by a deep learning algorithm. Second, 2 dimensional neuronal regions
are fused into 3 dimensional neuron entities. Third, by exploiting the neuronal
density distribution surrounding a neuron and relative positional information
between neurons, a multi-class artificial neural network transforms engineered
neuronal feature vectors into digital neuronal identities. Under the constraint
of a small number (20-40 volumes) of training samples, our bottom-up approach
is able to process each volume - $1024 \times 1024 \times 18$ in voxels - in
less than 1 second and achieves an accuracy of $91\%$ in neuronal detection and
$74\%$ in neuronal recognition. Our work represents an important development
towards a rapid and fully automated algorithm for decoding whole brain activity
underlying natural animal behaviors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Compositional Color Representations from Text. (arXiv:2109.10477v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10477">
<div class="article-summary-box-inner">
<span><p>We consider the cross-modal task of producing color representations for text
phrases. Motivated by the fact that a significant fraction of user queries on
an image search engine follow an (attribute, object) structure, we propose a
generative adversarial network that generates color profiles for such bigrams.
We design our pipeline to learn composition - the ability to combine seen
attributes and objects to unseen pairs. We propose a novel dataset curation
pipeline from existing public sources. We describe how a set of phrases of
interest can be compiled using a graph propagation technique, and then mapped
to images. While this dataset is specialized for our investigations on color,
the method can be extended to other visual dimensions where composition is of
interest. We provide detailed ablation studies that test the behavior of our
GAN architecture with loss functions from the contrastive learning literature.
We show that the generative model achieves lower Frechet Inception Distance
than discriminative ones, and therefore predicts color profiles that better
match those from real images. Finally, we demonstrate improved performance in
image retrieval and classification, indicating the crucial role that color
plays in these downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI in Osteoporosis. (arXiv:2109.10478v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10478">
<div class="article-summary-box-inner">
<span><p>In this chapter we explore and evaluate methods for trabecular bone
characterization and osteoporosis diagnosis with increased interest in sparse
approximations. We first describe texture representation and classification
techniques, patch-based methods such as Bag of Keypoints, and more recent deep
neural networks. Then we introduce the concept of sparse representations for
pattern recognition and we detail integrative sparse analysis methods and
classifier decision fusion methods. We report cross-validation results on
osteoporosis datasets of bone radiographs and compare the results produced by
the different categories of methods. We conclude that advances in the AI and
machine learning fields have enabled the development of methods that can be
used as diagnostic tools in clinical settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single Image Dehazing with An Independent Detail-Recovery Network. (arXiv:2109.10492v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10492">
<div class="article-summary-box-inner">
<span><p>Single image dehazing is a prerequisite which affects the performance of many
computer vision tasks and has attracted increasing attention in recent years.
However, most existing dehazing methods emphasize more on haze removal but less
on the detail recovery of the dehazed images. In this paper, we propose a
single image dehazing method with an independent Detail Recovery Network (DRN),
which considers capturing the details from the input image over a separate
network and then integrates them into a coarse dehazed image. The overall
network consists of two independent networks, named DRN and the dehazing
network respectively. Specifically, the DRN aims to recover the dehazed image
details through local and global branches respectively. The local branch can
obtain local detail information through the convolution layer and the global
branch can capture more global information by the Smooth Dilated Convolution
(SDC). The detail feature map is fused into the coarse dehazed image to obtain
the dehazed image with rich image details. Besides, we integrate the DRN, the
physical-model-based dehazing network and the reconstruction loss into an
end-to-end joint learning framework. Extensive experiments on the public image
dehazing datasets (RESIDE-Indoor, RESIDE-Outdoor and the TrainA-TestA)
illustrate the effectiveness of the modules in the proposed method and show
that our method outperforms the state-of-the-art dehazing methods both
quantitatively and qualitatively. The code is released in
https://github.com/YanLi-LY/Dehazing-DRN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification. (arXiv:2109.10498v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10498">
<div class="article-summary-box-inner">
<span><p>Person re-identification (re-ID) plays an important role in applications such
as public security and video surveillance. Recently, learning from synthetic
data, which benefits from the popularity of synthetic data engine, has
attracted attention from both academia and the public eye. However, existing
synthetic datasets are limited in quantity, diversity and realisticity, and
cannot be efficiently used for generalizable re-ID problem. To address this
challenge, we construct and label a large-scale synthetic person dataset named
FineGPR with fine-grained attribute distribution. Moreover, aiming to fully
exploit the potential of FineGPR and promote the efficient training from
millions of synthetic data, we propose an attribute analysis pipeline AOST to
learn attribute distribution in target domain, then apply style transfer
network to eliminate the gap between synthetic and real-world data and thus is
freely deployed to new scenarios. Experiments conducted on benchmarks
demonstrate that FineGPR with AOST outperforms (or is on par with) existing
real and synthetic datasets, which suggests its feasibility for re-ID and
proves the proverbial less-is-more principle. We hope this fine-grained dataset
could advance research towards re-ID in real scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Optical Neuroimaging Denoising with Semantic Tasks. (arXiv:2109.10499v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10499">
<div class="article-summary-box-inner">
<span><p>Optical neuroimaging is a vital tool for understanding the brain structure
and the connection between regions and nuclei. However, the image noise
introduced in the sample preparation and the imaging system hinders the
extraction of the possible knowlege from the dataset, thus denoising for the
optical neuroimaging is usually necessary. The supervised denoisng methods
often outperform the unsupervised ones, but the training of the supervised
denoising models needs the corresponding clean labels, which is not always
avaiable due to the high labeling cost. On the other hand, those semantic
labels, such as the located soma positions, the reconstructed neuronal fibers,
and the nuclei segmentation result, are generally available and accumulated
from everyday neuroscience research. This work connects a supervised denoising
and a semantic segmentation model together to form a end-to-end model, which
can make use of the semantic labels while still provides a denoised image as an
intermediate product. We use both the supervised and the self-supervised models
for the denoising and introduce a new cost term for the joint denoising and the
segmentation setup. We test the proposed approach on both the synthetic data
and the real-world data, including the optical neuroimaing dataset and the
electron microscope dataset. The result shows that the joint denoising result
outperforms the one using the denoising method alone and the joint model
benefits the segmentation and other downstream task as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KD-VLP: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation. (arXiv:2109.10504v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10504">
<div class="article-summary-box-inner">
<span><p>Self-supervised vision-and-language pretraining (VLP) aims to learn
transferable multi-modal representations from large-scale image-text data and
to achieve strong performances on a broad scope of vision-language tasks after
finetuning. Previous mainstream VLP approaches typically adopt a two-step
strategy relying on external object detectors to encode images in a multi-modal
Transformer framework, which suffer from restrictive object concept space,
limited image context and inefficient computation. In this paper, we propose an
object-aware end-to-end VLP framework, which directly feeds image grid features
from CNNs into the Transformer and learns the multi-modal representations
jointly. More importantly, we propose to perform object knowledge distillation
to facilitate learning cross-modal alignment at different semantic levels. To
achieve that, we design two novel pretext tasks by taking object features and
their semantic labels from external detectors as supervision: 1.) Object-guided
masked vision modeling task focuses on enforcing object-aware representation
learning in the multi-modal Transformer; 2.) Phrase-region alignment task aims
to improve cross-modal alignment by utilizing the similarities between noun
phrases and object labels in the linguistic space. Extensive experiments on a
wide range of vision-language tasks demonstrate the efficacy of our proposed
framework, and we achieve competitive or superior performances over the
existing pretraining strategies. The code is available in supplementary
materials.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Data Uncertainty in Object Tracking Algorithms. (arXiv:2109.10521v1 [eess.SY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10521">
<div class="article-summary-box-inner">
<span><p>Methodologies for incorporating the uncertainties characteristic of
data-driven object detectors into object tracking algorithms are explored.
Object tracking methods rely on measurement error models, typically in the form
of measurement noise, false positive rates, and missed detection rates. Each of
these quantities, in general, can be dependent on object or measurement
location. However, for detections generated from neural-network processed
camera inputs, these measurement error statistics are not sufficient to
represent the primary source of errors, namely a dissimilarity between run-time
sensor input and the training data upon which the detector was trained. To this
end, we investigate incorporating data uncertainty into object tracking methods
such as to improve the ability to track objects, and particularly those which
out-of-distribution w.r.t. training data. The proposed methodologies are
validated on an object tracking benchmark as well on experiments with a real
autonomous aircraft.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Method For Adding Motion-Blur on Arbitrary Objects By using Auto-Segmentation and Color Compensation Techniques. (arXiv:2109.10524v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10524">
<div class="article-summary-box-inner">
<span><p>When dynamic objects are captured by a camera, motion blur inevitably occurs.
Such a blur is sometimes considered as just a noise, however, it sometimes
gives an important effect to add dynamism in the scene for photographs or
videos. Unlike the similar effects, such as defocus blur, which is now easily
controlled even by smartphones, motion blur is still uncontrollable and makes
undesired effects on photographs. In this paper, an unified framework to add
motion blur on per-object basis is proposed. In the method, multiple frames are
captured without motion blur and they are accumulated to create motion blur on
target objects. To capture images without motion blur, shutter speed must be
short, however, it makes captured images dark, and thus, a sensor gain should
be increased to compensate it. Since a sensor gain causes a severe noise on
image, we propose a color compensation algorithm based on non-linear filtering
technique for solution. Another contribution is that our technique can be used
to make HDR images for fast moving objects by using multi-exposure images. In
the experiments, effectiveness of the method is confirmed by ablation study
using several data sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Multimodal Transformer to Summarize Videos. (arXiv:2109.10559v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10559">
<div class="article-summary-box-inner">
<span><p>Although video summarization has achieved tremendous success benefiting from
Recurrent Neural Networks (RNN), RNN-based methods neglect the global
dependencies and multi-hop relationships among video frames, which limits the
performance. Transformer is an effective model to deal with this problem, and
surpasses RNN-based methods in several sequence modeling tasks, such as machine
translation, video captioning, \emph{etc}. Motivated by the great success of
transformer and the natural structure of video (frame-shot-video), a
hierarchical transformer is developed for video summarization, which can
capture the dependencies among frame and shots, and summarize the video by
exploiting the scene information formed by shots. Furthermore, we argue that
both the audio and visual information are essential for the video summarization
task. To integrate the two kinds of information, they are encoded in a
two-stream scheme, and a multimodal fusion mechanism is developed based on the
hierarchical transformer. In this paper, the proposed method is denoted as
Hierarchical Multimodal Transformer (HMT). Practically, extensive experiments
show that HMT surpasses most of the traditional, RNN-based and attention-based
video summarization methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving 360 Monocular Depth Estimation via Non-local Dense Prediction Transformer and Joint Supervised and Self-supervised Learning. (arXiv:2109.10563v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10563">
<div class="article-summary-box-inner">
<span><p>Due to difficulties in acquiring ground truth depth of equirectangular (360)
images, the quality and quantity of equirectangular depth data today is
insufficient to represent the various scenes in the world. Therefore, 360 depth
estimation studies, which relied solely on supervised learning, are destined to
produce unsatisfactory results. Although self-supervised learning methods
focusing on equirectangular images (EIs) are introduced, they often have
incorrect or non-unique solutions, causing unstable performance. In this paper,
we propose 360 monocular depth estimation methods which improve on the areas
that limited previous studies. First, we introduce a self-supervised 360 depth
learning method that only utilizes gravity-aligned videos, which has the
potential to eliminate the needs for depth data during the training procedure.
Second, we propose a joint learning scheme realized by combining supervised and
self-supervised learning. The weakness of each learning is compensated, thus
leading to more accurate depth estimation. Third, we propose a non-local fusion
block, which retains global information encoded by vision transformer when
reconstructing the depths. With the proposed methods, we successfully apply the
transformer to 360 depth estimations, to the best of our knowledge, which has
not been tried before. On several benchmarks, our approach achieves significant
improvements over previous works and establishes a state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Live Speech Portraits: Real-Time Photorealistic Talking-Head Animation. (arXiv:2109.10595v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10595">
<div class="article-summary-box-inner">
<span><p>To the best of our knowledge, we first present a live system that generates
personalized photorealistic talking-head animation only driven by audio signals
at over 30 fps. Our system contains three stages. The first stage is a deep
neural network that extracts deep audio features along with a manifold
projection to project the features to the target person's speech space. In the
second stage, we learn facial dynamics and motions from the projected audio
features. The predicted motions include head poses and upper body motions,
where the former is generated by an autoregressive probabilistic model which
models the head pose distribution of the target person. Upper body motions are
deduced from head poses. In the final stage, we generate conditional feature
maps from previous predictions and send them with a candidate image set to an
image-to-image translation network to synthesize photorealistic renderings. Our
method generalizes well to wild audio and successfully synthesizes
high-fidelity personalized facial details, e.g., wrinkles, teeth. Our method
also allows explicit control of head poses. Extensive qualitative and
quantitative evaluations, along with user studies, demonstrate the superiority
of our method over state-of-the-art techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Context-Aware Network for Abdominal Multi-organ Segmentation. (arXiv:2109.10601v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10601">
<div class="article-summary-box-inner">
<span><p>The contextual information, presented in abdominal CT scan, is relative
consistent. In order to make full use of the overall 3D context, we develop a
whole-volumebased coarse-to-fine framework for efficient and effective
abdominal multi-organ segmentation. We propose a new efficientSegNet network,
which is composed of encoder, decoder and context block. For the decoder
module, anisotropic convolution with a k*k*1 intra-slice convolution and a
1*1*k inter-slice convolution, is designed to reduce the computation burden.
For the context block, we propose strip pooling module to capture anisotropic
and long-range contextual information, which exists in abdominal scene.
Quantitative evaluation on the FLARE2021 validation cases, this method achieves
the average dice similarity coefficient (DSC) of 0.895 and average normalized
surface distance (NSD) of 0.775. The average running time is 9.8 s per case in
inference phase, and maximum used GPU memory is 1017 MB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LDC-VAE: A Latent Distribution Consistency Approach to Variational AutoEncoders. (arXiv:2109.10640v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10640">
<div class="article-summary-box-inner">
<span><p>Variational autoencoders (VAEs), as an important aspect of generative models,
have received a lot of research interests and reached many successful
applications. However, it is always a challenge to achieve the consistency
between the learned latent distribution and the prior latent distribution when
optimizing the evidence lower bound (ELBO), and finally leads to an
unsatisfactory performance in data generation. In this paper, we propose a
latent distribution consistency approach to avoid such substantial
inconsistency between the posterior and prior latent distributions in ELBO
optimizing. We name our method as latent distribution consistency VAE
(LDC-VAE). We achieve this purpose by assuming the real posterior distribution
in latent space as a Gibbs form, and approximating it by using our encoder.
However, there is no analytical solution for such Gibbs posterior in
approximation, and traditional approximation ways are time consuming, such as
using the iterative sampling-based MCMC. To address this problem, we use the
Stein Variational Gradient Descent (SVGD) to approximate the Gibbs posterior.
Meanwhile, we use the SVGD to train a sampler net which can obtain efficient
samples from the Gibbs posterior. Comparative studies on the popular image
generation datasets show that our method has achieved comparable or even better
performance than several powerful improvements of VAEs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-Aware Training for Cardiac Resynchronisation Therapy Response Prediction. (arXiv:2109.10641v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10641">
<div class="article-summary-box-inner">
<span><p>Evaluation of predictive deep learning (DL) models beyond conventional
performance metrics has become increasingly important for applications in
sensitive environments like healthcare. Such models might have the capability
to encode and analyse large sets of data but they often lack comprehensive
interpretability methods, preventing clinical trust in predictive outcomes.
Quantifying uncertainty of a prediction is one way to provide such
interpretability and promote trust. However, relatively little attention has
been paid to how to include such requirements into the training of the model.
In this paper we: (i) quantify the data (aleatoric) and model (epistemic)
uncertainty of a DL model for Cardiac Resynchronisation Therapy response
prediction from cardiac magnetic resonance images, and (ii) propose and perform
a preliminary investigation of an uncertainty-aware loss function that can be
used to retrain an existing DL image-based classification model to encourage
confidence in correct predictions and reduce confidence in incorrect
predictions. Our initial results are promising, showing a significant increase
in the (epistemic) confidence of true positive predictions, with some evidence
of a reduction in false negative confidence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Caption Enriched Samples for Improving Hateful Memes Detection. (arXiv:2109.10649v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10649">
<div class="article-summary-box-inner">
<span><p>The recently introduced hateful meme challenge demonstrates the difficulty of
determining whether a meme is hateful or not. Specifically, both unimodal
language models and multimodal vision-language models cannot reach the human
level of performance. Motivated by the need to model the contrast between the
image content and the overlayed text, we suggest applying an off-the-shelf
image captioning tool in order to capture the first. We demonstrate that the
incorporation of such automatic captions during fine-tuning improves the
results for various unimodal and multimodal models. Moreover, in the unimodal
case, continuing the pre-training of language models on augmented and original
caption pairs, is highly beneficial to the classification accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vehicle Behavior Prediction and Generalization Using Imbalanced Learning Techniques. (arXiv:2109.10656v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10656">
<div class="article-summary-box-inner">
<span><p>The use of learning-based methods for vehicle behavior prediction is a
promising research topic. However, many publicly available data sets suffer
from class distribution skews which limits learning performance if not
addressed. This paper proposes an interaction-aware prediction model consisting
of an LSTM autoencoder and SVM classifier. Additionally, an imbalanced learning
technique, the multiclass balancing ensemble is proposed. Evaluations show that
the method enhances model performance, resulting in improved classification
accuracy. Good generalization properties of learned models are important and
therefore a generalization study is done where models are evaluated on unseen
traffic data with dissimilar traffic behavior stemming from different road
configurations. This is realized by using two distinct highway traffic
recordings, the publicly available NGSIM US-101 and I80 data sets. Moreover,
methods for encoding structural and static features into the learning process
for improved generalization are evaluated. The resulting methods show
substantial improvements in classification as well as generalization
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TACTIC: Joint Rate-Distortion-Accuracy Optimisation for Low Bitrate Compression. (arXiv:2109.10658v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10658">
<div class="article-summary-box-inner">
<span><p>We present TACTIC: Task-Aware Compression Through Intelligent Coding. Our
lossy compression model learns based on the rate-distortion-accuracy trade-off
for a specific task. By considering what information is important for the
follow-on problem, the system trades off visual fidelity for good task
performance at a low bitrate. When compared against JPEG at the same bitrate,
our approach is able to improve the accuracy of ImageNet subset classification
by 4.5%. We also demonstrate the applicability of our approach to other
problems, providing a 3.4% accuracy and 4.9% mean IoU improvements in
performance over task-agnostic compression for semantic segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A deep neural network for multi-species fish detection using multiple acoustic cameras. (arXiv:2109.10664v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10664">
<div class="article-summary-box-inner">
<span><p>Underwater acoustic cameras are high potential devices for many applications
in ecology, notably for fisheries management and monitoring. However how to
extract such data into high value information without a time-consuming entire
dataset reading by an operator is still a challenge. Moreover the analysis of
acoustic imaging, due to its low signal-to-noise ratio, is a perfect training
ground for experimenting with new approaches, especially concerning Deep
Learning techniques. We present hereby a novel approach that takes advantage of
both CNN (Convolutional Neural Network) and classical CV (Computer Vision)
techniques, able to detect a generic class ''fish'' in acoustic video streams.
The pipeline pre-treats the acoustic images to extract 2 features, in order to
localise the signals and improve the detection performances. To ensure the
performances from an ecological point of view, we propose also a two-step
validation, one to validate the results of the trainings and one to test the
method on a real-world scenario. The YOLOv3-based model was trained with data
of fish from multiple species recorded by the two common acoustic cameras,
DIDSON and ARIS, including species of high ecological interest, as Atlantic
salmon or European eels. The model we developed provides satisfying results
detecting almost 80% of fish and minimizing the false positive rate, however
the model is much less efficient for eel detections on ARIS videos. The first
CNN pipeline for fish monitoring exploiting video data from two models of
acoustic cameras satisfies most of the required features. Many challenges are
still present, such as the automation of fish species identification through a
multiclass model. 1 However the results point a new solution for dealing with
complex data, such as sonar data, which can also be reapplied in other cases
where the signal-to-noise ratio is a challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Training Based Unsupervised Cross-Modality Domain Adaptation for Vestibular Schwannoma and Cochlea Segmentation. (arXiv:2109.10674v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10674">
<div class="article-summary-box-inner">
<span><p>With the advances of deep learning, many medical image segmentation studies
achieve human-level performance when in fully supervised condition. However, it
is extremely expensive to acquire annotation on every data in medical fields,
especially on magnetic resonance images (MRI) that comprise many different
contrasts. Unsupervised methods can alleviate this problem; however, the
performance drop is inevitable compared to fully supervised methods. In this
work, we propose a self-training based unsupervised-learning framework that
performs automatic segmentation of Vestibular Schwannoma (VS) and cochlea on
high-resolution T2 scans. Our method consists of 4 main stages: 1)
VS-preserving contrast conversion from contrast-enhanced T1 scan to
high-resolution T2 scan, 2) training segmentation on generated T2 scans with
annotations on T1 scans, and 3) Inferring pseudo-labels on non-annotated real
T2 scans, and 4) boosting the generalizability of VS and cochlea segmentation
by training with combined data (i.e., real T2 scans with pseudo-labels and
generated T2 scans with true annotations). Our method showed mean Dice score
and Average Symmetric Surface Distance (ASSD) of 0.8570 (0.0705) and 0.4970
(0.3391) for VS, 0.8446 (0.0211) and 0.1513 (0.0314) for Cochlea on
CrossMoDA2021 challenge validation phase leaderboard, outperforming most other
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Video Localization with Learnable Moment Proposals. (arXiv:2109.10678v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10678">
<div class="article-summary-box-inner">
<span><p>Given an untrimmed video and a natural language query, Natural Language Video
Localization (NLVL) aims to identify the video moment described by the query.
To address this task, existing methods can be roughly grouped into two groups:
1) propose-and-rank models first define a set of hand-designed moment
candidates and then find out the best-matching one. 2) proposal-free models
directly predict two temporal boundaries of the referential moment from frames.
Currently, almost all the propose-and-rank methods have inferior performance
than proposal-free counterparts. In this paper, we argue that propose-and-rank
approach is underestimated due to the predefined manners: 1) Hand-designed
rules are hard to guarantee the complete coverage of targeted segments. 2)
Densely sampled candidate moments cause redundant computation and degrade the
performance of ranking process. To this end, we propose a novel model termed
LPNet (Learnable Proposal Network for NLVL) with a fixed set of learnable
moment proposals. The position and length of these proposals are dynamically
adjusted during training process. Moreover, a boundary-aware loss has been
proposed to leverage frame-level information and further improve the
performance. Extensive ablations on two challenging NLVL benchmarks have
demonstrated the effectiveness of LPNet over existing state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Application of Video-to-Video Translation Networks to Computational Fluid Dynamics. (arXiv:2109.10679v1 [physics.flu-dyn])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10679">
<div class="article-summary-box-inner">
<span><p>In recent years, the evolution of artificial intelligence, especially deep
learning, has been remarkable, and its application to various fields has been
growing rapidly. In this paper, I report the results of the application of
generative adversarial networks (GANs), specifically video-to-video translation
networks, to computational fluid dynamics (CFD) simulations. The purpose of
this research is to reduce the computational cost of CFD simulations with GANs.
The architecture of GANs in this research is a combination of the
image-to-image translation networks (the so-called "pix2pix") and Long
Short-Term Memory (LSTM). It is shown that the results of high-cost and
high-accuracy simulations (with high-resolution computational grids) can be
estimated from those of low-cost and low-accuracy simulations (with
low-resolution grids). In particular, the time evolution of density
distributions in the cases of a high-resolution grid is reproduced from that in
the cases of a low-resolution grid through GANs, and the density inhomogeneity
estimated from the image generated by GANs recovers the ground truth with good
accuracy. Qualitative and quantitative comparisons of the results of the
proposed method with those of several super-resolution algorithms are also
presented.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers. (arXiv:2109.10686v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10686">
<div class="article-summary-box-inner">
<span><p>There remain many open questions pertaining to the scaling behaviour of
Transformer architectures. These scaling decisions and findings can be
critical, as training runs often come with an associated computational cost
which have both financial and/or environmental impact. The goal of this paper
is to present scaling insights from pretraining and finetuning Transformers.
While Kaplan et al. presents a comprehensive study of the scaling behaviour of
Transformer language models, the scope is only on the upstream (pretraining)
loss. Therefore, it is still unclear if these set of findings transfer to
downstream task within the context of the pretrain-finetune paradigm. The key
findings of this paper are as follows: (1) we show that aside from only the
model size, model shape matters for downstream fine-tuning, (2) scaling
protocols operate differently at different compute regions, (3) widely adopted
T5-base and T5-large sizes are Pareto-inefficient. To this end, we present
improved scaling protocols whereby our redesigned models achieve similar
downstream fine-tuning quality while having 50\% fewer parameters and training
40\% faster compared to the widely adopted T5-base model. We publicly release
over 100 pretrained checkpoints of different T5 configurations to facilitate
future research and analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding Facial Forgery Artifacts with Parts-Based Detectors. (arXiv:2109.10688v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10688">
<div class="article-summary-box-inner">
<span><p>Manipulated videos, especially those where the identity of an individual has
been modified using deep neural networks, are becoming an increasingly relevant
threat in the modern day. In this paper, we seek to develop a generalizable,
explainable solution to detecting these manipulated videos. To achieve this, we
design a series of forgery detection systems that each focus on one individual
part of the face. These parts-based detection systems, which can be combined
and used together in a single architecture, meet all of our desired criteria -
they generalize effectively between datasets and give us valuable insights into
what the network is looking at when making its decision. We thus use these
detectors to perform detailed empirical analysis on the FaceForensics++,
Celeb-DF, and Facebook Deepfake Detection Challenge datasets, examining not
just what the detectors find but also collecting and analyzing useful related
statistics on the datasets themselves.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Surface Triangulation. (arXiv:2109.10695v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10695">
<div class="article-summary-box-inner">
<span><p>Triangle meshes remain the most popular data representation for surface
geometry. This ubiquitous representation is essentially a hybrid one that
decouples continuous vertex locations from the discrete topological
triangulation. Unfortunately, the combinatorial nature of the triangulation
prevents taking derivatives over the space of possible meshings of any given
surface. As a result, to date, mesh processing and optimization techniques have
been unable to truly take advantage of modular gradient descent components of
modern optimization frameworks. In this work, we present a differentiable
surface triangulation that enables optimization for any per-vertex or per-face
differentiable objective function over the space of underlying surface
triangulations. Our method builds on the result that any 2D triangulation can
be achieved by a suitably perturbed weighted Delaunay triangulation. We
translate this result into a computational algorithm by proposing a soft
relaxation of the classical weighted Delaunay triangulation and optimizing over
vertex weights and vertex locations. We extend the algorithm to 3D by
decomposing shapes into developable sets and differentiably meshing each set
with suitable boundary constraints. We demonstrate the efficacy of our method
on various planar and surface meshes on a range of difficult-to-optimize
objective functions. Our code can be found online:
https://github.com/mrakotosaon/diff-surface-triangulation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Quantitative Comparison of Epistemic Uncertainty Maps Applied to Multi-Class Segmentation. (arXiv:2109.10702v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10702">
<div class="article-summary-box-inner">
<span><p>Uncertainty assessment has gained rapid interest in medical image analysis. A
popular technique to compute epistemic uncertainty is the Monte-Carlo (MC)
dropout technique. From a network with MC dropout and a single input, multiple
outputs can be sampled. Various methods can be used to obtain epistemic
uncertainty maps from those multiple outputs. In the case of multi-class
segmentation, the number of methods is even larger as epistemic uncertainty can
be computed voxelwise per class or voxelwise per image. This paper highlights a
systematic approach to define and quantitatively compare those methods in two
different contexts: class-specific epistemic uncertainty maps (one value per
image, voxel and class) and combined epistemic uncertainty maps (one value per
image and voxel). We applied this quantitative analysis to a multi-class
segmentation of the carotid artery lumen and vessel wall, on a multi-center,
multi-scanner, multi-sequence dataset of (MR) images. We validated our analysis
over 144 sets of hyperparameters of a model. Our main analysis considers the
relationship between the order of the voxels sorted according to their
epistemic uncertainty values and the misclassification of the prediction. Under
this consideration, the comparison of combined uncertainty maps reveals that
the multi-class entropy and the multi-class mutual information statistically
out-perform the other combined uncertainty maps under study. In a
class-specific scenario, the one-versus-all entropy statistically out-performs
the class-wise entropy, the class-wise variance and the one versus all mutual
information. The class-wise entropy statistically out-performs the other
class-specific uncertainty maps in terms of calibration. We made a python
package available to reproduce our analysis on different data and tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Plane Adjustment of Orthopedic Intra-operative Flat Panel Detector CT-Volumes. (arXiv:2109.10731v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10731">
<div class="article-summary-box-inner">
<span><p>Purpose
</p>
<p>3D acquisitions are often acquired to assess the result in orthopedic trauma
surgery. With a mobile C-Arm system, these acquisitions can be performed
intra-operatively. That reduces the number of required revision surgeries.
However, due to the operation room setup, the acquisitions typically cannot be
performed such that the acquired volumes are aligned to the anatomical regions.
Thus, the multiplanar reconstructed (MPR) planes need to be adjusted manually
during the review of the volume. In this paper, we present a detailed study of
multi-task learning (MTL) regression networks to estimate the parameters of the
MPR planes.
</p>
<p>Approach
</p>
<p>First, various mathematical descriptions for rotation, including Euler angle,
quaternion, and matrix representation, are revised. Then, three different MTL
network architectures based on the PoseNet are compared with a single task
learning network.
</p>
<p>Results
</p>
<p>Using a matrix description rather than the Euler angle description, the
accuracy of the regressed normals improves from $7.7^{\circ}$ to $7.3^{\circ}$
in the mean value for single anatomies. The multi-head approach improves the
regression of the plane position from $7.4mm$ to $6.1mm$, while the orientation
does not benefit from this approach.
</p>
<p>Conclusions
</p>
<p>The results show that a multi-head approach can lead to slightly better
results than the individual tasks networks. The most important benefit of the
MTL approach is that it is a single network for standard plane regression for
all body regions with a reduced number of stored parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Animal inspired Application of a Variant of Mel Spectrogram for Seismic Data Processing. (arXiv:2109.10733v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10733">
<div class="article-summary-box-inner">
<span><p>Predicting disaster events from seismic data is of paramount importance and
can save thousands of lives, especially in earthquake-prone areas and
habitations around volcanic craters. The drastic rise in the number of seismic
monitoring stations in recent years has allowed the collection of a huge
quantity of data, outpacing the capacity of seismologists. Due to the complex
nature of the seismological data, it is often difficult for seismologists to
detect subtle patterns with major implications. Machine learning algorithms
have been demonstrated to be effective in classification and prediction tasks
for seismic data. It has been widely known that some animals can sense
disasters like earthquakes from seismic signals well before the disaster
strikes. Mel spectrogram has been widely used for speech recognition as it
scales the actual frequencies according to human hearing. In this paper, we
propose a variant of the Mel spectrogram to scale the raw frequencies of
seismic data to the hearing of such animals that can sense disasters from
seismic signals. We are using a Computer vision algorithm along with clustering
that allows for the classification of unlabelled seismic data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DyStyle: Dynamic Neural Network for Multi-Attribute-Conditioned Style Editing. (arXiv:2109.10737v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10737">
<div class="article-summary-box-inner">
<span><p>Great diversity and photorealism have been achieved by unconditional GAN
frameworks such as StyleGAN and its variations. In the meantime, persistent
efforts have been made to enhance the semantic controllability of StyleGANs.
For example, a dozen of style manipulation methods have been recently proposed
to perform attribute-conditioned style editing. Although some of these methods
work well in manipulating the style codes along one attribute, the control
accuracy when jointly manipulating multiple attributes tends to be problematic.
To address these limitations, we propose a Dynamic Style Manipulation Network
(DyStyle) whose structure and parameters vary by input samples, to perform
nonlinear and adaptive manipulation of latent codes for flexible and precise
attribute control. Additionally, a novel easy-to-hard training procedure is
introduced for efficient and stable training of the DyStyle network. Extensive
experiments have been conducted on faces and other objects. As a result, our
approach demonstrates fine-grained disentangled edits along multiple numeric
and binary attributes. Qualitative and quantitative comparisons with existing
style manipulation methods verify the superiority of our method in terms of the
attribute control accuracy and identity preservation without compromising the
photorealism. The advantage of our method is even more significant for joint
multi-attribute control. The source codes are made publicly available at
\href{https://github.com/phycvgan/DyStyle}{phycvgan/DyStyle}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Early Lane Change Prediction for Automated Driving Systems Using Multi-Task Attention-based Convolutional Neural Networks. (arXiv:2109.10742v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10742">
<div class="article-summary-box-inner">
<span><p>Lane change (LC) is one of the safety-critical manoeuvres in highway driving
according to various road accident records. Thus, reliably predicting such
manoeuvre in advance is critical for the safe and comfortable operation of
automated driving systems. The majority of previous studies rely on detecting a
manoeuvre that has been already started, rather than predicting the manoeuvre
in advance. Furthermore, most of the previous works do not estimate the key
timings of the manoeuvre (e.g., crossing time), which can actually yield more
useful information for the decision making in the ego vehicle. To address these
shortcomings, this paper proposes a novel multi-task model to simultaneously
estimate the likelihood of LC manoeuvres and the time-to-lane-change (TTLC). In
both tasks, an attention-based convolutional neural network (CNN) is used as a
shared feature extractor from a bird's eye view representation of the driving
environment. The spatial attention used in the CNN model improves the feature
extraction process by focusing on the most relevant areas of the surrounding
environment. In addition, two novel curriculum learning schemes are employed to
train the proposed approach. The extensive evaluation and comparative analysis
of the proposed method in existing benchmark datasets show that the proposed
method outperforms state-of-the-art LC prediction models, particularly
considering long-term prediction performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FaceEraser: Removing Facial Parts for Augmented Reality. (arXiv:2109.10760v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10760">
<div class="article-summary-box-inner">
<span><p>Our task is to remove all facial parts (e.g., eyebrows, eyes, mouth and
nose), and then impose visual elements onto the ``blank'' face for augmented
reality. Conventional object removal methods rely on image inpainting
techniques (e.g., EdgeConnect, HiFill) that are trained in a self-supervised
manner with randomly manipulated image pairs. Specifically, given a set of
natural images, randomly masked images are used as inputs and the raw images
are treated as ground truths. Whereas, this technique does not satisfy the
requirements of facial parts removal, as it is hard to obtain ``ground-truth''
images with real ``blank'' faces. To address this issue, we propose a novel
data generation technique to produce paired training data that well mimic the
``blank'' faces. In the mean time, we propose a novel network architecture for
improved inpainting quality for our task. Finally, we demonstrate various
face-oriented augmented reality applications on top of our facial parts removal
model. Our method has been integrated into commercial products and its
effectiveness has been verified with unconstrained user inputs. The source
codes, pre-trained models and training data will be released for research
purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HybridSDF: Combining Free Form Shapes and Geometric Primitives for effective Shape Manipulation. (arXiv:2109.10767v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10767">
<div class="article-summary-box-inner">
<span><p>CAD modeling typically involves the use of simple geometric primitives
whereas recent advances in deep-learning based 3D surface modeling have opened
new shape design avenues. Unfortunately, these advances have not yet been
accepted by the CAD community because they cannot be integrated into
engineering workflows. To remedy this, we propose a novel approach to
effectively combining geometric primitives and free-form surfaces represented
by implicit surfaces for accurate modeling that preserves interpretability,
enforces consistency, and enables easy manipulation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Variational Clustering Framework for Self-labeling of Large-scale Medical Images. (arXiv:2109.10777v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10777">
<div class="article-summary-box-inner">
<span><p>We propose a Deep Variational Clustering (DVC) framework for unsupervised
representation learning and clustering of large-scale medical images. DVC
simultaneously learns the multivariate Gaussian posterior through the
probabilistic convolutional encoder and the likelihood distribution with the
probabilistic convolutional decoder; and optimizes cluster labels assignment.
Here, the learned multivariate Gaussian posterior captures the latent
distribution of a large set of unlabeled images. Then, we perform unsupervised
clustering on top of the variational latent space using a clustering loss. In
this approach, the probabilistic decoder helps to prevent the distortion of
data points in the latent space and to preserve the local structure of data
generating distribution. The training process can be considered as a
self-training process to refine the latent space and simultaneously optimizing
cluster assignments iteratively. We evaluated our proposed framework on three
public datasets that represented different medical imaging modalities. Our
experimental results show that our proposed framework generalizes better across
different datasets. It achieves compelling results on several medical imaging
benchmarks. Thus, our approach offers potential advantages over conventional
deep unsupervised learning in real-world applications. The source code of the
method and all the experiments are available publicly at:
https://github.com/csfarzin/DVC
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label Cleaning Multiple Instance Learning: Refining Coarse Annotations on Single Whole-Slide Images. (arXiv:2109.10778v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10778">
<div class="article-summary-box-inner">
<span><p>Annotating cancerous regions in whole-slide images (WSIs) of pathology
samples plays a critical role in clinical diagnosis, biomedical research, and
machine learning algorithms development. However, generating exhaustive and
accurate annotations is labor-intensive, challenging, and costly. Drawing only
coarse and approximate annotations is a much easier task, less costly, and it
alleviates pathologists' workload. In this paper, we study the problem of
refining these approximate annotations in digital pathology to obtain more
accurate ones. Some previous works have explored obtaining machine learning
models from these inaccurate annotations, but few of them tackle the refinement
problem where the mislabeled regions should be explicitly identified and
corrected, and all of them require a - often very large - number of training
samples. We present a method, named Label Cleaning Multiple Instance Learning
(LC-MIL), to refine coarse annotations on a single WSI without the need of
external training data. Patches cropped from a WSI with inaccurate labels are
processed jointly with a MIL framework, and a deep-attention mechanism is
leveraged to discriminate mislabeled instances, mitigating their impact on the
predictive model and refining the segmentation. Our experiments on a
heterogeneous WSI set with breast cancer lymph node metastasis, liver cancer,
and colorectal cancer samples show that LC-MIL significantly refines the coarse
annotations, outperforming the state-of-the-art alternatives, even while
learning from a single slide. These results demonstrate the LC-MIL is a
promising, lightweight tool to provide fine-grained annotations from coarsely
annotated pathology sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural network relief: a pruning algorithm based on neural activity. (arXiv:2109.10795v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10795">
<div class="article-summary-box-inner">
<span><p>Current deep neural networks (DNNs) are overparameterized and use most of
their neuronal connections during inference for each task. The human brain,
however, developed specialized regions for different tasks and performs
inference with a small fraction of its neuronal connections. We propose an
iterative pruning strategy introducing a simple importance-score metric that
deactivates unimportant connections, tackling overparameterization in DNNs and
modulating the firing patterns. The aim is to find the smallest number of
connections that is still capable of solving a given task with comparable
accuracy, i.e. a simpler subnetwork. We achieve comparable performance for
LeNet architectures on MNIST, and significantly higher parameter compression
than state-of-the-art algorithms for VGG and ResNet architectures on
CIFAR-10/100 and Tiny-ImageNet. Our approach also performs well for the two
different optimizers considered -- Adam and SGD. The algorithm is not designed
to minimize FLOPs when considering current hardware and software
implementations, although it performs reasonably when compared to the state of
the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DVC-P: Deep Video Compression with Perceptual Optimizations. (arXiv:2109.10849v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10849">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed the significant development of learning-based
video compression methods, which aim at optimizing objective or perceptual
quality and bit rates. In this paper, we introduce deep video compression with
perceptual optimizations (DVC-P), which aims at increasing perceptual quality
of decoded videos. Our proposed DVC-P is based on Deep Video Compression (DVC)
network, but improves it with perceptual optimizations. Specifically, a
discriminator network and a mixed loss are employed to help our network trade
off among distortion, perception and rate. Furthermore, nearest-neighbor
interpolation is used to eliminate checkerboard artifacts which can appear in
sequences encoded with DVC frameworks. Thanks to these two improvements, the
perceptual quality of decoded sequences is improved. Experimental results
demonstrate that, compared with the baseline DVC, our proposed method can
generate videos with higher perceptual quality achieving 12.27% reduction in a
perceptual BD-rate equivalent, on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pix2seq: A Language Modeling Framework for Object Detection. (arXiv:2109.10852v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10852">
<div class="article-summary-box-inner">
<span><p>This paper presents Pix2Seq, a simple and generic framework for object
detection. Unlike existing approaches that explicitly integrate prior knowledge
about the task, we simply cast object detection as a language modeling task
conditioned on the observed pixel inputs. Object descriptions (e.g., bounding
boxes and class labels) are expressed as sequences of discrete tokens, and we
train a neural net to perceive the image and generate the desired sequence. Our
approach is based mainly on the intuition that if a neural net knows about
where and what the objects are, we just need to teach it how to read them out.
Beyond the use of task-specific data augmentations, our approach makes minimal
assumptions about the task, yet it achieves competitive results on the
challenging COCO dataset, compared to highly specialized and well optimized
detection algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Batch norm with entropic regularization turns deterministic autoencoders into generative models. (arXiv:2002.10631v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.10631">
<div class="article-summary-box-inner">
<span><p>The variational autoencoder is a well defined deep generative model that
utilizes an encoder-decoder framework where an encoding neural network outputs
a non-deterministic code for reconstructing an input. The encoder achieves this
by sampling from a distribution for every input, instead of outputting a
deterministic code per input. The great advantage of this process is that it
allows the use of the network as a generative model for sampling from the data
distribution beyond provided samples for training. We show in this work that
utilizing batch normalization as a source for non-determinism suffices to turn
deterministic autoencoders into generative models on par with variational ones,
so long as we add a suitable entropic regularization to the training objective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tighter risk certificates for neural networks. (arXiv:2007.12911v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.12911">
<div class="article-summary-box-inner">
<span><p>This paper presents an empirical study regarding training probabilistic
neural networks using training objectives derived from PAC-Bayes bounds. In the
context of probabilistic neural networks, the output of training is a
probability distribution over network weights. We present two training
objectives, used here for the first time in connection with training neural
networks. These two training objectives are derived from tight PAC-Bayes
bounds. We also re-implement a previously used training objective based on a
classical PAC-Bayes bound, to compare the properties of the predictors learned
using the different training objectives. We compute risk certificates for the
learnt predictors, based on part of the data used to learn the predictors. We
further experiment with different types of priors on the weights (both
data-free and data-dependent priors) and neural network architectures. Our
experiments on MNIST and CIFAR-10 show that our training methods produce
competitive test set errors and non-vacuous risk bounds with much tighter
values than previous results in the literature, showing promise not only to
guide the learning algorithm through bounding the risk but also for model
selection. These observations suggest that the methods studied here might be
good candidates for self-certified learning, in the sense of using the whole
data set for learning a predictor and certifying its risk on any unseen data
(from the same distribution as the training data) potentially without the need
for holding out test data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Detection of Aedes aegypti Breeding Grounds Based on Deep Networks with Spatio-Temporal Consistency. (arXiv:2007.14863v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.14863">
<div class="article-summary-box-inner">
<span><p>Every year, the Aedes aegypti mosquito infects millions of people with
diseases such as dengue, zika, chikungunya, and urban yellow fever. The main
form to combat these diseases is to avoid mosquito reproduction by searching
for and eliminating the potential mosquito breeding grounds. In this work, we
introduce a comprehensive dataset of aerial videos, acquired with an unmanned
aerial vehicle, containing possible mosquito breeding sites. All frames of the
video dataset were manually annotated with bounding boxes identifying all
objects of interest. This dataset was employed to develop an automatic
detection system of such objects based on deep convolutional networks. We
propose the exploitation of the temporal information contained in the videos by
the incorporation, in the object detection pipeline, of a spatio-temporal
consistency module that can register the detected objects, minimizing most
false-positive and false-negative occurrences. Using the ResNet-50-FPN as a
backbone, we achieve F$_1$-scores of 0.65 and 0.77 on the object-level
detection of `tires' and `water tanks', respectively, illustrating the system
capabilities to properly locate potential mosquito breeding objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Model Compression via Stage-wise Pruning. (arXiv:2011.04908v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.04908">
<div class="article-summary-box-inner">
<span><p>Automated Machine Learning(Auto-ML) pruning methods aim at searching a
pruning strategy automatically to reduce the computational complexity of deep
Convolutional Neural Networks(deep CNNs). However, some previous work found
that the results of many Auto-ML pruning methods cannot even surpass the
results of the uniformly pruning method. In this paper, the ineffectiveness of
Auto-ML pruning which is caused by unfull and unfair training of the supernet
is shown. A deep supernet suffers from unfull training because it contains too
many candidates. To overcome the unfull training, a stage-wise pruning(SWP)
method is proposed, which splits a deep supernet into several stage-wise
supernets to reduce the candidate number and utilize inplace distillation to
supervise the stage training. Besides, A wide supernet is hit by unfair
training since the sampling probability of each channel is unequal. Therefore,
the fullnet and the tinynet are sampled in each training iteration to ensure
each channel can be overtrained. Remarkably, the proxy performance of the
subnets trained with SWP is closer to the actual performance than that of most
of the previous Auto-ML pruning work. Experiments show that SWP achieves the
state-of-the-art on both CIFAR-10 and ImageNet under the mobile setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PSGCNet: A Pyramidal Scale and Global Context Guided Network for Dense Object Counting in Remote Sensing Images. (arXiv:2012.03597v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.03597">
<div class="article-summary-box-inner">
<span><p>Object counting, which aims to count the accurate number of object instances
in images, has been attracting more and more attention. However, challenges
such as large scale variation, complex background interference, and non-uniform
density distribution greatly limit the counting accuracy, particularly striking
in remote sensing imagery. To mitigate the above issues, this paper proposes a
novel framework for dense object counting in remote sensing images, which
incorporates a pyramidal scale module (PSM) and a global context module (GCM),
dubbed PSGCNet, where PSM is used to adaptively capture multi-scale information
and GCM is to guide the model to select suitable scales generated from PSM.
Moreover, a reliable supervision manner improved from Bayesian and Counting
loss (BCL) is utilized to learn the density probability and then compute the
count expectation at each annotation. It can relieve non-uniform density
distribution to a certain extent. Extensive experiments on four remote sensing
counting datasets demonstrate the effectiveness of the proposed method and the
superiority of it compared with state-of-the-arts. Additionally, experiments
extended on four commonly used crowd counting datasets further validate the
generalization ability of the model. Code is available at
https://github.com/gaoguangshuai/PSGCNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter Efficient Multimodal Transformers for Video Representation Learning. (arXiv:2012.04124v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04124">
<div class="article-summary-box-inner">
<span><p>The recent success of Transformers in the language domain has motivated
adapting it to a multimodal setting, where a new visual model is trained in
tandem with an already pretrained language model. However, due to the excessive
memory requirements from Transformers, existing work typically fixes the
language model and train only the vision module, which limits its ability to
learn cross-modal information in an end-to-end manner. In this work, we focus
on reducing the parameters of multimodal Transformers in the context of
audio-visual video representation learning. We alleviate the high memory
requirement by sharing the parameters of Transformers across layers and
modalities; we decompose the Transformer into modality-specific and
modality-shared parts so that the model learns the dynamics of each modality
both individually and together, and propose a novel parameter sharing scheme
based on low-rank approximation. We show that our approach reduces parameters
of the Transformers up to 97$\%$, allowing us to train our model end-to-end
from scratch. We also propose a negative sampling approach based on an instance
similarity measured on the CNN embedding space that our model learns together
with the Transformers. To demonstrate our approach, we pretrain our model on
30-second clips (480 frames) from Kinetics-700 and transfer it to audio-visual
classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Closer Look at Temporal Sentence Grounding in Videos: Dataset and Metric. (arXiv:2101.09028v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.09028">
<div class="article-summary-box-inner">
<span><p>Temporal Sentence Grounding in Videos (TSGV), i.e., grounding a natural
language sentence which indicates complex human activities in a long and
untrimmed video sequence, has received unprecedented attentions over the last
few years. Although each newly proposed method plausibly can achieve better
performance than previous ones, current TSGV models still tend to capture the
moment annotation biases and fail to take full advantage of multi-modal inputs.
Even more incredibly, several extremely simple baselines without training can
also achieve state-of-the-art performance. In this paper, we take a closer look
at the existing evaluation protocols for TSGV, and find that both the
prevailing dataset splits and evaluation metrics are the devils to cause
unreliable benchmarking. To this end, we propose to re-organize two widely-used
TSGV benchmarks (ActivityNet Captions and Charades-STA). Specifically, we
deliberately make the ground-truth moment distribution different in the
training and test splits, i.e., out-of-distribution (OOD) testing. Meanwhile,
we introduce a new evaluation metric dR@n,IoU@m to calibrate the basic IoU
scores by penalizing on the bias-influenced moment predictions and alleviate
the inflating evaluations caused by the dataset annotation biases such as
overlong ground-truth moments. Under our new evaluation protocol, we conduct
extensive experiments and ablation studies on eight state-of-the-art TSGV
methods. All the results demonstrate that the re-organized dataset splits and
new metric can better monitor the progress in TSGV. Our reorganized datsets are
available at https://github.com/yytzsy/grounding_changing_distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mask Guided Attention For Fine-Grained Patchy Image Classification. (arXiv:2102.02771v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.02771">
<div class="article-summary-box-inner">
<span><p>In this work, we present a novel mask guided attention (MGA) method for
fine-grained patchy image classification. The key challenge of fine-grained
patchy image classification lies in two folds, ultra-fine-grained
inter-category variances among objects and very few data available for
training. This motivates us to consider employing more useful supervision
signal to train a discriminative model within limited training samples.
Specifically, the proposed MGA integrates a pre-trained semantic segmentation
model that produces auxiliary supervision signal, i.e., patchy attention mask,
enabling a discriminative representation learning. The patchy attention mask
drives the classifier to filter out the insignificant parts of images (e.g.,
common features between different categories), which enhances the robustness of
MGA for the fine-grained patchy image classification. We verify the
effectiveness of our method on three publicly available patchy image datasets.
Experimental results demonstrate that our MGA method achieves superior
performance on three datasets compared with the state-of-the-art methods. In
addition, our ablation study shows that MGA improves the accuracy by 2.25% and
2% on the SoyCultivarVein and BtfPIS datasets, indicating its practicality
towards solving the fine-grained patchy image classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Mean Teacher for Semi-supervised Chest X-ray Classification. (arXiv:2103.03629v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03629">
<div class="article-summary-box-inner">
<span><p>The training of deep learning models generally requires a large amount of
annotated data for effective convergence and generalisation. However, obtaining
high-quality annotations is a laboursome and expensive process due to the need
of expert radiologists for the labelling task. The study of semi-supervised
learning in medical image analysis is then of crucial importance given that it
is much less expensive to obtain unlabelled images than to acquire images
labelled by expert radiologists. Essentially, semi-supervised methods leverage
large sets of unlabelled data to enable better training convergence and
generalisation than using only the small set of labelled images. In this paper,
we propose Self-supervised Mean Teacher for Semi-supervised (S$^2$MTS$^2$)
learning that combines self-supervised mean-teacher pre-training with
semi-supervised fine-tuning. The main innovation of S$^2$MTS$^2$ is the
self-supervised mean-teacher pre-training based on the joint contrastive
learning, which uses an infinite number of pairs of positive query and key
features to improve the mean-teacher representation. The model is then
fine-tuned using the exponential moving average teacher framework trained with
semi-supervised learning. We validate S$^2$MTS$^2$ on the multi-label
classification problems from Chest X-ray14 and CheXpert, and the multi-class
classification from ISIC2018, where we show that it outperforms the previous
SOTA semi-supervised learning methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Attacks on Camera-LiDAR Models for 3D Car Detection. (arXiv:2103.09448v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09448">
<div class="article-summary-box-inner">
<span><p>Most autonomous vehicles (AVs) rely on LiDAR and RGB camera sensors for
perception. Using these point cloud and image data, perception models based on
deep neural nets (DNNs) have achieved state-of-the-art performance in 3D
detection. The vulnerability of DNNs to adversarial attacks has been heavily
investigated in the RGB image domain and more recently in the point cloud
domain, but rarely in both domains simultaneously. Multi-modal perception
systems used in AVs can be divided into two broad types: cascaded models which
use each modality independently, and fusion models which learn from different
modalities simultaneously. We propose a universal and physically realizable
adversarial attack for each type, and study and contrast their respective
vulnerabilities to attacks. We place a single adversarial object with specific
shape and texture on top of a car with the objective of making this car evade
detection. Evaluating on the popular KITTI benchmark, our adversarial object
made the host vehicle escape detection by each model type more than 50% of the
time. The dense RGB input contributed more to the success of the adversarial
attacks on both cascaded and fusion models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimising the selection of samples for robust lidar camera calibration. (arXiv:2103.12287v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12287">
<div class="article-summary-box-inner">
<span><p>We propose a robust calibration pipeline that optimises the selection of
calibration samples for the estimation of calibration parameters that fit the
entire scene. We minimise user error by automating the data selection process
according to a metric, called Variability of Quality (VOQ) that gives a score
to each calibration set of samples. We show that this VOQ score is correlated
with the estimated calibration parameter's ability to generalise well to the
entire scene, thereby overcoming the overfitting problems of existing
calibration algorithms. Our approach has the benefits of simplifying the
calibration process for practitioners of any calibration expertise level and
providing an objective measure of the quality for our calibration pipeline's
input and output data. We additionally use a novel method of assessing the
accuracy of the calibration parameters. It involves computing reprojection
errors for the entire scene to ensure that the parameters are well fitted to
all features in the scene. Our proposed calibration pipeline takes 90s, and
obtains an average reprojection error of 1-1.2cm, with standard deviation of
0.4-0.5cm over 46 poses evenly distributed in a scene. This process has been
validated by experimentation on a high resolution, software definable lidar,
Baraja Spectrum-Scan; and a low, fixed resolution lidar, Velodyne VLP-16. We
have shown that despite the vast differences in lidar technologies, our
proposed approach manages to estimate robust calibration parameters for both.
Our code and data set used for this paper are made available as open-source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media. (arXiv:2104.05893v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05893">
<div class="article-summary-box-inner">
<span><p>Online misinformation is a prevalent societal issue, with adversaries relying
on tools ranging from cheap fakes to sophisticated deep fakes. We are motivated
by the threat scenario where an image is used out of context to support a
certain narrative. While some prior datasets for detecting image-text
inconsistency generate samples via text manipulation, we propose a dataset
where both image and text are unmanipulated but mismatched. We introduce
several strategies for automatically retrieving convincing images for a given
caption, capturing cases with inconsistent entities or semantic context. Our
large-scale automatically generated NewsCLIPpings Dataset: (1) demonstrates
that machine-driven image repurposing is now a realistic threat, and (2)
provides samples that represent challenging instances of mismatch between text
and image in news that are able to mislead humans. We benchmark several
state-of-the-art multimodal models on our dataset and analyze their performance
across different pretraining domains and visual backbones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video-based Person Re-identification without Bells and Whistles. (arXiv:2105.10678v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10678">
<div class="article-summary-box-inner">
<span><p>Video-based person re-identification (Re-ID) aims at matching the video
tracklets with cropped video frames for identifying the pedestrians under
different cameras. However, there exists severe spatial and temporal
misalignment for those cropped tracklets due to the imperfect detection and
tracking results generated with obsolete methods. To address this issue, we
present a simple re-Detect and Link (DL) module which can effectively reduce
those unexpected noise through applying the deep learning-based detection and
tracking on the cropped tracklets. Furthermore, we introduce an improved model
called Coarse-to-Fine Axial-Attention Network (CF-AAN). Based on the typical
Non-local Network, we replace the non-local module with three 1-D
position-sensitive axial attentions, in addition to our proposed coarse-to-fine
structure. With the developed CF-AAN, compared to the original non-local
operation, we can not only significantly reduce the computation cost but also
obtain the state-of-the-art performance (91.3% in rank-1 and 86.5% in mAP) on
the large-scale MARS dataset. Meanwhile, by simply adopting our DL module for
data alignment, to our surprise, several baseline models can achieve better or
comparable results with the current state-of-the-arts. Besides, we discover the
errors not only for the identity labels of tracklets but also for the
evaluation protocol for the test data of MARS. We hope that our work can help
the community for the further development of invariant representation without
the hassle of the spatial and temporal alignment and dataset noise. The code,
corrected labels, evaluation protocol, and the aligned data will be available
at https://github.com/jackie840129/CF-AAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAT: 2D Semantics Assisted Training for 3D Visual Grounding. (arXiv:2105.11450v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11450">
<div class="article-summary-box-inner">
<span><p>3D visual grounding aims at grounding a natural language description about a
3D scene, usually represented in the form of 3D point clouds, to the targeted
object region. Point clouds are sparse, noisy, and contain limited semantic
information compared with 2D images. These inherent limitations make the 3D
visual grounding problem more challenging. In this study, we propose 2D
Semantics Assisted Training (SAT) that utilizes 2D image semantics in the
training stage to ease point-cloud-language joint representation learning and
assist 3D visual grounding. The main idea is to learn auxiliary alignments
between rich, clean 2D object representations and the corresponding objects or
mentioned entities in 3D scenes. SAT takes 2D object semantics, i.e., object
label, image feature, and 2D geometric feature, as the extra input in training
but does not require such inputs during inference. By effectively utilizing 2D
semantics in training, our approach boosts the accuracy on the Nr3D dataset
from 37.7% to 49.2%, which significantly surpasses the non-SAT baseline with
the identical network architecture and inference input. Our approach
outperforms the state of the art by large margins on multiple 3D visual
grounding datasets, i.e., +10.4% absolute accuracy on Nr3D, +9.9% on Sr3D, and
+5.6% on ScanRef.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Attention Free Transformer. (arXiv:2105.14103v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14103">
<div class="article-summary-box-inner">
<span><p>We introduce Attention Free Transformer (AFT), an efficient variant of
Transformers that eliminates the need for dot product self attention. In an AFT
layer, the key and value are first combined with a set of learned position
biases, the result of which is multiplied with the query in an element-wise
fashion. This new operation has a memory complexity linear w.r.t. both the
context size and the dimension of features, making it compatible to both large
input and model sizes. We also introduce AFT-local and AFT-conv, two model
variants that take advantage of the idea of locality and spatial weight sharing
while maintaining global connectivity. We conduct extensive experiments on two
autoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image
recognition task (ImageNet-1K classification). We show that AFT demonstrates
competitive performance on all the benchmarks, while providing excellent
efficiency at the same time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anticipative Video Transformer. (arXiv:2106.02036v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02036">
<div class="article-summary-box-inner">
<span><p>We propose Anticipative Video Transformer (AVT), an end-to-end
attention-based video modeling architecture that attends to the previously
observed video in order to anticipate future actions. We train the model
jointly to predict the next action in a video sequence, while also learning
frame feature encoders that are predictive of successive future frames'
features. Compared to existing temporal aggregation strategies, AVT has the
advantage of both maintaining the sequential progression of observed actions
while still capturing long-range dependencies--both critical for the
anticipation task. Through extensive experiments, we show that AVT obtains the
best reported performance on four popular action anticipation benchmarks:
EpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and 50-Salads; and it wins
first place in the EpicKitchens-100 CVPR'21 challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GasHisSDB: A New Gastric Histopathology Image Dataset for Computer Aided Diagnosis of Gastric Cancer. (arXiv:2106.02473v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02473">
<div class="article-summary-box-inner">
<span><p>Gastric cancer has turned out to be the fifth most common cancer globally,
and early detection of gastric cancer is essential to save lives.
Histopathological examination of gastric cancer is the gold standard for the
diagnosis of gastric cancer. However, computer-aided diagnostic techniques are
challenging to evaluate due to the scarcity of publicly available gastric
histopathology image datasets.In this paper, a noble publicly available Gastric
Histopathology Sub-size Image Database (GasHisSDB) is published to identify
classifiers' performance. Specifically, two types of data are included: normal
and abnormal, with a total of 245,196 tissue case images.This study also
performed extensive experiments using traditional machine learning and deep
learning methods to prove that the methods of different periods have
discrepancies on GasHisSDB. To the best of our knowledge, it is the first
publicly available gastric cancer histopathology dataset containing a large
number of images for weakly supervised learning. We believe that GasHisSDB can
attract researchers to explore new algorithms for the automated diagnosis of
gastric cancer, which can help physicians and patients in the clinical setting.
GasHisSDB is available at the URL:https://gitee.com/neuhwm/GasHisSDB.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EAR-NET: Error Attention Refining Network For Retinal Vessel Segmentation. (arXiv:2107.01351v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01351">
<div class="article-summary-box-inner">
<span><p>The precise detection of blood vessels in retinal images is crucial to the
early diagnosis of the retinal vascular diseases, e.g., diabetic, hypertensive
and solar retinopathies. Existing works often fail in predicting the abnormal
areas, e.g, sudden brighter and darker areas and are inclined to predict a
pixel to background due to the significant class imbalance, leading to high
accuracy and specificity while low sensitivity. To that end, we propose a novel
error attention refining network (ERA-Net) that is capable of learning and
predicting the potential false predictions in a two-stage manner for effective
retinal vessel segmentation. The proposed ERA-Net in the refine stage drives
the model to focus on and refine the segmentation errors produced in the
initial training stage. To achieve this, unlike most previous attention
approaches that run in an unsupervised manner, we introduce a novel error
attention mechanism which considers the differences between the ground truth
and the initial segmentation masks as the ground truth to supervise the
attention map learning. Experimental results demonstrate that our method
achieves state-of-the-art performance on two common retinal blood vessel
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TA2N: Two-Stage Action Alignment Network for Few-shot Action Recognition. (arXiv:2107.04782v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.04782">
<div class="article-summary-box-inner">
<span><p>Few-shot action recognition aims to recognize novel action classes (query)
using just a few samples (support). The majority of current approaches follow
the metric learning paradigm, which learns to compare the similarity between
videos. Recently, it has been observed that directly measuring this similarity
is not ideal since different action instances may show distinctive temporal
distribution, resulting in severe misalignment issues across query and support
videos. In this paper, we arrest this problem from two distinct aspects --
action duration misalignment and action evolution misalignment. We address them
sequentially through a Two-stage Action Alignment Network (TA2N). The first
stage locates the action by learning a temporal affine transform, which warps
each video feature to its action duration while dismissing the
action-irrelevant feature (e.g. background). Next, the second stage coordinates
query feature to match the spatial-temporal action evolution of support by
performing temporally rearrange and spatially offset prediction. Extensive
experiments on benchmark datasets show the potential of the proposed method in
achieving state-of-the-art performance for few-shot action recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Learning with Global Relatedness Decoupled-Distillation. (arXiv:2107.05583v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05583">
<div class="article-summary-box-inner">
<span><p>Despite the success that metric learning based approaches have achieved in
few-shot learning, recent works reveal the ineffectiveness of their episodic
training mode. In this paper, we point out two potential reasons for this
problem: 1) the random episodic labels can only provide limited supervision
information, while the relatedness information between the query and support
samples is not fully exploited; 2) the meta-learner is usually constrained by
the limited contextual information of the local episode. To overcome these
problems, we propose a new Global Relatedness Decoupled-Distillation (GRDD)
method using the global category knowledge and the Relatedness
Decoupled-Distillation (RDD) strategy. Our GRDD learns new visual concepts
quickly by imitating the habit of humans, i.e. learning from the deep knowledge
distilled from the teacher. More specifically, we first train a global learner
on the entire base subset using category labels as supervision to leverage the
global context information of the categories. Then, the well-trained global
learner is used to simulate the query-support relatedness in global
dependencies. Finally, the distilled global query-support relatedness is
explicitly used to train the meta-learner using the RDD strategy, with the goal
of making the meta-learner more discriminative. The RDD strategy aims to
decouple the dense query-support relatedness into the groups of sparse
decoupled relatedness. Moreover, only the relatedness of a single support
sample with other query samples is considered in each group. By distilling the
sparse decoupled relatedness group by group, sharper relatedness can be
effectively distilled to the meta-learner, thereby facilitating the learning of
a discriminative meta-learner. We conduct extensive experiments on the
miniImagenet and CIFAR-FS datasets, which show the state-of-the-art performance
of our GRDD method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PVT: Point-Voxel Transformer for 3D Deep Learning. (arXiv:2108.06076v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06076">
<div class="article-summary-box-inner">
<span><p>In this paper, we present an efficient and high-performance neural
architecture, termed Point-Voxel Transformer (PVT)for 3D deep learning, which
deeply integrates both 3D voxel-based and point-based self-attention
computation to learn more discriminative features from 3D data. Specifically,
we conduct multi-head self-attention (MSA) computation in voxels to obtain the
efficient learning pattern and the coarse-grained local features while
performing self-attention in points to provide finer-grained information about
the global context. In addition, to reduce the cost of MSA computation with
high efficiency, we design a cyclic shifted boxing scheme by limiting the MSA
computation to non-overlapping local box and also preserving cross-box
connection. Evaluated on classification benchmark, our method not only achieves
state-of-the-art accuracy of 94.0% (no voting) but outperforms previous
Transformer-based models with 7x measured speedup on average. On part and
semantic segmentation, our model also obtains strong performance(86.5% and
68.2% mIoU, respectively). For 3D object detection task, we replace the
primitives in Frustrum PointNet with PVT block and achieve an improvement of
8.6% AP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Continual Learning with Natural Distribution Shifts: An Empirical Study with Visual Data. (arXiv:2108.09020v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09020">
<div class="article-summary-box-inner">
<span><p>Continual learning is the problem of learning and retaining knowledge through
time over multiple tasks and environments. Research has primarily focused on
the incremental classification setting, where new tasks/classes are added at
discrete time intervals. Such an "offline" setting does not evaluate the
ability of agents to learn effectively and efficiently, since an agent can
perform multiple learning epochs without any time limitation when a task is
added. We argue that "online" continual learning, where data is a single
continuous stream without task boundaries, enables evaluating both information
retention and online learning efficacy. In online continual learning, each
incoming small batch of data is first used for testing and then added to the
training set, making the problem truly online. Trained models are later
evaluated on historical data to assess information retention. We introduce a
new benchmark for online continual visual learning that exhibits large scale
and natural distribution shifts. Through a large-scale analysis, we identify
critical and previously unobserved phenomena of gradient-based optimization in
continual learning, and propose effective strategies for improving
gradient-based online continual learning with real data. The source code and
dataset are available in: https://github.com/IntelLabs/continuallearning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">External Knowledge enabled Text Visual Question Answering. (arXiv:2108.09717v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09717">
<div class="article-summary-box-inner">
<span><p>The open-ended question answering task of Text-VQA requires reading and
reasoning about local, often previously unseen, scene-text content of an image
to generate answers. In this work, we propose the generalized use of external
knowledge to augment our understanding of the said scene-text. We design a
framework to extract, validate, and reason with knowledge using a standard
multimodal transformer for vision language understanding tasks. Through
empirical evidence and qualitative results, we demonstrate how external
knowledge can highlight instance-only cues and thus help deal with training
data bias, improve answer entity type correctness, and detect multiword named
entities. We generate results comparable to the state-of-the-art on two
publicly available datasets, under the constraints of similar upstream OCR
systems and training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Transformer-based Semantic Segmentation Networks for Pathological Image Segmentation. (arXiv:2108.11993v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11993">
<div class="article-summary-box-inner">
<span><p>Histopathology has played an essential role in cancer diagnosis. With the
rapid advances in convolutional neural networks (CNN). Various CNN-based
automated pathological image segmentation approaches have been developed in
computer-assisted pathological image analysis. In the past few years,
Transformer neural networks (Transformer) have shown the unique merit of
capturing the global long-distance dependencies across the entire image as a
new deep learning paradigm. Such merit is appealing for exploring spatially
heterogeneous pathological images. However, there have been very few, if any,
studies that have systematically evaluated the current Transformer-based
approaches in pathological image segmentation. To assess the performance of
Transformer segmentation models on whole slide images (WSI), we quantitatively
evaluated six prevalent transformer-based models on tumor segmentation, using
the widely used PAIP liver histopathological dataset. For a more comprehensive
analysis, we also compare the transformer-based models with six major
traditional CNN-based models. The results show that the Transformer-based
models exhibit a general superior performance over the CNN-based models. In
particular, Segmenter, Swin-Transformer and TransUNet-all
transformer-based-came out as the best performers among the twelve evaluated
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00590">
<div class="article-summary-box-inner">
<span><p>Web search is fundamentally multimodal and multihop. Often, even before
asking a question we choose to go directly to image search to find our answers.
Further, rarely do we find an answer from a single source but aggregate
information and reason through implications. Despite the frequency of this
everyday occurrence, at present, there is no unified question answering
benchmark that requires a single model to answer long-form natural language
questions from text and open-ended visual sources -- akin to a human's
experience. We propose to bridge this gap between the natural language and
computer vision communities with WebQA. We show that A. our multihop text
queries are difficult for a large-scale transformer model, and B. existing
multi-modal transformers and visual representations do not perform well on
open-domain visual queries. Our challenge for the community is to create a
unified multimodal reasoning model that seamlessly transitions and reasons
regardless of the source modality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Melatect: A Machine Learning Model Approach For Identifying Malignant Melanoma in Skin Growths. (arXiv:2109.03310v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03310">
<div class="article-summary-box-inner">
<span><p>Malignant melanoma is a common skin cancer that is mostly curable before
metastasis -when growths spawn in organs away from the original site. Melanoma
is the most dangerous type of skin cancer if left untreated due to the high
risk of metastasis. This paper presents Melatect, a machine learning (ML) model
embedded in an iOS app that identifies potential malignant melanoma. Melatect
accurately classifies lesions as malignant or benign over 96.6% of the time
with no apparent bias or overfitting. Using the Melatect app, users have the
ability to take pictures of skin lesions (moles) and subsequently receive a
mole classification. The Melatect app provides a convenient way to get free
advice on lesions and track these lesions over time. A recursive computer image
analysis algorithm and modified MLOps pipeline was developed to create a model
that performs at a higher accuracy than existing models. Our training dataset
included 18,400 images of benign and malignant lesions, including 18,000 from
the International Skin Imaging Collaboration (ISIC) archive, as well as 400
images gathered from local dermatologists; these images were augmented using
DeepAugment, an AutoML tool, to 54,054 images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling. (arXiv:2109.04699v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04699">
<div class="article-summary-box-inner">
<span><p>While large scale pre-training has achieved great achievements in bridging
the gap between vision and language, it still faces several challenges. First,
the cost for pre-training is expensive. Second, there is no efficient way to
handle the data noise which degrades model performance. Third, previous methods
only leverage limited image-text paired data, while ignoring richer
single-modal data, which may result in poor generalization to single-modal
downstream tasks. In this work, we propose an EfficientCLIP method via Ensemble
Confident Learning to obtain a less noisy data subset. Extra rich non-paired
single-modal text data is used for boosting the generalization of text branch.
We achieve the state-of-the-art performance on Chinese cross-modal retrieval
tasks with only 1/10 training resources compared to CLIP and WenLan, while
showing excellent generalization to single-modal tasks, including text
retrieval and text classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAN3D: Fast 3D Medical Image Segmentation via Compact Context Aggregation. (arXiv:2109.05443v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05443">
<div class="article-summary-box-inner">
<span><p>Direct automatic segmentation of objects from 3D medical imaging, such as
magnetic resonance (MR) imaging, is challenging as it often involves accurately
identifying a number of individual objects with complex geometries within a
large volume under investigation. To address these challenges, most deep
learning approaches typically enhance their learning capability by
substantially increasing the complexity or the number of trainable parameters
within their models. Consequently, these models generally require long
inference time on standard workstations operating clinical MR systems and are
restricted to high-performance computing hardware due to their large memory
requirement. Further, to fit 3D dataset through these large models using
limited computer memory, trade-off techniques such as patch-wise training are
often used which sacrifice the fine-scale geometric information from input
images which could be clinically significant for diagnostic purposes. To
address these challenges, we present a compact convolutional neural network
with a shallow memory footprint to efficiently reduce the number of model
parameters required for state-of-art performance. This is critical for
practical employment as most clinical environments only have low-end hardware
with limited computing power and memory. The proposed network can maintain data
integrity by directly processing large full-size 3D input volumes with no
patches required and significantly reduces the computational time required for
both training and inference. We also propose a novel loss function with extra
shape constraint to improve the accuracy for imbalanced classes in 3D MR
images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAFNe: A One-Stage Anchor-Free Deep Model for Oriented Object Detection. (arXiv:2109.06148v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06148">
<div class="article-summary-box-inner">
<span><p>Object detection is a fundamental task in computer vision. While approaches
for axis-aligned bounding box detection have made substantial progress in
recent years, they perform poorly on oriented objects which are common in
several real-world scenarios such as aerial view imagery and security camera
footage. In these cases, a large part of a predicted bounding box will,
undesirably, cover non-object related areas. Therefore, oriented object
detection has emerged with the aim of generalizing object detection to
arbitrary orientations. This enables a tighter fit to oriented objects, leading
to a better separation of bounding boxes especially in case of dense object
distributions. The vast majority of the work in this area has focused on
complex two-stage anchor-based approaches. Anchors act as priors on the
bounding box shape and require attentive hyper-parameter fine-tuning on a
per-dataset basis, increased model size, and come with computational overhead.
In this work, we present DAFNe: A Dense one-stage Anchor-Free deep Network for
oriented object detection. As a one-stage model, DAFNe performs predictions on
a dense grid over the input image, being architecturally simpler and faster, as
well as easier to optimize than its two-stage counterparts. Furthermore, as an
anchor-free model, DAFNe reduces the prediction complexity by refraining from
employing bounding box anchors. Moreover, we introduce an orientation-aware
generalization of the center-ness function for arbitrarily oriented bounding
boxes to down-weight low-quality predictions and a center-to-corner bounding
box prediction strategy that improves object localization performance. DAFNe
improves the prediction accuracy over the previous best one-stage anchor-free
model results on DOTA 1.0 by 4.65% mAP, setting the new state-of-the-art
results by achieving 76.95% mAP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoGG3D-Net: Locally Guided Global Descriptor Learning for 3D Place Recognition. (arXiv:2109.08336v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08336">
<div class="article-summary-box-inner">
<span><p>Retrieval-based place recognition is an efficient and effective solution for
enabling re-localization within a pre-built map or global data association for
Simultaneous Localization and Mapping (SLAM). The accuracy of such an approach
is heavily dependent on the quality of the extracted scene-level
representation. While end-to-end solutions, which learn a global descriptor
from input point clouds, have demonstrated promising results, such approaches
are limited in their ability to enforce desirable properties at the local
feature level. In this paper, we demonstrate that the inclusion of an
additional training signal (local consistency loss) can guide the network to
learning local features which are consistent across revisits, hence leading to
more repeatable global descriptors resulting in an overall improvement in place
recognition performance. We formulate our approach in an end-to-end trainable
architecture called LoGG3D-Net. Experiments on two large-scale public
benchmarks (KITTI and MulRan) show that our method achieves mean $F1_{max}$
scores of $0.939$ and $0.968$ on KITTI and MulRan, respectively while operating
in near real-time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ElasticFace: Elastic Margin Loss for Deep Face Recognition. (arXiv:2109.09416v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09416">
<div class="article-summary-box-inner">
<span><p>Learning discriminative face features plays a major role in building
high-performing face recognition models. The recent state-of-the-art face
recognition solutions proposed to incorporate a fixed penalty margin on
commonly used classification loss function, softmax loss, in the normalized
hypersphere to increase the discriminative power of face recognition models, by
minimizing the intra-class variation and maximizing the inter-class variation.
Marginal softmax losses, such as ArcFace and CosFace, assume that the geodesic
distance between and within the different identities can be equally learned
using a fixed margin. However, such a learning objective is not realistic for
real data with inconsistent inter-and intra-class variation, which might limit
the discriminative and generalizability of the face recognition model. In this
paper, we relax the fixed margin constrain by proposing elastic margin loss
(ElasticFace) that allows flexibility in the push for class separability. The
main idea is to utilize random margin values drawn from a normal distribution
in each training iteration. This aims at giving the margin chances to extract
and retract to allow space for flexible class separability learning. We
demonstrate the superiority of our elastic margin loss over ArcFace and CosFace
losses, using the same geometric transformation, on a large set of mainstream
benchmarks. From a wider perspective, our ElasticFace has advanced the
state-of-the-art face recognition performance on six out of nine mainstream
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StereOBJ-1M: Large-scale Stereo Image Dataset for 6D Object Pose Estimation. (arXiv:2109.10115v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10115">
<div class="article-summary-box-inner">
<span><p>We present a large-scale stereo RGB image object pose estimation dataset
named the $\textbf{StereOBJ-1M}$ dataset. The dataset is designed to address
challenging cases such as object transparency, translucency, and specular
reflection, in addition to the common challenges of occlusion, symmetry, and
variations in illumination and environments. In order to collect data of
sufficient scale for modern deep learning models, we propose a novel method for
efficiently annotating pose data in a multi-view fashion that allows data
capturing in complex and flexible environments. Fully annotated with 6D object
poses, our dataset contains over 396K frames and over 1.5M annotations of 18
objects recorded in 183 scenes constructed in 11 different environments. The 18
objects include 8 symmetric objects, 7 transparent objects, and 8 reflective
objects. We benchmark two state-of-the-art pose estimation frameworks on
StereOBJ-1M as baselines for future work. We also propose a novel object-level
pose optimization method for computing 6D pose from keypoint predictions in
multiple images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models. (arXiv:2109.10282v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10282">
<div class="article-summary-box-inner">
<span><p>Text recognition is a long-standing research problem for document
digitalization. Existing approaches for text recognition are usually built
based on CNN for image understanding and RNN for char-level text generation. In
addition, another language model is usually needed to improve the overall
accuracy as a post-processing step. In this paper, we propose an end-to-end
text recognition approach with pre-trained image Transformer and text
Transformer models, namely TrOCR, which leverages the Transformer architecture
for both image understanding and wordpiece-level text generation. The TrOCR
model is simple but effective, and can be pre-trained with large-scale
synthetic data and fine-tuned with human-labeled datasets. Experiments show
that the TrOCR model outperforms the current state-of-the-art models on both
printed and handwritten text recognition tasks. The code and models will be
publicly available at https://aka.ms/TrOCR.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-23 23:02:08.194726692 UTC">2021-09-23 23:02:08 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>