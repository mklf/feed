<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-02-22T01:30:00Z">02-22</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Disinformation Attacks on Automated Fact Verification Systems. (arXiv:2202.09381v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09381">
<div class="article-summary-box-inner">
<span><p>Automated fact-checking is a needed technology to curtail the spread of
online misinformation. One current framework for such solutions proposes to
verify claims by retrieving supporting or refuting evidence from related
textual sources. However, the realistic use cases for fact-checkers will
require verifying claims against evidence sources that could be affected by the
same misinformation. Furthermore, the development of modern NLP tools that can
produce coherent, fabricated content would allow malicious actors to
systematically generate adversarial disinformation for fact-checkers.
</p>
<p>In this work, we explore the sensitivity of automated fact-checkers to
synthetic adversarial evidence in two simulated settings: AdversarialAddition,
where we fabricate documents and add them to the evidence repository available
to the fact-checking system, and AdversarialModification, where existing
evidence source documents in the repository are automatically altered. Our
study across multiple models on three benchmarks demonstrates that these
systems suffer significant performance drops against these attacks. Finally, we
discuss the growing threat of modern NLG systems as generators of
disinformation in the context of the challenges they pose to automated
fact-checkers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying the Adoption or Rejection of Misinformation Targeting COVID-19 Vaccines in Twitter Discourse. (arXiv:2202.09445v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09445">
<div class="article-summary-box-inner">
<span><p>Although billions of COVID-19 vaccines have been administered, too many
people remain hesitant. Misinformation about the COVID-19 vaccines, propagating
on social media, is believed to drive hesitancy towards vaccination. However,
exposure to misinformation does not necessarily indicate misinformation
adoption. In this paper we describe a novel framework for identifying the
stance towards misinformation, relying on attitude consistency and its
properties. The interactions between attitude consistency, adoption or
rejection of misinformation and the content of microblogs are exploited in a
novel neural architecture, where the stance towards misinformation is organized
in a knowledge graph. This new neural framework is enabling the identification
of stance towards misinformation about COVID-19 vaccines with state-of-the-art
results. The experiments are performed on a new dataset of misinformation
towards COVID-19 vaccines, called CoVaxLies, collected from recent Twitter
discourse. Because CoVaxLies provides a taxonomy of the misinformation about
COVID-19 vaccines, we are able to show which type of misinformation is mostly
adopted and which is mostly rejected.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VaccineLies: A Natural Language Resource for Learning to Recognize Misinformation about the COVID-19 and HPV Vaccines. (arXiv:2202.09449v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09449">
<div class="article-summary-box-inner">
<span><p>Billions of COVID-19 vaccines have been administered, but many remain
hesitant. Misinformation about the COVID-19 vaccines and other vaccines,
propagating on social media, is believed to drive hesitancy towards
vaccination. The ability to automatically recognize misinformation targeting
vaccines on Twitter depends on the availability of data resources. In this
paper we present VaccineLies, a large collection of tweets propagating
misinformation about two vaccines: the COVID-19 vaccines and the Human
Papillomavirus (HPV) vaccines. Misinformation targets are organized in
vaccine-specific taxonomies, which reveal the misinformation themes and
concerns. The ontological commitments of the Misinformation taxonomies provide
an understanding of which misinformation themes and concerns dominate the
discourse about the two vaccines covered in VaccineLies. The organization into
training, testing and development sets of VaccineLies invites the development
of novel supervised methods for detecting misinformation on Twitter and
identifying the stance towards it. Furthermore, VaccineLies can be a stepping
stone for the development of datasets focusing on misinformation targeting
additional vaccines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From FreEM to D'AlemBERT: a Large Corpus and a Language Model for Early Modern French. (arXiv:2202.09452v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09452">
<div class="article-summary-box-inner">
<span><p>Language models for historical states of language are becoming increasingly
important to allow the optimal digitisation and analysis of old textual
sources. Because these historical states are at the same time more complex to
process and more scarce in the corpora available, specific efforts are
necessary to train natural language processing (NLP) tools adapted to the data.
In this paper, we present our efforts to develop NLP tools for Early Modern
French (historical French from the 16$^\text{th}$ to the 18$^\text{th}$
centuries). We present the $\text{FreEM}_{\text{max}}$ corpus of Early Modern
French and D'AlemBERT, a RoBERTa-based language model trained on
$\text{FreEM}_{\text{max}}$. We evaluate the usefulness of D'AlemBERT by
fine-tuning it on a part-of-speech tagging task, outperforming previous work on
the test set. Importantly, we find evidence for the transfer learning capacity
of the language model, since its performance on lesser-resourced time periods
appears to have been boosted by the more resourced ones. We release D'AlemBERT
and the open-sourced subpart of the $\text{FreEM}_{\text{max}}$ corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Hesitancy Framings to Vaccine Hesitancy Profiles: A Journey of Stance, Ontological Commitments and Moral Foundations. (arXiv:2202.09456v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09456">
<div class="article-summary-box-inner">
<span><p>While billions of COVID-19 vaccines have been administered, too many people
remain hesitant. Twitter, with its substantial reach and daily exposure, is an
excellent resource for examining how people frame their vaccine hesitancy and
to uncover vaccine hesitancy profiles. In this paper we expose our processing
journey from identifying Vaccine Hesitancy Framings in a collection of
9,133,471 original tweets discussing the COVID-19 vaccines, establishing their
ontological commitments, annotating the Moral Foundations they imply to the
automatic recognition of the stance of the tweet authors toward any of the
CoVaxFrames that we have identified. When we found that 805,336 Twitter users
had a stance towards some CoVaxFrames in either the 9,133,471 original tweets
or their 17,346,664 retweets, we were able to derive nine different Vaccine
Hesitancy Profiles of these users and to interpret these profiles based on the
ontological commitments of the frames they evoked in their tweets and on value
of their stance towards the evoked frames.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-Driven Mitigation of Adversarial Text Perturbation. (arXiv:2202.09483v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09483">
<div class="article-summary-box-inner">
<span><p>Social networks have become an indispensable part of our lives, with billions
of people producing ever-increasing amounts of text. At such scales, content
policies and their enforcement become paramount. To automate moderation,
questionable content is detected by Natural Language Processing (NLP)
classifiers. However, high-performance classifiers are hampered by misspellings
and adversarial text perturbations. In this paper, we classify intentional and
unintentional adversarial text perturbation into ten types and propose a
deobfuscation pipeline to make NLP models robust to such perturbations. We
propose Continuous Word2Vec (CW2V), our data-driven method to learn word
embeddings that ensures that perturbations of words have embeddings similar to
those of the original words. We show that CW2V embeddings are generally more
robust to text perturbations than embeddings based on character ngrams. Our
robust classification pipeline combines deobfuscation and classification, using
proposed defense methods and word embeddings to classify whether Facebook posts
are requesting engagement such as likes. Our pipeline results in engagement
bait classification that goes from 0.70 to 0.67 AUC with adversarial text
perturbation, while character ngram-based word embedding methods result in
downstream classification that goes from 0.76 to 0.64.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PETCI: A Parallel English Translation Dataset of Chinese Idioms. (arXiv:2202.09509v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09509">
<div class="article-summary-box-inner">
<span><p>Idioms are an important language phenomenon in Chinese, but idiom translation
is notoriously hard. Current machine translation models perform poorly on idiom
translation, while idioms are sparse in many translation datasets. We present
PETCI, a parallel English translation dataset of Chinese idioms, aiming to
improve idiom translation by both human and machine. The dataset is built by
leveraging human and machine effort. Baseline generation models show
unsatisfactory abilities to improve translation, but structure-aware
classification models show good performance on distinguishing good
translations. Furthermore, the size of PETCI can be easily increased without
expertise. Overall, PETCI can be helpful to language learners and machine
translation systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Hate Speech Detection: A Comparative Study. (arXiv:2202.09517v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09517">
<div class="article-summary-box-inner">
<span><p>Automated hate speech detection is an important tool in combating the spread
of hate speech, particularly in social media. Numerous methods have been
developed for the task, including a recent proliferation of deep-learning based
approaches. A variety of datasets have also been developed, exemplifying
various manifestations of the hate-speech detection problem. We present here a
large-scale empirical comparison of deep and shallow hate-speech detection
methods, mediated through the three most commonly used datasets. Our goal is to
illuminate progress in the area, and identify strengths and weaknesses in the
current state-of-the-art. We particularly focus our analysis on measures of
practical performance, including detection accuracy, computational efficiency,
capability in using pre-trained models, and domain generalization. In doing so
we aim to provide guidance as to the use of hate-speech detection in practice,
quantify the state-of-the-art, and identify future research directions. Code
and dataset are available at
https://github.com/jmjmalik22/Hate-Speech-Detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Models and Datasets for Cross-Lingual Summarisation. (arXiv:2202.09583v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09583">
<div class="article-summary-box-inner">
<span><p>We present a cross-lingual summarisation corpus with long documents in a
source language associated with multi-sentence summaries in a target language.
The corpus covers twelve language pairs and directions for four European
languages, namely Czech, English, French and German, and the methodology for
its creation can be applied to several other languages. We derive cross-lingual
document-summary instances from Wikipedia by combining lead paragraphs and
articles' bodies from language aligned Wikipedia titles. We analyse the
proposed cross-lingual summarisation task with automatic metrics and validate
it with a human study. To illustrate the utility of our dataset we report
experiments with multi-lingual pre-trained models in supervised, zero- and
few-shot, and out-of-domain scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CALCS 2021 Shared Task: Machine Translation for Code-Switched Data. (arXiv:2202.09625v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09625">
<div class="article-summary-box-inner">
<span><p>To date, efforts in the code-switching literature have focused for the most
part on language identification, POS, NER, and syntactic parsing. In this
paper, we address machine translation for code-switched social media data. We
create a community shared task. We provide two modalities for participation:
supervised and unsupervised. For the supervised setting, participants are
challenged to translate English into Hindi-English (Eng-Hinglish) in a single
direction. For the unsupervised setting, we provide the following language
pairs: English and Spanish-English (Eng-Spanglish), and English and Modern
Standard Arabic-Egyptian Arabic (Eng-MSAEA) in both directions. We share
insights and challenges in curating the "into" code-switching language
evaluation data. Further, we provide baselines for all language pairs in the
shared task. The leaderboard for the shared task comprises 12 individual system
submissions corresponding to 5 different teams. The best performance achieved
is 12.67% BLEU score for English to Hinglish and 25.72% BLEU score for MSAEA to
English.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reward Modeling for Mitigating Toxicity in Transformer-based Language Models. (arXiv:2202.09662v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09662">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models are able to generate fluent text and be
efficiently adapted across various natural language generation tasks. However,
language models that are pretrained on large unlabeled web text corpora have
been shown to suffer from degenerating toxic content and social bias behaviors,
consequently hindering their safe deployment. Various detoxification methods
were proposed to mitigate the language model's toxicity; however, these methods
struggled to detoxify language models when conditioned on prompts that contain
specific social identities related to gender, race, or religion. In this study,
we propose Reinforce-Detoxify; A reinforcement learning-based method for
mitigating toxicity in language models. We address the challenge of safety in
language models and propose a new reward model that is able to detect toxic
content and mitigate unintended bias towards social identities in toxicity
prediction. The experiments demonstrate that the Reinforce-Detoxify method for
language model detoxification outperforms existing detoxification approaches in
automatic evaluation metrics, indicating the ability of our approach in
language model detoxification and less prone to unintended bias toward social
identities in generated content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is there an aesthetic component of language?. (arXiv:2202.09689v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09689">
<div class="article-summary-box-inner">
<span><p>Speakers of all human languages make use of grammatical devices to express
attributional qualities, feelings, and opinions as well as to provide
meta-commentary on topics in discourse. In general, linguists refer to this
category as 'expressives'in spite of the fact that defining exactly what
'expressives' are remains elusive. The elusiveness of expressives has given
rise to considerable speculation about the nature of expressivity as a
linguistic principle. Specifically, several scholars have pointed out the
'special' or 'unusual' nature of expressives vis-a-vis 'normal' or 'natural'
morpho-syntax.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MACRONYM: A Large-Scale Dataset for Multilingual and Multi-Domain Acronym Extraction. (arXiv:2202.09694v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09694">
<div class="article-summary-box-inner">
<span><p>Acronym extraction is the task of identifying acronyms and their expanded
forms in texts that is necessary for various NLP applications. Despite major
progress for this task in recent years, one limitation of existing AE research
is that they are limited to the English language and certain domains (i.e.,
scientific and biomedical). As such, challenges of AE in other languages and
domains is mainly unexplored. Lacking annotated datasets in multiple languages
and domains has been a major issue to hinder research in this area. To address
this limitation, we propose a new dataset for multilingual multi-domain AE.
Specifically, 27,200 sentences in 6 typologically different languages and 2
domains, i.e., Legal and Scientific, is manually annotated for AE. Our
extensive experiments on the proposed dataset show that AE in different
languages and different learning settings has unique challenges, emphasizing
the necessity of further research on multilingual and multi-domain AE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Punctuation Restoration. (arXiv:2202.09695v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09695">
<div class="article-summary-box-inner">
<span><p>Given the increasing number of livestreaming videos, automatic speech
recognition and post-processing for livestreaming video transcripts are crucial
for efficient data management as well as knowledge mining. A key step in this
process is punctuation restoration which restores fundamental text structures
such as phrase and sentence boundaries from the video transcripts. This work
presents a new human-annotated corpus, called BehancePR, for punctuation
restoration in livestreaming video transcripts. Our experiments on BehancePR
demonstrate the challenges of punctuation restoration for this domain.
Furthermore, we show that popular natural language processing toolkits are
incapable of detecting sentence boundary on non-punctuated transcripts of
livestreaming videos, calling for more research effort to develop robust models
for this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph-based Extractive Explainer for Recommendations. (arXiv:2202.09730v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09730">
<div class="article-summary-box-inner">
<span><p>Explanations in a recommender system assist users in making informed
decisions among a set of recommended items. Great research attention has been
devoted to generating natural language explanations to depict how the
recommendations are generated and why the users should pay attention to them.
However, due to different limitations of those solutions, e.g., template-based
or generation-based, it is hard to make the explanations easily perceivable,
reliable and personalized at the same time.
</p>
<p>In this work, we develop a graph attentive neural network model that
seamlessly integrates user, item, attributes, and sentences for
extraction-based explanation. The attributes of items are selected as the
intermediary to facilitate message passing for user-item specific evaluation of
sentence relevance. And to balance individual sentence relevance, overall
attribute coverage, and content redundancy, we solve an integer linear
programming problem to make the final selection of sentences. Extensive
empirical evaluations against a set of state-of-the-art baseline methods on two
benchmark review datasets demonstrated the generation quality of the proposed
solution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Semantic Embeddings for Ontology Subsumption Prediction. (arXiv:2202.09791v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09791">
<div class="article-summary-box-inner">
<span><p>Automating ontology curation is a crucial task in knowledge engineering.
Prediction by machine learning techniques such as semantic embedding is a
promising direction, but the relevant research is still preliminary. In this
paper, we present a class subsumption prediction method named BERTSubs, which
uses the pre-trained language model BERT to compute contextual embeddings of
the class labels and customized input templates to incorporate contexts of
surrounding classes. The evaluation on two large-scale real-world ontologies
has shown its better performance than the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Interpretation of Neural Text Classification. (arXiv:2202.09792v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09792">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed increasing interests in developing interpretable
models in Natural Language Processing (NLP). Most existing models aim at
identifying input features such as words or phrases important for model
predictions. Neural models developed in NLP however often compose word
semantics in a hierarchical manner. Interpretation by words or phrases only
thus cannot faithfully explain model decisions. This paper proposes a novel
Hierarchical INTerpretable neural text classifier, called Hint, which can
automatically generate explanations of model predictions in the form of
label-associated topics in a hierarchical manner. Model interpretation is no
longer at the word level, but built on topics as the basic semantic unit.
Experimental results on both review datasets and news datasets show that our
proposed approach achieves text classification results on par with existing
state-of-the-art text classifiers, and generates interpretations more faithful
to model predictions and better understood by humans than other interpretable
neural text classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$\mathcal{Y}$-Tuning: An Efficient Tuning Paradigm for Large-Scale Pre-Trained Models via Label Representation Learning. (arXiv:2202.09817v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09817">
<div class="article-summary-box-inner">
<span><p>With the success of large-scale pre-trained models (PTMs), how efficiently
adapting PTMs to downstream tasks has attracted tremendous attention,
especially for PTMs with billions of parameters. Although some
parameter-efficient tuning paradigms have been proposed to address this
problem, they still require large resources to compute the gradients in the
training phase. In this paper, we propose $\mathcal{Y}$-Tuning, an efficient
yet effective paradigm to adapt frozen large-scale PTMs to specific downstream
tasks. $\mathcal{Y}$-tuning learns dense representations for labels
$\mathcal{Y}$ defined in a given task and aligns them to fixed feature
representation. Without tuning the features of input text and model parameters,
$\mathcal{Y}$-tuning is both parameter-efficient and training-efficient. For
$\text{DeBERTa}_\text{XXL}$ with 1.6 billion parameters, $\mathcal{Y}$-tuning
achieves performance more than $96\%$ of full fine-tuning on GLUE Benchmark
with only $2\%$ tunable parameters and much fewer training costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Meta-embedding-based Ensemble Approach for ICD Coding Prediction. (arXiv:2102.13622v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.13622">
<div class="article-summary-box-inner">
<span><p>International Classification of Diseases (ICD) are the de facto codes used
globally for clinical coding. These codes enable healthcare providers to claim
reimbursement and facilitate efficient storage and retrieval of diagnostic
information. The problem of automatically assigning ICD codes has been
approached in literature as a multilabel classification, using neural models on
unstructured data. Our proposed approach enhances the performance of neural
models by effectively training word vectors using routine medical data as well
as external knowledge from scientific articles. Furthermore, we exploit the
geometric properties of the two sets of word vectors and combine them into a
common dimensional space, using meta-embedding techniques. We demonstrate the
efficacy of this approach for a multimodal setting, using unstructured and
structured information. We empirically show that our approach improves the
current state-of-the-art deep learning architectures and benefits ensemble
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Understanding for Argumentative Dialogue Systems in the Opinion Building Domain. (arXiv:2103.02691v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02691">
<div class="article-summary-box-inner">
<span><p>This paper introduces a natural language understanding (NLU) framework for
argumentative dialogue systems in the information-seeking and opinion building
domain. The proposed framework consists of two sub-models, namely intent
classifier and argument similarity. Intent classifier model stacks BiLSTM with
attention mechanism on top of the pre-trained BERT model and fine-tune the
model for recognizing the user intent, whereas the argument similarity model
employs BERT+BiLSTM for identifying system arguments the user refers to in his
or her natural language utterances. Our model is evaluated in an argumentative
dialogue system that engages the user to inform him-/herself about a
controversial topic by exploring pro and con arguments and build his/her
opinion towards the topic. In order to evaluate the proposed approach, we
collect user utterances for the interaction with the respective system labeling
intent and referenced argument in an extensive online study. The data
collection includes multiple topics and two different user types (native
English speakers from the UK and non-native English speakers from China).
Additionally, we evaluate the proposed intent classifier and argument
similarity models separately on the publicly available Banking77 and STS
benchmark datasets. The evaluation indicates a clear advantage of the utilized
techniques over baseline approaches on several datasets, as well as the
robustness of the proposed approach against new topics and different language
proficiency as well as the cultural background of the user. Furthermore,
results show that our intent classifier model outperforms DIET, DistillBERT,
and BERT fine-tuned models in few-shot setups (i.e., with 10, 20, or 30 labeled
examples per intent) and full data setup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for Improved Cross-Modal Retrieval. (arXiv:2103.11920v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11920">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art approaches to cross-modal retrieval process text and
visual input jointly, relying on Transformer-based architectures with
cross-attention mechanisms that attend over all words and objects in an image.
While offering unmatched retrieval performance, such models: 1) are typically
pretrained from scratch and thus less scalable, 2) suffer from huge retrieval
latency and inefficiency issues, which makes them impractical in realistic
applications. To address these crucial gaps towards both improved and efficient
cross-modal retrieval, we propose a novel fine-tuning framework that turns any
pretrained text-image multi-modal model into an efficient retrieval model. The
framework is based on a cooperative retrieve-and-rerank approach which
combines: 1) twin networks (i.e., a bi-encoder) to separately encode all items
of a corpus, enabling efficient initial retrieval, and 2) a cross-encoder
component for a more nuanced (i.e., smarter) ranking of the retrieved small set
of items. We also propose to jointly fine-tune the two components with shared
weights, yielding a more parameter-efficient model. Our experiments on a series
of standard cross-modal retrieval benchmarks in monolingual, multilingual, and
zero-shot setups, demonstrate improved accuracy and huge efficiency benefits
over the state-of-the-art cross-encoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets. (arXiv:2103.12028v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12028">
<div class="article-summary-box-inner">
<span><p>With the success of large-scale pre-training and multilingual modeling in
Natural Language Processing (NLP), recent years have seen a proliferation of
large, web-mined text datasets covering hundreds of languages. We manually
audit the quality of 205 language-specific corpora released with five major
public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource
corpora have systematic issues: At least 15 corpora have no usable text, and a
significant fraction contains less than 50% sentences of acceptable quality. In
addition, many are mislabeled or use nonstandard/ambiguous language codes. We
demonstrate that these issues are easy to detect even for non-proficient
speakers, and supplement the human audit with automatic analyses. Finally, we
recommend techniques to evaluate and improve multilingual corpora and discuss
potential risks that come with low-quality data releases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recommending Metamodel Concepts during Modeling Activities with Pre-Trained Language Models. (arXiv:2104.01642v3 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01642">
<div class="article-summary-box-inner">
<span><p>The design of conceptually sound metamodels that embody proper semantics in
relation to the application domain is particularly tedious in Model-Driven
Engineering. As metamodels define complex relationships between domain
concepts, it is crucial for a modeler to define these concepts thoroughly while
being consistent with respect to the application domain. We propose an approach
to assist a modeler in the design of a metamodel by recommending relevant
domain concepts in several modeling scenarios. Our approach does not require to
extract knowledge from the domain or to hand-design completion rules. Instead,
we design a fully data-driven approach using a deep learning model that is able
to abstract domain concepts by learning from both structural and lexical
metamodel properties in a corpus of thousands of independent metamodels. We
evaluate our approach on a test set containing 166 metamodels, unseen during
the model training, with more than 5000 test samples. Our preliminary results
show that the trained model is able to provide accurate top-$5$ lists of
relevant recommendations for concept renaming scenarios. Although promising,
the results are less compelling for the scenario of the iterative construction
of the metamodel, in part because of the conservative strategy we use to
evaluate the recommendations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On migration to Perpetual Enterprise System. (arXiv:2104.04844v3 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04844">
<div class="article-summary-box-inner">
<span><p>This document describes a pragmatic approach on how to migrate an
organisation computer system towards a new system that could evolve forever,
addresses the whole organisation and it is integrated.
</p>
<p>Governance aspects are as important, if not more, than purely technical IT
aspects: human resources, call for tenders, and similar. Migration implies that
one is not starting from a green field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners. (arXiv:2108.13161v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13161">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained language models have contributed significantly to
natural language processing by demonstrating remarkable abilities as few-shot
learners. However, their effectiveness depends mainly on scaling the model
parameters and prompt design, hindering their implementation in most real-world
applications. This study proposes a novel pluggable, extensible, and efficient
approach named DifferentiAble pRompT (DART), which can convert small language
models into better few-shot learners without any prompt engineering. The main
principle behind this approach involves reformulating potential natural
language processing tasks into the task of a pre-trained language model and
differentially optimizing the prompt template as well as the target label with
backpropagation. Furthermore, the proposed approach can be: (i) Plugged to any
pre-trained language models; (ii) Extended to widespread classification tasks.
A comprehensive evaluation of standard NLP tasks demonstrates that the proposed
approach achieves a better few-shot performance. Code is available in
https://github.com/zjunlp/DART.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ArchivalQA: A Large-scale Benchmark Dataset for Open Domain Question Answering over Archival News Collections. (arXiv:2109.03438v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03438">
<div class="article-summary-box-inner">
<span><p>In the last few years, open-domain question answering (ODQA) has advanced
rapidly due to the development of deep learning techniques and the availability
of large-scale QA datasets. However, the current datasets are essentially
designed for synchronic document collections (e.g., Wikipedia). Temporal news
collections such as long-term news archives spanning several decades, are
rarely used in training the models despite they are quite valuable for our
society. To foster the research in the field of ODQA on such historical
collections, we present ArchivalQA, a large question answering dataset
consisting of 532,444 question-answer pairs which is designed for temporal news
QA. We divide our dataset into four subparts based on the question difficulty
levels and the containment of temporal expressions, which we believe are useful
for training and testing ODQA systems characterized by different strengths and
abilities. The novel QA dataset-constructing framework that we introduce can be
also applied to generate non-ambiguous questions of good quality over other
types of temporal document collections.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TopiOCQA: Open-domain Conversational Question Answering with Topic Switching. (arXiv:2110.00768v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00768">
<div class="article-summary-box-inner">
<span><p>In a conversational question answering scenario, a questioner seeks to
extract information about a topic through a series of interdependent questions
and answers. As the conversation progresses, they may switch to related topics,
a phenomenon commonly observed in information-seeking search sessions. However,
current datasets for conversational question answering are limiting in two
ways: 1) they do not contain topic switches; and 2) they assume the reference
text for the conversation is given, i.e., the setting is not open-domain. We
introduce TopiOCQA (pronounced Tapioca), an open-domain conversational dataset
with topic switches on Wikipedia. TopiOCQA contains 3,920 conversations with
information-seeking questions and free-form answers. On average, a conversation
in our dataset spans 13 question-answer turns and involves four topics
(documents). TopiOCQA poses a challenging test-bed for models, where efficient
retrieval is required on multiple turns of the same conversation, in
conjunction with constructing valid responses using conversational history. We
evaluate several baselines, by combining state-of-the-art document retrieval
methods with neural reader models. Our best model achieves F1 of 55.8, falling
short of human performance by 14.2 points, indicating the difficulty of our
dataset. Our dataset and code is available at
https://mcgill-nlp.github.io/topiocqa
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Internal Language Model Adaptation with Text-Only Data for End-to-End Speech Recognition. (arXiv:2110.05354v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05354">
<div class="article-summary-box-inner">
<span><p>Text-only adaptation of an end-to-end (E2E) model remains a challenging task
for automatic speech recognition (ASR). Language model (LM) fusion-based
approaches require an additional external LM during inference, significantly
increasing the computation cost. To overcome this, we propose an internal LM
adaptation (ILMA) of the E2E model using text-only data. Trained with
audio-transcript pairs, an E2E model implicitly learns an internal LM that
characterizes the token sequence probability which is approximated by the E2E
model output after zeroing out the encoder contribution. During ILMA, we
fine-tune the internal LM, i.e., the E2E components excluding the encoder, to
minimize a cross-entropy loss. To make ILMA effective, it is essential to train
the E2E model with an internal LM loss besides the standard E2E loss.
Furthermore, we propose to regularize ILMA by minimizing the Kullback-Leibler
divergence between the output distributions of the adapted and unadapted
internal LMs. ILMA is the most effective when we update only the last linear
layer of the joint network. ILMA enables a fast text-only adaptation of the E2E
model without increasing the run-time computational cost. Experimented with
30K-hour trained transformer transducer models, ILMA achieves up to 34.9%
relative word error rate reduction from the unadapted baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning. (arXiv:2110.13214v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13214">
<div class="article-summary-box-inner">
<span><p>Current visual question answering (VQA) tasks mainly consider answering
human-annotated questions for natural images. However, aside from natural
images, abstract diagrams with semantic richness are still understudied in
visual understanding and reasoning research. In this work, we introduce a new
challenge of Icon Question Answering (IconQA) with the goal of answering a
question in an icon image context. We release IconQA, a large-scale dataset
that consists of 107,439 questions and three sub-tasks: multi-image-choice,
multi-text-choice, and filling-in-the-blank. The IconQA dataset is inspired by
real-world diagram word problems that highlight the importance of abstract
diagram understanding and comprehensive cognitive reasoning. Thus, IconQA
requires not only perception skills like object recognition and text
understanding, but also diverse cognitive reasoning skills, such as geometric
reasoning, commonsense reasoning, and arithmetic reasoning. To facilitate
potential IconQA models to learn semantic representations for icon images, we
further release an icon dataset Icon645 which contains 645,687 colored icons on
377 classes. We conduct extensive user studies and blind experiments and
reproduce a wide range of advanced VQA methods to benchmark the IconQA task.
Also, we develop a strong IconQA baseline Patch-TRM that applies a pyramid
cross-modal Transformer with input diagram embeddings pre-trained on the icon
dataset. IconQA and Icon645 are available at https://iconqa.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Fine-Grained Reasoning for Fake News Detection. (arXiv:2110.15064v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15064">
<div class="article-summary-box-inner">
<span><p>The detection of fake news often requires sophisticated reasoning skills,
such as logically combining information by considering word-level subtle clues.
In this paper, we move towards fine-grained reasoning for fake news detection
by better reflecting the logical processes of human thinking and enabling the
modeling of subtle clues. In particular, we propose a fine-grained reasoning
framework by following the human information-processing model, introduce a
mutual-reinforcement-based method for incorporating human knowledge about which
evidence is more important, and design a prior-aware bi-channel kernel graph
network to model subtle differences between pieces of evidence. Extensive
experiments show that our model outperforms the state-of-the-art methods and
demonstrate the explainability of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-Cost Algorithmic Recourse for Users With Uncertain Cost Functions. (arXiv:2111.01235v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01235">
<div class="article-summary-box-inner">
<span><p>People affected by machine learning model decisions may benefit greatly from
access to recourses, i.e. suggestions about what features they could change to
receive a more favorable decision from the model. Current approaches try to
optimize for the cost incurred by users when adopting a recourse, but they
assume that all users share the same cost function. This is an unrealistic
assumption because users might have diverse preferences about their willingness
to change certain features. In this work, we introduce a new method for
identifying recourse sets for users which does not assume that users'
preferences are known in advance. We propose an objective function, Expected
Minimum Cost (EMC), based on two key ideas: (1) when presenting a set of
options to a user, there only needs to be one low-cost solution that the user
could adopt; (2) when we do not know the user's true cost function, we can
approximately optimize for user satisfaction by first sampling plausible cost
functions from a distribution, then finding a recourse set that achieves a good
cost for these samples. We optimize EMC with a novel discrete optimization
algorithm, Cost Optimized Local Search (COLS), which is guaranteed to improve
the recourse set quality over iterations. Experimental evaluation on popular
real-world datasets with simulated users demonstrates that our method satisfies
up to 25.89 percentage points more users compared to strong baseline methods,
while, the human evaluation shows that our recourses are preferred more than
twice as often as the strongest baseline recourses. Finally, using standard
fairness metrics we show that our method can provide more fair solutions across
demographic groups than baselines. We provide our source code at:
https://github.com/prateeky2806/EMC-COLS-recourse
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversational speech recognition leveraging effective fusion methods for cross-utterance language modeling. (arXiv:2111.03333v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.03333">
<div class="article-summary-box-inner">
<span><p>Conversational speech normally is embodied with loose syntactic structures at
the utterance level but simultaneously exhibits topical coherence relations
across consecutive utterances. Prior work has shown that capturing longer
context information with a recurrent neural network or long short-term memory
language model (LM) may suffer from the recent bias while excluding the
long-range context. In order to capture the long-term semantic interactions
among words and across utterances, we put forward disparate conversation
history fusion methods for language modeling in automatic speech recognition
(ASR) of conversational speech. Furthermore, a novel audio-fusion mechanism is
introduced, which manages to fuse and utilize the acoustic embeddings of a
current utterance and the semantic content of its corresponding conversation
history in a cooperative way. To flesh out our ideas, we frame the ASR N-best
hypothesis rescoring task as a prediction problem, leveraging BERT, an iconic
pre-trained LM, as the ingredient vehicle to facilitate selection of the oracle
hypothesis from a given N-best hypothesis list. Empirical experiments conducted
on the AMI benchmark dataset seem to demonstrate the feasibility and efficacy
of our methods in relation to some current top-of-line methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conformer-based Hybrid ASR System for Switchboard Dataset. (arXiv:2111.03442v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.03442">
<div class="article-summary-box-inner">
<span><p>The recently proposed conformer architecture has been successfully used for
end-to-end automatic speech recognition (ASR) architectures achieving
state-of-the-art performance on different datasets. To our best knowledge, the
impact of using conformer acoustic model for hybrid ASR is not investigated. In
this paper, we present and evaluate a competitive conformer-based hybrid model
training recipe. We study different training aspects and methods to improve
word-error-rate as well as to increase training speed. We apply time
downsampling methods for efficient training and use transposed convolutions to
upsample the output sequence again. We conduct experiments on Switchboard 300h
dataset and our conformer-based hybrid model achieves competitive results
compared to other architectures. It generalizes very well on Hub5'01 test set
and outperforms the BLSTM-based hybrid model significantly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts. (arXiv:2111.08276v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08276">
<div class="article-summary-box-inner">
<span><p>Most existing methods in vision language pre-training rely on object-centric
features extracted through object detection, and make fine-grained alignments
between the extracted features and texts. We argue that object detection may
not be necessary for vision language pre-training. To this end, we propose a
new method called X-VLM to perform `multi-grained vision language
pre-training.' The key of learning multi-grained alignments is to locate visual
concepts in the image given the associated texts, and in the meantime align the
texts with the visual concepts, where the alignments are in multi-granularity.
Experimental results show that X-VLM effectively leverages the learned
alignments to many downstream vision language tasks and consistently
outperforms state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why KDAC? A general activation function for knowledge discovery. (arXiv:2111.13858v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13858">
<div class="article-summary-box-inner">
<span><p>Named entity recognition based on deep learning (DNER) can effectively mine
expected knowledge from large-scale unstructured and semi-structured text, and
has gradually become the paradigm of knowledge discovery. Currently, Tanh, ReLU
and Sigmoid dominate DNER, however, these activation functions failed to treat
gradient vanishing, no negative output or non-differentiable existence, which
may impede DNER's exploration of knowledge caused by the omission and the
incomplete representation of latent semantics. To surmount the non-negligible
obstacle, we present a novel and general activation function termed KDAC.
Detailly, KDAC is a thought that can aggregate and inherit the merits of Tanh
and ReLU since they are widely leveraged in various knowledge domains. The
positive region corresponds to an adaptive linear design encouraged by ReLU.
The negative region considers the interaction between exponent and linearity to
surmount the obstacle of gradient vanishing and no negative value. Crucially,
the non-differentiable points are alerted and eliminated by a smooth
approximation. We perform experiments based on BERT-BiLSTM-CNN-CRF model on six
benchmark datasets containing different domain knowledge, such as Weibo,
Clinical, E-commerce, Resume, HAZOP and People's daily. The experimental
results show that KDAC is advanced and effective, and can provide more
generalized activation to stimulate the performance of DNER. We hope that KDAC
can be exploited as a promising alternative activation function in DNER to
devote itself to the construction of knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoDistil: Few-shot Task-agnostic Neural Architecture Search for Distilling Large Language Models. (arXiv:2201.12507v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12507">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) methods compress large models into smaller
students with manually-designed student architectures given pre-specified
computational cost. This requires several trials to find a viable student, and
further repeating the process for each student or computational budget change.
We use Neural Architecture Search (NAS) to automatically distill several
compressed students with variable cost from a large model. Current works train
a single SuperLM consisting of millions of subnetworks with weight-sharing,
resulting in interference between subnetworks of different sizes. Our framework
AutoDistil addresses above challenges with the following steps: (a)
Incorporates inductive bias and heuristics to partition Transformer search
space into K compact sub-spaces (K=3 for typical student sizes of base, small
and tiny); (b) Trains one SuperLM for each sub-space using task-agnostic
objective (e.g., self-attention distillation) with weight-sharing of students;
(c) Lightweight search for the optimal student without re-training. Fully
task-agnostic training and search allow students to be reused for fine-tuning
on any downstream task. Experiments on GLUE benchmark against state-of-the-art
KD and NAS methods demonstrate AutoDistil to outperform leading compression
techniques with upto 2.7x reduction in computational cost and negligible loss
in task performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A multi-task semi-supervised framework for Text2Graph & Graph2Text. (arXiv:2202.06041v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06041">
<div class="article-summary-box-inner">
<span><p>The Artificial Intelligence industry regularly develops applications that
mostly rely on Knowledge Bases, a data repository about specific, or general,
domains, usually represented in a graph shape. Similar to other databases, they
face two main challenges: information ingestion and information retrieval. We
approach these challenges by jointly learning graph extraction from text and
text generation from graphs. The proposed solution, a T5 architecture, is
trained in a multi-task semi-supervised environment, with our collected
non-parallel data, following a cycle training regime. Experiments on WebNLG
dataset show that our approach surpasses unsupervised state-of-the-art results
in text-to-graph and graph-to-text. More relevantly, our framework is more
consistent across seen and unseen domains than supervised models. The resulting
model can be easily trained in any new domain with non-parallel data, by simply
adding text and graphs about it, in our cycle framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distribution augmentation for low-resource expressive text-to-speech. (arXiv:2202.06409v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06409">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel data augmentation technique for text-to-speech
(TTS), that allows to generate new (text, audio) training examples without
requiring any additional data. Our goal is to increase diversity of text
conditionings available during training. This helps to reduce overfitting,
especially in low-resource settings. Our method relies on substituting text and
audio fragments in a way that preserves syntactical correctness. We take
additional measures to ensure that synthesized speech does not contain
artifacts caused by combining inconsistent audio samples. The perceptual
evaluations show that our method improves speech quality over a number of
datasets, speakers, and TTS architectures. We also demonstrate that it greatly
improves robustness of attention-based TTS models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Did It Happen? Duration-informed Temporal Localization of Narrated Actions in Vlogs. (arXiv:2202.08138v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08138">
<div class="article-summary-box-inner">
<span><p>We consider the task of temporal human action localization in lifestyle
vlogs. We introduce a novel dataset consisting of manual annotations of
temporal localization for 13,000 narrated actions in 1,200 video clips. We
present an extensive analysis of this data, which allows us to better
understand how the language and visual modalities interact throughout the
videos. We propose a simple yet effective method to localize the narrated
actions based on their expected duration. Through several experiments and
analyses, we show that our method brings complementary information with respect
to previous methods, and leads to improvements over previous work for the task
of temporal action localization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SGPT: GPT Sentence Embeddings for Semantic Search. (arXiv:2202.08904v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08904">
<div class="article-summary-box-inner">
<span><p>GPT transformers are the largest language models available, yet semantic
search is dominated by BERT transformers. We present SGPT-BE and SGPT-CE for
applying GPT models as Bi-Encoders or Cross-Encoders to symmetric or asymmetric
search.
</p>
<p>SGPT-BE produces semantically meaningful sentence embeddings by contrastive
fine-tuning of only bias tensors and a novel pooling method. A 5.8 billion
parameter SGPT-BE outperforms the best available sentence embeddings by 6%
setting a new state-of-the-art on BEIR. It outperforms the concurrently
proposed OpenAI Embeddings of the 175B Davinci endpoint, which fine-tunes
250,000 times more parameters.
</p>
<p>SGPT-CE uses log probabilities from GPT models without any fine-tuning. A 6.1
billion parameter SGPT-CE sets an unsupervised state-of-the-art on BEIR. It
beats the supervised state-of-the-art on 7 datasets, but significantly loses on
other datasets. We show how this can be alleviated by adapting the prompt.
</p>
<p>SGPT-BE and SGPT-CE performance scales with model size. Yet, increased
latency, storage and compute costs should be considered. Code, models and
result files are freely available at https://github.com/Muennighoff/sgpt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLP: A Survey on Vision-Language Pre-training. (arXiv:2202.09061v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09061">
<div class="article-summary-box-inner">
<span><p>In the past few years, the emergence of pre-training models has brought
uni-modal fields such as computer vision (CV) and natural language processing
(NLP) to a new era. Substantial works have shown they are beneficial for
downstream uni-modal tasks and avoid training a new model from scratch. So can
such pre-trained models be applied to multi-modal tasks? Researchers have
explored this problem and made significant progress. This paper surveys recent
advances and new frontiers in vision-language pre-training (VLP), including
image-text and video-text pre-training. To give readers a better overall grasp
of VLP, we first review its recent advances from five aspects: feature
extraction, model architecture, pre-training objectives, pre-training datasets,
and downstream tasks. Then, we summarize the specific VLP models in detail.
Finally, we discuss the new frontiers in VLP. To the best of our knowledge,
this is the first survey on VLP. We hope that this survey can shed light on
future research in the VLP field.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Snowflake Point Deconvolution for Point Cloud Completion and Generation with Skip-Transformer. (arXiv:2202.09367v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09367">
<div class="article-summary-box-inner">
<span><p>Most existing point cloud completion methods suffered from discrete nature of
point clouds and unstructured prediction of points in local regions, which
makes it hard to reveal fine local geometric details. To resolve this issue, we
propose SnowflakeNet with Snowflake Point Deconvolution (SPD) to generate the
complete point clouds. SPD models the generation of complete point clouds as
the snowflake-like growth of points, where the child points are progressively
generated by splitting their parent points after each SPD. Our insight of
revealing detailed geometry is to introduce skip-transformer in SPD to learn
point splitting patterns which can fit local regions the best. Skip-transformer
leverages attention mechanism to summarize the splitting patterns used in
previous SPD layer to produce the splitting in current SPD layer. The locally
compact and structured point clouds generated by SPD precisely reveal the
structure characteristic of 3D shape in local patches, which enables us to
predict highly detailed geometries. Moreover, since SPD is a general operation,
which is not limited to completion, we further explore the applications of SPD
on other generative tasks, including point cloud auto-encoding, generation,
single image reconstruction and upsampling. Our experimental results outperform
the state-of-the-art methods under widely used benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Molecular Prior Distribution for Bayesian Inference Based on Wilson Statistics. (arXiv:2202.09388v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09388">
<div class="article-summary-box-inner">
<span><p>Background and Objective: Wilson statistics describe well the power spectrum
of proteins at high frequencies. Therefore, it has found several applications
in structural biology, e.g., it is the basis for sharpening steps used in
cryogenic electron microscopy (cryo-EM). A recent paper gave the first rigorous
proof of Wilson statistics based on a formalism of Wilson's original argument.
This new analysis also leads to statistical estimates of the scattering
potential of proteins that reveal a correlation between neighboring Fourier
coefficients. Here we exploit these estimates to craft a novel prior that can
be used for Bayesian inference of molecular structures. Methods: We describe
the properties of the prior and the computation of its hyperparameters. We then
evaluate the prior on two synthetic linear inverse problems, and compare
against a popular prior in cryo-EM reconstruction at a range of SNRs. Results:
We show that the new prior effectively suppresses noise and fills-in low SNR
regions in the spectral domain. Furthermore, it improves the resolution of
estimates on the problems considered for a wide range of SNR and produces
Fourier Shell Correlation curves that are insensitive to masking effects.
Conclusions: We analyze the assumptions in the model, discuss relations to
other regularization strategies, and postulate on potential implications for
structure determination in cryo-EM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Representations Robust to Group Shifts and Adversarial Examples. (arXiv:2202.09446v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09446">
<div class="article-summary-box-inner">
<span><p>Despite the high performance achieved by deep neural networks on various
tasks, extensive studies have demonstrated that small tweaks in the input could
fail the model predictions. This issue of deep neural networks has led to a
number of methods to improve model robustness, including adversarial training
and distributionally robust optimization. Though both of these two methods are
geared towards learning robust models, they have essentially different
motivations: adversarial training attempts to train deep neural networks
against perturbations, while distributional robust optimization aims at
improving model performance on the most difficult "uncertain distributions". In
this work, we propose an algorithm that combines adversarial training and group
distribution robust optimization to improve robust representation learning.
Experiments on three image benchmark datasets illustrate that the proposed
method achieves superior results on robust metrics without sacrificing much of
the standard measures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modern Augmented Reality: Applications, Trends, and Future Directions. (arXiv:2202.09450v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09450">
<div class="article-summary-box-inner">
<span><p>Augmented reality (AR) is one of the relatively old, yet trending areas in
the intersection of computer vision and computer graphics with numerous
applications in several areas, from gaming and entertainment, to education and
healthcare. Although it has been around for nearly fifty years, it has seen a
lot of interest by the research community in the recent years, mainly because
of the huge success of deep learning models for various computer vision and AR
applications, which made creating new generations of AR technologies possible.
This work tries to provide an overview of modern augmented reality, from both
application-level and technical perspective. We first give an overview of main
AR applications, grouped into more than ten categories. We then give an
overview of around 100 recent promising machine learning based works developed
for AR systems, such as deep learning works for AR shopping (clothing, makeup),
AR based image filters (such as Snapchat's lenses), AR animations, and more. In
the end we discuss about some of the current challenges in AR domain, and the
future directions in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAGE: SLAM with Appearance and Geometry Prior for Endoscopy. (arXiv:2202.09487v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09487">
<div class="article-summary-box-inner">
<span><p>In endoscopy, many applications (e.g., surgical navigation) would benefit
from a real-time method that can simultaneously track the endoscope and
reconstruct the dense 3D geometry of the observed anatomy from a monocular
endoscopic video. To this end, we develop a Simultaneous Localization and
Mapping system by combining the learning-based appearance and optimizable
geometry priors and factor graph optimization. The appearance and geometry
priors are explicitly learned in an end-to-end differentiable training pipeline
to master the task of pair-wise image alignment, one of the core components of
the SLAM system. In our experiments, the proposed SLAM system is shown to
robustly handle the challenges of texture scarceness and illumination variation
that are commonly seen in endoscopy. The system generalizes well to unseen
endoscopes and subjects and performs favorably compared with a state-of-the-art
feature-based SLAM system. The code repository is available at
https://github.com/lppllppl920/SAGE-SLAM.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Highlighting Object Category Immunity for the Generalization of Human-Object Interaction Detection. (arXiv:2202.09492v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09492">
<div class="article-summary-box-inner">
<span><p>Human-Object Interaction (HOI) detection plays a core role in activity
understanding. As a compositional learning problem (human-verb-object),
studying its generalization matters. However, widely-used metric mean average
precision (mAP) fails to model the compositional generalization well. Thus, we
propose a novel metric, mPD (mean Performance Degradation), as a complementary
of mAP to evaluate the performance gap among compositions of different objects
and the same verb. Surprisingly, mPD reveals that previous methods usually
generalize poorly. With mPD as a cue, we propose Object Category (OC) Immunity
to boost HOI generalization. The idea is to prevent model from learning
spurious object-verb correlations as a short-cut to over-fit the train set. To
achieve OC-immunity, we propose an OC-immune network that decouples the inputs
from OC, extracts OC-immune representations, and leverages uncertainty
quantification to generalize to unseen objects. In both conventional and
zero-shot experiments, our method achieves decent improvements. To fully
evaluate the generalization, we design a new and more difficult benchmark, on
which we present significant advantage. The code is available at
https://github.com/Foruck/OC-Immunity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PMP-Net++: Point Cloud Completion by Transformer-Enhanced Multi-step Point Moving Paths. (arXiv:2202.09507v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09507">
<div class="article-summary-box-inner">
<span><p>Point cloud completion concerns to predict missing part for incomplete 3D
shapes. A common strategy is to generate complete shape according to incomplete
input. However, unordered nature of point clouds will degrade generation of
high-quality 3D shapes, as detailed topology and structure of unordered points
are hard to be captured during the generative process using an extracted latent
code. We address this problem by formulating completion as point cloud
deformation process. Specifically, we design a novel neural network, named
PMP-Net++, to mimic behavior of an earth mover. It moves each point of
incomplete input to obtain a complete point cloud, where total distance of
point moving paths (PMPs) should be the shortest. Therefore, PMP-Net++ predicts
unique PMP for each point according to constraint of point moving distances.
The network learns a strict and unique correspondence on point-level, and thus
improves quality of predicted complete shape. Moreover, since moving points
heavily relies on per-point features learned by network, we further introduce a
transformer-enhanced representation learning network, which significantly
improves completion performance of PMP-Net++. We conduct comprehensive
experiments in shape completion, and further explore application on point cloud
up-sampling, which demonstrate non-trivial improvement of PMP-Net++ over
state-of-the-art point cloud completion/up-sampling methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPNet: A novel deep neural network for retinal vessel segmentation based on shared decoder and pyramid-like loss. (arXiv:2202.09515v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09515">
<div class="article-summary-box-inner">
<span><p>Segmentation of retinal vessel images is critical to the diagnosis of
retinopathy. Recently, convolutional neural networks have shown significant
ability to extract the blood vessel structure. However, it remains challenging
to refined segmentation for the capillaries and the edges of retinal vessels
due to thickness inconsistencies and blurry boundaries. In this paper, we
propose a novel deep neural network for retinal vessel segmentation based on
shared decoder and pyramid-like loss (SPNet) to address the above problems.
Specifically, we introduce a decoder-sharing mechanism to capture multi-scale
semantic information, where feature maps at diverse scales are decoded through
a sequence of weight-sharing decoder modules. Also, to strengthen
characterization on the capillaries and the edges of blood vessels, we define a
residual pyramid architecture which decomposes the spatial information in the
decoding phase. A pyramid-like loss function is designed to compensate possible
segmentation errors progressively. Experimental results on public benchmarks
show that the proposed method outperforms the backbone network and the
state-of-the-art methods, especially in the regions of the capillaries and the
vessel contours. In addition, performances on cross-datasets verify that SPNet
shows stronger generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">C2N: Practical Generative Noise Modeling for Real-World Denoising. (arXiv:2202.09533v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09533">
<div class="article-summary-box-inner">
<span><p>Learning-based image denoising methods have been bounded to situations where
well-aligned noisy and clean images are given, or samples are synthesized from
predetermined noise models, e.g., Gaussian. While recent generative noise
modeling methods aim to simulate the unknown distribution of real-world noise,
several limitations still exist. In a practical scenario, a noise generator
should learn to simulate the general and complex noise distribution without
using paired noisy and clean images. However, since existing methods are
constructed on the unrealistic assumption of real-world noise, they tend to
generate implausible patterns and cannot express complicated noise maps.
Therefore, we introduce a Clean-to-Noisy image generation framework, namely
C2N, to imitate complex real-world noise without using any paired examples. We
construct the noise generator in C2N accordingly with each component of
real-world noise characteristics to express a wide range of noise accurately.
Combined with our C2N, conventional denoising CNNs can be trained to outperform
existing unsupervised methods on challenging real-world benchmarks by a large
margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BP-Triplet Net for Unsupervised Domain Adaptation: A Bayesian Perspective. (arXiv:2202.09541v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09541">
<div class="article-summary-box-inner">
<span><p>Triplet loss, one of the deep metric learning (DML) methods, is to learn the
embeddings where examples from the same class are closer than examples from
different classes. Motivated by DML, we propose an effective BP-Triplet Loss
for unsupervised domain adaption (UDA) from the perspective of Bayesian
learning and we name the model as BP-Triplet Net. In previous metric learning
based methods for UDA, sample pairs across domains are treated equally, which
is not appropriate due to the domain bias. In our work, considering the
different importance of pair-wise samples for both feature learning and domain
alignment, we deduce our BP-Triplet loss for effective UDA from the perspective
of Bayesian learning. Our BP-Triplet loss adjusts the weights of pair-wise
samples in intra domain and inter domain. Especially, it can self attend to the
hard pairs (including hard positive pair and hard negative pair). Together with
the commonly used adversarial loss for domain alignment, the quality of target
pseudo labels is progressively improved. Our method achieved low joint error of
the ideal source and target hypothesis. The expected target error can then be
upper bounded following Ben-David s theorem. Comprehensive evaluations on five
benchmark datasets, handwritten digits, Office31, ImageCLEF-DA, Office-Home and
VisDA-2017 demonstrate the effectiveness of the proposed approach for UDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Going Deeper into Recognizing Actions in Dark Environments: A Comprehensive Benchmark Study. (arXiv:2202.09545v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09545">
<div class="article-summary-box-inner">
<span><p>While action recognition (AR) has gained large improvements with the
introduction of large-scale video datasets and the development of deep neural
networks, AR models robust to challenging environments in real-world scenarios
are still under-explored. We focus on the task of action recognition in dark
environments, which can be applied to fields such as surveillance and
autonomous driving at night. Intuitively, current deep networks along with
visual enhancement techniques should be able to handle AR in dark environments,
however, it is observed that this is not always the case in practice. To dive
deeper into exploring solutions for AR in dark environments, we launched the
UG2+ Challenge Track 2 (UG2-2) in IEEE CVPR 2021, with a goal of evaluating and
advancing the robustness of AR models in dark environments. The challenge
builds and expands on top of a novel ARID dataset, the first dataset for the
task of dark video AR, and guides models to tackle such a task in both fully
and semi-supervised manners. Baseline results utilizing current AR models and
enhancement methods are reported, justifying the challenging nature of this
task with substantial room for improvements. Thanks to the active participation
from the research community, notable advances have been made in participants'
solutions, while analysis of these solutions helped better identify possible
directions to tackle the challenge of AR in dark environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">student dangerous behavior detection in school. (arXiv:2202.09550v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09550">
<div class="article-summary-box-inner">
<span><p>Video surveillance systems have been installed to ensure the student safety
in schools. However, discovering dangerous behaviors, such as fighting and
falling down, usually depends on untimely human observations. In this paper, we
focus on detecting dangerous behaviors of students automatically, which faces
numerous challenges, such as insufficient datasets, confusing postures,
keyframes detection and prompt response. To address these challenges, we first
build a danger behavior dataset with locations and labels from surveillance
videos, and transform action recognition of long videos to an object detection
task that avoids keyframes detection. Then, we propose a novel end-to-end
dangerous behavior detection method, named DangerDet, that combines multi-scale
body features and keypoints-based pose features. We could improve the accuracy
of behavior classification due to the highly correlation between pose and
behavior. On our dataset, DangerDet achieves 71.0\% mAP with about 11 FPS. It
keeps a better balance between the accuracy and time cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Holistic Attention-Fusion Adversarial Network for Single Image Defogging. (arXiv:2202.09553v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09553">
<div class="article-summary-box-inner">
<span><p>Adversarial learning-based image defogging methods have been extensively
studied in computer vision due to their remarkable performance. However, most
existing methods have limited defogging capabilities for real cases because
they are trained on the paired clear and synthesized foggy images of the same
scenes. In addition, they have limitations in preserving vivid color and rich
textual details in defogging. To address these issues, we develop a novel
generative adversarial network, called holistic attention-fusion adversarial
network (HAAN), for single image defogging. HAAN consists of a Fog2Fogfree
block and a Fogfree2Fog block. In each block, there are three learning-based
modules, namely, fog removal, color-texture recovery, and fog synthetic, that
are constrained each other to generate high quality images. HAAN is designed to
exploit the self-similarity of texture and structure information by learning
the holistic channel-spatial feature correlations between the foggy image with
its several derived images. Moreover, in the fog synthetic module, we utilize
the atmospheric scattering model to guide it to improve the generative quality
by focusing on an atmospheric light optimization with a novel sky segmentation
network. Extensive experiments on both synthetic and real-world datasets show
that HAAN outperforms state-of-the-art defogging methods in terms of
quantitative accuracy and subjective visual quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SODA: Site Object Detection dAtaset for Deep Learning in Construction. (arXiv:2202.09554v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09554">
<div class="article-summary-box-inner">
<span><p>Computer vision-based deep learning object detection algorithms have been
developed sufficiently powerful to support the ability to recognize various
objects. Although there are currently general datasets for object detection,
there is still a lack of large-scale, open-source dataset for the construction
industry, which limits the developments of object detection algorithms as they
tend to be data-hungry. Therefore, this paper develops a new large-scale image
dataset specifically collected and annotated for the construction site, called
Site Object Detection dAtaset (SODA), which contains 15 kinds of object classes
categorized by workers, materials, machines, and layout. Firstly, more than
20,000 images were collected from multiple construction sites in different site
conditions, weather conditions, and construction phases, which covered
different angles and perspectives. After careful screening and processing,
19,846 images including 286,201 objects were then obtained and annotated with
labels in accordance with predefined categories. Statistical analysis shows
that the developed dataset is advantageous in terms of diversity and volume.
Further evaluation with two widely-adopted object detection algorithms based on
deep learning (YOLO v3/ YOLO v4) also illustrates the feasibility of the
dataset for typical construction scenarios, achieving a maximum mAP of 81.47%.
In this manner, this research contributes a large-scale image dataset for the
development of deep learning-based object detection methods in the construction
industry and sets up a performance benchmark for further evaluation of
corresponding algorithms in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HDAM: Heuristic Difference Attention Module for Convolutional Neural Networks. (arXiv:2202.09556v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09556">
<div class="article-summary-box-inner">
<span><p>The attention mechanism is one of the most important priori knowledge to
enhance convolutional neural networks. Most attention mechanisms are bound to
the convolutional layer and use local or global contextual information to
recalibrate the input. This is a popular attention strategy design method.
Global contextual information helps the network to consider the overall
distribution, while local contextual information is more general. The
contextual information makes the network pay attention to the mean or maximum
value of a particular receptive field. Different from the most attention
mechanism, this article proposes a novel attention mechanism with the heuristic
difference attention module, HDAM. HDAM's input recalibration is based on the
difference between the local and global contextual information instead of the
mean and maximum values. At the same time, to make different layers have a more
suitable local receptive field size and increase the exibility of the local
receptive field design, we use genetic algorithm to heuristically produce local
receptive fields. First, HDAM extracts the mean value of the global and local
receptive fields as the corresponding contextual information. Then the
difference between the global and local contextual information is calculated.
Finally HDAM uses this difference to recalibrate the input. In addition, we use
the heuristic ability of genetic algorithm to search for the local receptive
field size of each layer. Our experiments on CIFAR-10 and CIFAR-100 show that
HDAM can use fewer parameters than other attention mechanisms to achieve higher
accuracy. We implement HDAM with the Python library, Pytorch, and the code and
models will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Priming Cross-Session Motor Imagery Classification with A Universal Deep Domain Adaptation Framework. (arXiv:2202.09559v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09559">
<div class="article-summary-box-inner">
<span><p>Motor imagery (MI) is a common brain computer interface (BCI) paradigm. EEG
is non-stationary with low signal-to-noise, classifying motor imagery tasks of
the same participant from different EEG recording sessions is generally
challenging, as EEG data distribution may vary tremendously among different
acquisition sessions. Although it is intuitive to consider the cross-session MI
classification as a domain adaptation problem, the rationale and feasible
approach is not elucidated. In this paper, we propose a Siamese deep domain
adaptation (SDDA) framework for cross-session MI classification based on
mathematical models in domain adaptation theory. The proposed framework can be
easily applied to most existing artificial neural networks without altering the
network structure, which facilitates our method with great flexibility and
transferability. In the proposed framework, domain invariants were firstly
constructed jointly with channel normalization and Euclidean alignment. Then,
embedding features from source and target domain were mapped into the
Reproducing Kernel Hilbert Space (RKHS) and aligned accordingly. A cosine-based
center loss was also integrated into the framework to improve the
generalizability of the SDDA. The proposed framework was validated with two
classic and popular convolutional neural networks from BCI research field
(EEGNet and ConvNet) in two MI-EEG public datasets (BCI Competition IV IIA,
IIB). Compared to the vanilla EEGNet and ConvNet, the proposed SDDA framework
was able to boost the MI classification accuracy by 15.2%, 10.2% respectively
in IIA dataset, and 5.5%, 4.2% in IIB dataset. The final MI classification
accuracy reached 82.01% in IIA dataset and 87.52% in IIB, which outperformed
the state-of-the-art methods in the literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bit-wise Training of Neural Network Weights. (arXiv:2202.09571v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09571">
<div class="article-summary-box-inner">
<span><p>We introduce an algorithm where the individual bits representing the weights
of a neural network are learned. This method allows training weights with
integer values on arbitrary bit-depths and naturally uncovers sparse networks,
without additional constraints or regularization techniques. We show better
results than the standard training technique with fully connected networks and
similar performance as compared to standard training for convolutional and
residual networks. By training bits in a selective manner we found that the
biggest contribution to achieving high accuracy is given by the first three
most significant bits, while the rest provide an intrinsic regularization. As a
consequence more than 90\% of a network can be used to store arbitrary codes
without affecting its accuracy. These codes may be random noise, binary files
or even the weights of previously trained networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diversity aware image generation. (arXiv:2202.09573v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09573">
<div class="article-summary-box-inner">
<span><p>The machine learning generative algorithms such as GAN and VAE show
impressive results in practice when constructing images similar to those in a
training set. However, the generation of new images builds mainly on the
understanding of the hidden structure of the training database followed by a
mere sampling from a multi-dimensional normal variable. In particular each
sample is independent from the other ones and can repeatedly propose same type
of images. To cure this drawback we propose a kernel-based measure
representation method that can produce new objects from a given target measure
by approximating the measure as a whole and even staying away from objects
already drawn from that distribution. This ensures a better variety of the
produced images. The method is tested on some classic machine learning
benchmarks.\end{abstract}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tripartite: Tackle Noisy Labels by a More Precise Partition. (arXiv:2202.09579v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09579">
<div class="article-summary-box-inner">
<span><p>Samples in large-scale datasets may be mislabeled due to various reasons, and
Deep Neural Networks can easily over-fit to the noisy label data. To tackle
this problem, the key point is to alleviate the harm of these noisy labels.
Many existing methods try to divide training data into clean and noisy subsets
in terms of loss values, and then process the noisy label data varied. One of
the reasons hindering a better performance is the hard samples. As hard samples
always have relatively large losses whether their labels are clean or noisy,
these methods could not divide them precisely. Instead, we propose a Tripartite
solution to partition training data more precisely into three subsets: hard,
noisy, and clean. The partition criteria are based on the inconsistent
predictions of two networks, and the inconsistency between the prediction of a
network and the given label. To minimize the harm of noisy labels but maximize
the value of noisy label data, we apply a low-weight learning on hard data and
a self-supervised learning on noisy label data without using the given labels.
Extensive experiments demonstrate that Tripartite can filter out noisy label
data more precisely, and outperforms most state-of-the-art methods on five
benchmark datasets, especially on real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image-to-Graph Transformers for Chemical Structure Recognition. (arXiv:2202.09580v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09580">
<div class="article-summary-box-inner">
<span><p>For several decades, chemical knowledge has been published in written text,
and there have been many attempts to make it accessible, for example, by
transforming such natural language text to a structured format. Although the
discovered chemical itself commonly represented in an image is the most
important part, the correct recognition of the molecular structure from the
image in literature still remains a hard problem since they are often
abbreviated to reduce the complexity and drawn in many different styles. In
this paper, we present a deep learning model to extract molecular structures
from images. The proposed model is designed to transform the molecular image
directly into the corresponding graph, which makes it capable of handling
non-atomic symbols for abbreviations. Also, by end-to-end learning approach it
can fully utilize many open image-molecule pair data from various sources, and
hence it is more robust to image style variation than other tools. The
experimental results show that the proposed model outperforms the existing
models with 17.1 % and 12.8 % relative improvement for well-known benchmark
datasets and large molecular images that we collected from literature,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Lightweight Dual-Domain Attention Framework for Sparse-View CT Reconstruction. (arXiv:2202.09609v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09609">
<div class="article-summary-box-inner">
<span><p>Computed Tomography (CT) plays an essential role in clinical diagnosis. Due
to the adverse effects of radiation on patients, the radiation dose is expected
to be reduced as low as possible. Sparse sampling is an effective way, but it
will lead to severe artifacts on the reconstructed CT image, thus sparse-view
CT image reconstruction has been a prevailing and challenging research area.
With the popularity of mobile devices, the requirements for lightweight and
real-time networks are increasing rapidly. In this paper, we design a novel
lightweight network called CAGAN, and propose a dual-domain reconstruction
pipeline for parallel beam sparse-view CT. CAGAN is an adversarial
auto-encoder, combining the Coordinate Attention unit, which preserves the
spatial information of features. Also, the application of Shuffle Blocks
reduces the parameters by a quarter without sacrificing its performance. In the
Radon domain, the CAGAN learns the mapping between the interpolated data and
fringe-free projection data. After the restored Radon data is reconstructed to
an image, the image is sent into the second CAGAN trained for recovering the
details, so that a high-quality image is obtained. Experiments indicate that
the CAGAN strikes an excellent balance between model complexity and
performance, and our pipeline outperforms the DD-Net and the DuDoNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Unsupervised Attentive-Adversarial Learning Framework for Single Image Deraining. (arXiv:2202.09635v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09635">
<div class="article-summary-box-inner">
<span><p>Single image deraining has been an important topic in low-level computer
vision tasks. The atmospheric veiling effect (which is generated by rain
accumulation, similar to fog) usually appears with the rain. Most deep
learning-based single image deraining methods mainly focus on rain streak
removal by disregarding this effect, which leads to low-quality deraining
performance. In addition, these methods are trained only on synthetic data,
hence they do not take into account real-world rainy images. To address the
above issues, we propose a novel unsupervised attentive-adversarial learning
framework (UALF) for single image deraining that trains on both synthetic and
real rainy images while simultaneously capturing both rain streaks and rain
accumulation features. UALF consists of a Rain-fog2Clean (R2C) transformation
block and a Clean2Rain-fog (C2R) transformation block. In R2C, to better
characterize the rain-fog fusion feature and to achieve high-quality deraining
performance, we employ an attention rain-fog feature extraction network (ARFE)
to exploit the self-similarity of global and local rain-fog information by
learning the spatial feature correlations. Moreover, to improve the
transformation ability of C2R, we design a rain-fog feature decoupling and
reorganization network (RFDR) by embedding a rainy image degradation model and
a mixed discriminator to preserve richer texture details. Extensive experiments
on benchmark rain-fog and rain datasets show that UALF outperforms
state-of-the-art deraining methods. We also conduct defogging performance
evaluation experiments to further demonstrate the effectiveness of UALF
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Echofilter: A Deep Learning Segmentation Model Improves the Automation, Standardization, and Timeliness for Post-Processing Echosounder Data in Tidal Energy Streams. (arXiv:2202.09648v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09648">
<div class="article-summary-box-inner">
<span><p>Understanding the abundance and distribution of fish in tidal energy streams
is important for assessing the risk presented by the introduction of tidal
energy devices into the habitat. However, the impressive tidal currents that
make sites favorable for tidal energy development are often highly turbulent
and entrain air into the water, complicating the interpretation of echosounder
data. The portion of the water column contaminated by returns from entrained
air must be excluded from data used for biological analyses. Application of a
single algorithm to identify the depth-of-penetration of entrained-air is
insufficient for a boundary that is discontinuous, depth-dynamic, porous, and
widely variable across the tidal flow speeds which can range from 0 to 5m/s.
Using a case study at a tidal energy demonstration site in the Bay of Fundy, we
describe the development and application of deep learning models that produce a
pronounced, consistent, substantial, and measurable improvement of the
automated detection of the extent to which entrained-air has penetrated the
water column.
</p>
<p>Our model, Echofilter, was highly responsive to the dynamic range of
turbulence conditions and sensitive to the fine-scale nuances in the boundary
position, producing an entrained-air boundary line with an average error of
0.32m on mobile downfacing and 0.5-1.0m on stationary upfacing data. The
model's annotations had a high level of agreement with the human segmentation
(mobile downfacing Jaccard index: 98.8%; stationary upfacing: 93-95%). This
resulted in a 50% reduction in the time required for manual edits compared to
the time required to manually edit the line placed by currently available
algorithms. Because of the improved initial automated placement, the
implementation of the models generated a marked increase in the standardization
and repeatability of line placement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Region-Based Semantic Factorization in GANs. (arXiv:2202.09649v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09649">
<div class="article-summary-box-inner">
<span><p>Despite the rapid advancement of semantic discovery in the latent space of
Generative Adversarial Networks (GANs), existing approaches either are limited
to finding global attributes or rely on a number of segmentation masks to
identify local attributes. In this work, we present a highly efficient
algorithm to factorize the latent semantics learned by GANs concerning an
arbitrary image region. Concretely, we revisit the task of local manipulation
with pre-trained GANs and formulate region-based semantic discovery as a dual
optimization problem. Through an appropriately defined generalized Rayleigh
quotient, we manage to solve such a problem without any annotations or
training. Experimental results on various state-of-the-art GAN models
demonstrate the effectiveness of our approach, as well as its superiority over
prior arts regarding precise control, region robustness, speed of
implementation, and simplicity of use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MSSNet: Multi-Scale-Stage Network for Single Image Deblurring. (arXiv:2202.09652v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09652">
<div class="article-summary-box-inner">
<span><p>Most of traditional single image deblurring methods before deep learning
adopt a coarse-to-fine scheme that estimates a sharp image at a coarse scale
and progressively refines it at finer scales. While this scheme has also been
adopted to several deep learning-based approaches, recently a number of
single-scale approaches have been introduced showing superior performance to
previous coarse-to-fine approaches both in quality and computation time, making
the traditional coarse-to-fine scheme seemingly obsolete. In this paper, we
revisit the coarse-to-fine scheme, and analyze defects of previous
coarse-to-fine approaches that degrade their performance. Based on the
analysis, we propose Multi-Scale-Stage Network (MSSNet), a novel deep
learning-based approach to single image deblurring that adopts our remedies to
the defects. Specifically, MSSNet adopts three novel technical components:
stage configuration reflecting blur scales, an inter-scale information
propagation scheme, and a pixel-shuffle-based multi-scale scheme. Our
experiments show that MSSNet achieves the state-of-the-art performance in terms
of quality, network size, and computation time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Punctuation Restoration. (arXiv:2202.09695v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09695">
<div class="article-summary-box-inner">
<span><p>Given the increasing number of livestreaming videos, automatic speech
recognition and post-processing for livestreaming video transcripts are crucial
for efficient data management as well as knowledge mining. A key step in this
process is punctuation restoration which restores fundamental text structures
such as phrase and sentence boundaries from the video transcripts. This work
presents a new human-annotated corpus, called BehancePR, for punctuation
restoration in livestreaming video transcripts. Our experiments on BehancePR
demonstrate the challenges of punctuation restoration for this domain.
Furthermore, we show that popular natural language processing toolkits are
incapable of detecting sentence boundary on non-punctuated transcripts of
livestreaming videos, calling for more research effort to develop robust models
for this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MANet: Improving Video Denoising with a Multi-Alignment Network. (arXiv:2202.09704v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09704">
<div class="article-summary-box-inner">
<span><p>In video denoising, the adjacent frames often provide very useful
information, but accurate alignment is needed before such information can be
harnassed. In this work, we present a multi-alignment network, which generates
multiple flow proposals followed by attention-based averaging. It serves to
mimics the non-local mechanism, suppressing noise by averaging multiple
observations. Our approach can be applied to various state-of-the-art models
that are based on flow estimation. Experiments on a large-scale video dataset
demonstrate that our method improves the denoising baseline model by 0.2dB, and
further reduces the parameters by 47% with model distillation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Statistical and Topological Summaries Aid Disease Detection for Segmented Retinal Vascular Images. (arXiv:2202.09708v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09708">
<div class="article-summary-box-inner">
<span><p>Disease complications can alter vascular network morphology and disrupt
tissue functioning. Diabetic retinopathy, for example, is a complication of
type 1 and 2 diabetus mellitus that can cause blindness. Microvascular diseases
are assessed by visual inspection of retinal images, but this can be
challenging when diseases exhibit silent symptoms or patients cannot attend
in-person meetings. We examine the performance of machine learning algorithms
in detecting microvascular disease when trained on either statistical or
topological summaries of segmented retinal vascular images. We apply our
methods to four publicly-available datasets and find that the fractal dimension
performs best for high resolution images. By contrast, we find that topological
descriptor vectors quantifying the number of loops in the data achieve the
highest accuracy for low resolution images. Further analysis, using the
topological approach, reveals that microvascular disease may alter morphology
by reducing the number of loops in the retinal vasculature. Our work provides
preliminary guidelines on which methods are most appropriate for assessing
disease in high and low resolution images. In the longer term, these methods
could be incorporated into automated disease assessment tools.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ARM3D: Attention-based relation module for indoor 3D object detection. (arXiv:2202.09715v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09715">
<div class="article-summary-box-inner">
<span><p>Relation context has been proved to be useful for many challenging vision
tasks. In the field of 3D object detection, previous methods have been taking
the advantage of context encoding, graph embedding, or explicit relation
reasoning to extract relation context. However, there exists inevitably
redundant relation context due to noisy or low-quality proposals. In fact,
invalid relation context usually indicates underlying scene misunderstanding
and ambiguity, which may, on the contrary, reduce the performance in complex
scenes. Inspired by recent attention mechanism like Transformer, we propose a
novel 3D attention-based relation module (ARM3D). It encompasses object-aware
relation reasoning to extract pair-wise relation contexts among qualified
proposals and an attention module to distribute attention weights towards
different relation contexts. In this way, ARM3D can take full advantage of the
useful relation context and filter those less relevant or even confusing
contexts, which mitigates the ambiguity in detection. We have evaluated the
effectiveness of ARM3D by plugging it into several state-of-the-art 3D object
detectors and showing more accurate and robust detection results. Extensive
experiments show the capability and generalization of ARM3D on 3D object
detection. Our source code is available at https://github.com/lanlan96/ARM3D.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Unified Approach to Homography Estimation Using Image Features and Pixel Intensities. (arXiv:2202.09716v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09716">
<div class="article-summary-box-inner">
<span><p>The homography matrix is a key component in various vision-based robotic
tasks. Traditionally, homography estimation algorithms are classified into
feature- or intensity-based. The main advantages of the latter are their
versatility, accuracy, and robustness to arbitrary illumination changes. On the
other hand, they have a smaller domain of convergence than the feature-based
solutions. Their combination is hence promising, but existing techniques only
apply them sequentially. This paper proposes a new hybrid method that unifies
both classes into a single nonlinear optimization procedure, applies the same
minimization method, and uses the same homography parametrization and warping
function. Experimental validation using a classical testing framework shows
that the proposed unified approach has improved convergence properties compared
to each individual class. These are also demonstrated in a visual tracking
application. As a final contribution, our ready-to-use implementation of the
algorithm is made publicly available to the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3DRM:Pair-wise relation module for 3D object detection. (arXiv:2202.09721v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09721">
<div class="article-summary-box-inner">
<span><p>Context has proven to be one of the most important factors in object layout
reasoning for 3D scene understanding. Existing deep contextual models either
learn holistic features for context encoding or rely on pre-defined scene
templates for context modeling. We argue that scene understanding benefits from
object relation reasoning, which is capable of mitigating the ambiguity of 3D
object detections and thus helps locate and classify the 3D objects more
accurately and robustly. To achieve this, we propose a novel 3D relation module
(3DRM) which reasons about object relations at pair-wise levels. The 3DRM
predicts the semantic and spatial relationships between objects and extracts
the object-wise relation features. We demonstrate the effects of 3DRM by
plugging it into proposal-based and voting-based 3D object detection pipelines,
respectively. Extensive evaluations show the effectiveness and generalization
of 3DRM on 3D object detection. Our source code is available at
https://github.com/lanlan96/3DRM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overparametrization improves robustness against adversarial attacks: A replication study. (arXiv:2202.09735v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09735">
<div class="article-summary-box-inner">
<span><p>Overparametrization has become a de facto standard in machine learning.
Despite numerous efforts, our understanding of how and where
overparametrization helps model accuracy and robustness is still limited. To
this end, here we conduct an empirical investigation to systemically study and
replicate previous findings in this area, in particular the study by Madry et
al. Together with this study, our findings support the "universal law of
robustness" recently proposed by Bubeck et al. We argue that while critical for
robust perception, overparametrization may not be enough to achieve full
robustness and smarter architectures e.g. the ones implemented by the human
visual cortex) seem inevitable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Loop Game: Quality Assessment and Optimization for Low-Light Image Enhancement. (arXiv:2202.09738v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09738">
<div class="article-summary-box-inner">
<span><p>There is an increasing consensus that the design and optimization of low
light image enhancement methods need to be fully driven by perceptual quality.
With numerous approaches proposed to enhance low-light images, much less work
has been dedicated to quality assessment and quality optimization of low-light
enhancement. In this paper, to close the gap between enhancement and
assessment, we propose a loop enhancement framework that produces a clear
picture of how the enhancement of low-light images could be optimized towards
better visual quality. In particular, we create a large-scale database for
QUality assessment Of The Enhanced LOw-Light Image (QUOTE-LOL), which serves as
the foundation in studying and developing objective quality assessment
measures. The objective quality assessment measure plays a critical bridging
role between visual quality and enhancement and is further incorporated in the
optimization in learning the enhancement model towards perceptual optimally.
Finally, we iteratively perform the enhancement and optimization tasks,
enhancing the low-light images continuously. The superiority of the proposed
scheme is validated based on various low-light scenes. The database as well as
the code will be available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Attention Network. (arXiv:2202.09741v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09741">
<div class="article-summary-box-inner">
<span><p>While originally designed for natural language processing (NLP) tasks, the
self-attention mechanism has recently taken various computer vision areas by
storm. However, the 2D nature of images brings three challenges for applying
self-attention in computer vision. (1) Treating images as 1D sequences neglects
their 2D structures. (2) The quadratic complexity is too expensive for
high-resolution images. (3) It only captures spatial adaptability but ignores
channel adaptability. In this paper, we propose a novel large kernel attention
(LKA) module to enable self-adaptive and long-range correlations in
self-attention while avoiding the above issues. We further introduce a novel
neural network based on LKA, namely Visual Attention Network (VAN). While
extremely simple and efficient, VAN outperforms the state-of-the-art vision
transformers and convolutional neural networks with a large margin in extensive
experiments, including image classification, object detection, semantic
segmentation, instance segmentation, etc. Code is available at
https://github.com/Visual-Attention-Network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RDP-Net: Region Detail Preserving Network for Change Detection. (arXiv:2202.09745v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09745">
<div class="article-summary-box-inner">
<span><p>Change detection (CD) is an essential earth observation technique. It
captures the dynamic information of land objects. With the rise of deep
learning, neural networks (NN) have shown great potential in CD. However,
current NN models introduce backbone architectures that lose the detail
information during learning. Moreover, current NN models are heavy in
parameters, which prevents their deployment on edge devices such as drones. In
this work, we tackle this issue by proposing RDP-Net: a region detail
preserving network for CD. We propose an efficient training strategy that
quantifies the importance of individual samples during the warmup period of NN
training. Then, we perform non-uniform sampling based on the importance score
so that the NN could learn detail information from easy to hard. Next, we
propose an effective edge loss that improves the network's attention on details
such as boundaries and small regions. As a result, we provide a NN model that
achieves the state-of-the-art empirical performance in CD with only 1.70M
parameters. We hope our RDP-Net would benefit the practical CD applications on
compact devices and could inspire more people to bring change detection to a
new level with the efficient training strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Spatial Propagation Network for Depth Completion. (arXiv:2202.09769v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09769">
<div class="article-summary-box-inner">
<span><p>Image-guided depth completion aims to generate dense depth maps with sparse
depth measurements and corresponding RGB images. Currently, spatial propagation
networks (SPNs) are the most popular affinity-based methods in depth
completion, but they still suffer from the representation limitation of the
fixed affinity and the over smoothing during iterations. Our solution is to
estimate independent affinity matrices in each SPN iteration, but it is
over-parameterized and heavy calculation. This paper introduces an efficient
model that learns the affinity among neighboring pixels with an
attention-based, dynamic approach. Specifically, the Dynamic Spatial
Propagation Network (DySPN) we proposed makes use of a non-linear propagation
model (NLPM). It decouples the neighborhood into parts regarding to different
distances and recursively generates independent attention maps to refine these
parts into adaptive affinity matrices. Furthermore, we adopt a diffusion
suppression (DS) operation so that the model converges at an early stage to
prevent over-smoothing of dense depth. Finally, in order to decrease the
computational cost required, we also introduce three variations that reduce the
amount of neighbors and attentions needed while still retaining similar
accuracy. In practice, our method requires less iteration to match the
performance of other SPNs and yields better results overall. DySPN outperforms
other state-of-the-art (SoTA) methods on KITTI Depth Completion (DC) evaluation
by the time of submission and is able to yield SoTA performance in NYU Depth v2
dataset as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo Numerical Methods for Diffusion Models on Manifolds. (arXiv:2202.09778v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09778">
<div class="article-summary-box-inner">
<span><p>Denoising Diffusion Probabilistic Models (DDPMs) can generate high-quality
samples such as image and audio samples. However, DDPMs require hundreds to
thousands of iterations to produce final samples. Several prior works have
successfully accelerated DDPMs through adjusting the variance schedule (e.g.,
Improved Denoising Diffusion Probabilistic Models) or the denoising equation
(e.g., Denoising Diffusion Implicit Models (DDIMs)). However, these
acceleration methods cannot maintain the quality of samples and even introduce
new noise at a high speedup rate, which limit their practicability. To
accelerate the inference process while keeping the sample quality, we provide a
fresh perspective that DDPMs should be treated as solving differential
equations on manifolds. Under such a perspective, we propose pseudo numerical
methods for diffusion models (PNDMs). Specifically, we figure out how to solve
differential equations on manifolds and show that DDIMs are simple cases of
pseudo numerical methods. We change several classical numerical methods to
corresponding pseudo numerical methods and find that the pseudo linear
multi-step method is the best in most situations. According to our experiments,
by directly using pre-trained models on Cifar10, CelebA and LSUN, PNDMs can
generate higher quality synthetic images with only 50 steps compared with
1000-step DDIMs (20x speedup), significantly outperform DDIMs with 250 steps
(by around 0.4 in FID) and have good generalization on different variance
schedules. Our implementation is available at
https://github.com/luping-liu/PNDM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clustering by the Probability Distributions from Extreme Value Theory. (arXiv:2202.09784v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09784">
<div class="article-summary-box-inner">
<span><p>Clustering is an essential task to unsupervised learning. It tries to
automatically separate instances into coherent subsets. As one of the most
well-known clustering algorithms, k-means assigns sample points at the boundary
to a unique cluster, while it does not utilize the information of sample
distribution or density. Comparably, it would potentially be more beneficial to
consider the probability of each sample in a possible cluster. To this end,
this paper generalizes k-means to model the distribution of clusters. Our novel
clustering algorithm thus models the distributions of distances to centroids
over a threshold by Generalized Pareto Distribution (GPD) in Extreme Value
Theory (EVT). Notably, we propose the concept of centroid margin distance, use
GPD to establish a probability model for each cluster, and perform a clustering
algorithm based on the covering probability function derived from GPD. Such a
GPD k-means thus enables the clustering algorithm from the probabilistic
perspective. Correspondingly, we also introduce a naive baseline, dubbed as
Generalized Extreme Value (GEV) k-means. GEV fits the distribution of the block
maxima. In contrast, the GPD fits the distribution of distance to the centroid
exceeding a sufficiently large threshold, leading to a more stable performance
of GPD k-means. Notably, GEV k-means can also estimate cluster structure and
thus perform reasonably well over classical k-means. Thus, extensive
experiments on synthetic datasets and real datasets demonstrate that GPD
k-means outperforms competitors. The github codes are released in
https://github.com/sixiaozheng/EVT-K-means.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image quality assessment by overlapping task-specific and task-agnostic measures: application to prostate multiparametric MR images for cancer segmentation. (arXiv:2202.09798v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09798">
<div class="article-summary-box-inner">
<span><p>Image quality assessment (IQA) in medical imaging can be used to ensure that
downstream clinical tasks can be reliably performed. Quantifying the impact of
an image on the specific target tasks, also named as task amenability, is
needed. A task-specific IQA has recently been proposed to learn an
image-amenability-predicting controller simultaneously with a target task
predictor. This allows for the trained IQA controller to measure the impact an
image has on the target task performance, when this task is performed using the
predictor, e.g. segmentation and classification neural networks in modern
clinical applications. In this work, we propose an extension to this
task-specific IQA approach, by adding a task-agnostic IQA based on
auto-encoding as the target task. Analysing the intersection between
low-quality images, deemed by both the task-specific and task-agnostic IQA, may
help to differentiate the underpinning factors that caused the poor target task
performance. For example, common imaging artefacts may not adversely affect the
target task, which would lead to a low task-agnostic quality and a high
task-specific quality, whilst individual cases considered clinically
challenging, which can not be improved by better imaging equipment or
protocols, is likely to result in a high task-agnostic quality but a low
task-specific quality. We first describe a flexible reward shaping strategy
which allows for the adjustment of weighting between task-agnostic and
task-specific quality scoring. Furthermore, we evaluate the proposed algorithm
using a clinically challenging target task of prostate tumour segmentation on
multiparametric magnetic resonance (mpMR) images, from 850 patients. The
proposed reward shaping strategy, with appropriately weighted task-specific and
task-agnostic qualities, successfully identified samples that need
re-acquisition due to defected imaging process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distortion-Aware Loop Filtering of Intra 360^o Video Coding with Equirectangular Projection. (arXiv:2202.09802v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09802">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a distortion-aware loop filtering model to improve
the performance of intra coding for 360$^o$ videos projected via
equirectangular projection (ERP) format. To enable the awareness of distortion,
our proposed module analyzes content characteristics based on a coding unit
(CU) partition mask and processes them through partial convolution to activate
the specified area. The feature recalibration module, which leverages cascaded
residual channel-wise attention blocks (RCABs) to adjust the inter-channel and
intra-channel features automatically, is capable of adapting with different
quality levels. The perceptual geometry optimization combining with weighted
mean squared error (WMSE) and the perceptual loss guarantees both the local
field of view (FoV) and global image reconstruction with high quality.
Extensive experimental results show that our proposed scheme achieves
significant bitrate savings compared with the anchor (HM + 360Lib), leading to
8.9%, 9.0%, 7.1% and 7.4% on average bit rate reductions in terms of PSNR,
WPSNR, and PSNR of two viewports for luminance component of 360^o videos,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alternative design of DeepPDNet in the context of image restoration. (arXiv:2202.09810v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09810">
<div class="article-summary-box-inner">
<span><p>This work designs an image restoration deep network relying on unfolded
Chambolle-Pock primal-dual iterations. Each layer of our network is built from
Chambolle-Pock iterations when specified for minimizing a sum of a
$\ell_2$-norm data-term and an analysis sparse prior. The parameters of our
network are the step-sizes of the Chambolle-Pock scheme and the linear operator
involved in sparsity-based penalization, including implicitly the
regularization parameter. A backpropagation procedure is fully described.
Preliminary experiments illustrate the good behavior of such a deep primal-dual
network in the context of image restoration on BSD68 database.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparsity Winning Twice: Better Robust Generaliztion from More Efficient Training. (arXiv:2202.09844v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09844">
<div class="article-summary-box-inner">
<span><p>Recent studies demonstrate that deep networks, even robustified by the
state-of-the-art adversarial training (AT), still suffer from large robust
generalization gaps, in addition to the much more expensive training costs than
standard training. In this paper, we investigate this intriguing problem from a
new perspective, i.e., injecting appropriate forms of sparsity during
adversarial training. We introduce two alternatives for sparse adversarial
training: (i) static sparsity, by leveraging recent results from the lottery
ticket hypothesis to identify critical sparse subnetworks arising from the
early training; (ii) dynamic sparsity, by allowing the sparse subnetwork to
adaptively adjust its connectivity pattern (while sticking to the same sparsity
ratio) throughout training. We find both static and dynamic sparse methods to
yield win-win: substantially shrinking the robust generalization gap and
alleviating the robust overfitting, meanwhile significantly saving training and
inference FLOPs. Extensive experiments validate our proposals with multiple
network architectures on diverse datasets, including CIFAR-10/100 and
Tiny-ImageNet. For example, our methods reduce robust generalization gap and
overfitting by 34.44% and 4.02%, with comparable robust/standard accuracy
boosts and 87.83%/87.82% training/inference FLOPs savings on CIFAR-100 with
ResNet-18. Besides, our approaches can be organically combined with existing
regularizers, establishing new state-of-the-art results in AT. Codes are
available in https://github.com/VITA-Group/Sparsity-Win-Robust-Generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Framework for Brain Tumor Detection Based on Convolutional Variational Generative Models. (arXiv:2202.09850v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09850">
<div class="article-summary-box-inner">
<span><p>Brain tumor detection can make the difference between life and death.
Recently, deep learning-based brain tumor detection techniques have gained
attention due to their higher performance. However, obtaining the expected
performance of such deep learning-based systems requires large amounts of
classified images to train the deep models. Obtaining such data is usually
boring, time-consuming, and can easily be exposed to human mistakes which
hinder the utilization of such deep learning approaches. This paper introduces
a novel framework for brain tumor detection and classification. The basic idea
is to generate a large synthetic MRI images dataset that reflects the typical
pattern of the brain MRI images from a small class-unbalanced collected
dataset. The resulted dataset is then used for training a deep model for
detection and classification. Specifically, we employ two types of deep models.
The first model is a generative model to capture the distribution of the
important features in a set of small class-unbalanced brain MRI images. Then by
using this distribution, the generative model can synthesize any number of
brain MRI images for each class. Hence, the system can automatically convert a
small unbalanced dataset to a larger balanced one. The second model is the
classifier that is trained using the large balanced dataset to detect brain
tumors in MRI images. The proposed framework acquires an overall detection
accuracy of 96.88% which highlights the promise of the proposed framework as an
accurate low-overhead brain tumor detection system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Deterministic Face Mask Removal Based On 3D Priors. (arXiv:2202.09856v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09856">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel image inpainting framework for face mask removal.
Although current methods have demonstrated their impressive ability in
recovering damaged face images, they suffer from two main problems: the
dependence on manually labeled missing regions and the deterministic result
corresponding to each input. The proposed approach tackles these problems by
integrating a multi-task 3D face reconstruction module with a face inpainting
module. Given a masked face image, the former predicts a 3DMM-based
reconstructed face together with a binary occlusion map, providing dense
geometrical and textural priors that greatly facilitate the inpainting task of
the latter. By gradually controlling the 3D shape parameters, our method
generates high-quality dynamic inpainting results with different expressions
and mouth movements. Qualitative and quantitative experiments verify the
effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SRL-SOA: Self-Representation Learning with Sparse 1D-Operational Autoencoder for Hyperspectral Image Band Selection. (arXiv:2202.09918v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09918">
<div class="article-summary-box-inner">
<span><p>The band selection in the hyperspectral image (HSI) data processing is an
important task considering its effect on the computational complexity and
accuracy. In this work, we propose a novel framework for the band selection
problem: Self-Representation Learning (SRL) with Sparse 1D-Operational
Autoencoder (SOA). The proposed SLR-SOA approach introduces a novel autoencoder
model, SOA, that is designed to learn a representation domain where the data
are sparsely represented. Moreover, the network composes of 1D-operational
layers with the non-linear neuron model. Hence, the learning capability of
neurons (filters) is greatly improved with shallow architectures. Using compact
architectures is especially crucial in autoencoders as they tend to overfit
easily because of their identity mapping objective. Overall, we show that the
proposed SRL-SOA band selection approach outperforms the competing methods over
two HSI data including Indian Pines and Salinas-A considering the achieved land
cover classification accuracies. The software implementation of the SRL-SOA
approach is shared publicly at https://github.com/meteahishali/SRL-SOA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deconstructing Distributions: A Pointwise Framework of Learning. (arXiv:2202.09931v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09931">
<div class="article-summary-box-inner">
<span><p>In machine learning, we traditionally evaluate the performance of a single
model, averaged over a collection of test inputs. In this work, we propose a
new approach: we measure the performance of a collection of models when
evaluated on a $\textit{single input point}$. Specifically, we study a point's
$\textit{profile}$: the relationship between models' average performance on the
test distribution and their pointwise performance on this individual point. We
find that profiles can yield new insights into the structure of both models and
data -- in and out-of-distribution. For example, we empirically show that real
data distributions consist of points with qualitatively different profiles. On
one hand, there are "compatible" points with strong correlation between the
pointwise and average performance. On the other hand, there are points with
weak and even $\textit{negative}$ correlation: cases where improving overall
model accuracy actually $\textit{hurts}$ performance on these inputs. We prove
that these experimental observations are inconsistent with the predictions of
several simplified models of learning proposed in prior work. As an
application, we use profiles to construct a dataset we call CIFAR-10-NEG: a
subset of CINIC-10 such that for standard models, accuracy on CIFAR-10-NEG is
$\textit{negatively correlated}$ with accuracy on CIFAR-10 test. This
illustrates, for the first time, an OOD dataset that completely inverts
"accuracy-on-the-line" (Miller, Taori, Raghunathan, Sagawa, Koh, Shankar,
Liang, Carmon, and Schmidt 2021)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imbalanced Malware Images Classification: a CNN based Approach. (arXiv:1708.08042v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1708.08042">
<div class="article-summary-box-inner">
<span><p>Deep convolutional neural networks (CNNs) can be applied to malware binary
detection via image classification. The performance, however, is degraded due
to the imbalance of malware families (classes). To mitigate this issue, we
propose a simple yet effective weighted softmax loss which can be employed as
the final layer of deep CNNs. The original softmax loss is weighted, and the
weight value can be determined according to class size. A scaling parameter is
also included in computing the weight. Proper selection of this parameter is
studied and an empirical option is suggested. The weighted loss aims at
alleviating the impact of data imbalance in an end-to-end learning fashion. To
validate the efficacy, we deploy the proposed weighted loss in a pre-trained
deep CNN model and fine-tune it to achieve promising results on malware images
classification. Extensive experiments also demonstrate that the new loss
function can well fit other typical CNNs, yielding an improved classification
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pyramid Convolutional RNN for MRI Image Reconstruction. (arXiv:1912.00543v6 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.00543">
<div class="article-summary-box-inner">
<span><p>Fast and accurate MRI image reconstruction from undersampled data is crucial
in clinical practice. Deep learning based reconstruction methods have shown
promising advances in recent years. However, recovering fine details from
undersampled data is still challenging. In this paper, we introduce a novel
deep learning based method, Pyramid Convolutional RNN (PC-RNN), to reconstruct
images from multiple scales. Based on the formulation of MRI reconstruction as
an inverse problem, we design the PC-RNN model with three convolutional RNN
(ConvRNN) modules to iteratively learn the features in multiple scales. Each
ConvRNN module reconstructs images at different scales and the reconstructed
images are combined by a final CNN module in a pyramid fashion. The multi-scale
ConvRNN modules learn a coarse-to-fine image reconstruction. Unlike other
common reconstruction methods for parallel imaging, PC-RNN does not employ coil
sensitive maps for multi-coil data and directly model the multiple coils as
multi-channel inputs. The coil compression technique is applied to standardize
data with various coil numbers, leading to more efficient training. We evaluate
our model on the fastMRI knee and brain datasets and the results show that the
proposed model outperforms other methods and can recover more details. The
proposed method is one of the winner solutions in the 2019 fastMRI competition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Causality-Aware Inferring: A Sequential Discriminative Approach for Medical Diagnosis. (arXiv:2003.06534v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.06534">
<div class="article-summary-box-inner">
<span><p>Medical diagnosis assistant (MDA) aims to build an interactive diagnostic
agent to sequentially inquire about symptoms for discriminating diseases.
However, since the dialogue records used to build a patient simulator are
collected passively, the data might be deteriorated by some task-unrelated
biases, such as the preference of the collectors. These biases might hinder the
diagnostic agent to capture transportable knowledge from the simulator. This
work attempts to address these critical issues in MDA by taking advantage of
the causal diagram to identify and resolve two representative non-causal
biases, i.e., (i) default-answer bias and (ii) distributional inquiry bias.
Specifically, Bias (i) originates from the patient simulator which tries to
answer the unrecorded inquiries with some biased default answers. Consequently,
the diagnostic agents cannot fully demonstrate their advantages due to the
biased answers. To eliminate this bias and inspired by the propensity score
matching technique with causal diagram, we propose a propensity-based patient
simulator to effectively answer unrecorded inquiry by drawing knowledge from
the other records; Bias (ii) inherently comes along with the passively
collected data, and is one of the key obstacles for training the agent towards
"learning how" rather than "remembering what". For example, within the
distribution of training data, if a symptom is highly coupled with a certain
disease, the agent might learn to only inquire about that symptom to
discriminate that disease, thus might not generalize to the out-of-distribution
cases. To this end, we propose a progressive assurance agent, which includes
the dual processes accounting for symptom inquiry and disease diagnosis
respectively. The inquiry process is driven by the diagnosis process in a
top-down manner to inquire about symptoms for enhancing diagnostic confidence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Isotropic multichannel total variation framework for joint reconstruction of multicontrast parallel MRI. (arXiv:2006.04128v5 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.04128">
<div class="article-summary-box-inner">
<span><p>Purpose: To develop a synergistic image reconstruction framework that
exploits multicontrast (MC), multicoil, and compressed sensing (CS)
redundancies in magnetic resonance imaging (MRI).
</p>
<p>Approach: CS, MC acquisition, and parallel imaging (PI) have been
individually well developed, but the combination of the three has not been
equally well studied, much less the potential benefits of isotropy within such
a setting. Inspired by total variation theory, we introduce an isotropic MC
image regularizer and attain its full potential by integrating it into
compressed MC multicoil MRI. A convex optimization problem is posed to model
the new variational framework and a first-order algorithm is developed to solve
the problem.
</p>
<p>Results: It turns out that the proposed isotropic regularizer outperforms
many of the state-of-the-art reconstruction methods not only in terms of
rotation-invariance preservation of symmetrical features, but also in
suppressing noise or streaking artifacts, which are normally encountered in PI
methods at aggressive undersampling rates. Moreover, the new framework
significantly prevents intercontrast leakage of contrast-specific details,
which seems to be a difficult situation to handle for some variational and
low-rank MC reconstruction approaches.
</p>
<p>Conclusions: The new framework is a viable option for image reconstruction in
fast protocols of MC parallel MRI, potentially reducing patient discomfort in
otherwise long and time-consuming scans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Post-hoc Calibration of Neural Networks by g-Layers. (arXiv:2006.12807v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.12807">
<div class="article-summary-box-inner">
<span><p>Calibration of neural networks is a critical aspect to consider when
incorporating machine learning models in real-world decision-making systems
where the confidence of decisions are equally important as the decisions
themselves. In recent years, there is a surge of research on neural network
calibration and the majority of the works can be categorized into post-hoc
calibration methods, defined as methods that learn an additional function to
calibrate an already trained base network. In this work, we intend to
understand the post-hoc calibration methods from a theoretical point of view.
Especially, it is known that minimizing Negative Log-Likelihood (NLL) will lead
to a calibrated network on the training set if the global optimum is attained
(Bishop, 1994). Nevertheless, it is not clear learning an additional function
in a post-hoc manner would lead to calibration in the theoretical sense. To
this end, we prove that even though the base network ($f$) does not lead to the
global optimum of NLL, by adding additional layers ($g$) and minimizing NLL by
optimizing the parameters of $g$ one can obtain a calibrated network $g \circ
f$. This not only provides a less stringent condition to obtain a calibrated
network but also provides a theoretical justification of post-hoc calibration
methods. Our experiments on various image classification benchmarks confirm the
theory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatio-Temporal EEG Representation Learning on Riemannian Manifold and Euclidean Space. (arXiv:2008.08633v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.08633">
<div class="article-summary-box-inner">
<span><p>We present a novel deep neural architecture for learning Electroencephalogram
(EEG). To learn the spatial information, our model first obtains the Riemannian
mean and distance from Spatial Covariance Matrices (SCMs) on the Riemannian
manifold. We then project the spatial information onto the Euclidean space via
tangent space learning. Following, two fully connected layers are used to learn
the spatial information embeddings. Moreover, our proposed method learns the
temporal information via differential entropy and logarithm power spectrum
density features extracted from EEG signals in Euclidean space using a deep
long short-term memory network with a soft attention mechanism. To combine the
spatial and temporal information, we use an effective fusion strategy, which
learns attention weights applied to embedding-specific features for decision
making. We evaluate our proposed framework on four public datasets across three
popular EEG-related tasks, notably emotion recognition, vigilance estimation,
and motor imagery classification, containing various types of tasks such as
binary classification, multi-class classification, and regression. Our proposed
architecture approaches the state-of-the-art on one dataset (SEED) and
outperforms other methods on the other three datasets (SEED-VIG, BCI-IV 2A, and
BCI-IV 2B), setting new state-of-the-art values and showing the robustness of
our framework in EEG representation learning. The source code of our paper is
publicly available at https://github.com/guangyizhangbci/EEG_Riemannian.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Attack with Fewer Pixels: A Probabilistic Post-hoc Framework for Refining Arbitrary Dense Adversarial Attacks. (arXiv:2010.06131v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.06131">
<div class="article-summary-box-inner">
<span><p>Deep neural network image classifiers are reported to be susceptible to
adversarial evasion attacks, which use carefully crafted images created to
mislead a classifier. Many adversarial attacks belong to the category of dense
attacks, which generate adversarial examples by perturbing all the pixels of a
natural image. To generate sparse perturbations, sparse attacks have been
recently developed, which are usually independent attacks derived by modifying
a dense attack's algorithm with sparsity regularisations, resulting in reduced
attack efficiency. In this paper, we aim to tackle this task from a different
perspective. We select the most effective perturbations from the ones generated
from a dense attack, based on the fact we find that a considerable amount of
the perturbations on an image generated by dense attacks may contribute little
to attacking a classifier. Accordingly, we propose a probabilistic post-hoc
framework that refines given dense attacks by significantly reducing the number
of perturbed pixels but keeping their attack power, trained with mutual
information maximisation. Given an arbitrary dense attack, the proposed model
enjoys appealing compatibility for making its adversarial images more realistic
and less detectable with fewer perturbations. Moreover, our framework performs
adversarial attacks much faster than existing sparse attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stochastic sparse adversarial attacks. (arXiv:2011.12423v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12423">
<div class="article-summary-box-inner">
<span><p>This paper introduces stochastic sparse adversarial attacks (SSAA), standing
as simple, fast and purely noise-based targeted and untargeted attacks of
neural network classifiers (NNC). SSAA offer new examples of sparse (or $L_0$)
attacks for which only few methods have been proposed previously. These attacks
are devised by exploiting a small-time expansion idea widely used for Markov
processes. Experiments on small and large datasets (CIFAR-10 and ImageNet)
illustrate several advantages of SSAA in comparison with the-state-of-the-art
methods. For instance, in the untargeted case, our method called Voting Folded
Gaussian Attack (VFGA) scales efficiently to ImageNet and achieves a
significantly lower $L_0$ score than SparseFool (up to $\frac{2}{5}$) while
being faster. Moreover, VFGA achieves better $L_0$ scores on ImageNet than
Sparse-RS when both attacks are fully successful on a large number of samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Reenactment as Inductive Bias for Content-Motion Disentanglement. (arXiv:2102.00324v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00324">
<div class="article-summary-box-inner">
<span><p>Independent components within low-dimensional representations are essential
inputs in several downstream tasks, and provide explanations over the observed
data. Video-based disentangled factors of variation provide low-dimensional
representations that can be identified and used to feed task-specific models.
We introduce MTC-VAE, a self-supervised motion-transfer VAE model to
disentangle motion and content from videos. Unlike previous work on video
content-motion disentanglement, we adopt a chunk-wise modeling approach and
take advantage of the motion information contained in spatiotemporal
neighborhoods. Our model yields independent per-chunk representations that
preserve temporal consistency. Hence, we reconstruct whole videos in a single
forward-pass. We extend the ELBO's log-likelihood term and include a Blind
Reenactment Loss as an inductive bias to leverage motion disentanglement, under
the assumption that swapping motion features yields reenactment between two
videos. We evaluate our model with recently-proposed disentanglement metrics
and show that it outperforms a variety of methods for video motion-content
disentanglement. Experiments on video reenactment show the effectiveness of our
disentanglement in the input space where our model outperforms the baselines in
reconstruction quality and motion alignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Child-Computer Interaction: Recent Works, New Dataset, and Age Detection. (arXiv:2102.01405v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.01405">
<div class="article-summary-box-inner">
<span><p>This article provides an overview of recent research in Child-Computer
Interaction with mobile devices and describe our framework ChildCI intended
for: i) overcoming the lack of large-scale publicly available databases in the
area, ii) generating a better understanding of the cognitive and neuromotor
development of children along time, contrary to most previous studies in the
literature focused on a single-session acquisition, and iii) enabling new
applications in e-Learning and e-Health through the acquisition of additional
information such as the school grades and children's disorders, among others.
Our framework includes a new mobile application, specific data acquisition
protocols, and a first release of the ChildCI dataset (ChildCIdb v1), which is
planned to be extended yearly to enable longitudinal studies.
</p>
<p>In our framework children interact with a tablet device, using both a pen
stylus and the finger, performing different tasks that require different levels
of neuromotor and cognitive skills. ChildCIdb is the first database in the
literature that comprises more than 400 children from 18 months to 8 years old,
considering therefore the first three development stages of the Piaget's
theory. In addition, and as a demonstration of the potential of the ChildCI
framework, we include experimental results for one of the many applications
enabled by ChildCIdb: children age detection based on device interaction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PURSUhInT: In Search of Informative Hint Points Based on Layer Clustering for Knowledge Distillation. (arXiv:2103.00053v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00053">
<div class="article-summary-box-inner">
<span><p>We propose a novel knowledge distillation methodology for compressing deep
neural networks. One of the most efficient methods for knowledge distillation
is hint distillation, where the student model is injected with information
(hints) from several different layers of the teacher model. Although the
selection of hint points can drastically alter the compression performance,
conventional distillation approaches overlook this fact. Therefore, we propose
a clustering based hint selection methodology, where the layers of teacher
model are clustered with respect to several metrics and the cluster centers are
used as the hint points. Our method is applicable for any student network, once
it is applied on a chosen teacher network. The proposed approach is validated
in CIFAR-100 and ImageNet datasets, using various teacher-student pairs and
numerous hint distillation methods. Our results show that hint points selected
by our algorithm results in superior compression performance with respect to
state-of-the-art knowledge distillation algorithms on the same student models
and datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent Advances on Neural Network Pruning at Initialization. (arXiv:2103.06460v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06460">
<div class="article-summary-box-inner">
<span><p>Neural network pruning typically removes connections or neurons from a
pretrained converged model; while a new pruning paradigm, pruning at
initialization (PaI), attempts to prune a randomly initialized network. This
paper offers the first survey concentrated on this emerging pruning fashion. We
first introduce a generic formulation of neural network pruning, followed by
the major classic pruning topics. Then, as the main body of this paper, a
thorough and structured literature review of PaI methods is presented,
consisting of two major tracks (sparse training and sparse selection). Finally,
we summarize the surge of PaI compared to traditional pruning and discuss the
open problems. Apart from the dedicated paper review, this paper also offers a
code base for easy sanity-checking and benchmarking of different PaI methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for Improved Cross-Modal Retrieval. (arXiv:2103.11920v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11920">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art approaches to cross-modal retrieval process text and
visual input jointly, relying on Transformer-based architectures with
cross-attention mechanisms that attend over all words and objects in an image.
While offering unmatched retrieval performance, such models: 1) are typically
pretrained from scratch and thus less scalable, 2) suffer from huge retrieval
latency and inefficiency issues, which makes them impractical in realistic
applications. To address these crucial gaps towards both improved and efficient
cross-modal retrieval, we propose a novel fine-tuning framework that turns any
pretrained text-image multi-modal model into an efficient retrieval model. The
framework is based on a cooperative retrieve-and-rerank approach which
combines: 1) twin networks (i.e., a bi-encoder) to separately encode all items
of a corpus, enabling efficient initial retrieval, and 2) a cross-encoder
component for a more nuanced (i.e., smarter) ranking of the retrieved small set
of items. We also propose to jointly fine-tune the two components with shared
weights, yielding a more parameter-efficient model. Our experiments on a series
of standard cross-modal retrieval benchmarks in monolingual, multilingual, and
zero-shot setups, demonstrate improved accuracy and huge efficiency benefits
over the state-of-the-art cross-encoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M6-UFC: Unifying Multi-Modal Controls for Conditional Image Synthesis via Non-Autoregressive Generative Transformers. (arXiv:2105.14211v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14211">
<div class="article-summary-box-inner">
<span><p>Conditional image synthesis aims to create an image according to some
multi-modal guidance in the forms of textual descriptions, reference images,
and image blocks to preserve, as well as their combinations. In this paper,
instead of investigating these control signals separately, we propose a new
two-stage architecture, M6-UFC, to unify any number of multi-modal controls. In
M6-UFC, both the diverse control signals and the synthesized image are
uniformly represented as a sequence of discrete tokens to be processed by
Transformer. Different from existing two-stage autoregressive approaches such
as DALL-E and VQGAN, M6-UFC adopts non-autoregressive generation (NAR) at the
second stage to enhance the holistic consistency of the synthesized image, to
support preserving specified image blocks, and to improve the synthesis speed.
Further, we design a progressive algorithm that iteratively improves the
non-autoregressively generated image, with the help of two estimators developed
for evaluating the compliance with the controls and evaluating the fidelity of
the synthesized image, respectively. Extensive experiments on a newly collected
large-scale clothing dataset M2C-Fashion and a facial dataset Multi-Modal
CelebA-HQ verify that M6-UFC can synthesize high-fidelity images that comply
with flexible multi-modal controls.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Z2P: Instant Visualization of Point Clouds. (arXiv:2105.14548v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14548">
<div class="article-summary-box-inner">
<span><p>We present a technique for visualizing point clouds using a neural network.
Our technique allows for an instant preview of any point cloud, and bypasses
the notoriously difficult surface reconstruction problem or the need to
estimate oriented normals for splat-based rendering. We cast the preview
problem as a conditional image-to-image translation task, and design a neural
network that translates point depth-map directly into an image, where the point
cloud is visualized as though a surface was reconstructed from it. Furthermore,
the resulting appearance of the visualized point cloud can be, optionally,
conditioned on simple control variables (e.g., color and light). We demonstrate
that our technique instantly produces plausible images, and can, on-the-fly
effectively handle noise, non-uniform sampling, and thin surfaces sheets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The 2021 Image Similarity Dataset and Challenge. (arXiv:2106.09672v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09672">
<div class="article-summary-box-inner">
<span><p>This paper introduces a new benchmark for large-scale image similarity
detection. This benchmark is used for the Image Similarity Challenge at
NeurIPS'21 (ISC2021). The goal is to determine whether a query image is a
modified copy of any image in a reference corpus of size 1~million. The
benchmark features a variety of image transformations such as automated
transformations, hand-crafted image edits and machine-learning based
manipulations. This mimics real-life cases appearing in social media, for
example for integrity-related problems dealing with misinformation and
objectionable content. The strength of the image manipulations, and therefore
the difficulty of the benchmark, is calibrated according to the performance of
a set of baseline approaches. Both the query and reference set contain a
majority of "distractor" images that do not match, which corresponds to a
real-life needle-in-haystack setting, and the evaluation metric reflects that.
We expect the DISC21 benchmark to promote image copy detection as an important
and challenging computer vision task and refresh the state of the art. Code and
data are available at https://github.com/facebookresearch/isc2021
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Technical Document Classification. (arXiv:2106.14269v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14269">
<div class="article-summary-box-inner">
<span><p>In large technology companies, the requirements for managing and organizing
technical documents created by engineers and managers have increased
dramatically in recent years, which has led to a higher demand for more
scalable, accurate, and automated document classification. Prior studies have
only focused on processing text for classification, whereas technical documents
often contain multimodal information. To leverage multimodal information for
document classification to improve the model performance, this paper presents a
novel multimodal deep learning architecture, TechDoc, which utilizes three
types of information, including natural language texts and descriptive images
within documents and the associations among the documents. The architecture
synthesizes the convolutional neural network, recurrent neural network, and
graph neural network through an integrated training process. We applied the
architecture to a large multimodal technical document database and trained the
model for classifying documents based on the hierarchical International Patent
Classification system. Our results show that TechDoc presents a greater
classification accuracy than the unimodal methods and other state-of-the-art
benchmarks. The trained model can potentially be scaled to millions of
real-world multimodal technical documents, which is useful for data and
knowledge management in large technology companies and organizations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Application of artificial intelligence techniques for automated detection of myocardial infarction: A review. (arXiv:2107.06179v2 [eess.SP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06179">
<div class="article-summary-box-inner">
<span><p>Myocardial infarction (MI) results in heart muscle injury due to receiving
insufficient blood flow. MI is the most common cause of mortality in
middle-aged and elderly individuals around the world. To diagnose MI,
clinicians need to interpret electrocardiography (ECG) signals, which requires
expertise and is subject to observer bias. Artificial intelligence-based
methods can be utilized to screen for or diagnose MI automatically using ECG
signals. In this work, we conducted a comprehensive assessment of artificial
intelligence-based approaches for MI detection based on ECG as well as other
biophysical signals, including machine learning (ML) and deep learning (DL)
models. The performance of traditional ML methods relies on handcrafted
features and manual selection of ECG signals, whereas DL models can automate
these tasks. The review observed that deep convolutional neural networks
(DCNNs) yielded excellent classification performance for MI diagnosis, which
explains why they have become prevalent in recent years. To our knowledge, this
is the first comprehensive survey of artificial intelligence techniques
employed for MI diagnosis using ECG and other biophysical signals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VILENS: Visual, Inertial, Lidar, and Leg Odometry for All-Terrain Legged Robots. (arXiv:2107.07243v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07243">
<div class="article-summary-box-inner">
<span><p>We present VILENS (Visual Inertial Lidar Legged Navigation System), an
odometry system for legged robots based on factor graphs. The key novelty is
the tight fusion of four different sensor modalities to achieve reliable
operation when the individual sensors would otherwise produce degenerate
estimation. To minimize leg odometry drift, we extend the robot's state with a
linear velocity bias term which is estimated online. This bias is observable
because of the tight fusion of this preintegrated velocity factor with vision,
lidar, and IMU factors. Extensive experimental validation on different ANYmal
quadruped robots is presented, for a total duration of 2 h and 1.8 km traveled.
The experiments involved dynamic locomotion over loose rocks, slopes, and mud
which caused challenges like slippage and terrain deformation. Perceptual
challenges included dark and dusty underground caverns, and open and
feature-deprived areas. We show an average improvement of 62% translational and
51% rotational errors compared to a state-of-the-art loosely coupled approach.
To demonstrate its robustness, VILENS was also integrated with a perceptive
controller and a local path planner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GANmapper: geographical data translation. (arXiv:2108.04232v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04232">
<div class="article-summary-box-inner">
<span><p>We present a new method to create spatial data using a generative adversarial
network (GAN). Our contribution uses coarse and widely available geospatial
data to create maps of less available features at the finer scale in the built
environment, bypassing their traditional acquisition techniques (e.g. satellite
imagery or land surveying). In the work, we employ land use data and road
networks as input to generate building footprints and conduct experiments in 9
cities around the world. The method, which we implement in a tool we release
openly, enables the translation of one geospatial dataset to another with high
fidelity and morphological accuracy. It may be especially useful in locations
missing detailed and high-resolution data and those that are mapped with
uncertain or heterogeneous quality, such as much of OpenStreetMap. The quality
of the results is influenced by the urban form and scale. In most cases, the
experiments suggest promising performance as the method tends to truthfully
indicate the locations, amount, and shape of buildings. The work has the
potential to support several applications, such as energy, climate, and urban
morphology studies in areas previously lacking required data or inpainting
geospatial data in regions with incomplete data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DKM: Differentiable K-Means Clustering Layer for Neural Network Compression. (arXiv:2108.12659v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12659">
<div class="article-summary-box-inner">
<span><p>Deep neural network (DNN) model compression for efficient on-device inference
is becoming increasingly important to reduce memory requirements and keep user
data on-device. To this end, we propose a novel differentiable k-means
clustering layer (DKM) and its application to train-time weight
clustering-based DNN model compression. DKM casts k-means clustering as an
attention problem and enables joint optimization of the DNN parameters and
clustering centroids. Unlike prior works that rely on additional regularizers
and parameters, DKM-based compression keeps the original loss function and
model architecture fixed. We evaluated DKM-based compression on various DNN
models for computer vision and natural language processing (NLP) tasks. Our
results demonstrate that DKM delivers superior compression and accuracy
trade-off on ImageNet1k and GLUE benchmarks. For example, DKM-based compression
can offer 74.5% top-1 ImageNet1k accuracy on ResNet50 DNN model with 3.3MB
model size (29.4x model compression factor). For MobileNet-v1, which is a
challenging DNN to compress, DKM delivers 63.9% top-1 ImageNet1k accuracy with
0.72 MB model size (22.4x model compression factor). This result is 6.8% higher
top-1accuracy and 33% relatively smaller model size than the current
state-of-the-art DNN compression algorithms. Additionally, DKM enables
compression of DistilBERT model by 11.8x with minimal (1.1%) accuracy loss on
GLUE NLP benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fair Representation: Guaranteeing Approximate Multiple Group Fairness for Unknown Tasks. (arXiv:2109.00545v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00545">
<div class="article-summary-box-inner">
<span><p>Motivated by scenarios where data is used for diverse prediction tasks, we
study whether fair representation can be used to guarantee fairness for unknown
tasks and for multiple fairness notions simultaneously. We consider seven group
fairness notions that cover the concepts of independence, separation, and
calibration. Against the backdrop of the fairness impossibility results, we
explore approximate fairness. We prove that, although fair representation might
not guarantee fairness for all prediction tasks, it does guarantee fairness for
an important subset of tasks -- the tasks for which the representation is
discriminative. Specifically, all seven group fairness notions are linearly
controlled by fairness and discriminativeness of the representation. When an
incompatibility exists between different fairness notions, fair and
discriminative representation hits the sweet spot that approximately satisfies
all notions. Motivated by our theoretical findings, we propose to learn both
fair and discriminative representations using pretext loss which
self-supervises learning, and Maximum Mean Discrepancy as a fair regularizer.
Experiments on tabular, image, and face datasets show that using the learned
representation, downstream predictions that we are unaware of when learning the
representation indeed become fairer for seven group fairness notions, and the
fairness guarantees computed from our theoretical results are all valid.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fair Conformal Predictors for Applications in Medical Imaging. (arXiv:2109.04392v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04392">
<div class="article-summary-box-inner">
<span><p>Deep learning has the potential to augment several clinically useful aspects
of the radiologist's workflow such as medical imaging interpretation. However,
the translation of deep learning algorithms into clinical practice has been
hindered by relative lack of transparency in these algorithms compared to more
traditional statistical methods. Specifically, common deep learning models lack
intuitive and rigorous methods of conveying prediction confidence in a
calibrated manner, which ultimately restricts widespread use of these "black
box" systems for critical decision-making. Furthermore, numerous demonstrations
of algorithmic bias in clinical machine learning have caused considerable
hesitancy towards the deployment of these models for clinical application. To
this end, we explore how conformal predictions can complement existing deep
learning approaches by providing an intuitive way of expressing model
uncertainty to facilitate greater transparency to clinical users. In this
paper, we conduct field interviews with radiologists to assess potential
use-cases of conformal predictors. Using insights collected from these
interviews, we devise two use-cases and empirically evaluate several conformal
methods on a dermatology photography dataset for skin lesion classification.
Additionally, we show how group conformal predictors are more adaptive to
differences between patient skin tones for malignant skin lesions. We find our
conformal predictors to be a promising and generally applicable approach to
increasing clinical usability and trustworthiness -- hopefully facilitating
better modes of collaboration between medical AI tools and their clinical
users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning a Metacognition for Object Detection. (arXiv:2110.03105v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03105">
<div class="article-summary-box-inner">
<span><p>In contrast to object recognition models, humans do not blindly trust their
perception when building representations of the world, instead recruiting
metacognition to detect percepts that are unreliable or false, such as when we
realize that we mistook one object for another. We propose METAGEN, an
unsupervised model that enhances object recognition models through a
metacognition. Given noisy output from an object-detection model, METAGEN
learns a meta-representation of how its perceptual system works and uses it to
infer the objects in the world responsible for the detections. METAGEN achieves
this by conditioning its inference on basic principles of objects that even
human infants understand (known as Spelke principles: object permanence,
cohesion, and spatiotemporal continuity). We test METAGEN on a variety of
state-of-the-art object detection neural networks. We find that METAGEN quickly
learns an accurate metacognitive representation of the neural network, and that
this improves detection accuracy by filling in objects that the detection model
missed and removing hallucinated objects. This approach enables generalization
to out-of-sample data and outperforms comparison models that lack a
metacognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subspace Regularizers for Few-Shot Class Incremental Learning. (arXiv:2110.07059v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07059">
<div class="article-summary-box-inner">
<span><p>Few-shot class incremental learning -- the problem of updating a trained
classifier to discriminate among an expanded set of classes with limited
labeled data -- is a key challenge for machine learning systems deployed in
non-stationary environments. Existing approaches to the problem rely on complex
model architectures and training procedures that are difficult to tune and
re-use. In this paper, we present an extremely simple approach that enables the
use of ordinary logistic regression classifiers for few-shot incremental
learning. The key to this approach is a new family of subspace regularization
schemes that encourage weight vectors for new classes to lie close to the
subspace spanned by the weights of existing classes. When combined with
pretrained convolutional feature extractors, logistic regression models trained
with subspace regularization outperform specialized, state-of-the-art
approaches to few-shot incremental image classification by up to 22% on the
miniImageNet dataset. Because of its simplicity, subspace regularization can be
straightforwardly extended to incorporate additional background information
about the new classes (including class names and descriptions specified in
natural language); these further improve accuracy by up to 2%. Our results show
that simple geometric regularization of class representations offers an
effective tool for continual learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep multi-modal aggregation network for MR image reconstruction with auxiliary modality. (arXiv:2110.08080v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08080">
<div class="article-summary-box-inner">
<span><p>Magnetic resonance (MR) imaging produces detailed images of organs and
tissues with better contrast, but it suffers from a long acquisition time,
which makes the image quality vulnerable to say motion artifacts. Recently,
many approaches have been developed to reconstruct full-sampled images from
partially observed measurements to accelerate MR imaging. However, most
approaches focused on reconstruction over a single modality, neglecting the
discovery of correlation knowledge between the different modalities. Here we
propose a Multi-modal Aggregation network for mR Image recOnstruction with
auxiliary modality (MARIO), which is capable of discovering complementary
representations from a fully sampled auxiliary modality, with which to
hierarchically guide the reconstruction of a given target modality. This
implies that our method can selectively aggregate multi-modal representations
for better reconstruction, yielding comprehensive, multi-scale, multi-modal
feature fusion. Extensive experiments on IXI and fastMRI datasets demonstrate
the superiority of the proposed approach over state-of-the-art MR image
reconstruction methods in removing artifacts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-Aware Lung Nodule Segmentation with Multiple Annotations. (arXiv:2110.12372v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12372">
<div class="article-summary-box-inner">
<span><p>Since radiologists have different training and clinical experience, they may
provide various segmentation maps for a lung nodule. As a result, for a
specific lung nodule, some regions have a higher chance of causing segmentation
uncertainty, which brings difficulty for lung nodule segmentation with multiple
annotations. To address this problem, this paper proposes an Uncertainty-Aware
Segmentation Network (UAS-Net) based on multi-branch U-Net, which can learn the
valuable visual features from the regions that may cause segmentation
uncertainty and contribute to a better segmentation result. Meanwhile, this
network can provide a Multi-Confidence Mask (MCM) simultaneously, pointing out
regions with different segmentation uncertainty levels. We introduce a
Feature-Aware Concatenation structure for different learning targets and let
each branch have a specific learning preference. Moreover, a joint adversarial
learning process is also adopted to help learn discriminative features of
complex structures. Experimental results show that our method can predict the
reasonable regions with higher uncertainty and improve lung nodule segmentation
performance in LIDC-IDRI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning. (arXiv:2110.13214v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13214">
<div class="article-summary-box-inner">
<span><p>Current visual question answering (VQA) tasks mainly consider answering
human-annotated questions for natural images. However, aside from natural
images, abstract diagrams with semantic richness are still understudied in
visual understanding and reasoning research. In this work, we introduce a new
challenge of Icon Question Answering (IconQA) with the goal of answering a
question in an icon image context. We release IconQA, a large-scale dataset
that consists of 107,439 questions and three sub-tasks: multi-image-choice,
multi-text-choice, and filling-in-the-blank. The IconQA dataset is inspired by
real-world diagram word problems that highlight the importance of abstract
diagram understanding and comprehensive cognitive reasoning. Thus, IconQA
requires not only perception skills like object recognition and text
understanding, but also diverse cognitive reasoning skills, such as geometric
reasoning, commonsense reasoning, and arithmetic reasoning. To facilitate
potential IconQA models to learn semantic representations for icon images, we
further release an icon dataset Icon645 which contains 645,687 colored icons on
377 classes. We conduct extensive user studies and blind experiments and
reproduce a wide range of advanced VQA methods to benchmark the IconQA task.
Also, we develop a strong IconQA baseline Patch-TRM that applies a pyramid
cross-modal Transformer with input diagram embeddings pre-trained on the icon
dataset. IconQA and Icon645 are available at https://iconqa.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RRNet: Relational Reasoning Network with Parallel Multi-scale Attention for Salient Object Detection in Optical Remote Sensing Images. (arXiv:2110.14223v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14223">
<div class="article-summary-box-inner">
<span><p>Salient object detection (SOD) for optical remote sensing images (RSIs) aims
at locating and extracting visually distinctive objects/regions from the
optical RSIs. Despite some saliency models were proposed to solve the intrinsic
problem of optical RSIs (such as complex background and scale-variant objects),
the accuracy and completeness are still unsatisfactory. To this end, we propose
a relational reasoning network with parallel multi-scale attention for SOD in
optical RSIs in this paper. The relational reasoning module that integrates the
spatial and the channel dimensions is designed to infer the semantic
relationship by utilizing high-level encoder features, thereby promoting the
generation of more complete detection results. The parallel multi-scale
attention module is proposed to effectively restore the detail information and
address the scale variation of salient objects by using the low-level features
refined by multi-scale attention. Extensive experiments on two datasets
demonstrate that our proposed RRNet outperforms the existing state-of-the-art
SOD competitors both qualitatively and quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts. (arXiv:2111.08276v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08276">
<div class="article-summary-box-inner">
<span><p>Most existing methods in vision language pre-training rely on object-centric
features extracted through object detection, and make fine-grained alignments
between the extracted features and texts. We argue that object detection may
not be necessary for vision language pre-training. To this end, we propose a
new method called X-VLM to perform `multi-grained vision language
pre-training.' The key of learning multi-grained alignments is to locate visual
concepts in the image given the associated texts, and in the meantime align the
texts with the visual concepts, where the alignments are in multi-granularity.
Experimental results show that X-VLM effectively leverages the learned
alignments to many downstream vision language tasks and consistently
outperforms state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Modified Indicator Functions for Surface Reconstruction. (arXiv:2111.09526v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09526">
<div class="article-summary-box-inner">
<span><p>Surface reconstruction is a fundamental problem in 3D graphics. In this
paper, we propose a learning-based approach for implicit surface reconstruction
from raw point clouds without normals. Our method is inspired by Gauss Lemma in
potential energy theory, which gives an explicit integral formula for the
indicator functions. We design a novel deep neural network to perform surface
integral and learn the modified indicator functions from un-oriented and noisy
point clouds. We concatenate features with different scales for accurate
point-wise contributions to the integral. Moreover, we propose a novel Surface
Element Feature Extractor to learn local shape properties. Experiments show
that our method generates smooth surfaces with high normal consistency from
point clouds with different noise scales and achieves state-of-the-art
reconstruction performance compared with current data-driven and
non-data-driven approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial-context-aware deep neural network for multi-class image classification. (arXiv:2111.12296v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12296">
<div class="article-summary-box-inner">
<span><p>Multi-label image classification is a fundamental but challenging task in
computer vision. Over the past few decades, solutions exploring relationships
between semantic labels have made great progress. However, the underlying
spatial-contextual information of labels is under-exploited. To tackle this
problem, a spatial-context-aware deep neural network is proposed to predict
labels taking into account both semantic and spatial information. This proposed
framework is evaluated on Microsoft COCO and PASCAL VOC, two widely used
benchmark datasets for image multi-labelling. The results show that the
proposed approach is superior to the state-of-the-art solutions on dealing with
the multi-label image classification problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Pose Manipulation and Novel View Synthesis using Differentiable Rendering. (arXiv:2111.12731v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12731">
<div class="article-summary-box-inner">
<span><p>We present a new approach for synthesizing novel views of people in new
poses. Our novel differentiable renderer enables the synthesis of highly
realistic images from any viewpoint. Rather than operating over mesh-based
structures, our renderer makes use of diffuse Gaussian primitives that directly
represent the underlying skeletal structure of a human. Rendering these
primitives gives results in a high-dimensional latent image, which is then
transformed into an RGB image by a decoder network. The formulation gives rise
to a fully differentiable framework that can be trained end-to-end. We
demonstrate the effectiveness of our approach to image reconstruction on both
the Human3.6M and Panoptic Studio datasets. We show how our approach can be
used for motion transfer between individuals; novel view synthesis of
individuals captured from just a single camera; to synthesize individuals from
any virtual viewpoint; and to re-render people in novel poses. Code and video
results are available at
https://github.com/GuillaumeRochette/HumanViewSynthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting full Resolution Feature Context for Liver Tumor and Vessel Segmentation via Fusion Encoder: Application to Liver Tumor and Vessel 3D reconstruction. (arXiv:2111.13299v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13299">
<div class="article-summary-box-inner">
<span><p>Liver cancer is one of the most common malignant diseases in the world.
Segmentation and labeling of liver tumors and blood vessels in CT images can
provide convenience for doctors in liver tumor diagnosis and surgical
intervention. In the past decades, automatic CT segmentation methods based on
deep learning have received widespread attention in the medical field. Many
state-of-the-art segmentation algorithms appeared during this period. Yet, most
of the existing segmentation methods only care about the local feature context
and have a perception defect in the global relevance of medical images, which
significantly affects the segmentation effect of liver tumors and blood
vessels. We introduce a multi-scale feature context fusion network called
TransFusionNet based on Transformer and SEBottleNet. This network can
accurately detect and identify the details of the region of interest of the
liver vessel, meanwhile it can improve the recognition of morphologic margins
of liver tumors by exploiting the global information of CT images. Experiments
show that TransFusionNet is better than the state-of-the-art method on both the
public dataset LITS and 3Dircadb and our clinical dataset. Finally, we propose
an automatic 3D reconstruction algorithm based on the trained model. The
algorithm can complete the reconstruction quickly and accurately in 1 second.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PTTR: Relational 3D Point Cloud Object Tracking with Transformer. (arXiv:2112.02857v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02857">
<div class="article-summary-box-inner">
<span><p>In a point cloud sequence, 3D object tracking aims to predict the location
and orientation of an object in the current search point cloud given a template
point cloud. Motivated by the success of transformers, we propose Point
Tracking TRansformer (PTTR), which efficiently predicts high-quality 3D
tracking results in a coarse-to-fine manner with the help of transformer
operations. PTTR consists of three novel designs. 1) Instead of random
sampling, we design Relation-Aware Sampling to preserve relevant points to
given templates during subsampling. 2) Furthermore, we propose a Point Relation
Transformer (PRT) consisting of a self-attention and a cross-attention module.
The global self-attention operation captures long-range dependencies to enhance
encoded point features for the search area and the template, respectively.
Subsequently, we generate the coarse tracking results by matching the two sets
of point features via cross-attention. 3) Based on the coarse tracking results,
we employ a novel Prediction Refinement Module to obtain the final refined
prediction. In addition, we create a large-scale point cloud single object
tracking benchmark based on the Waymo Open Dataset. Extensive experiments show
that PTTR achieves superior point cloud tracking in both accuracy and
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Annotation-efficient cancer detection with report-guided lesion annotation for deep learning-based prostate cancer detection in bpMRI. (arXiv:2112.05151v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05151">
<div class="article-summary-box-inner">
<span><p>Deep learning-based diagnostic performance increases with more annotated
data, but large-scale manual annotations are expensive and labour-intensive.
Experts evaluate diagnostic images during clinical routine, and write their
findings in reports. Leveraging unlabelled exams paired with clinical reports
could overcome the manual labelling bottleneck. We hypothesise that detection
models can be trained semi-supervised with automatic annotations generated
using model predictions, guided by sparse information from clinical reports. To
demonstrate efficacy, we train clinically significant prostate cancer (csPCa)
segmentation models, where automatic annotations are guided by the number of
clinically significant findings in the radiology reports. We included 7,756
prostate MRI examinations, of which 3,050 were manually annotated. We evaluated
prostate cancer detection performance on 300 exams from an external centre with
histopathology-confirmed ground truth. Semi-supervised training improved
patient-based diagnostic area under the receiver operating characteristic curve
from $87.2 \pm 0.8\%$ to $89.4 \pm 1.0\%$ ($P&lt;10^{-4}$) and improved
lesion-based sensitivity at one false positive per case from $76.4 \pm 3.8\%$
to $83.6 \pm 2.3\%$ ($P&lt;10^{-4}$). Semi-supervised training was 14$\times$ more
annotation-efficient for case-based performance and 6$\times$ more
annotation-efficient for lesion-based performance. This improved performance
demonstrates the feasibility of our training procedure. Source code is publicly
available at github.com/DIAGNijmegen/Report-Guided-Annotation. Best csPCa
detection algorithm is available at
grand-challenge.org/algorithms/bpmri-cspca-detection-report-guided-annotations/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Homography Decomposition Networks for Planar Object Tracking. (arXiv:2112.07909v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07909">
<div class="article-summary-box-inner">
<span><p>Planar object tracking plays an important role in AI applications, such as
robotics, visual servoing, and visual SLAM. Although the previous planar
trackers work well in most scenarios, it is still a challenging task due to the
rapid motion and large transformation between two consecutive frames. The
essential reason behind this problem is that the condition number of such a
non-linear system changes unstably when the searching range of the homography
parameter space becomes larger. To this end, we propose a novel Homography
Decomposition Networks(HDN) approach that drastically reduces and stabilizes
the condition number by decomposing the homography transformation into two
groups. Specifically, a similarity transformation estimator is designed to
predict the first group robustly by a deep convolution equivariant network. By
taking advantage of the scale and rotation estimation with high confidence, a
residual transformation is estimated by a simple regression model. Furthermore,
the proposed end-to-end network is trained in a semi-supervised fashion.
Extensive experiments show that our proposed approach outperforms the
state-of-the-art planar tracking methods at a large margin on the challenging
POT, UCSB and POIC datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformer with Deformable Attention. (arXiv:2201.00520v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00520">
<div class="article-summary-box-inner">
<span><p>Transformers have recently shown superior performances on various vision
tasks. The large, sometimes even global, receptive field endows Transformer
models with higher representation power over their CNN counterparts.
Nevertheless, simply enlarging receptive field also gives rise to several
concerns. On the one hand, using dense attention e.g., in ViT, leads to
excessive memory and computational cost, and features can be influenced by
irrelevant parts which are beyond the region of interests. On the other hand,
the sparse attention adopted in PVT or Swin Transformer is data agnostic and
may limit the ability to model long range relations. To mitigate these issues,
we propose a novel deformable self-attention module, where the positions of key
and value pairs in self-attention are selected in a data-dependent way. This
flexible scheme enables the self-attention module to focus on relevant regions
and capture more informative features. On this basis, we present Deformable
Attention Transformer, a general backbone model with deformable attention for
both image classification and dense prediction tasks. Extensive experiments
show that our models achieve consistently improved results on comprehensive
benchmarks. Code is available at https://github.com/LeapLabTHU/DAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniFormer: Unifying Convolution and Self-attention for Visual Recognition. (arXiv:2201.09450v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09450">
<div class="article-summary-box-inner">
<span><p>It is a challenging task to learn discriminative representation from images
and videos, due to large local redundancy and complex global dependency in
these visual data. Convolution neural networks (CNNs) and vision transformers
(ViTs) have been two dominant frameworks in the past few years. Though CNNs can
efficiently decrease local redundancy by convolution within a small
neighborhood, the limited receptive field makes it hard to capture global
dependency. Alternatively, ViTs can effectively capture long-range dependency
via self-attention, while blind similarity comparisons among all the tokens
lead to high redundancy. To resolve these problems, we propose a novel Unified
transFormer (UniFormer), which can seamlessly integrate the merits of
convolution and self-attention in a concise transformer format. Different from
the typical transformer blocks, the relation aggregators in our UniFormer block
are equipped with local and global token affinity respectively in shallow and
deep layers, allowing to tackle both redundancy and dependency for efficient
and effective representation learning. Finally, we flexibly stack our UniFormer
blocks into a new powerful backbone, and adopt it for various vision tasks from
image to video domain, from classification to dense prediction. Without any
extra training data, our UniFormer achieves 86.3 top-1 accuracy on ImageNet-1K
classification. With only ImageNet-1K pre-training, it can simply achieve
state-of-the-art performance in a broad range of downstream tasks, e.g., it
obtains 82.9/84.8 top-1 accuracy on Kinetics-400/600, 60.9/71.2 top-1 accuracy
on Something-Something V1/V2 video classification tasks, 53.8 box AP and 46.4
mask AP on COCO object detection task, 50.8 mIoU on ADE20K semantic
segmentation task, and 77.4 AP on COCO pose estimation task. Code is available
at https://github.com/Sense-X/UniFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proximal denoiser for convergent plug-and-play optimization with nonconvex regularization. (arXiv:2201.13256v2 [math.OC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.13256">
<div class="article-summary-box-inner">
<span><p>Plug-and-Play (PnP) methods solve ill-posed inverse problems through
iterative proximal algorithms by replacing a proximal operator by a denoising
operation. When applied with deep neural network denoisers, these methods have
shown state-of-the-art visual performance for image restoration problems.
However, their theoretical convergence analysis is still incomplete. Most of
the existing convergence results consider nonexpansive denoisers, which is
non-realistic, or limit their analysis to strongly convex data-fidelity terms
in the inverse problem to solve. Recently, it was proposed to train the
denoiser as a gradient descent step on a functional parameterized by a deep
neural network. Using such a denoiser guarantees the convergence of the PnP
version of the Half-Quadratic-Splitting (PnP-HQS) iterative algorithm. In this
paper, we show that this gradient denoiser can actually correspond to the
proximal operator of another scalar function. Given this new result, we exploit
the convergence theory of proximal algorithms in the nonconvex setting to
obtain convergence results for PnP-PGD (Proximal Gradient Descent) and PnP-ADMM
(Alternating Direction Method of Multipliers). When built on top of a smooth
gradient denoiser, we show that PnP-PGD and PnP-ADMM are convergent and target
stationary points of an explicit functional. These convergence results are
confirmed with numerical experiments on deblurring, super-resolution and
inpainting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Coding Framework and Benchmark towards Compressed Video Understanding. (arXiv:2202.02813v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02813">
<div class="article-summary-box-inner">
<span><p>Most video understanding methods are learned on high-quality videos. However,
in real-world scenarios, the videos are first compressed before the
transportation and then decompressed for understanding. The decompressed videos
may have lost the critical information to the downstream tasks. To address this
issue, we propose the first coding framework for compressed video
understanding, where another learnable analytic bitstream is simultaneously
transported with the original video bitstream. With the dedicatedly designed
self-supervised optimization target and dynamic network architectures, this new
stream largely boosts the downstream tasks yet with a small bit cost. By only
one-time training, our framework can be deployed for multiple downstream tasks.
Our framework also enjoys the best of both two worlds, (1) high efficiency of
industrial video codec and (2) flexible coding capability of neural networks
(NNs). Finally, we build a rigorous benchmark for compressed video
understanding on three popular tasks over seven large-scale datasets and four
different compression levels. The proposed Understanding oriented Video Coding
framework UVC consistently demonstrates significantly stronger performances
than the baseline industrial codec.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patch-Based Stochastic Attention for Image Editing. (arXiv:2202.03163v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03163">
<div class="article-summary-box-inner">
<span><p>Attention mechanisms have become of crucial importance in deep learning in
recent years. These non-local operations, which are similar to traditional
patch-based methods in image processing, complement local convolutions.
However, computing the full attention matrix is an expensive step with a heavy
memory and computational load. These limitations curb network architectures and
performances, in particular for the case of high resolution images. We propose
an efficient attention layer based on the stochastic algorithm PatchMatch,
which is used for determining approximate nearest neighbors. We refer to our
proposed layer as a "Patch-based Stochastic Attention Layer" (PSAL).
Furthermore, we propose different approaches, based on patch aggregation, to
ensure the differentiability of PSAL, thus allowing end-to-end training of any
network containing our layer. PSAL has a small memory footprint and can
therefore scale to high resolution images. It maintains this footprint without
sacrificing spatial precision and globality of the nearest neighbours, which
means that it can be easily inserted in any level of a deep architecture, even
in shallower levels. We demonstrate the usefulness of PSAL on several image
editing tasks, such as image inpainting and image colorization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Paced Imbalance Rectification for Class Incremental Learning. (arXiv:2202.03703v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03703">
<div class="article-summary-box-inner">
<span><p>Exemplar-based class-incremental learning is to recognize new classes while
not forgetting old ones, whose samples can only be saved in limited memory. The
ratio fluctuation of new samples to old exemplars, which is caused by the
variation of memory capacity at different environments, will bring challenges
to stabilize the incremental optimization process. To address this problem, we
propose a novel self-paced imbalance rectification scheme, which dynamically
maintains the incremental balance during the representation learning phase.
Specifically, our proposed scheme consists of a frequency compensation strategy
that adjusts the logits margin between old and new classes with the
corresponding number ratio to strengthen the expression ability of the old
classes, and an inheritance transfer strategy to reduce the representation
confusion by estimating the similarity of different classes in the old
embedding space. Furthermore, a chronological attenuation mechanism is proposed
to mitigate the repetitive optimization of the older classes at multiple
step-wise increments. Extensive experiments on three benchmarks demonstrate
stable incremental performance, significantly outperforming the
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face2PPG: An unsupervised pipeline for blood volume pulse extraction from faces. (arXiv:2202.04101v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04101">
<div class="article-summary-box-inner">
<span><p>Photoplethysmography (PPG) signals have become a key technology in many
fields such as medicine, well-being, or sports. Our work proposes a set of
pipelines to extract remote PPG signals (rPPG) from the face, robustly,
reliably, and in a configurable manner. We identify and evaluate the possible
choices in the critical steps of unsupervised rPPG methodologies. We evaluate a
state-of-the-art processing pipeline in six different datasets, incorporating
important corrections in the methodology that ensure reproducible and fair
comparisons. In addition, we extend the pipeline by proposing three novel
ideas; 1) a new method to stabilize the detected face based on a rigid mesh
normalization; 2) a new method to dynamically select the different regions in
the face that provide the best raw signals, and 3) a new RGB to rPPG
transformation method called Orthogonal Matrix Image Transformation (OMIT)
based on QR decomposition, that increases robustness against compression
artifacts. We show that all three changes introduce noticeable improvements in
retrieving rPPG signals from faces, obtaining state-of-the-art results compared
with unsupervised, non-learning-based methodologies, and in some databases,
very close to supervised, learning-based methods. We perform a comparative
study to quantify the contribution of each proposed idea. In addition, we
depict a series of observations that could help in future implementations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion Sickness Modeling with Visual Vertical Estimation and Its Application to Autonomous Personal Mobility Vehicles. (arXiv:2202.06299v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06299">
<div class="article-summary-box-inner">
<span><p>Passengers (drivers) of level 3-5 autonomous personal mobility vehicles
(APMV) and cars can perform non-driving tasks, such as reading books and
smartphones, while driving. It has been pointed out that such activities may
increase motion sickness. Many studies have been conducted to build
countermeasures, of which various computational motion sickness models have
been developed. Many of these are based on subjective vertical conflict (SVC)
theory, which describes vertical changes in direction sensed by human sensory
organs vs. those expected by the central nervous system. Such models are
expected to be applied to autonomous driving scenarios. However, no current
computational model can integrate visual vertical information with vestibular
sensations.
</p>
<p>We proposed a 6 DoF SVC-VV model which add a visually perceived vertical
block into a conventional six-degrees-of-freedom SVC model to predict VV
directions from image data simulating the visual input of a human. Hence, a
simple image-based VV estimation method is proposed.
</p>
<p>As the validation of the proposed model, this paper focuses on describing the
fact that the motion sickness increases as a passenger reads a book while using
an AMPV, assuming that visual vertical (VV) plays an important role. In the
static experiment, it is demonstrated that the estimated VV by the proposed
method accurately described the gravitational acceleration direction with a low
mean absolute deviation. In addition, the results of the driving experiment
using an APMV demonstrated that the proposed 6 DoF SVC-VV model could describe
that the increased motion sickness experienced when the VV and gravitational
acceleration directions were different.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A precortical module for robust CNNs to light variations. (arXiv:2202.07432v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07432">
<div class="article-summary-box-inner">
<span><p>We present a simple mathematical model for the mammalian low visual pathway,
taking into account its key elements: retina, lateral geniculate nucleus (LGN),
primary visual cortex (V1). The analogies between the cortical level of the
visual system and the structure of popular CNNs, used in image classification
tasks, suggests the introduction of an additional preliminary convolutional
module inspired to precortical neuronal circuits to improve robustness with
respect to global light intensity and contrast variations in the input images.
We validate our hypothesis on the popular databases MNIST, FashionMNIST and
SVHN, obtaining significantly more robust CNNs with respect to these
variations, once such extra module is added.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuously Learning to Detect People on the Fly: A Bio-inspired Visual System for Drones. (arXiv:2202.08023v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08023">
<div class="article-summary-box-inner">
<span><p>This paper demonstrates for the first time that a biologically-plausible
spiking neural network (SNN) equipped with Spike-Timing-Dependent Plasticity
(STDP) can continuously learn to detect walking people on the fly using
retina-inspired, event-based cameras. Our pipeline works as follows. First, a
short sequence of event data ($&lt;2$ minutes), capturing a walking human by a
flying drone, is forwarded to a convolutional SNNSTDP system which also
receives teacher spiking signals from a readout (forming a semi-supervised
system). Then, STDP adaptation is stopped and the learned system is assessed on
testing sequences. We conduct several experiments to study the effect of key
parameters in our system and to compare it against conventionally-trained CNNs.
We show that our system reaches a higher peak $F_1$ score (+19%) compared to
CNNs with event-based camera frames, while enabling on-line adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Did It Happen? Duration-informed Temporal Localization of Narrated Actions in Vlogs. (arXiv:2202.08138v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08138">
<div class="article-summary-box-inner">
<span><p>We consider the task of temporal human action localization in lifestyle
vlogs. We introduce a novel dataset consisting of manual annotations of
temporal localization for 13,000 narrated actions in 1,200 video clips. We
present an extensive analysis of this data, which allows us to better
understand how the language and visual modalities interact throughout the
videos. We propose a simple yet effective method to localize the narrated
actions based on their expected duration. Through several experiments and
analyses, we show that our method brings complementary information with respect
to previous methods, and leads to improvements over previous work for the task
of temporal action localization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextualize differential privacy in image database: a lightweight image differential privacy approach based on principle component analysis inverse. (arXiv:2202.08309v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08309">
<div class="article-summary-box-inner">
<span><p>Differential privacy (DP) has been the de-facto standard to preserve
privacy-sensitive information in database. Nevertheless, there lacks a clear
and convincing contextualization of DP in image database, where individual
images' indistinguishable contribution to a certain analysis can be achieved
and observed when DP is exerted. As a result, the privacy-accuracy trade-off
due to integrating DP is insufficiently demonstrated in the context of
differentially-private image database. This work aims at contextualizing DP in
image database by an explicit and intuitive demonstration of integrating
conceptional differential privacy with images. To this end, we design a
lightweight approach dedicating to privatizing image database as a whole and
preserving the statistical semantics of the image database to an adjustable
level, while making individual images' contribution to such statistics
indistinguishable. The designed approach leverages principle component analysis
(PCA) to reduce the raw image with large amount of attributes to a lower
dimensional space whereby DP is performed, so as to decrease the DP load of
calculating sensitivity attribute-by-attribute. The DP-exerted image data,
which is not visible in its privatized format, is visualized through PCA
inverse such that both a human and machine inspector can evaluate the
privatization and quantify the privacy-accuracy trade-off in an analysis on the
privatized image database. Using the devised approach, we demonstrate the
contextualization of DP in images by two use cases based on deep learning
models, where we show the indistinguishability of individual images induced by
DP and the privatized images' retention of statistical semantics in deep
learning tasks, which is elaborated by quantitative analyses on the
privacy-accuracy trade-off under different privatization settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Colonoscopy polyp detection with massive endoscopic images. (arXiv:2202.08730v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08730">
<div class="article-summary-box-inner">
<span><p>We improved an existing end-to-end polyp detection model with better average
precision validated by different data sets with trivial cost on detection
speed. Our previous work on detecting polyps within colonoscopy provided an
efficient end-to-end solution to alleviate doctor's examination overhead.
However, our later experiments found this framework is not as robust as before
as the condition of polyp capturing varies. In this work, we conducted several
studies on data set, identifying main issues that causes low precision rate in
the task of polyp detection. We used an optimized anchor generation methods to
get better anchor box shape and more boxes are used for detection as we believe
this is necessary for small object detection. A alternative backbone is used to
compensate the heavy time cost introduced by dense anchor box regression. With
use of the attention gate module, our model can achieve state-of-the-art polyp
detection performance while still maintain real-time detection speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLP: A Survey on Vision-Language Pre-training. (arXiv:2202.09061v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09061">
<div class="article-summary-box-inner">
<span><p>In the past few years, the emergence of pre-training models has brought
uni-modal fields such as computer vision (CV) and natural language processing
(NLP) to a new era. Substantial works have shown they are beneficial for
downstream uni-modal tasks and avoid training a new model from scratch. So can
such pre-trained models be applied to multi-modal tasks? Researchers have
explored this problem and made significant progress. This paper surveys recent
advances and new frontiers in vision-language pre-training (VLP), including
image-text and video-text pre-training. To give readers a better overall grasp
of VLP, we first review its recent advances from five aspects: feature
extraction, model architecture, pre-training objectives, pre-training datasets,
and downstream tasks. Then, we summarize the specific VLP models in detail.
Finally, we discuss the new frontiers in VLP. To the best of our knowledge,
this is the first survey on VLP. We hope that this survey can shed light on
future research in the VLP field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Multiple-Object Tracking with a Dynamical Variational Autoencoder. (arXiv:2202.09315v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09315">
<div class="article-summary-box-inner">
<span><p>In this paper, we present an unsupervised probabilistic model and associated
estimation algorithm for multi-object tracking (MOT) based on a dynamical
variational autoencoder (DVAE), called DVAE-UMOT. The DVAE is a latent-variable
deep generative model that can be seen as an extension of the variational
autoencoder for the modeling of temporal sequences. It is included in DVAE-UMOT
to model the objects' dynamics, after being pre-trained on an unlabeled
synthetic dataset of single-object trajectories. Then the distributions and
parameters of DVAE-UMOT are estimated on each multi-object sequence to track
using the principles of variational inference: Definition of an approximate
posterior distribution of the latent variables and maximization of the
corresponding evidence lower bound of the data likehood function. DVAE-UMOT is
shown experimentally to compete well with and even surpass the performance of
two state-of-the-art probabilistic MOT models. Code and data are publicly
available.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-02-22 23:08:11.363706097 UTC">2022-02-22 23:08:11 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>